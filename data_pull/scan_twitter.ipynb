{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c39a43-66c2-445c-9f99-f40de494fde5",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "May 5, 2022\n",
    "## Description:\n",
    "In this notebook we collect data from Twitter, looking for tweets about new arXiv submission, written by the paper's author.\n",
    "\n",
    "To access Twitter database, we employ Twitter API (v2) using the tweepy package. We use Academic Research access (which is superior compared to usual access plans primarily due to the access to the entire databse, not only the last week). The bearer_token is saved in twitter_key.txt.\n",
    "\n",
    "To access arXiv, we use a simple HTML parser as defined in arxiv_utils.py. Its job is to access export.arxiv.org/abs/xxxx.xxxxx pages, read the title, author list and abstract. Currently we do not access the paper.\n",
    "\n",
    "The mining algorithm is the following:\n",
    "\n",
    "<code>\n",
    "for each month in range of years (~2016 to 2022)\n",
    "    search Twitter for tweets in month (plus 15 days into next month) \n",
    "        use query 'arxiv.org/abs/yymm' + 'new paper' or 'we find', etc.\n",
    "        append found tweets to tweets list\n",
    "for every tweet in tweets list\n",
    "    find arXiv link and mine arXiv data\n",
    "    retain tweet if the tweet was written by the one of the authors listed on arXiv (by some similarity metric)\n",
    "        search Twitter for subsequent thread of the tweet, if exists\n",
    "save dataset of {TweetID, AuthorID, AuthorName, Tweet thread, arXiv link, abstract}\n",
    "</code>\n",
    "\n",
    "### Reasoning of some design choices:\n",
    "We search Twitter month by month because (i) we find that the Twitter search function does not handle queries well for old papers well if a ~month time range is not specified; (ii) queries typically return < 500 results, not exceeding the maximum amount per query; (iii) the search can fail due to several reasons (max limit, connection error, etc.), therefore it is convenient to restrict it to a month.\n",
    "\n",
    "We impose early that the tweet was written by an author because many tweets are written by bots or unrelated people, which is data that we are not interested in.\n",
    "\n",
    "## Ways to improve:\n",
    "The methodology here is fairly basic and could be made more sophisticated, to the end goal of collecting overlooked data and avoid contamination of irrelevant data.\n",
    "1) We currently do not use Pagination, which allows for more than 500 results per query. The main reason is that it seemed tricky to get the author names for tweets with Pagination. In any case, the current queries (per month) seem to cap at ~200, therefore it is not a serious limitation.\n",
    "2) The queries can be revised and be made more comprehensive.\n",
    "3) To determine whether a tweet was written by a paper's author, we compare the tweet's author to the arXiv author list, and comparing them using a similarity metric (see method clean_coarse_list). This similarity metric could be improved.\n",
    "4) We try to expand links in various ways (see method link_entities_to_clean_arxiv_list in tweet_utils) but fail in some. For example, the url-expanders fail with Facebook-redirection (e.g. https://t.co/tr7qNaQjMd)\n",
    "5) It would be very effective if the initial query would also download the entire thread for each tweet. Currently, every tweet from the initial query (passing some quality criteria) has to be sent in its own query (see method get_tweets_thread). This is quite expensive because each query has 1 second downtime due to Twitter's rate limits.\n",
    "6) Save more data, perhaps even raw tweets for any case.\n",
    "7) Access other forms of data from tweets: links to other websites (e.g. GitHub), pictures and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d440f49-e859-4393-bb2b-4ab83753d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import tweet_utils\n",
    "import arxiv_utils\n",
    "import pandas as pd\n",
    "import time\n",
    "import difflib\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5938f9f-815f-44ae-b6f7-24bc344241a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Twitter client with proper authentication\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# we save the Twitter project \"bearer token\" in twitter_key.txt two folders up. Can easily change to another directory.\n",
    "path_here = os.getcwd()\n",
    "path = Path(path_here)\n",
    "with open(str(path.parent.parent.absolute())+'/twitter_key.txt') as key_f:\n",
    "    bearer_token = key_f.read()\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75fb7634-1cf6-49a4-9a66-6136aa25c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions, may move it to separate .py file\n",
    "\n",
    "def next_month_yr_mon(year,month):\n",
    "    if month<12:\n",
    "        return year, month+1\n",
    "    else:\n",
    "        return year+1, 1\n",
    "        \n",
    "# converts a year and month to start_time and end_time\n",
    "def year_month_to_start_end_time(year, month, extra_days=15, hour_diff=-3):\n",
    "    \n",
    "    new_yr, new_mon = next_month_yr_mon(year,month)\n",
    "    # standard values for looking into past \n",
    "    start_time = str(year)+'-'+str(month)+'-'+str(1)+'T00:00:00Z'\n",
    "    end_time = str(new_yr)+'-'+str(new_mon)+'-'+str(extra_days)+'T00:00:00Z'\n",
    "\n",
    "    # check if end_time > current time, and regulate it\n",
    "    t = time.gmtime()\n",
    "    if (new_yr > t.tm_year) or (new_yr == t.tm_year and new_mon > t.tm_mon) or (new_yr == t.tm_year and new_mon == t.tm_mon and extra_days >= t.tm_mday):\n",
    "        end_time = str(t.tm_year)+'-'+str(t.tm_mon)+'-'+str(t.tm_mday)+'T'+str(t.tm_hour+hour_diff)+':'+str(t.tm_min)+':'+str(t.tm_sec)+'Z'\n",
    "    \n",
    "    return start_time, end_time\n",
    "\n",
    "# generates a list of [yr, month] from [first_yr, first_mon] to [last_yr, last_mon]\n",
    "def get_year_month_list(first_yr, first_mon, last_yr, last_mon):\n",
    "    mon_block = []\n",
    "    for yr in range(first_yr, last_yr+1):\n",
    "        last_mon_i = 12 if yr<last_yr else last_mon\n",
    "        first_mon_i = 1 if yr>first_yr else first_mon\n",
    "        \n",
    "        mon_block.append([[yr,month] for month in range(first_mon_i,last_mon_i+1)])\n",
    "    mon_block = [num for sublist in mon_block for num in sublist]\n",
    "    return mon_block\n",
    "\n",
    "def author_overlap_metric(str1,str2):\n",
    "    d = difflib.SequenceMatcher(None,str1,str2)\n",
    "    match = max(d.get_matching_blocks(),key=lambda x:x[2])\n",
    "    return match.size/max(len(str1),len(str2))\n",
    "\n",
    "def clean_tweet_from_links(tweet, link_token='<LINK>'):\n",
    "    s = tweet['text']\n",
    "    if tweet.entities != None:\n",
    "        for i in range(len(tweet.entities['urls'])):\n",
    "            s = s.replace(tweet.entities['urls'][i]['url'],link_token)\n",
    "    return s\n",
    "\n",
    "def get_username(user_id,user_list):\n",
    "    for user in user_list:\n",
    "        if user.id == user_id:\n",
    "            return user.name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c013ebfd-035c-4c8f-89ae-eff7699d5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_coarse_data_for(year=2022,month=4):\n",
    "    coarse_tweets = []\n",
    "    start_time, end_time = year_month_to_start_end_time(year, month)\n",
    "    arxiv_prompt_1 = 'arxiv.org/abs/'+str((year-2000))+(str(month) if month>=10 else '0'+str(month)) ## set /abs/yymm format\n",
    "    arxiv_prompt_2 = 'arxiv.org/pdf/'+str((year-2000))+(str(month) if month>=10 else '0'+str(month)) ## set /pdf/yymm format\n",
    "    \n",
    "    tweets_1 = client.search_all_tweets(query=arxiv_prompt_1+' new paper -replaced -is:retweet -is:reply -from:750791077012107264', \n",
    "                                  tweet_fields=['author_id','entities','conversation_id','created_at'], user_fields=['name'], expansions=['author_id'], \n",
    "                                  max_results=500, start_time=start_time, end_time=end_time)\n",
    "    time.sleep(1.1) # to avoid exceeding the rate\n",
    "    tweets_2 = client.search_all_tweets(query=arxiv_prompt_2+' new paper -replaced -is:retweet -is:reply', \n",
    "                                  tweet_fields=['author_id','entities','conversation_id','created_at'], user_fields=['name'], expansions=['author_id'], \n",
    "                                  max_results=500, start_time=start_time, end_time=end_time)\n",
    "    time.sleep(1.1)\n",
    "    tweets_3 = client.search_all_tweets(query=arxiv_prompt_1+' we (study OR studied OR find OR propose) -is:retweet -is:reply', \n",
    "                                  tweet_fields=['author_id','entities','conversation_id','created_at'], user_fields=['name'], expansions=['author_id'], \n",
    "                                  max_results=500, start_time=start_time, end_time=end_time)\n",
    "    time.sleep(1.1)\n",
    "    tweets_4 = client.search_all_tweets(query=arxiv_prompt_2+' we (study OR studied OR find OR found OR propose) -is:retweet -is:reply', \n",
    "                                  tweet_fields=['author_id','entities','conversation_id','created_at'], user_fields=['name'], expansions=['author_id'], \n",
    "                                  max_results=500, start_time=start_time, end_time=end_time)\n",
    "    time.sleep(1.1)\n",
    "\n",
    "    coarse_tweets.append(tweets_1)\n",
    "    coarse_tweets.append(tweets_2)\n",
    "    coarse_tweets.append(tweets_3)\n",
    "    coarse_tweets.append(tweets_4)\n",
    "    return coarse_tweets\n",
    "\n",
    "def clean_coarse_list(coarse_tweets, author_closeness_cut = 0.3):\n",
    "    # 2nd run on Twitter: keep only tweets whose authors appear on arXiv link (by some similarity metric)\n",
    "# rearrange data to one list of (tweets, users, urls)\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    for coarse_tweets_i in coarse_tweets:\n",
    "        print('new query')\n",
    "        if coarse_tweets_i.data == None:\n",
    "            print('Empty query')\n",
    "            continue\n",
    "            \n",
    "        user_list = coarse_tweets_i.includes['users']\n",
    "        tweet_data = coarse_tweets_i.data\n",
    "\n",
    "        for j, tweet in enumerate(coarse_tweets_i.data):\n",
    "            user_name = get_username(tweet.author_id, user_list)\n",
    "            \n",
    "            #tweet_urls = tweet_utils.get_urls_single_tweet(tweet)\n",
    "            tweet_urls_dict = tweet.entities['urls']\n",
    "            url_list = tweet_utils.link_entities_to_clean_arxiv_list(tweet_urls_dict)\n",
    "\n",
    "            #new_url_list = tweet_utils.remove_non_arxiv_and_replace_pdf_single(tweet_urls)\n",
    "\n",
    "            if len(url_list) == 0:\n",
    "                print('EMPTY!')\n",
    "                print(tweet.entities['urls'])\n",
    "                continue\n",
    "\n",
    "            count = 0\n",
    "            while True:\n",
    "                try: # try to get arXiv details. Some\n",
    "                    arxiv_title, author_list, paper_abstract = arxiv_utils.get_arXiv_details(url_list[0])\n",
    "                except:\n",
    "                    print('arXiv connection failed. Trying again. Attempt: ' + str(count+1))\n",
    "                    count = count+1\n",
    "                    time.sleep(1)\n",
    "                    if count>9:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                break\n",
    "            \n",
    "            #print('Compare: ' + \",\".join(author_list) + ' vs. ' + user_name)\n",
    "            author_closeness_metric = [author_overlap_metric(author,user_name) for author in author_list]\n",
    "\n",
    "            if len(author_closeness_metric) == 0: # somehow link passed previous checks but still broken somehow... avoid\n",
    "                continue\n",
    "            if max(author_closeness_metric) > author_closeness_cut:\n",
    "                #print('Passed author cut!')\n",
    "                tweets.append((tweet, user_name, url_list, paper_abstract, arxiv_title))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def get_tweets_thread(tweets, add_threads_flag=True, single_tweet_token='<SNGTWT>',\n",
    "                      multi_tweet_token='<MLTTWT>', LINK_TOKEN = '<LINK>'):\n",
    "# 3rd run: look for underlying threads in Twitter\n",
    "\n",
    "    tweetIDs = []\n",
    "    texts = []\n",
    "    links = []\n",
    "    authorID = []\n",
    "    authorName = []\n",
    "    abstracts = []\n",
    "    arxivTitle = []\n",
    "\n",
    "    for i, (tweet, user_name, url_list, abstract, arxiv_title) in enumerate(tweets):\n",
    "\n",
    "        # keep some useful information\n",
    "        authorID.append(tweet.author_id)\n",
    "        tweetIDs.append(tweet['id'])\n",
    "        abstracts.append(abstract)\n",
    "        authorName.append(user_name)\n",
    "        arxivTitle.append(arxiv_title)\n",
    "        \n",
    "        first_tweet_text = tweet['text']\n",
    "        first_tweet_text = clean_tweet_from_links(tweet, link_token=LINK_TOKEN) # in case we want to remove links\n",
    "\n",
    "        # Look for additional tweets in the thread\n",
    "        if add_threads_flag:\n",
    "            query = 'conversation_id:' + str(tweet['id']) + ' from:' + str(tweet.author_id)\n",
    "            #thread_tweets = client.search_all_tweets(query=query, tweet_fields=['author_id','entities','conversation_id'], \n",
    "            #                                         user_fields=['name'], expansions=['author_id'], max_results=10)\n",
    "            create_date = tweet.created_at\n",
    "            start_time = (create_date + timedelta(hours=-5)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            end_time = (create_date + timedelta(hours=15)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "            thread_tweets = client.search_all_tweets(query=query, tweet_fields=['author_id'], max_results=500, start_time=start_time, end_time=end_time)\n",
    "\n",
    "            thread_text = ''\n",
    "            if thread_tweets.data != None:\n",
    "                for tweet_thread in thread_tweets.data:\n",
    "                    thread_text = clean_tweet_from_links(tweet_thread, link_token=LINK_TOKEN) + thread_text #tweet_thread['text'] + thread_text\n",
    "                texts.append(multi_tweet_token + first_tweet_text + thread_text)\n",
    "            else:\n",
    "                texts.append(single_tweet_token + first_tweet_text)\n",
    "        else:\n",
    "            texts.append(first_tweet_text)\n",
    "\n",
    "        links.append(url_list[0])\n",
    "\n",
    "        time.sleep(1) # somehow works better...\n",
    "\n",
    "    texts = [text.replace('\\n',' ') for text in texts]\n",
    "    \n",
    "    data = {'TweetID': tweetIDs,\n",
    "        'AuthorID': authorID,\n",
    "        'AuthorName': authorName,\n",
    "        'Tweet': texts,\n",
    "        'arxiv': links,\n",
    "        'Abstract': abstracts,\n",
    "        'Title': arxivTitle}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_tweets_thread_list(tweets, add_threads_flag=True, single_tweet_token='<SNGTWT>',\n",
    "                      multi_tweet_token='<MLTTWT>', LINK_TOKEN = '<LINK>'):\n",
    "# 3rd run: look for underlying threads in Twitter\n",
    "\n",
    "    tweetIDs = []\n",
    "    tweets_text = []\n",
    "    links = []\n",
    "    authorID = []\n",
    "    authorName = []\n",
    "    abstracts = []\n",
    "    arxivTitle = []\n",
    "\n",
    "    for i, (tweet, user_name, url_list, abstract, arxiv_title) in enumerate(tweets):\n",
    "\n",
    "        # keep some useful information\n",
    "        authorID.append(tweet.author_id)\n",
    "        tweetIDs.append(tweet['id'])\n",
    "        abstracts.append(abstract)\n",
    "        authorName.append(user_name)\n",
    "        arxivTitle.append(arxiv_title)\n",
    "        \n",
    "        first_tweet_text = tweet['text']\n",
    "        first_tweet_text = clean_tweet_from_links(tweet, link_token=LINK_TOKEN) # in case we want to remove links\n",
    "        \n",
    "        tweet_list = []\n",
    "        # Look for additional tweets in the thread\n",
    "        if add_threads_flag:\n",
    "            query = 'conversation_id:' + str(tweet['id']) + ' from:' + str(tweet.author_id)\n",
    "            #thread_tweets = client.search_all_tweets(query=query, tweet_fields=['author_id','entities','conversation_id'], \n",
    "            #                                         user_fields=['name'], expansions=['author_id'], max_results=10)\n",
    "            create_date = tweet.created_at\n",
    "            start_time = (create_date + timedelta(hours=-5)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            end_time = (create_date + timedelta(hours=15)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "            thread_tweets = client.search_all_tweets(query=query, tweet_fields=['author_id'], max_results=500, start_time=start_time, end_time=end_time)\n",
    "\n",
    "            thread_text = ''\n",
    "            if thread_tweets.data != None:\n",
    "                for tweet_thread in thread_tweets.data:\n",
    "                    tweet_list.insert(0, clean_tweet_from_links(tweet_thread, link_token=LINK_TOKEN)) #tweet_thread['text'] + thread_text\n",
    "                #texts.append(multi_tweet_token + first_tweet_text + thread_text)\n",
    "            #else:\n",
    "                #texts.append(single_tweet_token + first_tweet_text)\n",
    "        #else:\n",
    "            #texts.append(first_tweet_text)\n",
    "        tweet_list.insert(0, first_tweet_text)\n",
    "\n",
    "        links.append(url_list[0])\n",
    "        tweets_text.append(tweet_list)\n",
    "\n",
    "        time.sleep(1) # somehow works better...\n",
    "\n",
    "    #texts = [text.replace('\\n',' ') for text in texts]\n",
    "    \n",
    "    data = {'TweetID': tweetIDs,\n",
    "        'AuthorID': authorID,\n",
    "        'AuthorName': authorName,\n",
    "        'Tweets': tweets_text,\n",
    "        'arxiv_link': links,\n",
    "        'Abstract': abstracts,\n",
    "        'Title': arxivTitle}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a32e8c0f-5869-4a29-9ed5-508ac5cee6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [[2016, [1,12]],\n",
    "         [2017, [1,12]],\n",
    "         [2018, [1,12]],\n",
    "         [2019, [1,12]],\n",
    "         [2020, [1,12]],\n",
    "         [2021, [1,12]],\n",
    "         [2022, [1,4]]]\n",
    "\n",
    "to_do_yr_mon = []\n",
    "for year, months in years:\n",
    "    for month in range(months[0],months[1]+1):\n",
    "        to_do_yr_mon.append([year,month])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf9e9518-ca7b-4b18-93b7-af1f5e3eecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new query\n",
      "Gave up on https://github.com/drprojects/DeepViewAgg\n",
      "Gave up on https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model\n",
      "Gave up on https://twitter.com/DeepMind/status/1519686445258231811\n",
      "Gave up on https://www.linkedin.com/feed/update/urn:li:share:6927525467763716096\n",
      "Gave up on https://github.com/jwwangchn/BONAI\n",
      "Gave up on https://github.com/christophebedard/ros2-message-flow-analysis\n",
      "Gave up on https://isorc2022.github.io/\n",
      "Gave up on https://twitter.com/yayitsamyzhang/status/1520105865365270529\n",
      "Gave up on https://twitter.com/PhilippSiedler/status/1519788983852711937/video/1\n",
      "Gave up on https://twitter.com/PlotAstro/status/1518603424845152257\n",
      "Gave up on https://tweetedtimes.com/v/9144\n",
      "Gave up on https://schemathesis.io:443/\n",
      "Gave up on https://github.com/philipph77/ACSE-Framework\n",
      "Gave up on https://twitter.com/astroash42/status/1508718414055059458\n",
      "Gave up on https://twitter.com/KhoaVuUmn/status/1484634165714853890\n",
      "Gave up on https://www.youtube.com/watch\n",
      "Gave up on https://www.youtube.com/watch\n",
      "Gave up on https://openai.com/dall-e-2/\n",
      "Gave up on https://twitter.com/nickcammarata/status/1511861061988892675\n",
      "Gave up on https://github.com/kbatsuren/KinDiv\n",
      "Gave up on https://www.youtube.com/watch\n",
      "Gave up on https://github.com/google-research-datasets/taperception\n",
      "Gave up on https://www.youtube.com/watch\n",
      "Gave up on https://twitter.com/teddykareta/status/1510032796021977091\n",
      "new query\n",
      "Gave up on https://github.com/amazon-research/wikiwiki-dataset\n",
      "Gave up on https://github.com/tikhonovpavel/wikimulti\n",
      "Gave up on https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding\n",
      "Gave up on https://github.com/Kaleidophon/deep-significance\n",
      "Gave up on https://twitter.com/Underfox3/status/1516281932375601154\n",
      "Gave up on https://twitter.com/federicobianchy/status/1513526080279306249\n",
      "new query\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/KeRbNmnfoI', 'expanded_url': 'https://bit.ly/3vxm2A8', 'display_url': 'bit.ly/3vxm2A8'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/nRr7Cpiifj', 'expanded_url': 'https://bit.ly/3KyS578', 'display_url': 'bit.ly/3KyS578'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/zgGVI2l5Us', 'expanded_url': 'https://bit.ly/3OIkB9v', 'display_url': 'bit.ly/3OIkB9v'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/uZmm1RE20O', 'expanded_url': 'https://bit.ly/39kD1gy', 'display_url': 'bit.ly/39kD1gy'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/O77ozOu47E', 'expanded_url': 'https://bit.ly/3Lnohvx', 'display_url': 'bit.ly/3Lnohvx'}]\n",
      "Gave up on http://science.org/__CLIENT_ERROR__\n",
      "Gave up on https://twitter.com/astroash42/status/1508718414055059458\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/xYcpaTnYyM', 'expanded_url': 'https://bit.ly/3JVp2dB', 'display_url': 'bit.ly/3JVp2dB'}]\n",
      "Gave up on https://gitlab.inria.fr/epione/federated-multi-views-ppca\n",
      "Gave up on https://blogs.ncl.ac.uk/quasarfeedbacksurvey/\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/H2HhiL6Loh', 'expanded_url': 'https://bit.ly/3vurpPB', 'display_url': 'bit.ly/3vurpPB'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/EPLNzC2xVH', 'expanded_url': 'https://bit.ly/3uwXgji', 'display_url': 'bit.ly/3uwXgji'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/ZkBOPM3Bo9', 'expanded_url': 'https://bit.ly/3jeOjEP', 'display_url': 'bit.ly/3jeOjEP'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/itqVSVXdM6', 'expanded_url': 'https://bit.ly/3NQnbtR', 'display_url': 'bit.ly/3NQnbtR'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/oTNGJTxhKf', 'expanded_url': 'https://bit.ly/3jaTDcg', 'display_url': 'bit.ly/3jaTDcg'}]\n",
      "Gave up on https://www.facebook.com/flx/warn/\n",
      "EMPTY!\n",
      "[{'start': 259, 'end': 282, 'url': 'https://t.co/eINPgTI31R', 'expanded_url': 'https://bit.ly/3j3bJwG', 'display_url': 'bit.ly/3j3bJwG'}]\n",
      "new query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for extraction rate or connection reasons, the process sometimes crashes. For now, we retain this part of the notebook \n",
    "count = 0\n",
    "yr_mon = to_do_yr_mon.copy()\n",
    "\n",
    "for yr, month in yr_mon:\n",
    "    try:\n",
    "        coarse_tweets = obtain_coarse_data_for(year=yr,month=month)\n",
    "        tweets = clean_coarse_list(coarse_tweets)\n",
    "        df = get_tweets_thread_list(tweets)\n",
    "        df.to_csv('output_n/data'+str(yr)+'_'+str(month)+'.csv')\n",
    "        to_do_yr_mon.remove([yr, month])\n",
    "        time.sleep(1)\n",
    "    except ConnectionError:#Exception: \n",
    "        print('------Error encountered. Will try again later-------')\n",
    "        print(str(Exception))\n",
    "    count = count + 1\n",
    "    if count > 100:\n",
    "        break\n",
    "        \n",
    "to_do_yr_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28454136-b086-4e11-8d3c-48c43e0cf0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b8c6bb3c-5750-4b50-8e7c-c375f6c057d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have published a new blog entry on our recent paper (<LINK>) on #2HDM at <LINK> #np3']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important for Tweets list evaluation\n",
    "\n",
    "import ast\n",
    "ast.literal_eval(file['Tweets'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9042ef-671c-4bc6-9e77-b68206d63260",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8f4015d-699a-49fa-84dd-b22d87cf20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# All files and directories ending with .txt and that don't begin with a dot:\n",
    "files = glob.glob(\"output/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f4a8ff2-e532-498e-b408-4f451abbdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(files[0])\n",
    "for i in range(1,len(files)):\n",
    "    df_i = pd.read_csv(files[i])\n",
    "    if len(df_i) > 0:\n",
    "        #df_all.append(pd.read_csv(files[i]))\n",
    "        df_all = pd.concat([df_all, df_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb6a6641-eedd-4551-8b92-25a641e14762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf547f19-053d-48c3-b7c6-b6b841469fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07eac4dd-b046-4e5b-aba1-3581f7247a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b6eceb-deaf-4228-a7d8-65011fdc7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('data_x.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c0036-7776-4409-a924-b1b5caeb30c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5d12a-3019-433a-8756-12f3dd3af013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
