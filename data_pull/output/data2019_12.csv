,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1214929544236240896,990346198908223488,Nikolai Matni,"['New paper on the Sample Complexity of Kalman Filtering for Unknown Systems.  A setting where imposing robustness *improves* performance!  <LINK>, w/ @pappasg69 &amp; A. Tsiamis.']",https://arxiv.org/abs/1912.12309,"In this paper, we consider the task of designing a Kalman Filter (KF) for an unknown and partially observed autonomous linear time invariant system driven by process and sensor noise. To do so, we propose studying the following two step process: first, using system identification tools rooted in subspace methods, we obtain coarse finite-data estimates of the state-space parameters and Kalman gain describing the autonomous system; and second, we use these approximate parameters to design a filter which produces estimates of the system state. We show that when the system identification step produces sufficiently accurate estimates, or when the underlying true KF is sufficiently robust, that a Certainty Equivalent (CE) KF, i.e., one designed using the estimated parameters directly, enjoys provable sub-optimality guarantees. We further show that when these conditions fail, and in particular, when the CE KF is marginally stable (i.e., has eigenvalues very close to the unit circle), that imposing additional robustness constraints on the filter leads to similar sub-optimality guarantees. We further show that with high probability, both the CE and robust filters have mean prediction error bounded by $\tilde O(1/\sqrt{N})$, where $N$ is the number of data points collected in the system identification step. To the best of our knowledge, these are the first end-to-end sample complexity bounds for the Kalman Filtering of an unknown system. ",Sample Complexity of Kalman Filtering for Unknown Systems
1,1214750791833137153,145682585,Edwin Pacheco,['New paper available on ArXiv // Nuevo artículo disponible en ArXiv\n\n<LINK>'],https://arxiv.org/abs/1912.11413,"We compute all liftings of braidings of unidentified types ufo(7) and ufo(8). Some of these examples had been previously computed by Helbig. We also present a list of examples of liftings of modular type br(2,a). ","Examples of liftings of modular and unidentified type: ufo(7,8) and
  br(2,a)"
2,1214601140245012481,290099997,Marco Melis,"['New hotfix v0.11.2 for #SecML addressing a few issues with attack and optimization classes.\n\nAlso, check the new tutorial on advanced attacks using  #imagenet and #neuralnetworks from our paper:\n<LINK>\n\n<LINK>\n\nChangelog: <LINK> <LINK>']",https://arxiv.org/abs/1912.10013,"We present \texttt{secml}, an open-source Python library for secure and explainable machine learning. It implements the most popular attacks against machine learning, including test-time evasion attacks to generate adversarial examples against deep neural networks and training-time poisoning attacks against support vector machines and many other algorithms. These attacks enable evaluating the security of learning algorithms and the corresponding defenses under both white-box and black-box threat models. To this end, \texttt{secml} provides built-in functions to compute security evaluation curves, showing how quickly classification performance decreases against increasing adversarial perturbations of the input data. \texttt{secml} also includes explainability methods to help understand why adversarial attacks succeed against a given model, by visualizing the most influential features and training prototypes contributing to each decision. It is distributed under the Apache License 2.0 and hosted at \url{this https URL}. ",secml: A Python Library for Secure and Explainable Machine Learning
3,1214185967009054720,2385131,Dirk Eddelbuettel,"['New draft paper I worked on in December aiming to update the older JSS survey paper.  Feedback most welcome!\n\n<LINK>\n\nAnd big big thanks to @henrikbengtsson and @axiomsofxyz for some initial discussions, comments and suggestions. <LINK>']",https://arxiv.org/abs/1912.11144,"Parallel computing has established itself as another standard method for applied research and data analysis. The R system, being internally constrained to mostly singly-threaded operations, can nevertheless be used along with different parallel computing approaches. This brief review covers OpenMP and Intel TBB at the cpu- and compiler level, moves to process-parallel approaches before discussing message-passing parallelism and big data technologies for parallel processing such as Spark, Docker and Kubernetes before concluding with a focus on the future package integrating many of these approaches. ",Parallel Computing With R: A Brief Review
4,1213997876805697536,952949678533849088,Kareem El-Badry,"['Our LB-1 paper is now accepted, and the ArXiv version is updated. Our main conclusions are unchanged. We did notice something new: there is a narrow peak of the H-alpha emission near line center that tracks the B star (dashed vertical line in Fig 3). <LINK> <LINK>', 'Next up: modeling of where the whole H-alpha line comes from. Stay tuned!']",https://arxiv.org/abs/1912.04185,"The recently discovered binary LB-1 has been reported to contain a $\sim$\,$70\,M_{\odot}$ black hole (BH). The evidence for the unprecedentedly high mass of the unseen companion comes from reported radial velocity (RV) variability of the H$\alpha$ emission line, which has been proposed to originate from an accretion disk around a BH. We show that there is in fact no evidence for RV variability of the H$\alpha$ emission line, and that its apparent shifts instead originate from shifts in the luminous star's H$\alpha$ absorption line. If not accounted for, such shifts will cause a stationary emission line to appear to shift in anti-phase with the luminous star. We show that once the template spectrum of a B star is subtracted from the observed Keck/HIRES spectra of LB-1, evidence for RV variability vanishes. Indeed, the data rule out periodic variability of the line with velocity semi-amplitude $K_{\rm H\alpha} > 1.3\,\rm km\,s^{-1}$. This strongly suggests that the observed H$\alpha$ emission does not originate primarily from an accretion disk around a BH, and thus that the mass ratio cannot be constrained from the relative velocity amplitudes of the emission and absorption lines. The nature of the unseen companion remains uncertain, but a ""normal"" stellar-mass BH with mass $5\lesssim M/M_{\odot}\lesssim 20 $ seems most plausible. The H$\alpha$ emission likely originates primarily from circumbinary material, not from either component of the binary. ",Not so fast: LB-1 is unlikely to contain a 70 $M_{\odot}$ black hole
5,1213889108751241217,2800204849,Andrew Gordon Wilson,"['“What I cannot create, I do not understand”. We develop normalizing flows for end-to-end fully generative semi-supervised classification (with code)! Our new paper, with @Pavel_Izmailov, @polkirichenko, @m_finzi: <LINK> (1/8) <LINK>', 'The discriminative approach to classification models the probability of a class label given an input p(y|x) directly. The generative, approach, by contrast, models the class conditional density p(x|y), then finds p(y|x) with Bayes rule. 2/8', 'Nearly all classifiers are discriminative. Even approaches that use a generator typically involve a discriminator in the pipeline. For example, sometimes one learns a generator on unlabelled data, then recycles the representation as part of a discriminative classifier. 3/8', 'Generative models are compelling because we are trying to create an object of interest. The challenge in generative modelling is that standard approaches to density estimation are poor descriptions of high-dimensional natural signals. 4/8', 'For example, a Gaussian mixture directly over images, while highly flexible for density estimation, would specify similarities between images as related to Euclidean distances between pixel intensities, which is a poor inductive bias for translation and other invariances. 5/8', 'Normalizing flows provide a pleasingly simple approach to generative modelling. By transforming a latent distribution through an invertible network, we have both an exact likelihood for the data, and useful inductive biases from a convolutional neural network. 6/8', 'FlowGMM models the latent space as a Gaussian mixture, where each mixture component is associated with a class label. This approach specifies an exact joint likelihood over both labelled and unlabelled data for end-to-end training. 7/8', 'FlowGMM has broad applicability. We consider text, tabular, and image data. FlowGMM can also discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions. 8/8', '@matvil @Pavel_Izmailov @polkirichenko @m_finzi Normalizing flows require an invertible NN, which imposes some constraints on speed and architectural design. However, invertible NNs are rapidly improving, such that approaches based on normalizing flows are becoming increasingly compelling.']",http://arxiv.org/abs/1912.13025,"Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions. ",Semi-Supervised Learning with Normalizing Flows
6,1212896084671905792,2800204849,Andrew Gordon Wilson,"['One can surprisingly reduce many regression problems to a single dimension through additive projections, even without learning the projections! Our new paper (incl. GPyTorch code), with @DelbridgeIan, @DavidBindel: <LINK> <LINK>']",https://arxiv.org/abs/1912.12834,"Gaussian processes (GPs) provide flexible distributions over functions, with inductive biases controlled by a kernel. However, in many applications Gaussian processes can struggle with even moderate input dimensionality. Learning a low dimensional projection can help alleviate this curse of dimensionality, but introduces many trainable hyperparameters, which can be cumbersome, especially in the small data regime. We use additive sums of kernels for GP regression, where each kernel operates on a different random projection of its inputs. Surprisingly, we find that as the number of random projections increases, the predictive performance of this approach quickly converges to the performance of a kernel operating on the original full dimensional inputs, over a wide range of data sets, even if we are projecting into a single dimension. As a consequence, many problems can remarkably be reduced to one dimensional input spaces, without learning a transformation. We prove this convergence and its rate, and additionally propose a deterministic approach that converges more quickly than purely random projections. Moreover, we demonstrate our approach can achieve faster inference and improved predictive accuracy for high-dimensional inputs compared to kernels in the original input space. ",Randomly Projected Additive Gaussian Processes for Regression
7,1212709585636728832,1179505593851232264,Giulia Gubitosi,['New paper is out! AKA The Galilei and Carroll limits might get mixed up in #quantumgravity <LINK>'],https://arxiv.org/abs/1912.12878,"We show that the Lorentzian Snyder models, together with their Galilei and Carroll limiting cases, can be rigorously constructed through the projective geometry description of Lorentzian, Galilean and Carrollian spaces with nonvanishing constant curvature. The projective coordinates of such curved spaces take the role of momenta, while translation generators over the same spaces are identified with noncommutative spacetime coordinates. In this way, one obtains a deformed phase space algebra, which fully characterizes the Snyder model and is invariant under boosts and rotations of the relevant kinematical symmetries. While the momentum space of the Lorentzian Snyder models is given by certain projective coordinates on (Anti-)de Sitter spaces, we discover that the momentum space of the Galilean (Carrollian) Snyder models is given by certain projective coordinates on curved Carroll (Newton--Hooke) spaces. This exchange between the Galilei and Carroll limits emerging in the transition from the geometric picture to the phase space picture is traced back to an interchange of the role of coordinates and translation operators. As a physically relevant feature, we find that in Galilean Snyder spacetimes the time coordinate does not commute with space coordinates, in contrast with previous proposals for non-relativistic Snyder models, which assume that time and space decouple in the non-relativistic limit $c\to \infty$. This remnant mixing between space and time in the non-relativistic limit is a quite general Planck-scale effect found in several quantum spacetime models. ","Lorentzian Snyder spacetimes and their Galilei and Carroll limits from
  projective geometry"
8,1212440767077244929,313814795,M. Sohaib Alam,"['My new paper, from the last decade, explores the feasibility of using reinforcement learning for quantum programming, particularly state preparation and gate compilation. (1/n)\n\n<LINK>', 'It does so by forming finite MDPs for single-qubit state prep and gate compilation, exactly solving for these cases using policy iteration, and comparing against brute-force calculations, finding that the two can be made to agree. (2/n)', 'This shows that the reinforcement learning notion of optimality can be made to agree with our intuitive notion of ""optimality"" in the sense of the shortest possible quantum circuit to prep a state, or compile a gate, up to some accuracy (3/n)', ""While policy iteration/dynamic programming is fine to use for the 1q problems set up here, it won't be practical for larger q scenarios, but it does mean that you could throw reinforcement learning techniques at the problem, and hope to find optimally short circuits (4/n)"", 'My guess is similar considerations would play a big part for larger q states, in particular choice of good coordinate systems for state and action spaces, the form and scale of the discretization etc (5/n)', 'I suppose one way to express the results in plainer but drastically simplifying words would be that a robot could be trained through rewards to produce the shortest possible quantum circuit necessary to prepare a state or compile a gate, up to some accuracy. (n/n)', ""@razaa_aasad Yep, and it's duly acknowledged in the references! Drawing from your paper, one could similarly hope that RL would discover close-to-optimal sequences for state prep + gate compilation at the logical gate level."", '@razaa_aasad As my new paper shows, exact-optimality, i.e. shortest length circuits, is a feature (upto caveats) of the underlying MDP for the single-qubit case. RL can therefore be expected to find at least close-to-optimal gate sequences  for state prep/compilation in the n-qubit case.']",https://arxiv.org/abs/1912.12002,"Reinforcement learning has witnessed recent applications to a variety of tasks in quantum programming. The underlying assumption is that those tasks could be modeled as Markov Decision Processes (MDPs). Here, we investigate the feasibility of this assumption by exploring its consequences for two of the simplest tasks in quantum programming: state preparation and gate compilation. By forming discrete MDPs, focusing exclusively on the single-qubit case, we solve for the optimal policy exactly through policy iteration. We find optimal paths that correspond to the shortest possible sequence of gates to prepare a state, or compile a gate, up to some target accuracy. As an example, we find sequences of H and T gates with length as small as 11 producing ~99% fidelity for states of the form (HT)^{n} |0> with values as large as n=10^{10}. This work provides strong evidence that reinforcement learning can be used for optimal state preparation and gate compilation for larger qubit spaces. ",Quantum Logic Gate Synthesis as a Markov Decision Process
9,1212404920219045888,223440240,Nathan Kallus,"[""To bring in the new year @XiaojieMao+Masa+I just posted a paper on Localized Debiased ML for estimating causal quantities using ML methods when hi-dim nuisances depend on estimand <LINK> In this thread I'll explain why this prob is so important and what we did 1/"", 'Causal inference on quantile treatment effects is important in assessing the risk to the population to be treated. When dealing with rich confounders/relationships we need to use ML to adjust them. But existing ML-based approaches for efficient estimation in this case, 2/', ""including DML and TMLE, would require we learn a whole conditional distribution function nonparametrically, which is practically challenging for ML -- especially compared to standard classification/regression, which is all we'd need for efficient average effect estimation. 3/"", 'Instead LDML uses an initial bad guess (eg IPW) to localize the estimation. Via a new 3-way cross-fold method and a finer analysis, we can ensure oracle-like behavior for our estimator without ever learning such complicated nuisances: just plain ML classification/regression. 4/', 'Sneak peak: under a Fréchet-deriv (stronger than Gateaux deriv used in DML) orthogonality (holds for quantile est + other cases), the oracle estimation equation is asymp equivalent to one where nuisances are evaluated at true parameter value. LDML targets this new formulation. 5/']",https://arxiv.org/abs/1912.12945,"We consider the efficient estimation of a low-dimensional parameter in an estimating equation involving high-dimensional nuisances that depend on the parameter of interest. An important example is the (local) quantile treatment effect ((L)QTE) in causal inference, for which the efficient estimating equation involves as a nuisance the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Debiased machine learning (DML) is a data-splitting approach to address the need to estimate nuisances using flexible machine learning methods that may not satisfy strong metric entropy conditions, but applying it to problems with parameter-dependent nuisances is impractical. For (L)QTE estimation, DML requires we learn the whole conditional cumulative distribution function, conditioned on potentially high-dimensional covariates, which is far more challenging than the standard supervised regression task in machine learning. We instead propose localized debiased machine learning (LDML), a new data-splitting approach that avoids this burdensome step and needs only estimate the nuisances at a single initial rough guess for the parameter. For (L)QTE estimation, this involves just learning two binary regression (i.e., classification) models, for which many standard, time-tested machine learning methods exist, and the initial rough guess may be given by inverse propensity weighting. We prove that under lax rate conditions on nuisances, our estimator has the same favorable asymptotic behavior as the infeasible oracle estimator that solves the estimating equation with the unknown true nuisance functions. Thus, our proposed approach uniquely enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs and in other coarsened data settings. ","Localized Debiased Machine Learning: Efficient Inference on Quantile
  Treatment Effects and Beyond"
10,1212049248545468418,1128508767404838912,Thayne Currie,['Our new SEEDS paper on the FS Tau circumbinary disk posted over Christmas:\n<LINK>'],https://arxiv.org/abs/1912.11301,"We analyzed the young (2.8-Myr-old) binary system FS Tau A using near-infrared (H-band) high-contrast polarimetry data from Subaru/HiCIAO and sub-millimeter CO (J=2-1) line emission data from ALMA. Both the near-infrared and sub-millimeter observations reveal several clear structures extending to $\sim$240 AU from the stars. Based on these observations at different wavelengths, we report the following discoveries. One arm-like structure detected in the near-infrared band initially extends from the south of the binary with a subsequent turn to the northeast, corresponding to two bar-like structures detected in ALMA observations with an LSRK velocity of 1.19-5.64 km/s. Another feature detected in the near-infrared band extends initially from the north of the binary, relating to an arm-like structure detected in ALMA observations with an LSRK velocity of 8.17-16.43 km/s. From their shapes and velocities, we suggest that these structures can mostly be explained by two streamers that connect the outer circumbinary disk and the central binary components. These discoveries will be helpful for understanding the evolution of streamers and circumstellar disks in young binary systems. ","High-Resolution Near-Infrared Polarimetry and Sub-Millimeter Imaging of
  FS Tau A: Possible Streamers in Misaligned Circumbinary Disk System"
11,1211690814335791106,64454976,Dr. Kiyoko Gotanda,['Super excited about @swannegordon @lopezsepulcre and my symposium on behaviour and #EcoEvolutionaryDynamics for #AmNat2020. How nice #behaviouralecologywhiskyclub has a new paper on this on @arXiv_Daily <LINK> <LINK>'],https://arxiv.org/abs/1912.09505,"The mechanisms underlying eco-evolutionary dynamics (the feedback between ecological and evolutionary processes) are often unknown. Here, we propose that classical theory from behavioral ecology can provide a greater understanding of the mechanisms underlying eco-evolutionary dynamics, and thus improve predictions about the outcomes of these dynamics. ",Animal behavior facilitates eco-evolutionary dynamics
12,1211662907156652032,700704725826572290,"Joe Guinness, valued customer",['New paper: Inverses of Matern Covariances on Grids\n<LINK>'],https://arxiv.org/abs/1912.11914,"We conduct a study of the aliased spectral densities of Mat\'ern covariance functions on a regular grid of points, providing clarity on the properties of a popular approximation based on stochastic partial differential equations; while others have shown that it can approximate the covariance function well, we find that it assigns too much power at high frequencies and does not provide increasingly accurate approximations to the inverse as the grid spacing goes to zero, except in the one-dimensional exponential covariance case. We provide numerical results to support our theory, and in a simulation study, we investigate the implications for parameter estimation, finding that the SPDE approximation tends to overestimate spatial range parameters. ",Inverses of Matern Covariances on Grids
13,1211357909063524353,842127983569690632,Casper Hesp,"['Very happy to have co-authored this paper in the aftermath of my work in astrophysics. ""A new GPU-accelerated GRMHD Code for Exascale Computing"" ( <LINK>), used for one of the largest astrophysical simulations ever!\n\nFuture work: Exascale active inference :D', '@kv_wellstein Exascale computing refers to computing systems that are capable of at least one exaFLOPS, or a billion billion (i.e. a quintillion) calculations per second. Such massively parallel computation will allow us to upscale active inference towards extremely exciting domains.', '@kv_wellstein E.g., simulating gene-culture coevolution, with thousands of active-inference agents making inferences about each other, forming their social networks, competing &amp; cooperating, growing up, selecting their mates, etc.\n\nEventually enabling socio-cultural Computational Psychiatry😀', ""@kv_wellstein Thanks for your interest, I'm also super excited about all of this! 😁""]",http://arxiv.org/abs/1912.10192,"General-relativistic magnetohydrodynamic (GRMHD) simulations have revolutionized our understanding of black-hole accretion. Here, we present a GPU-accelerated GRMHD code H-AMR with multi-faceted optimizations that, collectively, accelerate computation by 2-5 orders of magnitude for a wide range of applications. Firstly, it involves a novel implementation of a spherical-polar grid with 3D adaptive mesh refinement that operates in each of the 3 dimensions independently. This allows us to circumvent the Courant condition near the polar singularity, which otherwise cripples high-res computational performance. Secondly, we demonstrate that local adaptive time-stepping (LAT) on a logarithmic spherical-polar grid accelerates computation by a factor of $\lesssim10$ compared to traditional hierarchical time-stepping approaches. Jointly, these unique features lead to an effective speed of $\sim10^9$ zone-cycles-per-second-per-node on 5,400 NVIDIA V100 GPUs (i.e., 900 nodes of the OLCF Summit supercomputer). We demonstrate its computational performance by presenting the first GRMHD simulation of a tilted thin accretion disk threaded by a toroidal magnetic field around a rapidly spinning black hole. With an effective resolution of $13$,$440\times4$,$608\times8$,$092$ cells, and a total of $\lesssim22$ billion cells and $\sim0.65\times10^8$ timesteps, it is among the largest astrophysical simulations ever performed. We find that frame-dragging by the black hole tears up the disk into two independently precessing sub-disks. The innermost sub-disk rotation axis intermittently aligns with the black hole spin, demonstrating for the first time that such long-sought alignment is possible in the absence of large-scale poloidal magnetic fields. ","H-AMR: A New GPU-accelerated GRMHD Code for Exascale Computing With 3D
  Adaptive Mesh Refinement and Local Adaptive Time-stepping"
14,1209872335429390340,395150807,Sihao Huang,['My new paper on building synthetic neural networks &amp; biological ensembles using gene transcription is now live! <LINK> <LINK>'],https://arxiv.org/abs/1912.11423,"Artificial neurons built on synthetic gene networks have potential applications ranging from complex cellular decision-making to bioreactor regulation. Furthermore, due to the high information throughput of natural systems, it provides an interesting candidate for biologically-based supercomputing and analog simulations of traditionally intractable problems. In this paper, we propose an architecture for constructing multicellular neural networks and programmable nonlinear systems. We design an artificial neuron based on gene regulatory networks and optimize its dynamics for modularity. Using gene expression models, we simulate its ability to perform arbitrary linear classifications from multiple inputs. Finally, we construct a two-layer neural network to demonstrate scalability and nonlinear decision boundaries and discuss future directions for utilizing uncontrolled neurons in computational tasks. ","Towards Multicellular Biological Deep Neural Nets Based on
  Transcriptional Regulation"
15,1209747112042409984,96253726,Navin Sridhar,"['Holiday special!\nNew paper out – <LINK>! This is a paper that I started working on during my undergrad @CaltechSURF project with @jaj_garcia, which later grew on to become a wonderful collaboration with @vicgrinberg, Jack Steiner, Riley Connors among others (1/8)', 'In this paper, we investigate the evolution of certain properties of black hole accretion disk/corona viz., inner disk radius, ionization parameter, temperatures of—inner disk, corona, and its optical depth, etc., across the bright hard to soft state transition of GX 339–4 (2/8)', 'By employing a set of relativistic reflection models, we deduce that the inner disk truncation radius approaches R_in~ISCO during the early onset of bright hard state, and the disk inner edge remains small (&lt;9 Gravitational radii) throughout the hard to soft state transition(3/8)', 'We compare the disk properties (mentioned in 2/8) between outbursts with state transitions occurring at *different luminosities*, and find identical evolutionary trends in the disk properties (including R_in~ISCO), with differences seen only in the temp. and optical depth (4/8)', 'By applying a self-consistent Comptonized accretion disk model accounting for the scatter of disk photons by corona, we find R_in~ISCO, using the temperature dependent values of spectral hardening factor—thereby independently confirming our results from reflection analysis (5/8)', 'With the inner disk barely moving towards the black hole during the bright hard to soft state transition, the changes seen in the disk/coronal properties can be attributed\nto factors like coronal compactification, increase in accretion rate, spectral hardening factor, etc. (6/8)', 'In the end, we also establish that for ~Kerr black holes, data from RXTE/PCA along with relxill family of relativistic reflection models is capable of discerning Fe K fluorescent features, narrow enough to being able to constrain disk inner radius as large as R_in~120*ISCO (7/8)', 'For a broader discussion of even more model parameters, and for a detailed analysis procedure with the underlying statistical footing (MCMC), please go through our paper:  https://t.co/Js9xltjc6B. Feel free to share this work, and your questions/comments if any :) (8/8)']",https://arxiv.org/abs/1912.11447,"We present the analysis of several observations of the black hole binary GX 339--4 during its bright intermediate states from two different outbursts (2002 and 2004), as observed by RXTE/PCA. We perform a consistent study of its reflection spectrum by employing the relxill family of relativistic reflection models to probe the evolutionary properties of the accretion disk including the inner disk radius ($R_{\rm in}$), ionization parameter ($\xi$), temperatures of the inner disk ($T_{\rm in}$), corona ($kT_{\rm e}$), and its optical depth ($\tau$). Our analysis indicates that the disk inner edge approaches the inner-most stable circular orbit (ISCO) during the early onset of bright hard state, and that the truncation radius of the disk remains low ($\lesssim 14 R_{\rm g}$) throughout the transition from hard to soft state. This suggests that the changes observed in the accretion disk properties during the state transition are driven by variation in accretion rate, and not necessarily due to changes in the inner disk's radius. We compare the aforementioned disk properties in two different outbursts, with state transitions occurring at dissimilar luminosities, and find identical evolutionary trends in the disk properties, with differences only seen in corona's $kT_{\rm e}$ and $\tau$. We also perform an analysis by employing a self-consistent Comptonized accretion disk model accounting for the scatter of disk photons by the corona, and measure low inner disk truncation radius across the bright intermediate states, using the temperature dependent values of spectral hardening factor, thereby independently confirming our results from the reflection spectrum analysis. ","Evolution of the accretion disk-corona during bright hard-to-soft state
  transition: A reflection spectroscopic study with GX 339-4"
16,1209601174535434240,374233623,Shane Barratt,"['New paper - Learning Convex Optimization Control Policies, w/ @akshaykagrawal, @b_stellato, and Stephen Boyd.\n\nPaper: <LINK>', '@akshaykagrawal @b_stellato We consider the problem of tuning convex optimization control policies (COCPs), which are control policies that compute the input or action by solving a convex optimization problem that depends on the current state and some parameters. https://t.co/rcXAX9AdrZ', '@akshaykagrawal @b_stellato Many control policies used in practice are in fact COCPs. Some examples include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex approximate dynamic programming (ADP) or control-Lyapunov policies.', '@akshaykagrawal @b_stellato Tuning the parameters in these policies is often done by hand, or by grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of a performance metric with respect to the parameters.', '@akshaykagrawal @b_stellato Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters, namely cvxpylayers (https://t.co/J8WqcTPDsf).', '@akshaykagrawal @b_stellato As for numerical examples, we start by applying our method to the classical LQR problem, where the parameters are the coefficients of an approximate (quadratic) value function. We see that our method is able to recover a policy with the same cost as the LQR solution. https://t.co/9DjGH6KHXe', '@akshaykagrawal @b_stellato Next we apply our method to a box-constrained LQR problem, which has no known exact solution. We use the same COCP as the LQR problem except we add the constraint that the input is in a box. https://t.co/Pgkt46TG04', '@akshaykagrawal @b_stellato Our (simple) method is able to reach the performance of a (sophisticated) method based on LMIs, introduced by Wang &amp; Boyd (https://t.co/1cusAMKp7Q). https://t.co/KwT7RMdISN', '@akshaykagrawal @b_stellato We provide examples in portfolio optimization (""Tuning a Markowitz policy to maximize utility""), vehicle control (""Tuning a vehicle controller to track curved paths""), and supply-chain management ("" Tuning a supply chain policy to maximize profit""). Check them out in the paper! https://t.co/uIQoCnEJZR', '@akshaykagrawal @b_stellato Code for all of the examples is available online: https://t.co/4biJaag0wb.']",https://arxiv.org/abs/1912.09529,"Many control policies used in various applications determine the input or action by solving a convex optimization problem that depends on the current state and some parameters. Common examples of such convex optimization control policies (COCPs) include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex control-Lyapunov or approximate dynamic programming (ADP) policies. These types of control policies are tuned by varying the parameters in the optimization problem, such as the LQR weights, to obtain good performance, judged by application-specific metrics. Tuning is often done by hand, or by simple methods such as a crude grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of the performance metric with respect to the parameters. Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters. We illustrate our method on several examples. ",Learning Convex Optimization Control Policies
17,1209477746700804097,2425754287,Roman Orus,['New paper out! And merry Christmas! <LINK>'],https://arxiv.org/abs/1912.10756,"We study the zero-temperature phase diagram of the spin-$\frac{1}{2}$ Heisenberg model with breathing anisotropy (i.e., with different coupling strength on the upward and downward triangles) on the kagome lattice. Our study relies on large scale tensor network simulations based on infinite projected entangled-pair state and infinite projected entangled-simplex state methods adapted to the kagome lattice. Our energy analysis suggests that the U(1) algebraic quantum spin-liquid (QSL) ground-state of the isotropic Heisenberg model is stable up to very large breathing anisotropy until it breaks down to a critical lattice-nematic phase that breaks rotational symmetry in real space through a first-order quantum phase transition. Our results also provide further insight into the recent experiment on vanadium oxyfluoride compounds which has been shown to be relevant platforms for realizing QSL in the presence of breathing anisotropy. ","Spin-$\frac{1}{2}$ kagome Heisenberg antiferromagnet with strong
  breathing anisotropy"
18,1209407413381685253,844700194197397506,Alex Kendall,"[""Happy to share our final paper of 2019 proposing a spatial-temporal video-pixel embedding loss which sets a new state of the art for video instance segmentation. Here's the paper\n<LINK>\nand demo video <LINK>"", ""This work was led by Anthony Hu and extends Bert De Brabandere et al.'s method to video. Our method is online and runs in real-time and explicitly uses motion and geometric cues for video instance segmentation.""]",http://arxiv.org/abs/1912.08969,"We present a novel embedding approach for video instance segmentation. Our method learns a spatio-temporal embedding integrating cues from appearance, motion, and geometry; a 3D causal convolutional network models motion, and a monocular self-supervised depth loss models geometry. In this embedding space, video-pixels of the same instance are clustered together while being separated from other instances, to naturally track instances over time without any complex post-processing. Our network runs in real-time as our architecture is entirely causal - we do not incorporate information from future frames, contrary to previous methods. We show that our model can accurately track and segment instances, even with occlusions and missed detections, advancing the state-of-the-art on the KITTI Multi-Object and Tracking Dataset. ",Learning a Spatio-Temporal Embedding for Video Instance Segmentation
19,1209288887400570880,312448486,Dr. Karan Jani,"['NEW PAPER - we revisit an intermediate mass black hole trigger in @LIGO.\n\nWe find it to be consistent with binary black hole merger of 150 SOLAR MASSES. \n\nHeard right - BLACK HOLES that are one-hundred-fifty times heavier than Sun ! \n\nBeat that 2019🖖🏻\n\n<LINK> <LINK>', '@alexandernitz @LIGO Indeed we focused on the new PE machinery. This trigger was very marginal for the matched filtering searches. Was loud (significant) only for the Burst search. Not surprising though 🙌🏻']",https://arxiv.org/abs/1912.10533,"Gravitational wave (GW) measurements provide the most robust constraints of the mass of astrophysical black holes. Using state-of-the-art GW signal models and a unique parameter estimation technique, we infer the source parameters of the loudest marginal trigger, GW170502, found by LIGO from 2015 to 2017. If this trigger is assumed to be a binary black hole merger, we find it corresponds to a total mass in the source frame of $157^{+55}_{-41}~\rm{M}_\odot$ at redshift $z=1.37^{+0.93}_{-0.64}$. The primary and secondary black hole masses are constrained to $94^{+44}_{-28}~\rm{M}_{\odot}$ and $62^{+30}_{-25}~\rm{M}_{\odot}$ respectively, with 90\% confidence. Across all signal models, we find $\gtrsim 70\%$ probability for the effective spin parameter $\chi_\mathrm{eff}>0.1$. Furthermore, we find that the inclusion of higher-order modes in the analysis narrows the confidence region for the primary black hole mass by 10\%, however, the evidence for these modes in the data remains negligible. The techniques outlined in this study could lead to robust inference of the physical parameters for all intermediate-mass black hole binary candidates $(\gtrsim100~\mathrm{M}_\odot)$ in the current GW network. ","Inferring Parameters of GW170502: The Loudest Intermediate-mass Black
  Hole Trigger in LIGO's O1/O2 data"
20,1209212489927397376,712621727554146304,Oliver Stockdale,"['New paper on the arXiv! Analytical, numerical, and even experimental(!) work that uncovers universal dynamics of vortices in a 2D superfluid. @FLEETCentre @ARC_EQUS \n\n<LINK> <LINK>', 'Each point represents a vortex in this figure. As the vortex cluster expands, the distribution (regardless of initial condition) becomes a top-hat distribution, aka a Rankine vortex - this is forbidden in classical fluids!']",https://arxiv.org/abs/1912.09535,"A large ensemble of quantum vortices in a superfluid may itself be treated as a novel kind of fluid that exhibits anomalous hydrodynamics. Here we consider the dynamics of vortex clusters with thermal friction, and present an analytic solution that uncovers a new universality class in the out-of-equilibrium dynamics of dissipative superfluids. We find that the long-time dynamics of the vorticity distribution is an expanding Rankine vortex (i.e.~top-hat distribution) independent of initial conditions. This highlights a fundamentally different decay process to classical fluids, where the Rankine vortex is forbidden by viscous diffusion. Numerical simulations of large ensembles of point vortices confirm the universal expansion dynamics, and further reveal the emergence of a frustrated lattice structure marked by strong correlations. We present experimental results in a quasi-two-dimensional Bose-Einstein condensate that are in excellent agreement with the vortex fluid theory predictions, demonstrating that the signatures of vortex fluid theory can be observed with as few as $N\sim 11$ vortices. Our theoretical, numerical, and experimental results establish the validity of the vortex fluid theory for superfluid systems. ","Universal expansion of vortex clusters in a dissipative two-dimensional
  superfluid"
21,1208029887757459456,84822240,Luca Bertinetto 🇮🇹 🇪🇺 🌐,"['New paper! <LINK>\n\nSomething curious about the recent history of deep neural networks 👉 While the top-1 error has enjoyed a spectacular improvement, the *severity* of the mistakes made (in a hierarchical sense) has remained almost unchanged ⬇️ 1/4 <LINK>', 'Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. ⬇️ 2/4', 'We aim to renew interest in this important problem by reviewing past approaches and proposing two embarrassingly simple modifications of the\ncross-entropy loss which perform surprisingly well under several notions of ""mistake severity"". ⬇️ 3/4', 'Work done @_FiveAI in collaboration with the fabulous @RomainMueller, @ktertikas, @sinjax\nand @NicholasLordCV. \n\nCode will be available soon at https://t.co/3t8xvgbOzh\n4/4 🎅🎄🙏👋']",https://arxiv.org/abs/1912.09393,"Deep neural networks have improved image classification dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modifications of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist'19. ",Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks
22,1208025985679007746,1200141005233836032,Pedro Fernandes,"['New very interesting paper by my friend Nelson Eiró and my ex-supervisor Carlos Herdeiro on ""lensing and shadow of a black hole surrounded by a heavy accretion disk"" <LINK>\nCheck it out!']",https://arxiv.org/abs/1912.08833,"We consider a static, axially symmetric spacetime describing the superposition of a Schwarzschild black hole (BH) with a thin and heavy accretion disk. The BH-disk configuration is a solution of the Einstein field equations within the Weyl class. The disk is sourced by a distributional energy-momentum tensor and it is located at the equatorial plane. It can be interpreted as two streams of counter-rotating particles, yielding a total vanishing angular momentum. The phenomenology of the composed system depends on two parameters: the fraction of the total mass in the disk, $m$, and the location of the inner edge of the disk, $a$. We start by determining the sub-region of the space of parameters wherein the solution is physical, by requiring the velocity of the disk particles to be sub-luminal and real. Then, we study the null geodesic flow by performing backwards ray-tracing under two scenarios. In the first scenario the composed system is illuminated by the disk and in the second scenario the composed system is illuminated by a far-away celestial sphere. Both cases show that, as $m$ grows, the shadow becomes more prolate. Additionally, the first scenario makes clear that as $m$ grows, for fixed $a$, the geometrically thin disk appears optically enlarged, i.e., thicker, when observed from the equatorial plane. This is to due to light rays that are bent towards the disk, when backwards ray traced. In the second scenario, these light rays can cross the disk (which is assumed to be transparent) and may oscillate up to a few times before reaching the far away celestial sphere. Consequently, an almost equatorial observer sees different patches of the sky near the equatorial plane, as a chaotic ""mirage"". As $m\rightarrow 0$ one recovers the standard test, i.e., negligible mass, disk appearance. ",Lensing and shadow of a black hole surrounded by a heavy accretion disk
23,1207917567597318144,40299444,Alexey Petrov,"['New paper! if you ever wanted to know what Artificial Neural Networks and semileptonic decays of heavy mesons have in common, read <LINK>! It was a pleasure working with WSU grad students Cody Grant and Ayesh Gunawardana!']",https://arxiv.org/abs/1912.09058,"Experimental checks of the second row unitarity of the Cabibbo-Kobayashi-Maskawa (CKM) matrix involve extractions of the matrix element $V_{cd}$, which may be obtained from semileptonic decay rates of $D$ to $\pi$. These decay rates are proportional to hadronic form factors which parameterize how the quark $c \to d$ transition is realized in $D \to \pi$ meson decays. The form factors can not yet be analytically computed over the whole range of available momentum transfer $q^2$, but can be parameterized with a varying degree of model dependency. We propose using artificial neural networks trained from experimental pseudo-data to predict the shape of these form factors with a prescribed uncertainty. We comment on the parameters of several commonly-used model parameterizations of semileptonic form factors. We extract shape parameters and use unitarity to bound the form factor at a given $q^2$, which then allows us to bound the CKM matrix element $|V_{cd}|$. ",Semileptonic decays of heavy mesons with artificial neural networks
24,1207874607350370305,3245949691,Rebecca Leane,"['New paper out today!\nw/@DanHooperAstro @dai_tsai Shalma Wegsman &amp; Sam Witte\n<LINK>\n\nWe characterize leading dark matter annihilation processes for indirect detection, and identify hidden sector models that can explain both the gamma ray and antiproton excesses.']",https://arxiv.org/abs/1912.08821,"In hidden sector models, dark matter does not directly couple to the particle content of the Standard Model, strongly suppressing rates at direct detection experiments, while still allowing for large signals from annihilation. In this paper, we conduct an extensive study of hidden sector dark matter, covering a wide range of dark matter spins, mediator spins, interaction diagrams, and annihilation final states, in each case determining whether the annihilations are s-wave (thus enabling efficient annihilation in the universe today). We then go on to consider a variety of portal interactions that allow the hidden sector annihilation products to decay into the Standard Model. We broadly classify constraints from relic density requirements and dwarf spheroidal galaxy observations. In the scenario that the hidden sector was in equilibrium with the Standard Model in the early universe, we place a lower bound on the portal coupling, as well as on the dark matter's elastic scattering cross section with nuclei. We apply our hidden sector results to the observed Galactic Center gamma-ray excess and the cosmic-ray antiproton excess. We find that both of these excesses can be simultaneously explained by a variety of hidden sector models, without any tension with constraints from observations of dwarf spheroidal galaxies. ","A Systematic Study of Hidden Sector Dark Matter: Application to the
  Gamma-Ray and Antiproton Excesses"
25,1207846154924437504,2902687319,Mike Hudson,"['Our new paper, led by Supranta Boruah, showing the tight cosmological constraints from supernova and Tully-Fisher peculiar velocities. <LINK> <LINK>']",https://arxiv.org/abs/1912.09383,"The peculiar velocity field offers a unique way to probe dark matter density field on large scales at low redshifts. In this work, we have compiled a new sample of 465 peculiar velocities from low redshift $(z < 0.067)$ Type Ia supernovae. We compare the reconstructed velocity field derived from the 2M++ galaxy redshift compilation to the supernovae, the SFI++ and the 2MTF Tully-Fisher distance catalogues. We used a forward method to jointly infer the distances and the velocities of distance indicators by comparing the observations to the reconstruction. Comparison of the reconstructed peculiar velocity fields to observations allows us to infer the cosmological parameter combination $f\sigma_8$, and the bulk flow velocity arising from outside the survey volume. The residual bulk flow arising from outside the 2M++ volume is inferred to be $171^{+11}_{-11}$ km s$^{-1}$ in the direction $l=301^{\circ} \pm 4^{\circ}$ and $b=0^{\circ} \pm 3^{\circ}$. We obtain $f\sigma_{8} = 0.400 \pm 0.017$, equivalent to $S_8 \approx \sigma_8(\Omega_m/0.3)^{0.55}=0.776\pm0.033$, which corresponds to an approximately $ 4\%\,$ statistical uncertainty on the value of $f\sigma_8$. Our inferred value is consistent with other low redshift results in the literature. ","Cosmic flows in the nearby Universe: new peculiar velocities from SNe
  and cosmological constraints"
26,1207809524913950721,252867237,Juan Miguel Arrazola,"['New paper out! It introduces new software and summarizes 2 years of ideas on photonic algorithms. Team effort from the software and algorithms group at @XanaduAI, including @3rdquantization &amp; @corsairenard  <LINK> <LINK>', 'The software is a new addition to Strawberry Fields (with brand new documentation + tutorials) https://t.co/djnVYFVBnC, leveraging state-of-the-art simulators from The Walrus https://t.co/VtMZOb6oCV https://t.co/j4SnFBLFhK', 'The algorithms we describe are novel heuristic methods\nthat leverage the unique properties of photonic devices. In some cases, they serves as accelerators\nfor classical algorithms; in others, as a method for building statistical models.', 'Algorithms for dense subgraph identification employ the property that photonic devices can be programmed to sample dense subgraphs with high probability https://t.co/NJ4V18JUuN', 'In combination with classical search algorithms, sampling dense subgraphs with high probability is helpful in identifying large cliques in graphs https://t.co/mV0Up3duev', 'Encoding graphs into photonic devices can be used to determine graph similarity by building feature vectors from coarse-grained probabilities https://t.co/vSOLteSb8a', 'By encoding kernel matrices that quantify proximity between points, point processes can be generated that create random clusters of points. In some cases, this can be done using fast quantum-inspired methods https://t.co/YuK0DPJeAK', 'The bosonic properties of photonic quantum computers can also be harnessed compute vibronic spectra of molecules https://t.co/ipS79BJ3pY', 'I truly hope that our ideas and software will help others to deepen our understanding of what is possible when we have fun with photonic quantum computers 🎉', ""Didn't realize (until it was too late) that the images had transparencies instead of white backgrounds! Sorry! 😱😰""]",https://arxiv.org/abs/1912.07634,"Gaussian Boson Sampling (GBS) is a near-term platform for photonic quantum computing. Recent efforts have led to the discovery of GBS algorithms with applications to graph-based problems, point processes, and molecular vibronic spectra in chemistry. The development of dedicated quantum software is a key enabler in permitting users to program devices and implement algorithms. In this work, we introduce a new applications layer for the Strawberry Fields photonic quantum computing library. The applications layer provides users with the necessary tools to design and implement algorithms using GBS with only a few lines of code. This paper serves a dual role as an introduction to the software, supported with example code, and also a review of the current state of the art in GBS algorithms. ","Applications of Near-Term Photonic Quantum Computers: Software and
  Algorithms"
27,1207710455290114054,1142447618322313216,Florian Marquardt,['Our new paper: <LINK> \n\nRapid Exploration of Topological Band Structures using Deep Learning <LINK>'],http://arxiv.org/abs/1912.03296,"The design of periodic nanostructures allows to tailor the transport of photons, phonons, and matter waves for specific applications. Recent years have seen a further expansion of this field by engineering topological properties. However, what is missing currently are efficient ways to rapidly explore and optimize band structures and to classify their topological characteristics, for arbitrary unit cell geometries. In this work, we show how deep learning can address this challenge. We introduce an approach where a neural network first maps the geometry to a tight-binding model. This allows us to exploit any underlying space group and predict the symmetries of Bloch waves. We demonstrate how that helps to rapidly categorize a large set of geometries in terms of their band representations, identifying designs for fragile topologies. Engineering of domain walls and optimization are also accelerated by orders of magnitude. The approach is general enough to permit future applications to the geometry discovery in other classes of materials (e.g. active and nonlinear metamaterials). ",Rapid Exploration of Topological Band Structures using Deep Learning
28,1207707174237024257,483912082,Kjetil Børkje,['New paper on arXiv: <LINK>\nCritical quantum fluctuations and photon antibunching in optomechanical systems with large single-photon cooperativity'],https://arxiv.org/abs/1912.08589,"A pertinent question in cavity optomechanics is whether reaching the regime of large single-photon cooperativity, where the single-photon coupling rate exceeds the geometric mean of the cavity and mechanical decay rates, can enable any new phenomena. We show that in some multimode optomechanical systems, the single-photon cooperativity can indeed be a figure of merit. We first study a system with one cavity mode and two mechanical oscillators which combines the concepts of levitated optomechanics and coherent scattering with standard dispersive optomechanics. Later, we study a more complicated setup comprising three cavity modes which does not rely on levitated optomechanics and only features dispersive optomechanical interactions with direct cavity driving. These systems can effectively realize the degenerate or the nondegenerate parametric oscillator models known from quantum optics, but in the unusual finite-size regime for the fundamental mode(s) when the single-photon cooperativity is large. We show that the response of these systems to a coherent optical probe can be highly nonlinear in probe power even for average photon occupation numbers below unity. The nonlinear optomechanical interaction has the peculiar consequence that the probe drive will effectively amplitude-squeeze itself. For large single-photon cooperativity, this occurs for small occupation numbers, which enables observation of nonclassical antibunching of the transmitted probe photons due to a destructive interference effect. Finally, we show that as the probe power is increased even further, the system enters a critical regime characterized by intrinsically nonlinear dynamics and non-Gaussian states. ","Critical quantum fluctuations and photon antibunching in optomechanical
  systems with large single-photon cooperativity"
29,1207596673599496192,1073331211039334407,Julia Westermayr,"['Our new paper on #MachineLearning for excited states is on #Arxiv - a collaborative work with @ProfvLilienfeld @AndersSChristen <LINK>\n #compchem @marquetand', '@stevain Thank you! 😊 enjoy reading']",https://arxiv.org/abs/1912.08484,"Excited-state dynamics simulations are a powerful tool to investigate photo-induced reactions of molecules and materials and provide complementary information to experiments. Since the applicability of these simulation techniques is limited by the costs of the underlying electronic structure calculations, we develop and assess different machine learning models for this task. The machine learning models are trained on {\emph ab initio} calculations for excited electronic states, using the methylenimmonium cation (CH$_2$NH$_2^+$) as a model system. For the prediction of excited-state properties, multiple outputs are desirable, which is straightforward with neural networks but less explored with kernel ridge regression. We overcome this challenge for kernel ridge regression in the case of energy predictions by encoding the electronic states explicitly in the inputs, in addition to the molecular representation. We adopt this strategy also for our neural networks for comparison. Such a state encoding enables not only kernel ridge regression with multiple outputs but leads also to more accurate machine learning models for state-specific properties. An important goal for excited-state machine learning models is their use in dynamics simulations, which needs not only state-specific information but also couplings, i.e., properties involving pairs of states. Accordingly, we investigate the performance of different models for such coupling elements. Furthermore, we explore how combining all properties in a single neural network affects the accuracy. As an ultimate test for our machine learning models, we carry out excited-state dynamics simulations based on the predicted energies, forces and couplings and, thus, show the scopes and possibilities of machine learning for the treatment of electronically excited states. ","Neural networks and kernel ridge regression for excited states dynamics
  of CH$_2$NH$_2^+$: From single-state to multi-state representations and
  multi-property machine learning models"
30,1207479615645728768,29149902,Isaac Tamblyn,['Check out our new paper\n\nLearning to grow: control of materials self-assembly using evolutionary reinforcement learning\n\n<LINK>'],http://arxiv.org/abs/1912.08333,"We show that neural networks trained by evolutionary reinforcement learning can enact efficient molecular self-assembly protocols. Presented with molecular simulation trajectories, networks learn to change temperature and chemical potential in order to promote the assembly of desired structures or choose between competing polymorphs. In the first case, networks reproduce in a qualitative sense the results of previously-known protocols, but faster and with higher fidelity; in the second case they identify strategies previously unknown, from which we can extract physical insight. Networks that take as input the elapsed time of the simulation or microscopic information from the system are both effective, the latter more so. The evolutionary scheme we have used is simple to implement and can be applied to a broad range of examples of experimental self-assembly, whether or not one can monitor the experiment as it proceeds. Our results have been achieved with no human input beyond the specification of which order parameter to promote, pointing the way to the design of synthesis protocols by artificial intelligence. ","Learning to grow: control of material self-assembly using evolutionary
  reinforcement learning"
31,1207421256880021504,733646548370919433,Pᵉtɘr Wᵉil8acher,"['Happy to report about another new paper with MUSE data: ""Stellar populations and physical properties of starbursts in the Antennae galaxy from self-consistent modelling of MUSE spectra"" (<LINK>), this time using the data of which I am PI. #musevlt', 'Madusha managed to import high-resolution stellar model atmospheres into Starburst99 to model the youngest ages well. This nicely allows to constrain ages and metallicities --among other properties-- of the starburst regions and we see spatial differences in the Antennae Galaxy. https://t.co/HOTgMEehVn']",https://arxiv.org/abs/1912.08151,"We have modelled the stellar and nebular continua and emission-line intensity ratios of massive stellar populations in the Antennae galaxy using high resolution and self-consistent libraries of model HII regions around central clusters of aging stars. The model libraries are constructed using the stellar population synthesis code, Starburst99, and photoionisation model, Cloudy. The Geneva and PARSEC stellar evolutionary models are plugged into Starburst99 to allow comparison between the two models. Using a spectrum-fitting methodology that allows the spectral features in the stellar and nebular continua (e.g. Wolf-Rayet features, Paschen jump), and emission-line diagnostics to constrain the models, we apply the libraries to the high-resolution MUSE spectra of the starbursting regions in the Antennae galaxy. Through this approach, we were able to model the continuum emission from Wolf-Rayet stars and extract stellar and gas metallicities, ages, electron temperatures and densities of starbursts by exploiting the full spectrum. From the application to the Antennae galaxy, we find that (1) the starbursts in the Antennae galaxy are characterised by stellar and gas metallicities of around solar, (2) the star-forming gas in starbursts in the Western loop of NGC 4038 appear to be more enriched, albeit slightly, than the rest of galaxy, (3) the youngest starbursts are found across the overlap region and over parts of the western-loop, though in comparison, the regions in the western-loop appear to be at a slightly later stage in star-formation than the overlap region, and (4) the results obtained from fitting the Geneva and Parsec models are largely consistent. ","Stellar populations and physical properties of starbursts in the
  Antennae galaxy from self-consistent modelling of MUSE spectra"
32,1207298267178000385,312448486,Dr. Karan Jani,"['Our new white paper: ""Building A Field: The Future of Astronomy with Gravitational Waves"" \n\nTo students - there has never been a better time to be in this field!\n\n<LINK> <LINK>']",https://arxiv.org/abs/1912.07642,"Harnessing the sheer discovery potential of gravitational wave astronomy will require bold, deliberate, and sustained efforts to train and develop the requisite workforce. The next decade requires a strategic plan to build -- from the ground up -- a robust, open, and well-connected gravitational wave astronomy community with deep participation from traditional astronomers, physicists, data scientists, and instrumentalists. This basic infrastructure is sorely needed as an enabling foundation for research. We outline a set of recommendations for funding agencies, universities, and professional societies to help build a thriving, diverse, and inclusive new field. ","Building A Field: The Future of Astronomy with Gravitational Waves, A
  State of The Profession Consideration for Astro2020"
33,1207268023956123651,299894471,Micah Altman,['New white paper on how to keep information forever: <LINK> (Forthcoming IDCC2020/IJDC)'],https://arxiv.org/abs/1912.07908,"This article addresses the problem of formulating efficient and reliable operational preservation policies that ensure bit-level information integrity over long periods, and in the presence of a diverse range of real-world technical, legal, organizational, and economic threats. We develop a systematic, quantitative prediction framework that combines formal modeling, discrete-event-based simulation, hierarchical modeling, and then use empirically calibrated sensitivity analysis to identify effective strategies. The framework offers flexibility for the modeling of a wide range of preservation policies and threats. Since this framework is open source and easily deployed in a cloud computing environment, it can be used to produce analysis based on independent estimates of scenario-specific costs, reliability, and risks. ","Selecting efficient and reliable preservation strategies: modeling
  long-term information integrity using large-scale hierarchical discrete event
  simulation"
34,1207164620848402432,22148802,Leo C. Stein 🦁,"['In case you\'re wondering, my new paper is here: <LINK>\n""The location of the last stable orbit in Kerr spacetime""\nIt includes this neat image, which I can describe more below. 1/ <LINK> <LINK>', '""Separatrix"" (https://t.co/vIdmoXpyye) is just a fancy word for a dividing surface in a parameter space between two different types of behavior. The first separatrix most physics students learn about is the one in this figure, for the 1-d (anharmonic) pendulum. 2/ https://t.co/gs2e42ZWJb', ""Inside the red curve, the pendulum's motion is bobbing back and forth near the bottom. It might go up pretty high, but it never goes over the top of the pivot. 3/ https://t.co/B7UV6BGHIg"", 'Outside of the red curve, the pendulum has enough energy to go over the top, so it just keeps circulating in the same direction, always clockwise or counter-clockwise, instead of constantly changing directions. 4/ https://t.co/Zcdn6oTz9h', 'So this phase space has two *qualitatively* different types of motion (sometimes called circulating vs. libration).\nWell, motion around black holes can also have qualitatively different types of behavior! 5/', ""There's already different types of motion in Newtonian gravity, before we get to anything exotic like black holes. In Newtonian gravity, you can have a bound orbit, or you can just flyby a body without being gravitationally bound to it. 6/"", 'Between these two possibilities is a parabolic orbit — one which goes arbitrarily far away, but slows down to arbitrarily slow speed as it escapes (instead of coasting away with some final velocity like for a ""hyperbolic"" encounter). 7/', 'So, in Newtonian gravity, parabolic trajectories are on a ""separatrix"" between bound and unbound orbits. 8/', ""Ok, on to black holes! Here's where things get really interesting. There are more possibilities for orbits around black holes than in Newtonian gravity. There are still hyperbolic orbits, parabolic ones, and bound eccentric orbits — but wait, there's more. 9/"", 'You can also get sucked into a black hole! So, there\'s a dividing line between bound orbits and ""plunging"" ones that end up inside. That dividing line is what me and @Niels_JW studied in this latest paper (https://t.co/gOtDobgqgq). 10/', '@Niels_JW Now, this is not the first time anybody has studied this separatrix. Far from it. Part of the story goes all the way back to Karl Schwarzschild in 1916, but most of the story is much more recent. 11/', ""@Niels_JW We already had numerical code for finding the location of the separatrix in parameter space. This is the space we use to talk about the shape of an orbit — its size, eccentricity, and how it's inclined with respect to the spin axis of the black hole. 12/"", ""@Niels_JW But this is (to the best of our knowledge) the first time anybody's noticed the fact that the separatrix is given by a *polynomial* of these parameters. I think that's really cool! For the math nerds out there, the separatrix is an 'affine algebraic variety'. 13/"", ""@Niels_JW Knowing that some surface is given by the solution of a polynomial equation means that you can study the heck out of it! I'm partly surprised that this polynomial nature hadn't been appreciated before in the literature. There's still a bit of low-hanging fruit in this field! 14/"", '@Niels_JW And, numerical approaches to solving polynomial equations have been very extensively studied. So, we replaced the previous algorithms in the Black Hole Perturbation Toolkit (https://t.co/YvXCs6TOez) with new ones. These new ones are 45x faster!! 15/', '@Niels_JW Anyway, if you like black holes and/or algebraic varieties, maybe you\'ll like our paper! Give it a read?\nhttps://t.co/gOtDobgqgq\n""The location of the last stable orbit in Kerr spacetime""\n\nFin 16/16 https://t.co/DZj1ATdo1A', ""@liuyao12 @Niels_JW Yeah! That's a deeper part of the story that I didn't really go into here, or much in the paper. The actual separatrix is just one of these 'leaves' that cross through each other, but they can't be factored apart."", '@liuyao12 @Niels_JW I would love for an algebraic geometer to study all of the singular structure of this surface!', '@liuyao12 @Niels_JW That might be the local structure of one of the singular points! I might try to check later today.', ""@liuyao12 @Niels_JW Well, the singular point of interest does not look to me like it's locally the Whitney umbrella, but I only know physicists' tools for studying the local structure. Any tips would be appreciated. For reference, this point is:\n  (a,p,e,x) = (a, 2 (1 + √(1 - a^2)), -1, 0)."", '@liuyao12 @Niels_JW Yep, I did a series expansion about the point above. The leading order was deg 2 in only two of the variables, and then at 3rd order another one of the variables entered.', ""@liuyao12 @Niels_JW It's not nearly so simple as the Whitney Umbrella, though. Here's what it looks like, with only a touch of simplification. The coords away from the singular point are (e1,p1,x1). At O(\\eps^2), there are terms in p1^2, e1*p1, and e1^2. At O(\\eps^3) ... (contd) https://t.co/h68BQC5Jg9"", '@liuyao12 @Niels_JW At O(\\eps^3), there are terms p1^3, e1^3, p1^2 e1, e1^2 p1; and x1^2 p1, and x1^2 e1.']",https://arxiv.org/abs/1912.07609,"Black hole spacetimes, like the Kerr spacetime, admit both stable and plunging orbits, separated in parameter space by the separatrix. Determining the location of the separatrix is of fundamental interest in understanding black holes, and is of crucial importance for modeling extreme mass-ratio inspirals. Previous numerical approaches to locating the Kerr separatrix were not always efficient or stable across all of parameter space. In this paper we show that the Kerr separatrix is the zero set of a single polynomial in parameter space. This gives two main results. First, we thoroughly analyze special cases (extreme Kerr, polar orbits, etc.), finding strict bounds on the limits of roots, and unifying a number of results in the literature. Second, we pose a stable numerical method which is guaranteed to quickly and robustly converge to the separatrix. This new approach is implemented in the Black Hole Perturbation Toolkit, and results in a ~45x speedup over the prior robust approach. ",The location of the last stable orbit in Kerr spacetime
35,1207125777365356544,1002606609527443462,Dr. Angela Collier,['Here is the link to my new paper: Violent Buckling Benefits Galactic Bars (<LINK>)\n\nA description of the buckling instability as classical Euler buckling followed by Landau damping of the perturbation. I did math in this one! #AcademicTwitter'],https://arxiv.org/abs/1912.08190,Galactic bars are unstable to a vertical buckling instability which heats the disk and in some cases forms a boxy/peanut shaped bulge. We analyze the buckling instability as an application of classical Euler buckling followed by nonlinear gravitational Landau damping in the collisionless system. We find that the buckling instability is dictated by the kinematic properties and geometry of the bar. The analytical result is compared to simulations of isolated galaxies containing the disk and dark matter components. Our results demonstrate that violent buckling does not destroy bars while a less energetic buckling can dissolve the bar. The disks that undergo gentle buckling remain stable to bar formation which may explain the observed bar fraction in the local universe. Our results align with the results from recent surveys. ,Violent Buckling Benefits Galactic Bars
36,1206971341078851584,1008861969510739969,Christian Hayes,['Check out our new paper picking out Sgr stream stars in APOGEE DR16 and finding metallicity and alpha abundance gradients along the Sgr stream - now on the arXiv! <LINK> <LINK>'],https://arxiv.org/abs/1912.06707v1,"Using 3D positions and kinematics of stars relative to the Sagittarius (Sgr) orbital plane and angular momentum, we identify 166 Sgr stream members observed by the Apache Point Observatory Galactic Evolution Experiment (APOGEE) that also have Gaia DR2 astrometry. This sample of 63/103 stars in the Sgr trailing/leading arm are combined with an APOGEE sample of 710 members of the Sgr dwarf spheroidal core (385 of them newly presented here) to establish differences of 0.6 dex in median metallicity and 0.1 dex in [$\alpha$/Fe] between our Sgr core and dynamically older stream samples. Mild chemical gradients are found internally along each arm, but these steepen when anchored by core stars. With a model of Sgr tidal disruption providing estimated dynamical ages (i.e., stripping times) for each stream star, we find a mean metallicity gradient of 0.12 +/- 0.03 dex/Gyr for stars stripped from Sgr over time. For the first time, an [$\alpha$/Fe] gradient is also measured within the stream, at 0.02 +/- 0.01 dex/Gyr using magnesium abundances and 0.04 +/- 0.01 dex/Gyr using silicon, which imply that the Sgr progenitor had significant radial abundance gradients. We discuss the magnitude of those inferred gradients and their implication for the nature of the Sgr progenitor within the context of the current family of Milky Way satellite galaxies, and suggest that more sophisticated Sgr models are needed to properly interpret the growing chemodynamical detail we have on the Sgr system. ","] Metallicity and $\alpha$-element Abundance Gradients along the
  Sagittarius Stream as Seen by APOGEE"
37,1206931405198036992,2354296903,Rahul Gopinath,"['New paper ""Inferring Input Grammars from Dynamic Control Flow"". Given a program, our Mimid prototype automatically infers a human-readable context-free grammar that accurately specifies its input syntax.  Great helper for fuzzing and program understanding! <LINK>', 'A fully documented python prototype is available as a self contained Jupyter notebook here: https://t.co/SE1Q1jd72J', '@tathanhdinh Thanks for letting me know. Could you please give me a link to your thesis?', '@AdamOfDc949 Thanks for the pointer. SynFuzz looks really cool.', '@tathanhdinh Thanks! Much appreciated.', '@tathanhdinh I will certainly read it, and cite it. Grammar fuzzers and grammar recovery have become more important these days. So I do not think that your paper would be forgotten.', '@reyeetengineer Thank you!']",https://arxiv.org/abs/1912.05937,"A program is characterized by its input model, and a formal input model can be of use in diverse areas including vulnerability analysis, reverse engineering, fuzzing and software testing, clone detection and refactoring. Unfortunately, input models for typical programs are often unavailable or out of date. While there exist algorithms that can mine the syntactical structure of program inputs, they either produce unwieldy and incomprehensible grammars, or require heuristics that target specific parsing patterns. In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all program stack based recursive descent input parsers, including PEG and parser combinators, and can do entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including expr, URLparse, and microJSON. ",Inferring Input Grammars from Dynamic Control Flow
38,1206842037443710977,59413748,Reuben Binns,"[""New paper: 'On the Apparent Conflict Between Individual and Group Fairness' accepted at @fatconference, (now up on <LINK>). The paper addresses a distinction drawn between two broad approaches to measuring fairness in machine learning"", ""'individual fairness' measures compare individuals, e.g. people 'similar' according to some task-relevant metric should get the same outcome. 'Group fairness' measures compare protected groups (e.g. gender, race, age) for differences in errors/outcomes/calibration/etc."", 'As typically presented, they reflect different + conflicting normative principles: group fairness seems to ensure egalitarian equality, whereas individual fairness seems to ensure consistency (the more/less qualified an individual, the better/worse their outcome).', ""But (I argue) the normative conflict only aligns that way if you pick a version of individual fairness which assumes there *isn't* any structural discrimination, and a version of group fairness which assumes there *is*."", ""But some versions of group fairness preserve consistency and ignore structural discrimination (e.g. equal calibration), and conversely, some versions of individual fairness can factor structural discrimination into the metric, adjusting scores so otherwise 'different' people..."", ""... are 'similar' once disadvantage taken into account (e.g. in Dwork etal '12 re: SATs in college admissions). So conflict isnt between individual vs group per se; its about different worldviews, which can be reflected in variants from either kind of measure"", 'Finally: individual fairness may appear to protect what 🇩🇪 constitution calls Einzelfallgerechtigkeit (individual justice). But even individually-fair ML is not individually just in this sense, because it still generalises between individuals who share same point in feature space', '@lilianedwards Not in a comprehensive way, but some of the jurisprudence literature I draw from has parallels']",https://arxiv.org/abs/1912.06883,"A distinction has been drawn in fair machine learning research between `group' and `individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on theoretical discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artifact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of `unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it. ",On the Apparent Conflict Between Individual and Group Fairness
39,1206804886228217857,5637892,Dafydd Gibbon,['For prosodists: prepublication version of my new paper on computational linguistic and phonetic analysis of rhythm. <LINK>'],http://arxiv.org/abs/1912.07050,"The present study has two goals relating to the grammar of prosody, understood as the rhythms and melodies of speech. First, an overview is provided of the computable grammatical and phonetic approaches to prosody analysis which use hypothetico-deductive methods and are based on learned hermeneutic intuitions about language. Second, a proposal is presented for an inductive grounding in the physical signal, in which prosodic structure is inferred using a language-independent method from the low-frequency spectrum of the speech signal. The overview includes a discussion of computational aspects of standard generative and post-generative models, and suggestions for reformulating these to form inductive approaches. Also included is a discussion of linguistic phonetic approaches to analysis of annotations (pairs of speech unit labels with time-stamps) of recorded spoken utterances. The proposal introduces the inductive approach of Rhythm Formant Theory (RFT) and the associated Rhythm Formant Analysis (RFA) method are introduced, with the aim of completing a gap in the linguistic hypothetico-deductive cycle by grounding in a language-independent inductive procedure of speech signal analysis. The validity of the method is demonstrated and applied to rhythm patterns in read-aloud Mandarin Chinese, finding differences from English which are related to lexical and grammatical differences between the languages, as well as individual variation. The overall conclusions are (1) that normative language-to-language phonological or phonetic comparisons of rhythm, for example of Mandarin and English, are too simplistic, in view of diverse language-internal factors due to genre and style differences as well as utterance dynamics, and (2) that language-independent empirical grounding of rhythm in the physical signal is called for. ",Computational Induction of Prosodic Structure
40,1206757872782077952,980073199332282369,Toshihiko Yamasaki,"['New publication on arXiv (to appear in IEEE TMM).\nThis is an extended version (16 pages) from our AAAI-19 paper (8 pages).\n\nR. Furuta, N. Inoue, and T. Yamasaki\nPixelRL: Fully Convolutional Network with Reinforcement Learning for Image Processing\n\n<LINK>']",https://arxiv.org/abs/1912.07190,"This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep reinforcement learning (RL) for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. Besides, it is possible to visualize what kind of operation is employed for each pixel at each iteration, which would help us understand why and how such an operation is chosen. We also believe that our technology can enhance the explainability and interpretability of the deep neural networks. In addition, because the operations executed at each pixels are visualized, we can change or modify the operations if necessary. We apply the proposed method to a variety of image processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven image editing. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning. The source code is available on this https URL ","PixelRL: Fully Convolutional Network with Reinforcement Learning for
  Image Processing"
41,1206715477180862464,1268474822,Shirley Li,"['Our new paper is out! So excited!\n\nLDMX is a proposed light mark matter experiment. We propose to use it to measure GeV electron-nucleus interaction cross sections, which would benefit the long-baseline neutrino oscillation program!\n\n<LINK>']",https://arxiv.org/abs/1912.06140,"We point out that the LDMX (Light Dark Matter eXperiment) detector design, conceived to search for sub-GeV dark matter, will also have very advantageous characteristics to pursue electron-nucleus scattering measurements of direct relevance to the neutrino program at DUNE and elsewhere. These characteristics include a 4-GeV electron beam, a precision tracker, electromagnetic and hadronic calorimeters with near 2$\pi$ azimuthal acceptance from the forward beam axis out to $\sim$40$^\circ$ angle, and low reconstruction energy threshold. LDMX thus could provide (semi)exclusive cross section measurements, with detailed information about final-state electrons, pions, protons, and neutrons. We compare the predictions of two widely used neutrino generators (GENIE, GiBUU) in the LDMX region of acceptance to illustrate the large modeling discrepancies in electron-nucleus interactions at DUNE-like kinematics. We argue that discriminating between these predictions is well within the capabilities of the LDMX detector. ","Lepton-Nucleus Cross Section Measurements for DUNE with the LDMX
  Detector"
42,1206655404274413569,51692050,Najmeh,"['Our new paper about lensed dwarf galaxies at z~2 is on Arxiv today: <LINK>.\nWe determine the efficiency of dwarf galaxies in producing ionizing photons and argue its importance to the cosmic reionization. @briansiana @anahitaalavi  @novaric @bigticketdw <LINK>', '@briansiana @anahitaalavi @novaric @bigticketdw To determine the role of galaxies in the reionization of the universe, 3 key components need to be known: the total non-ionizing UV photons of galaxies, a conversion fraction from non-ionizing to ionizing UV photons, and the fraction of UV photons that escape from galaxies. 2/n', '@briansiana @anahitaalavi @novaric @bigticketdw Among many possibilities, dwarfs are likely more responsible for the reionization due to their significant contribution to the non-ionizing UV luminosity Fn. and large escape fractions. Here we measure the 3rd component for dwarf galaxies at high z through lensing technique. 3/n', '@briansiana @anahitaalavi @novaric @bigticketdw Our high-z lensed dwarfs produce the same amount of ionizing photons as more massive galaxies in other high-z studies. Also, there is an increase in Xi_ion of our sample relative to low-z galaxies of similar mass, indicating an evolution in the stellar properties of galaxies. 4/4 https://t.co/MHOWC6ntGn']",https://arxiv.org/abs/1912.06152,"We measure the ionizing photon production efficiency ($\xi_{ion}$) of low-mass galaxies ($10^{7.8}$-$10^{9.8}$ $M_{\odot}$) at $1.4<z<2.7$, allowing us to better understand the contribution of dwarf galaxies to the ionizing background and cosmic reionization. We target galaxies that are magnified by the strong lensing galaxy clusters Abell 1689, MACS J0717, and MACS J1149. We utilize Keck/MOSFIRE spectra to measure optical nebular emission line fluxes and HST imaging to measure the rest-UV and rest-optical photometry. We present two methods of stacking. First, we take the average of the log(L$_{H\alpha}$ /L$_{UV}$) of galaxies in our sample to determine the typical log($\xi_{ion}$). Second, we take the logarithm of the total L$_{H\alpha}$ over the total L$_{UV}$. We prefer the latter as it provides the total ionizing UV luminosity density of galaxies when multiplied by the non-ionizing UV luminosity density from the UV luminosity function. log($\xi_{ion}$) calculated from the second method is $\sim$ 0.2 dex higher than the first method. We do not find any strong dependence between log($\xi_{ion}$) and stellar mass, M$_{UV}$ or UV spectral slope ($\beta$). We report a value of log($\xi_{ion}$) $\sim25.47\pm 0.09$ for our UV-complete sample ($-22<M_{UV}<-17.3$) and $\sim25.37\pm0.11$ for our mass-complete sample ($7.8<\log(M_*)<9.8)$. These values are consistent with measurements of more massive, more luminous galaxies in other high-redshift studies that use the same stacking technique. Our log($\xi_{ion}$) is $0.2-0.3$ dex higher than low-redshift galaxies of similar mass, indicating an evolution in the stellar properties, possibly due to metallicity, age, or the prevalence of binary stars. We also find a correlation between log($\xi_{ion}$) and the equivalent widths of H$\alpha$ and [OIII]$\lambda$5007 fluxes, confirming that these equivalent widths can be used to estimate $\xi_{ion}$. ","The Ionizing Photon Production Efficiency ($\xi_{ion}$) Of Lensed Dwarf
  Galaxies At $z \sim 2 $"
43,1206497262798561281,977906884886827008,Marcos Mariño,['What are renormalons? In our new paper we find an infrared renormalon in a very simple two-dimensional theory: <LINK>'],https://arxiv.org/abs/1912.06228,"According to standard lore, perturbative series of super-renormalizable theories have only instanton singularities. In this paper we show that two-dimensional scalar theories with a spontaneously broken $O(N)$ symmetry at the classical level, which are super-renormalizable, have an IR renormalon singularity at large $N$. Since perturbative expansions in these theories are made around the ""false vacuum"" in which the global symmetry is broken, this singularity can be regarded as a manifestation of the non-perturbative absence of Goldstone bosons. We conjecture that the Borel singularity in the ground state energy of the Lieb--Liniger model is a non-relativistic manifestation of this phenomenon. We also provide {\it en passant} a detailed perturbative calculation of the Lieb--Liniger energy up to two-loops, and we check that it agrees with the prediction of the Bethe ansatz. ",A new renormalon in two dimensions
44,1206469220327993346,2603024598,Ricardo Pérez-Marco,"['A new correction with @CGrunspan on Nakamoto\'s #bitcoin paper and the double spend strategy presented in section 11. We applied our profitability theory and compute sharp formulas.\n\n""On profitability of Nakamoto double spend""\n\n<LINK> <LINK>']",https://arxiv.org/abs/1912.06412,"Nakamoto double spend strategy, described in Bitcoin foundational article, leads to total ruin with positive probability and does not make sense from the profitability point of view. The simplest strategy that can be profitable incorporates a stopping threshold when success is unlikely. We solve and compute the exact profitability for this strategy. We compute the minimal amount of the double spend that is profitable. For a given amount of the transaction, we determine the minimal number of confirmations to be requested by the recipient such that this double spend strategy is non-profitable. We find that this number of confirmations is only 1 or 2 for average transactions and a small hashrate of the attacker. This is substantially lower than the original Nakamoto numbers that are widely used and are only based on the success probability instead of the profitability. ",On Profitability of Nakamoto double spend
45,1206457206306222081,10666172,Sabine Hossenfelder,['You are interested Quantum Mechanics but have no idea what the heck Superdeterminism is? Our new paper will help:\n\n<LINK>'],https://arxiv.org/abs/1912.06462,"Quantum mechanics has irked physicists ever since its conception more than 100 years ago. While some of the misgivings, such as it being unintuitive, are merely aesthetic, quantum mechanics has one serious shortcoming: it lacks a physical description of the measurement process. This ""measurement problem"" indicates that quantum mechanics is at least an incomplete theory -- good as far as it goes, but missing a piece -- or, more radically, is in need of complete overhaul. Here we describe an approach which may provide this sought-for completion or replacement: Superdeterminism. A superdeterministic theory is one which violates the assumption of Statistical Independence (that distributions of hidden variables are independent of measurement settings). Intuition suggests that Statistical Independence is an essential ingredient of any theory of science (never mind physics), and for this reason Superdeterminism is typically discarded swiftly in any discussion of quantum foundations. The purpose of this paper is to explain why the existing objections to Superdeterminism are based on experience with classical physics and linear systems, but that this experience misleads us. Superdeterminism is a promising approach not only to solve the measurement problem, but also to understand the apparent nonlocality of quantum physics. Most importantly, we will discuss how it may be possible to test this hypothesis in an (almost) model independent way. ",Rethinking Superdeterminism
46,1205914131817279490,717056861619376128,Juan Carlos Loredo,['New paper up on the arXiv for generating cluster states with reduced resources. A beautiful collaboration with our friends from Israel. Nice job everybody! @AntnSo @quandela @SagnesIsabelle @senellartqd @C2N_com  \n\n<LINK> <LINK>'],https://arxiv.org/abs/1912.04375,"Light states composed of multiple entangled photons - such as cluster states - are essential for developing and scaling-up quantum computing networks. Photonic cluster states with discrete variables can be obtained from single-photon sources and entangling gates, but so far this has only been done with probabilistic sources constrained to intrinsically-low efficiencies, and an increasing hardware overhead. Here, we report the resource-efficient generation of polarization-encoded, individually-addressable, photons in linear cluster states occupying a single spatial mode. We employ a single entangling-gate in a fiber loop configuration to sequentially entangle an ever-growing stream of photons originating from the currently most efficient single-photon source technology - a semiconductor quantum dot. With this apparatus, we demonstrate the generation of linear cluster states up to four photons in a single-mode fiber. The reported architecture can be programmed to generate linear-cluster states of any number of photons with record scaling ratios, potentially enabling practical implementation of photonic quantum computing schemes. ","Sequential generation of linear cluster states from a single photon
  emitter"
47,1205902384112848899,2239670346,Jonathan Frankle,"['How do the lottery ticket hypothesis and the loss landscape relate? Winning lottery tickets always find the same, linearly-connected optimum. Check out our (@KDziugaite, @roydanroy, @mcarbin) poster at the SEDL workshop (West 121) and our new paper <LINK> <LINK>']",https://arxiv.org/abs/1912.05671,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet). ",Linear Mode Connectivity and the Lottery Ticket Hypothesis
48,1205588946954133504,148996025,Ye Quanzhi (叶泉志),"['""Don\'t aim your telescope near the Sun,"" they say, but here at @ztfsurvey we gave it a try. Not during daytime of course, but during the brief twilight hours. The goal? Explore the innermost region in the solar system. New paper\'s out: <LINK> <LINK>', '@ztfsurvey Findings: (1) at a decent site like Palomar, you can go down to ~35 deg from the Sun without trading away too much sensitivity (for our case, we lose ~0.5-1 mag).', '@ztfsurvey The ""solar avoidance distance"" for most surveys is 45-60 deg, so there are a lot of new things to be found between 35-45 deg. We found two km-sized (large!) Atira-group NEOs during our 7-month survey.', ""@ztfsurvey Atiras -- NEOs whose orbits are entirely confined by Earth's orbit. As view from the Earth, they are always within 90 deg from the Sun, so they are difficult to discover."", '@ztfsurvey But Atiras can tell us many things -- from dynamical history of the inner solar system to physical (thermal) evolution of asteroids. So we want to learn about them.', ""@ztfsurvey (2) we also unsuccessfully look for Earth and Venus co-orbital asteroids (asteroids that share the same orbit with Earth/Venus). This is not unexpected, because these asteroids are likely small in sizes and don't exist in large numbers."", '@ztfsurvey So we need deep surveys that can search the near-Sun sky. (e.g. @NasaNEOCam )', '@ztfsurvey @NasaNEOCam [end of the tweets]', '@AdrienCoffinet @ztfsurvey @NasaNEOCam Yes, their model is posted at https://t.co/gHTe9I0B9V. Right, Atiras include Vatiras.']",https://arxiv.org/abs/1912.06109,"Near-Earth Objects (NEOs) that orbit the Sun on or within Earth's orbit are tricky to detect for Earth-based observers due to their proximity to the Sun in the sky. These small bodies hold clues to the dynamical history of the inner solar system as well as the physical evolution of planetesimals in extreme environments. Populations in this region include the Atira and Vatira asteroids, as well as Venus and Earth co-orbital asteroids. Here we present a twilight search for these small bodies, conducted using the 1.2-m Oschin Schmidt and the Zwicky Transient Facility (ZTF) camera at Palomar Observatory. The ZTF twilight survey operates at solar elongations down to $35^\circ$ with limiting magnitude of $r=19.5$. During a total of 40 evening sessions and 62 morning sessions conducted between 2018 November 15 and 2019 June 23, we detected 6 Atiras, including 2 new discoveries 2019 AQ$_3$ and 2019 LF$_6$, but no Vatiras or Earth/Venus co-orbital asteroids. NEO population models show that these new discoveries are likely only the tip of the iceberg, with the bulk of the population yet to be found. The population models also suggest that we have only detected 5--$7\%$ of the $H<20$ Atira population over the 7-month survey. Co-orbital asteroids are smaller in diameters and require deeper surveys. A systematic and efficient survey of the near-Sun region will require deeper searches and/or facilities that can operate at small solar elongations. ","A Twilight Search for Atiras, Vatiras and Co-orbital Asteroids:
  Preliminary Results"
49,1205496598433689600,119837224,Jason Baldridge,"[""New paper on extending ML models toward human-level language understanding! It's a joint effort with Jay McClelland, @FelixHill84, Maja Rudolph, and Hinrich Schütze that integrates our diverse perspectives on cognition, grounding, modeling and language.\n\n<LINK> <LINK>"", 'Key takeaway: we have seen tremendous progress with estimation of deep, contextualized models, and we argue for a renewed focus on modeling situations and objects, inspired by cognitive models and driven by grounding in active environments. https://t.co/CQxNk2EGhn', 'Both Jay and I are speaking today at the ViGIL workshop and will both cover aspects of this and our own perspectives in these general topics!\n\nhttps://t.co/cI8ScrNKWV', '@FelixHill84 Thanks!', '@volkancirik Will post later! Bug me if you don’t see them. :-)', '@texastacos @Lextremist @FelixHill84 Wombats are also so cute! And they have cube-shaped poop!']",https://arxiv.org/abs/1912.05877,"Language is crucial for human intelligence, but what exactly is its role? We take language to be a part of a system for understanding and communicating about situations. The human ability to understand and communicate about situations emerges gradually from experience and depends on domain-general principles of biological neural networks: connection-based learning, distributed representation, and context-sensitive, mutual constraint satisfaction-based processing. Current artificial language processing systems rely on the same domain general principles, embodied in artificial neural networks. Indeed, recent progress in this field depends on \emph{query-based attention}, which extends the ability of these systems to exploit context and has contributed to remarkable breakthroughs. Nevertheless, most current models focus exclusively on language-internal tasks, limiting their ability to perform tasks that depend on understanding situations. These systems also lack memory for the contents of prior situations outside of a fixed contextual span. We describe the organization of the brain's distributed understanding system, which includes a fast learning system that addresses the memory problem. We sketch a framework for future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and exploiting query-based attention. We highlight relevant current directions and consider further developments needed to fully capture human-level language understanding in a computational system. ","Extending Machine Language Models toward Human-Level Language
  Understanding"
50,1205356160137564160,543846704,Rowena Ball,"['New paper to appear in PCCP shows how stochastic dynamical media and the @constructal law may have facilitated the origin of life: ""The long-term effect  is to produce more structure, and more complex structure, and allow greater persistence of structure."" <LINK>']",https://arxiv.org/abs/1912.05781,"An environment far from equilibrium is thought to be a necessary condition for the origin and persistence of life. In this context we report open-flow simulations of a non-enzymic proto-metabolic system, in which hydrogen peroxide acts both as oxidant and driver of thermochemical cycling. We find that a Gaussian perturbed input produces a non-Boltzmann output fluctuation distribution around the mean oscillation maximum. Our main result is that net biosynthesis can occur under fluctuating cyclical but not steady drive. Consequently we may revise the necessary condition to ""dynamically far from equilibrium"". ","Anomalous thermal fluctuation distribution sustains proto-metabolic
  cycles and biomolecule synthesis"
51,1205314221816393728,922847904058011649,Romy Rodríguez,"[""If you're curious about flares on the coolest (figuratively &amp; literally!) stars, check out my new paper, in which we identified and characterized strong flares on some of the brightest and nearest M dwarfs!! Now on arXiv! \n<LINK>""]",https://arxiv.org/abs/1912.05549,"We analyzed the light curves of 1376 early-to-late, nearby M dwarfs to search for white-light flares using photometry from the All-Sky Automated Survey for Supernovae (ASAS-SN). We identified 480 M dwarfs with at least one potential flare employing a simple statistical algorithm that searches for sudden increases in $V$-band flux. After more detailed evaluation, we identified 62 individual flares on 62 stars. The event amplitudes range from $0.12 <\Delta V < 2.04$ mag. Using classical-flare models, we place lower limits on the flare energies and obtain $V$-band energies spanning $2.0\times10^{30} \lesssim E_{V} \lesssim 6.9\times10^{35}$ erg. The fraction of flaring stars increases with spectral type, and most flaring stars show moderate to strong H$\alpha$ emission. Additionally, we find that 14 of the 62 flaring stars are rotational variables, and they have shorter rotation periods and stronger H$\alpha$ emission than non-flaring rotational variable M dwarfs. ",A Catalog of M-dwarf Flares with ASAS-SN
52,1205288366746394624,1115880604560691200,NII Yamagishi Lab,"['Our new paper, ""Detecting and Correcting Adversarial Images Using Image Processing Operations and Convolutional Neural Networks,"" is online:  <LINK>']",https://arxiv.org/abs/1912.05391,"Deep neural networks (DNNs) have achieved excellent performance on several tasks and have been widely applied in both academia and industry. However, DNNs are vulnerable to adversarial machine learning attacks, in which noise is added to the input to change the network output. We have devised an image-processing-based method to detect adversarial images based on our observation that adversarial noise is reduced after applying these operations while the normal images almost remain unaffected. In addition to detection, this method can be used to restore the adversarial images' original labels, which is crucial to restoring the normal functionalities of DNN-based systems. Testing using an adversarial machine learning database we created for generating several types of attack using images from the ImageNet Large Scale Visual Recognition Challenge database demonstrated the efficiency of our proposed method for both detection and correction. ","Detecting and Correcting Adversarial Images Using Image Processing
  Operations"
53,1205157782707589121,1658162341,Narayanan Rengaswamy,"[""New paper out on adaptive strategies for discriminating tensor product quantum states! We focus on cases where the individual subsystems might not be copies of each other. See <LINK> for details. This is primarily Sarah Brandsen's project. @kenbrownquantum""]",https://arxiv.org/abs/1912.05087,"Discrimination between quantum states is a fundamental task in quantum information theory. Given two arbitrary tensor-product quantum states (TPQS) $\rho_{\pm} = \rho_{\pm}^{(1)} \otimes \cdots \otimes \rho_{\pm}^{(N)}$, determining the joint $N$-system measurement to optimally distinguish between the two states is a hard problem. Thus, there is great interest in identifying local measurement schemes that are optimal or close-to-optimal. In this work, we focus on distinguishing between two general TPQS. We begin by generalizing previous work by Acin et al. (Phys. Rev. A 71, 032338) to show that a locally greedy (LG) scheme using Bayesian updating can optimally distinguish between two states that can be written as tensor products of arbitrary pure states. Then, we show that even in the limit of large $N$ the same algorithm cannot distinguish tensor products of mixed states with vanishing error probability. This poor asymptotic behavior occurs because the Helstrom measurement becomes trivial for sufficiently biased priors. Based on this, we introduce a modified locally greedy (MLG) scheme with strictly better performance. In the second part of this work, we compare these simple local schemes with a general dynamic programming (DP) approach that finds the optimal series of local measurements to distinguish the two states. When the subsystems are non-identical, we demonstrate that the ordering of the systems affects performance and we extend the DP technique to determine the optimal ordering adaptively. Finally, in contrast to the binary optimal collective measurement, we show that adaptive protocols on sufficiently large (e.g., qutrit) subsystems must contain non-binary measurements to be optimal. (The code that produced the simulation results in this paper can be found at: this https URL) ","Adaptive Procedures for Discrimination Between Arbitrary Tensor-Product
  Quantum States"
54,1205157218611535882,59595964,Nic Ross,"['I know y\'all are having fun at the #UKElection #GeneralElection2019 #GE2019, but there\'s this very nice new paper on the arXiv today:: \n\n""The first high-redshift changing-look quasars""\n<LINK>', ""Bottom line(s): (i) CIV exhibits CLQ behaviour and (ii) even in their 'low-state', 10^9 supermassive black holes seem to accrete at a decent fraction of Eddington. \n\n👍🔭🌌🪐""]",https://arxiv.org/abs/1912.05310v1,"We report on three redshift $z>2$ quasars with dramatic changes in their C IV emission lines, the first sample of changing-look quasars (CLQs) at high redshift. This is also the first time the changing-look behaviour has been seen in a high-ionisation emission line. SDSS J1205+3422, J1638+2827, and J2228+2201 show interesting behaviour in their observed optical light curves, and subsequent spectroscopy shows significant changes in the C IV broad emission line, with both line collapse and emergence being displayed on rest-frame timescales of $\sim$240-1640 days. These are rapid changes, especially when considering virial black hole mass estimates of $M_{\rm BH} > 10^{9} M_{\odot}$ for all three quasars. Continuum and emission line measurements from the three quasars show changes in the continuum-equivalent width plane with the CLQs seen to be on the edge of the full population distribution, and showing indications of an intrinsic Baldwin effect. We put these observations in context with recent state-change models, and note that even in their observed low-state, the C IV CLQs are generally above $\sim$5\% in Eddington luminosity. ",] The first high-redshift changing-look quasars
55,1205147135802560512,1232021550,Karen Levy,"['I am *really* excited about our new paper, Roles for Computing in Social Change, on its way to @fatconference: <LINK>\nThis paper comes from *years* of thinking and talking among ourselves (me + @red_abebe @s010n @manish_raghavan @dgrobinson Jon Kleinberg) ...', ""@fatconference @red_abebe @s010n @manish_raghavan @dgrobinson We wanted to think about ways computing research can support, not supplant, other forms of action toward a more just society -- taking advantage of computing's unique capabilities, while also recognizing what it can't do."", '@fatconference @red_abebe @s010n @manish_raghavan @dgrobinson We describe 4 ways computing researchers can position their work in the service of broad change:\nto diagnose and measure problems;\nto explicitly specify general policy goals;\nto clarify the limits of what technology can do;\nand to foreground long-standing social problems anew.', '@fatconference @red_abebe @s010n @manish_raghavan @dgrobinson In working on the paper, we thought hard about what we like so much about some of our favorite work, by folks like @latanyasweeney @PopTechWorks @jovialjoy @timnitGebru @annaeveryday @niftyc @mmitchell_ai @aylin_cim @random_walker @j2bryson and many others.', ""@fatconference @red_abebe @s010n @manish_raghavan @dgrobinson @LatanyaSweeney @PopTechWorks @jovialjoy @timnitGebru @annaeveryday @niftyc @mmitchell_ai @aylin_cim @random_walker @j2bryson I learned so much from my co-authors in the course of writing this, and I'm really happy to have it out in the world! https://t.co/UgGLBleipL""]",https://arxiv.org/abs/1912.04883,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems -- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own. ",Roles for Computing in Social Change
56,1205122101201559552,18593948,Duncan Brown,['New paper placing first limits on the rate of eccentric binary neutron star mergers with @LIGO by @amberkiana_ and @alexandernitz #PyCBC #OpenData #opensource  <LINK>'],https://arxiv.org/abs/1912.05464,"We present a search for gravitational waves from merging binary neutron stars which have non-negligible eccentricity as they enter the LIGO observing band. We use the public Advanced LIGO data which covers the period from 2015 through 2017 and contains $\sim164$ days of LIGO-Hanford and LIGO-Livingston coincident observing time. The search was conducted using matched-filtering using the PyCBC toolkit. We find no significant binary neutron star candidates beyond GW170817, which has previously been reported by searches for binaries in circular orbits. We place a 90% upper limit of $\sim1700$ mergers $\textrm{Gpc}^{-3} \textrm{Yr}^{-1}$ for eccentricities $\lesssim 0.43$ at a dominant-mode gravitational-wave frequency of 10 Hz. The absence of a detection with these data is consistent with theoretical predictions of eccentric binary neutron star merger rates. Using our measured rate we estimate the sensitive volume of future gravitational-wave detectors and compare this to theoretical rate predictions. We find that, in the absence of a prior detection, the rate limits set by six months of Cosmic Explorer observations would constrain all current plausible models of eccentric binary neutron star formation. ","Search for Eccentric Binary Neutron Star Mergers in the first and second
  observing runs of Advanced LIGO"
57,1205091495025491968,856472551710756864,Juan Sensio,"['My first paper is now on @arxiv_org  ! We propose a new way of solving Partial Differential Equations, replacing traditional numerical techniques with Neural Networks (which brings lots of benefits). Check it out -&gt; <LINK> @jmaronasm @RobertoParPal']",https://arxiv.org/abs/1912.04737,"Many scientific and industrial applications require solving Partial Differential Equations (PDEs) to describe the physical phenomena of interest. Some examples can be found in the fields of aerodynamics, astrodynamics, combustion and many others. In some exceptional cases an analytical solution to the PDEs exists, but in the vast majority of the applications some kind of numerical approximation has to be computed. In this work, an alternative approach is proposed using neural networks (NNs) as the approximation function for the PDEs. Unlike traditional numerical methods, NNs have the property to be able to approximate any function given enough parameters. Moreover, these solutions are continuous and derivable over the entire domain removing the need for discretization. Another advantage that NNs as function approximations provide is the ability to include the free-parameters in the process of finding the solution. As a result, the solution can generalize to a range of situations instead of a particular case, avoiding the need of performing new calculations every time a parameter is changed dramatically decreasing the optimization time. We believe that the presented method has the potential to disrupt the physics simulation field enabling real-time physics simulation and geometry optimization without the need of big supercomputers to perform expensive and time consuming simulations ",Solving Partial Differential Equations with Neural Networks
58,1205050720187289600,974769773539155968,Daniel Baumann,"['New paper with Chia, Porto and Stout. I am very proud of the hard work of my brilliant student Horng Sheng and my postdoc John Stout. (All figures made by John!)\n<LINK> <LINK>']",https://arxiv.org/abs/1912.04932,"We study the imprints of new ultralight particles on the gravitational-wave signals emitted by binary black holes. Superradiant instabilities may create large clouds of scalar or vector fields around rotating black holes. The presence of a binary companion then induces transitions between different states of the cloud, which become resonantly enhanced when the orbital frequency matches the energy gap between the states. We find that the time dependence of the orbit significantly impacts the cloud's dynamics during a transition. Following an analogy with particle colliders, we introduce an S-matrix formalism to describe the evolution through multiple resonances. We show that the state of the cloud, as it approaches the merger, carries vital information about its spectrum via time-dependent finite-size effects. Moreover, due to the transfer of energy and angular momentum between the cloud and the orbit, a dephasing of the gravitational-wave signal can occur which is correlated with the positions of the resonances. Notably, for intermediate and extreme mass ratio inspirals, long-lived floating orbits are possible, as well as kicks that yield large eccentricities. Observing these effects, through the precise reconstruction of waveforms, has the potential to unravel the internal structure of the boson clouds, ultimately probing the masses and spins of new particles. ",Gravitational Collider Physics
59,1205044677113548800,2563532985,Prof Anna Watts,"['New group paper! Relativistic ocean r-modes during Type I X-ray bursts, Frank Chambers and Anna Watts, MNRAS in press. <LINK>', ""One of the puzzles we've been working on is the development of unusually bright patches in the oceans of accreting neutron stars. They form at times when the ocean is exploding (dangerous places!), but why one part of the exploding ocean gets hotter than the rest we have no idea."", 'How do we know the ocean has hotter patches? The exploding ocean is so hot that it glows in X-rays, which we detect with space telescopes. But the stars spin - several hundred times a second - so if part of the ocean is hotter and brighter we see the X-ray emission vary.', ""We've observed thousands of these explosions, but only some of them develop brighter patches. We've no idea why! (Yet...)"", 'One idea is that the explosion, perhaps the way that the flame spreads over the ocean, triggers formation of a large-scale global wave pattern. There are lots of different types of wave that can form in oceans, but the front-runner is a ""buoyant r-mode"".', 'It relies on the temperature and composition difference between burnt and unburned layers on the spinning ocean, so fades away as the explosion cools off.', ""But there has been a problem - previous calculations of mode properties didn't quite match observations - in particular how much and how fast the hotter patch should move around on the ocean surface."", ""In previous work, we've shown that with more up to date explosion models (better nuclear reaction physics) some of the problems go away."", ""And in this latest paper we've added relativity into the mix (neutron stars have very strong gravity so this is important). And now we can get things that are much closer to observations!"", ""So where are we now? We know that this type of ocean mode is strongly affected by nuclear physics, and relativity - and that it might be the answer - but we've only looked at a handful of explosion models. Next stage is a larger scale study!"", 'This research was supported by @ERC_Research , and benefits from our collaborations within @jina_cee.', '@ProfTimOB Could be! We had a great workshop at @lorentzcenter earlier this year in which we started to talk about this with white dwarf folk including @starsumner !', '@BlackPhysicists 3D, mode calculations are much less intensive than full time-dependent hydro sims!', '@BlackPhysicists The ocean is the burning material!', '@BlackPhysicists The ocean as a whole is a few hundred meters thick, but most bursts take place in the outer few metres.']",https://arxiv.org/abs/1912.05369,"Accreting neutron stars (NS) can exhibit high frequency modulations in their lightcurves during thermonuclear X-ray bursts, known as burst oscillations. These frequencies can be offset from the NS spin frequency by several Hz (where known independently) and can drift by 1-3 Hz. One plausible explanation is that a wave is present in the bursting ocean, the rotating frame frequency of which is the offset. The frequency of the wave should decrease (in the rotating frame) as the burst cools hence explaining the drift. A strong candidate is a buoyant $r$-mode. To date, models that calculated the frequency of this mode taking into account the radial structure neglected relativistic effects and predicted rotating frame frequencies of $\sim$ 4 Hz and frequency drifts of > 5 Hz; too large to be consistent with observations. We present a calculation that includes frame-dragging and gravitational redshift that reduces the rotating frame frequency by up to 30 % and frequency drift by up to 20 %. Updating previous models for the ocean cooling in the aftermath of the burst to a model more representative of detailed calculations of thermonuclear X-ray bursts reduces the frequency of the mode still further. This model, combined with relativistic effects, can reduce the rotating frequency of the mode to $\sim$ 2 Hz and frequency drift to $\sim$ 2 Hz, which is closer to the observed values. ",Relativistic ocean $r$-modes during type-I X-ray bursts
60,1204885601583009797,3172667405,Dr. Vicki Henderson,"['As of yesterday, the new BECCAL paper is on the arXiv: <LINK>  Check it out for exciting spacey lasers and cold atomy things. I hear that the laser system section is a particularly good read😉#coldatoms #WomenInSTEM #WomenInPhyics #PostdocLife', ""I've got to say, getting a paper out, dealing with #GeneralElection2019  stress, and only having one full week at home since October, is not my favouritest of combos.... #PostdocLife #GetTheToriesOut""]",https://arxiv.org/abs/1912.04849,"Microgravity eases several constraints limiting experiments with ultracold and condensed atoms on ground. It enables extended times of flight without suspension and eliminates the gravitational sag for trapped atoms. These advantages motivated numerous initiatives to adapt and operate experimental setups on microgravity platforms. We describe the design of the payload, motivations for design choices, and capabilities of the Bose-Einstein Condensate and Cold Atom Laboratory (BECCAL), a NASA-DLR collaboration. BECCAL builds on the heritage of previous devices operated in microgravity, features rubidium and potassium, multiple options for magnetic and optical trapping, different methods for coherent manipulation, and will offer new perspectives for experiments on quantum optics, atom optics, and atom interferometry in the unique microgravity environment on board the International Space Station. ",The Bose-Einstein Condensate and Cold Atom Laboratory
61,1204853493170876419,3014054421,Timothée Lesort,"['If you are interested in the regularization methods limitations for continual learning, have a look at our new paper: ""Regularization Shortcomings for Continual Learning"" w A. Stoian and @drFilliat \n<LINK>']",https://arxiv.org/abs/1912.03049,"In most machine learning algorithms, training data is assumed to be independent and identically distributed (iid). When it is not the case, the algorithm's performances are challenged, leading to the famous phenomenon of catastrophic forgetting. Algorithms dealing with it are gathered in the Continual Learning research field. In this paper, we study the regularization based approaches to continual learning and show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: the class-incremental scenario. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments. Moreover, we show that it can have some important consequences on continual multi-tasks reinforcement learning or in pre-trained models used for continual learning. We believe that highlighting and understanding the shortcomings of regularization strategies will help us to use them more efficiently. ",Regularization Shortcomings for Continual Learning
62,1204826730734571520,308361234,Murray Brightman,"['New paper on arXiv today! What we found when we asked @NASASwift to look at the M51 galaxies repeatedly over the last year and a half - <LINK>', 'M51 is a pair of merging galaxies, each with an accreting supermassive black hole (SMBH) at its center, plus several ultraluminous X-ray sources (ULXs). We know 2 of these ULXs are powered by neutron stars, a million times less massive then the SMBHs! https://t.co/jRBnELffbW', 'We got great long-term X-ray lightcurves of all these sources https://t.co/F7XlzXls3P', 'The X-ray lightcurve of one of these neutron star ULXs shows huge swings in brightness, on a period of 38 days. The orbit of the neutron star and its companion is known to be only 2 days, so this is a super-orbital modulation, possibly caused by an extreme disk precession. https://t.co/Clfx9K9H41', 'We also found a new ULX, that appeared and disappeared during the year and a half. Maybe also a neutron star, but maybe something more exotic (red line shows the typical decline of a tidal disruption event...). Now looking for more of these in other Swift observations! https://t.co/7i1sa9j24O', '@AstroGnomie @NASASwift That wasn’t me :-) It is weird what’s happening with that source though!']",https://arxiv.org/abs/1912.04431,"We present the results from a monitoring campaign made with the Neil Gehrels Swift Observatory of the M51 galaxies, which contain several variable ultraluminous X-ray sources (ULXs). The ongoing campaign started in May 2018, and we report here on $\sim1.5$ years of observations. The campaign, which consists of 105 observations, has a typical cadence of 3--6 days, and has the goal of determining the long-term X-ray variability of the ULXs. Two of the most variable sources were ULX7 and ULX8, both of which are known to be powered by neutron stars that are exceeding their isotropic Eddington luminosities by factors of up to 100. This is further evidence that neutron star powered ULXs are the most variable. Our two main results are, first, that ULX7 exhibits a periodic flux modulation with a period of 38 days varying over a magnitude and a half in flux from peak to trough. Since the orbital period of the system is known to be 2 days, the modulation is super-orbital, which is a near-ubiquitous property of ULX pulsars. Secondly we identify a new transient ULX, M51 XT-1, the onset of which occurred during our campaign, reaching a peak luminosity of $\sim10^{40}$ erg s$^{-1}$, before gradually fading over the next $\sim200$ days until it slipped below the detection limit of our observations. Combined with the high-quality Swift/XRT lightcurve of the transient, serendipitous observations made with Chandra and XMM-Newton provide insights into the onset and evolution of a likely super-Eddington event. ","Swift monitoring of M51: A 38-day super-orbital period for the pulsar
  ULX7 and a new transient ULX"
63,1204724454938546176,981529346774102018,Jamie O'Halloran,"[""A research project I have worked on has been picked up by the New Scientist. Key message: Don't bother tapping your beer to prevent beer loss. @SDU_PhD @DaCHE_SDU @PublicHealthSDU @newscientist \n\n<LINK> \n\nFull paper: <LINK>\n\n#research #beers""]",https://arxiv.org/abs/1912.01999,"Objective: Preventing or minimising beer loss when opening a can of beer is socially and economically desirable. One theoretically grounded approach is tapping the can prior to opening, although this has never been rigorously evaluated. We aimed to evaluate the effect of tapping a can of beer on beer loss. Methods: Single centre parallel-group randomised controlled trial. 1031 cans of cans of beer of 330mL were randomised into one of four groups before the experiment: unshaken/untapped (n=256), unshaken/tapped (n=251), shaken/untapped (n=249), or shaken/tapped (n=244). The intervention was tapping the can of beer three times on its side with a single finger. We compared tapping versus non-tapping for cans that had been shaken for 2 minutes or were unshaken. Three teams weighed, tapped or did not tap, opened cans, absorbed any beer loss using paper towels, then re-weighed cans. The teams recorded the mass of each can before and after opening with an accuracy of +/-0.01 grams. Main outcome measure: The main outcome measure was beer loss (in grams). This was calculated as the difference in the mass of the beer after the can was opened compared to before the can was opened. Results: For shaken cans, there was no statistically significant difference in the mass of beer lost when tapping compared to not tapping (mean difference of -0.159g beer lost with tapping, 95% CI -0.36 to 0.04). For unshaken cans, there was also no statistically significant difference between tapping and not tapping. Conclusion: These findings suggest that tapping shaken beer cans does not prevent beer loss when the container is opened. Thus, the practice of tapping a beer prior to opening is unsupported. The only apparent remedy to avoid liquid loss is to wait for bubbles to settle before opening the can. ","To beer or not to beer: does tapping beer cans prevent beer loss? A
  randomised controlled trial"
64,1204610705397186561,1160578862419345408,"Takinoue Lab, Tokyo Tech","['Our new paper entitled ""Collective ratchet transport generated by particle crowding under asymmetric sawtooth-shaped static potential"" by Masayuki Hayakawa, Yusuke Kishino, *Masahiro Takinoue has been uploaded on arXiv.\n<LINK> <LINK>']",https://arxiv.org/abs/1912.04659,"In this study, we describe the ratchet transport of particles under static asymmetric potential with periodicity. Ratchet transport has garnered considerable attention due to its potential for developing smart transport techniques on a micrometer scale. In previous studies, either particle self-propulsion or time varying potential was introduced to realize unidirectional transport. Without utilizing these two factors, we experimentally demonstrate ratchet transport through particle interactions during crowding. Such ratchet transport induced by particle interaction has not been experimentally demonstrated thus far, although some theoretical studies had suggested that particle crowding enhances ratchet transport. In addition, we constructed a model for such transport in which the potential varies depending on the particle density, which agrees well with our experimental results. This study can accelerate the development of transport techniques on a micrometer scale. ","Collective ratchet transport generated by particle crowding under
  asymmetric sawtooth-shaped static potential"
65,1204584181864390656,2337598033,Geraint F. Lewis,"['And another new paper on the arxiv “A SkyMapper view of the Large Magellanic Cloud: The dynamics of stellar populations”, again led by PhD student, Zhen Wan, ex-PhD student @astroconfusion, @dougalmackey and Rodrigo Ibata <LINK> <LINK>']",https://arxiv.org/abs/1912.04657,"We present the first SkyMapper stellar population analysis of the Large Magellanic Cloud (hereafter LMC),including the identification of 3578 candidate Carbon Stars through their extremely red $g-r$ colours. Coupled with Gaia astrometry, we analyse the distribution and kinematics of this Carbon Star population, finding the LMC to be centred at $(R.A., Dec.) = (80.90^{\circ}\pm{0.29}, -68.74^{\circ}\pm{0.12})$, with a bulk proper motion of $(\mu_{\alpha},\mu_{\delta}) = (1.878\pm0.007,0.293\pm0.018) \mathrm{mas\ yr^{-1}}$ and a disk inclination of $i = 25.6^{\circ}\pm1.1$ at position angle $\theta = 135.6^{\circ}\pm 3.3^{\circ}$. We complement this study with the identification and analysis of additional stellar populations, finding that the dynamical centre for Red Giant Branch (RGB) stars is similar to that seen for the Carbon Stars, whereas for young stars the dynamical centre is significantly offset from the older populations. This potentially indicates that the young stars were formed as a consequence of a strong tidal interaction, probably with the Small Magellanic Cloud (SMC). In terms of internal dynamics, the tangential velocity profile increases linearly within $\sim3\ \mathrm{kpc}$, after which it maintains an approximately constant value of $V_{rot} = 83.6\pm 1.7 \mathrm{km\ s^{-1}}$ until $\sim7 \mathrm{kpc}$. With an asymmetric drift correction, we estimate the mass within $7 \mathrm{kpc}$ to be $M_{\rm LMC}(<7\mathrm{kpc}) = (2.5\pm0.1)\times10^{10}\ \mathrm{M}_{\odot}$ and within the tidal radius ($\sim 30\ \mathrm{kpc}$) to be $M_{\rm LMC}(<30\mathrm{kpc}) = (1.06 \pm 0.32)\times10^{11}\ \mathrm{M}_{\odot}$, consistent with other recent measurements. ","A SkyMapper view of the Large Magellanic Cloud: The dynamics of stellar
  populations"
66,1204504082700013569,2427184074,Christopher Berry,"['🌞New paper with Pablo Marchant, @spacetimekatie, Ilya Mandel &amp; @sciencejedi on how to look inside stars with gravitational waves!\n<LINK> <LINK>', ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav It turns out stars are really big. It's hard to figure out what's in them—you can't use light to see inside. Asteroseismology uses sound waves to probe stellar structure. Neutrinos can be used to measure nuclear reactions inside a star. Here, we propose using gravitational waves"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav Gravitational waves interact weakly with matter (it's why they're hard to detect). This means they travel freely through stars. However, the star's gravity will deflect the path the wave takes. If we can measure this, we can work out the structure of the star!"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav It turns out the binary coalescence signals we have observed so far are bad for this. The probability a signal would travel through a star is tiny, and we wouldn't know it was deflected as we wouldn't have a reference to compare against. However…"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav Continuous gravitational wave sources are perfect. They are on all the time, so you can measure them before, during and after an eclipse. Also, since the Sun will eventually cover a notable fraction of the sky over a year, it's not too improbable we'll find an eclipsed source! https://t.co/hkzxxsUnQ5"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav The best chance of seeing an eclipsed source actually comes from a continuous wave source in a binary, as it can be eclipsed by its companion. However, I'll focus on sources being eclipsed by the Sun, as they're a bit easier to deal with https://t.co/J7S9KvJH5b"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav The effect of being eclipsed by the Sun on a gravitational wave is tiny. We estimated how well you could measure these effects. It turns out you'll need a signal-to-noise ratio of 100 during the eclipse, meaning &gt;2500 for a signal measured for a year! https://t.co/LLvq7Eow6j"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav The high signal-to-noise needed to make this measurements is challenging, but there is good news: \n1. It means we don't need to worry about these effects when making first detections\n2. It means that we'll know if a source is eclipsed way ahead of time…"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav Potentially we could detect a continuous wave source with current detectors. We'll know it's source location, so we can work out if it is eclipsed and *when* it'll eclipse. With next generation detectors we could then make the measurements"", ""@spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav I especially like the idea of building a detector which is very sensitive to a narrow range of frequencies. That's easier than sensitive across a whole band. We could tune it for one eclipsing source on one day, then retune for the next eclipse—we'll know the schedule in advance!"", '@CJHaster @spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav https://t.co/3D4vQJZ308', '@CJHaster @spacetimekatie @sciencejedi @NUCIERA @ARC_OzGRav Or, we orbit the bar about a black hole, and chanfe the orbit so that the signal is blueshifted to the right frequency']",https://arxiv.org/abs/1912.04268,"Although gravitational waves only interact weakly with matter, their propagation is affected by a gravitational potential. If a gravitational wave source is eclipsed by a star, measuring these perturbations provides a way to directly measure the distribution of mass throughout the stellar interior. We compute the expected Shapiro time delay, amplification and deflection during an eclipse, and show how this can be used to infer the mass distribution of the eclipsing body. We identify continuous gravitational waves from neutron stars as the best candidates to detect this effect. When the Sun eclipses a far-away source, depending on the depth of the eclipse the time-delay can change by up to $\sim 0.034$ ms, the gravitational-wave strain amplitude can increase by $\sim 4$%, and the apparent position of the source in the sky can vary by $4''$. Accreting neutron stars with Roche-lobe filling companion stars have a high probability of exhibiting eclipses, producing similar time delays but undetectable changes in amplitude and sky location. Even for the most rapidly rotating neutron stars, this time delay only corresponds to a few percent of the phase of the gravitational wave, making it an extremely challenging measurement. However, if sources of continuous gravitational waves exist just below the limit of detection of current observatories, next-generation instruments will be able to observe them with enough precision to measure the signal of an eclipsing star. Detecting this effect would provide a new direct probe to the interior of stars, complementing asteroseismology and the detection of solar neutrinos. ","Eclipses of continuous gravitational waves as a probe of stellar
  structure"
67,1204397702634594307,60893773,James Bullock,"['Very excited about a new paper led by @AstroBananna using FIRE simulations. Proposes that stars born in outflows populate the stellar halos of Milky Way type galaxies.  <LINK> <LINK>', 'These stars can be ~50-400 kpc from the galaxy and contribute similar outer halo mass as accreted stars !  They dominate the metal rich outer halo, but unlike accreted stars they are usually smoothly distributed https://t.co/qaH56lRMzZ', 'Outflow stars should be more alpha-element enhanced than stars from accreted, disrupted dwarfs. This is because they’re born in starbursts https://t.co/7ziyDARMtP', 'Fun thing about this idea: it’s an  inverse of the classic ‘rapid collapse’ model of ELS 1962 who suggested that the outer galaxy formed from rapidly infalling gas clouds. We’re proposing halo stars formed from rapid *outflow 💫 💥']",https://arxiv.org/abs/1912.03316,"We study stellar-halo formation using six Milky Way-mass galaxies in FIRE-2 cosmological zoom simulations. We find that $5-40\%$ of the outer ($50-300$ kpc) stellar halo in each system consists of $\textit{in-situ}$ stars that were born in outflows from the main galaxy. Outflow stars originate from gas accelerated by super-bubble winds, which can be compressed, cool, and form co-moving stars. The majority of these stars remain bound to the halo and fall back with orbital properties similar to the rest of the stellar halo at $z=0$.In the outer halo, outflow stars are more spatially homogeneous, metal rich, and alpha-element-enhanced than the accreted stellar halo. At the solar location, up to $\sim 10 \%$ of our kinematically-identified halo stars were born in outflows; the fraction rises to as high as $\sim 40\%$ for the most metal-rich local halo stars ([Fe/H] $> -0.5$). We conclude that the Milky Way stellar halo could contain local counterparts to stars that are observed to form in molecular outflows in distant galaxies. Searches for such a population may provide a new, near-field approach to constraining feedback and outflow physics. A stellar halo contribution from outflows is a phase-reversal of the classic halo formation scenario of Eggen, Lynden-Bell $\&$ Sandange, who suggested that halo stars formed in rapidly $\textit{infalling}$ gas clouds. Stellar outflows may be observable in direct imaging of external galaxies and could provide a source for metal-rich, extreme velocity stars in the Milky Way. ",Stars made in outflows may populate the stellar halo of the Milky Way
68,1204386350503485450,720998241073029120,MacKenzie Warren,"['New paper up today!!  With @sean_couch, @evanoc, and Viktoriya Morozova.  The short version: how much can we learn from detections of CCSN neutrinos + GWs about the progenitor star, explosion, &amp; remnant compact object?? A TON <LINK> <LINK>', 'We used several hundred 1D CCSN simulations to study multi-messenger (neutrino + gravitational wave) signals.  There is no ""universal"" neutrino or GW signal, but the differences between CCSN events will tell us a lot about the progenitor structure and explosion (or lack thereof)', ""But seriously, that's all you need - total neutrino counts, neutrino average energy, and/or the dominant GW frequency.  It's stupidly simple.  We can do this with current neutrino + GW detectors.  Near future facilities will do this even better."", 'This will give us constraints on things like the explosion energy, progenitor star, etc long before shock breakout and w/o relying on pre-explosion imaging.  Pre-explosion imaging can constrain the stellar surface before collapse &amp; this constrains the stellar core. Complimentary!', ""I don't have any thoughts here on 70 M_sun BHs, but I can get an estimate for the BH mass for a failed CCSN event within a few seconds of core-collapse.  So that's something https://t.co/FXZzlTY78L"", ""I'll be posting the neutrino + GW info from all of these simulations publicly whenever I get it all uploaded.  There are already some awesome follow-up studies in the works.  Keep an eye out for @AstroBarker's work on CCSN light curves!"", ""This is one of those papers that took approx an eternity, with endless iterations on simulations + analysis. Thanks to my coauthors for their endless patience &amp; insights! As happy as I am to have it submitted, I'm mostly relieved to have it off my back. Now back to more writing😅""]",https://arxiv.org/abs/1912.03328,"With the advent of modern neutrino and gravitational wave detectors, the promise of multi-messenger detections of the next galactic core-collapse supernova has become very real. Such detections will give insight into the core-collapse supernova mechanism, the structure of the progenitor star, and may resolve longstanding questions in fundamental physics. In order to properly interpret these detections, a thorough understanding of the landscape of possible core-collapse supernova events, and their multi-messenger signals, is needed. We present detailed predictions of neutrino and gravitational wave signals from 1D simulations of stellar core collapse, spanning the landscape of core-collapse progenitors from $9-120\,\mathrm{M}_{\odot}$. In order to achieve explosions in 1D, we use the STIR model, which includes the effects of turbulence and convection in 1D supernova simulations to mimic the 3D explosion mechanism. We study the gravitational wave emission from the 1D simulations using an astroseismology analysis of the proto-neutron star. We find that the neutrino and gravitational wave signals are strongly correlated with the structure of the progenitor star and remnant compact object. Using these correlations, future detections of the first few seconds of neutrino and gravitational wave emission from a galactic core-collapse supernova may be able to provide constraints on stellar evolution independent of pre-explosion imaging and the mass of the compact object remnant prior to fallback accretion. ","Constraining properties of the next nearby core-collapse supernova with
  multi-messenger signals"
69,1204327613180862464,916407589,Anders Kvellestad,['Review paper on global fits of new physics models with GAMBIT out today: <LINK>'],https://arxiv.org/abs/1912.04079,"The Global and Modular Beyond-Standard Model Inference Tool (GAMBIT) is an open source software framework for performing global statistical fits of particle physics models, using a wide range of particle and astroparticle data. In this review, we describe the design principles of the package, the statistical and sampling frameworks, the experimental data included, and the first two years of physics results generated with it. This includes supersymmetric models, axion theories, Higgs portal dark matter scenarios and an extension of the Standard Model to include right-handed neutrinos. Owing to the broad spectrum of physics scenarios tackled by the GAMBIT community, this also serves as a convenient, self-contained review of the current experimental and theoretical status of the most popular models of dark matter. ","GAMBIT and its Application in the Search for Physics Beyond the Standard
  Model"
70,1204223826705272832,16079444,Ying-Jer Kao,"['New paper with Tarun, Kai-Hsin (@wukaihsin), Tsung-Cheng, Chia-Min on Entanglement Renyi negativity across a finite temperature transition\n\n<LINK>']",https://arxiv.org/abs/1912.03313,"Quantum entanglement is fragile to thermal fluctuations, which raises the question whether finite temperature phase transitions support long-range entanglement similar to their zero temperature counterparts. Here we use quantum Monte Carlo simulations to study the third Renyi negativity, a generalization of entanglement negativity, as a proxy of mixed-state entanglement in the 2D transverse field Ising model across its finite temperature phase transition. We find that the area-law coefficient of the Renyi negativity is singular across the transition, while its subleading constant is zero within the statistical error. This indicates that the entanglement is short-ranged at the critical point despite a divergent correlation length. Renyi negativity in several exactly solvable models also shows qualitative similarities to that in the 2D transverse field Ising model. ","Entanglement Renyi negativity across a finite temperature transition: a
  Monte Carlo study"
71,1204000149728571397,2421923048,Anne-Marie Weijmans,"['Today is @sdssurveys Data Release Day! So I get to take over their twitter account. #DR16 will go live later today, but you can already read the data release paper here: <LINK>. Lots of new data!']",https://arxiv.org/abs/1912.02905,"This paper documents the sixteenth data release (DR16) from the Sloan Digital Sky Surveys; the fourth and penultimate from the fourth phase (SDSS-IV). This is the first release of data from the southern hemisphere survey of the Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2); new data from APOGEE-2 North are also included. DR16 is also notable as the final data release for the main cosmological program of the Extended Baryon Oscillation Spectroscopic Survey (eBOSS), and all raw and reduced spectra from that project are released here. DR16 also includes all the data from the Time Domain Spectroscopic Survey (TDSS) and new data from the SPectroscopic IDentification of ERosita Survey (SPIDERS) programs, both of which were co-observed on eBOSS plates. DR16 has no new data from the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey (or the MaNGA Stellar Library ""MaStar""). We also preview future SDSS-V operations (due to start in 2020), and summarize plans for the final SDSS-IV data release (DR17). ","The Sixteenth Data Release of the Sloan Digital Sky Surveys: First
  Release from the APOGEE-2 Southern Survey and Full Release of eBOSS Spectra"
72,1203993921749225472,4249537197,Christian Wolf,"['New paper from @CorentK, co-supervised  with @moezbac  @antigregory. We show that object-word alignment does not emerge naturally in BERT like training of transformers on vision-language task and propose weak supervision for it, improving performance: \n<LINK> <LINK>']",https://arxiv.org/abs/1912.03063,"The large adoption of the self-attention (i.e. transformer model) and BERT-like training principles has recently resulted in a number of high performing models on a large panoply of vision-and-language problems (such as Visual Question Answering (VQA), image retrieval, etc.). In this paper we claim that these State-Of-The-Art (SOTA) approaches perform reasonably well in structuring information inside a single modality but, despite their impressive performances , they tend to struggle to identify fine-grained inter-modality relationships. Indeed, such relations are frequently assumed to be implicitly learned during training from application-specific losses, mostly cross-entropy for classification. While most recent works provide inductive bias for inter-modality relationships via cross attention modules, in this work, we demonstrate (1) that the latter assumption does not hold, i.e. modality alignment does not necessarily emerge automatically, and (2) that adding weak supervision for alignment between visual objects and words improves the quality of the learned models on tasks requiring reasoning. In particular , we integrate an object-word alignment loss into SOTA vision-language reasoning models and evaluate it on two tasks VQA and Language-driven Comparison of Images. We show that the proposed fine-grained inter-modality supervision significantly improves performance on both tasks. In particular, this new learning signal allows obtaining SOTA-level performances on GQA dataset (VQA task) with pre-trained models without finetuning on the task, and a new SOTA on NLVR2 dataset (Language-driven Comparison of Images). Finally, we also illustrate the impact of the contribution on the models reasoning by visualizing attention distributions. ","Weak Supervision helps Emergence of Word-Object Alignment and improves
  Vision-Language Tasks"
73,1203945242929647617,157973000,Michael Pfarrhofer,['My new working paper with @annstelzer is now online @arxiv. It explores the international effects of central bank policy announcements (monetary policy and central bank information shocks) by the @federalreserve and the @ecb #econometrics\n<LINK>'],https://arxiv.org/abs/1912.03158,"We explore the international transmission of monetary policy and central bank information shocks by the Federal Reserve and the European Central Bank. Identification of these shocks is achieved by using a combination of high-frequency market surprises around announcement dates of policy decisions and sign restrictions. We propose a high-dimensional macroeconometric framework for modeling aggregate quantities alongside country-specific variables to study international shock propagation and spillover effects. Our results are in line with the established literature focusing on individual economies, and moreover suggest substantial international spillover effects in both directions for monetary policy and central bank information shocks. In addition, we detect heterogeneities in the transmission of ECB policy actions to individual member states. ",The international effects of central bank information shocks
74,1203042407543001088,84902368,Abhishek Das,"['New work from @VishvakM!\n\nBERT for visual dialog. Achieves single-model SoTA.\n\nPaper: <LINK>\nCode: <LINK> <LINK>', '@gchnkang @VishvakM https://t.co/WpPbeu5MsG']",https://arxiv.org/abs/1912.02379,"Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn visually-grounded conversations. Our model is pretrained on the Conceptual Captions and Visual Question Answering datasets, and finetuned on VisDial. Our best single model outperforms prior published work (including model ensembles) by more than 1% absolute on NDCG and MRR. Next, we find that additional finetuning using ""dense"" annotations in VisDial leads to even higher NDCG -- more than 10% over our base model -- but hurts MRR -- more than 17% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions. ","Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art
  Baseline"
75,1203009449838993413,8514822,asim kadav,"['Our new paper ""15 Keypoints Is All You Need"" <LINK> is on arXiv. It describes ""KeyTrack"" which has been #1 on the PoseTrack \'17 leaderboard since last month. With just 0.43M parameters, it can be trained on 1 GPU in under two hours. #computervision <LINK>', 'Paper also includes comparisons of transformer and convolutions, and at low resolutions, transformers can outperform convolutions for the tracking task.']",http://arxiv.org/abs/1912.02323,"Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information. ",15 Keypoints Is All You Need
76,1202996894131195906,1187340991453368320,Gor Oganesyan,['Our new paper on the mystery of γ-ray bursts: protons emitting by synchrotron could solve the incomplete cooling puzzle <LINK> <LINK>'],https://arxiv.org/abs/1912.02185,"We discuss the new surprising observational results that indicate quite convincingly that the prompt emission of Gamma-Ray Bursts (GRBs) is due to synchrotron radiation produced by a particle distribution that has a low energy cut-off. The evidence of this is provided by the low energy part of the spectrum of the prompt emission, that shows the characteristic F(nu) \propto nu^(1/3) shape followed by F(nu) \propto nu^(-1/2) up to the peak frequency. This implies that although the emitting particles are in fast cooling, they do not cool completely. This poses a severe challenge to the basic ideas about how and where the emission is produced, because the incomplete cooling requires a small value of the magnetic field, to limit synchrotron cooling, and a large emitting region, to limit the self-Compton cooling, even considering Klein-Nishina scattering effects. Some new and fundamental ingredient is required for understanding the GRBs prompt emission. We propose proton-synchrotron as a promising mechanism to solve the incomplete cooling puzzle. ","Proton-synchrotron as the radiation mechanism of the prompt emission of
  GRBs?"
77,1202988141759455233,4870078413,Sam Schoenholz,"[""After a ton of work by a bunch of people, we're releasing an entirely new Neural Tangents.\n\nPaper: <LINK>\nGithub: <LINK>\nColab Notebook: <LINK>"", 'Over the past few years there has been a lot of research into very wide / infinitely wide neural networks. This limit has a lot of appealing properties: neural networks become Gaussian Processes and their gradient-descent training dynamics become completely tractable.', 'This gives unprecedented insight into how / why neural networks behave the way they do. However, the math for infinite networks can be tricky and had to be worked out from scratch  for each new architecture. \n\nThis is very similar to deep learning pre-automatic differentiation.', ""The core of Neural Tangents is a high level neural network library. Any network specified in Neural Tangents automatically comes with a function to compute the infinite-width limit analytically. \n\nHere's an example for a two-hidden layer FC network: https://t.co/xtk3BTkGbD"", ""But of course, why stop there. Here's the code to compute the infinite-width limit of a Wide Residual Network (Resnet 28-\\infty): https://t.co/3fQc8tj28b"", ""We also include functions to perform exact Bayesian inference on any of these models as well as continuous time gradient descent via the Neural Tangent Kernel. Here's how one could do gradient descent training. Ofc these models naturally come with uncertainty estimates. https://t.co/9Ef1y1bJkj"", ""There's a lot more in NT and we're actively working on it to make it even better. \n\nIf you're around NeurIPS, Roman Novak will be giving a talk at the AABI and we'll also be at the Bayesian DL workshop and the Science meets Engineering workshop. Come say hi!"", 'This project was the work of Roman Novak, @Locchiu, Jiri Hron, @hoonkp, @alemi, @jaschasd, and myself. I am very fortunate to work with such an outstanding group of researchers!', '@Locchiu @hoonkp @alemi @jaschasd Also thanks to @yasamanbb and @TheGregYang who gave a lot of fantastic feedback / discussion / advice.', '@jessebett @SingularMattrix @FieldsInstitute @VectorInst Great question! There are a number of factors here (hardware, neural network architecture etc...). Thanks to JAX (@SingularMattrix) all of the code works on GPU / TPU which speeds things up considerably. We also support multi-device parallelism which can speed things up further.', ""@jessebett @SingularMattrix @FieldsInstitute @VectorInst It's a bit hard to do a direct comparison with stax bc the kernel is quadratic in the number of data points whereas the finite-width fprop is linear. However, you don't have to do gradient descent training on the infinite network."", '@jessebett @SingularMattrix @FieldsInstitute @VectorInst Nonetheless, for 50 inputs in a fully-connected network the finite-width calculation takes 190 microseconds and for the infinite 150 microseconds on a P100 GPU. Convolutions are much slower since the cov matrix is O((|dataset| * pixels)^2). (Numbers are in the paper).', ""@jessebett @SingularMattrix @FieldsInstitute @VectorInst Yes, this is a good point. TBH I think it's a dataset size thing. Infinite networks are probably a bad fit for imagenet or aggressive data augmentation. Benefits are mostly 1) small data regime 2) everything is analytic and so we can understand why models behave the way they do."", '@dustinvtran @jessebett @SingularMattrix @FieldsInstitute @VectorInst Yes! It is over the full dataset. We do provide tools (linearizing / taylor series approximation to the finite-width network) that can be easily minibatched. \n\nIndeed @dustinvtran, all our analytic solutions are for MSE only. We provide numerical ODE solving for cross-entropy.', '@dustinvtran @jessebett @SingularMattrix @FieldsInstitute @VectorInst I imagine you have a lot more expertise here, would love to talk sometime re: things that we can do for cross-entropy.']",https://arxiv.org/abs/1912.02803,"Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space. The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook. ",Neural Tangents: Fast and Easy Infinite Neural Networks in Python
78,1202943855630401536,789125136481910784,Raphael Shirley,['New paper by N. Krefting et al. The role of environment in galaxy evolution in the SERVS Survey I: density maps and cluster candidates <LINK>'],https://arxiv.org/abs/1912.02238,"We use photometric redshifts derived from new $u$-band through 4.5$\mu$m Spitzer IRAC photometry in the 4.8\,deg$^2$ of the XMM-LSS field to construct surface density maps in the redshift range 0.1-1.5. Our density maps show evidence for large-scale structure in the form of filaments spanning several tens of Mpc. Using these maps, we identify 339 overdensities that our simulated lightcone analysis suggests are likely associated with dark matter haloes with masses, $M_{\rm halo}$, log($M_{\rm halo}/M_{\odot})>$13.7. From this list of overdensities we recover 43 of 70 known X-ray detected and spectroscopically confirmed clusters. The missing X-ray clusters are largely at lower redshifts and lower masses than our target log($M_{\rm halo}/M_{\odot})>$13.7. The bulk of the overdensities are compact, but a quarter show extended morphologies which include likely projection effects, clusters embedded in apparent filaments as well as at least one potential cluster merger (at $z\sim1.28$). The strongest overdensity in our highest redshift slice (at $z\sim1.5$) shows a compact red galaxy core potentially implying a massive evolved cluster. ","The role of environment in galaxy evolution in the SERVS Survey I:
  density maps and cluster candidates"
79,1202935540175310854,938463903754862593,George Papamakarios,"['Check out our extensive review paper on normalizing flows!\n\nThis paper is the product of years of thinking about flows: it contains everything we know about them, and many new insights.\n\nWith @eric_nalisnick, @DeepSpiker, @shakir_za, @balajiln.\n\n<LINK>\n\nThread 👇 <LINK>', 'We hope there is something there for everyone interested in flows:\n\n- A gentle introduction for those wanting to get started.\n\n- Explanations of existing flows for practitioners who want to deepen their understanding.\n\n- Advanced topics for seasoned experts. https://t.co/j7q5jcgrJD']",http://arxiv.org/abs/1912.02762,"Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning. ",Normalizing Flows for Probabilistic Modeling and Inference
80,1202868429780336640,797433864,Danilo J. Rezende,"['Looking for something to read in your flight to #NeurIPS2019?  Read about Normalizing Flows from our extensive review paper (also with new insights on how to think about and derive new flows) <LINK> with @gpapamak @eric_nalisnick @DeepSpiker  @balajiln @shakir_za <LINK>', 'A big shout-out to @gpapamak and @eric_nalisnick for their great efforts to make sure this becomes a reality!']",https://arxiv.org/abs/1912.02762,"Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning. ",Normalizing Flows for Probabilistic Modeling and Inference
81,1202803478776541184,1107999313,Yazhuo Deng,"['Excited to share our paper ""The #AutoregressiveStructuralModel for analyzing #longitudinal health data of an aging population in China"" on arXiv.\n\nThe new ASM as a SEM can capture the #dynamics of a complex relationships/system using #longitudinal data. \n<LINK> <LINK>', 'Thanks to Drs. @audreyqyfu and Dave Paul.']",https://arxiv.org/abs/1912.02359,"We seek to elucidate the impact of social activity, physical activity and functional health status (factors) on depressive symptoms (outcome) in the China Health and Retirement Longitudinal Study (CHARLS), a multi-year study of aging involving 20,000 participants 45 years of age and older. Although a variety of statistical methods are available for analyzing longitudinal data, modeling the dynamics within a complex system remains a difficult methodological challenge. We develop an Autoregressive Structural Model (ASM) to examine these factors on depressive symptoms while accounting for temporal dependence. The ASM builds on the structural equation model and also consists of two components: a measurement model that connects observations to latent factors, and a structural model that delineates the mechanism among latent factors. Our ASM further incorporates autoregressive dependence into both components for repeated measurements. The results from applying the ASM to the CHARLS data indicate that social and physical activity independently and consistently mitigated depressive symptoms over the course of five years, by mediating through functional health status. ","The Autoregressive Structural Model for analyzing longitudinal health
  data of an aging population in China"
82,1202797622324551680,2899735410,Caleb Harada 🏳️‍🌈🪐🌌,"[""📣 Radiative feedback between clouds and atmospheric thermal structure is an important consideration in high-resolution thermal emission spectra of hot Jupiters ‼\n\nCheck out this cool new paper on astro-ph y'all! 💃\n<LINK>""]",https://arxiv.org/abs/1912.02268,"Observations of scattered light and thermal emission from hot Jupiter exoplanets have suggested the presence of inhomogeneous aerosols in their atmospheres. 3D general circulation models (GCMs) that attempt to model the effects of aerosols have been developed to understand the physical processes that underlie their dynamical structures. In this work, we investigate how different approaches to aerosol modeling in GCMs of hot Jupiters affect high-resolution thermal emission spectra throughout the duration of the planet's orbit. Using results from a GCM with temperature-dependent cloud formation, we calculate spectra of a representative hot Jupiter with different assumptions regarding the vertical extent and thickness of clouds. We then compare these spectra to models in which clouds are absent or simply post-processed (i.e., added subsequently to the completed clear model). We show that the temperature-dependent treatment of clouds in the GCM produces high-resolution emission spectra that are markedly different from the clear and post-processed cases -- both in the continuum flux levels and line profiles -- and that increasing the vertical extent and thickness of clouds leads to bigger changes in these features. We evaluate the net Doppler shifts of the spectra induced by global winds and the planet's rotation and show that they are strongly phase-dependent, especially for models with thicker and more extended clouds. This work further demonstrates the importance of radiative feedback in cloudy atmospheric models of hot Jupiters, as this can have a significant impact on interpreting spectroscopic observations of exoplanet atmospheres. ","Signatures of Clouds in Hot Jupiter Atmospheres: Modeled High Resolution
  Emission Spectra from 3D General Circulation Models"
83,1202785095616204801,1012125662117851136,Edward Kennedy,"['New paper alert!\n\n""Sensitivity analysis via % of unmeasured confounding""\n<LINK>\n\nSA is absolutely crucial for causality: holy grail is finding one that\'s interpretable *&amp;* flexible\n\n@bonv3 &amp; I propose contamination model approach, w/effect bds across % confounded <LINK>', '@bonv3 Matteo Bonvini @bonv3 is a star PhD student in stats @CMU_Stats @CarnegieMellon', '@Basucally @bonv3 Thanks Anirban!\n\nYes the E-value is a perhaps more standard sensitivity analysis where one assumes bounds on U-trt &amp; U-Y relationships.\n\nHere are some relevant screen grabs from our paper where we discuss &amp; compare (pgs 1 &amp; 8): https://t.co/SA6MAsjejt']",http://arxiv.org/abs/1912.02793,"In observational studies, identification of ATEs is generally achieved by assuming that the correct set of confounders has been measured and properly included in the relevant models. Because this assumption is both strong and untestable, a sensitivity analysis should be performed. Common approaches include modeling the bias directly or varying the propensity scores to probe the effects of a potential unmeasured confounder. In this paper, we take a novel approach whereby the sensitivity parameter is the ""proportion of unmeasured confounding:"" the proportion of units for whom the treatment is not as good as randomized even after conditioning on the observed covariates. We consider different assumptions on the probability of a unit being unconfounded. In each case, we derive sharp bounds on the average treatment effect as a function of the sensitivity parameter and propose nonparametric estimators that allow flexible covariate adjustment. We also introduce a one-number summary of a study's robustness to the number of confounded units. Finally, we explore finite-sample properties via simulation, and apply the methods to an observational database used to assess the effects of right heart catheterization. ",Sensitivity analysis via the proportion of unmeasured confounding
84,1202783008261935104,2337598033,Geraint F. Lewis,['New paper on the arXiv “Microlensing and Photon Bunching: The impact of decoherence“\n\n<LINK> <LINK>'],https://arxiv.org/abs/1912.02246,"Gravitational microlensing within the Galaxy offers the prospect of probing the details of distant stellar sources, as well as revealing the distribution of compact (and potentially non-luminous) masses along the line-of-sight. Recently, it has been suggested that additional constraints on the lensing properties can be determined through the measurement of the time delay between images through the correlation of the bunching of photon arrival times; an application of the Hanbury-Brown Twiss effect. In this paper, we revisit this analysis, examining the impact of decoherence of the radiation from a spatially extended source along the multiple paths to an observer. The result is that, for physically reasonable situations, such decoherence completely erases any correlation that could otherwise be used to measure the gravitational lensing time delay. Indeed, the divergent light paths traverse extremely long effective baselines at the lens plane, corresponding to extremes of angular resolving power well beyond those attainable with any terrestrial technologies; the drawback being that few conceivable celestial objects would be sufficiently compact with high enough surface brightness to yield usable signals. ",Microlensing and Photon Bunching: The impact of decoherence
85,1202595911773413376,76331020,Caius Selhorst,['How the 37 GHz solar radius varies through the solar cycles? Check it in our new paper <LINK>\n(with @guiguesp)'],http://arxiv.org/abs/1912.01671,"To better understand the influence of the activity cycle on the solar atmosphere, we report the time variation of the radius observed at 37 GHz ($\lambda$=8.1 mm) obtained by the Mets\""ahovi Radio Observatory (MRO) through Solar Cycles 22 to 24 (1989-2015). Almost 5800 maps were analyzed, however, due to instrumental setups changes the data set showed four distinct behaviors, which requested a normalisation process to allow the whole interval analysis. When the whole period was considered, the results showed a positive correlation index of 0.17 between the monthly means of the solar radius at 37 GHz and solar flux obtained at 10.7 cm (F10.7). This correlation index increased to 0.44, when only the data obtained during the last period without instrumental changes were considered (1999-2015). The solar radius correlation with the solar cycle agrees with the previous results obtained at mm/cm wavelengths (17 and 48 GHz), nevertheless, this result is the opposite of that reported at submillimetre wavelengths (212 and 405 GHz). ",The Solar Radius at 37 GHz through Cycles 22 to 24
86,1202411191118651392,923512102555475970,Goold_Group,"['So proud of this work by @QuSysTCD Phd student Marlon Brenes and the rest of team and collaboration. This paper is really a big job - a new methodology to simulate quantum thermal machines with highly non trivial working media <LINK> @TCD_physics <LINK>', '@TCD_physics @jenseisert @MarkMitchison @quantumwizard @royalsociety @scienceirel @BullshitQuantum @QuantumThermo']",https://arxiv.org/abs/1912.02053,"We present a methodology to simulate the quantum thermodynamics of thermal machines which are built from an interacting working medium in contact with fermionic reservoirs at fixed temperature and chemical potential. Our method works at finite temperature, beyond linear response and weak system-reservoir coupling, and allows for non-quadratic interactions in the working medium. The method uses mesoscopic reservoirs, continuously damped towards thermal equilibrium, in order to represent continuum baths and a novel tensor network algorithm to simulate the steady-state thermodynamics. Using the example of a quantum-dot heat engine, we demonstrate that our technique replicates the well known Landauer-B\""uttiker theory for efficiency and power. We then go beyond the quadratic limit to demonstrate the capability of our method by simulating a three-site machine with non-quadratic interactions. Remarkably, we find that such interactions lead to power enhancement, without being detrimental to the efficiency. Furthermore, we demonstrate the capability of our method to tackle complex many-body systems by extracting the super-diffusive exponent for high-temperature transport in the isotropic Heisenberg model. Finally, we discuss transport in the gapless phase of the anisotropic Heisenberg model at finite temperature and its connection to charge conjugation-parity, going beyond the predictions of single-site boundary driving configurations. ","Tensor-network method to simulate strongly interacting quantum thermal
  machines"
87,1202405325329989632,838292815,Ofir Nachum,"['Very excited about my new paper! <LINK>\nWe formulate the on-policy max-return RL objective w.r.t *arbitrary* offline data and without *any* explicit importance correction. Amazingly, the gradient of the objective w.r.t pi is exactly the on-policy policy gradient! <LINK>']",https://arxiv.org/abs/1912.02074,"In many real-world applications of reinforcement learning (RL), interactions with the environment are limited due to cost or feasibility. This presents a challenge to traditional RL algorithms since the max-return objective involves an expectation over on-policy samples. We introduce a new formulation of max-return optimization that allows the problem to be re-expressed by an expectation over an arbitrary behavior-agnostic and off-policy data distribution. We first derive this result by considering a regularized version of the dual max-return objective before extending our findings to unregularized objectives through the use of a Lagrangian formulation of the linear programming characterization of Q-values. We show that, if auxiliary dual variables of the objective are optimized, then the gradient of the off-policy objective is exactly the on-policy policy gradient, without any use of importance weighting. In addition to revealing the appealing theoretical properties of this approach, we also show that it delivers good practical performance. ",AlgaeDICE: Policy Gradient from Arbitrary Experience
88,1202311874173382656,2797706535,olga afanasjeva,"['Attending #neurips2019? Join ""Multi Agent Reinforcement Learning (MARL and related topics)"" meetup at 12/12/2019 7:30 PM through the Whova app. Place TBD. Brought by the people behind #NeurIPS2016 epic party😎😄 Also, check out our new paper: <LINK> <LINK>']",https://arxiv.org/abs/1912.01513,"In this work, we propose a novel memory-based multi-agent meta-learning architecture and learning procedure that allows for learning of a shared communication policy that enables the emergence of rapid adaptation to new and unseen environments by learning to learn learning algorithms through communication. Behavior, adaptation and learning to adapt emerges from the interactions of homogeneous experts inside a single agent. The proposed architecture should allow for generalization beyond the level seen in existing methods, in part due to the use of a single policy shared by all experts within the agent as well as the inherent modularity of 'Badger'. ","BADGER: Learning to (Learn [Learning Algorithms] through Multi-Agent
  Communication)"
89,1202270193080078336,922847904058011649,Romy Rodríguez,"['New paper from the KELT survey!!! Here we present a substellar companion and an ultra-hot Jupiter, both orbiting early A-stars which were observed by @NASA_TESS this year! \n<LINK>', '@NASA_TESS With zero-albedo equilibrium temperatures of ~2300K (KELT-25b) and ~2400 K (KELT-26b), both companions join are among the hottest exoplanets known', '@NASA_TESS These planets are also both highly inflated, making them excellent targets for follow-up detailed atmospheric characterization', '@NASA_TESS We see a subtle asymmetry in the light curve of KELT-26, which, combined with the observation of a polar orbit &amp; a low visini of the host star, leads us to think it is a signature of gravity darkening https://t.co/jGiAkhMo0R', '@NASA_TESS This effect likely not observable from the ground, which is why we only see it in the TESS light curve!']",https://arxiv.org/abs/1912.01017,"We present the discoveries of KELT-25b (TIC 65412605, TOI-626.01) and KELT-26b (TIC 160708862, TOI-1337.01), two transiting companions orbiting relatively bright, early A-stars. The transit signals were initially detected by the KELT survey, and subsequently confirmed by \textit{TESS} photometry. KELT-25b is on a 4.40-day orbit around the V = 9.66 star CD-24 5016 ($T_{\rm eff} = 8280^{+440}_{-180}$ K, $M_{\star}$ = $2.18^{+0.12}_{-0.11}$ $M_{\odot}$), while KELT-26b is on a 3.34-day orbit around the V = 9.95 star HD 134004 ($T_{\rm eff}$ =$8640^{+500}_{-240}$ K, $M_{\star}$ = $1.93^{+0.14}_{-0.16}$ $M_{\odot}$), which is likely an Am star. We have confirmed the sub-stellar nature of both companions through detailed characterization of each system using ground-based and \textit{TESS} photometry, radial velocity measurements, Doppler Tomography, and high-resolution imaging. For KELT-25, we determine a companion radius of $R_{\rm P}$ = $1.64^{+0.039}_{-0.043}$ $R_{\rm J}$, and a 3-sigma upper limit on the companion's mass of $\sim64~M_{\rm J}$. For KELT-26b, we infer a planetary mass and radius of $M_{\rm P}$ = $1.41^{+0.43}_{-0.51}$ $M_{\rm J}$ and $R_{\rm P}$ = $1.940^{+0.060}_{-0.058}$ $R_{\rm J}$. From Doppler Tomographic observations, we find KELT-26b to reside in a highly misaligned orbit. This conclusion is weakly corroborated by a subtle asymmetry in the transit light curve from the \textit{TESS} data. KELT-25b appears to be in a well-aligned, prograde orbit, and the system is likely a member of a cluster or moving group. ","KELT-25b and KELT-26b: A Hot Jupiter and a Substellar Companion
  Transiting Young A-stars Observed by TESS"
90,1202241655086010369,82859989,David Bamman,['We just published a new dataset for coreference resolution in English literature (as a new layer in LitBank) -- Paper: <LINK> Data: <LINK>'],https://arxiv.org/abs/1912.01140,"We present in this work a new dataset of coreference annotations for works of literature in English, covering 29,103 mentions in 210,532 tokens from 100 works of fiction. This dataset differs from previous coreference datasets in containing documents whose average length (2,105.3 words) is four times longer than other benchmark datasets (463.7 for OntoNotes), and contains examples of difficult coreference problems common in literature. This dataset allows for an evaluation of cross-domain performance for the task of coreference resolution, and analysis into the characteristics of long-distance within-document coreference. ",An Annotated Dataset of Coreference in English Literature
91,1202233945020157952,91420905,Alex Smith,"['New paper on arxiv today by César Hernández-Aguayo, featuring myself and @DarkerMatters  <LINK>']",https://arxiv.org/abs/1912.01099,"We investigate if, for a fixed number density of targets and redshift, there is an optimal way to select a galaxy sample in order to measure the baryon acoustic oscillation (BAO) scale, which is used as a standard ruler to constrain the cosmic expansion. Using the mock galaxy catalogue built by Smith et al. in the Millennium-XXL N-body simulation with a technique to assign galaxies to dark matter haloes based on halo occupation distribution modelling, we consider the clustering of galaxies selected by luminosity, colour and local density. We assess how well the BAO scale can be extracted by fitting a template to the power spectrum measured for each sample. We find that the BAO peak position is recovered equally well for samples defined by luminosity or colour, while there is a bias in the BAO scale recovered for samples defined by density. The BAO position is contracted to smaller scales for the densest galaxy quartile and expanded to large scales for the two least dense galaxy quartiles. For fixed galaxy number density, density-selected samples have higher uncertainties in the recovered BAO scale than luminosity- or colour-selected samples. ",Measuring the BAO peak position with different galaxy selections
92,1202178956063064064,806058672619212800,Guillaume Lample,"['Our new paper, Deep Learning for Symbolic Mathematics, is now on arXiv <LINK>\nWe added *a lot* of new results compared to the original submission. With @f_charton (1/7) <LINK>', 'Although neural networks struggle on simple arithmetic tasks such as addition and multiplication, we show that transformers perform surprisingly well on difficult mathematical problems such as function integration and differential equations. (2/7)', 'We define a general framework to adapt seq2seq models to various mathematical problems, and present different techniques to generate arbitrarily large datasets of functions with their integrals, and differential equations with their solutions. (3/7)', 'On samples of randomly generated functions, we show that transformers achieve state-of-the-art performance and outperform computer algebra systems such as Mathematica. (4/7)', 'We show that beam search can generate alternative solutions for a differential equation, all equivalent, but written in very different ways. The model was never trained to do this, but managed to figure out that different expressions correspond to the same mathematical object 5/7', 'We also observe that a transformer trained on functions that SymPy can integrate, is able at test time to integrate functions that SymPy is not able to integrate, i.e. the model was able to generalize beyond the set of functions integrable by SymPy. (6/7)', 'A purely neural approach is not sufficient, since it still requires a symbolic framework to check generated hypotheses. Yet, our models perform best on very long inputs, where computer algebra systems struggle. Symbolic computation may benefit from hybrid approaches. (7/7)', '@AndrewTouchet @f_charton Yes, we will open source our datasets and models soon!', '@ogrisel We used https://t.co/XvZQbwOpQj to visualize what is happening in the network, but we only tried very quickly and we did not observe anything concrete. Some papers like https://t.co/iGqnO5ArJO may be useful to get insights about the hidden layer activations / attention weights.', ""@leloykun @f_charton Not much. We ran experiments on 8 GPUs because it's faster, but even with 1 GPU you get most of the performance in a few hours.""]",https://arxiv.org/abs/1912.01412,"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica. ",Deep Learning for Symbolic Mathematics
93,1201941750690074627,3356991700,Guðmundur Stefánsson,"['🚨New paper ! 🚨Our paper on the validation on the nearby M-dwarf planet G 9-40b is now accepted for publication and available on the arXiv: <LINK> Highlights from the paper below.', 'G 9-40b is a sub-Neptune sized (2 Earth Radii) planet around the nearby high proper motion M2.5 dwarf G 9-40. Being close by (d~28pc) the host star is bright at NIR wavelengths (J=10, K=9.2).', 'Combining the brightness of the host star in the NIR with the large transit depth of the planet (~3500ppm), makes G 9-40b one of the most favorable sub-Neptune-sized M-dwarf planets for transmission spectroscopy with @JWSTObserver in the future (more in the thread below).', 'G 9-40b/EPIC 212048748 was first highlighted as a planet candidate in Campaign 16 data of the K2 Mission @KeplerGO by Yu+2018 (https://t.co/7PM02ayyhP). It looked interesting to us, so we decided to follow it up and characterize it better !', 'At a distance of only ~28 pc, G 9-40b is among the closest transiting planets known, and is currently the second closest transiting planets discovered by the K2 @KeplerGO mission to date. This plot compares the distance of G 9-40 to other nearby transiting systems. https://t.co/SS3ZxjUoUm', 'To constrain the planet parameters and further update the orbital ephemeris of the system, we used high-precision diffuser-assisted photometry using the ARCTIC imager on the 3.5m Telescope at @ApachePointObs. The plot below shows the K2 and our diffuser-assisted photometry. https://t.co/Z9McOh37vR', 'Using the diffuser, in 1min bins we achieve a precision of ~700ppm, and further bins down to ~138ppm in 30min bins. On other targets we have achieved diffuser-assisted precisions of ~60-100ppm in 30min bins, depending on the brightness of the target and reference stars. https://t.co/9nMt3Zs6ft', 'Our ground-based transit allows us to constrain the orbital ephemeris of the system, collapsing the transit midpoint errors from ~13minutes down to &lt;2minutes in the JWST era, enabling easy scheduling for in-transit spectroscopic follow-up in the future. https://t.co/9dutjIPuUv', 'What is diffuser-assisted photometry ? Engineered Diffusers are nano-fabricated devices that you can easily place in a telescope beam-path to mold the focal plane image of a star to a stabilized top-hat shape. The diffusers we use are from RPC Photonics: https://t.co/YeqxuPhEIc https://t.co/ueJfImWaDk', 'Engineered Diffusers allow you to keep the image (or the PSF of the star) stable throughout the night, minimizing seeing effects. The video below compares defocused and diffused PSFs, where the diffused PSF is stable throughout the night:\nhttps://t.co/EjXJq81Zym', 'In spreading out the light over many pixels opens up the possibility to observe bright targets (e.g., @NASA_TESS @TESSatMIT targets) on large telescopes with high duty cycles, minimizing scintillation errors, and maximizing your achievable ground-based photometric precision.', 'If you are interested in using Engineered Diffusers, don’t hesitate to reach out. We have also described their use in our previous paper here: https://t.co/wQkXAAqSnV We also have an online (and in Python!) sizing calculator here: https://t.co/rvuL7wVn46', 'In addition to the diffuser, we used a custom-built narrow-band filter made by Semrock / AVR Optics (https://t.co/iqENVxEtWR) that operates in a band in the red-optical (857nm center, 37nm width) with little-to-no water absorption. We often call this filter the ‘cloud-killer’. https://t.co/KB9f3Jsjkg', 'To further constrain any blends or false positive scenarios (e.g., eclipsing binaries or background eclipsing binaries), we used adaptive optics imaging from the ShaneAO system at @LickObservatory. We see no evidence of blending or other companions. https://t.co/i0ItnqDcqb', 'This paper includes some of the first precision near-infrared (NIR) radial velocities from the Habitable-zone Planet Finder (@HPFSpectrograph) #HPF. We recently installed HPF on the 10m Hobby-Eberly Telescope at @mcdonald \nhttps://t.co/vHMHrPRixa', 'In a previous paper led by AJ Metcalf (https://t.co/IWE8twhpKz ), we showed that HPF is capable of ~1.5m/s RV precision in the NIR enabled by the precise near-infrared HPF Laser Frequency Comb (LFC).', 'Using precision NIR RVs from HPF, we place an upper limit on the mass of G 9-40b of &lt;12 MEarth at 3 sigma. We hope to obtain more RVs with HPF and/or the new @NEID_at_WIYN spectrograph in the future, to enable a robust mass measurement. https://t.co/SQB5Rxp6Sj', 'As we do not have a firm mass measurement yet, we predict the mass of G 9-40b from its radius using two methods: the Forecaster package of Chen and Kipping 2017 and using the new neat non-parametric MRExo package by @shubhamkanodia Both methods predict masses of 4-5MEarth', 'The plot below shows the Forecaster posteriors, which predict a mass of ~5MEarth, and an RV amplitude of ~4m/s, making the mass imminently measurable with current and upcoming precision RV spectrographs. https://t.co/6gNXxogEI4', 'Using the predicted mass and the Transmission Spectroscopy Metric (TSM) by Kempton+2018, we show that G 9-40b has a high TSM~96. The plot below shows that G 9-40b is among the best small M-dwarf planets for transmission spectroscopic observations in the future. https://t.co/1GxuoV75sH', 'We urge further RV follow-up observations in the future to constrain the mass of G 9-40b, to enable precise transmission spectroscopic follow-up measurements in the future with JWST, ARIEL and the upcoming suite of extremely large telescopes.', 'We spun up a spectral matching library using as-observed HPF M-dwarf spectra. In doing so, we obtain an effective temperature of Teff=3404±73K, and metallicity of [Fe/H]=−0.08±0.13. This expanding library will enable us to obtain further precise M-dwarf parameters in future work https://t.co/hXmoY37vLQ', 'For the precision RV aficionados, we dive into details of the HPF drift correction. HPF shows a characteristic RV drift pattern on the order of ~10m/s daily, which has to do with the daily fill of the HPF LN2 tank. We describe our drift model devised by Joe Ninan @PSUScience https://t.co/kO7fTj8Xiy', 'We want to dearly thank the Resident Astronomers and Telescope Operators at HET @mcdonaldobs for expertly carrying out our queue HPF observations, and the support staff at @ApachePointObs and @LickObservatory for the help with our diffuser and high-contrast imaging observations.', 'This paper was a fantastic collaboration of a number of people, some of which are on Twitter: @HPFspectrograph at @PSUScience, @SuvrathM, @Astro_Wright, @shubhamkanodia, @wistwitski, @Interstellar_Em, @brettmor, @steinly0, @rcterrien - let me know if I missed anyone !']",https://arxiv.org/abs/1912.00291,"We validate the discovery of a 2 Earth radii sub-Neptune-size planet around the nearby high proper motion M2.5-dwarf G 9-40 (EPIC 212048748), using high-precision near-infrared (NIR) radial velocity (RV) observations with the Habitable-zone Planet Finder (HPF), precision diffuser-assisted ground-based photometry with a custom narrow-band photometric filter, and adaptive optics imaging. At a distance of $d=27.9\mathrm{pc}$, G 9-40b is the second closest transiting planet discovered by K2 to date. The planet's large transit depth ($\sim$3500ppm), combined with the proximity and brightness of the host star at NIR wavelengths (J=10, K=9.2) makes G 9-40b one of the most favorable sub-Neptune-sized planet orbiting an M-dwarf for transmission spectroscopy with JWST, ARIEL, and the upcoming Extremely Large Telescopes. The star is relatively inactive with a rotation period of $\sim$29 days determined from the K2 photometry. To estimate spectroscopic stellar parameters, we describe our implementation of an empirical spectral matching algorithm using the high-resolution NIR HPF spectra. Using this algorithm, we obtain an effective temperature of $T_{\mathrm{eff}}=3404\pm73$K, and metallicity of $\mathrm{[Fe/H]}=-0.08\pm0.13$. Our RVs, when coupled with the orbital parameters derived from the transit photometry, exclude planet masses above $11.7 M_\oplus$ with 99.7% confidence assuming a circular orbit. From its radius, we predict a mass of $M=5.0^{+3.8}_{-1.9} M_\oplus$ and an RV semi-amplitude of $K=4.1^{+3.1}_{-1.6}\mathrm{m\:s^{-1}}$, making its mass measurable with current RV facilities. We urge further RV follow-up observations to precisely measure its mass, to enable precise transmission spectroscopic measurements in the future. ","A sub-Neptune sized planet transiting the M2.5-dwarf G 9-40: Validation
  with the Habitable-zone Planet Finder"
94,1201908838732926976,4013975440,Cinjon Resnick,"['New paper from Joan, Zeping, and me - Probing the State of the Art: A Critical Look at Visual Representation Evaluation (<LINK>). Central thesis is that using linear probes to evaluate self-supervised models is insufficient to adjudicate progress. 1/6', 'We show that the ranking of models we attain when using linear probes for classification is different from the ranking we get when performing temporal activity localization (which needs a nonlinear transformation) on two datasets - Thumos14 and a new Gymnastics dataset. 2/6', 'The accepted ordering in the self-supervised literature is not conclusive, but nor is it here! Sometimes AMDIM is better than TimeCycle and vice versa. 3/6', ""We also showed that the self-supervised objective can create a representation space that isn't linearly separable, obviating this approach to discriminating models. We did this by finding one, TimeCycle, that did better when just initialized than after training. cc @xiaolonw 4/6"", 'What we actually care about in self-supervised learning is producing representation spaces that transfer well. Our experiments suggest that the self-supervised representations are just as good as the supervised ones at transferring to a much harder task on a new distribution. 5/6', 'Arguably, a better approach is to pose evaluation not in terms of linear or nonlinear tasks, but instead as a few-shot learning problem. Measure not with probes but with the necessary sample complexity to yield a successful transfer. 6/6 (cc @hugo_larochelle @giffmana)', 'Much obliged to @jamberto and @snakamori above all for making this even possible. Huge thanks to @curran922, @_willfalcon, @kchonyc, @wfwhitney, and Shubho Sengupta for contributions to the paper, and also to @douglas_eck, @ada_rob, @jesseengel, @iamandrewdai, and @Mo_Norouzi.']",https://arxiv.org/abs/1912.00215,"Self-supervised research improved greatly over the past half decade, with much of the growth being driven by objectives that are hard to quantitatively compare. These techniques include colorization, cyclical consistency, and noise-contrastive estimation from image patches. Consequently, the field has settled on a handful of measurements that depend on linear probes to adjudicate which approaches are the best. Our first contribution is to show that this test is insufficient and that models which perform poorly (strongly) on linear classification can perform strongly (weakly) on more involved tasks like temporal activity localization. Our second contribution is to analyze the capabilities of five different representations. And our third contribution is a much needed new dataset for temporal activity localization. ","Probing the State of the Art: A Critical Look at Visual Representation
  Evaluation"
95,1201853727059132416,987061319378587649,Maksym Andriushchenko 🇺🇦,"['Excited to share our new black-box attack based on simple *random search*! Despite its simplicity it outperforms the recent SOTA by several times in terms of query efficiency.\nThere are some interesting ideas behind\nPaper: <LINK>\nCode: <LINK>\n1/n <LINK>', 'We use the simplest form of random search from 1960s. The only thing we modify is the sampling distribution. We select it in a way so that each iterate always stays at the boundary of the feasible set. This simple idea significantly improves query efficiency. \n2/n https://t.co/lVQWsJkQCx', 'As a result, the Square Attack outperforms all existing methods by a large margin with a simple random search scheme. The attack achieves both best query efficiency (*2x - 7x better*, depending on the model) and success rate (also in the low-query regime) on ImageNet.\n3/n https://t.co/7MVi0QULBb', 'Square Attack is also useful for robustness evaluation of new defenses. There are cases (post-averaging, CLP, LSQ models) where it can significantly outperform even white-box PGD attack *with random restarts*. Thus, we recommend to use it to prevent false robustness claims.\n4/n https://t.co/ZCMbLlQiWT']",https://arxiv.org/abs/1912.00049,"We propose the Square Attack, a score-based black-box $l_2$- and $l_\infty$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least $1.8$ and up to $3$ compared to the recent state-of-the-art $l_\infty$-attack of Al-Dujaili & O'Reilly. Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at this https URL ","Square Attack: a query-efficient black-box adversarial attack via random
  search"
96,1201816012464803840,27700085,Luis G Natera Orozco,"['We (@Orsi_Vasarhelyi Anna Vancsó and @DavidDeritei) have a new paper! “Quantifying Life Quality as Walkability on Urban Networks: The Case of Budapest” Available here: <LINK> and a PrePrint in ArXiv: <LINK> A thread about the work: <LINK>', '@Orsi_Vasarhelyi @DavidDeritei In this paper we develop a data-driven, network-based method to quantify the liveability of a city at the level of street intersections. based on pedestrian accessibility to amenities and services, safety and environmental variables. https://t.co/fF6JC2De0B', '@Orsi_Vasarhelyi @DavidDeritei We worked with data from @OpenStreetMap downloaded with #OSMnx, we classified the amenities and points of interest in six categories, providing a category based and overall life quality measure for each intersection of the city. https://t.co/Xai0nh8rOR', '@Orsi_Vasarhelyi @DavidDeritei @openstreetmap A data-driven approach for quantifying life quality at such a granular level like our proposed method can help decision-makers to tackle social and environmental challenges better. https://t.co/3NKilni5Bu', '@Orsi_Vasarhelyi @DavidDeritei @openstreetmap Read all the details: Paper (https://t.co/gMG250kdFA) PrePrint (https://t.co/OUfk3mubEu) and Blog post (https://t.co/Khz4bMyuXu) https://t.co/ssPqg1nxCE', 'Dears @ebkent @gboeing @TrivikV  @luyibov @smomara1 I think our latest paper might be of your interest. We measure walking accessibility to amenities and services to quantify the livability of a city. \nHope it is an interesting read :) https://t.co/4cEcG80YiT', '@TrivikV Sure, see you in Lisbon next week']",https://arxiv.org/abs/1912.00893,"Life quality in cities is deeply related to the mobility options, and how easily one can access different services and attractions. The pedestrian infrastructure network provides the backbone for social life in cities. While there are many approaches to quantify life quality, most do not take specifically into account the walkability of the city, and rather offer a city-wide measure. Here we develop a data-driven, network-based method to quantify the liveability of a city. We introduce a life quality index (LQI) based on pedestrian accessibility to amenities and services, safety and environmental variables. Our computational approach outlines novel ways to measure life quality in a more granular scale, that can become valuable for urban planners, city officials and stakeholders. We apply data-driven methods to Budapest, but as having an emphasis on the online and easily available quantitative data, the methods can be generalized and applied to any city. ","Quantifying Life Quality as Walkability on Urban Networks: The Case of
  Budapest"
97,1201709300277633024,369569444,Takahiro TERADA (寺田 隆広),"['Our new paper: ""Gauge Invariance of Induced Gravitational Waves""\n<LINK>\nGauge dependence is shown to vanish in the late-time limit. <LINK>']",http://arxiv.org/abs/1912.00785,"We study gauge (in)dependence of the gravitational waves (GWs) induced from curvature perturbations. For the GWs produced in a radiation-dominated era, we find that the observable (late-time) GWs in the TT gauge and in the Newtonian gauge are the same in contrast to a claim in the literature. We also mention the interpretation of the gauge dependence of the tensor perturbations which appears in the context of the induced GWs. ",Gauge Independence of Induced Gravitational Waves
98,1215192815900037122,713389493076758528,Xavier Bresson,"['New paper on graph/sparse Transformer applied to sketch recognition. \n\nInterestingly, a standard Transformer applied to sketch points does not work well. But a Transformer on graphs of sketch points perform quite well. \n\npaper <LINK>\ncode <LINK> <LINK> <LINK>', 'A next step would be to generate sketches using graph Transformers instead of RNNs. See the nice blog post of @hardmaru about the sketch generative task.\nhttps://t.co/9ORZaMgsWi']",https://arxiv.org/abs/1912.11258,"Learning meaningful representations of free-hand sketches remains a challenging task given the signal sparsity and the high-level abstraction of sketches. Existing techniques have focused on exploiting either the static nature of sketches with Convolutional Neural Networks (CNNs) or the temporal sequential property with Recurrent Neural Networks (RNNs). In this work, we propose a new representation of sketches as multiple sparsely connected graphs. We design a novel Graph Neural Network (GNN), the Multi-Graph Transformer (MGT), for learning representations of sketches from multiple graphs which simultaneously capture global and local geometric stroke structures, as well as temporal information. We report extensive numerical experiments on a sketch recognition task to demonstrate the performance of the proposed approach. Particularly, MGT applied on 414k sketches from Google QuickDraw: (i) achieves small recognition gap to the CNN-based performance upper bound (72.80% vs. 74.22%), and (ii) outperforms all RNN-based models by a significant margin. To the best of our knowledge, this is the first work proposing to represent sketches as graphs and apply GNNs for sketch recognition. Code and trained models are available at this https URL ",Multi-Graph Transformer for Free-Hand Sketch Recognition
99,1212022041462923266,371963327,Francesco Sanna Passino,"[""New paper out on arXiv, in collaboration with Anna Bertiger and Joshua Neil from Microsoft MDATP, and my supervisor at Imperial College, Nick Heard: 'Link prediction in dynamic networks using random dot product graphs'. Have a look! <LINK>""]",https://arxiv.org/abs/1912.10419,"The problem of predicting links in large networks is an important task in a variety of practical applications, including social sciences, biology and computer security. In this paper, statistical techniques for link prediction based on the popular random dot product graph model are carefully presented, analysed and extended to dynamic settings. Motivated by a practical application in cyber-security, this paper demonstrates that random dot product graphs not only represent a powerful tool for inferring differences between multiple networks, but are also efficient for prediction purposes and for understanding the temporal evolution of the network. The probabilities of links are obtained by fusing information at two stages: spectral methods provide estimates of latent positions for each node, and time series models are used to capture temporal dynamics. In this way, traditional link prediction methods, usually based on decompositions of the entire network adjacency matrix, are extended using temporal information. The methods presented in this article are applied to a number of simulated and real-world graphs, showing promising results. ",Link prediction in dynamic networks using random dot product graphs
100,1212020206412808193,954465907539152897,Dr. Doctor,"['To ring in the new year, here’s my first single-author paper!\n<LINK>\n\nLong story short, it’s about a method for measuring the parameters that describe our universe using gravitational waves and light associated with those waves. More below!', 'One major aim of physical cosmology is to understand the relation between the distances and redshifts of far-away objects which tells us how the universe is and has been expanding. A classic way of doing this is to use “standard candles”.', 'A standard candle has a known *intrinsic* brightness, so if we observe one, we can tell how far away it is. Just  like a real candle! If it appears bright, it’s close, if it appears dim, it’s far. Type Ia supernovae are the canonical standard candles used in cosmology.', 'Mergers of black holes and/or neutron stars have a standard “brightness” in gravitational waves, so we can use them as “standard sirens” to measure distances. The standard siren method was used for the first time in 2017 after the detection of GW170817.', 'One cool thing about neutron star mergers is that they potentially also have a standard brightness in the electromagnetic spectrum. So in principle they could be used as standard sirens AND standard candles, as a couple of teams pointed out earlier this year.', 'In my article, I flesh out the statistical approach for using neutron star mergers as simultaneous standard candles and standard sirens, while accounting for selection effects. I also enumerate the potential pitfalls of using such a method.', 'The most important take-away from the article is that systematic errors can plague inferences of cosmological parameters. You’ll never get a believable precise measurement of cosmology if there are large systematics.', 'The rub is that you have to model the electromagnetic signal from the neutron stars merger, which involves nuclear matter in strong gravitational and EM fields. All 4 forces (strong, weak, EM, and gravity) are at work. It’s VERY hard to model this to the precision needed here.', 'You don’t quite have this modeling issue on the gravitational wave side because gravitational wave emission of two inspiraling bodies is straightforward in comparison (except during the merger).', 'Another important subtlety is that the models used for the EM signal can’t have been tuned on previous studies which assumed a specific cosmology. That would be circular logic!', 'So, I show how you can do these types of standard candle + standard siren measurements in a fully Bayesian manner, but also offer some words of caution: systematics, selection effects, and independence of analyses are crucial! Happy holidays 🙃', '@TomD_Santiago You still need host galaxy redshift! But now instead of just having the GW likelihood of distance, you also have EM likelihood of distance based on observed luminosity.']",https://arxiv.org/abs/1912.12218,"With the detection of gravitational wave (GW) GW170817 and its associated electromagnetic (EM) counterparts from a binary neutron star (NS) merger, the ""standard siren"" method for Hubble-constant measurements is expected to play a role in the Hubble-constant tension in the next few years. One intriguing proposal put forward in multiple studies is to use an NS merger's optical counterpart, known as a kilonova, as a standard candle, because its absolute magnitude can in principle be calculated from simulations. In this work, I detail the statistical framework for performing joint standard-candle and standard-siren measurements using GWs, EM follow-up data, and simulations of EM counterparts. I then perform an example analysis using GW170817 and its optical counterpart AT2017gfo to illustrate the method and the method's limitations. Crucially, the inferences using this method are only as robust as the EM counterpart models, so significant theoretical advances are needed before this method can be employed for precision cosmology. ","Thunder and Lightning: Using Neutron-Star Mergers as Simultaneous
  Standard Candles and Sirens to Measure Cosmological Parameters"
101,1208124264198492160,65915718,Hamed Zamani,"['1/12 Let me introduce you to Macaw, the new member of the ""IR zoo""! Macaw is a platform for conversational information seeking research. See the thread👇\n\nPaper (with @nick_craswell): <LINK>\n\nCode and instructions: <LINK>\n\n#convsearch #sigir2020', '2/12 Macaw supports multiple interfaces: stdio for development; fileio for batch experimentation. It also supports interfaces for interacting with users. Thanks to Telegram bots, it can be used on different devices (e.g., desktop &amp; mobile) and OSs (andriod, iOS, linux &amp; windows).', '3/12 Here is a couple of screenshots from the interactions with Macaw using the Telegram interface: https://t.co/3NWT11gXiX', '4/12 As shown in the screenshots, Macaw supports multi-modal interactions, including text and speech. It also handles speech-only interactions. \n\nUsers can also click on the options provided by the system.', '5/12 Macaw supports multi-turn interactions. It handles co-reference resolution and query generation from a conversation.', ""6/12 Macaw handles mixed-initiative interactions. In more detail, Macaw can send a message to the user at any time and it doesn't have to be a response to a user request."", '7/12 Macaw has a modular architecture, which makes it easily extensible for supporting different conversational information seeking tasks (conversational search, QA, recommendation, natural language interface to structured data). \n\nBTW, Macaw is implemented in Python! https://t.co/RgzwmexvEZ', '8/12 Are you an IR researcher who cares about retrieval in conversational systems? \n\nDo you want to generate user responses based on your favorite text collection? \n\nMacaw supports Indri as the back end search engine for you!', ""9/12 Are you interested in result presentation in conversational systems and don't want to be involved in the retrieval part?\n\nDo you want to generate user responses from the Web?\n\nMacaw uses the Bing Web Search API as the back end search engine for you!"", '10/12 Are you interested in user studies? Do you explore human-human interactions in information seeking conversations? Do you want to collect more realistic data for conversational tasks?\n\nMacaw supports intermediary-based or wizard of oz setups &amp; logs all interactions for you!', '11/12 and many more, including machine reading comprehension for answer selection / generation!\n\nLearn more about Macaw: https://t.co/3NLGbrI5z3\n\nWe would love to hear your feedback!', '12/12 Why is it called Macaw? \n\nBecause macaw is an animal (IR folks should know the rational)! and is a talking bird! The naming kind of shows the expectations you should have from such systems!']",https://arxiv.org/abs/1912.08904,"Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License. ",Macaw: An Extensible Conversational Information Seeking Platform
102,1207761729007915008,2368348172,Pavlo Molchanov,"['Turning discriminative model into generative via optimization, our new paper at #NvidiaAI called #DeepInversion: <LINK> . Work with H Yin, Z Li, @ALVAREZ_JOSEM, @arunmallya, D Hoiem, N Jha, @jankautz.\nImages inverted from ResNet50 trained for classification: <LINK>', '@ALVAREZ_JOSEM @arunmallya @jankautz We build on top of #DeepDream by extending it with the loss to constrain intermediate representations with pre-stored statistics from batch-norm.\nWith #DeepInversion we can invert the network and train a fresh ResNet50 to 73.8% on ImageNet with no real data. Diagram: https://t.co/ehAy9MPLqy']",https://arxiv.org/abs/1912.08795,"We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning. Code is available at this https URL ",Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion
103,1206964719720030208,26001948,Aaron Strauss,"[""Fascinating new paper on the FB ad algorithm -- I knew that the algo skewed toward partisans, but I DIDN'T know that FB compared new ads to old ads to begin that skew from the very first dollar spent: <LINK> (h/t @jon_m_rob) <LINK>"", ""@jon_m_rob This must be part of the reason why FB works so well for fundraising/acquisition, but creates a steeper hill for ads that try to change people's behavior. https://t.co/V1URuud10j"", '@jon_m_rob Also, it seems so obvious in retrospect that FB would juice the algorithm from the beginning of a buy, but I guess I never really thought too hard about it before. Very clever, money-making idea for FB from whoever thought of it initially.', '@petealbrecht @jon_m_rob I think this has nothing to do with politics and is applied generally']",https://arxiv.org/abs/1912.04255,"Political campaigns are increasingly turning to digital advertising to reach voters. These platforms empower advertisers to target messages to platform users with great precision, including through inferences about those users' political affiliations. However, prior work has shown that platforms' ad delivery algorithms can selectively deliver ads within these target audiences in ways that can lead to demographic skews along race and gender lines, often without an advertiser's knowledge. In this study, we investigate the impact of Facebook's ad delivery algorithms on political ads. We run a series of political ads on Facebook and measure how Facebook delivers those ads to different groups, depending on an ad's content (e.g., the political viewpoint featured) and targeting criteria. We find that Facebook's ad delivery algorithms effectively differentiate the price of reaching a user based on their inferred political alignment with the advertised content, inhibiting political campaigns' ability to reach voters with diverse political views. This effect is most acute when advertisers use small budgets, as Facebook's delivery algorithm tends to preferentially deliver to the users who are, according to Facebook's estimation, most relevant. Our findings point to advertising platforms' potential role in political polarization and creating informational filter bubbles. Furthermore, some large ad platforms have recently changed their policies to restrict the targeting tools they offer to political campaigns; our findings show that such reforms will be insufficient if the goal is to ensure that political ads are shown to users of diverse political views. Our findings add urgency to calls for more meaningful public transparency into the political advertising ecosystem. ",Ad Delivery Algorithms: The Hidden Arbiters of Political Messaging
104,1206782610380333056,2521363488,Ray Sharma,"[""I'm excited to share a new paper that I've been working on! It uses data from the Romulus25 cosmological simulation to study how black holes grow (or don't grow) in dwarf galaxies. Find it here: <LINK>, or keep reading to find out more!"", 'Dwarf galaxies in Rom25 exhibit a diversity of black hole masses, so my goal was to understand why these BHs sometimes grow a lot, yet other times grow very little.  A natural extension was to understand how these BHs may affect the evolution of dwarf galaxies.', 'We split our sample of dwarf galaxies with stellar masses &lt; 10^10 Msun by whether they host a BH, and whether the BH is ""overmassive"" or ""undermassive"" relative to the median. An exciting thing to note is BHs form based on local gas properties in Rom25 -- not all dwarfs have BHs!', 'Rom25 has a z=0 BH mass - stellar mass relation that extends down to stellar masses 10^8 Msun, but only shows agreement with observations of early-type galaxies. Below  stellar mass 10^10, there is a lot of scatter in BH masses. https://t.co/ZBl2vHmrPj', 'If we follow the evolution onto the relation, it turns out hosts of undermassive BHs build up their stars first, and BHs second. On the other hand, median and overmassive BHs grow in tandem with their host galaxies. https://t.co/UGI1dYC62Z', 'Turns out the hosts also grow quite differently, where dwarf galaxies that host overmassive BHs first formed (and stopped growing) much earlier. Galaxies that formed late are far less likely to even form a BH. To use a common phrase, the early dwarf gets the big BH. https://t.co/V4qe2DIO1Z', 'Finally, the dwarfs that have overmassive BHs also show evidence of huge amounts of energy injected by the BH,  gas depletion in the host, and star formation suppression. There are extreme cases where the galaxies seem to quench completely because of the overmassive BH!', 'Future work will look at some of these galaxies in higher resolution to really understand how some of the least massive BHs observed to date may grow. Keep an eye out!']",https://arxiv.org/abs/1912.06646,"We investigate the effects of massive black hole growth on the structural evolution of dwarf galaxies within the Romulus25 cosmological hydrodynamical simulation. We study a sample of 228 central, isolated dwarf galaxies with stellar masses $M_{star} < 10^{10} M_\odot$ and a central BH. We find that the local $M_{BH} - M_{star}$ relation exhibits a high degree of scatter below $M_{star} < 10^{10} M_\odot$, which we use to classify BHs as overmassive or undermassive relative to their host $M_{star}$. Overmassive BHs grow through a mixture of BH mergers and relatively high average accretion rates, while undermassive BHs grow slowly through accretion. We find that isolated dwarf galaxies that host overmassive BHs also follow different evolutionary tracks relative to their undermassive BH counterparts, building up their stars and dark matter earlier and experiencing star formation suppression starting around $z=2$. By $z=0.05$, overmassive BH hosts above $M_{star} > 10^{9} M_\odot$ are more likely to exhibit lower central stellar mass density, lower HI gas content, and lower star formation rates than their undermassive BH counterparts. Our results suggest that overmassive BHs in isolated galaxies above $M_{star} > 10^{9} M_\odot$ are capable of driving feedback, in many cases suppressing and even quenching star formation by late times. ",Black Hole Growth and Feedback in Isolated Romulus25 Dwarf Galaxies
105,1205310172291166209,276494400,Prateek Mittal,"['New paper on ""Advances and Open Problems in Federated Learning"", together with 58 co-authors from 25 institutions: <LINK>', 'Thanks to @KairouzPeter and Brendan McMahan @Google for leading this effort!']",https://arxiv.org/abs/1912.04977,"Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges. ",Advances and Open Problems in Federated Learning
106,1203079832747118592,349172730,Ranjay Krishna,"['New paper out! Typical active learning algorithms assume there is only one correct answer, which is not true for many tasks, like question answering. Our new uncertainty measurement is 5x more data-efficient even when there are multiple correct answers. <LINK> <LINK>', 'Collaborators: my advisors (@drfeifei and @msb) and my amazing undergrad student (@kjedoui).']",https://arxiv.org/abs/1912.01119,"Typical active learning strategies are designed for tasks, such as classification, with the assumption that the output space is mutually exclusive. The assumption that these tasks always have exactly one correct answer has resulted in the creation of numerous uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. Unfortunately, many real-world vision tasks, like visual question answering and image captioning, have multiple correct answers, causing these measurements to overestimate uncertainty and sometimes perform worse than a random sampling baseline. In this paper, we propose a new paradigm that estimates uncertainty in the model's internal hidden space instead of the model's output space. We specifically study a manifestation of this problem for visual question answer generation (VQA), where the aim is not to classify the correct answer but to produce a natural language answer, given an image and a question. Our method overcomes the paraphrastic nature of language. It requires a semantic space that structures the model's output concepts and that enables the usage of techniques like dropout-based Bayesian uncertainty. We build a visual-semantic space that embeds paraphrases close together for any existing VQA model. We empirically show state-of-art active learning results on the task of VQA on two datasets, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0. ",Deep Bayesian Active Learning for Multiple Correct Outputs
107,1202907675652186112,955370246,Krzakala Florent,"['What is the link between the theoretical work on neural nets in physics in the 90s, Statistical learning theories in the 80s and over-parametrization the 2010s? Find out in our new paper ""Rademacher complexity and Spin Glasses"" <LINK>']",https://arxiv.org/abs/1912.02729,"Statistical learning theory provides bounds of the generalization gap, using in particular the Vapnik-Chervonenkis dimension and the Rademacher complexity. An alternative approach, mainly studied in the statistical physics literature, is the study of generalization in simple synthetic-data models. Here we discuss the connections between these approaches and focus on the link between the Rademacher complexity in statistical learning and the theories of generalization for typical-case synthetic models from statistical physics, involving quantities known as Gardner capacity and ground state energy. We show that in these models the Rademacher complexity is closely related to the ground state energy computed by replica theories. Using this connection, one may reinterpret many results of the literature as rigorous Rademacher bounds in a variety of models in the high-dimensional statistics limit. Somewhat surprisingly, we also show that statistical learning theory provides predictions for the behavior of the ground-state energies in some full replica symmetry breaking models. ","Rademacher complexity and spin glasses: A link between the replica and
  statistical theories of learning"
108,1202867946651033600,883039700,Lenka Zdeborova,['Did you know that the Rademacher complexity is just the ground state energy of the perceptron model? If you want to know more check out our new paper: <LINK> with @KrzakalaF'],https://arxiv.org/abs/1912.02729,"Statistical learning theory provides bounds of the generalization gap, using in particular the Vapnik-Chervonenkis dimension and the Rademacher complexity. An alternative approach, mainly studied in the statistical physics literature, is the study of generalization in simple synthetic-data models. Here we discuss the connections between these approaches and focus on the link between the Rademacher complexity in statistical learning and the theories of generalization for typical-case synthetic models from statistical physics, involving quantities known as Gardner capacity and ground state energy. We show that in these models the Rademacher complexity is closely related to the ground state energy computed by replica theories. Using this connection, one may reinterpret many results of the literature as rigorous Rademacher bounds in a variety of models in the high-dimensional statistics limit. Somewhat surprisingly, we also show that statistical learning theory provides predictions for the behavior of the ground-state energies in some full replica symmetry breaking models. ","Rademacher complexity and spin glasses: A link between the replica and
  statistical theories of learning"
109,1202088914078572544,1147039217534537728,Rohan Chandra,"['(1/2) New paper on arXiv: <LINK>\n\nWe propose a trajectory prediction algorithm for autonomous driving. We use spectral graph theory to reduce long term RMSE and identify/predict behavior (over-speeding etc).\n\nCode,Video, Datasets: <LINK>', '(2/2) More research on autonomous driving from our group found here: https://t.co/hpBOMaIy0c']",https://arxiv.org/abs/1912.01118,"We present a novel approach for traffic forecasting in urban traffic scenarios using a combination of spectral graph analysis and deep learning. We predict both the low-level information (future trajectories) as well as the high-level information (road-agent behavior) from the extracted trajectory of each road-agent. Our formulation represents the proximity between the road agents using a weighted dynamic geometric graph (DGG). We use a two-stream graph-LSTM network to perform traffic forecasting using these weighted DGGs. The first stream predicts the spatial coordinates of road-agents, while the second stream predicts whether a road-agent is going to exhibit overspeeding, underspeeding, or neutral behavior by modeling spatial interactions between road-agents. Additionally, we propose a new regularization algorithm based on spectral clustering to reduce the error margin in long-term prediction (3-5 seconds) and improve the accuracy of the predicted trajectories. Moreover, we prove a theoretical upper bound on the regularized prediction error. We evaluate our approach on the Argoverse, Lyft, Apolloscape, and NGSIM datasets and highlight the benefits over prior trajectory prediction methods. In practice, our approach reduces the average prediction error by approximately 75% over prior algorithms and achieves a weighted average accuracy of 91.2% for behavior prediction. Additionally, our spectral regularization improves long-term prediction by up to 70%. ","Forecasting Trajectory and Behavior of Road-Agents Using Spectral
  Clustering in Graph-LSTMs"
110,1201846587653337088,41117987,Dr Michelle Collins,"[""It's paper day! My former student @alexlgregory, and our colleagues (incl. @deniserkal, @astro_md, @eteq, @caprastro) have been studying the Hercules dwarf spheroidal galaxy with a combination of new Keck data, and Gaia proper motions! Read more here:  <LINK>""]",https://arxiv.org/abs/1912.00156,"We present new chemo--kinematics of the Hercules dwarf galaxy based on Keck II-- DEIMOS spectroscopy. Our 21 confirmed members have a systemic velocity of $v_{\mathrm{Herc}}=46.4\pm1.3$ kms$^{-1}$ and a velocity dispersion $\sigma_{v,\mathrm{Herc}}=4.4^{+1.4}_{-1.2}$ kms$^{-1}$. From the strength of the Ca II triplet, we obtain a metallicity of [Fe/H]= $-2.48\pm0.19$ dex and dispersion of $\sigma_{\rm{[Fe/H]}}= 0.63^{+0.18}_{-0.13}$ dex. This makes Hercules a particularly metal--poor galaxy, placing it slightly below the standard mass--metallicity relation. Previous photometric and spectroscopic evidence suggests that Hercules is tidally disrupting and may be on a highly radial orbit. From our identified members, we measure no significant velocity gradient. By cross--matching with the second \textit{Gaia} data release, we determine an uncertainty--weighted mean proper motion of $\mu_{\alpha}^*=\mu_{\alpha}\cos(\delta)=-0.153\pm{0.074}$ mas yr$^{-1}$, $\mu_{\delta}=-0.397\pm0.063$ mas yr$^{-1}$. This proper motion is slightly misaligned with the elongation of Hercules, in contrast to models which suggest that any tidal debris should be well aligned with the orbital path. Future observations may resolve this tension. ",Uncovering the Orbit of the Hercules Dwarf Galaxy
111,1201825657711407105,738769492122214400,Johannes Lischner,['Very excited about our new paper where we present a quantum-mechanical method to calculate the energies and wavefunctions of d-band electrons in large silver #nanoparticles.  Read here: <LINK> #nanotechnology #plasmonics #compchem'],https://arxiv.org/abs/1912.00460,"We present an approach to master the well-known challenge of calculating the contribution of d-bands to plasmon-induced hot carrier rates in metallic nanoparticles. We generalise the widely used spherical well model for the nanoparticle wavefunctions to flat d-bands using the envelope function technique. Using Fermi's golden rule, we calculate the generation rates of hot carriers after the decay of the plasmon due to transitions from either a d-band state to an sp-band state or from an sp-band state to another sp-band state. We apply this formalism to spherical silver nanoparticles with radii up to 20~nm and also study the dependence of hot carrier rates on the energy of the d-bands. We find that for nanoparticles with a radius less than 2.5~nm sp-band state to sp-band state transitions dominate hot carrier production while d-band state to sp-band state transitions give the largest contribution for larger nanoparticles. ","Generation of plasmonic hot carriers from d-bands in metallic
  nanoparticles"
112,1214550962884546561,3387297904,Belle II Experiment,"[""Meet our first physics paper! (<LINK>) In this study, we set exclusion limits for a hypothetical Z' particle that could be a mediator to the #DarkMatter Universe. #Belle2 #Physics <LINK>""]",https://arxiv.org/abs/1912.11276,"Theories beyond the standard model often predict the existence of an additional neutral boson, the $Z^{\prime}$. Using data collected by the Belle II experiment during 2018 at the SuperKEKB collider, we perform the first searches for the invisible decay of a $Z^{\prime}$ in the process $e^+ e^- \to \mu^+ \mu^- Z^{\prime}$ and of a lepton-flavor-violating $Z^{\prime}$ in $e^+ e^- \to e^{\pm} \mu^{\mp} Z^{\prime}$. We do not find any excess of events and set 90\% credibility level upper limits on the cross sections of these processes. We translate the former, in the framework of an $L_{\mu}-L_{\tau}$ theory, into upper limits on the $Z^{\prime}$ coupling constant at the level of $5 \times 10^{-2}$ -- $1$ $M_{Z^\prime}\leq 6$ GeV/$c^2$. ","Search for an invisibly decaying $Z^{\prime}$ boson at Belle II in $e^+
  e^- \to \mu^+ \mu^- (e^{\pm} \mu^{\mp})$ plus missing energy final states"
113,1212304062819053569,760022547895377920,Federico Errica,"[""Happy New Year with our #Tutorial on #DeepLearning for #Graphs we just released on arXiv! Any feedback would be very appreciated!\n\nSpoiler: let's stop calling everything a #GNN. We instead propose the term Deep Graph Networks, (DGNs).\n<LINK>"", 'With D. Bacciu, A. Micheli and @poddaccio!']",https://arxiv.org/abs/1912.12693,"The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field. ",A Gentle Introduction to Deep Learning for Graphs
114,1210743158184693760,3096775849,Mark Ledwich 🌐,"['1. I worked with Anna Zaitsev (Berkely postdoc) to study YouTube recommendation radicalization. We painstakingly collected and grouped channels (768) and recommendations (23M) and found that the algo has a deradicalizing influence.\n\nPre-print:\n<LINK>\n🧵', '2. It turns out the late 2019 algorithm \n*DESTROYS* conspiracy theorists, provocateurs and white identitarians\n*Helps* partisans\n*Hurts* almost everyone else.\n\n👇 compares an estimate of the recommendations presented (grey) to received (green) for each of the groups: https://t.co/5qPtyi5ZIP', '3. Check out https://t.co/w5YciDywBi to have a play with this new dataset. We also include categorization from @manoelribeiro et a.l. and other studies so you can see some alternative groupings.\n\nAll of the code and data is free to review to use https://t.co/vsLVK7wYt4 https://t.co/s17z9kr5lA', '4. My new article explains in detail.  It takes aim at the NYT (in particular, @kevinroose)  who have been on myth-filled crusade vs social media. We should start questioning the authoritative status of outlets that have soiled themselves with agendas.\n\nhttps://t.co/bt3mMscJi6', '6. Thanks 🙏 to everyone that helped: @LTF_01 @sudonhim @jmrphy @Waldo000000  and our anonymous third channel reviewer. \n\nEND', ""@elliottmcollins These are good points. I think it's a mixed bag of news. And it doesn't say anything about YT's influence outside of recommendations."", ""@ludwig_raal Thanks. I'm glad it's getting out there."", '@shriphani There is manual effort to identify and categorize political channels for each country. But apart from that, it would be simple to allow filtering to different countries.', ""@kylefbutts I use about 10 different us west/central datacenter IP's per day and fall back to US residential proxies when those are blocked."", ""@Musicalmindz @kylefbutts I'm using https://t.co/rARx8PKWY6""]",https://arxiv.org/abs/1912.11211,"The role that YouTube and its behind-the-scenes recommendation algorithm plays in encouraging online radicalization has been suggested by both journalists and academics alike. This study directly quantifies these claims by examining the role that YouTube's algorithm plays in suggesting radicalized content. After categorizing nearly 800 political channels, we were able to differentiate between political schemas in order to analyze the algorithm traffic flows out and between each group. After conducting a detailed analysis of recommendations received by each channel type, we refute the popular radicalization claims. To the contrary, these data suggest that YouTube's recommendation algorithm actively discourages viewers from visiting radicalizing or extremist content. Instead, the algorithm is shown to favor mainstream media and cable news content over independent YouTube channels with slant towards left-leaning or politically neutral channels. Our study thus suggests that YouTube's recommendation algorithm fails to promote inflammatory or radicalized content, as previously claimed by several outlets. ",Algorithmic Extremism: Examining YouTube's Rabbit Hole of Radicalization
115,1209288887400570880,312448486,Dr. Karan Jani,"['NEW PAPER - we revisit an intermediate mass black hole trigger in @LIGO.\n\nWe find it to be consistent with binary black hole merger of 150 SOLAR MASSES. \n\nHeard right - BLACK HOLES that are one-hundred-fifty times heavier than Sun ! \n\nBeat that 2019🖖🏻\n\n<LINK> <LINK>', '@alexandernitz @LIGO Indeed we focused on the new PE machinery. This trigger was very marginal for the matched filtering searches. Was loud (significant) only for the Burst search. Not surprising though 🙌🏻']",https://arxiv.org/abs/1912.10533,"Gravitational wave (GW) measurements provide the most robust constraints of the mass of astrophysical black holes. Using state-of-the-art GW signal models and a unique parameter estimation technique, we infer the source parameters of the loudest marginal trigger, GW170502, found by LIGO from 2015 to 2017. If this trigger is assumed to be a binary black hole merger, we find it corresponds to a total mass in the source frame of $157^{+55}_{-41}~\rm{M}_\odot$ at redshift $z=1.37^{+0.93}_{-0.64}$. The primary and secondary black hole masses are constrained to $94^{+44}_{-28}~\rm{M}_{\odot}$ and $62^{+30}_{-25}~\rm{M}_{\odot}$ respectively, with 90\% confidence. Across all signal models, we find $\gtrsim 70\%$ probability for the effective spin parameter $\chi_\mathrm{eff}>0.1$. Furthermore, we find that the inclusion of higher-order modes in the analysis narrows the confidence region for the primary black hole mass by 10\%, however, the evidence for these modes in the data remains negligible. The techniques outlined in this study could lead to robust inference of the physical parameters for all intermediate-mass black hole binary candidates $(\gtrsim100~\mathrm{M}_\odot)$ in the current GW network. ","Inferring Parameters of GW170502: The Loudest Intermediate-mass Black
  Hole Trigger in LIGO's O1/O2 data"
116,1208053979881467905,125494187,Faisal Mahmood,"['Our work on Pathomic Fusion is now on arXiv, <LINK> \nTL;DR: We propose a scalable method for integrating histology and -omic data using attention gating and tensor fusion for prognosis, treatment response.. prediction #pathology #pathomics #ComputationalPathology <LINK>']",https://arxiv.org/abs/1912.08937,"Cancer diagnosis, prognosis, and therapeutic response predictions are based on morphological information from histology slides and molecular profiles from genomic data. However, most deep learning-based objective outcome prediction and grading paradigms are based on histology or genomics alone and do not make use of the complementary information in an intuitive manner. In this work, we propose Pathomic Fusion, an interpretable strategy for end-to-end multimodal fusion of histology image and genomic (mutations, CNV, RNA-Seq) features for survival outcome prediction. Our approach models pairwise feature interactions across modalities by taking the Kronecker product of unimodal feature representations and controls the expressiveness of each representation via a gating-based attention mechanism. Following supervised learning, we are able to interpret and saliently localize features across each modality, and understand how feature importance shifts when conditioning on multimodal input. We validate our approach using glioma and clear cell renal cell carcinoma datasets from the Cancer Genome Atlas (TCGA), which contains paired whole-slide image, genotype, and transcriptome data with ground truth survival and histologic grade labels. In a 15-fold cross-validation, our results demonstrate that the proposed multimodal fusion paradigm improves prognostic determinations from ground truth grading and molecular subtyping, as well as unimodal deep networks trained on histology and genomic data alone. The proposed method establishes insight and theory on how to train deep networks on multimodal biomedical data in an intuitive manner, which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment. ","Pathomic Fusion: An Integrated Framework for Fusing Histopathology and
  Genomic Features for Cancer Diagnosis and Prognosis"
117,1207572380308123649,882303076505456642,Timon Emken,"[""Our Christmas #paper is out on today's @arxiv - a collaboration between @ChalmersPhysics and @NicolaSpaldin and her group @ETH.\n\n<LINK>\n\nWe study sub-GeV #DarkMatter induced atomic ionizations for direct DM searches with general effective theory methods.\n\n(1/6) <LINK>"", '@arxiv @ChalmersPhysics @NicolaSpaldin @ETH In past studies on DM searches via electron scatterings, a ""dark photon model"" is typically used to describe the DM-electron interactions.\nFor this model, the atomic physics, needed to describe ionizations, can neatly be absorbed into ""ionization form factors"", shown here.\n\n(2/6) https://t.co/JdPbfceKad', '@arxiv @ChalmersPhysics @NicolaSpaldin @ETH In our paper, we do not use a specific model but a more general effective approach. \n\nInstead of just one ionization form factor, we find that four electron response functions appear, which we computed and are shown here for the 5p orbital of xenon.\n\n(3/6) https://t.co/tlQnTy52fd', '@arxiv @ChalmersPhysics @NicolaSpaldin @ETH A cool feature of this is the idea that DM (after its discovery in the near future...) could maybe be used to probe hidden material properties. General DM-electron interactions could trigger new atomic responses, which are inaccessible to electromagnetic interactions.\n\n(4/6)', '@arxiv @ChalmersPhysics @NicolaSpaldin @ETH This way, #darkmatter particles would act as a probe to study condensed matter systems.\n\nWe also look at present direct detection experiments with dual-phase noble targets (@XENONexperiment), and set exclusion limits for different scenarios.\n\n(5/6) https://t.co/PguaCLr4w5', '@arxiv @ChalmersPhysics @NicolaSpaldin @ETH @XENONexperiment The code used to compute the electron response functions is available on @Github.\n\nhttps://t.co/a5Ngc9rE0E\n\n#OpenScience\n\n(6/6) https://t.co/3dqmVs6W2K']",https://arxiv.org/abs/1912.08204,"In the leading paradigm of modern cosmology, about 80% of our Universe's matter content is in the form of hypothetical, as yet undetected particles. These do not emit or absorb radiation at any observable wavelengths, and therefore constitute the so-called Dark Matter (DM) component of the Universe. Detecting the particles forming the Milky Way DM component is one of the main challenges for astroparticle physics and basic science in general. One promising way to achieve this goal is to search for rare DM-electron interactions in low-background deep underground detectors. Key to the interpretation of this search is the response of detectors' materials to elementary DM-electron interactions defined in terms of electron wave functions' overlap integrals. In this work, we compute the response of atomic argon and xenon targets used in operating DM search experiments to general, so far unexplored DM-electron interactions. We find that the rate at which atoms can be ionized via DM-electron scattering can in general be expressed in terms of four independent atomic responses, three of which we identify here for the first time. We find our new atomic responses to be numerically important in a variety of cases, which we identify and investigate thoroughly using effective theory methods. We then use our atomic responses to set 90% confidence level (C.L.) exclusion limits on the strength of a wide range of DM-electron interactions from the null result of DM search experiments using argon and xenon targets. ",Atomic responses to general dark matter-electron interactions
118,1207213778368159744,64506207,Víctor Aguirre Børsen-Koch,"['Paper out today! Detection and characterisation of oscillating red giants: first results from the TESS satellite. What did we find? <LINK>', 'This is the first asteroseismic ensemble study of red giants observed by TESS that have been monitored by one or two sectors.  We expect that about 97% of asteroseismic detections in red giants from TESS to come from stars observed for one or two sectors.', 'We selected a sample of bright giants where we assumed we would detect oscillations if TESS was performing as well as Kepler did (apart from its smaller aperture). We detected the asteroseismic signal in all but one of them (we know why) =&gt; TESS is doing great!', 'Combining TESS asteroseismology with Gaia parallaxes yields stellar ages to a precision of 20%, taking asteroseismology for Galactic archaeology to a new scale as TESS should detect oscillations in ~500,000 red giants (an order of magnitude higher than any previous mission)']",https://arxiv.org/abs/1912.07604,"Since the onset of the `space revolution' of high-precision high-cadence photometry, asteroseismology has been demonstrated as a powerful tool for informing Galactic archaeology investigations. The launch of the NASA TESS mission has enabled seismic-based inferences to go full sky -- providing a clear advantage for large ensemble studies of the different Milky Way components. Here we demonstrate its potential for investigating the Galaxy by carrying out the first asteroseismic ensemble study of red giant stars observed by TESS. We use a sample of 25 stars for which we measure their global asteroseimic observables and estimate their fundamental stellar properties, such as radius, mass, and age. Significant improvements are seen in the uncertainties of our estimates when combining seismic observables from TESS with astrometric measurements from the Gaia mission compared to when the seismology and astrometry are applied separately. Specifically, when combined we show that stellar radii can be determined to a precision of a few percent, masses to 5-10% and ages to the 20% level. This is comparable to the precision typically obtained using end-of-mission Kepler data ","Detection and characterisation of oscillating red giants: first results
  from the TESS satellite"
119,1206715477180862464,1268474822,Shirley Li,"['Our new paper is out! So excited!\n\nLDMX is a proposed light mark matter experiment. We propose to use it to measure GeV electron-nucleus interaction cross sections, which would benefit the long-baseline neutrino oscillation program!\n\n<LINK>']",https://arxiv.org/abs/1912.06140,"We point out that the LDMX (Light Dark Matter eXperiment) detector design, conceived to search for sub-GeV dark matter, will also have very advantageous characteristics to pursue electron-nucleus scattering measurements of direct relevance to the neutrino program at DUNE and elsewhere. These characteristics include a 4-GeV electron beam, a precision tracker, electromagnetic and hadronic calorimeters with near 2$\pi$ azimuthal acceptance from the forward beam axis out to $\sim$40$^\circ$ angle, and low reconstruction energy threshold. LDMX thus could provide (semi)exclusive cross section measurements, with detailed information about final-state electrons, pions, protons, and neutrons. We compare the predictions of two widely used neutrino generators (GENIE, GiBUU) in the LDMX region of acceptance to illustrate the large modeling discrepancies in electron-nucleus interactions at DUNE-like kinematics. We argue that discriminating between these predictions is well within the capabilities of the LDMX detector. ","Lepton-Nucleus Cross Section Measurements for DUNE with the LDMX
  Detector"
120,1206572429247733763,4844847993,Alexandre Bovet,['Our work with @leoguti85\n and Jean-Charles Delvenne on multi-scale anomaly detection in attributed networks was accepted at #AAAI20. <LINK>. \nWe use a graph-signal processing framework to find anomalies and their contexts at all scales.\nMy first CS conference 🙂 <LINK>'],https://arxiv.org/abs/1912.04144,"Many social and economic systems can be represented as attributed networks encoding the relations between entities who are themselves described by different node attributes. Finding anomalies in these systems is crucial for detecting abuses such as credit card frauds, web spams or network intrusions. Intuitively, anomalous nodes are defined as nodes whose attributes differ starkly from the attributes of a certain set of nodes of reference, called the context of the anomaly. While some methods have proposed to spot anomalies locally, globally or within a community context, the problem remain challenging due to the multi-scale composition of real networks and the heterogeneity of node metadata. Here, we propose a principled way to uncover outlier nodes simultaneously with the context with respect to which they are anomalous, at all relevant scales of the network. We characterize anomalous nodes in terms of the concentration retained for each node after smoothing specific signals localized on the vertices of the graph. Besides, we introduce a graph signal processing formulation of the Markov stability framework used in community detection, in order to find the context of anomalies. The performance of our method is assessed on synthetic and real-world attributed networks and shows superior results concerning state of the art algorithms. Finally, we show the scalability of our approach in large networks employing Chebychev polynomial approximations. ",Multi-scale Anomaly Detection on Attributed Networks
121,1206497262798561281,977906884886827008,Marcos Mariño,['What are renormalons? In our new paper we find an infrared renormalon in a very simple two-dimensional theory: <LINK>'],https://arxiv.org/abs/1912.06228,"According to standard lore, perturbative series of super-renormalizable theories have only instanton singularities. In this paper we show that two-dimensional scalar theories with a spontaneously broken $O(N)$ symmetry at the classical level, which are super-renormalizable, have an IR renormalon singularity at large $N$. Since perturbative expansions in these theories are made around the ""false vacuum"" in which the global symmetry is broken, this singularity can be regarded as a manifestation of the non-perturbative absence of Goldstone bosons. We conjecture that the Borel singularity in the ground state energy of the Lieb--Liniger model is a non-relativistic manifestation of this phenomenon. We also provide {\it en passant} a detailed perturbative calculation of the Lieb--Liniger energy up to two-loops, and we check that it agrees with the prediction of the Bethe ansatz. ",A new renormalon in two dimensions
122,1206493694951657472,483834412,Linda Nab 📏,"['Hi #epitwitter 👋 a confounder is measured with error. What do you do?\n1️⃣run🏃\u200d♀️🏃\u200d♂️\n2️⃣tackle the problem🤺\n\n2 but not sure how? @ProfGroenwold @MaartenvSmeden @RuthHKeogh and I studied this in our new preprint: <LINK> . We distinguish 2 settings... 1/6', 'Suppose you’re estimating a treatment effect. \n1️⃣Is trtmnt initiation based on the error-prone confounder?👇\n\nA: HIV therapy\nY: mortality\nL: CD4 count\nObserved CD4 count (L*) = error-prone but HIV therapy is initiated based on obs, not true, CD4 count. 2/6 https://t.co/omyXgFHtkS', '2️⃣Is trtmnt initiation based on the true confounder?👇\n\nIn an elderly population,\nA: influenza vaccination\nY: mortality\nL: frailty\nOnly proxies for frailty (L*) may be available in obs data &amp; frailty as observed by the clinician may be unknown. 3/6 https://t.co/BBV4rLc3yN', 'If you’re in setting 1️⃣, controlling for L* io L will block the backdoor path from A to Y 👉✅\nIf you’re in setting 2️⃣, controlling for L* won’t fully block the backdoor path 👉💔there’s residual confounding. Consider doing a sensitivity analysis 💪. 4/6', 'To inform sens analyses, we derived bias expressions for when,\nA=treatment indicator\nY=continuous outcome\nL=binary confounder\n&amp; the treatment effect is estimated in a marginal structural model estimated using inverse prob weighting (msm-ipw) ⚖️.  5/6 https://t.co/rmxGWJf78Y', '🌟Check our Shiny to see how classification err in a confounder biases trtmnt effects (📢often not towards the null)👉https://t.co/Xk3K31iY4K\n📄Read our preprint to find out if bias in a msm-ipw differs from that in a covariate adjusted model👉https://t.co/oKvxQBi59Q 6/6', 'ps: comments/thoughts on our preprint + Shiny app are very much appreciated! 💌', ""@raj_mehta That's very kind of you! Thank you!""]",https://arxiv.org/abs/1912.05800,"In observational research treatment effects, the average treatment effect (ATE) estimator may be biased if a confounding variable is misclassified. We discuss the impact of classification error in a dichotomous confounding variable in analyses using marginal structural models estimated using inverse probability weighting (MSMs-IPW) and compare this with its impact in conditional regression models, focusing on a point-treatment study with a continuous outcome. Expressions were derived for the bias in the ATE estimator from a MSM-IPW and conditional model by using the potential outcome framework. Based on these expressions, we propose a sensitivity analysis to investigate and quantify the bias due to classification error in a confounding variable in MSMs-IPW. Compared to bias in the ATE estimator from a conditional model, the bias in MSM-IPW can be dissimilar in magnitude but the bias will always be equal in sign. A simulation study was conducted to study the finite sample performance of MSMs-IPW and conditional models if a confounding variable is misclassified. Simulation results showed that confidence intervals of the treatment effect obtained from MSM-IPW are generally wider and coverage of the true treatment effect is higher compared to a conditional model, ranging from over coverage if there is no classification error to smaller under coverage when there is classification error. The use of the bias expressions to inform a sensitivity analysis was demonstrated in a study of blood pressure lowering therapy. It is important to consider the potential impact of classification error in a confounding variable in studies of treatment effects and a sensitivity analysis provides an opportunity to quantify the impact of such errors on causal conclusions. An online tool for sensitivity analyses was developed: this https URL ","Sensitivity analysis for bias due to a misclassfied confounding variable
  in marginal structural models"
123,1205091495025491968,856472551710756864,Juan Sensio,"['My first paper is now on @arxiv_org  ! We propose a new way of solving Partial Differential Equations, replacing traditional numerical techniques with Neural Networks (which brings lots of benefits). Check it out -&gt; <LINK> @jmaronasm @RobertoParPal']",https://arxiv.org/abs/1912.04737,"Many scientific and industrial applications require solving Partial Differential Equations (PDEs) to describe the physical phenomena of interest. Some examples can be found in the fields of aerodynamics, astrodynamics, combustion and many others. In some exceptional cases an analytical solution to the PDEs exists, but in the vast majority of the applications some kind of numerical approximation has to be computed. In this work, an alternative approach is proposed using neural networks (NNs) as the approximation function for the PDEs. Unlike traditional numerical methods, NNs have the property to be able to approximate any function given enough parameters. Moreover, these solutions are continuous and derivable over the entire domain removing the need for discretization. Another advantage that NNs as function approximations provide is the ability to include the free-parameters in the process of finding the solution. As a result, the solution can generalize to a range of situations instead of a particular case, avoiding the need of performing new calculations every time a parameter is changed dramatically decreasing the optimization time. We believe that the presented method has the potential to disrupt the physics simulation field enabling real-time physics simulation and geometry optimization without the need of big supercomputers to perform expensive and time consuming simulations ",Solving Partial Differential Equations with Neural Networks
124,1204760270423371777,2322575761,Prof Roberto Trotta,"['We have measured the mass of the Milky Way  w @ImperialPhysics co-authors A. Geringer-Sameth and @curiousfabio (final checks in the pic below) and find for the total mass:\n log10(Mtot/M⊙)=11.95±0.04(stat)±0.25±0.25(syst) \nor 8.9±1 (stat) 10^11 M⊙\n\n<LINK> <LINK>', '@JohnEvansTW1 @ImperialPhysics @curiousfabio Yeah - the visible part is the minority of the mass (but a majority of the calories :) \n\n(Thanks!)']",https://arxiv.org/abs/1912.04296,"We present a new estimate of the mass of the Milky Way, inferred via a Bayesian approach by making use of tracers of the circular velocity in the disk plane and stars in the stellar halo, as from the publicly available {\tt galkin} compilation. We use the rotation curve method to determine the dark matter distribution and total mass under different assumptions for the dark matter profile, while the total stellar mass is constrained by surface stellar density and microlensing measurements. We also include uncertainties on the baryonic morphology via Bayesian model averaging, thus converting a potential source of systematic error into a more manageable statistical uncertainty. We evaluate the robustness of our result against various possible systematics, including rotation curve data selection, uncertainty on the Sun's velocity $V_0$, dependence on the dark matter profile assumptions, and choice of priors. We find the Milky Way's dark matter virial mass to be $\log_{10}M_{200}^{\rm DM}/ {\rm M_\odot} = 11.92^{+0.06}_{-0.05}{\rm(stat)}\pm{0.28}\pm0.27{\rm(syst)}$ ($M_{200}^{\rm DM}=8.3^{+1.2}_{-0.9}{\rm(stat)}\times10^{11}\,{\rm M_\odot}$). We also apply our framework to Gaia DR2 rotation curve data and find good statistical agreement with the above results. ",A robust estimate of the Milky Way mass from rotation curve data
125,1204568191642427392,83767498,Nitish Srivastava,"['Happy to share our recent work, ""Geometric Capsule Autoencoders for 3D Point Clouds."" The main idea is that instead of finding agreement among parts of an object, we find agreement among different views of the object. <LINK> <LINK>']",https://arxiv.org/abs/1912.03310,"We propose a method to learn object representations from 3D point clouds using bundles of geometrically interpretable hidden units, which we call geometric capsules. Each geometric capsule represents a visual entity, such as an object or a part, and consists of two components: a pose and a feature. The pose encodes where the entity is, while the feature encodes what it is. We use these capsules to construct a Geometric Capsule Autoencoder that learns to group 3D points into parts (small local surfaces), and these parts into the whole object, in an unsupervised manner. Our novel Multi-View Agreement voting mechanism is used to discover an object's canonical pose and its pose-invariant feature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the properties of the learned representations and show the benefits of having multiple votes agree. We perform alignment and retrieval of arbitrarily rotated objects -- tasks that evaluate our model's object identification and canonical pose recovery capabilities -- and obtained insightful results. ",Geometric Capsule Autoencoders for 3D Point Clouds
126,1204228552586981376,1057550940331540480,Jakub Otwinowski,"['New work with Colin Lamont at MPIDS where we propose a new evolutionary optimization algorithm inspired by quantitative genetics. <LINK>  cc @togelius -- summary to follow', 'First we point out that natural selection performs a natural gradient step (known but under-appreciated). This is similar to what CMA-ES and NES do, but without any linear algebra, all you need are exponential weights.', ""In other words there's a lot of similarity between classical multivariate quantitative genetics and information geometric optimization algorithms (chiefly NES)."", 'Then we discuss how new variants are generated. Mutations can knock you off a ridge in a fitness landscape. In quantitative genetics, recombination can sort of (not quite) make a variant that is aligned with the population, which is in turn aligned with the landscape', 'We invent a phenotype recombination operator that preserves the phenotype covariance of the population, so new variants are aligned with the landscape. This recombination is like drawing from a normal distribution in CMA-ES, without having to store a covariance matrix!', 'We put it all together to make a Quantitative Genetic Algorithm (QGA). We implement an adaptive scheme that increases selection based on adjusting the exponential weights to reach a target population entropy. It naturally incorporates all past information, and adapts to curvature', ""QGA works pretty well, more or less like CMA-ES in benchmarks... but only after scanning over the single hyperparameter first. So figuring out other adaptive schemes is something to think about. We're new to this, and hope QGA inspires more work in evolutionary optimization."", ""Maybe someone can try QGA on a large problem. The advantage is that the algorithm is ridiculously simple and you don't need to store a covariance matrix.""]",https://arxiv.org/abs/1912.03395,"Evolutionary algorithms, inspired by natural evolution, aim to optimize difficult objective functions without computing derivatives. Here we detail the relationship between population genetics and evolutionary optimization and formulate a new evolutionary algorithm. Optimization of a continuous objective function is analogous to searching for high fitness phenotypes on a fitness landscape. We summarize how natural selection moves a population along the non-euclidean gradient that is induced by the population on the fitness landscape (the natural gradient). Under normal approximations common in quantitative genetics, we show how selection is related to Newton's method in optimization. We find that intermediate selection is most informative of the fitness landscape. We describe the generation of new phenotypes and introduce an operator that recombines the whole population to generate variants that preserve normal statistics. Finally, we introduce a proof-of-principle algorithm that combines natural selection, our recombination operator, and an adaptive method to increase selection. Our algorithm is similar to covariance matrix adaptation and natural evolutionary strategies in optimization, and has similar performance. The algorithm is extremely simple in implementation with no matrix inversion or factorization, does not require storing a covariance matrix, and may form the basis of more general model-based optimization algorithms with natural gradient updates. ",Information-geometric optimization with natural selection
127,1204055660368990208,1038513938600796160,Thomas Feuillen,"['[1912.02880] Here is our new preprint with @jacquesdurden  @vdd_ucl_1  and Mike E. Davies where we study the reconstruction accuracy of Projected Back-Projection with complex Phase-Only measurement leveraging the (l1,l2)-RIP. <LINK> <LINK>', '@jacquesdurden @vdd_ucl_1 To that end we also extended this property to the case of complex Gaussian matrices. This is not as straightforward as extending real compressed sensing to the complex case; recasting complex to real is not possible because of the l1-norm of this RIP variant.']",https://arxiv.org/abs/1912.02880,"This letter analyzes the performances of a simple reconstruction method, namely the Projected Back-Projection (PBP), for estimating the direction of a sparse signal from its phase-only (or amplitude-less) complex Gaussian random measurements, i.e., an extension of one-bit compressive sensing to the complex field. To study the performances of this algorithm, we show that complex Gaussian random matrices respect, with high probability, a variant of the Restricted Isometry Property (RIP) relating to the l1 -norm of the sparse signal measurements to their l2 -norm. This property allows us to upper-bound the reconstruction error of PBP in the presence of phase noise. Monte Carlo simulations are performed to highlight the performance of our approach in this phase-only acquisition model when compared to error achieved by PBP in classical compressive sensing. ","(l1,l2)-RIP and Projected Back-Projection Reconstruction for Phase-Only
  Measurements"
128,1203993921749225472,4249537197,Christian Wolf,"['New paper from @CorentK, co-supervised  with @moezbac  @antigregory. We show that object-word alignment does not emerge naturally in BERT like training of transformers on vision-language task and propose weak supervision for it, improving performance: \n<LINK> <LINK>']",https://arxiv.org/abs/1912.03063,"The large adoption of the self-attention (i.e. transformer model) and BERT-like training principles has recently resulted in a number of high performing models on a large panoply of vision-and-language problems (such as Visual Question Answering (VQA), image retrieval, etc.). In this paper we claim that these State-Of-The-Art (SOTA) approaches perform reasonably well in structuring information inside a single modality but, despite their impressive performances , they tend to struggle to identify fine-grained inter-modality relationships. Indeed, such relations are frequently assumed to be implicitly learned during training from application-specific losses, mostly cross-entropy for classification. While most recent works provide inductive bias for inter-modality relationships via cross attention modules, in this work, we demonstrate (1) that the latter assumption does not hold, i.e. modality alignment does not necessarily emerge automatically, and (2) that adding weak supervision for alignment between visual objects and words improves the quality of the learned models on tasks requiring reasoning. In particular , we integrate an object-word alignment loss into SOTA vision-language reasoning models and evaluate it on two tasks VQA and Language-driven Comparison of Images. We show that the proposed fine-grained inter-modality supervision significantly improves performance on both tasks. In particular, this new learning signal allows obtaining SOTA-level performances on GQA dataset (VQA task) with pre-trained models without finetuning on the task, and a new SOTA on NLVR2 dataset (Language-driven Comparison of Images). Finally, we also illustrate the impact of the contribution on the models reasoning by visualizing attention distributions. ","Weak Supervision helps Emergence of Word-Object Alignment and improves
  Vision-Language Tasks"
129,1202834441992916992,617492880,Benoit Famaey,"['Happy to announce the results of our latest study, led by @LePaolo87 , with R. Ibata, on the kinematics in the outskirts of the globular cluster NGC 3201 with @ESAGaia. Did we detect a clear dark-matter-like effect in the outskirts of a GC? May well be!! <LINK>', '@LePaolo87 @ESAGaia The origin of globular clusters is clearly one of the major open questions in modern astrophysics. Long ago already, it has been suggested that they might form in their own dark matter halos https://t.co/KEpi8yHPbV', '@LePaolo87 @ESAGaia If that is true, we might detect a signature of this dark matter mini-halo -- left-over from the formation of a globular cluster -- only in the outskirts... as baryons of course fully dominate the central density', '@LePaolo87 @ESAGaia One of the smoking guns for this would be a flattening, or even slight increase, of the projected velocity dispersion profile at large distances, see e.g. https://t.co/O7eMkqX6VE', '@LePaolo87 @ESAGaia Of course, this is *very* difficult to measure... because of contamination, of course, but also because of tidal effects, accompanied by the presence of potential escaper stars that have orbital energy greater than the escape energy, but are confined within the Jacobi radius', '@LePaolo87 @ESAGaia Now, thanks to @ESAGaia, one can in principle do this out to the Jacobi radius. One needs to find a relatively closeby GC, with easily identifiable kinematics. NGC 3201 appeared like a good choice to start with, with a retrograde and eccentric orbit at a distance of 4.9 kpc', '@LePaolo87 @ESAGaia So @LePaolo87 made a super careful selection, based on proper motions, Grp-Gbp color, G magnitude and parallax, out to twice the Jacobi radius. He checked for contamination, included an additional error on the proper motion errors, and checked for magnitude-dependent biases', ""@LePaolo87 @ESAGaia And the result is this flattish on-sky projected velocity dispersion profile! It's higher than what would be expected from the presence of potential escapers (red arrow). And it's flat all the way out to 60 pc, and even out to the Jacobi radius where it even seems to rise https://t.co/GBzMLlPpGy"", '@LePaolo87 @ESAGaia Admittedly, it is not the first time that a flattening of the velocity dispersion of a GC is reported, *but* it is the first time it is probed so far away (note the previously measured l.o.s. disp. profile in red) out to the Jacobi radius, a whopping ~80pc for this 4*10^5 Msun GC', ""@LePaolo87 @ESAGaia It's actually a bit like going from the optical radius to the HI radius of disk galaxies when analyzing their rotation curves"", ""@LePaolo87 @ESAGaia Of course, the tidal history of this cluster might perhaps explain this. More simulations will be needed. And also, if this cluster indeed had a peculiar tidal history, then we shouldn't find this in other ones. Further similar velocity dispersion profile measurements will come"", ""@LePaolo87 @ESAGaia And btw, could it be due to something else than dark matter, like modified gravity? Well, the asymptotic velocity dispersion you would expect in something like MOND would be ~4 km/s, not so far to the observed asymptotic value. Intriguing. But that'd be true for MOND in isolation"", '@LePaolo87 @ESAGaia However the acceleration of the MW gravitational field, even at apocenter, is larger than the typical acceleration from the GC itself in the outskirts. This implies that no flattening would *a priori* be expected in modified gravity MOND because of the ""external field effect""', '@LePaolo87 @ESAGaia But again, let\'s be careful, as there could be possible additional MOND effects due to the varying tidal field along the highly eccentric orbit of the GC, so again, more simulations are needed to conclude... And things are different in ""modified inertia""', ""@LePaolo87 @ESAGaia In any case, these results are cool, and we'll see what the future holds :)"", 'ps: note that depending on how much DM would be left, the actual ""Jacobi radius"" would be further out, but it\'s still too early to think in these terms', '@FuzzyDarkMatter @LePaolo87 @ESAGaia If confirmed, it will open a few of such interesting questions indeed, and put formation scenarios to the test']",https://arxiv.org/abs/1912.02195,"The outskirts of globular clusters (GCs) simultaneously retain crucial information about their formation mechanism and the properties of their host galaxy. Thanks to the advent of precision astrometry both their morphological and kinematic properties are now accessible. Here we present the first dynamical study of the outskirts of the retrograde GC NGC 3201 until twice its Jacobi radius (< 100 pc), using specifically-selected high-quality astrometric data from Gaia DR2. We report the discovery of a stellar overdensity along the South-East/North-West direction that we identify as tidal tails. The GC is characterized globally by radial anisotropy and a hint of isotropy in the outer parts, with an excess of tangential orbits around the lobes corresponding to the tidal tails, in qualitative agreement with an N-body simulation. Moreover, we measure flat velocity dispersion profiles, reaching values of $3.5\pm0.9$ km/s until beyond the Jacobi radius. While tidal tails could contribute to such a flattening, this high velocity dispersion value is in disagreement with the expectation from the sole presence of potential escapers. To explain this puzzling observation, we discuss the possibility of an accreted origin of the GC, the presence of a dark matter halo --leftover of its formation at high redshift -- and the possible effects of non-Newtonian dynamics. Our study uncovers a new path for the study of GC formation and of the properties of the Milky Way potential in the era of precision astrometry. ","Exploring the outskirts of globular clusters: the peculiar kinematics of
  NGC 3201"
130,1202538011545743360,23000769,Christopher Conselice,"[""On arXiv today -- Stockmann et al. (including myself) study with ~100 hours of XSHOOTER spectra and HST imaging a sample of the most massive z &gt; 2 galaxies we know of - the progenitors of today's most massive systems.  Lots of details in the paper, but..\n\n<LINK>"", 'The velocity dispersion and size evolution are consistent with minor mergers as the dominate way in which they evolve at z &lt; 2.  Also, these massive galaxies must have formed very early, pushing back the epoch of massive galaxy assembly.  Fundamental plane paper coming out soon.']",https://arxiv.org/abs/1912.01619,"We present a detailed analysis of a large sample of spectroscopically confirmed ultra-massive quiescent galaxies (${\rm{log}}(M_{\ast}/M_{\odot})\sim11.5$) at $z\gtrsim2$. This sample comprises 15 galaxies selected in the COSMOS and UDS fields by their bright K-band magnitudes and followed up with VLT/X-shooter spectroscopy and HST/WFC3 $H_{F160W}$ imaging. These observations allow us to unambiguously confirm their redshifts ascertain their quiescent nature and stellar ages, and to reliably assess their internal kinematics and effective radii. We find that these galaxies are compact, consistent with the high mass end of the mass-size relation for quiescent galaxies at $z=2$. Moreover, the distribution of the measured stellar velocity dispersions of the sample is consistent with the most massive local early-type galaxies from the MASSIVE Survey showing that evolution in these galaxies, is dominated by changes in size. The HST images reveal, as surprisingly high, that $40\ \%$ of the sample have tidal features suggestive of mergers and companions in close proximity, including three galaxies experiencing ongoing major mergers. The absence of velocity dispersion evolution from $z=2$ to $0$, coupled with a doubling of the stellar mass, with a factor of four size increase and the observed disturbed stellar morphologies support dry minor mergers as the primary drivers of the evolution of the massive quiescent galaxies over the last 10 billion years. ","X-shooter Spectroscopy and HST Imaging of 15 Ultra Massive Quiescent
  Galaxies at $z\gtrsim2$"
131,1201908198350811136,3119778197,Hanie Sedghi,"['Excited to share our latest work on generalization in DL <LINK> w/ Niladri Chatterji &amp; @bneyshabur\nWe study the phenomenon that some modules of DNNs are more critical than others: rewinding their values back to initialization, strongly harms performance.(1/3)', 'Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connects the initial and final values of the module parameters. (2/3)', 'We show that module criticality is able to explain the superior generalization performance of some architectures over others, whereas earlier measures fail to do so. (3/3)']",https://arxiv.org/abs/1912.00528,"We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connects the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas earlier measures fail to do so. ","The intriguing role of module criticality in the generalization of deep
  networks"
132,1201696514663469056,786684380966191104,Dusty Ray Madison,"[""Check out this paper led by @rocket_ship97 showing that we could detect planetesimals down to a tenth of a moon mass orbiting many of our @NANOGrav pulsars but we don't find any. Goes to show that pulsars are amazing and the known pulsar planets are weird. <LINK>""]",https://arxiv.org/abs/1912.00482,"We search for extrasolar planets around millisecond pulsars using pulsar timing data and seek to determine the minimum detectable planetary masses as a function of orbital period. Using the 11-year data set from the North American Nanohertz Observatory for Gravitational Waves (NANOGrav), we look for variations from our models of pulse arrival times due to the presence of exoplanets. No planets are detected around the millisecond pulsars in the NANOGrav 11-year data set, but taking into consideration the noise levels of each pulsar and the sampling rate of our observations, we develop limits that show we are sensitive to planetary masses as low as that of the moon. We analyzed potential planet periods, P, in the range 7 days < P < 2000 days, with somewhat smaller ranges for some binary pulsars. The planetary mass limit for our median-sensitivity pulsar within this period range is 1 M_moon (P / 100 days)^(-2/3). ","The NANOGrav 11-year Data Set: Constraints on Planetary Masses Around 45
  Millisecond Pulsars"
133,1212764548492668928,907232486735958018,Jaki Noronha-Hostler,"[""Check out Jamie Stafford's preview of work that will be done shortly on the freeze-out of light versus strange particles: <LINK>\n\nWe study the influence of different # of particles &amp; find the flavor hierarchy still holds. Thermal fit results should be out soon. <LINK>""]",https://arxiv.org/abs/1912.12968,"We calculate the mean-over-variance ratio of the net-kaon fluctuations in the Hadron Resonance Gas (HRG) Model for the five highest energies of the RHIC Beam Energy Scan (BES) for different particle data lists. We compare these results with the latest experimental data from the STAR collaboration in order to extract sets of chemical freeze-out parameters for each list. We focused on the PDG2012 and PDG2016+ particle lists, which differ largely in the number of resonant states. Our analysis determines the effect of the amount of resonances included in the HRG on the freeze-out conditions. ","Determination of Chemical Freeze-out Parameters from Net-kaon
  Fluctuations at RHIC"
134,1212528853207408641,4106835983,Daniel Moghimi,"['We studied the potential side-channel threat of integrating FPGAs with the CPU memory subsystem in the ongoing collaboration with @IntelSecurity to make these platforms more secure.  \n\n""JackHammer: Efficient Rowhammer on Heterogeneous FPGA-CPU Platforms"" <LINK>', '@IntelSecurity An FPGA-originated rowhammer can hammer faster and flip more bits compared to the CPU rowhammer on the same platform.', '@IntelSecurity @ThoreTiemann @defparam @tomcrypt @berksunar']",https://arxiv.org/abs/1912.11523,"After years of development, FPGAs are finally making an appearance on multi-tenant cloud servers. These heterogeneous FPGA-CPU architectures break common assumptions about isolation and security boundaries. Since the FPGA and CPU architectures share hardware resources, a new class of vulnerabilities requires us to reassess the security and dependability of these platforms. In this work, we analyze the memory and cache subsystem and study Rowhammer and cache attacks enabled on two proposed heterogeneous FPGA-CPU platforms by Intel: the Arria 10 GX with an integrated FPGA-CPU platform, and the Arria 10 GX PAC expansion card which connects the FPGA to the CPU via the PCIe interface. We show that while Intel PACs currently are immune to cache attacks from FPGA to CPU, the integrated platform is indeed vulnerable to Prime and Probe style attacks from the FPGA to the CPU's last level cache. Further, we demonstrate JackHammer, a novel and efficient Rowhammer from the FPGA to the host's main memory. Our results indicate that a malicious FPGA can perform twice as fast as a typical Rowhammer attack from the CPU on the same system and causes around four times as many bit flips as the CPU attack. We demonstrate the efficacy of JackHammer from the FPGA through a realistic fault attack on the WolfSSL RSA signing implementation that reliably causes a fault after an average of fifty-eight RSA signatures, 25% faster than a CPU rowhammer attack. In some scenarios our JackHammer attack produces faulty signatures more than three times more often and almost three times faster than a conventional CPU rowhammer attack. ",JackHammer: Efficient Rowhammer on Heterogeneous FPGA-CPU Platforms
135,1202423246663979008,1138872700376571905,Roozbeh Mottaghi,"['Our work on Visual Reaction. The problem is to forecast the future of an evolving environment and plan accordingly. We study the problem in the context of playing catch with a drone. The drone learns to catch the object that we throw. @DeepHaoHao #AI \n<LINK> <LINK>', '@DeepHaoHao The full video: https://t.co/WzCdXE22sq']",https://arxiv.org/abs/1912.02155,"In this paper we address the problem of visual reaction: the task of interacting with dynamic environments where the changes in the environment are not necessarily caused by the agent itself. Visual reaction entails predicting the future changes in a visual environment and planning accordingly. We study the problem of visual reaction in the context of playing catch with a drone in visually rich synthetic environments. This is a challenging problem since the agent is required to learn (1) how objects with different physical properties and shapes move, (2) what sequence of actions should be taken according to the prediction, (3) how to adjust the actions based on the visual feedback from the dynamic environment (e.g., when objects bouncing off a wall), and (4) how to reason and act with an unexpected state change in a timely manner. We propose a new dataset for this task, which includes 30K throws of 20 types of objects in different directions with different forces. Our results show that our model that integrates a forecaster with a planner outperforms a set of strong baselines that are based on tracking as well as pure model-based and model-free RL baselines. The code and dataset are available at github.com/KuoHaoZeng/Visual_Reaction. ",Visual Reaction: Learning to Play Catch with Your Drone
136,1202088914078572544,1147039217534537728,Rohan Chandra,"['(1/2) New paper on arXiv: <LINK>\n\nWe propose a trajectory prediction algorithm for autonomous driving. We use spectral graph theory to reduce long term RMSE and identify/predict behavior (over-speeding etc).\n\nCode,Video, Datasets: <LINK>', '(2/2) More research on autonomous driving from our group found here: https://t.co/hpBOMaIy0c']",https://arxiv.org/abs/1912.01118,"We present a novel approach for traffic forecasting in urban traffic scenarios using a combination of spectral graph analysis and deep learning. We predict both the low-level information (future trajectories) as well as the high-level information (road-agent behavior) from the extracted trajectory of each road-agent. Our formulation represents the proximity between the road agents using a weighted dynamic geometric graph (DGG). We use a two-stream graph-LSTM network to perform traffic forecasting using these weighted DGGs. The first stream predicts the spatial coordinates of road-agents, while the second stream predicts whether a road-agent is going to exhibit overspeeding, underspeeding, or neutral behavior by modeling spatial interactions between road-agents. Additionally, we propose a new regularization algorithm based on spectral clustering to reduce the error margin in long-term prediction (3-5 seconds) and improve the accuracy of the predicted trajectories. Moreover, we prove a theoretical upper bound on the regularized prediction error. We evaluate our approach on the Argoverse, Lyft, Apolloscape, and NGSIM datasets and highlight the benefits over prior trajectory prediction methods. In practice, our approach reduces the average prediction error by approximately 75% over prior algorithms and achieves a weighted average accuracy of 91.2% for behavior prediction. Additionally, our spectral regularization improves long-term prediction by up to 70%. ","Forecasting Trajectory and Behavior of Road-Agents Using Spectral
  Clustering in Graph-LSTMs"
