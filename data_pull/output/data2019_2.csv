,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1105314402004135936,503452360,William Wang,"['Super excited about @WenhuChen\'s new #NAACL2019 paper ""How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection"". PDF: <LINK>  CODE: <LINK> #NLProc']",https://arxiv.org/abs/1902.10339,"With the rapid development in deep learning, deep neural networks have been widely adopted in many real-life natural language applications. Under deep neural networks, a pre-defined vocabulary is required to vectorize text inputs. The canonical approach to select pre-defined vocabulary is based on the word frequency, where a threshold is selected to cut off the long tail distribution. However, we observed that such simple approach could easily lead to under-sized vocabulary or over-sized vocabulary issues. Therefore, we are interested in understanding how the end-task classification accuracy is related to the vocabulary size and what is the minimum required vocabulary size to achieve a specific performance. In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to perform vocabulary selection, which can intelligently select the subset of the vocabulary to achieve the required performance. To evaluate different algorithms on the newly proposed vocabulary selection problem, we propose two new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X\% Accuracy Drop. Through extensive experiments on various NLP classification tasks, our variational framework is shown to significantly outperform the frequency-based and other selection baselines on these metrics. ","How Large a Vocabulary Does Text Classification Need? A Variational
  Approach to Vocabulary Selection"
1,1104075634593095681,4870078413,Sam Schoenholz,"['1/3 Our new paper analyzing batch normalization in neural networks at initialization is out (and will be at @iclr2019). We find that batch norm + MLPs always feature exploding gradients for any choice of nonlinearity and batch size. <LINK> <LINK>', '2/3 We propose several schemes to ameliorate this by careful parameter tuning. The formalism here also opens the door to performing Bayesian inference on Gaussian Process that correspond to neural networks with batch normalization.', '3/3 As always, this was a really fun collaboration with @TheGregYang, Jeffrey Pennington, @vinaysrao, @jaschasd.']",https://arxiv.org/abs/1902.08129,"We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest. ",A Mean Field Theory of Batch Normalization
2,1103784103629938688,2210861,Jacob Andreas,"['New paper: we often want to know ""how compositional"" learned representation models are. Here\'s a super-simple evaluation called TRE (inspired by formal &amp; vector-space semantics) &amp; explorations of compositionality in a bunch of different learning problems. <LINK> <LINK>', '(my first single-author paper!)', 'And it looks like people are already using a modified version of this approach as a regularizer to encourage better generalization in few-shot learning: https://t.co/RAUrmpyg3n.', '@TheGradient Sections 6 and 7 are both about multi-level compositions (unless that means something different to you)', '@kchonyc yeah the whole getting-cited-pre-preprint thing will take some getting used to']",https://arxiv.org/abs/1902.07181,"Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization. ",Measuring Compositionality in Representation Learning
3,1103420631167709184,1411875962,Sarah Rugheimer,['Paul Rimmer and I have a new paper out on HCN formation in early Earth-like exoplanet atmospheres! Check it out: <LINK> @pbrandonrimmer'],https://arxiv.org/abs/1902.08022,"Hydrogen cyanide (HCN) is a key feedstock molecule for the production of life's building blocks. The formation of HCN in an N$_2$-rich atmospheres requires first that the triple bond between N$\equiv$N be severed, and then that the atomic nitrogen find a carbon atom. These two tasks can be accomplished via photochemistry, lightning, impacts, or volcanism. The key requirements for producing appreciable amounts of HCN are the free availability of N$_2$ and a local carbon to oxygen ratio of C/O $\geq 1$. We discuss the chemical mechanisms by which HCN can be formed and destroyed on rocky exoplanets with Earth-like N$_2$ content and surface water inventories, varying the oxidation state of the dominant carbon-containing atmospheric species. HCN is most readily produced in an atmosphere rich in methane (CH$_4$) or acetylene (C$_2$H$_2$), but can also be produced in significant amounts ($> 1$ ppm) within CO-dominated atmospheres. Methane is not necessary for the production of HCN. We show how destruction of HCN in a CO$_2$-rich atmosphere depends critically on the poorly-constrained energetic barrier for the reaction of HCN with atomic oxygen. We discuss the implications of our results for detecting photochemically produced HCN, for concentrating HCN on the planet's surface, and its importance for prebiotic chemistry. ",Hydrogen Cyanide in Nitrogen-Rich Atmospheres of Rocky Exoplanets
4,1102834077453950976,237235302,Matthijs R. Koot,"['Discrimination in the Age of Algorithms (2019), by Kleinberg, Ludwig, @m_sendhil &amp; @CassSunstein <LINK> Direct link to paper (.pdf, 45p) <LINK> &lt; on improving algorithmic accountability by forcing a new level of specificity. c @_cryptome_ #ethics']",https://arxiv.org/abs/1902.03731,"The law forbids discrimination. But the ambiguity of human decision-making often makes it extraordinarily hard for the legal system to know whether anyone has actually discriminated. To understand how algorithms affect discrimination, we must therefore also understand how they affect the problem of detecting discrimination. By one measure, algorithms are fundamentally opaque, not just cognitively but even mathematically. Yet for the task of proving discrimination, processes involving algorithms can provide crucial forms of transparency that are otherwise unavailable. These benefits do not happen automatically. But with appropriate requirements in place, the use of algorithms will make it possible to more easily examine and interrogate the entire decision process, thereby making it far easier to know whether discrimination has occurred. By forcing a new level of specificity, the use of algorithms also highlights, and makes transparent, central tradeoffs among competing values. Algorithms are not only a threat to be regulated; with the right safeguards in place, they have the potential to be a positive force for equity. ",Discrimination in the Age of Algorithms
5,1102376713525317632,3078444975,Jayadev Acharya,"['New paper with @canondetortugas, @chrismdesa, and Karthik Sridharan: Distributed Learning with Sublinear Communication. Available at <LINK>']",https://arxiv.org/abs/1902.11259,"In distributed statistical learning, $N$ samples are split across $m$ machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine. This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup. However, in high-dimensional settings, where the number examples is smaller than the number of features (""dimension""), the speedup afforded by distributed learning may be overshadowed by the cost of communicating a single example. This paper investigates the following question: When is it possible to learn a $d$-dimensional model in the distributed setting with total communication sublinear in $d$? Starting with a negative result, we show that for learning $\ell_1$-bounded or sparse linear models, no algorithm can obtain optimal error until communication is linear in dimension. Our main result is that that by slightly relaxing the standard boundedness assumptions for linear models, we can obtain distributed algorithms that enjoy optimal error with communication logarithmic in dimension. This result is based on a family of algorithms that combine mirror descent with randomized sparsification/quantization of iterates, and extends to the general stochastic convex optimization model. ",Distributed Learning with Sublinear Communication
6,1102272124239126528,152741512,Prof. Andy Way,"['New paper ""No Padding Please: Efficient Neural Handwriting Recognition"" with Gideon Maillette de Buy Wenniger from @AdaptCentre at @DublinCityUni &amp; Lambert Schomaker from @univgroningen <LINK> <LINK>']",https://arxiv.org/abs/1902.11208,"Neural handwriting recognition (NHR) is the recognition of handwritten text with deep learning models, such as multi-dimensional long short-term memory (MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved state-of-the art results on handwritten text recognition tasks. While multi-directional MDLSTM-layers have an unbeaten ability to capture the complete context in all directions, this strength limits the possibilities for parallelization, and therefore comes at a high computational cost. In this work we develop methods to create efficient MDLSTM-based models for NHR, particularly a method aimed at eliminating computation waste that results from padding. This proposed method, called example-packing, replaces wasteful stacking of padded examples with efficient tiling in a 2-dimensional grid. For word-based NHR this yields a speed improvement of factor 6.6 over an already efficient baseline of minimal padding for each batch separately. For line-based NHR the savings are more modest, but still significant. In addition to example-packing, we propose: 1) a technique to optimize parallelization for dynamic graph definition frameworks including PyTorch, using convolutions with grouping, 2) a method for parallelization across GPUs for variable-length example batches. All our techniques are thoroughly tested on our own PyTorch re-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM dataset shows that our models are performing similar to earlier implementations of state-of-the-art models. Our efficient NHR model and some of the reusable techniques discussed with it offer ways to realize relatively efficient models for the omnipresent scenario of variable-length inputs in deep learning. ",No Padding Please: Efficient Neural Handwriting Recognition
7,1101634552689131523,970560725432315904,Gabriel Huang,"['Our new paper on learning to cluster ""Centroid Networks for Few-Shot Clustering and Unsupervised Few-Shot Classification"" is on arXiv !\nMany thanks to my coauthors @hugo_larochelle and @SimonLacosteJ <LINK>', 'Check out the @shortscienceorg version for a quick executive summary :)\nhttps://t.co/qLQaLN4RDq']",https://arxiv.org/abs/1902.08605,"We show that several popular few-shot learning benchmarks can be solved with varying degrees of success without using support set Labels at Test-time (LT). To this end, we introduce a new baseline called Centroid Networks, a modification of Prototypical Networks in which the support set labels are hidden from the method at test-time and have to be recovered through clustering. A benchmark that can be solved perfectly without LT does not require proper task adaptation and is therefore inadequate for evaluating few-shot methods. In practice, most benchmarks cannot be solved perfectly without LT, but running our baseline on any new combinations of architectures and datasets gives insights on the baseline performance to be expected from leveraging a good representation, before any adaptation to the test-time labels. ","Are Few-Shot Learning Benchmarks too Simple ? Solving them without Task
  Supervision at Test-Time"
8,1101483108036165633,1064262393981820928,Benedikt Diemer,"['Shameless self-promotion: new paper is out! IllustrisTNG fits HI and H2 observations better than it has any right to. Some interesting tensions though, including too much neutral gas overall and perhaps overly effective stripping. All data is public!\n\n<LINK>']",https://arxiv.org/abs/1902.10714,"We have recently developed a post-processing framework to estimate the abundance of atomic and molecular hydrogen (HI and H2, respectively) in galaxies in large-volume cosmological simulations. Here we compare the HI and H2 content of IllustrisTNG galaxies to observations. We mostly restrict this comparison to $z \approx 0$ and consider six observational metrics: the overall abundance of HI and H2, their mass functions, gas fractions as a function of stellar mass, the correlation between H2 and star formation rate, the spatial distribution of gas, and the correlation between gas content and morphology. We find generally good agreement between simulations and observations, particularly for the gas fractions and the HI mass-size relation. The H2 mass correlates with star formation rate as expected, revealing an almost constant depletion time that evolves up to z = 2 as observed. However, we also discover a number of tensions with varying degrees of significance, including an overestimate of the total neutral gas abundance at z = 0 by about a factor of two and a possible excess of satellites with no or very little neutral gas. These conclusions are robust to the modelling of the HI/H2 transition. In terms of their neutral gas properties, the IllustrisTNG simulations represent an enormous improvement over the original Illustris run. All data used in this paper are publicly available as part of the IllustrisTNG data release. ",Atomic and molecular gas in IllustrisTNG galaxies at low redshift
9,1101378463905337345,2778729792,Saquib Sarfraz,"['FINCH, a new clustering algorithm, absolutily no hyperparameters ,  no need to specify no. of clusters. Scalable(Memory O(N)), very fast (ON(logN)) clusters ~8 million samples in 18 minutes on standard CPU.  #DataAnalytics Paper: <LINK> Code:  to appear #CVPR2019']",https://arxiv.org/abs/1902.11266,"We present a new clustering method in the form of a single clustering equation that is able to directly discover groupings in the data. The main proposition is that the first neighbor of each sample is all one needs to discover large chains and finding the groups in the data. In contrast to most existing clustering algorithms our method does not require any hyper-parameters, distance thresholds and/or the need to specify the number of clusters. The proposed algorithm belongs to the family of hierarchical agglomerative methods. The technique has a very low computational overhead, is easily scalable and applicable to large practical problems. Evaluation on well known datasets from different domains ranging between 1077 and 8.1 million samples shows substantial performance gains when compared to the existing clustering techniques. ",Efficient Parameter-free Clustering Using First Neighbor Relations
10,1101307155141951488,30775583,Dennis V. Perepelitsa 🇺🇦🇺🇲,"['Here\'s a new paper by our post-docs Sanghoon and Qipeng, and colleague @ronbelmont, performing a closure test of the popular ""non-flow subtraction"" methods in small systems. For LHC data, they are mostly reasonable, but take care in RHIC p+A (or smaller!): <LINK> <LINK>']",https://arxiv.org/abs/1902.11290,"Two particle correlations have been used extensively to study hydrodynamic flow patterns in heavy-ion collisions. In small collision systems, such as $p$$+$$p$ and $p$$+$$A$, where particle multiplicities are much smaller than in $A$$+$$A$ collisions, non-flow effects from jet correlations, momentum conservation, particle decays, etc. can be significant, even when imposing a large pseudorapidity gap between the particles. A number of techniques to subtract the non-flow contribution in two particle correlations have been developed by experiments at the Large Hadron Collider (LHC) and then used to measure particle flow in $p$$+$$p$ and $p$$+$Pb collisions. Recently, experiments at the Relativistic Heavy Ion Collider (RHIC) have explored the possibility of adopting these techniques for small collision systems at lower energies. In this paper, we test these techniques using Monte Carlo generators PYTHIA and HIJING, which do not include any collective flow, and AMPT, which does. We find that it is crucial to examine the results of such tests both for correlations integrated over particle transverse momentum $p_T$ and differentially as a function of $p_T$. Our results indicate reasonable non-flow subtraction for $p$$+$$p$ collisions at the highest LHC energies, while failing if applied to $p$$+$$p$ collisions at RHIC. In the case of $p$$+$Au collisions at RHIC, both HIJING and AMPT results indicate a substantial over-subtraction of non-flow for $p_{T}\gtrsim1~{\rm GeV}/c$ and hence an underestimate of elliptic flow. ","Examination of Flow and Non-Flow Factorization Methods in Small
  Collision Systems"
11,1101296578327912449,128070661,Ernesto Galvão,"['If B=A and C=A, then B=C. But what if A,B,C are outcomes of probabilistic processes/quantum measurements? In a new paper I and Daniel Brod @Danjost describe what we can infer from a set of two-state comparisons (bonus: 2 quantum info applications). <LINK>', ""And here's a short blog post in which I explain the main ideas of the paper: https://t.co/N1lH7cDJPH""]",https://arxiv.org/abs/1902.11039,"Suppose we have $N$ quantum systems in unknown states $\lvert\psi_i \rangle $, but know the value of some pairwise overlaps $\left| \langle \psi_k \lvert \psi_l \rangle \right|^2$. What can we say about the values of the unknown overlaps? We provide a complete answer to this problem for three pure states and two given overlaps, and a way to obtain bounds for the general case. We discuss how the answer contrasts from that of a classical model featuring only coherence-free, diagonal states, and describe three applications: basis-independent coherence witnesses, dimension witnesses, and characterisation of multi-photon indistinguishability. ",Quantum and classical bounds for two-state overlaps
12,1101070845127081984,38448896,Georgi Tinchev,"['Our new work on deep robot localization on a CPU is now out!\nCheck out the paper: <LINK>\nAnd our video: <LINK>\n\nThanks to Adrian, @MauriceFallon @oxfordrobots and @UniofOxford for making this possible!']",https://arxiv.org/abs/1902.10194,"Localization in challenging, natural environments such as forests or woodlands is an important capability for many applications from guiding a robot navigating along a forest trail to monitoring vegetation growth with handheld sensors. In this work we explore laser-based localization in both urban and natural environments, which is suitable for online applications. We propose a deep learning approach capable of learning meaningful descriptors directly from 3D point clouds by comparing triplets (anchor, positive and negative examples). The approach learns a feature space representation for a set of segmented point clouds that are matched between a current and previous observations. Our learning method is tailored towards loop closure detection resulting in a small model which can be deployed using only a CPU. The proposed learning method would allow the full pipeline to run on robots with limited computational payload such as drones, quadrupeds or UGVs. ","Learning to See the Wood for the Trees: Deep Laser Localization in Urban
  and Natural Environments on a CPU"
13,1101025177054441474,75249390,Axel Maas,"['I have published a new paper, in which I look at the interaction between scalar particles and gluons, see <LINK> #np3']",https://arxiv.org/abs/1902.10568,"The question of whether confining effects are visible in correlation functions is a long-standing one. Complementing investigations on the propagators of fundamental and adjoint scalar matter particles here the quenched scalar-gluon vertex is investigated. For this purpose a multitude of lattice setups in two, three, and four dimensions is analyzed in quenched SU(2) lattice gauge theory. Though both cases are quantitatively different, neither a qualitative difference nor any singularities are observed. ",The quenched SU(2) scalar-gluon vertex in minimal Landau gauge
14,1101004766413553664,1005035166513991681,Andreas K. Maier,"[""Joint training using confocal laser endoscopy images from oral cavity and vocal cords improves tissue classification as Marc's new @arxiv paper demonstrates.\n\n#freeaccess link:\n<LINK>\n\n@FAU_Germany #deeplearning #machinelearning @BVM_Community <LINK>""]",https://arxiv.org/abs/1902.08985,"Squamous Cell Carcinoma (SCC) is the most common cancer type of the epithelium and is often detected at a late stage. Besides invasive diagnosis of SCC by means of biopsy and histo-pathologic assessment, Confocal Laser Endomicroscopy (CLE) has emerged as noninvasive method that was successfully used to diagnose SCC in vivo. For interpretation of CLE images, however, extensive training is required, which limits its applicability and use in clinical practice of the method. To aid diagnosis of SCC in a broader scope, automatic detection methods have been proposed. This work compares two methods with regard to their applicability in a transfer learning sense, i.e. training on one tissue type (from one clinical team) and applying the learnt classification system to another entity (different anatomy, different clinical team). Besides a previously proposed, patch-based method based on convolutional neural networks, a novel classification method on image level (based on a pre-trained Inception V.3 network with dedicated preprocessing and interpretation of class activation maps) is proposed and evaluated. The newly presented approach improves recognition performance, yielding accuracies of 91.63% on the first data set (oral cavity) and 92.63% on a joint data set. The generalization from oral cavity to the second data set (vocal folds) lead to similar area-under-the-ROC curve values than a direct training on the vocal folds data set, indicating good generalization. ","Transferability of Deep Learning Algorithms for Malignancy Detection in
  Confocal Laser Endomicroscopy Images from Different Anatomical Locations of
  the Upper Gastrointestinal Tract"
15,1100867398683504640,700704725826572290,"Joe Guinness, valued customer","['New paper on arxiv! An alternative to the derived motion winds algorithm for estimating winds from geostationary satellite images.\n<LINK>', ""This was part of Indranil Sahoo's thesis. He's just accepted a faculty position at Virginia Commonwealth University.""]",https://arxiv.org/abs/1902.09653,"Geostationary satellites collect high-resolution weather data comprising a series of images which can be used to estimate wind speed and direction at different altitudes. The Derived Motion Winds (DMW) Algorithm is commonly used to process these data and estimate atmospheric winds by tracking features in images taken by the GOES-R series of the NOAA geostationary meteorological satellites. However, the wind estimates from the DMW Algorithm are sparse and do not come with uncertainty measures. This motivates us to statistically model wind motions as a spatial process drifting in time. We propose a covariance function that depends on spatial and temporal lags and a drift parameter to capture the wind speed and wind direction. We estimate the parameters by local maximum likelihood. Our method allows us to compute standard errors of the estimates, enabling spatial smoothing of the estimates using a Gaussian kernel weighted by the inverses of the estimated variances. We conduct extensive simulation studies to determine the situations where our method performs well. The proposed method is applied to the GOES-15 brightness temperature data over Colorado and reduces prediction error of brightness temperature compared to the DMW Algorithm. ","Estimating Atmospheric Motion Winds from Satellite Image Data using
  Space-time Drift Models"
16,1100818380892692481,1023452666712666113,Hadi Salman,"['[1/6] How tight can convex-relaxed robustness verification for neural networks be in practice? We thoroughly investigate this in our new paper <LINK>!\n\nIn collaboration w\\ @TheGregYang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. Special thanks to @ilyaraz2! <LINK>', '[2/6] We unify all existing LP-relaxed verifiers under a general convex relaxation framework.', '[3/6] We perform extensive experiments, amounting to more than 22 CPU-Years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks.', '[4/6] We find the exact solution does not significantly improve upon the gap between exact verifiers and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR-10 datasets.', '[5/6] Our results suggest there is an inherent barrier to tight robustness verification for the large class of methods captured by our framework.', '[6/6] Finally, we discuss possible causes of this barrier and potential future directions for bypassing it.']",http://arxiv.org/abs/1902.08722,"Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of robustness verification. We further prove strong duality between the primal and dual problems under very mild conditions. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it. Our code and trained models are available at this http URL . ","A Convex Relaxation Barrier to Tight Robustness Verification of Neural
  Networks"
17,1100817646520418304,1092693586263457792,Greg Yang,"['[1/4] Everybody knows adversarial examples are a problem, and a lot of people tried to provably verify NN robustness. But seems convex relaxation alone runs into a theoretical and empirical barrier --- not tight enough! See our new paper <LINK> <LINK>', '[2/4] This may be obvious to some, but at least 10 papers last year kept pushing convex relaxation, like @RICEric22 and @zicokolter’s LP method and recently @pushmeet’s January paper on interval bound propagation.', '[3/4] Algorithms that bypass this barrier include Raghunathan’s SDP formulations, MILP from Tedrake’s group, SMT (reluplex), Lipschitz constant bound, and hybrid approaches ← we encourage folks to explore these ideas further!', '[4/4] Collaboration with our super duper awesome AI resident @hadisalman94, along with Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. Also special thanks to @ilyaraz2!', '@zicokolter Thanks Zico! You are right, and indeed we discuss this more carefully in the conclusion of the paper: ""In general, none of [the above] are strictly better than the convex relaxation approach, sacrificing either speed or accuracy"" --- so 100% on the same page here!', '@zicokolter Yep big fans of the randomized smoothing @deepcohen (very impressed by this) and Wasserstein robustness @RICEric22 papers! :)']",http://arxiv.org/abs/1902.08722,"Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of robustness verification. We further prove strong duality between the primal and dual problems under very mild conditions. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it. Our code and trained models are available at this http URL . ","A Convex Relaxation Barrier to Tight Robustness Verification of Neural
  Networks"
18,1100718536618520577,1959599407,Hamid Rezatofighi,"['We are thrilled to announce our new paper about bounding box regression, the most trivial task on computer vision, has been accepted in CVPR 2019. All codes and models will be released soon.                <LINK>']",https://arxiv.org/abs/1902.09630,"Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that $IoU$ can be directly used as a regression loss. However, $IoU$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of $IoU$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, $IoU$ based, and new, $GIoU$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO. ","Generalized Intersection over Union: A Metric and A Loss for Bounding
  Box Regression"
19,1100677306065055744,398987599,Andrea Rapisarda,['New paper online <LINK>'],https://arxiv.org/abs/1902.10035,"Growing network models have been shown to display emergent quantum statistics when nodes are associated to a fitness value describing the intrinsic ability of a node to acquire new links. Recently it has been shown that quantum statistics emerge also in a growing simplicial complex model called Network Geometry with Flavor which allow for the description of many-body interaction between the nodes. This model depend on an external parameter called flavor that is responsible for the underlying topology of the simplicial complex. When the flavor takes the value $s=-1$ the $d$-dimensional simplicial complex is a manifold in which every $(d-1)$-dimensional face can only have an incidence number $n_{\alpha}\in\{0,1\}$. In this case the faces of the simplicial complex are naturally described by the Bose-Einstein, Boltzmann and Fermi-Dirac distribution depending on their dimension. In this paper we extent the study of Network Geometry with Flavor to fractional values of the flavor $s=-1/m$ in which every $(d-1)$-dimensional face can only have incidence number $n_{\alpha}\in\{0,1,2,\dots, m\}$. We show that in this case the statistical properties of the faces of the simplicial complex are described by the Bose-Einstein or the Fermi-Dirac distribution only. Finally we comment on the spectral properties of the networks constituting the underlying structure of the considered simplicial complexes. ",Quantum statistics in Network Geometry with Fractional Flavor
20,1100619058649550848,1030693296,Nicolas Martin,"['New paper: we (Ibata, Bellazzini, @kmalhan07, Bianchini and myself) show that the Fimbulthul stream is produced by the Omega Centauri stream. <LINK> 1/3 <LINK>', ""Omega Cen is the most massive Milky Way globular cluster and looks quite odd. It's possibly the nuclei of an accreted dwarf galaxy. We don't shed light on this but we show where stripped Omega Cen stars are and were to look for the leftovers of its progenitor if it exists. 2/3"", 'This stream was found as part of our effort searching for nearby stellar streams in #GaiaDR2. That search, led by @khayati and Rodrigo Ibata has been quite successful so far (https://t.co/jLLrImcNGD)! https://t.co/JvBYE7i6rv']",https://arxiv.org/abs/1902.09544,"Omega Centauri is the most massive globular cluster of the Milky Way, and possesses many peculiar properties. In particular, the cluster contains distinct multiple stellar populations, with a large spread in metallicity and different kinematics as a function of light elements abundance, implying that it formed over an extended period of time. This has lead to the suggestion that $\omega$ Cen is the remnant core of an accreted dwarf galaxy. If this scenario is correct, $\omega$ Cen should be tidally limited, and one should expect to find tidal debris spread along its orbit. Here we use N-body simulations to show that the recently-discovered `Fimbulthul' structure, identified in the second data release (DR2) of the Gaia mission, is the long sought-for tidal stream of $\omega$ Cen, extending up to $28\deg$ from the cluster. Follow-up high-resolution spectroscopy of 5 stars in the stream show that they are closely-grouped in velocity, and have metallicities consistent with having originated in that cluster. Guided by our N-body simulations, we devise a selection filter that we apply to Gaia data to also uncover the portion of the stream in the highly-contaminated and crowded field within $10\deg$ of $\omega$ Cen. Further modelling of the stream may help to constrain the dynamical history of the dwarf galaxy progenitor of this disrupting system and guide future searches for its remnant stars in the Milky Way. ","Identification of the Long Stellar Stream of the Prototypical Massive
  Globular Cluster $\omega$ Centauri"
21,1100572332492505088,952949678533849088,Kareem El-Badry,"['New paper about superbubbles driven by clustered SNe. Really proud of this one. We derive an analytic solution for superbubbles that includes cooling, updating Weaver et al. A suite of simulations validates the model. <LINK> <LINK>']",http://arxiv.org/abs/1902.09547,"We use spherically symmetric hydrodynamic simulations to study the dynamical evolution and internal structure of superbubbles (SBs) driven by clustered supernovae (SNe), focusing on the effects of thermal conduction and cooling in the interface between the hot bubble interior and cooled shell. Our simulations employ an effective diffusivity to account for turbulent mixing from nonlinear instabilities that are not captured in 1D. The conductive heat flux into the shell is balanced by a combination of cooling in the interface and evaporation of shell gas into the bubble interior. This evaporation increases the density, and decreases the temperature, of the SB interior by more than an order of magnitude relative to simulations without conduction. However, most of the energy conducted into the interface is immediately lost to cooling, reducing the evaporative mass flux required to balance conduction. As a result, the evaporation rate is typically a factor of $\sim$3-30 lower than predicted by the classical similarity solution of Weaver et al. (1977), which neglects cooling. Blast waves from the first $\sim$30 SNe remain supersonic in the SB interior because reduced evaporation from the interface lowers the mass they sweep up in the hot interior. Updating the Weaver solution to include cooling, we construct a new analytic model to predict the cooling rate, evaporation rate, and temporal evolution of SBs. The cooling rate, and hence the hot gas mass, momentum, and energy delivered by SBs, is set by the ambient ISM density and the efficiency of nonlinear mixing at the bubble/shell interface. ",Evolution of supernovae-driven superbubbles with conduction and cooling
22,1100572283821871104,1019760963569049601,Almog Yalinewich,['Objects in radio telescope are closer than they appear. Our new paper shows the distances to fast radio burst is much smaller than the upper bound set by the dispersion measure. <LINK>'],https://arxiv.org/abs/1902.10120,"The distances to fast radio bursts (FRBs) are crucial for understanding their underlying engine, and for their use as cosmological probes. In this paper, we provide three statistical estimates of the distance to ASKAP FRBs. First, we show that the number of events of similar luminosity in ASKAP does not scale as distance cubed, as one would expect, when directly using the observed dispersion measure (DM) to infer distance. Second, by comparing the average DMs of FRBs observed with different instruments, we estimated the average redshift of ASKAP FRBs to be $z\sim 0.01$ using CHIME and ASKAP, and $z\lesssim0.07$ using Parkes and ASKAP. Both values are much smaller than the upper limit $z\sim0.3$ estimated directly from the DM. Third, we cross-correlate the locations of the ASKAP FRBs with existing large-area redshift surveys, and see a 3$\sigma$ correlation with the 2MASS Redshift Survey and a 5$\sigma$ correlation with the HI Parkes All Sky Survey at $z\sim0.007$. This corresponds well with the redshift of the most likely host galaxy of ASKAP FRB 171020, which is at $z=0.00867$. These arguments combined suggest an extremely nearby origin of ASKAP FRBs and a local environment with accumulated electrons that contribute a DM of several hundred pc/cm$^3$, which should be accounted for in theoretical models. ",Statistical inference of the distance to ASKAP FRBs
23,1100507758896599040,323231207,Bruno Merín,['Check our new paper applying #MachineLearning to @esagaia data in Astro-ph (just submitted) : A census of rho Oph candidate members from \u2066@ESAGaia\u2069 DR2. <LINK>'],https://arxiv.org/abs/1902.07600,"The Ophiuchus cloud complex is one of the best laboratories to study the earlier stages of the stellar and protoplanetary disc evolution. The wealth of accurate astrometric measurements contained in the Gaia Data Release 2 can be used to update the census of Ophiuchus member candidates. We seek to find potential new members of Ophiuchus and identify those surrounded by a circumstellar disc. We constructed a control sample composed of 188 bona fide Ophiuchus members. Using this sample as a reference we applied three different density-based machine learning clustering algorithms (DBSCAN, OPTICS, and HDBSCAN) to a sample drawn from the Gaia catalogue centred on the Ophiuchus cloud. The clustering analysis was applied in the five astrometric dimensions defined by the three-dimensional Cartesian space and the proper motions in right ascension and declination. The three clustering algorithms systematically identify a similar set of candidate members in a main cluster with astrometric properties consistent with those of the control sample. The increased flexibility of the OPTICS and HDBSCAN algorithms enable these methods to identify a secondary cluster. We constructed a common sample containing 391 member candidates including 166 new objects, which have not yet been discussed in the literature. By combining the Gaia data with 2MASS and WISE photometry, we built the spectral energy distributions from 0.5 to $22\microm$ for a subset of 48 objects and found a total of 41 discs, including 11 Class II and 1 Class III new discs. Density-based clustering algorithms are a promising tool to identify candidate members of star forming regions in large astrometric databases. If confirmed, the candidate members discussed in this work would represent an increment of roughly 40% of the current census of Ophiuchus. ",A census of $\rho$ Oph candidate members from Gaia DR2
24,1100465252196593668,22216766,Noah Stephens-Davidowitz,"['New (very short) paper: A time-distance trade-off for GDD with preprocessing---Instantiating the DLW heuristic <LINK>', '@DemocraticLuntz Not yet I think. They were kind enough to share early copies with me.', '@DemocraticLuntz Oops.. apparently it went up on eprint a couple of days ago: https://t.co/4FaWgq9gDg . Sorry for the confusion :-/.']",https://arxiv.org/abs/1902.08340,"For $0 \leq \alpha \leq 1/2$, we show an algorithm that does the following. Given appropriate preprocessing $P(\mathcal{L})$ consisting of $N_\alpha := 2^{O(n^{1-2\alpha} + \log n)}$ vectors in some lattice $\mathcal{L} \subset \mathbb{R}^n$ and a target vector $\boldsymbol{t}\in \mathbb{R}^n$, the algorithm finds $\boldsymbol{y} \in \mathcal{L}$ such that $\|\boldsymbol{y}- \boldsymbol{t}\| \leq n^{1/2 + \alpha} \eta(\mathcal{L})$ in time $\mathrm{poly}(n) \cdot N_\alpha$, where $\eta(\mathcal{L})$ is the smoothing parameter of the lattice. The algorithm itself is very simple and was originally studied by Doulgerakis, Laarhoven, and de Weger (to appear in PQCrypto, 2019), who proved its correctness under certain reasonable heuristic assumptions on the preprocessing $P(\mathcal{L})$ and target $\boldsymbol{t}$. Our primary contribution is a choice of preprocessing that allows us to prove correctness without any heuristic assumptions. Our main motivation for studying this is the recent breakthrough algorithm for IdealSVP due to Hanrot, Pellet--Mary, and Stehl\'e (to appear in Eurocrypt, 2019), which uses the DLW algorithm as a key subprocedure. In particular, our result implies that the HPS IdealSVP algorithm can be made to work with fewer heuristic assumptions. Our only technical tool is the discrete Gaussian distribution over $\mathcal{L}$, and in particular, a lemma showing that the one-dimensional projections of this distribution behave very similarly to the continuous Gaussian. This lemma might be of independent interest. ","A time-distance trade-off for GDD with preprocessing---Instantiating the
  DLW heuristic"
25,1100282433692491776,195122448,J W F Valle,['NEW ARXIV PAPER [1902.08962] Peng Chen et al : Predicting neutrino oscillations with bi-large lepton mixing matrices <LINK> <LINK>'],https://arxiv.org/abs/1902.08962,"We propose two schemes for the lepton mixing matrix $U = U_l^\dagger U_\nu$, where $U = U_l$ refers to the charged sector, and $U_\nu$ denotes the neutrino diagonalization matrix. We assume $U_\nu$ to be CP conserving and its three angles to be connected with the Cabibbo angle in a simple manner. CP violation arises solely from the $U_l$, assumed to have the CKM form, $U_l\simeq V_{\rm CKM}$, suggested by unification. Oscillation parameters depend on a single parameter, leading to narrow ranges for the ""solar"" and ""accelerator"" angles $\theta_{12}$ and $\theta_{23}$, as well as for the CP phase, predicted as $\delta_{\rm CP}\sim 1.3\pi$. ","Predicting neutrino oscillations with ""bi-large"" lepton mixing matrices"
26,1100183640208801792,2818695390,Sasho Nikolov,"['New paper with two amazing U of T undergrads <LINK>. The problem is geometric transportation/Earth Mover’s distance: transport one discrete distribution on R^d to another to minimize total distance traveled. We get (1+eps)-apx in time n*polylog (n=total support)', 'The key idea is that the classical embedding of Earth Mover’s distance into \\ell_1 can be reinterpreted as a preconditioner for the transportation problem, to plug into Sherman’s generalized preconditioning framework.']",https://arxiv.org/abs/1902.08384,"In the geometric transportation problem, we are given a collection of points $P$ in $d$-dimensional Euclidean space, and each point is given a supply of $\mu(p)$ units of mass, where $\mu(p)$ could be a positive or a negative integer, and the total sum of the supplies is $0$. The goal is to find a flow (called a transportation map) that transports $\mu(p)$ units from any point $p$ with $\mu(p) > 0$, and transports $-\mu(p)$ units into any point $p$ with $\mu(p) < 0$. Moreover, the flow should minimize the total distance traveled by the transported mass. The optimal value is known as the transportation cost, or the Earth Mover's Distance (from the points with positive supply to those with negative supply). This problem has been widely studied in many fields of computer science: from theoretical work in computational geometry, to applications in computer vision, graphics, and machine learning. In this work we study approximation algorithms for the geometric transportation problem. We give an algorithm which, for any fixed dimension $d$, finds a $(1+\varepsilon)$-approximate transportation map in time nearly-linear in $n$, and polynomial in $\varepsilon^{-1}$ and in the logarithm of the total supply. This is the first approximation scheme for the problem whose running time depends on $n$ as $n\cdot \mathrm{polylog}(n)$. Our techniques combine the generalized preconditioning framework of Sherman, which is grounded in continuous optimization, with simple geometric arguments to first reduce the problem to a minimum cost flow problem on a sparse graph, and then to design a good preconditioner for this latter problem. ",Preconditioning for the Geometric Transportation Problem
27,1100145932761526272,2917258349,Dr Gareth Nye,['New paper held on @arxiv @assertpub_  exploring metabolism and microvascular networks in the placenta <LINK>\n\n@MFH_Research @IFPA_Official #placenta #ResearchHighlight <LINK>'],https://arxiv.org/abs/1902.08578,"The primary exchange units in the human placenta are terminal villi, in which fetal capillary networks are surrounded by a thin layer of villous tissue, separating fetal from maternal blood. To understand how the complex spatial structure of villi influences their function, we use an image-based theoretical model to study the effect of tissue metabolism on the transport of solutes from maternal blood into the fetal circulation. For solute that is taken up under first-order kinetics, we show that the transition between flow-limited and diffusion-limited transport depends on two new dimensionless parameters defined in terms of key geometric quantities, with strong solute uptake promoting flow-limited transport conditions. We present a simple algebraic approximation for solute uptake rate as a function of flow conditions, metabolic rate and villous geometry. For oxygen, accounting for nonlinear kinetics using physiological parameter values, our model predicts that villous metabolism does not significantly impact oxygen transfer to fetal blood, although the partitioning of fluxes between the villous tissue and the capillary network depends strongly on the flow regime. ","Quantifying the impact of tissue metabolism on solute transport in
  feto-placental microvascular networks"
28,1099917104466329600,2969696397,Ion Nechita,['New paper out with Motohisa Fukuda and Robert König on a software package for computing Haar integrals over the unitary group. \nPaper --&gt; <LINK>\nCode --&gt; <LINK>\nRead more about it --&gt; <LINK> <LINK>'],https://arxiv.org/abs/1902.08539,"We provide a computer algebra package called Random Tensor Network Integrator (RTNI). It allows to compute averages of tensor networks containing multiple Haar-distributed random unitary matrices and deterministic symbolic tensors. Such tensor networks are represented as multigraphs, with vertices corresponding to tensors or random unitaries and edges corresponding to tensor contractions. Input and output spaces of random unitaries may be subdivided into arbitrary tensor factors, with dimensions treated symbolically. The algorithm implements the graphical Weingarten calculus and produces a weighted sum of tensor networks representing the average over the unitary group. We illustrate the use of this algorithmic tool on some examples from quantum information theory, including entropy calculations for random tensor network states as considered in toy models for holographic duality. Mathematica and Python implementations are supplied. ",RTNI - A symbolic integrator for Haar-random tensor networks
29,1099854544127778819,2492016278,Adrian Raftery,['Our new paper on projecting the smoking-attributable mortality fraction for both sexes in 69 countries w Yicheng Li: <LINK> <LINK>'],https://arxiv.org/abs/1902.07791,"Smoking is one of the preventable threats to human health and is a major risk factor for lung cancer, upper aero-digestive cancer, and chronic obstructive pulmonary disease. Estimating and forecasting the smoking attributable fraction (SAF) of mortality can yield insights into smoking epidemics and also provide a basis for more accurate mortality and life expectancy projection. Peto et al. (1992) proposed a method to estimate the SAF using the lung cancer mortality rate as an indicator of exposure to smoking in the population of interest. Here we use the same method to estimate the all-age SAF (ASAF) for both genders for over 60 countries. We document a strong and cross-nationally consistent pattern of the evolution of the SAF over time. We use this as the basis for a new Bayesian hierarchical model to project future male and female ASAF from over 60 countries simultaneously. This gives forecasts as well as predictive distributions that can be used to find uncertainty intervals for any quantities of interest. We assess the model using out-of-sample predictive validation, and find that it provides good forecasts and well calibrated forecast intervals. ","Estimating and Forecasting the Smoking-Attributable Mortality Fraction
  for Both Genders Jointly in Over 60 Countries"
30,1099368294702706693,53464710,Eric Wong,"['New paper on Wasserstein adversarial examples, as a step towards considering convex metrics for perturbation regions that capture structure beyond norm-balls. \n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1902.07906,"A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by $\ell_p$ norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which naturally cover ""standard"" image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for projecting onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers. Code for all experiments in the paper is available at this https URL ",Wasserstein Adversarial Examples via Projected Sinkhorn Iterations
31,1098835565309353984,906604424851623936,Philippe Faist,"['How well exactly does quantum error correction mix with continuous symmetries? Find out in our new paper on the arXiv with Sepehr, @Comradevva, Grant, @FerPastawski, Patrick, and @preskill: <LINK>']",https://arxiv.org/abs/1902.07714,"Quantum error correction and symmetry arise in many areas of physics, including many-body systems, metrology in the presence of noise, fault-tolerant computation, and holographic quantum gravity. Here we study the compatibility of these two important principles. If a logical quantum system is encoded into $n$ physical subsystems, we say that the code is covariant with respect to a symmetry group $G$ if a $G$ transformation on the logical system can be realized by performing transformations on the individual subsystems. For a $G$-covariant code with $G$ a continuous group, we derive a lower bound on the error correction infidelity following erasure of a subsystem. This bound approaches zero when the number of subsystems $n$ or the dimension $d$ of each subsystem is large. We exhibit codes achieving approximately the same scaling of infidelity with $n$ or $d$ as the lower bound. Leveraging tools from representation theory, we prove an approximate version of the Eastin-Knill theorem: If a code admits a universal set of transversal gates and corrects erasure with fixed accuracy, then, for each logical qubit, we need a number of physical qubits per subsystem that is inversely proportional to the error parameter. We construct codes covariant with respect to the full logical unitary group, achieving good accuracy for large $d$ (using random codes) or $n$ (using codes based on $W$-states). We systematically construct codes covariant with respect to general groups, obtaining natural generalizations of qubit codes to, for instance, oscillators and rotors. In the context of the AdS/CFT correspondence, our approach provides insight into how time evolution in the bulk corresponds to time evolution on the boundary without violating the Eastin-Knill theorem, and our five-rotor code can be stacked to form a covariant holographic code. ",Continuous symmetries and approximate quantum error correction
32,1098588649392881665,3199605543,Afonso S. Bandeira,['New paper on Computational Hardness of Certifying Bounds on Constrained PCA Problems (and quadratic forms in the hypercube) <LINK>'],https://arxiv.org/abs/1902.07324,"Given a random $n \times n$ symmetric matrix $\boldsymbol W$ drawn from the Gaussian orthogonal ensemble (GOE), we consider the problem of certifying an upper bound on the maximum value of the quadratic form $\boldsymbol x^\top \boldsymbol W \boldsymbol x$ over all vectors $\boldsymbol x$ in a constraint set $\mathcal{S} \subset \mathbb{R}^n$. For a certain class of normalized constraint sets $\mathcal{S}$ we show that, conditional on certain complexity-theoretic assumptions, there is no polynomial-time algorithm certifying a better upper bound than the largest eigenvalue of $\boldsymbol W$. A notable special case included in our results is the hypercube $\mathcal{S} = \{ \pm 1 / \sqrt{n}\}^n$, which corresponds to the problem of certifying bounds on the Hamiltonian of the Sherrington-Kirkpatrick spin glass model from statistical physics. Our proof proceeds in two steps. First, we give a reduction from the detection problem in the negatively-spiked Wishart model to the above certification problem. We then give evidence that this Wishart detection problem is computationally hard below the classical spectral threshold, by showing that no low-degree polynomial can (in expectation) distinguish the spiked and unspiked models. This method for identifying computational thresholds was proposed in a sequence of recent works on the sum-of-squares hierarchy, and is believed to be correct for a large class of problems. Our proof can be seen as constructing a distribution over symmetric matrices that appears computationally indistinguishable from the GOE, yet is supported on matrices whose maximum quadratic form over $\boldsymbol x \in \mathcal{S}$ is much larger than that of a GOE matrix. ",Computational Hardness of Certifying Bounds on Constrained PCA Problems
33,1098507567393849344,995269493403373568,AlessandroIlBello,"['Our new paper on ""#Quantum Storage of Frequency-Multiplexed Heralded Single Photons"" is finally online!\n\nWe demonstrate storage of the whole spectrum of a photon composed by 15 different frequency modes. \n\nThis is the link to the @arxiv version:\n<LINK> <LINK>']",https://arxiv.org/abs/1902.06657,"We report on the quantum storage of a heralded frequency-multiplexed single photon in an integrated laser-written rare-earth doped waveguide. The single photon contains 15 discrete frequency modes separated by 261 MHz and spaning across 4 GHz. It is obtained from a non-degenerate photon pair created via cavity-enhanced spontaneous down conversion, where the heralding photon is at telecom wavelength and the heralded photon is at 606 nm. The frequency-multimode photon is stored in a praseodymium-doped waveguide using the atomic frequency comb (AFC) scheme, by creating multiple combs within the inhomogeneous broadening of the crystal. Thanks to the intrinsic temporal multimodality of the AFC scheme, each spectral bin includes 9 temporal modes, such that the total number of stored modes is about 130. We demonstrate that the storage preserves the non-classical properties of the single photon, and its normalized frequency spectrum. ",Quantum Storage of Frequency-Multiplexed Heralded Single Photons
34,1098499113392459776,41117987,Dr Michelle Collins,"[""I'm very excited to share my PhD student, @alexlgregory's first paper with you all! In this work (along with @ReadDark, @nfmartin1980), she presents new spectroscopic data for stars in the Tucana dwarf spheroidal galaxy. Her results are really exciting! <LINK>"", 'From a study of Tuc by Fraternali et al. in 2009, the central velocity dispersion was found to be very high, much higher than that of other dwarf galaxies in the Local Group. As such, it\'s consistent with living in a ""too big to fail"" halo. But, the uncertainties were large. https://t.co/IBPurDKKI6', 'We wanted to see whether Tuc really was so much more centrally dense, so we went to the VLT and observed a bunch of stars with FLAMES. @alexlgregory was able to confirm the high central density of Fraternali et al. It was consistent with one of these massive failure halos https://t.co/x209S2Hwbh', ""Then, with the help of @ReadDark, @alexlgregory modelled the dynamics of Tuc to learn about its dark halo. She finds Tuc is very centrally dense, consistent with hosting a cusp! Given Tuc's truncated star formation history, this agrees with expectations for dark matter heating! https://t.co/Ag8qjEmypi"", ""It's a really nice result (in my unbiased opinion 😉), and I recommend you all check out her paper!"", '@RbwBoe It’s worth it!', '@fdmtweets @alexlgregory @ReadDark @nfmartin1980 Thanks!']",https://arxiv.org/abs/1902.07228,"We present new FLAMES$+$GIRAFFE spectroscopy of 36 member stars in the isolated Local Group dwarf spheroidal galaxy Tucana. We measure a systemic velocity for the system of $v_{\mathrm{Tuc}}=216.7_{-2.8}^{+2.9}$kms$^{-1}$, and a velocity dispersion of $\sigma_{\mathrm{v,Tuc}}=14.4_{-2.3}^{+2.8}$kms$^{-1}$. We also detect a rotation gradient of $\frac{dv_{r}}{d\chi}=7.6^{+4.2}_{-4.3}$ kms$^{-1}$ kpc$^{-1}$, which reduces the systemic velocity to $v_{\mathrm{Tuc}}=215.2_{-2.7}^{+2.8}$ kms$^{-1}$ and the velocity dispersion to $\sigma_{v,\mathrm{Tuc}}=13.3_{-2.3}^{+2.7}$ kms$^{-1}$. We perform Jeans modelling of the density profile of Tucana using the line--of--sight velocities of the member stars. We find that it favours a high central density consistent with `pristine' subhalos in $\Lambda$CDM, and a massive dark matter halo ($\sim$$10^{10}$\,M$_\odot$) consistent with expectations from abundance matching. Tucana appears to be significantly more centrally dense than other isolated Local Group dwarfs, making it an ideal laboratory for testing dark matter models. ","Kinematics of the Tucana Dwarf Galaxy: An Unusually Dense Dwarf in the
  Local Group"
35,1098401025864589312,482360453,Martin Ingram,"['I just submitted a new paper: ""Gaussian Process Priors for Dynamic Paired Comparison Modelling"". Alternative to Elo &amp; Glicko for rating competitor skill over time. Preprint available here: <LINK>', ""Highlights: (1) not limited to random walk as latent dynamics; (2) can incorporate covariates; (3) performs well in evaluation. Illutrating (2), figure shows Nadal's skill on different surfaces. https://t.co/Z4pePWae6F"", 'Python code is available here if anyone would like to give it a try on their data: https://t.co/tsVtAjryAk', '@RankingSw Thanks Darren!', '@optibrebs Thanks! Hope the method is useful to you! :)']",https://arxiv.org/abs/1902.07378,"Dynamic paired comparison models, such as Elo and Glicko, are frequently used for sports prediction and ranking players or teams. We present an alternative dynamic paired comparison model which uses a Gaussian Process (GP) as a prior for the time dynamics rather than the Markovian dynamics usually assumed. In addition, we show that the GP model can easily incorporate covariates. We derive an efficient approximate Bayesian inference procedure based on the Laplace Approximation and sparse linear algebra. We select hyperparameters by maximising their marginal likelihood using Bayesian Optimisation, comparing the results against random search. Finally, we fit and evaluate the model on the 2018 season of ATP tennis matches, where it performs competitively, outperforming Elo and Glicko on log loss, particularly when surface covariates are included. ",Gaussian Process Priors for Dynamic Paired Comparison Modelling
36,1098207580801912832,144938323,Aymeric Spiga,"['""Mesoscale modeling of Venus’ bow-shape waves"" new paper submitted to Icarus by Maxence Lefèvre, yours truly and Sébastien Lebonnois, available as preprint on @arxiv <LINK>']",https://arxiv.org/abs/1902.07010,"The Akatsuki instrument LIR measured an unprecedented wave feature at the top of Venusian cloud layer. Stationary bow-shape waves of thousands of kilometers large lasting several Earth days have been observed over the main equatorial mountains. Here we use for the first time a mesoscale model of the Venus's atmosphere with high-resolution topography and fully coupled interactive radiative transfer computations. Mountain waves resolved by the model form large-scale bow shape waves with an amplitude of about 1.5 K and a size up to several decades of latitude similar to the ones measured by the Akatsuki spacecraft. The maximum amplitude of the waves appears in the afternoon due to an increase of the near-surface stability. Propagating vertically the waves encounter two regions of low static stability, the mixed layer between approximately 18 and 30 km and the convective layer between 50 and 55 km. Some part of the wave energy can pass through these regions via wave tunneling. These two layers act as wave filter, especially the deep atmosphere layer. The encounter with these layers generates trapped lee waves propagating horizontally. No stationary waves is resolved at cloud top over the polar regions because of strong circumpolar transient waves, and a thicker deep atmosphere mixed layer that filters most the mountain waves ",Mesoscale modeling of Venus' bow-shape waves
37,1098187348137967617,1098175843799629824,Babak Oskooei,"['Good morning. Can I introduce you to my Design paper on the issue of adding new research arms to randomised clinical trials?\nCurrently under review, the pre-print is at:\n\n<LINK> <LINK>']",https://arxiv.org/abs/1902.05336,"Background: Experimental treatments pass through various stages of development. If a treatment passes through early phase experiments, the investigators may want to assess it in a late phase randomised controlled trial. An efficient way to do this is adding it as a new research arm to an ongoing trial. This allows to add the new treatment while the existing arms continue. The familywise type I error rate (FWER) is often a key quantity of interest in any multi-arm trial. We set out to clarify how it should be calculated when new arms are added to a trial some time after it has started. Methods: We show how the FWER, any-pair and all-pairs powers can be calculated when a new arm is added to a platform trial. We extend the Dunnett probability and derive analytical formulae for the correlation between the test statistics of the existing pairwise comparison and that of the newly added arm. We also verify our analytical derivation via simulations. Results: Our results indicate that the FWER depends on the shared control arm information (i.e. individuals in continuous and binary outcomes and primary outcome events in time-to-event outcomes) from the common control arm patients and the allocation ratio. The FWER is driven more by the number of pairwise comparisons and the corresponding (pairwise) Type I error rates than by the timing of the addition of the new arms. The FWER can be estimated using \v{S}id\'{a}k's correction if the correlation between the test statistics of pairwise comparisons is less than 0:30. Conclusions: The findings we present in this article can be used to design trials with pre-planned deferred arms or to design new pairwise comparisons within an ongoing platform trial where control of the pairwise error rate (PWER) or FWER (for a subset of pairwise comparisons) is required. ","Adding new experimental arms to randomised clinical trials: impact on
  error rates"
38,1098134866506534912,21902101,Jim Geach,['New paper on @arXiv today - we measure the halo masses of z=1-2 quasars by using the fact that their dark matter halos lens the CMB... and we find tentative evidence for a luminosity dependence on the halo mass <LINK> @UniofHerts <LINK>'],https://arxiv.org/abs/1902.06955,"We measure the average deflection of cosmic microwave background photons by quasars at $\langle z \rangle =1.7$. Our sample is selected from the Sloan Digital Sky Survey to cover the redshift range $0.9\leq z\leq2.2$ with absolute i-band magnitudes of $M_i\leq-24$ (K-corrected to z=2). A stack of nearly 200,000 targets reveals an 8$\sigma$ detection of Planck's estimate of the lensing convergence towards the quasars. We fit the signal with a model comprising a Navarro-Frenk-White density profile and a 2-halo term accounting for correlated large scale structure, which dominates the observed signal. The best-fitting model is described by an average halo mass $\log_{10}(M_{\rm h}/h^{-1}M_\odot)=12.6\pm0.2$ and linear bias $b=2.7\pm0.3$ at $z=1.7$, in excellent agreement with clustering studies. We also report of a hint, at a 90% confidence level, of a correlation between the convergence amplitude and luminosity, indicating that quasars brighter than $M_i\lesssim -26$ reside in halos of typical mass ${M_{\rm h}\approx 10^{13}\,h^{-1}M_\odot}$, scaling roughly as ${M_{\rm h}\propto L_{\rm opt}^{3/4}}$ at ${M_i\lesssim-24}$, in good agreement with physically-motivated quasar demography models. Although we acknowledge this luminosity dependence is a marginal result, the observed $M_{\rm h}$-$L_{\rm opt}$ relationship could be interpreted as a reflection of the cutoff in the distribution of black hole accretion rates towards high Eddington ratios: the weak trend of $M_{\rm h}$ with $L_{\rm opt}$ observed at low luminosity becomes stronger for the most powerful quasars, which tend to be accreting close to the Eddington limit. ","The halo mass of optically-luminous quasars at z=1-2 measured via
  gravitational deflection of the cosmic microwave background"
39,1098107695931052032,1359666092,Kaustuv Datta,"[""Shameless self-promotion 😅: new paper with Ben Nachman and Andrew Larkoski on automating the construction of jet observables with ML. This is something we've been working on for a while, so I'm happy to finally see it out in the world!\n\n<LINK>"", '@HEPfeickert Thanks! :)', '@HEPfeickert ^also, goes without saying, we welcome feedback.', ""@Karl_Nordstrom Looking forward to it! :) Thanks for putting in the work! I'm sure Andrew (he's not on twitter, I think) will also see it as soon as it's on arXiv."", '@Karl_Nordstrom Also, as per our discussions at BOOST/your last paper, we do expect the N-sub bases, jet imgs, to perform similarly. I guess, it is a matter of choice wrt who wants to use what ML approach. \n\nIn addition, theres also these single observables now. 😇', '@claranellist @srrappoccio Thank you! :)', ""@Karl_Nordstrom I guess that's expected!"", ""Also, since Twitter still doesn't allow edits, tagging @BPNachman on here in a reply!  \n\n(sorry, I forgot you were on twitter! 😐)""]",https://arxiv.org/abs/1902.07180,"Machine-learning assisted jet substructure tagging techniques have the potential to significantly improve searches for new particles and Standard Model measurements in hadronic final states. Techniques with simple analytic forms are particularly useful for establishing robustness and gaining physical insight. We introduce a procedure to automate the construction of a large class of observables that are chosen to completely specify $M$-body phase space. The procedure is validated on the task of distinguishing $H\rightarrow b\bar{b}$ from $g\rightarrow b\bar{b}$, where $M=3$ and previous brute-force approaches to construct an optimal product observable for the $M$-body phase space have established the baseline performance. We then use the new method to design tailored observables for the boosted $Z'$ search, where $M=4$ and brute-force methods are intractable. The new classifiers outperform standard $2$-prong tagging observables, illustrating the power of the new optimization method for improving searches and measurement at the LHC and beyond. ",Automating the Construction of Jet Observables with Machine Learning
40,1098034229013868544,20703003,Peter B Denton,"['New neutrino paper out! <LINK>\n\nThis one is unique for me in that it is the shortest (3 pages) and the fastest from conception (3 weeks).\n\nIn it we build on previous work by Wang &amp; Zhou and find that a simple correction improves their result by a factor of ~20! <LINK>', 'This paper is also special to me because it means that I now have more papers than there are James Bond movies. How will I celebrate finishing a paper now?']",https://arxiv.org/abs/1902.07185,"For neutrino propagation in matter, we show that the Jarlskog invariant, which controls the size of true CP violation in neutrino oscillation appearance experiments, factorizes into three pieces: the vacuum Jarlskog invariant times two simple two-flavor matter resonance factors that control the matter effects for the solar and atmospheric resonances independently. If the solar effective matter potential and the atmospheric effective $\Delta m^2$ are chosen carefully for these two resonance factors, then the fractional corrections to this factorization are an impressive 0.04\% or smaller. We also show that the inverse of the square of the Jarlskog in matter ($1/\hat{J}^2$) is a fourth order polynomial in the matter potential which guarantees that it can be factored into two quadratics which immediately implies the functional form of our approximate, factorized expression. ","Simple and Precise Factorization of the Jarlskog Invariant for Neutrino
  Oscillations in Matter"
41,1097771749503049728,21902101,Jim Geach,['New paper on arXiv today led by @DaphneJacksonTr Fellow Carolyn Devereux @UniofHerts where we estimate the halo mass of radio galaxies at z~0.3 using the CMB... <LINK>'],https://arxiv.org/abs/1902.06581,"We present a new measurement of the linear bias of radio loud active galactic nuclei (RLAGN) at $z\approx0.3$ and $L_{\rm 1.4GHz}>10^{23}\,{\rm W\,Hz^{-1}}$ selected from the Best & Heckman (2012) sample, made by cross-correlating the RLAGN surface density with a map of the convergence of the weak lensing field of the cosmic microwave background from Planck. We detect the cross-power signal at a significance of $3\sigma$ and use the amplitude of the cross-power spectrum to estimate the linear bias of RLAGN, $b=2.5 \pm 0.8$, corresponding to a typical dark matter halo mass of $\log_{10}(M_{\rm h} /h^{-1} M_\odot)=14.0^{+0.3}_{-0.5}$. When RLAGN associated with optically-selected clusters are removed we measure a lower bias corresponding to $\log_{10}(M_{\rm h} /h^{-1} M_\odot)=13.7^{+0.4}_{-1.0}$. These observations support the view that powerful RLAGN typically inhabit rich group and cluster environments. ","The linear bias of radio galaxies at z~0.3 via cosmic microwave
  background lensing"
42,1097741790831091713,195122448,J W F Valle,['NEW PAPER [1902.05966] Sin Kyu Kang et al :                       \nScotogenic dark matter stability from gauged matter parity <LINK> <LINK>'],https://arxiv.org/abs/1902.05966,"We explore the idea that dark matter stability results from the presence of a matter-parity symmetry, arising naturally as a consequence of the spontaneous breaking of an extended $\mathrm{SU(3) \otimes SU(3)_L \otimes U(1)_X \otimes U(1)_{N}}$ electroweak gauge symmetry with fully gauged B-L. Using this framework we construct a theory for scotogenic dark matter and analyze its main features. ",Scotogenic dark matter stability from gauged matter parity
43,1097687830774669312,774299949580357632,Milan Curcic,['New paper: <LINK> #fortran'],https://arxiv.org/abs/1902.06714,"This paper describes neural-fortran, a parallel Fortran framework for neural networks and deep learning. It features a simple interface to construct feed-forward neural networks of arbitrary structure and size, several activation functions, and stochastic gradient descent as the default optimization algorithm. Neural-fortran also leverages the Fortran 2018 standard collective subroutines to achieve data-based parallelism on shared- or distributed-memory machines. First, I describe the implementation of neural networks with Fortran derived types, whole-array arithmetic, and collective sum and broadcast operations to achieve parallelism. Second, I demonstrate the use of neural-fortran in an example of recognizing hand-written digits from images. Finally, I evaluate the computational performance in both serial and parallel modes. Ease of use and computational performance are similar to an existing popular machine learning framework, making neural-fortran a viable candidate for further development and use in production. ",A parallel Fortran framework for neural networks and deep learning
44,1097506042592874497,3131054139,Pierre (피에르),['Published a new paper about GPS trackers (in)security with @EMHacktivity and Eiman: TL;DR: no security/privacy in gps trackers and backdoors/vulns everywhere. <LINK> #IoT #WhereIsWaldo #pwn #YOLO'],https://arxiv.org/abs/1902.05318,"Tracking expensive goods and/or targeted individuals with high-tech devices has been of high interest for the last 30 years. More recently, other use cases such as parents tracking their children have become popular. One primary functionality of these devices has been the collection of GPS coordinates of the location of the trackers, and to send these to remote servers through a cellular modem and a SIM card. Reviewing existing devices, it has been observed that beyond simple GPS trackers many devices intend to enclose additional features such as microphones, cameras, or Wi-Fi interfaces enabling advanced spying activities. In this study, we propose to describe the methodology applied to evaluate the security level of GPS trackers with different capabilities. Several security flaws have been discovered during our security assessment highlighting the need of a proper hardening of these devices when used in critical environments. ",Spy the little Spies - Security and Privacy issues of Smart GPS trackers
45,1097418799370899456,523241142,Juste Raimbault,['New paper (translation from French): Modeling interactions between transportation networks and territories: a co-evolution approach. * Definition of co-evolution in territorial systems; * Synthesis of modeling results of my thesis <LINK>'],https://arxiv.org/abs/1902.04802,"Interactions between transportation networks and territories are the subject of open scientific debates, in particular regarding the possible existence of structuring effects of networks, and linked to crucial practical issues of territorial development. We propose an entry on these through co-evolution, and more particularly by the modeling of co-evolution processes between transportation networks and territories. We construct a multi-disciplinary definition of co-evolution which is proper to territorial systems and which can be tested empirically. We then develop the lessons learnt from the development of two types of models, macroscopic interaction models in systems of cities and mesoscopic morphogenesis models through co-evolution. This research opens the perspective of multi-scale models that could be applied to territorial prospective. ","Modeling interactions between transportation networks and territories: a
  co-evolution approach"
46,1097396076624121856,933826478038544384,Mark Williams,"['LHCb charm physics paper 2 of 2019: ""Dalitz plot analysis of the D⁺→K⁻K⁺K⁺ decay"". An impressive analysis, the first-of-its-kind in this channel, and a lot of new information for the community <LINK> @LHCbPhysics <LINK>']",https://arxiv.org/abs/1902.05884,"The resonant structure of the doubly Cabibbo-suppressed decay $D^+ \to K^-K^+K^+$ is studied for the first time. The measurement is based on a sample of pp-collision data, collected at a centre-of-mass energy of 8 TeV with the LHCb detector and corresponding to an integrated luminosity of 2 fb$^-1$. The amplitude analysis of this decay is performed with the isobar model and a phenomenological model based on an effective chiral Lagrangian. In both models the S-wave component in the $K^-K^+$ system is dominant, with a small contribution of the $\phi(1020)$ meson and a negligible contribution from tensor resonances. The $K^-K^+$ scattering amplitudes for the considered combinations of spin (0,1) and isospin (0,1) of the two-body system are obtained from the Dalitz plot fit with the phenomenological decay amplitude. ",Dalitz plot analysis of the $D^+\to K^-K^+K^+$ decay
47,1097314154224934912,1075649842955866114,Luca Cortese,['New @ICRAR @ARC_ASTRO3D paper out today using @SAMI_survey  data! We show that quenching of star formation (rather than change is  stellar spin) is the primary driver of galaxy transformation in groups. <LINK> <LINK>'],https://arxiv.org/abs/1902.05652,"At fixed stellar mass, satellite galaxies show higher passive fractions than centrals, suggesting that environment is directly quenching their star formation. Here, we investigate whether satellite quenching is accompanied by changes in stellar spin (quantified by the ratio of the rotational to dispersion velocity V/$\sigma$) for a sample of massive ($M_{*}>$10$^{10}$ M$_{\odot}$) satellite galaxies extracted from the SAMI Galaxy Survey. These systems are carefully matched to a control sample of main sequence, high $V/\sigma$ central galaxies. As expected, at fixed stellar mass and ellipticity, satellites have lower star formation rate (SFR) and spin than the control centrals. However, most of the difference is in SFR, whereas the spin decreases significantly only for satellites that have already reached the red sequence. We perform a similar analysis for galaxies in the EAGLE hydro-dynamical simulation and recover differences in both SFR and spin similar to those observed in SAMI. However, when EAGLE satellites are matched to their `true' central progenitors, the change in spin is further reduced and galaxies mainly show a decrease in SFR during their satellite phase. The difference in spin observed between satellites and centrals at $z\sim$0 is primarily due to the fact that satellites do not grow their angular momentum as fast as centrals after accreting into bigger halos, not to a reduction of $V/\sigma$ due to environmental effects. Our findings highlight the effect of progenitor bias in our understanding of galaxy transformation and they suggest that satellites undergo little structural change before and during their quenching phase. ","The SAMI Galaxy Survey: Satellite galaxies undergo little structural
  change during their quenching phase"
48,1096859454497263621,107023298,Tim Cooijmans,"['📜 New paper: On the Variance of Unbiased Online Recurrent Optimization (<LINK>, <LINK>) - a thorough investigation of the UORO algorithm for online training of recurrent neural networks, with variance reduction ideas and a link to REINFORCE. <LINK>']",https://arxiv.org/abs/1902.02405,"The recently proposed Unbiased Online Recurrent Optimization algorithm (UORO, arXiv:1702.05043) uses an unbiased approximation of RTRL to achieve fully online gradient-based learning in RNNs. In this work we analyze the variance of the gradient estimate computed by UORO, and propose several possible changes to the method which reduce this variance both in theory and practice. We also contribute significantly to the theoretical and intuitive understanding of UORO (and its existing variance reduction technique), and demonstrate a fundamental connection between its gradient estimate and the one that would be computed by REINFORCE if small amounts of noise were added to the RNN's hidden units. ",On the Variance of Unbiased Online Recurrent Optimization
49,1096787277358030848,168805139,"Yannic Kilcher, Tech Sister",['New Paper: The Odds are Odd - A Statistical Test for Detecting Adversarial Examples\n<LINK>'],https://arxiv.org/abs/1902.04818,"We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy. ",The Odds are Odd: A Statistical Test for Detecting Adversarial Examples
50,1096692363542609920,1005035166513991681,Andreas K. Maier,"[""See Marc's new paper on automatic selection of microscopic high power fields for mitotic cell counting using #deeplearning on @arxiv_org:\n\n<LINK>\n\n#AI predictions of cell count correlate with true counts in the range of 0.8 to 0.98!\n\n@FAU_Germany @FU_Berlin <LINK>""]",https://arxiv.org/abs/1902.05414,"Manual count of mitotic figures, which is determined in the tumor region with the highest mitotic activity, is a key parameter of most tumor grading schemes. It can be, however, strongly dependent on the area selection due to uneven mitotic figure distribution in the tumor section.We aimed to assess the question, how significantly the area selection could impact the mitotic count, which has a known high inter-rater disagreement. On a data set of 32 whole slide images of H&E-stained canine cutaneous mast cell tumor, fully annotated for mitotic figures, we asked eight veterinary pathologists (five board-certified, three in training) to select a field of interest for the mitotic count. To assess the potential difference on the mitotic count, we compared the mitotic count of the selected regions to the overall distribution on the slide.Additionally, we evaluated three deep learning-based methods for the assessment of highest mitotic density: In one approach, the model would directly try to predict the mitotic count for the presented image patches as a regression task. The second method aims at deriving a segmentation mask for mitotic figures, which is then used to obtain a mitotic density. Finally, we evaluated a two-stage object-detection pipeline based on state-of-the-art architectures to identify individual mitotic figures. We found that the predictions by all models were, on average, better than those of the experts. The two-stage object detector performed best and outperformed most of the human pathologists on the majority of tumor cases. The correlation between the predicted and the ground truth mitotic count was also best for this approach (0.963 to 0.979). Further, we found considerable differences in position selection between pathologists, which could partially explain the high variance that has been reported for the manual mitotic count. ","Deep learning algorithms out-perform veterinary pathologists in
  detecting the mitotically most active tumor region"
51,1096234046625964032,2416760538,Peter Gao,"[""On this Valentine's Day my student Danica Adams gifts you a new paper from her, myself, Imke de Pater, and @AstroCaroline all about fluffy aggregate hazes in the atmospheres of giant exoplanets and our favorite enigma, GJ 1214 b! Here it is: <LINK>""]",https://arxiv.org/abs/1902.05231,"Photochemical hazes have been frequently used to interpret exoplanet transmission spectra that show an upward slope towards shorter wavelengths and weak molecular features. While previous studies have only considered spherical haze particles, photochemical hazes composed of hydrocarbon aggregate particles are common throughout the solar system. We use an aerosol microphysics model to investigate the effect of aggregate photochemical haze particles on transmission spectra of warm exoplanets. We find that the wavelength dependence of the optical depth of aggregate particle hazes is flatter than for spheres since aggregates grow to larger radii. As a result, while spherical haze opacity displays a scattering slope towards shorter wavelengths, aggregate haze opacity can be gray in the optical and NIR, similar to those assumed for condensate cloud decks. We further find that haze opacity increases with increasing production rate, decreasing eddy diffusivity, and increasing monomer size, though the magnitude of the latter effect is dependent on production rate and the atmospheric pressure levels probed. We generate synthetic exoplanet transmission spectra to investigate the effect of these hazes on spectral features. For high haze opacity cases, aggregate hazes lead to flat, nearly featureless spectra, while spherical hazes produce sloped spectra with clear spectral features at long wavelengths. Finally, we generate synthetic transmission spectra of GJ 1214b for aggregate and spherical hazes and compare them to space-based observations. We find that aggregate hazes can reproduce the data significantly better than spherical hazes, assuming a production rate limited by delivery of methane to the upper atmosphere. ",Aggregate Hazes in Exoplanet Atmospheres
52,1096208471945433088,755966928150073344,Ben Toms,"['New paper from myself and collaborators at LBNL on deep learning in atmospheric science released on arXiv: <LINK>.  Happy to discuss any questions/comments/concerns!', '@katelynashley37 Thank ya!']",https://arxiv.org/abs/1902.04621,"We test the reliability of two neural network interpretation techniques, backward optimization and layerwise relevance propagation, within geoscientific applications by applying them to a commonly studied geophysical phenomenon, the Madden-Julian Oscillation. The Madden-Julian Oscillation is a multi-scale pattern within the tropical atmosphere that has been extensively studied over the past decades, which makes it an ideal test case to ensure the interpretability methods can recover the current state of knowledge regarding its spatial structure. The neural networks can, indeed, reproduce the current state of knowledge and can also provide new insights into the seasonality of the Madden-Julian Oscillation and its relationships with atmospheric state variables. The neural network identifies the phase of the Madden-Julian Oscillation twice as accurately as linear regression, which means that nonlinearities used by the neural network are important to the structure of the Madden-Julian Oscillation. Interpretations of the neural network show that it accurately captures the spatial structures of the Madden-Julian Oscillation, suggest that the nonlinearities of the Madden-Julian Oscillation are manifested through the uniqueness of each event, and offer physically meaningful insights into its relationship with atmospheric state variables. We also use the interpretations to identify the seasonality of the Madden-Julian Oscillation, and find that the conventionally defined extended seasons should be shifted later by one month. More generally, this study suggests that neural networks can be reliably interpreted for geoscientific applications and may there ","Testing the Reliability of Interpretable Neural Networks in Geoscience
  Using the Madden-Julian Oscillation"
53,1096141395297890304,1092693586263457792,Greg Yang,"['1/8 Modern deep networks (with conv, (self-)attention, batchnorm, LSTM, etc) become Gaussian Processes when randomly initialized, as their widths grow to infinity. This and more are shown in my new paper <LINK>. SOTA GPs here we come, @Jasch?', '2/8 How did we do this? Introducing ""tensor programs,"" a language expressive enough to encode neural network computations but simple enough to allow one to derive its scaling limits. https://t.co/qqnTNCC5Us', '3/8 Cute corollaries: Encoding the computation of matrix powers with a tensor program yields a new proof of the classical semicircle law. Likewise, encoding the alternating product of a random matrix and its transpose yields a new proof of the Marchenko-Pastur law.', '4/8 We also demystify the paradoxical phenomenon found by @sschoenholz where, in randomly initialized networks, the weights used in backprop seem to be ""independent"" from the weights in the forward pass. ""But they are the same?"" ""Yes that\'s why it\'s a paradox""', '5/8 Other corollaries concerning the Neural Tangent Kernel and Approximate Message Passing algorithms can be found in the paper.', '6/8 I hope very soon we will be able to compile Tensorflow graphs or pytorch tapes to tensor programs, allowing one to automatically convert a deep neural net to its limiting Gaussian process.', '7/8 Understanding deep nets is hard, but hopefully now we can understand randomly initialized overparametrized deep nets very well.', '8/8 special thanks to @jaschasd, @sschoenholz, @djhepcatnyc, Raphael Berthier, @ilyaraz2, Pengchuan Zhang, @hadisalman94, and Zeyuan Allen-Zhu', '@optiML @jasch 1/2 the classical version of this NN-GP correspondence is due to Neal (1994), for a shallow network. This was improved in various ways throughout the years but only recently did we obtain deep generalization to MLPs (Poole et al. 2016, Schoenholz et al. 2017, both semirigorous),', '@optiML @jasch 2/2 simple resnet (Yang and Schoenholz 2017, semirigorous), and CNNs (Novak et al. 2019, rigorous). My new paper is the first to rigorously generalize this correspondence to include batchnorm, RNNs (e.g. GRU and LSTM), (self-)attention, etc in a comprehensive framework.', '@optiML @jasch FYI, the history of this literature is discussed in more detail in the paper.', '@optiML @jasch The primary difficulty with reasoning about modern networks is the weight-tying, in CNNs, in RNNs, and implicitly in attention and other layers. The tensor program framework reduces the reasoning of such layers to some mechanical computations.', 'Example tensor program: MLP https://t.co/hb8easbIK8', 'Example tensor program: MLP backprop (continuing from the MLP program) https://t.co/4eCqDYfbNC', 'Example tensor program: RNN https://t.co/1WMnYfI36B', 'Example tensor program: batchnorm https://t.co/WYSc9i0Hwn']",https://arxiv.org/abs/1902.04760,"Several recent trends in machine learning theory and practice, from the design of state-of-the-art Gaussian Process to the convergence analysis of deep neural nets (DNNs) under stochastic gradient descent (SGD), have found it fruitful to study wide random neural networks. Central to these approaches are certain scaling limits of such networks. We unify these results by introducing a notion of a straightline \emph{tensor program} that can express most neural network computations, and we characterize its scaling limit when its tensors are large and randomized. From our framework follows (1) the convergence of random neural networks to Gaussian processes for architectures such as recurrent neural networks, convolutional neural networks, residual networks, attention, and any combination thereof, with or without batch normalization; (2) conditions under which the \emph{gradient independence assumption} -- that weights in backpropagation can be assumed to be independent from weights in the forward pass -- leads to correct computation of gradient dynamics, and corrections when it does not; (3) the convergence of the Neural Tangent Kernel, a recently proposed kernel used to predict training dynamics of neural networks under gradient descent, at initialization for all architectures in (1) without batch normalization. Mathematically, our framework is general enough to rederive classical random matrix results such as the semicircle and the Marchenko-Pastur laws, as well as recent results in neural network Jacobian singular values. We hope our work opens a way toward design of even stronger Gaussian Processes, initialization schemes to avoid gradient explosion/vanishing, and deeper understanding of SGD dynamics in modern architectures. ","Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian
  Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"
54,1096114410534457344,2800342562,T. Marshall Eubanks,"[""New paper is out on additional Project Lyra missions to 1I/'Oumuamua <LINK>""]",https://arxiv.org/abs/1902.04935,"In October 2017, the first interstellar object within our solar system was discovered. Today designated 1I/'Oumuamua, it shows characteristics that have never before been observed in a celestial body. Due to these characteristics, an in-situ investigation of 1I would be of extraordinary scientific value. Previous studies have demonstrated that a mission to 1I/'Oumuamua is feasible using current and near-term technologies however with an anticipated launch date of 2020-2021, this is too soon to be realistic. This paper aims at addressing the question of the feasibility of a mission to 1I/'Oumuamua in 2024 and beyond. Using the OITS trajectory simulation tool, various scenarios are analyzed, including a powered Jupiter flyby and Solar Oberth maneuver, a Jupiter powered flyby, and more complex flyby schemes including a Mars and Venus flyby. With a powered Jupiter flyby and Solar Oberth maneuver, we identify a trajectory to 1I/'Oumuamua with a launch date in 2033, a total velocity increment of 18.2 km/s, and arrival at 1I/'Oumuamua in 2048. With an additional deep space maneuver before the powered Jupiter flyby, a trajectory with a launch date in 2030, a total velocity increment of 15.3 km/s, and an arrival at 1I/'Oumuamua in 2052 were identified. Both launch dates would provide over a decade for spacecraft development, in contrast to the previously identified 2020-2021 launch dates. Furthermore, the distance from the Sun at the Oberth burn is at 5 Solar radii. This results in heat flux values, which are of the same order of magnitude as for the Parker Solar Probe. We conclude that a mission to 1I/'Oumuamua is feasible, using existing and near-term technologies and there is sufficient time for developing such a mission. ",Project Lyra: Catching 1I/'Oumuamua - Mission Opportunities After 2024
55,1095955484534865920,65444176,Kai Olav Ellefsen,"['I\'m happy to announce our new paper with Joost Huizinga and Jim Torresen, ""Guiding Neuroevolution with Structural Objectives"", soon to appear in MIT Press Evolutionary Computation Journal (but preprint out now: <LINK>).']",https://arxiv.org/abs/1902.04346,"The structure and performance of neural networks are intimately connected, and by use of evolutionary algorithms, neural network structures optimally adapted to a given task can be explored. Guiding such neuroevolution with additional objectives related to network structure has been shown to improve performance in some cases, especially when modular neural networks are beneficial. However, apart from objectives aiming to make networks more modular, such structural objectives have not been widely explored. We propose two new structural objectives and test their ability to guide evolving neural networks on two problems which can benefit from decomposition into subtasks. The first structural objective guides evolution to align neural networks with a user-recommended decomposition pattern. Intuitively, this should be a powerful guiding target for problems where human users can easily identify a structure. The second structural objective guides evolution towards a population with a high diversity in decomposition patterns. This results in exploration of many different ways to decompose a problem, allowing evolution to find good decompositions faster. Tests on our target problems reveal that both methods perform well on a problem with a very clear and decomposable structure. However, on a problem where the optimal decomposition is less obvious, the structural diversity objective is found to outcompete other structural objectives -- and this technique can even increase performance on problems without any decomposable structure at all. ",Guiding Neuroevolution with Structural Objectives
56,1095933851984117761,2969696397,Ion Nechita,['New paper <LINK> with Teiko Heinosaari from @turkuquantum and Maria Jivulescu from @UPTimisoara: Random positive operator valued measures. See also accompanying blog post <LINK> <LINK>'],https://arxiv.org/abs/1902.04751,"We introduce several notions of random positive operator valued measures (POVMs), and we prove that some of them are equivalent. We then study statistical properties of the effect operators for the canonical examples, obtaining limiting eigenvalue distributions with the help of free probability theory. Similarly, we obtain the large system limit for several quantities of interest in quantum information theory, such as the sharpness, the noise content, and the probability range. Finally, we study different compatibility criteria, and we compare them for generic POVMs. ",Random positive operator valued measures
57,1095357105173905411,2180565864,Robert Fisher,"[""Wouldn't it be great if astrophysicists could see the nuclei produced in stellar smashups with unprecedented clarity? About a hundred of my  closest colleagues and I think so too! We present the case for a new gamma ray mission in our new white paper.\n <LINK> <LINK>""]",https://arxiv.org/abs/1902.02915v1,"Gamma-ray astronomy explores the most energetic photons in nature to address some of the most pressing puzzles in contemporary astrophysics. It encompasses a wide range of objects and phenomena: stars, supernovae, novae, neutron stars, stellar-mass black holes, nucleosynthesis, the interstellar medium, cosmic rays and relativistic-particle acceleration, and the evolution of galaxies. MeV gamma-rays provide a unique probe of nuclear processes in astronomy, directly measuring radioactive decay, nuclear de-excitation, and positron annihilation. The substantial information carried by gamma-ray photons allows us to see deeper into these objects, the bulk of the power is often emitted at gamma-ray energies, and radioactivity provides a natural physical clock that adds unique information. New science will be driven by time-domain population studies at gamma-ray energies. This science is enabled by next-generation gamma-ray instruments with one to two orders of magnitude better sensitivity, larger sky coverage, and faster cadence than all previous gamma-ray instruments. This transformative capability permits: (a) the accurate identification of the gamma-ray emitting objects and correlations with observations taken at other wavelengths and with other messengers; (b) construction of new gamma-ray maps of the Milky Way and other nearby galaxies where extended regions are distinguished from point sources; and (c) considerable serendipitous science of scarce events -- nearby neutron star mergers, for example. Advances in technology push the performance of new gamma-ray instruments to address a wide set of astrophysical questions. ",] Catching Element Formation In The Act
58,1095259498372059136,916325122960711684,daniele varsano,"['Many-body perturbation theory calculations using the #yambo code\nTen years after the first article, the paper describing the new developments in yambo code is now available on @arxiv @CondensedPapers <LINK>\n@max_center2 @QuantumESPRESSO #HPC #compphys #compchem <LINK>']",https://arxiv.org/abs/1902.03837,"yambo is an open source project aimed at studying excited state properties of condensed matter systems from first principles using many-body methods. As input, yambo requires ground state electronic structure data as computed by density functional theory codes such as quantum-espresso and abinit. yambo's capabilities include the calculation of linear response quantities (both independent-particle and including electron-hole interactions), quasi-particle corrections based on the GW formalism, optical absorption, and other spectroscopic quantities. Here we describe recent developments ranging from the inclusion of important but oft-neglected physical effects such as electron-phonon interactions to the implementation of a real-time propagation scheme for simulating linear and non-linear optical properties. Improvements to numerical algorithms and the user interface are outlined. Particular emphasis is given to the new and efficient parallel structure that makes it possible to exploit modern high performance computing architectures. Finally, we demonstrate the possibility to automate workflows by interfacing with the yambopy and AiiDA software tools. ",Many-body perturbation theory calculations using the yambo code
59,1095176838660395008,1658162341,Narayanan Rengaswamy,['Our new paper on the Clifford hierarchy is on arXiv! We unify diagonal unitaries with Cliffords via symplectic matrices. <LINK>  @DukeEngineering @kenbrownquantum @JoshKoomz'],http://arxiv.org/abs/1902.04022,"The Clifford hierarchy is a foundational concept for universal quantum computation (UQC). It was introduced to show that UQC can be realized via quantum teleportation, given access to certain standard resources. While the full structure of the hierarchy is still not understood, Cui et al. (arXiv:1608.06596) recently described the structure of diagonal unitaries in the hierarchy. They considered diagonal gates whose action on a computational basis qudit state is described by a $2^k$-th root of unity raised to a polynomial function of the state, and they established the level of such unitaries in the hierarchy. For qubit systems, we consider $k$-th level diagonal gates that can be described just by quadratic forms of the state over the ring $\mathbb{Z}_{2^k}$ of integers mod $2^k$. These involve symmetric matrices over $\mathbb{Z}_{2^k}$ that can be used to efficiently describe all $2$-local and certain higher locality diagonal gates in the hierarchy. We also provide explicit algebraic descriptions of their action on Pauli matrices, which establishes a natural recursion to diagonal gates from lower levels. This involves symplectic matrices over $\mathbb{Z}_{2^k}$ and hence our perspective unifies these gates with the binary symplectic framework for Clifford gates. We augment our description with simple examples for certain standard gates. In addition to demonstrating structure, these formulas might prove useful in applications such as (i) classical simulation of quantum circuits, especially via the stabilizer rank approach, (ii) synthesis of logical non-Clifford unitaries, specifically alternatives to magic state distillation, and (iii) decomposition of arbitrary unitaries beyond the Clifford+$T$ set of gates, perhaps leading to shorter depth circuits. Our results suggest that some non-diagonal gates might be understood by generalizing other binary symplectic matrices to integer rings. ",Unifying the Clifford Hierarchy via Symmetric Matrices over Rings
60,1094920068708876288,29756336,Maria,['New 👏 paper 👏 out 👏\n\n<LINK>'],https://arxiv.org/abs/1902.03156,"Using the paradigm of information backflow to characterize a non-Markovian evolution, we introduce so-called precursors of non-Markovianity, i.e. necessary properties that the system and environment state must exhibit at earlier times in order for an ensuing dynamics to be non-Markovian. In particular, we consider a quantitative framework to assess the role that established system-environment correlations together with changes in environmental states play in an emerging non-Markovian dynamics. By defining the relevant contributions in terms of the Bures distance, which is conveniently expressed by means of the quantum state fidelity, these quantities are well defined and easily applicable to a wide range of physical settings. We exemplify this by studying our precursors of non-Markovianity in discrete and continuous variable non-Markovian collision models. ",Precursors of non-Markovianity
61,1094204332650758144,839104540985151490,IPPP Durham,"['New IPPP paper: ""Higgs Physics at the HL-LHC and HE-LHC"" by M. Cepeda et al. <LINK>']",http://arxiv.org/abs/1902.00134,"The discovery of the Higgs boson in 2012, by the ATLAS and CMS experiments, was a success achieved with only a percent of the entire dataset foreseen for the LHC. It opened a landscape of possibilities in the study of Higgs boson properties, Electroweak Symmetry breaking and the Standard Model in general, as well as new avenues in probing new physics beyond the Standard Model. Six years after the discovery, with a conspicuously larger dataset collected during LHC Run 2 at a 13 TeV centre-of-mass energy, the theory and experimental particle physics communities have started a meticulous exploration of the potential for precision measurements of its properties. This includes studies of Higgs boson production and decays processes, the search for rare decays and production modes, high energy observables, and searches for an extended electroweak symmetry breaking sector. This report summarises the potential reach and opportunities in Higgs physics during the High Luminosity phase of the LHC, with an expected dataset of pp collisions at 14 TeV, corresponding to an integrated luminosity of 3 ab$^{-1}$. These studies are performed in light of the most recent analyses from LHC collaborations and the latest theoretical developments. The potential of an LHC upgrade, colliding protons at a centre-of-mass energy of 27 TeV and producing a dataset corresponding to an integrated luminosity of 15 ab$^{-1}$, is also discussed. ",Higgs Physics at the HL-LHC and HE-LHC
62,1093911099332804608,2800204849,Andrew Gordon Wilson,"['We introduce SWAG, which uses the trajectory of SGD iterates for convenient and effective Bayesian uncertainty representation in deep learning (with code!). Our new paper: <LINK> <LINK>']",https://arxiv.org/abs/1902.02476,"We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling. ",A Simple Baseline for Bayesian Uncertainty in Deep Learning
63,1093692530120548352,2337598033,Geraint F. Lewis,['New paper on decaying dark matter now on arXiv - <LINK> Decay away!! <LINK>'],https://arxiv.org/abs/1902.02437,"We present new cosmological hydrodynamic simulations that incorporate Dark Matter Annihilation Feedback (DMAF), whereby energy released from the annihilation of dark matter particles through decay channels such as photon or positron-electron pairs provide additional heating sources for local baryonic material. For annihilation rates comparable to WIMP-like particles, we find that the key influence of DMAF is to inhibit gas accretion onto halos. Such diminished gas accretion early in the lifetimes of halos results in reduced gas fractions in smaller halos, and the delayed halo formation times of larger structures, suggesting that DMAF could impact the stellar age distribution in galaxies, and morphology of dwarfs. For a dark matter particle mass of $m_\chi\sim10$~MeV, there is a `critical halo mass' of $\sim10^{13}$ M$_{\odot}$ at $z=0$, below which there are large differences when compared to $\Lambda$CDM, such as a reduction in the abundance of halo structures as large as 25 percent, reduced gas content by 50 percent and central gas densities reduced down to 10 percent within halos of mass $\sim10^{12}$ M$_{\odot}$ but with increasing effects in smaller halos. Higher dark matter particle mass models have a smaller `critical halo mass'. For a $m_\chi\sim100$~MeV model, we find differences start appearing below halo masses of $\sim10^{12}$ M$_\odot$ and a $m_\chi\gtrsim 1$~GeV model, this mass scale lies below the resolution of our simulations, though we still observe changes in the morphology of dwarf galaxies. ","Dark Matter Annihilation Feedback in Cosmological Simulations II: The
  Influence on Gas and Halo Structure"
64,1093689797464358912,1576235694,Michael Brown,"['Richard Beare, who did his PhD with me and @jabberjabber0 at @Monash_Science, has a new paper on @arxiv.\n\nEvolution of the Stellar Mass Function and Infrared Luminosity Function of Galaxies since z=1.2 \n<LINK> <LINK>']",https://arxiv.org/abs/1902.02779,"We measured evolution of the $K$-band luminosity function and stellar mass function for red and blue galaxies at $z<1.2$ using a sample of 353 594 $I<24$ galaxies in 8.26 square degrees of Bo\""otes. We addressed several sources of systematic and random error in measurements of total galaxy light, photometric redshift and absolute magnitude. We have found that the $K$-band luminosity density for both red and blue galaxies increased by a factor of 1.2 from $z\sim1.1$ to $z\sim0.3$, while the most luminous red (blue) galaxies decreased in luminosity by 0.19 (0.33) mag or $\times0.83 (0.74)$. These results are consistent with $z<0.2$ studies while our large sample size and area result in smaller Poisson and cosmic variance uncertainties than most $z >0.4$ luminosity and mass function measurements. Using an evolving relation for $K$-band mass to light ratios as a function of $(B-V)$ color, we found a slowly decreasing rate of growth in red galaxy stellar mass density of $\times2.3$ from $z\sim1.1$ to $z\sim0.3$, indicating a slowly decreasing rate of migration from the blue cloud to the red sequence. Unlike some studies of the stellar mass function, we find that massive red galaxies grow by a factor of $\times1.7$ from $z\sim1.1$ to $z\sim0.3$, with the rate of growth due to mergers decreasing with time. These results are comparable with measurements of merger rates and clustering, and they are also consistent with the red galaxy stellar mass growth implied by comparing $K$-band luminosity evolution with the fading of passive stellar population models. ","Evolution of the Stellar Mass Function and Infrared Luminosity Function
  of Galaxies since $z = 1.2$"
65,1093610230062239745,2309687984,Arno Solin,['New paper with Yuxin (her first!) and Juho on unstructured multi-view depth estimation that infers depth from sequences of image-pose pairs. Very good results with a lightweight setup. <LINK> <LINK>'],https://arxiv.org/abs/1902.02166,"This paper presents a novel method, MaskMVS, to solve depth estimation for unstructured multi-view image-pose pairs. In the plane-sweep procedure, the depth planes are sampled by histogram matching that ensures covering the depth range of interest. Unlike other plane-sweep methods, we do not rely on a cost metric to explicitly build the cost volume, but instead infer a multiplane mask representation which regularizes the learning. Compared to many previous approaches, we show that our method is lightweight and generalizes well without requiring excessive training. We outperform the current state-of-the-art and show results on the sun3d, scenes11, MVS, and RGBD test data sets. ","Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane
  Representation"
66,1093581702453231623,560473379,nick frosst,"['My new paper with @NicolasPapernot and @GeoffreyHinton is out on arXiv today. It’s about the similarity structure of representations space, outlier data (e.g. adversarial attacks) and generative models. Don’t have time to read the paper? Read this instead! <LINK>', 'Our paper focused on a loss we call Soft Nearest Neighbor Loss (SNNL). It measures the entanglement of labeled data points. Data with high SNNL has muddled up classes, while the classes of a data set with low SNNL are easy to separate. https://t.co/s4NfvusLuT', 'We can measure the SNNL of the data in the hidden layers of a resnet during training and show that each layer separates the data slightly more than the previous layer. the last layer learns a representation of the data which separates the classes, so it has the lowest SNNL value https://t.co/mRtdpTOr74', 'But entanglement can be desirable! You want the output of a GAN to be entangled with real data. If we measure the SNNL between real and generated data, we can see that SNNL increases over training. It serves as a good tool for understanding GAN training. https://t.co/kaHpsgTiMv', 'What happens if we learn a classifier by maximizing the SNNL of each hidden layer in addition to minimizing cross-entropy?  We call these *Entangled Models* because their internal class representations are entangled. Surprisingly, this marginally increases performance! https://t.co/cfTSKSt1y8', 'Entangled models are better at detecting adversarial attacks using the DkNN. We estimate the uncertainty of each classification and find that entangled models project outlier data away from the expected manifold, making adversarial attacks easier to detect. https://t.co/Ov9qVayzC9', ""Entangled models are less vulnerable to black box attacks based on transferability. If we visualize the adversarial gradients of a targeted FGSM attack for normal models, we see shared class clusters. This enables transferability. These clusters don't exist with entangled models! https://t.co/dS8lWOOh8k"", 'Entangled models arent trained with a specific attack in mind, so they should be good at distinguishing all outlier data from real data. If we train a model on MNIST and test it on notMIST, we see that entangled models project the outlier data far away from the real test data. https://t.co/vJC4KRdmN3', 'Read the paper for a more thorough investigation of this exciting loss and the effects of entangling classes in classifications networks and adversarial examples as well as an investigation of SNNL loss in GAN settings :) thanks for reading :)\nhttps://t.co/axkqkB6VMU', '@pkgyawali @NicolasPapernot @geoffreyhinton i still find it hard to explain :P', '@Shahroz07 i think it is promising. its seems that models trained to maximize SNNL are just better suited for anomaly detection :)', '@tetraduzione i can say i expected it :P but that seems to be the case.', '@bermanmaxim @NicolasPapernot @geoffreyhinton thanks :) in some ways this is the opposite of greedy layer-wise training, and inception based approaches. by maximizing SNNL loss at each layer we are learning representations that would make a linear classifier do very poorly.', '@C_Glastonbury im not sure. SNNL does rely on labels, so i am not sure how you would make use of it in an unsupervised manor, but maybe there is a something that could be done with it!', '@Sergio_Soage thanks :) clickbait is what i am aiming for :P', '@Sergio_Soage this is a great idea :P i am gonna do this for everyone of my papers, so it should only take a few hundred years to generate the training data :P but thanks for reading! glad you like it!', '@ducha_aiki No i have not! That is an interesting idea :) thanks for the link!', '@ndronen @bermanmaxim @NicolasPapernot @geoffreyhinton Thanks for reading :)', '@SirrahChan Oh shit. Good catch.']",https://arxiv.org/abs/1902.01889,"We explore and expand the $\textit{Soft Nearest Neighbor Loss}$ to measure the $\textit{entanglement}$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that $\textit{maximizing}$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class. ","Analyzing and Improving Representations with the Soft Nearest Neighbor
  Loss"
67,1093577670091292675,2603024598,Ricardo Pérez-Marco,['New paper with @CGrunspan on selfish mining. Check it out!\n\n<LINK>\n\nWe literally trivialized selfish mining main formulas using Dyck words and their combinatorics related to Catalan numbers! \n\n<LINK>\n<LINK> <LINK>'],https://arxiv.org/abs/1902.01513,"We give a straightforward proof for the formula giving the long-term apparent hashrate of the Selfish Mining strategy in Bitcoin using only elementary probabilities and combinatorics, and more precisely, Dyck words. There is no need to compute stationary probabilities on Markov chain nor stopping times for Poisson processes as it was previously done. We consider also several other block withholding strategies. ",Bitcoin Selfish Mining and Dyck Words
68,1093455372302630912,1196494921,Steve McCormick,"[""New paper on arXiv today. Get it while it's hot! :)\n<LINK> \n(Re-posting with abstract since arXiv links don't seem to show a preview like I had hoped :) ) <LINK>""]",https://arxiv.org/abs/1902.02284,"It is conjectured that the full (spacetime) Bartnik mass of a surface $\Sigma$ is realised as the ADM mass of some stationary asymptotically flat manifold with boundary data prescribed by $\Sigma$. Assuming this holds true for a 1-parameter family of surfaces $\Sigma_t$ evolving in an initial data set {with the dominant energy condition}, we compute an expression for the derivative of the Bartnik mass along these surfaces. An immediate consequence of this formula is that the Bartnik mass of $\Sigma_t$ is monotone non-decreasing whenever $\Sigma_t$ flows outward. It is our pleasure to dedicate this paper to Robert Bartnik on the occasion of his $60$th birthday. ",On the evolution of the spacetime Bartnik mass
69,1093453006526398464,1196494921,Steve McCormick,"[""New paper on arXiv today. Get it while it's hot! :)\n<LINK>"", ""@dixgr Haha I'll take that as praise :P"", ""@PotatoAsad Thank you, that's kind of you to say :)""]",https://arxiv.org/abs/1902.02284,"It is conjectured that the full (spacetime) Bartnik mass of a surface $\Sigma$ is realised as the ADM mass of some stationary asymptotically flat manifold with boundary data prescribed by $\Sigma$. Assuming this holds true for a 1-parameter family of surfaces $\Sigma_t$ evolving in an initial data set {with the dominant energy condition}, we compute an expression for the derivative of the Bartnik mass along these surfaces. An immediate consequence of this formula is that the Bartnik mass of $\Sigma_t$ is monotone non-decreasing whenever $\Sigma_t$ flows outward. It is our pleasure to dedicate this paper to Robert Bartnik on the occasion of his $60$th birthday. ",On the evolution of the spacetime Bartnik mass
70,1093449460590567424,334470578,Chris J. Maddison,"[""New short paper on arXiv today. The idea hiding in Hamiltonian descent can be applied to gradient descent. We're looking for feedback! What did we miss? Was this known? <LINK>""]",https://arxiv.org/abs/1902.02257,"The conditions of relative smoothness and relative strong convexity were recently introduced for the analysis of Bregman gradient methods for convex optimization. We introduce a generalized left-preconditioning method for gradient descent, and show that its convergence on an essentially smooth convex objective function can be guaranteed via an application of relative smoothness in the dual space. Our relative smoothness assumption is between the designed preconditioner and the convex conjugate of the objective, and it generalizes the typical Lipschitz gradient assumption. Under dual relative strong convexity, we obtain linear convergence with a generalized condition number that is invariant under horizontal translations, distinguishing it from Bregman gradient methods. Thus, in principle our method is capable of improving the conditioning of gradient descent on problems with non-Lipschitz gradient or non-strongly convex structure. We demonstrate our method on p-norm regression and exponential penalty function minimization. ",Dual Space Preconditioning for Gradient Descent
71,1093326606876106752,280403336,Sean Welleck,"['Our new paper ""Non-Monotonic Sequential Text Generation"" (with @xkianteb, @haldaume3, and @kchonyc) - generating text in learned, non left-to-right orders\n<LINK>']",https://arxiv.org/abs/1902.02192,"Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy's own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation. ",Non-Monotonic Sequential Text Generation
72,1093314356522102784,1965415700,Greg Dobler,['Two fantastic parting gifts from @CUSPUO PhD student Dr. @ChengeDreamer who successfully defended her thesis in Jan: this amazing chinese hand fan and her TrackNet paper (a new multi-object tracker using spatial+motion features) on the arxiv!  Check it out <LINK> <LINK>'],https://arxiv.org/abs/1902.01466,"Object detection and object tracking are usually treated as two separate processes. Significant progress has been made for object detection in 2D images using deep learning networks. The usual tracking-by-detection pipeline for object tracking requires that the object is successfully detected in the first frame and all subsequent frames, and tracking is done by associating detection results. Performing object detection and object tracking through a single network remains a challenging open question. We propose a novel network structure named trackNet that can directly detect a 3D tube enclosing a moving object in a video segment by extending the faster R-CNN framework. A Tube Proposal Network (TPN) inside the trackNet is proposed to predict the objectness of each candidate tube and location parameters specifying the bounding tube. The proposed framework is applicable for detecting and tracking any object and in this paper, we focus on its application for traffic video analysis. The proposed model is trained and tested on UA-DETRAC, a large traffic video dataset available for multi-vehicle detection and tracking, and obtained very promising results. ","TrackNet: Simultaneous Object Detection and Tracking and Its Application
  in Traffic Video Analysis"
73,1092987178857070592,3087725630,Zack Kilpatrick (he/him/his),"['New paper with @kjosic @AdrianERadillo and Alan Veliz-Cuba analyzes accuracy wrt task parameters and different model fitting approaches for the normative and approximate models for the dynamic clicks task (pioneered by @alexpiet @zamakany @brody_lab).\n<LINK>', 'We find that accuracy of the normative remains constant when keeping SNR (which we show how to compute) constant as well as hazard rate rescaled interrogation time. These effective parameters also control accuracy wrt changepoints.', 'We also find that linear approximations are more sensitive to changes in their discounting parameter than the normative model, and we hypothesize that this is why fits using Bayesian posterior estimators converge to true parameters faster in the linear model.', 'In addition, fitting noisy models using a 0/1-loss function leads to biases in estimating the discounting parameters that favors less variable models. Also, this is our first submission to @NBDT_journal!']",http://arxiv.org/abs/1902.01535,"The aim of a number of psychophysics tasks is to uncover how mammals make decisions in a world that is in flux. Here we examine the characteristics of ideal and near-ideal observers in a task of this type. We ask when and how performance depends on task parameters and design, and, in turn, what observer performance tells us about their decision-making process. In the dynamic clicks task subjects hear two streams (left and right) of Poisson clicks with different rates. Subjects are rewarded when they correctly identify the side with the higher rate, as this side switches unpredictably. We show that a reduced set of task parameters defines regions in parameter space in which optimal, but not near-optimal observers, maintain constant response accuracy. We also show that for a range of task parameters an approximate normative model must be finely tuned to reach near-optimal performance, illustrating a potential way to distinguish between normative models and their approximations. In addition, we show that using the negative log-likelihood and the 0/1-loss functions to fit these types of models is not equivalent: the 0/1-loss leads to a bias in parameter recovery that increases with sensory noise. These findings suggest ways to tease apart models that are hard to distinguish when tuned exactly, and point to general pitfalls in experimental design, model fitting, and interpretation of the resulting data. ","Performance of normative and approximate evidence accumulation on the
  dynamic clicks task"
74,1092982552602697728,296161364,Chris Power,"['Another new paper on the ArXiv from our group led by @ICRAR @ARC_ASTRO3D researcher Pascal Elahi - presenting TreeFrog, a merger tree builder - read the paper at <LINK> and download the code from <LINK> <LINK>']",https://arxiv.org/abs/1902.01527,"We present TreeFrog, a massively parallel halo merger tree builder that is capable comparing different halo catalogues and producing halo merger trees. The code is written in c++11, use the MPI and OpenMP API's for parallelisation, and includes python tools to read/manipulate the data products produced. The code correlates binding energy sorted particle ID lists between halo catalogues, determining optimal descendant/progenitor matches using multiple snapshots, a merit function that maximises the number of shared particles using pseudo-radial moments, and a scheme for correcting halo merger tree pathologies. Focusing on VELOCIraptor catalogues for this work, we demonstrate how searching multiple snapshots spanning a dynamical time significantly reduces the number of stranded halos, those lacking a descendant or a progenitor, critically correcting poorly resolved halos. We present a new merit function that improves the distinction between primary and secondary progenitors, reducing tree pathologies. We find FOF accretion rates and merger rates show similar mass ratio dependence. The model merger rates from Poole et al, (2017) agree with the measured net growth of halos through mergers. ",Climbing Halo Merger Trees with TreeFrog
75,1092839267858968576,901303999529312256,Quanquan Gu,['Why and how does over-parameterization help generalization in deep learning? Turns out gradient descent with random initialization can learn over-parameterized DNNs to achieve arbitrarily small generalization error! Check out our new paper with @_YuanCao_: <LINK>'],https://arxiv.org/abs/1902.01384,"Empirical studies show that gradient-based methods can learn deep neural networks (DNNs) with very good generalization performance in the over-parameterization regime, where DNNs can easily fit a random labeling of the training data. Very recently, a line of work explains in theory that with over-parameterization and proper random initialization, gradient-based methods can find the global minima of the training loss for DNNs. However, existing generalization error bounds are unable to explain the good generalization performance of over-parameterized DNNs. The major limitation of most existing generalization bounds is that they are based on uniform convergence and are independent of the training algorithm. In this work, we derive an algorithm-dependent generalization error bound for deep ReLU networks, and show that under certain assumptions on the data distribution, gradient descent (GD) with proper random initialization is able to train a sufficiently over-parameterized DNN to achieve arbitrarily small generalization error. Our work sheds light on explaining the good generalization performance of over-parameterized deep neural networks. ","Generalization Error Bounds of Gradient Descent for Learning
  Over-parameterized Deep ReLU Networks"
76,1092706483920424961,892059194240532480,Mikel Artetxe,"['Check out our new paper on ""An Effective Approach to Unsupervised Machine Translation"" (w/ @glabaka &amp; @eagirre). We propose a more principled unsupervised SMT approach and hybridize it with NMT, improving previous SOTA by 5-7 BLEU points.\n<LINK>', '5 years later, we outperform the WMT14 English-German winner using monolingual corpora only!']",https://arxiv.org/abs/1902.01313,"While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014. ",An Effective Approach to Unsupervised Machine Translation
77,1092612172033732608,1610691422,Patrick Schwab,"[""What would be an individual's potential response to various levels of exposure to a treatment? In our latest work, we introduce new performance metrics, selection criteria, model architectures, and open benchmarks for this important task.\n\nPaper: <LINK> <LINK>""]",https://arxiv.org/abs/1902.00981,"Estimating what would be an individual's potential response to varying levels of exposure to a treatment is of high practical relevance for several important fields, such as healthcare, economics and public policy. However, existing methods for learning to estimate counterfactual outcomes from observational data are either focused on estimating average dose-response curves, or limited to settings with only two treatments that do not have an associated dosage parameter. Here, we present a novel machine-learning approach towards learning counterfactual representations for estimating individual dose-response curves for any number of treatments with continuous dosage parameters with neural networks. Building on the established potential outcomes framework, we introduce performance metrics, model selection criteria, model architectures, and open benchmarks for estimating individual dose-response curves. Our experiments show that the methods developed in this work set a new state-of-the-art in estimating individual dose-response. ","Learning Counterfactual Representations for Estimating Individual
  Dose-Response Curves"
78,1092610749568958464,296161364,Chris Power,"['New paper on the ArXiv from our group led by @ICRAR @ARC_ASTRO3D researcher Pascal Elahi - presenting VELOCIraptor, a phase space halo and galaxy finder - read the paper at <LINK> and download the code from <LINK> <LINK>']",https://arxiv.org/abs/1902.01010,"We present VELOCIraptor, a massively parallel galaxy/(sub)halo finder that is also capable of robustly identifying tidally disrupted objects and separate stellar halos from galaxies. The code is written in c++11, use the MPI and OpenMP API's for parallelisation, and includes python tools to read/manipulate the data products produced. We demonstrate the power of the VELOCIraptor (sub)halo finder, showing how it can identify subhalos deep within the host that have negligible density contrasts to their parent halo. We find a subhalo mass-radial distance dependence: large subhalos with mass ratios of $\gtrsim10^{-2}$ are more common in the central regions that smaller subhalos, a result of dynamical friction and low tidal mass loss rates. This dependence is completely absent in (sub)halo finders in common use, which generally search for substructure in configuration space, yet is present in codes that track particles belonging to halos as they fall into other halos, such as HBT+. VELOCIraptor largely reproduces the dependence seen without tracking, finding a similar radial dependence to HBT+ in well resolved halos from our limited resolution fiducial simulation. ",Hunting for Galaxies and Halos in simulations with VELOCIraptor
79,1092340647078621184,481539448,Richard Alexander,"['New paper, led by @bec_nealon, looking at the observational appearance of discs warped by planets on inclined orbits. Bec shows that even modest misalignments (few degrees) are enough for gas-giants to create observable shadows in scattered light images. \n<LINK> <LINK>', 'There are movies...😀\nhttps://t.co/5Y2CXGNZvp', 'https://t.co/4oAUVPftBx', 'Finally, we apply our model to the Hubble observations of TW Hya by @JohnDebes and colleagues. In general we find good agreement with the structures observed in the disc, but matching the variability time-scales is tricky.']",https://arxiv.org/abs/1902.00036,"Three-dimensional hydrodynamic numerical simulations have demonstrated that the structure of a protoplanetary disc may be strongly affected by a planet orbiting in a plane that is misaligned to the disc. When the planet is able to open a gap, the disc is separated into an inner, precessing disc and an outer disc with a warp. In this work, we compute infrared scattered light images to investigate the observational consequences of such an arrangement. We find that an inner disc misaligned by a less than a degree to the outer disc is indeed able to cast a shadow at larger radii. In our simulations a planet of around 6 Jupiter masses inclined by around 2 degrees is enough to warp the disc and cast a shadow with a depth of more than 10% of the average flux at that radius. We also demonstrate that warp in the outer disc can cause a variation in the azimuthal brightness profile at large radii. Importantly, this latter effect is a function of the distance from the star and is most prominent in the outer disc. We apply our model to the TW Hya system, where a misaligned, precessing inner disc has been invoked to explain an recently observed shadow in the outer disc. Consideration of the observational constraints suggest that an inner disc precessing due to a misaligned planet is an unlikely explanation for the features found in TW Hya. ",Scattered light shadows in warped protoplanetary discs
80,1103150160975220738,163434088,Jian Tang,"[""Our recent work RotatE (ICLR'19) is online now: a new state-of-the-art model for knowledge graph embedding, which defines entities as complex vectors and relations as rotations. The paper and codes are available at: <LINK>, <LINK>""]",https://arxiv.org/abs/1902.10197,"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction. ","RotatE: Knowledge Graph Embedding by Relational Rotation in Complex
  Space"
81,1102272124239126528,152741512,Prof. Andy Way,"['New paper ""No Padding Please: Efficient Neural Handwriting Recognition"" with Gideon Maillette de Buy Wenniger from @AdaptCentre at @DublinCityUni &amp; Lambert Schomaker from @univgroningen <LINK> <LINK>']",https://arxiv.org/abs/1902.11208,"Neural handwriting recognition (NHR) is the recognition of handwritten text with deep learning models, such as multi-dimensional long short-term memory (MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved state-of-the art results on handwritten text recognition tasks. While multi-directional MDLSTM-layers have an unbeaten ability to capture the complete context in all directions, this strength limits the possibilities for parallelization, and therefore comes at a high computational cost. In this work we develop methods to create efficient MDLSTM-based models for NHR, particularly a method aimed at eliminating computation waste that results from padding. This proposed method, called example-packing, replaces wasteful stacking of padded examples with efficient tiling in a 2-dimensional grid. For word-based NHR this yields a speed improvement of factor 6.6 over an already efficient baseline of minimal padding for each batch separately. For line-based NHR the savings are more modest, but still significant. In addition to example-packing, we propose: 1) a technique to optimize parallelization for dynamic graph definition frameworks including PyTorch, using convolutions with grouping, 2) a method for parallelization across GPUs for variable-length example batches. All our techniques are thoroughly tested on our own PyTorch re-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM dataset shows that our models are performing similar to earlier implementations of state-of-the-art models. Our efficient NHR model and some of the reusable techniques discussed with it offer ways to realize relatively efficient models for the omnipresent scenario of variable-length inputs in deep learning. ",No Padding Please: Efficient Neural Handwriting Recognition
82,1101102870303330305,2613619922,byron wallace,"[""Attention weights in NLP are often presented as providing 'explanations' for (neural) model predictions. They don't, at least not consistently. New #naacl2019 paper (w/PhD student Sarthak Jain): <LINK>"", ""@AaronJaech yeah this is a good question and something we're hoping to explore -- I'm a bit more optimistic for CNNs actually because context obviously less likely to be entangled in attended-to representations""]",https://arxiv.org/abs/1902.10186,"Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at this https URL ",Attention is not Explanation
83,1100426825807863813,3018751880,Prof. Katelin Schutz,"['Here we go, time for my first thread about a new paper, out today on your local arXiv server! #sciencetwitter #physics #hepph #darkmatter #cosmology #earlyuniverse #plasma <LINK>', ""1/ Backstory: we've been looking for dark matter for a long, long time and we haven't found it. We're pretty sure it's a particle based on the concordance of evidence from a wide variety of astrophysical environments, but we're not sure what kind of particle it is."", ""2/ Since we've been looking pretty hard in a couple of particle mass ranges (most notably we've been looking for dark matter particles that are heavier than a proton) and since we haven't found it, folks are starting to wonder if other dark matter masses are possible/interesting"", ""3/ One range that's pretty unique is the range where dark matter is lighter than an electron. It's pretty hard to make dark matter in this mass range that isn't in tension with observations of the universe. This mass range corresponds to energies where we're pretty confident..."", ""4) ...that we understand what's going on in the universe. If there are a lot of these dark matter particles floating around with this mass at a time when the thermal energy of the Universe is comparable to the mass, then Big Bang Nucleosynthesis is likely to get messed up"", ""5) (by the way, Big Bang Nucleosynthesis is how helium and other light elements get made in the early universe. It's a delicate balance of different nuclear reactions and if you tweak stuff even a little then things can get badly thrown off... lots of exponents in the math)"", ""6) anyway, what we realized in this paper is that there's actually a way to make dark matter in this mass range without having too much dark matter around messing up nucleosynthesis. And you guys, I think this is REALLY COOL (but I am obviously biased here...)"", '7) We realized that the early Universe is a dense plasma, and in a plasma photons pick up an in-medium mass. This is sort of similar to the drag you feel while swimming-- your inertial mass is higher because you have to push water out of the way to move, which takes energy', '8/ because photons have this inertial mass (let\'s call massive photons ""plasmons""), they can actually **decay** into light particles. At first, this blew my mind, but it\'s actually standard when thinking about stars (which are also plasmas) and how they lose energy to neutrinos', ""9/ anyway, if the dark matter is somehow able to interact with photons, then dark matter can be effectively produced by these decaying plasmons. Because of how the math works out, this turns out to be most effective in the dark matter mass range below the electron's mass"", ""10/ this is how we make dark matter out of light! and this dark matter is pretty funky relative to other models, especially with regard to how quickly it's moving and how we would best be able to detect it (hint: maybe not in a lab)"", '11/ The effective plasmon mass is a function of temperature, and the Universe is cooling pretty quickly at this time, so when working out conservation of energy for this decay, you end up with a really funky distribution of speeds that does not look thermal', ""12/ even more peculiar is that the dark matter inherits kinematic properties of the hot photons, so on average the dark matter is born going pretty close to the speed of light... because it's going so fast it's going to have a hard time clustering (gravity won't easily trap it)"", '13/ we can look for the suppression of clustering using cosmological probes to see if we can find dark matter gets born this way. Another way to look for dark matter is using the CMB (the afterglow of the big bang) to see if this hot dark matter was dragging around regular matter', '14/ if this dragging (which would happen via similar interactions as the kind of interactions that make the dark matter in the first place) was significant then it would mess up fluctuations we see in the CMB, fuzzing them out in a detectable way', ""15/ after all this cosmology, if we still don't see this kind of dark matter, we have ways to potentially look for it in the lab. For instance, there are some proposals to use weird materials with interesting properties to look for dark matter in this mass range."", '16/ The energy thresholds would have to be super small, but with some R&amp;D this might be possible in a few years. I think it\'s definitely worth pursuing for science reasons, and maybe we could even find something useful to do with this tech for ""everyday"" purposes', ""17/ to summarize, even though dark matter is dark it's future is bright and its past may have been as well! Stay tuned for more work in this space: we have another paper coming out soon that explores the cosmology side a lot more (the paper I linked here is mainly about plasmons)"", ""18/ I'm so grateful to have had an opportunity to learn more about plasmas and non-equilibrium stat mech. At the same time it was a humbling experience and I'm learning new stuff all the time. Our Universe is always keeping me on my toes and it's crucial to keep an open mind!"", '19/ Thanks for reading!! Schutzie out https://t.co/b7oiMO7Wgq', 'wanna say a big THANK YOU to my collaborators Tongyan Lin and @CoraDvorkin']",https://arxiv.org/abs/1902.08623,"Dark matter (DM) could couple to particles in the Standard Model (SM) through a light vector mediator. In the limit of small coupling, this portal could be responsible for producing the observed DM abundance through a mechanism known as freeze-in. Furthermore, the requisite DM-SM couplings provide a concrete benchmark for direct and indirect searches for DM. In this paper, we present updated calculations of the relic abundance for DM produced by freeze-in through a light vector mediator. We identify an additional production channel: the decay of photons that acquire an in-medium plasma mass. These plasmon decays are a dominant channel for DM production for sub-MeV DM masses, and including this channel leads to a significant reduction in the predicted signal strength for DM searches. Accounting for production from both plasmon decays and annihilations of SM fermions, the DM acquires a highly non-thermal phase space distribution which impacts the cosmology at later times; these cosmological effects will be explored in a companion paper. ",Making dark matter out of light: freeze-in from plasma effects
84,1100137278683385856,14093970,Esteban Moro,"['A new version of our paper on communication strategies between agents in Reinforcement Learning hit the arXiv <LINK> : It seems that agents that interact through Erdos-Renyi networks over-perform traditional fully-connected networks. <LINK>', 'It is funny that human networks deviate from Erdos-Renyi in many characteristics. But in this case, they are better and faster to find a solution. Maybe we are doing it all-wrong?']",https://arxiv.org/abs/1902.06740,"A common technique to improve learning performance in deep reinforcement learning (DRL) and many other machine learning algorithms is to run multiple learning agents in parallel. A neglected component in the development of these algorithms has been how best to arrange the learning agents involved to improve distributed search. Here we draw upon results from the networked optimization literatures suggesting that arranging learning agents in communication networks other than fully connected topologies (the implicit way agents are commonly arranged in) can improve learning. We explore the relative performance of four popular families of graphs and observe that one such family (Erdos-Renyi random graphs) empirically outperforms the de facto fully-connected communication topology across several DRL benchmark tasks. Additionally, we observe that 1000 learning agents arranged in an Erdos-Renyi graph can perform as well as 3000 agents arranged in the standard fully-connected topology, showing the large learning improvement possible when carefully designing the topology over which agents communicate. We complement these empirical results with a theoretical investigation of why our alternate topologies perform better. Overall, our work suggests that distributed machine learning algorithms could be made more effective if the communication topology between learning agents was optimized. ","Leveraging Communication Topologies Between Learning Agents in Deep
  Reinforcement Learning"
85,1098293606018924545,1037877848768430081,Boris Oreshkin,"['New paper is out: Chen Xing, Negar Rostamzadeh, Boris N. Oreshkin, Pedro O. Pinheiro, Adaptive Cross-Modal Few-Shot Learning. <LINK> <LINK>']",https://arxiv.org/abs/1902.07104,"Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small. ",Adaptive Cross-Modal Few-Shot Learning
86,1097677218879287296,907232486735958018,Jaki Noronha-Hostler,"['New paper out on the arxiv tonight! We reconstruct the Quantum Chromodynamic Equation of State for 3 conserved charges: Baryon number, Strangeness, &amp; Electric. Thanks to Jamie Stafford &amp; Paolo Parotto we have parameterizations of all susceptibilities.\n<LINK>', '@ronbelmont Thanks!']",https://arxiv.org/abs/1902.06723,"We construct an equation of state for Quantum Chromodynamics (QCD) at finite temperature and chemical potentials for baryon number $B$, electric charge $Q$ and strangeness $S$. We use the Taylor expansion method, up to the fourth power for the chemical potentials. This requires the knowledge of all diagonal and non-diagonal $BQS$ correlators up to fourth order: these results recently became available from lattice QCD simulations, albeit only at a finite lattice spacing $N_t=12$. We smoothly merge these results to the Hadron Resonance Gas (HRG) model, to be able to reach temperatures as low as 30 MeV; in the high temperature regime, we impose a smooth approach to the Stefan-Boltzmann limit. We provide a parameterization for each one of these $BQS$ correlators as functions of the temperature. We then calculate pressure, energy density, entropy density, baryonic, strangeness, electric charge densities and compare the two cases of strangeness neutrality and $\mu_S=\mu_Q=0$. We also calculate the isentropic trajectories and compare them in the two cases. Our equation of state can be readily used as an input of hydrodynamical simulations of matter created at the Relativistic Heavy Ion Collider (RHIC). ","Lattice-based equation of state at finite baryon number, electric charge
  and strangeness chemical potentials"
87,1097051968923529216,937127267850846208,Mohammad Javad Amiri,"['Checkout our new paper on permissioned blockchain: ""ParBlockchain: Leveraging Transaction Parallelism in Permissioned Blockchain Systems"".\n<LINK> #Blockchain']",https://arxiv.org/abs/1902.01457,"Many existing blockchains do not adequately address all the characteristics of distributed system applications and suffer from serious architectural limitations resulting in performance and confidentiality issues. While recent permissioned blockchain systems, have tried to overcome these limitations, their focus has mainly been on workloads with no-contention, i.e., no conflicting transactions. In this paper, we introduce OXII, a new paradigm for permissioned blockchains to support distributed applications that execute concurrently. OXII is designed for workloads with (different degrees of) contention. We then present ParBlockchain, a permissioned blockchain designed specifically in the OXII paradigm. The evaluation of ParBlockchain using a series of benchmarks reveals that its performance in workloads with any degree of contention is better than the state of the art permissioned blockchain systems. ","ParBlockchain: Leveraging Transaction Parallelism in Permissioned
  Blockchain Systems"
88,1095969960625610752,607831012,Hoda Heidari,"['Have you ever wondered ""out of the growing list of mathematical formulations of fairness, which one does most closely resemble humans\' perception of fairness in a given domain?"" Check out our new paper---just posted on ArXiv: <LINK>', '@eredmil1 Thanks a lot Elissa! :) We took a lot of inspiration from your work on human perception of fairness for feature selection.']",https://arxiv.org/abs/1902.04783,"Fairness for Machine Learning has received considerable attention, recently. Various mathematical formulations of fairness have been proposed, and it has been shown that it is impossible to satisfy all of them simultaneously. The literature so far has dealt with these impossibility results by quantifying the tradeoffs between different formulations of fairness. Our work takes a different perspective on this issue. Rather than requiring all notions of fairness to (partially) hold at the same time, we ask which one of them is the most appropriate given the societal domain in which the decision-making model is to be deployed. We take a descriptive approach and set out to identify the notion of fairness that best captures \emph{lay people's perception of fairness}. We run adaptive experiments designed to pinpoint the most compatible notion of fairness with each participant's choices through a small number of tests. Perhaps surprisingly, we find that the most simplistic mathematical definition of fairness---namely, demographic parity---most closely matches people's idea of fairness in two distinct application scenarios. This conclusion remains intact even when we explicitly tell the participants about the alternative, more complicated definitions of fairness, and we reduce the cognitive burden of evaluating those notions for them. Our findings have important implications for the Fair ML literature and the discourse on formalizing algorithmic fairness. ","Mathematical Notions vs. Human Perception of Fairness: A Descriptive
  Approach to Fairness for Machine Learning"
89,1095766942630191105,632362071,David Ho,"['What can we learn about magnetic monopoles from heavy-ion collisions at colliders like the LHC? A new paper from Oliver Gould, @ArttuRajantie and myself makes some important steps towards answering this question: preprint available at <LINK>']",https://arxiv.org/abs/1902.04388,"Magnetic monopoles may be produced by the Schwinger effect in the strong magnetic fields of peripheral heavy-ion collisions. We review the form of the electromagnetic fields in such collisions and calculate from first principles the cross section for monopole pair production. Using the worldline instanton method, we work to all orders in the magnetic charge, and hence are not hampered by the breakdown of perturbation theory. Our result depends on the spacetime inhomogeneity through a single dimensionless parameter, the Keldysh parameter, which is independent of collision energy for a given monopole mass. For realistic heavy-ion collisions, the computational cost of the calculation becomes prohibitive and the finite size of the monopoles needs to be taken into account, and therefore our current results are not applicable to them. Nonetheless, our results show that the spacetime dependence enhances the production cross section and would therefore lead to stronger monopole mass bounds than in the constant-field case. ","Towards Schwinger production of magnetic monopoles in heavy-ion
  collisions"
90,1095138575119572992,383142451,Shinnosuke Takamichi (高道 慎之介),['Our #ICASSP2019 paper for a new singing voice synthesis scheme for reproducing natural voice randomness.\n<LINK> <LINK>'],https://arxiv.org/abs/1902.03389,"This paper proposes a generative moment matching network (GMMN)-based post-filter that provides inter-utterance pitch variation for deep neural network (DNN)-based singing voice synthesis. The natural pitch variation of a human singing voice leads to a richer musical experience and is used in double-tracking, a recording method in which two performances of the same phrase are recorded and mixed to create a richer, layered sound. However, singing voices synthesized using conventional DNN-based methods never vary because the synthesis process is deterministic and only one waveform is synthesized from one musical score. To address this problem, we use a GMMN to model the variation of the modulation spectrum of the pitch contour of natural singing voices and add a randomized inter-utterance variation to the pitch contour generated by conventional DNN-based singing voice synthesis. Experimental evaluations suggest that 1) our approach can provide perceptible inter-utterance pitch variation while preserving speech quality. We extend our approach to double-tracking, and the evaluation demonstrates that 2) GMMN-based neural double-tracking is perceptually closer to natural double-tracking than conventional signal processing-based artificial double-tracking is. ","Generative Moment Matching Network-based Random Modulation Post-filter
  for DNN-based Singing Voice Synthesis and Neural Double-tracking"
91,1093021715792318470,133148364,Devendra Chaplot,"['Posted a new paper on Embodied Multimodal Multitask Learning for semantic goal navigation and embodied question answering. (with @rl_agent, @rsalakhu, @deviparikh, @DhruvBatraDB)\n\nPDF: \n<LINK>\n\nDemo Videos: \n<LINK> <LINK>', 'We present an interpretable and modular multitask model capable of learning disentangled task-invariant visual and textual representations. It is able to transfer knowledge of words across the tasks and generalize to unseen instructions and questions.']",https://arxiv.org/abs/1902.01385,"Recent efforts on training visual navigation agents conditioned on language using deep reinforcement learning have been successful in learning policies for different multimodal tasks, such as semantic goal navigation and embodied question answering. In this paper, we propose a multitask model capable of jointly learning these multimodal tasks, and transferring knowledge of words and their grounding in visual objects across the tasks. The proposed model uses a novel Dual-Attention unit to disentangle the knowledge of words in the textual representations and visual concepts in the visual representations, and align them with each other. This disentangled task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. We show that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. We also show that this disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by leveraging object detectors. ",Embodied Multimodal Multitask Learning
92,1092735670458687488,174298756,Adel Bibi,"['New paper on derivative free optimization with importance sampling. New best known rates in derivative free setting with cool RL exps. Joint work with Elhoucine, @ozansener, Bernard and Peter.\n<LINK>']",https://arxiv.org/abs/1902.01272,"We consider the problem of unconstrained minimization of a smooth objective function in $\R^n$ in a setting where only function evaluations are possible. While importance sampling is one of the most popular techniques used by machine learning practitioners to accelerate the convergence of their models when applicable, there is not much existing theory for this acceleration in the derivative-free setting. In this paper, we propose the first derivative free optimization method with importance sampling and derive new improved complexity results on non-convex, convex and strongly convex functions. We conduct extensive experiments on various synthetic and real LIBSVM datasets confirming our theoretical results. We further test our method on a collection of continuous control tasks on MuJoCo environments with varying difficulty. Experiments suggest that our algorithm is practical for high dimensional continuous control problems where importance sampling results in a significant sample complexity improvement. ","A Stochastic Derivative-Free Optimization Method with Importance
  Sampling: Theory and Learning to Control"
93,1104075634593095681,4870078413,Sam Schoenholz,"['1/3 Our new paper analyzing batch normalization in neural networks at initialization is out (and will be at @iclr2019). We find that batch norm + MLPs always feature exploding gradients for any choice of nonlinearity and batch size. <LINK> <LINK>', '2/3 We propose several schemes to ameliorate this by careful parameter tuning. The formalism here also opens the door to performing Bayesian inference on Gaussian Process that correspond to neural networks with batch normalization.', '3/3 As always, this was a really fun collaboration with @TheGregYang, Jeffrey Pennington, @vinaysrao, @jaschasd.']",https://arxiv.org/abs/1902.08129,"We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest. ",A Mean Field Theory of Batch Normalization
94,1101515274510053376,30673001,Tom Hosking,"['Super excited that my paper with @riedelcastro has been accepted at #naacl2019! In which we fine tune a question generation model on various sensible objectives, and find that higher rewards != better questions.\n\n<LINK>']",https://arxiv.org/abs/1902.11049,"Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have proposed countering this bias by optimising for a reward that is less tightly coupled to the training data, using reinforcement learning. We optimise directly for quality metrics, including a novel approach using a discriminator learned directly from the training data. We confirm that policy gradient methods can be used to decouple training from the ground truth, leading to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source. ",Evaluating Rewards for Question Generation Models
95,1100692376371236864,1327867824,Antreas Antoniou,"['As children, many of us played the game of ""find the pattern"", by attempting to find abstract connections between random objects in our environment. Inspired from this idea, we develop an unsupervised few-shot meta-learning technique that generalizes well <LINK>']",https://arxiv.org/abs/1902.09884,"The field of few-shot learning has been laboriously explored in the supervised setting, where per-class labels are available. On the other hand, the unsupervised few-shot learning setting, where no labels of any kind are required, has seen little investigation. We propose a method, named Assume, Augment and Learn or AAL, for generating few-shot tasks using unlabeled data. We randomly label a random subset of images from an unlabeled dataset to generate a support set. Then by applying data augmentation on the support set's images, and reusing the support set's labels, we obtain a target set. The resulting few-shot tasks can be used to train any standard meta-learning framework. Once trained, such a model, can be directly applied on small real-labeled datasets without any changes or fine-tuning required. In our experiments, the learned models achieve good generalization performance in a variety of established few-shot learning tasks on Omniglot and Mini-Imagenet. ","Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via
  Random Labels and Data Augmentation"
96,1100668340132081664,2791947167,Miryam de Lhoneux,"['Now available on the arXiv: <LINK> and code: <LINK>. We find that a tree layer does not reliably help a BiLSTM transition-based parser 1/3 <LINK>', 'We perform some model ablations and find that the backward LSTM is crucial in the BiLSTM, especially for head-final languages. The forward LSTM is less crucial and a backward LSTM + a tree layer performs close to a BiLSTM 2/3', 'We find that POS information and a tree layer are two partially redundant ways of constructing contextual information 3/3']",https://arxiv.org/abs/1902.09781,"The need for tree structure modelling on top of sequence modelling is an open issue in neural dependency parsing. We investigate the impact of adding a tree layer on top of a sequential model by recursively composing subtree representations (composition) in a transition-based parser that uses features extracted by a BiLSTM. Composition seems superfluous with such a model, suggesting that BiLSTMs capture information about subtrees. We perform model ablations to tease out the conditions under which composition helps. When ablating the backward LSTM, performance drops and composition does not recover much of the gap. When ablating the forward LSTM, performance drops less dramatically and composition recovers a substantial part of the gap, indicating that a forward LSTM and composition capture similar information. We take the backward LSTM to be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages. ",Recursive Subtree Composition in LSTM-Based Dependency Parsing
97,1100558706624741376,990433714948661250,Sergey Levine,"['Should agents decode language instructions into actions or rewards? We find that recovering a reward function from an instruction and then optimizing it with RL results in substantially better instruction following\n\n<LINK>\n\nw/ Justin Fu, @sguada, A. Korattikara <LINK>']",https://arxiv.org/abs/1902.07742,"Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance. ","From Language to Goals: Inverse Reinforcement Learning for Vision-Based
  Instruction Following"
98,1100469838386999298,736907975353323520,Amir Erez,"['Can we reach a diverse #microbial #ecology in a serial-dilution culture? How does that relate to seasonal #ecosystems?\nFind out in our new preprint !\n<LINK>', '@mbauer67 you might find this one interesting!']",https://arxiv.org/abs/1902.09039,"Microbial communities feature an immense diversity of species and this diversity is linked with outcomes ranging from ecosystem stability to medical prognoses. Yet the mechanisms underlying microbial diversity are under debate. While simple resource-competition models don't allow for coexistence of a large number of species, it was recently shown that metabolic trade-offs can allow unlimited diversity. Does this diversity persist with more realistic, intermittent nutrient supply? Here, we demonstrate theoretically that in serial dilution culture, metabolic trade-offs allow for high diversity. When a small amount of nutrient is supplied to each batch, the serial dilution dynamics mimic a chemostat-like steady state. If more nutrient is supplied, diversity depends on the amount of nutrient supplied due to an ""early-bird"" effect. The interplay of this effect with different environmental factors and diversity-supporting mechanisms leads to a variety of relationships between nutrient supply and diversity, suggesting that real ecosystems may not obey a universal nutrient-diversity relationship. ","Nutrient levels and trade-offs control diversity in a serial dilution
  ecosystem"
99,1100453653603905543,130881465,Alex Nitz,"['My co-authors and I have just put onto the arxiv our results from our search of @LIGO and  @nasafermi-GBM data for gravitational-wave and gamma-ray coincident events. We find an interesting candidate that would occur once per 13 years from noise alone. \n<LINK> <LINK>', 'We find there is a 1 in 4 chance this is an astrophysical event, so while interesting, definitely not certain! You can derive this from the expected rate of binary neutron stars and compare to the number of events that are only accidentally coincident.', 'If this event is real, it would be the furthest binary neutron star merger observed with gravitational waves. ~600 million light years away. https://t.co/k1PHKROCrj']",https://arxiv.org/abs/1902.09496,"We present a search for binary neutron star mergers that produced gravitational-waves during the first observing run of Advanced LIGO and gamma-ray emission seen by either \textit{Swift}-BAT or Fermi-GBM, similar to GW170817 and GRB 170817A. We introduce a new method using a combined ranking statistic to detect sources that do not produce significant gravitational-wave or gamma-ray burst candidates individually. The current version of this search can increase by 70% the detections of joint gravitational-wave and gamma-ray signals. We find one possible candidate observed by LIGO and Fermi-GBM, 1-OGC 151030, at a false alarm rate of 1 in 13 years. If astrophysical, this candidate would correspond to a merger at $187^{+99}_{-87}\,$Mpc with source-frame chirp mass of $1.30^{+0.02}_{-0.03}\,\mathrm{M}_{\odot}$. If we assume the viewing angle must be $<30^{\circ}$ to be observed by \textit{Fermi}-GBM, our estimate of the distance would become $224^{+88}_{-78}\,$Mpc. By comparing the rate of binary neutron star mergers to our search-estimated rate of false alarms, we estimate that there is a 1 in 4 chance this candidate is astrophysical in origin. ","Potential Gravitational-wave and Gamma-ray Multi-messenger Candidate
  from Oct. 30, 2015"
100,1100343481166385152,1095632116933713920,Anton Mallasto,"['Natural gradient method utilizes the Fisher-Rao geometry for maximizing the likelihood of a model, corresponding to minimizing the KL-divergence. With @tomdelh  and Aasa Feragen, we studied how the notion generalizes to arbitrary similarity measures. <LINK> <LINK>']",http://arxiv.org/abs/1902.08959,"In optimization, the natural gradient method is well-known for likelihood maximization. The method uses the Kullback-Leibler divergence, corresponding infinitesimally to the Fisher-Rao metric, which is pulled back to the parameter space of a family of probability distributions. This way, gradients with respect to the parameters respect the Fisher-Rao geometry of the space of distributions, which might differ vastly from the standard Euclidean geometry of the parameter space, often leading to faster convergence. However, when minimizing an arbitrary similarity measure between distributions, it is generally unclear which metric to use. We provide a general framework that, given a similarity measure, derives a metric for the natural gradient. We then discuss connections between the natural gradient method and multiple other optimization techniques in the literature. Finally, we provide computations of the formal natural gradient to show overlap with well-known cases and to compute natural gradients in novel frameworks. ","A Formalization of The Natural Gradient Method for General Similarity
  Measures"
101,1099983763067543552,103634999,Thorsten Holz,"['We published a new technical report on arXiv: ""A Study of Newly Observed Hostnames and DNS Tunneling in the Wild"" - <LINK>']",https://arxiv.org/abs/1902.08454,"The domain name system (DNS) is a crucial backbone of the Internet and millions of new domains are created on a daily basis. While the vast majority of these domains are legitimate, adversaries also register new hostnames to carry out nefarious purposes, such as scams, phishing, or other types of attacks. In this paper, we present insights on the global utilization of DNS through a measurement study examining exclusively newly observed hostnames via passive DNS data analysis. We analyzed more than two billion such hostnames collected over a period of two months. Surprisingly, we find that only three second-level domains are responsible for more than half of all newly observed hostnames every day. More specifically, we found that Google's Accelerated Mobile Pages (AMP) project, the music streaming service Spotify, and a DNS tunnel provider generate the majority of new domains on the Internet. DNS tunneling is a covert channel technique to transfer arbitrary information over DNS via DNS queries and answers. This technique is often (ab)used by attackers to transfer data in a stealthy way, bypassing traditional network security systems. We find that potential DNS tunnels cause a significant fraction of the global DNS requests for new hostnames: our analysis reveals that nearly all resource record type NULL requests and more than a third of all TXT requests can be attributed to DNS tunnels. Motivated by these empirical measurement results, we propose and implement a method to identify DNS tunnels via a step-wise filtering approach that relies on general characteristics of such tunnels (e.g., number of subdomains or resource record type). Using our approach on empirical data, we successfully identified 273 suspicious domains related to DNS tunnels, including two known APT campaigns (Wekby and APT32). ",A Study of Newly Observed Hostnames and DNS Tunneling in the Wild
102,1098980168755044353,841858076193955841,Karan Desai (KD),"['Our paper ""Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering"" is now on ArXiv (<LINK> )! w/ @vrama91, @stefmlee, Marcus Rohrbach, @DhruvBatraDB, @deviparikh. We propose a class of probabilistic models for symbolic reasoning ...(1/3)', '... specifically extending Neural Module Networks (https://t.co/7EnuuJBMga ). Our formulation learns symbolic functional programs as a discrete, structured, stochastic latent variable. Our approach works well while requiring very less teaching examples, and ...(2/3)', '... offers interpretability, legibility, and coherence in its decisions (choice of functional programs for question answering). Exciting results on CLEVR and SHAPES!\nThanks to @vrama91 for getting me onboard, learned a lot about variational inference and symbolic reasoning! (3/3)']",https://arxiv.org/abs/1902.07864,"We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring lesser number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed. ","Probabilistic Neural-symbolic Models for Interpretable Visual Question
  Answering"
103,1098134866506534912,21902101,Jim Geach,['New paper on @arXiv today - we measure the halo masses of z=1-2 quasars by using the fact that their dark matter halos lens the CMB... and we find tentative evidence for a luminosity dependence on the halo mass <LINK> @UniofHerts <LINK>'],https://arxiv.org/abs/1902.06955,"We measure the average deflection of cosmic microwave background photons by quasars at $\langle z \rangle =1.7$. Our sample is selected from the Sloan Digital Sky Survey to cover the redshift range $0.9\leq z\leq2.2$ with absolute i-band magnitudes of $M_i\leq-24$ (K-corrected to z=2). A stack of nearly 200,000 targets reveals an 8$\sigma$ detection of Planck's estimate of the lensing convergence towards the quasars. We fit the signal with a model comprising a Navarro-Frenk-White density profile and a 2-halo term accounting for correlated large scale structure, which dominates the observed signal. The best-fitting model is described by an average halo mass $\log_{10}(M_{\rm h}/h^{-1}M_\odot)=12.6\pm0.2$ and linear bias $b=2.7\pm0.3$ at $z=1.7$, in excellent agreement with clustering studies. We also report of a hint, at a 90% confidence level, of a correlation between the convergence amplitude and luminosity, indicating that quasars brighter than $M_i\lesssim -26$ reside in halos of typical mass ${M_{\rm h}\approx 10^{13}\,h^{-1}M_\odot}$, scaling roughly as ${M_{\rm h}\propto L_{\rm opt}^{3/4}}$ at ${M_i\lesssim-24}$, in good agreement with physically-motivated quasar demography models. Although we acknowledge this luminosity dependence is a marginal result, the observed $M_{\rm h}$-$L_{\rm opt}$ relationship could be interpreted as a reflection of the cutoff in the distribution of black hole accretion rates towards high Eddington ratios: the weak trend of $M_{\rm h}$ with $L_{\rm opt}$ observed at low luminosity becomes stronger for the most powerful quasars, which tend to be accreting close to the Eddington limit. ","The halo mass of optically-luminous quasars at z=1-2 measured via
  gravitational deflection of the cosmic microwave background"
104,1098034229013868544,20703003,Peter B Denton,"['New neutrino paper out! <LINK>\n\nThis one is unique for me in that it is the shortest (3 pages) and the fastest from conception (3 weeks).\n\nIn it we build on previous work by Wang &amp; Zhou and find that a simple correction improves their result by a factor of ~20! <LINK>', 'This paper is also special to me because it means that I now have more papers than there are James Bond movies. How will I celebrate finishing a paper now?']",https://arxiv.org/abs/1902.07185,"For neutrino propagation in matter, we show that the Jarlskog invariant, which controls the size of true CP violation in neutrino oscillation appearance experiments, factorizes into three pieces: the vacuum Jarlskog invariant times two simple two-flavor matter resonance factors that control the matter effects for the solar and atmospheric resonances independently. If the solar effective matter potential and the atmospheric effective $\Delta m^2$ are chosen carefully for these two resonance factors, then the fractional corrections to this factorization are an impressive 0.04\% or smaller. We also show that the inverse of the square of the Jarlskog in matter ($1/\hat{J}^2$) is a fourth order polynomial in the matter potential which guarantees that it can be factored into two quadratics which immediately implies the functional form of our approximate, factorized expression. ","Simple and Precise Factorization of the Jarlskog Invariant for Neutrino
  Oscillations in Matter"
105,1095869032090103808,1095330293395800065,Danilo Vasconcellos Vargas,"['The one-pixel attack as well as other attacks have shown unknown and  until now  unexplainable behavior of DNNs.  Here we propose Propagation  Maps which reveal the influence of  adversarial samples throughout the  layers, allowing us to explain them. <LINK> <LINK>']",https://arxiv.org/abs/1902.02947,"Deep neural networks were shown to be vulnerable to single pixel modifications. However, the reason behind such phenomena has never been elucidated. Here, we propose Propagation Maps which show the influence of the perturbation in each layer of the network. Propagation Maps reveal that even in extremely deep networks such as Resnet, modification in one pixel easily propagates until the last layer. In fact, this initial local perturbation is also shown to spread becoming a global one and reaching absolute difference values that are close to the maximum value of the original feature maps in a given layer. Moreover, we do a locality analysis in which we demonstrate that nearby pixels of the perturbed one in the one-pixel attack tend to share the same vulnerability, revealing that the main vulnerability lies in neither neurons nor pixels but receptive fields. Hopefully, the analysis conducted in this work together with a new technique called propagation maps shall shed light into the inner workings of other adversarial samples and be the basis of new defense systems to come. ","Understanding the One-Pixel Attack: Propagation Maps and Locality
  Analysis"
106,1095733168198553600,17393929,JunZhao,"[""Young children care about online privacy more than we think, but they struggle to recognise online promotions. KOALA's new project report on a focus group study with ~30 6-10yos reports. <LINK>"", 'Children under 11 cared about their privacy online and were sensitive to, who might access their sensitive information. However, children need help to fully understand online privacy risks, especially those associated with implicit personal data collection and use. (2/3)', ""Children's ability to recognise risks impacts on their ability to cope with risks. Children may use the correct language to describe risks, without fully comprehending them; and they often relied on their own or their friends’ experiences to make risk-related decisions. (3/3)"", 'We recommend: \n\n1.  Continue to talk to children about being careful online, because they are facing challenges in their use of digital devices every day\n\n2. Pay attention to how children describe risks or things that made them uncomfortable online.']",https://arxiv.org/abs/1902.02635,"The age of children adopting digital technologies, such as tablets or smartphones, is increasingly young. However, children under 11 are often regarded as too young to comprehend the concept of online privacy. Limited research studies have focused on children of this age group. In the summer of 2018, we conducted 12 focus group studies with 29 children aged 6-10 from Oxfordshire primary schools. Our research has shown that children have a good understanding of certain privacy risks, such as information oversharing or avoiding revealing real identities online. They could use a range of descriptions to articulate the risks and describe their risk coping strategies. However, at the same time, we identified that children had less awareness concerning other risks, such as online tracking or game promotions. Inspired by Vygotsky's Zone of Proximal Development (ZPD), this study has identified critical knowledge gaps in children's understanding of online privacy, and several directions for future education and technology development. We call for attention to the needs of raising children's awareness and understanding of risks related to online recommendations and data tracking, which are becoming ever more prevalent in the games and content children encounter. We also call for attention to children's use of language to describe risks, which may be appropriate but not necessarily indicate a full understanding of the threats. ","Are Children Fully Aware of Online Privacy Risks and How Can We Improve
  Their Coping Ability?"
107,1095251116319821825,57793813,Teppei Katori (香取哲平),"[""#T2K near detector search of #neutrino neutral-current single photon produciton. This channel is a home of new physics, perhaps related to #MiniBooNE signal. We didn't find it and set limit. Can someone revenge for us? #nuxsec\n<LINK> <LINK>""]",https://arxiv.org/abs/1902.03848,"Neutrino neutral-current induced single photon production is a sub-leading order process for accelerator-based neutrino beam experiments including T2K. It is, however, an important process to understand because it is a background for electron (anti)neutrino appearance oscillation experiments. Here, we performed the first search of this process below 1 GeV using the fine-grained detector at the T2K ND280 off-axis near detector. By reconstructing single photon kinematics from electron-positron pairs, we achieved 95\% pure gamma ray sample from 5.738$\times 10^{20}$ protons-on-targets neutrino mode data. We do not find positive evidence of neutral current induced single photon production in this sample. We set the model-dependent upper limit on the cross-section for this process, at 0.114$\times 10^{-38}$ cm$^2$ (90\% C.L.) per nucleon, using the J-PARC off-axis neutrino beam with an average energy of $\left<E_\nu\right>\sim 0.6$ GeV. This is the first limit on this process below 1 GeV which is important for current and future oscillation experiments looking for electron neutrino appearance oscillation signals. ","Search for neutral-current induced single photon production at the ND280
  near detector in T2K"
108,1092779809422434304,748850869366468609,"Rishi Paudel, PhD","['Here is our paper: ""Origin of radio-quiet coronal mass ejections in flare stars"" <LINK>.\nWe have used a hybrid magnetic model to study the Alfen speed profile of flaring M dwarfs and explain the origin of ""radio-quiet"" CMEs.\n@johngizis @rachelosten @pkgw']",https://arxiv.org/abs/1902.00810,"Type II radio bursts are observed in the Sun in association with many coronal mass ejections (CME's. In view of this association, there has been an expectation that, by scaling from solar flares to the flares which are observed on M dwarfs, radio emission analogous to solar Type II bursts should be detectable in association with M dwarf flares. However, several surveys have revealed that this expectation does not seem to be fulfilled. Here we hypothesize that the presence of larger global field strengths in low-mass stars, suggested by recent magneto-convective modeling, gives rise to such large Alfven speeds in the corona that it becomes difficult to satisfy the conditions for the generation of Type II radio bursts. As a result, CME's propagating in the corona/wind of a flare stars are expected to be ""radio-quiet"" as regards Type II bursts. In view of this, we suggest that, in the context of Type II bursts, scaling from solar to stellar flares is of limited effectiveness. ",Origin of radio-quiet coronal mass ejections in flare stars
109,1092706483920424961,892059194240532480,Mikel Artetxe,"['Check out our new paper on ""An Effective Approach to Unsupervised Machine Translation"" (w/ @glabaka &amp; @eagirre). We propose a more principled unsupervised SMT approach and hybridize it with NMT, improving previous SOTA by 5-7 BLEU points.\n<LINK>', '5 years later, we outperform the WMT14 English-German winner using monolingual corpora only!']",https://arxiv.org/abs/1902.01313,"While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014. ",An Effective Approach to Unsupervised Machine Translation
110,1092511987429445632,191328145,David Ehrenreich 🌻,"['We have just detected sodium on the ultra-hot giant #exoplanet @WASPplanets-76b as part of our ❤️ survey at @ESO. @JuliaVSeidel et al. resolve the lines with HARPS &amp; find surprise broadening! Could be super-rotation in the planet upper atmosphere - <LINK> <LINK>', '(The plot shows the co-added lines of the sodium doublet. The HARPS instrumental function is shown in orange.)', '@V_Parmentier @WASPplanets @ESO @JuliaVSeidel Yes, compared to predictions for the lower atmosphere. Here, the signal arises from much higher up (thermosphere). Even higher up, the atmosphere must be escaping and ""winds"" must exceed the escape velocity of 28 km/s (green lines). Wind speed should be increasing with altitude.']",https://arxiv.org/abs/1902.00001,"High-resolution optical spectroscopy is a powerful tool to characterise exoplanetary atmospheres from the ground. The sodium D lines, with their large cross sections, are especially suited to study the upper layers of atmospheres in this context. We report on the results from HEARTS, a spectroscopic survey of exoplanet atmospheres, performing a comparative study of hot gas giants to determine the effects of stellar irradiation. In this second installation of the series, we highlight the detection of neutral sodium on the ultra-hot giant WASP-76b. We observed three transits of the planet using the HARPS high-resolution spectrograph at the ESO 3.6m telescope and collected 175 spectra of WASP-76. We repeatedly detect the absorption signature of neutral sodium in the planet atmosphere ($0.371\pm0.034\%$; $10.75 \sigma$ in a $0.75$ \r{A} passband). The sodium lines have a Gaussian profile with full width at half maximum (FWHM) of $27.6\pm2.8$ km s$^{-1}$. This is significantly broader than the line spread function of HARPS ($2.7$ km s$^{-1}$). We surmise that the observed broadening could trace the super-rotation in the upper atmosphere of this ultra-hot gas giant. ","Hot Exoplanet Atmospheres Resolved with Transit Spectroscopy (HEARTS) -
  II. A broadened sodium feature on the ultra-hot giant WASP-76b"
111,1096326868117188609,218250514,Heiko Hamann,['preprint of our paper\n\nEngineered Self-Organization for Resilient #Robot Self-Assembly with Minimal Surprise\n\n<LINK>\n\nwe deny our robots a defined task and let them find their own way of shaping their environment in a self-referential process\n#ml #ai @msminirobot <LINK>'],https://arxiv.org/abs/1902.05485,"In collective robotic systems, the automatic generation of controllers for complex tasks is still a challenging problem. Open-ended evolution of complex robot behaviors can be a possible solution whereby an intrinsic driver for pattern formation and self-organization may prove to be important. We implement such a driver in collective robot systems by evolving prediction networks as world models in pair with action-selection networks. Fitness is given for good predictions which causes a bias towards easily predictable environments and behaviors in the form of emergent patterns, that is, environments of minimal surprise. There is no task-dependent bias or any other explicit predetermination for the different qualities of the emerging patterns. A careful configuration of actions, sensor models, and the environment is required to stimulate the emergence of complex behaviors. We study self-assembly to increase the scenario's complexity for our minimal surprise approach and, at the same time, limit the complexity of our simulations to a grid world to manage the feasibility of this approach. We investigate the impact of different swarm densities and the shape of the environment on the emergent patterns. Furthermore, we study how evolution can be biased towards the emergence of desired patterns. We analyze the resilience of the resulting self-assembly behaviors by causing damages to the assembled pattern and observe the self-organized reassembly of the structure. In summary, we evolved swarm behaviors for resilient self-assembly and successfully engineered self-organization in simulation. In future work, we plan to transfer our approach to a swarm of real robots. ","Engineered Self-Organization for Resilient Robot Self-Assembly with
  Minimal Surprise"
112,1092796237601013764,179552827,Fabio Viola,"['How we tame distributed TensorFlow and target diverse cluster architectures, improving experiment turnaround, at DeepMind: find out more about TF-Replicator, paper on ArXiv now! <LINK> <LINK>']",https://arxiv.org/abs/1902.00465,"We describe TF-Replicator, a framework for distributed machine learning designed for DeepMind researchers and implemented as an abstraction over TensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel research code. The same models can be effortlessly deployed to different cluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU accelerators) using synchronous or asynchronous training regimes. To demonstrate the generality and scalability of TF-Replicator, we implement and benchmark three very different models: (1) A ResNet-50 for ImageNet classification, (2) a SN-GAN for class-conditional ImageNet image generation, and (3) a D4PG reinforcement learning agent for continuous control. Our results show strong scalability performance without demanding any distributed systems expertise of the user. The TF-Replicator programming model will be open-sourced as part of TensorFlow 2.0 (see this https URL). ",TF-Replicator: Distributed Machine Learning for Researchers
