,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1183774266384273408,830355049,Mohit Bansal,"['New @emnlp2019 paper by\xa0@mrnt0810, presenting initial thoughts/results on AutoAugment controller methods (input-agnostic &amp; input-aware) to automatically discover effective sequences of perturbn subpolicies for dialogue data-augmentatn &amp; adv-robustification\n<LINK> <LINK>']",https://arxiv.org/abs/1909.12868,"Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies. ",Automatically Learning Data Augmentation Policies for Dialogue Tasks
1,1182794510494863360,927974200274427904,Bruce Macintosh,"['Paper 2: Coronagraph SETI! led by Christina Vides at Cal Poly Pomona, is on #seti. To be specific, a new approach to optical/infrared OIRSETI using coronagraphs built to image exoplanets: CORSETI. <LINK>', 'Lasers are great communication tools for sending signals long distances, so SETI has been looking for those in addition to radio signals. Lasers can focused and aimed at a target to be surprisingly bright even at interstellar distances. (Originally suggested by Charlie Townes.)', 'One problem. You know what else is bright? Stars. Really really bright at optical and infrared wavelengths. Millions of times brighter then a practical laser. And most hypothetical civilizations trying to communicate with us probably live near a star.', 'So you need a way to distinguish bright starlight from faint laser light. Historically there have been two ways to do this by exploiting properties of the laser.', ""First, lasers only make a single color / wavelength of light, and usually that's a very very narrow range of wavelengths. Much more monochromatic than any natural source and at very different wavelengths than any astronomical narrow-emissions sources."", 'In a high resolution spectrograph, a laser would look like a narrow spectroscopic line. e.g. Tellis et al 2017 who looked at spectrograph data used for a planet search. \nhttps://t.co/iCQin2B26v', 'The other usual approach is timing. Stars emit photons continuously. If the laser is pulsed, it will emit photons in pulses - multiple photons within microseconds or nanoseconds and then a gap. Hi-speed detectors can see this. @ikashell has a brilliant plan for this.', ""A third way the laser might be different than the star is that the laser probably isn't right on top of the star. (Stars are hot.) So you could look to see if there's a light source at a different position. Still hard if the laser is close to the star."", 'But seeing faint things next to bright stars is kind of our job! Instruments like the Gemini Planet Imager and the WFIRST coronagraph are built to image faint planets next to bright stars, but they could also see faintish lasers.', ""So we decided to calculate how powerful a laser we could see. You need to make some assumptions about the transmitter. We assumed a 10-m space telescope  being used to send the signal so it can produce a tightly focused beam. (Here's a cartoon). https://t.co/JHQScxJMRU"", ""And as in all OIRSETI, the sender has to be transmitting just at our solar system - in fact just at the Earth - on purpose. (But that's reasonable, because they already know we're here using their own exoplanet imagers.)"", ""We then calculated the detection limits for GPI using real data. GPI can see things about a million times fainter then a star. Around most nearby stars, GPI could see a laser with a power of 10-40 kilowatts. That's less powerful than military lasers that exist right now."", ""And since GPI is a spectrograph as well as an imager, you could distinguish a laser from a real planet easily. GPI hasn't actually looked at many stars that are good SETI targets - our typical target star is very young, before life emerged on Earth (let alone lasers)"", 'WFIRST could do even better. The Wide-Field Infrared Survey Telescope will carry a prototype coronagraph capable of seeing things up to a billion times fainter than their host star. That translates into laser power from 0.5 - 50 watts.', ""That's pretty impressive. You can buy lasers that powerful on eBay. Hook it up to a 10-m telescope (not available on eBay) and you can signal tens of light years away."", 'I was actually surprised by how good this was, but in hindsight it makes sense. of course planets have much more total brightness than a laser. The luminosity of the Earth is about 10^16 watts. But the laser is aimed and focused!', 'A 10-m telescope launches a 1 micron laser into a beam that is 10^-7 radians across, for a 10^14 concentrate of power.  So laser transmitters can be as bright as a planet with either a big laser or a big transmitter telescope.', ""Where next? It'd be nice to reduce GPI and other current coronagraph data just to doublecheck, though we don't expect to see signals from most stars. otherwise, the nice thing about this sort of SETI is that it's free - while you're looking for planets, just pay attention"", ""Huge amounts of credit to Christina Vides, who led the paper. She was community college student when she first came to Stanford for summer  research via the CAMPARE program. She's now graduating from Cal Poly Pomona and applying to graduate schools! She's very sharp."", ""Thanks also to the @CAMPAREprogram  and Leadership Alliance Programs for funding Christina's summer, and to @BreXRB  and @polyastron  for supervising her research at Cal Poly.""]",https://arxiv.org/abs/1909.04128,"We present modeled detection limits of the Gemini Planet Imager (GPI) and the Wide-Field Infrared Space Telescope (WFIRST) to an optical and infrared laser which could be used by an extraterrestrial civilization to signal their presence. GPI and WFIRST could utilize a coronagraph to search for extraterrestrial intelligence (SETI) in the present and future. We use archival data for GPI stars and simulated WFIRST observations to find the detectable flux ratio of a laser signal to residual scattered starlight around the target star. This flux ratio is then converted to detectable power as a function of distance from the parent star. For GPI, we assume a monochromatic laser wavelength of 1.55 $\mu$m, and a wavelength of 575 nm for WFIRST. We assume the lasers are projected through a 10-m aperture, and that the intensity of the laser beam follows a Gaussian profile. Our analysis is performed on 6 stars with spectral types later than F within 20 pc (with an emphasis on solar analogs at different distances). The most notable result is the detection limit for $\tau$ Ceti, a G5V star with four known exoplanets, two of those within the habitable zone (HZ). The result shows that a 24 kW laser is detectable from $\tau$ Ceti from outside of the HZ with GPI and a 7.3 W laser is detectable from within $\tau$ Ceti's HZ by WFIRST. ","Model of the Search For Extraterrestrial Intelligence with Coronagraphic
  Imaging"
2,1182346455618469889,767345894,Xiang Ren,"['Reasoning over a knowledge graph that is dynamically enriched by external text? Check out our @emnlp2019 paper on collaborative policy learning for open KG reasoning. Two new datasets are released for the task. @nlp_usc\npaper: <LINK>\ncode: <LINK> <LINK>', '@PMinervini @Tim_Dettmers @emnlp2019 @nlp_usc yes we came across this paper in our follow-up study :) Interesting idea to combine rule-based reasoning and representation learning. Two papers definitely share some similar thoughts.']",https://arxiv.org/abs/1909.00230,"In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoning---a task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out ""irrelevant"" facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach. Source code and datasets used in this paper can be downloaded at this https URL ",Collaborative Policy Learning for Open Knowledge Graph Reasoning
3,1182015175685107712,3743366715,Aaron Mueller,"[""also! new #emnlp2019 paper with @marty_with_an_e, @amuuueller, and @tallinzen --- we explore whether increasing layer and/or training set size helps language models learn better syntax. the answer: a little, but these alone won't be enough <LINK> <LINK>""]",https://arxiv.org/abs/1909.00111,"Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures. ",Quantity doesn't buy quality syntax with neural language models
4,1181102493125791744,877201224004665345,Mattia Bulla,['New paper lead by @SuhailDaOne on using kilonovae to improve H0. Check this out!  <LINK>'],https://arxiv.org/abs/1909.13810,"There is a strong degeneracy between the luminosity distance ($D_L$) and the observer viewing angle ($\theta_{\rm obs}$; hereafter viewing angle) of the gravitational wave (GW) source with an electromagnetic counterpart, GW170817. Here, for the first time, we present independent constraints on $\theta_{\rm obs} = 32.5 ^{\circ +11.7}_{-9.7}$ from broad-band photometry of the kilonova (kN) AT2017gfo associated with GW170817. These constraints are consistent with independent results presented in the literature using the associated gamma ray burst GRB170817A. Combining the constraints on $\theta_{\rm obs}$ with the GW data, we find an improvement of 24$\%$ on $H_0$. The observer angle constraints are insensitive to other model parameters, e.g. the ejecta mass, half-opening angle of the lanthanide-rich region and the temperature. A broad wavelength coverage extending to the near infrared is helpful to robustly constrain $\theta_{\rm obs}$. While the improvement on $H_0$ presented here is smaller than the one from high angular resolution imaging of the radio counterpart of GW170817, kN observations are significantly more feasible at the typical distances of such events from current and future LIGO-Virgo Collaboration observing runs ($D_L \sim 100$ Mpc). Our results are insensitive to the assumption on the peculiar velocity of the kN host galaxy. ","Constraining the observer angle of the kilonova AT2017gfo associated
  with GW170817: Implications for the Hubble constant"
5,1180503054069780480,1701409680,Suhail Dhawan,['A few days old but new paper on constraining the inclination of gw170817 using the associated kilonova. Consistent results with the radio and improvements of about 25% <LINK>'],https://arxiv.org/abs/1909.13810,"There is a strong degeneracy between the luminosity distance ($D_L$) and the observer viewing angle ($\theta_{\rm obs}$; hereafter viewing angle) of the gravitational wave (GW) source with an electromagnetic counterpart, GW170817. Here, for the first time, we present independent constraints on $\theta_{\rm obs} = 32.5 ^{\circ +11.7}_{-9.7}$ from broad-band photometry of the kilonova (kN) AT2017gfo associated with GW170817. These constraints are consistent with independent results presented in the literature using the associated gamma ray burst GRB170817A. Combining the constraints on $\theta_{\rm obs}$ with the GW data, we find an improvement of 24$\%$ on $H_0$. The observer angle constraints are insensitive to other model parameters, e.g. the ejecta mass, half-opening angle of the lanthanide-rich region and the temperature. A broad wavelength coverage extending to the near infrared is helpful to robustly constrain $\theta_{\rm obs}$. While the improvement on $H_0$ presented here is smaller than the one from high angular resolution imaging of the radio counterpart of GW170817, kN observations are significantly more feasible at the typical distances of such events from current and future LIGO-Virgo Collaboration observing runs ($D_L \sim 100$ Mpc). Our results are insensitive to the assumption on the peculiar velocity of the kN host galaxy. ","Constraining the observer angle of the kilonova AT2017gfo associated
  with GW170817: Implications for the Hubble constant"
6,1179137754845863939,2427184074,Christopher Berry,"['New paper on localization for gravitational-wave sources with @LIGOLA, @LIGOWA, @ego_virgo, @KAGRA_PR AND @LIGOIndia \n<LINK>\nThis is our first paper with PhD students @mmrizz &amp; @ThatKaushikRao! <LINK>', ""@LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina From our public alerts during our third observing run, you might have noticed how much our localization depends upon the detectors online. With 1 detector it's basically the sky, with 2 it's a band, with 3 we're talking! Eventually, we'll have 5 online https://t.co/3tUc5tQhAK"", '@LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina The main benefit for localization of adding more gravitational-wave detectors is that you have a bigger fraction of the time where at least 3 are online. This plot shows performance for different duty cycles (fraction of time when each detector is on)\n\nüìà: https://t.co/VRdLqMRAS6 https://t.co/YTL2tY3aaM', '@LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina With an 80% duty factor (which is realistic), the 5 detector network will have a median 90% localization of 10 sq deg. That is one pointing of LSST! If we drop to 20% the median area is 10 times bigger! Improving gravitational-wave detectors is not all about sensitivity', ""@no_scooters @LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina Yes, it makes a big difference. It's going to make life a lot easier in some ways, but there'll be challenges too as we'll be detecting sources at further distance"", ""@LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina We looked at localization of binary neutron stars &amp; neutron star‚Äìblack hole binaries. Consider all combinations of @LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia, the 2 US detectors give the worst localization when paired. It's better to mix things up!\n\nüìähttps://t.co/VRdLqMRAS6 https://t.co/TlYe4jnhpM"", ""@no_scooters @LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina Yes, that's right! The similarity in orientation also contributes a little (as you don't get as much information on the polarizations), but I think distance is the main thing"", ""@CosmicRami I'm glad that you like it!"", ""@LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina How does localization vary between binary neutron stars and neutron star‚Äìblack holes? We're better at localizing binary neutron stars as the gravitational-wave signals are longer &amp; go up to higher frequencies. The difference depends on the mass distribution https://t.co/ROS91f2yUY"", '@maverick0246 @LIGOLA @LIGOWA @ego_virgo @KAGRA_PR @LIGOIndia @mmrizz @ThatKaushikRao @NUCIERA @matsoulina üì¶']",https://arxiv.org/abs/1909.12961,"GW170817 began gravitational-wave multimessenger astronomy. However, GW170817 will not be representative of detections in the coming years -- typical gravitational-wave sources will be closer the detection horizon, have larger localization regions, and (when present) will have correspondingly weaker electromagnetic emission. In its design state, the gravitational-wave detector network in the mid-2020s will consist of up to five similar-sensitivity second-generation interferometers. The instantaneous sky-coverage by the full network is nearly isotropic, in contrast to the configuration during the first \change{three} observing runs. Along with the coverage of the sky, there are also commensurate increases in the average horizon for a given binary mass. We present a realistic set of localizations for binary neutron stars and neutron star--black hole binaries, incorporating intra-network duty cycles and selection effects on the astrophysical distributions. Based on the assumption of an $80\%$ duty cycle, and that two instruments observe a signal above the detection threshold, we anticipate a median of $28$ sq.\ deg.\ for binary neutron stars, and $50$--$120$ sq.\ deg.\ for neutron star--black hole (depending on the population assumed). These distributions have a wide spread, and the best localizations, even for networks with fewer instruments, will have localizations of $1$--$10$ sq.\ deg.\ range. The full five instrument network reduces localization regions to a few tens of degrees at worst. ","Localization of Compact Binary Sources with Second Generation
  Gravitational-wave Interferometer Networks"
7,1178902598151221248,898575285121159168,Alexandre Santerne üá™üá∫,['Check-out the new paper from our @ESO - @NASAKepler large program on #HARPS that  presents the measurement of the planets‚Äô mass in the fascinating #K2-138 exoplanetary system. These planets will also be observed by @ESA_CHEOPS to measure their #TTVs.\n‚û°Ô∏è <LINK> <LINK>'],https://arxiv.org/abs/1909.13527,"The detection of low-mass transiting exoplanets in multiple systems brings new constraints to planetary formation and evolution processes and challenges the current planet formation theories. Nevertheless, only a mere fraction of the small planets detected by Kepler and K2 have precise mass measurements, which are mandatory to constrain their composition. We aim to characterise the planets that orbit the relatively bright star K2-138. This system is dynamically particular as it presents the longest chain known to date of planets close to the 3:2 resonance. We obtained 215 HARPS spectra from which we derived the radial-velocity variations of K2-138. Via a joint Bayesian analysis of both the K2 photometry and HARPS radial-velocities (RVs), we constrained the parameters of the six planets in orbit. The masses of the four inner planets, from b to e, are 3.1, 6.3, 7.9, and 13.0 $\mathrm{M}_{\oplus}$ with a precision of 34%, 20%, 18%, and 15%, respectively. The bulk densities are 4.9, 2.8, 3.2, and 1.8 g cm$^{-3}$, ranging from Earth to Neptune-like values. For planets f and g, we report upper limits. Finally, we predict transit timing variations of the order two to six minutes from the masses derived. Given its peculiar dynamics, K2-138 is an ideal target for transit timing variation (TTV) measurements from space with the upcoming CHaracterizing ExOPlanet Satellite (CHEOPS) to study this highly-packed system and compare TTV and RV masses. ","Exoplanet characterisation in the longest known resonant chain: the
  K2-138 system seen by HARPS"
8,1178885101842767872,986718011661762561,Pin-Yu Chen,"['Very happy to see methods originally developed for improving robustness (e.g., adversarial training) can be proven to be useful for other tasks. This recent paper shows (inexact) adversarial training + transformer-based model becomes the new SOTA <LINK> on GLUE']",https://arxiv.org/abs/1909.11764,"Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\% and 67.75\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \url{this https URL . ",FreeLB: Enhanced Adversarial Training for Natural Language Understanding
9,1178869323349118976,22399655,Ryota Kanaiüí°,['Our new paper on Information Closure Theory of Consciousness where we propose conscious processes form non-trivial information closure (NTIC). \n<LINK>'],https://arxiv.org/abs/1909.13045,"Information processing in neural systems can be described and analysed at multiple spatiotemporal scales. Generally, information at lower levels is more fine-grained and can be coarse-grained in higher levels. However, information processed only at specific levels seems to be available for conscious awareness. We do not have direct experience of information available at the level of individual neurons, which is noisy and highly stochastic. Neither do we have experience of more macro-level interactions such as interpersonal communications. Neurophysiological evidence suggests that conscious experiences co-vary with information encoded in coarse-grained neural states such as the firing pattern of a population of neurons. In this article, we introduce a new informational theory of consciousness: Information Closure Theory of Consciousness (ICT). We hypothesise that conscious processes are processes which form non-trivial informational closure (NTIC) with respect to the environment at certain coarse-grained levels. This hypothesis implies that conscious experience is confined due to informational closure from conscious processing to other coarse-grained levels. ICT proposes new quantitative definitions of both conscious content and conscious level. With the parsimonious definitions and a hypothesise, ICT provides explanations and predictions of various phenomena associated with consciousness. The implications of ICT naturally reconciles issues in many existing theories of consciousness and provides explanations for many of our intuitions about consciousness. Most importantly, ICT demonstrates that information can be the common language between consciousness and physical reality. ",Information Closure Theory of Consciousness
10,1178851738025111552,19637908,Bernd Weiss,"['New working paper w/ C Kern and @JanPhilippKolb on ""A Longitudinal Framework for Predicting Nonresponse in Panel Surveys"" utilizing @GESIS_Panel data <LINK>']",https://arxiv.org/abs/1909.13361,"Nonresponse in panel studies can lead to a substantial loss in data quality due to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal cross-validation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves. ",A Longitudinal Framework for Predicting Nonresponse in Panel Surveys
11,1178835353303695361,876274407995527169,David Madras,"['New paper! ""Causal Modeling for Fairness in Dynamical Systems"" <LINK>  w/ Elliot Creager, Toni Pitassi &amp; Rich Zemel\n\nTLDR: Through a series of case studies, we show causal DAGs can act as a unifying framework for the literature on long-term unfairness. 1/n <LINK>', 'Recently, many papers have presented models of unfairness in long-term systems. We show causal DAGs can be used as a unifying framework for this literature, + give several case studies.\n\nEg. this graphical formulation of ‚ÄúDelayed Impact of Fair Machine Learning‚Äù (Liu et al.) 2/n https://t.co/ynUh7FHgkW', 'We give 3 advantages of causal DAGs in this domain. First is visualization: graphical models are a compact way to communicate models to non-technical stakeholders.\n\nEg. this formulation of Hashimoto et al‚Äôs ‚ÄúFairness w/o Demographics in Repeated Loss Minimization‚Äù 3/n https://t.co/IVj6jwAWwR', '‚Ä¶ or this graphical formulation of an intricate, two-stage labor market model in Hu et al‚Äôs ‚ÄúA Short-term Intervention for Long-term Fairness in the Labor Market‚Äù. 4/n https://t.co/RgtTjQmg3V', 'The second reason to use these graphical models is introspection: these formulations make clear many implicit causal assumptions, and present straightforward methods for discussion and modification. 5/n', 'The third is evaluation: many long-term fairness papers must be evaluated using ""policy evaluation"" ‚Äî we give examples in a case study. 6/n https://t.co/sJrw4zYlvm', 'Furthermore, causal models allow for counterfactual-based policy evaluation methods, which are more practical/robust. 7/n https://t.co/pIeQ2HX6ad', 'In conclusion, we suggest a range of future questions at the intersection of various fields (e.g. RL, fairness, causal inference) which follow naturally from our formulation.  n/n https://t.co/FDJx7WRpP6', ""PS: Since finishing this paper, I've become fairly sure there's a literature on structural econometrics which is very related to all of this. If anyone has points on good places to read up on that (#econtwitter? does that work?) that would be swell"", '@jvmancuso This looks (tangentially) awesome!  I definitely thought this field did not exist. @adversariel will enjoy I think', ""@jvmancuso @adversariel Haha it's a small Twitter after all"", ""@jvmancuso Sounds cool! Fairness through awareness has a special place in my heart, I'll check this out""]",https://arxiv.org/abs/1909.09141,"In many application areas---lending, education, and online recommenders, for example---fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and off-policy estimation (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels. ",Causal Modeling for Fairness in Dynamical Systems
12,1178742652151287814,71950239,Sathya Ravi,"['Our new paper on optimizing F metrics directly using a lagrangian based approach is <LINK> For semantic segmentation, the accuracy gains vs to the baseline are *significant* ‚âà 10% on MS COCO dataset! We show that full implementation can be  auto-diffed! #PyTorch', 'Excited to be presenting the results of the paper at the @deepmath1 conference, later this month!']",https://arxiv.org/abs/1909.12398,"Data dependent regularization is known to benefit a wide variety of problems in machine learning. Often, these regularizers cannot be easily decomposed into a sum over a finite number of terms, e.g., a sum over individual example-wise terms. The $F_\beta$ measure, Area under the ROC curve (AUCROC) and Precision at a fixed recall (P@R) are some prominent examples that are used in many applications. We find that for most medium to large sized datasets, scalability issues severely limit our ability in leveraging the benefits of such regularizers. Importantly, the key technical impediment despite some recent progress is that, such objectives remain difficult to optimize via backpropapagation procedures. While an efficient general-purpose strategy for this problem still remains elusive, in this paper, we show that for many data-dependent nondecomposable regularizers that are relevant in applications, sizable gains in efficiency are possible with minimal code-level changes; in other words, no specialized tools or numerical schemes are needed. Our procedure involves a reparameterization followed by a partial dualization -- this leads to a formulation that has provably cheap projection operators. We present a detailed analysis of runtime and convergence properties of our algorithm. On the experimental side, we show that a direct use of our scheme significantly improves the state of the art IOU measures reported for MSCOCO Stuff segmentation dataset. ","Optimizing Nondecomposable Data Dependent Regularizers via Lagrangian
  Reparameterization offers Significant Performance and Efficiency Gains"
13,1178655631999426560,1556664198,Kyle Cranmer,"['Excited to announce a new paper with Alvaro Sanchez-Gonzalez, Victor Bapst, and @PeterWBattaglia  (@DeepMindAI) on \n""Hamiltonian Graph Networks with ODE Integrators""\nGives improvements in position &amp; energy accuracy, and zero-shot generalization. \n<LINK> <LINK>', 'We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states (OGN), and a Hamiltonian as an internal representation (HOGN). https://t.co/U9toZHyL7f', 'There are some parables for Machine Learning for physics and interpretability. What does it mean for a network with no physics constraints to be more accurate than the true Hamiltonian? This can happen if you use a simple ODE integrator and big time steps.', ""The graph network that directly learns position and momentum updates with no physics constraints has enough capacity to learn the residuals and make more accurate predictions (using the same crappy integrator). But that doesn't generalize or conserve energy well."", 'Instead, our Hamiltonian graph networks will suffer the same loss of predictive accuracy as the true Hamiltonian with a crappy integrator, but do better at conserving energy and generalize to different step sizes and integrator types.', 'It\'s interesting to look back and think about the question ""is the network learning the physics?"" It seems clear the hamiltonian network is, but this isn\'t reflected in the naive test accuracy with the same integrator and step size.', 'This was a fun collaboration. The @DeepMind team is amazing!', '@Deepmind Also a shout-out to a related project with @PeterWBattaglia and a different Cranmer.... what are the chances?\nhttps://t.co/sU6Q8msBku']",https://arxiv.org/abs/1909.12790,"We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains. ",Hamiltonian Graph Networks with ODE Integrators
14,1178569579934695424,3852572896,Hamid Zafar,"['Check out the pre-print of our new paper by me,  @marytavakol and @JLehmann82: an RL based approach in distantly supervised setting for shallow parsing and entity/relation linking <LINK>\n#QuestionAnswering #KBQA']",https://arxiv.org/abs/1909.12566,"The emergence of structured databases for Question Answering (QA) systems has led to developing methods, in which the problem of learning the correct answer efficiently is based on a linking task between the constituents of the question and the corresponding entries in the database. As a result, parsing the questions in order to determine their main elements, which are required for answer retrieval, becomes crucial. However, most datasets for QA systems lack gold annotations for parsing, i.e., labels are only available in the form of (question, formal-query, answer). In this paper, we propose a distantly supervised learning framework based on reinforcement learning to learn the mentions of entities and relations in questions. We leverage the provided formal queries to characterize delayed rewards for optimizing a policy gradient objective for the parsing model. An empirical evaluation of our approach shows a significant improvement in the performance of entity and relation linking compared to the state of the art. We also demonstrate that a more accurate parsing component enhances the overall performance of QA systems. ",Distantly Supervised Question Parsing
15,1178552191168200705,75249390,Axel Maas,"['We have published a new paper on the #QCDPhaseDiagram - we explore a new idea how to get the thermodynamic behavior from the properties of the individual gluons, borrowing an idea from solid state physics: <LINK> #np3']",https://arxiv.org/abs/1909.12727,"Reliably computing the free energy in a gauge theory like QCD is a challenging and resource-demanding endeavor. As an alternative, we explore here the possibility to obtain the associated thermodynamic anomaly by exploiting its relation to the Tan contact. Optimally, this would reduce the determination of the free energy to a high-precision calculation of two-point correlators. We study this possibility using the lattice and functional methods and compare them to the expected behavior for the SU(2) Yang-Mills case. ",Exploring the Tan contact term in Yang-Mills theory
16,1177770543434485760,67250704,Stephan Hoyer,"['I\'m happy to share a new paper, with @jaschasd and @samgreydanus: ""Neural reparameterization improves structural optimization""\n<LINK>\n\nWe use neural nets to parameterize inputs of a finite elements method, and differentiable through the whole thing: <LINK>', ""@jaschasd @samgreydanus Instead of optimizing designs in real space at each pixel, we now optimize the weights of a neural network. The approach retains the advantages standard physics-based optimization methods -- we're simply searching in a new, reparameterized space."", '@jaschasd @samgreydanus The neural reparameterization seems to improve the behavior of the optimization landscape, resulting in significantly better optimized designs. It also makes some pretty pictures of buildings and bridges -- see the paper for more! https://t.co/LnxbKEcQdg']",https://arxiv.org/abs/1909.04240,"Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produces the best design 50% more often than the best baseline method. ",Neural reparameterization improves structural optimization
17,1177601981474574336,157413536,Pierre-Yves Lajoie,"['Our new work on robust multi-robot SLAM!\nDOOR-SLAM: Distributed, Online, and Outlier Resilient SLAM for Robotic Teams\n@bramtoula, Yun Chang, @lucacarlone1, @jumpjoe78 \n\nPaper: <LINK>\nCode: <LINK>\nVideo: <LINK>']",https://arxiv.org/abs/1909.12198,"To achieve collaborative tasks, robots in a team need to have a shared understanding of the environment and their location within it. Distributed Simultaneous Localization and Mapping (SLAM) offers a practical solution to localize the robots without relying on an external positioning system (e.g. GPS) and with minimal information exchange. Unfortunately, current distributed SLAM systems are vulnerable to perception outliers and therefore tend to use very conservative parameters for inter-robot place recognition. However, being too conservative comes at the cost of rejecting many valid loop closure candidates, which results in less accurate trajectory estimates. This paper introduces DOOR-SLAM, a fully distributed SLAM system with an outlier rejection mechanism that can work with less conservative parameters. DOOR-SLAM is based on peer-to-peer communication and does not require full connectivity among the robots. DOOR-SLAM includes two key modules: a pose graph optimizer combined with a distributed pairwise consistent measurement set maximization algorithm to reject spurious inter-robot loop closures; and a distributed SLAM front-end that detects inter-robot loop closures without exchanging raw sensor data. The system has been evaluated in simulations, benchmarking datasets, and field experiments, including tests in GPS-denied subterranean environments. DOOR-SLAM produces more inter-robot loop closures, successfully rejects outliers, and results in accurate trajectory estimates, while requiring low communication bandwidth. Full source code is available at this https URL ","DOOR-SLAM: Distributed, Online, and Outlier Resilient SLAM for Robotic
  Teams"
18,1177471034674405378,4249537197,Christian Wolf,"['New paper, ""CoPhy: Counterfactual Learning of Physical Dynamics"". Work by @fabienbaradel  and with @NataliaNeverova, Julien Mille, @greg_mori. We learn to predict how physical outcomes are affected by arbitrary interventions on initial conditions. <LINK> [Thread] <LINK>', '@fabienbaradel @NataliaNeverova @greg_mori Compared to classical feed-forward forecasting of the future, this task requires the (unsupervised) estimation of confounders (masses, friction, gravity), which are not observable from a single frame. https://t.co/7NxfH8aHxE', '@fabienbaradel @NataliaNeverova @greg_mori Our model learns counterfactual reasoning in a weakly supervised way with graph networks: while we supervise the do-operator, we do not supervise the confounder variables (masses, frictions, gravity). https://t.co/yYM2vo9ssg', '@fabienbaradel @NataliaNeverova @greg_mori We also propose benchmark suite consisting of three tasks and associated large-scale datasets: (BlockTowerCF task: 146k sequences; BallsCF task: 100k sequences; CollisionsCF: 40k sequences). https://t.co/GmM3OrABXP']",https://arxiv.org/abs/1909.12000,"Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance. ",CoPhy: Counterfactual Learning of Physical Dynamics
19,1177460669966241794,820031736914513925,Fabrizio Leisen,"['NEW PAPER: ""Compound vectors of subordinators and their associated L√©vy copulas"" written by Alan Riva Palacio and myself.   <LINK>']",https://arxiv.org/abs/1909.12112,"L\'evy copulas are an important tool which can be used to build dependent L\'evy processes. In a classical setting, they have been used to model financial applications. In a Bayesian framework they have been employed to introduce dependent nonparametric priors which allow to model heterogeneous data. This paper focuses on introducing a new class of L\'evy copulas based on a class of subordinators recently appeared in the literature, called \textit{Compound Random Measures}. The well-known Clayton L\'evy copula is a special case of this new class. Furthermore, we provide some novel results about the underlying vector of subordinators such as a series representation and relevant moments. The article concludes with an application to a Danish fire dataset. ","Compound vectors of subordinators and their associated positive L\'evy
  copulas"
20,1177411575113646081,1128508767404838912,Thayne Currie,"['<LINK>\n\nOur SPIE paper on ""Linear Dark Field Control"", a new wavefront control method focused on maintaing a region around a star cleared of glare, so we can better see faint planets.']",https://arxiv.org/abs/1909.11664,"Imaging rocky planets in reflected light, a key focus of future NASA missions and ELTs, requires advanced wavefront control to maintain a deep, temporally correlated null of stellar halo at just several diffraction beam widths. We discuss development of Linear Dark Field Control (LDFC) to achieve this aim. We describe efforts to test spatial LDFC in a laboratory setting for the first time, using the Ames Coronagraph Experiment (ACE) testbed. Our preliminary results indicate that spatial LDFC is a promising method focal-plane wavefront control method capable of maintaining a static dark hole, at least at contrasts relevant for imaging mature planets with 30m-class telescopes. ","Developing Linear Dark-Field Control for Exoplanet Direct Imaging in the
  Laboratory and on Ground-based Telescopes"
21,1177399837890772992,268919557,Joshua T. Vogelstein (jovo/he/we),"['we just posted a new paper to arxiv on closing the gap between random forests and deep learning, check it out and please let us know what you think?<LINK> @danilobzdok @KordingLab @bttyeo @frankdonaldwood @julietibs']",https://arxiv.org/abs/1909.11799,"Decision forests (Forests), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, Forests dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to a permutation of the feature indices. However, in structured data lying on a manifold (such as images, text, and speech) deep networks (Networks), specifically convolutional deep networks (ConvNets), tend to outperform Forests. We conjecture that at least part of the reason for this is that the input to Networks is not simply the feature magnitudes, but also their indices. In contrast, naive Forest implementations fail to explicitly consider feature indices. A recently proposed Forest approach demonstrates that Forests, for each node, implicitly sample a random matrix from some specific distribution. These Forests, like some classes of Networks, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach and show that one can choose distributions in a manifold-aware fashion to incorporate feature locality. We demonstrate the empirical performance on data whose features live on three different manifolds: a torus, images, and time-series. Moreover, we demonstrate its strength in multivariate simulated settings and also show superiority in predicting surgical outcome in epilepsy patients and predicting movement direction from raw stereotactic EEG data from non-motor brain regions. In all simulations and real data, Manifold Oblique Random Forest (MORF) algorithm outperforms approaches that ignore feature space structure and challenges the performance of ConvNets. Moreover, MORF runs fast and maintains interpretability and theoretical justification. ","Manifold Oblique Random Forests: Towards Closing the Gap on
  Convolutional Deep Networks"
22,1177397120099241984,48144742,Gill Verdon ‚öõÔ∏èüé≤ü§ñ,"['New quantum paper w/ @TheteamatX &amp; @GoogleAI!\n\nQuantum Graph Neural Networks: learning graph-based representions on quantum computers. \n\nSee threadüëá\n\n<LINK> <LINK>', ""We use GQNN's to learn to generate graph-based quantum states &amp; quantum dynamics. For classical problems, we tested on graph clustering / isomorphism tasks https://t.co/Oqv7hhnxD7"", ""As it turns out, the QAOA and Graph Convolutional networks had a lot to do with each other!\nWe cooked up an analog of GCN's for as a QNN. A first test was the task of spectral clustering of graphs. Both the multi-qubit per node and single-qubit precision ansatz perform quite well https://t.co/bCu15s99Jp"", 'Going from unsupervised learning to supervised now, we also tested this ansatz for identifying whether two graphs are isomorphic. The performance was quite impressive, even at a reasonable number of samples for the quantum computer. https://t.co/goKPrlyTHo', 'On the quantum data learning sude, a first task we tested was learning quantum dynamics and the effective Hamiltonian/topology of a quantum system from black box access to its dynamics. We introduced a Quantum Recurrent Graph Neural Net architecture for this task. https://t.co/Q49VUGlMWS', ""As a hint of things to come, we test applications of QGCNN's for learning quantum protocols for quantum networks (quantum internet). \n\nWe learn how to create resources for quantum sensor networks (multipartite entanglement) without the need for local variational parameters. https://t.co/XM8FsyIlSM"", 'Overall excited for future applications of graph-based quantum neural nets. Next step: quantum chemistry? ü§î', 'Huge shoutout to the team! Special shoutout to @vsingh_5  and @eluzhnica who did a fantastic job on their first quantum paper (coming from a classical ML background)\n\nBig thanks to @jackhidary for putting together the awesome Quantum@X and AI@X residency programs!']",https://arxiv.org/abs/1909.12264,"We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification. ",Quantum Graph Neural Networks
23,1177388474518953984,942850262145863680,Leonardo Andreta de Castro,"['New paper on arXiv with Michael Newman and @kenbrownquantum , ‚ÄúGenerating Fault-Tolerant Cluster States from Crystal Structures‚Äù: <LINK>\n\nSee how to create and benchmark these states using known crystal lattices or entirely new tilings, like the one below. <LINK>']",https://arxiv.org/abs/1909.11817,"Measurement-based quantum computing (MBQC) is a promising alternative to traditional circuit-based quantum computing predicated on the construction and measurement of cluster states. Recent work has demonstrated that MBQC provides a more general framework for fault-tolerance that extends beyond foliated quantum error-correcting codes. We systematically expand on that paradigm, and use combinatorial tiling theory to study and construct new examples of fault-tolerant cluster states derived from crystal structures. Included among these is a robust self-dual cluster state requiring only degree-3 connectivity. We benchmark several of these cluster states in the presence of circuit-level noise, and find a variety of promising candidates whose performance depends on the specifics of the noise model. By eschewing the distinction between data and ancilla, this malleable framework lays a foundation for the development of creative and competitive fault-tolerance schemes beyond conventional error-correcting codes. ",Generating Fault-Tolerant Cluster States from Crystal Structures
24,1177382708164644866,907621053547126785,Pratyush Tiwary,"['Preprint of new review just submitted to Current Opinion in Structural Biology. Not the easiest review to write given word limits &amp; a field changing so rapidly. Please let us know if we missed something crucial you did so mayb we can update actual paper <LINK>', '@MicheleCeriotti @JimPfaendtner Thanks, this is exactly why we put this up as a preprint', '@olexandr awwwwww. but look @olexandr how could I have forgotten you and @adrian_roitberg ? https://t.co/UumRLB6fIZ']",https://arxiv.org/abs/1909.11748,"Molecular dynamics (MD) has become a powerful tool for studying biophysical systems, due to increasing computational power and availability of software. Although MD has made many contributions to better understanding these complex biophysical systems, there remain methodological difficulties to be surmounted. First, how to make the deluge of data generated in running even a microsecond long MD simulation human comprehensible. Second, how to efficiently sample the underlying free energy surface and kinetics. In this short perspective, we summarize machine learning based ideas that are solving both of these limitations, with a focus on their key theoretical underpinnings and remaining challenges. ","Machine learning approaches for analyzing and enhancing molecular
  dynamics simulations"
25,1177135037394894849,2816968963,Miguel A.F. Sanju√°n,['Very glad that our new paper on delay-induced resonance will be published in the International Journal of Bifurcation and Chaos @worldscientific\n <LINK> <LINK>'],https://arxiv.org/abs/1909.11357,"The phenomenon of delay-induced resonance implies that in a nonlinear system a time-delay term may be used as an effective enhancer of the oscillations caused by an external forcing maintaining the same frequency. This is possible for the parameters for which the time-delay induces sustained oscillations. Here, we study this type of resonance in the overdamped and underdamped time-delayed Duffing oscillators, and we explore some new features. One of them is the conjugate phenomenon: the oscillations caused by the time-delay may be enhanced by means of the forcing without modifying their frequency. The resonance takes place when the frequency of the oscillations induced by the time-delay matches the ones caused by the forcing and vice versa. This is an interesting result as the nature of both perturbations is different. Even for the parameters for which the time-delay does not induce sustained oscillations, we show that a resonance may appear following a different mechanism. ",Delay-Induced Resonance in the Time-Delayed Duffing Oscillator
26,1177134423453700096,2816968963,Miguel A.F. Sanju√°n,['Delay-induced resonance implies that in a nonlinear system a time-delay term may be used as an effective enhancer of the oscillations caused by an external forcing maintaining the same frequency. New paper to be published in IJBC @worldscientific <LINK>'],https://arxiv.org/abs/1909.11357,"The phenomenon of delay-induced resonance implies that in a nonlinear system a time-delay term may be used as an effective enhancer of the oscillations caused by an external forcing maintaining the same frequency. This is possible for the parameters for which the time-delay induces sustained oscillations. Here, we study this type of resonance in the overdamped and underdamped time-delayed Duffing oscillators, and we explore some new features. One of them is the conjugate phenomenon: the oscillations caused by the time-delay may be enhanced by means of the forcing without modifying their frequency. The resonance takes place when the frequency of the oscillations induced by the time-delay matches the ones caused by the forcing and vice versa. This is an interesting result as the nature of both perturbations is different. Even for the parameters for which the time-delay does not induce sustained oscillations, we show that a resonance may appear following a different mechanism. ",Delay-Induced Resonance in the Time-Delayed Duffing Oscillator
27,1177057013844475904,900040276034674689,Zijian Wang,"['1) Our new #emnlp19 paper (w/ @ChrisGPotts) on condescension modeling: <LINK>\n\nCondescending language use is caustic. However, little work was done on modeling condescension due to its difficulty in detection without context. To address this, we present TalkDown,', '2) a labeled Reddit dataset of condescending linguistic acts in context. We show the importance of context, motivate techniques to deal with low rates of condescension, and use our model to estimate condescension rates in various subreddits. Data &amp; model: https://t.co/nJYiTEztyU']",http://arxiv.org/abs/1909.11272,"Condescending language use is caustic; it can bring dialogues to an end and bifurcate communities. Thus, systems for condescension detection could have a large positive impact. A challenge here is that condescension is often impossible to detect from isolated utterances, as it depends on the discourse and social context. To address this, we present TalkDown, a new labeled dataset of condescending linguistic acts in context. We show that extending a language-only model with representations of the discourse improves performance, and we motivate techniques for dealing with the low rates of condescension overall. We also use our model to estimate condescension rates in various online communities and relate these differences to differing community norms. ",TalkDown: A Corpus for Condescension Detection in Context
28,1177014379822239744,383142451,Shinnosuke Takamichi (È´òÈÅì ÊÖé‰πã‰ªã),"['Our new paper ""HumanGAN: generative adversarial network with human-based discriminator and its evaluation in speech perception modeling"" is available in arXiv!\n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.11391,"We propose the HumanGAN, a generative adversarial network (GAN) incorporating human perception as a discriminator. A basic GAN trains a generator to represent a real-data distribution by fooling the discriminator that distinguishes real and generated data. Therefore, the basic GAN cannot represent the outside of a real-data distribution. In the case of speech perception, humans can recognize not only human voices but also processed (i.e., a non-existent human) voices as human voice. Such a human-acceptable distribution is typically wider than a real-data one and cannot be modeled by the basic GAN. To model the human-acceptable distribution, we formulate a backpropagation-based generator training algorithm by regarding human perception as a black-boxed discriminator. The training efficiently iterates generator training by using a computer and discrimination by crowdsourcing. We evaluate our HumanGAN in speech naturalness modeling and demonstrate that it can represent a human-acceptable distribution that is wider than a real-data distribution. ","HumanGAN: generative adversarial network with human-based discriminator
  and its evaluation in speech perception modeling"
29,1176881065941721088,532752544,Michael Lopez,"[""I'm going to ask a question that no one expects me to: Did we get fourth down analysis (partially) wrong?  \n\nA new paper about the challenges of analyzing traditional NFL data, and where player tracking data can help. <LINK> <LINK>"", ""Here's why: teams that went for it on 4th-and-1's were, on average, 20% closer to the line to gain than teams that did not go for it on 4th-and-1. We weren't comparing apples to apples when looking at fourth down strategy, even when we thought we were. https://t.co/zbPfWaeNVS"", ""The article I've shared above is (hopefully) the introduction to this special JQAS issue. If you are reading the article and wondering why there is some blank space, it depends on which articles are accepted https://t.co/uEMFD4lJwu"", ""@Mike_Champagne Player-level data would potentially help strategy, although it's not the once I focused on here"", '@rtelmore That was the idea of an anonymous but intelligent reviewer. Idea is to imagine offense moving forward', ""@zbinney_NFLinj Not something to ignore, and likely a sensitivity analysis would be appropriate. I used a second identification strategy (footnote 2, page 4) and the differences were magnified. Additionally, I'm assuming measurement error differences are independent of yardage remaining"", '@903124S In the paper, those same contextual factors are accounted for']",https://arxiv.org/abs/1909.10631,"Most historical National Football League (NFL) analysis, both mainstream and academic, has relied on public, play-level data to generate team and player comparisons. Given the number of oft omitted variables that impact on-field results, such as play call, game situation, and opponent strength, findings tend to be more anecdotal than actionable. With the release of player tracking data, however, analysts can better ask and answer questions to isolate skill and strategy. In this article, we highlight the limitations of traditional analyses, and use a decades-old punching bag for analysts, fourth-down strategy, as a microcosm for why tracking data is needed. Specifically, we assert that, in absence of using the precise yardage needed for a first down, past findings supporting an aggressive fourth down strategy may have been overstated. Next, we synthesize recent work that comprises this special Journal of Quantitative Analysis in Sports issue into player tracking data in football. Finally, we conclude with some best practices and limitations regarding usage of this data. The release of player tracking data marks a transition for the league and its' analysts, and we hope this issue helps guide innovation in football analytics for years to come. ","Bigger data, better questions, and a return to fourth down behavior: an
  introduction to a special issue on tracking data in the National football
  League"
30,1176820212131401729,752490550452953088,Dr. Joanna DrƒÖ≈ºkowska,['My new paper is now available at <LINK>'],http://arxiv.org/abs/1909.10526,"Dust growth is often neglected when building models of protoplanetary disks due to its complexity and computational expense. However, it does play a major role in shaping the evolution of protoplanetary dust and planet formation. In this paper, we present a numerical model coupling 2-D hydrodynamic evolution of a protoplanetary disk, including a Jupiter-mass planet, and dust coagulation. This is obtained by including multiple dust fluids in a single grid-based hydrodynamic simulation and solving the Smoluchowski equation for dust coagulation on top of solving for the hydrodynamic evolution. We find that fragmentation of dust aggregates trapped in a pressure bump outside of the planetary gap leads to an enhancement in density of small grains. We compare the results obtained from the full coagulation treatment to the commonly used, fixed dust size approach and to previously applied, less computationally intensive methods for including dust coagulation. We find that the full coagulation results cannot be reproduced using the fixed-size treatment, but some can be mimicked using a relatively simple method for estimating the characteristic dust size in every grid cell. ","Including Dust Coagulation in Hydrodynamic Models of Protoplanetary
  Disks: Dust Evolution in the Vicinity of a Jupiter-mass Planet"
31,1176774043409756160,292313052,Martin Kilbinger,"['HSC weak-lensing analysis of XXL galaxy clusters, new paper today on arXiv: <LINK>']",https://arxiv.org/abs/1909.10524,"We present a weak-lensing analysis of X-ray galaxy groups and clusters selected from the XMM-XXL survey using the first-year data from the Hyper Suprime-Cam (HSC) Subaru Strategic Program. Our joint weak-lensing and X-ray analysis focuses on 136 spectroscopically confirmed X-ray-selected systems at 0.031 < z < 1.033 detected in the 25sqdeg XXL-N region. We characterize the mass distributions of individual clusters and establish the concentration-mass (c-M) relation for the XXL sample, by accounting for selection bias and statistical effects, and marginalizing over the remaining mass calibration uncertainty. We find the mass-trend parameter of the c-M relation to be \beta = -0.07 \pm 0.28 and the normalization to be c200 = 4.8 \pm 1.0 (stat) \pm 0.8 (syst) at M200=10^{14}Msun/h and z = 0.3. We find no statistical evidence for redshift evolution. Our weak-lensing results are in excellent agreement with dark-matter-only c-M relations calibrated for recent LCDM cosmologies. The level of intrinsic scatter in c200 is constrained as \sigma(\ln[c200]) < 24% (99.7% CL), which is smaller than predicted for the full population of LCDM halos. This is likely caused in part by the X-ray selection bias in terms of the relaxation state. We determine the temperature-mass (Tx-M500) relation for a subset of 105 XXL clusters that have both measured HSC lensing masses and X-ray temperatures. The resulting Tx-M500 relation is consistent with the self-similar prediction. Our Tx-M500 relation agrees with the XXL DR1 results at group scales, but has a slightly steeper mass trend, implying a smaller mass scale in the cluster regime. The overall offset in the Tx-M500 relation is at the $1.5\sigma$ level, corresponding to a mean mass offset of (34\pm 20)%. We also provide bias-corrected, weak-lensing-calibrated M200 and M500 mass estimates of individual XXL clusters based on their measured X-ray temperatures. ","Weak lensing Analysis of X-Ray-selected XXL Galaxy Groups and Clusters
  with Subaru HSC Data"
32,1176670060410101760,990433714948661250,Sergey Levine,"[""Need a break from ICLR? Check out our new paper about why hierarchical RL (sometimes) works well. Turns out that we don't *really* need HRL in many cases, but lessons from HRL can be quite useful.\n\n<LINK>\nw/ @ofirnachum, H. Tang, X. Liu, @shaneguML, @honglaklee <LINK>""]",https://arxiv.org/abs/1909.10618,"Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard ""shallow"" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement. ",Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?
33,1176666973553532930,601568012,Dr. Michelle Ntampaka,"['My new paper! <LINK>\n\nML can learn cosmological params from 3D galaxy catalogs even with all the complicating factors we could throw at it. Complex architecture to fold in physics (Fig 3), unbiased predictions in a 12D param space (8), and a few fun surprises (9)']",https://arxiv.org/abs/1909.10527,"We present a deep machine learning (ML)-based technique for accurately determining $\sigma_8$ and $\Omega_m$ from mock 3D galaxy surveys. The mock surveys are built from the AbacusCosmos suite of $N$-body simulations, which comprises 40 cosmological volume simulations spanning a range of cosmological models, and we account for uncertainties in galaxy formation scenarios through the use of generalized halo occupation distributions (HODs). We explore a trio of ML models: a 3D convolutional neural network (CNN), a power-spectrum-based fully connected network, and a hybrid approach that merges the two to combine physically motivated summary statistics with flexible CNNs. We describe best practices for training a deep model on a suite of matched-phase simulations and we test our model on a completely independent sample that uses previously unseen initial conditions, cosmological parameters, and HOD parameters. Despite the fact that the mock observations are quite small ($\sim0.07h^{-3}\,\mathrm{Gpc}^3$) and the training data span a large parameter space (6 cosmological and 6 HOD parameters), the CNN and hybrid CNN can constrain $\sigma_8$ and $\Omega_m$ to $\sim3\%$ and $\sim4\%$, respectively. ","A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy
  Redshift Surveys"
34,1176666676160413696,2337598033,Geraint F. Lewis,"['Another new paper no the arXiv with @bigticketdw @nfmartin1980 @michelle_lmc and @dougalmackey \n\n<LINK> <LINK>', '@dougalmackey @bigticketdw @nfmartin1980 @michelle_lmc You must have had pretty hippy parents to name you 8!']",https://arxiv.org/abs/1909.11036,"We present the star formation histories (SFHs) of 20 faint M31 satellites ($-12 \lesssim M_V \lesssim -6$) that were measured by modeling sub-horizontal branch (HB) depth color-magnitude diagrams constructed from Hubble Space Telescope (HST) imaging. Reinforcing previous results, we find that virtually all galaxies quenched between 3 and 9 Gyr ago, independent of luminosity, with a notable concentration $3-6$ Gyr ago. This is in contrast to the Milky Way (MW) satellites, which are generally either faint with ancient quenching times or luminous with recent ($<3$ Gyr) quenching times. We suggest that systematic differences in the quenching times of M31 and MW satellites may be a reflection of the varying accretion histories of M31 and the MW. This result implies that the formation histories of low-mass satellites may not be broadly representative of low-mass galaxies in general. Among the M31 satellite population we identify two distinct groups based on their SFHs: one with exponentially declining SFHs ($\tau \sim 2$ Gyr) and one with rising SFHs with abrupt quenching. We speculate how these two groups could be related to scenarios for a recent major merger involving M31. The Cycle 27 HST Treasury survey of M31 satellites will provide well-constrained ancient SFHs to go along with the quenching times we measure here. The discovery and characterization of M31 satellites with $M_V \gtrsim -6$ would help quantify the relative contributions of reionization and environment to quenching of the lowest-mass satellites. ","Comparing the Quenching Times of Faint M31 and Milky Way Satellite
  Galaxies"
35,1176660747931148289,838292815,Ofir Nachum,"['new paper! <LINK>\nWe investigate the underlying reasons for success of hierarchical RL, finding that (surprisingly) much of it is due to exploration, and that this benefit can be achieved *without* explicit hierarchies of policies @svlevine @shaneguML @honglaklee']",https://arxiv.org/abs/1909.10618,"Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard ""shallow"" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement. ",Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?
36,1176656810201104384,1115880604560691200,NII Yamagishi Lab,"['New paper accepted at ASRU 2019:  ""Bootstrapping non-parallel voice conversion from speaker-adaptive text-to-speech,"" <LINK>']",https://arxiv.org/abs/1909.06532,"Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a similar objective, generating speech with a target voice. However, they are usually developed independently under vastly different frameworks. In this paper, we propose a methodology to bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Moreover by offloading the heavy data demand to the training stage of the TTS model, our VC system can be built using a small amount of target speaker speech data. It also opens up the possibility of using speech in a foreign unseen language to build the system. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language. ","Bootstrapping non-parallel voice conversion from speaker-adaptive
  text-to-speech"
37,1176616431544414209,167951109,Niall Madden,"[""For a bit of night-time reading, here's a new paper on the arxiv, on a FOSLS FEM for reaction-diffusion PDEs (using weighted norms: should've thought of that years ago). \nIt's continuous!\nIt's coercive!\nAnd it's implemented using @FiredrakeFEM!\n<LINK>""]",https://arxiv.org/abs/1909.08598,"We propose a new first-order-system least squares (FOSLS) finite-element discretization for singularly perturbed reaction-diffusion equations. Solutions to such problems feature layer phenomena, and are ubiquitous in many areas of applied mathematics and modelling. There is a long history of the development of specialized numerical schemes for their accurate numerical approximation. We follow a well-established practice of employing a priori layer-adapted meshes, but with a novel finite-element method that yields a symmetric formulation while also inducing a so-called ""balanced"" norm. We prove continuity and coercivity of the FOSLS weak form, present a suitable piecewise uniform mesh, and report on the results of numerical experiments that demonstrate the accuracy and robustness of the method. ","First-order system least squares finite-elements for singularly
  perturbed reaction-diffusion equations"
38,1176427748543229952,197729166,Geoff Chih-Fan Chen,['Congrats to Nikki Arendse. She is a new graduate student in our team and just put out her new paper on the arXiv: Cosmic dissonance: new physics or systematics behind a short sound horizon? \n<LINK>'],https://arxiv.org/abs/1909.07986,"Persistent tension between low-redshift observations and the Cosmic Microwave Background radiation (CMB), in terms of two fundamental distance scales set by the sound horizon $r_d$ and the Hubble constant $H_0$, suggests new physics beyond the Standard Model or residual systematics. We examine recently updated distance calibrations from Cepheids, gravitational lensing time-delay observations, and the Tip of the Red Giant Branch. Calibrating the Baryon Acoustic Oscillations (BAO) and Type Ia supernovae with combinations of the distance indicators, we obtain a joint and self-consistent measurement of $H_0$ and $r_d$ at low redshift, independent of cosmological models and CMB inference. In an attempt to alleviate the tension between late-time and CMB-based measurements, we consider four extensions of the standard $\Lambda$CDM model. The sound horizon from our different measurements is $r_d=(137\pm3^{stat.}\pm2^{syst.})$~Mpc. Depending on the adopted distance indicators, the $combined$ tension in $H_0$ and $r_d$ ranges between 2.3 and 5.1 $\sigma$. We find that modifications of $\Lambda$CDM that change the physics after recombination fail to solve the problem, for the reason that they only resolve the tension in $H_0$, while the tension in $r_d$ remains unchanged. Pre-recombination extensions (with early dark energy or the effective number of neutrinos $\rm{N}_{\rm{eff}}=3.24 \pm 0.16$) are allowed by the data, unless the calibration from Cepheids is included. Results from time-delay lenses are consistent with those from distance-ladder calibrations and point to a discrepancy between absolute distance scales measured from the CMB (assuming the standard cosmological model) and late-time observations. New proposals to resolve this tension should be examined with respect to reconciling not only the Hubble constant but also the sound horizon derived from the CMB and other cosmological probes. ","Cosmic dissonance: new physics or systematics behind a short sound
  horizon?"
39,1176417709770334209,1030005672090451969,Catherine Walsh,"['New paper led by @marievdsande : modelling the composition of an AGB outflow with surface chemistry : for high-density oxygen-rich outflows with cool dust, dust grains are delivered to the interstellar medium coated in (perhaps sticky) molecules üåüüß™ \n<LINK>']",https://arxiv.org/abs/1909.10410,"Chemical modelling of AGB outflows is typically focused on either non-thermodynamic equilibrium chemistry in the inner region or photon-driven chemistry in the outer region. We include, for the first time, a comprehensive dust-gas chemistry in our AGB outflow chemical kinetics model, including both dust-gas interactions and grain-surface chemistry. The dust is assumed to have formed in the inner region, and follows an interstellar-like dust-size distribution. Using radiative transfer modelling, we obtain dust temperature profiles for different dust types in an O-rich and a C-rich outflow. We calculate a grid of models, sampling different outflow densities, drift velocities between the dust and gas, and dust types. Dust-gas chemistry can significantly affect the gas-phase composition, depleting parent and daughter species and increasing the abundance of certain daughter species via grain-surface formation followed by desorption/sputtering. Its influence depends on four factors: outflow density, dust temperature, initial composition, and drift velocity. The largest effects are for higher density outflows with cold dust and O-rich parent species, as these species generally have a larger binding energy. At drift velocities larger than $\sim 10$ km s$^{-1}$, ice mantles undergo sputtering; however, they are not fully destroyed. Models with dust-gas chemistry can better reproduce the observed depletion of species in O-rich outflows. When including colder dust in the C-rich outflows and adjusting the binding energy of CS, the depletion in C-rich outflows is also better reproduced. To best interpret high-resolution molecular line observations from AGB outflows, dust-gas interactions are needed in chemical kinetics models. ","Dust-gas chemistry in AGB outflows: Chemical modelling of dust-gas
  chemistry within AGB outflows I. Effect on the gas-phase chemistry"
40,1176390510602272768,2503999452,Arnau Rios,"['The Hellmann-Feynman #theorem is a useful result of quantum mechanics. In a new paper with friends from @ICC_UB @FisicaUB @INFN_ we use its finite temperature extension to analyse 3 models that are useful for #physics #teaching \n<LINK> @NucTheorySurrey <LINK>', '@IsospinSymmetry @ICC_UB @FisicaUB @INFN_ @NucTheorySurrey Thanks Andrew! This is very useful feedback. Hope you liked it!']",https://arxiv.org/abs/1909.10298,"We present a simple derivation of the Hellmann-Feynman theorem at finite temperature. We illustrate its validity by considering three relevant examples which can be used in quantum mechanics lectures: the one-dimensional harmonic oscillator, the one-dimensional Ising model and the Lipkin model. We show that the Hellmann-Feynman theorem allows one to calculate expectation values of operators that appear in the Hamiltonian. This is particularly useful when the total free-energy is available, but there is not direct access to the thermal average of the operators themselves. ",The Hellmann-Feynman theorem at finite temperature
41,1176311637399072769,2337598033,Geraint F. Lewis,"['New paper on the Arxiv with @michelle_lmc, @eteq, @dougalmackey and @nfmartin1980 \n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.09661,"We present a kinematic and spectroscopic analysis of 38 red giant branch stars, in 7 fields, spanning the dwarf spheroidal galaxy Andromeda XXVII and the upper segment of the North West Stream. Both features are located in the outer halo of the Andromeda galaxy at a projected radius of 50-80 kpc, with the stream extending for $\sim$3$^{\circ}$ on the sky. Our data is obtained as part of the PAndAS survey and enables us to confirm that Andromeda XXVII's heliocentric distance is 827 $\pm$ 47 kpc and spectroscopic metallicity is -2.1$^{+0.4}_{-0.5}$. We also re-derive Andromeda XXVII's kinematic properties, measuring a systemic velocity = -526.1$^{+10.0}_{-11.0}${\kms} and a velocity dispersion that we find to be non-Gaussian but for which we derive a formal value of 27.0$^{+2.2}_{-3.9}${\kms}. In the upper segment of the North West Stream we measure mean values for the metallicity = -1.8$\pm$0.4, systemic velocity = -519.4 $\pm$4.0{\kms} and velocity dispersion = 10.0$\pm$4.0{\kms}. We also detect a velocity gradient of 1.7$\pm$0.3 {\kms} kpc$^{-1}$ on an infall trajectory towards M31. With a similar gradient, acting in the same direction, in the lower segment we suggest that the North West Stream is not a single structure. As the properties of the upper segment of the North West Stream and Andromeda XXVII are consistent within ~90\% confidence limits, it is likely that the two are related and plausible that Andromeda XXVII is the progenitor of this stream. ",A Dwarf Disrupting -- Andromeda XXVII and the North West Stream
42,1176310990729670656,1075649842955866114,Luca Cortese,['New paper out today by @ICRAR @UWAresearch @ARC_ASTRO3D PhD student @AstroRobbo studying the role of bulges on the global atomic gas content of galaxies in the #xGASS survey. <LINK> <LINK>'],https://arxiv.org/abs/1909.10202,"We present a structural decomposition analysis of the galaxies in the extended GALEX Arecibo SDSS Survey (xGASS) using (gri) images from the Sloan Digital Sky Survey. Utilising the 2D Bayesian light profile fitting code ProFit, we fit single- and double-component models taking advantage of a robust Markov chain Monte Carlo optimisation algorithm in which we assume a Sersic profile for single-component models and a combination of a Sersic bulge and near-exponential disc (0.5 < n < 1.5) for double-component models. We investigate the effect of bulges on the atomic hydrogen (HI) content in galaxies by revisiting the HI-to-stellar mass scaling relations with the bulge-to-total ratio measured in the ProFit decompositions. We show that, at both fixed total and disc stellar mass, more bulge-dominated galaxies have systematically lower HI masses, implying that bulge-dominated galaxies with large HI reservoirs are rare in the local Universe. We see similar trends when separating galaxies by a bulge-to-total ratio based either on luminosity or stellar mass, however, the trends are more evident with luminosity. Importantly, when controlling for both stellar mass and star formation rate, the separation of atomic gas content reduces to within 0.3 dex between galaxies of different bulge-to-total ratios. Our findings suggest that the presence of a photometric bulge has little effect on the global HI gas reservoirs of local galaxies. ","xGASS: The Impact of Photometric Bulges on the Scatter of HI Scaling
  Relations"
43,1176229083707072518,69282116,Fotios Petropoulos,"['New paper: ""D√©j√† vu: Forecasting with similarity""\nA model-free way to forecasting using cross-learning. When history repeats itself, why relying on statistical models that assume specific data generation processes?\n<LINK> @YanfeiKang @f3ngli @fsu_ntua']",https://arxiv.org/abs/1909.00221,"Accurate forecasts are vital for supporting the decisions of modern companies. Forecasters typically select the most appropriate statistical model for each time series. However, statistical models usually presume some data generation process while making strong assumptions about the errors. In this paper, we present a novel data-centric approach -- `forecasting with similarity', which tackles model uncertainty in a model-free manner. Existing similarity-based methods focus on identifying similar patterns within the series, i.e., `self-similarity'. In contrast, we propose searching for similar patterns from a reference set, i.e., `cross-similarity'. Instead of extrapolating, the future paths of the similar series are aggregated to obtain the forecasts of the target series. Building on the cross-learning concept, our approach allows the application of similarity-based forecasting on series with limited lengths. We evaluate the approach using a rich collection of real data and show that it yields competitive accuracy in both points forecasts and prediction intervals. ","D\'ej\`a vu: A data-centric forecasting approach through time series
  cross-similarity"
44,1176208931141685248,704799523,francesca dominici,['New paper in <LINK> by Rachel Nethery: Causal Inference and machine learning approaches for evaluation of the health impacts of large-scale air quality regulations'],https://arxiv.org/abs/1909.09611,"We develop a causal inference approach to estimate the number of adverse health events prevented by large-scale air quality regulations via changes in exposure to multiple pollutants. This approach is motivated by regulations that impact pollution levels in all areas within their purview. We introduce a causal estimand called the Total Events Avoided (TEA) by the regulation, defined as the difference in the expected number of health events under the no-regulation pollution exposures and the observed number of health events under the with-regulation pollution exposures. We propose a matching method and a machine learning method that leverage high-resolution, population-level pollution and health data to estimate the TEA. Our approach improves upon traditional methods for regulation health impact analyses by clarifying the causal identifying assumptions, utilizing population-level data, minimizing parametric assumptions, and considering the impacts of multiple pollutants simultaneously. To reduce model-dependence, the TEA estimate captures health impacts only for units in the data whose anticipated no-regulation features are within the support of the observed with-regulation data, thereby providing a conservative but data-driven assessment to complement traditional parametric approaches. We apply these methods to investigate the health impacts of the 1990 Clean Air Act Amendments in the US Medicare population. ","Causal inference and machine learning approaches for evaluation of the
  health impacts of large-scale air quality regulations"
45,1176181071727095808,888216099757490176,Maithra Raghu,"['Rapid Learning or Feature Reuse? \n\nNew paper: <LINK>\n\nWe analyze MAML (and meta-learning and meta learning more broadly) finding that feature reuse is the critical component in the efficient learning of new tasks -- leading to some algorithmic simplifications! <LINK>', '@solvay_1927 There are definitely connections, though in this paper we mostly looked at the standard setups for few-shot learning. Lots of open questions for future work!']",https://arxiv.org/abs/1909.09157,"An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly. ","Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness
  of MAML"
46,1176160049124773888,988835227354238977,Dongyeop Kang (DK),"['a new style language dataset, PASTEL: ~41K parallel sentences/stories annotated across different personas (e.g., gender, political view)\npaper: <LINK> \ndata: <LINK> - @dongyeopkang @VarunGangal @ehovy\n#emnlp2019 #NLProc #StyleTransfer @LTIatCMU <LINK>', ""@VarunGangal @ehovy @LTIatCMU [2/3] But how'd you elicit annotators to write stuff in their style but preserved meaning? Directly giving the same sentence? How about image-stories? That's better, but won't meanings drift? How about Image+Keywords! We exp. and choose the best among five denotation settings. https://t.co/RlKKwzxNUg"", '@VarunGangal @ehovy @LTIatCMU [3/3] title: ""(Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas"" to appear at EMNLP 2019.', ""@ink__pad @VarunGangal @ehovy @LTIatCMU @yoavgo Thank you for the pointer to Yoav's paper. We didn't aware of it at the time of submission. It looks relevant except that our parallel styles are mostly persona.""]",http://arxiv.org/abs/1909.00098,"Stylistic variation in text needs to be studied with different aspects including the writer's personal traits, interpersonal relations, rhetoric, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains ~41K parallel sentences (8.3K parallel stories) annotated across different personas. Each persona has different styles in conjunction: gender, age, country, political view, education, ethnic, and time-of-writing. The dataset is collected from human annotators with solid control of input denotation: not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the dataset on two interesting applications of style language, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple supervised model with our parallel text outperforms the unsupervised models using nonparallel text in style transfer. Our dataset is publicly available. ","(Male, Bachelor) and (Female, Ph.D) have different connotations:
  Parallelly Annotated Stylistic Language Dataset with Multiple Personas"
47,1176081762948173824,14408924,Chris Dyer,"['New paper <LINK> with Phil Blunsom and @GaborMelis showing that regular LSTMs can ""learn syntax"" as well as the Ordered Neurons LSTMs of Shen et al. (ICLR 2019) ... but that\'s only because the ""PRPN"" parsing algorithm is biased. 1/2', '@GaborMelis Our analysis of the algorithm shows it cannot find most parses, and the ones it does are likely to be right-branching, which inflates English accuracy, but it is a bad algorithm for studying the general problem of syntax induction, since not all languages are right-branching. 2/2']",http://arxiv.org/abs/1909.09428,"A series of recent papers has used a parsing algorithm due to Shen et al. (2018) to recover phrase-structure trees based on proxies for ""syntactic depth."" These proxy depths are obtained from the representations learned by recurrent language models augmented with mechanisms that encourage the (unsupervised) discovery of hierarchical structure latent in natural language sentences. Using the same parser, we show that proxies derived from a conventional LSTM language model produce trees comparably well to the specialized architectures used in previous work. However, we also provide a detailed analysis of the parsing algorithm, showing (1) that it is incomplete---that is, it can recover only a fraction of possible trees---and (2) that it has a marked bias for right-branching structures which results in inflated performance in right-branching languages like English. Our analysis shows that evaluating with biased parsing algorithms can inflate the apparent structural competence of language models. ",A Critical Analysis of Biased Parsers in Unsupervised Parsing
48,1175515759977611267,980451840180523014,Grigorios Kalliatakis,"[""Our new paper is now online: 'Class Feature Pyramids for Video Explanation' <LINK>. We created a a plug-and-play visualisation method specifically designed for 3D-CNNs. Paper will be presented at #ICCV2019 workshop on Interpreting and Explaining Visual AI Models <LINK>"", 'Code is available online at https://t.co/tLPkTj4uuv']",https://arxiv.org/abs/1909.08611,"Deep convolutional networks are widely used in video action recognition. 3D convolutions are one prominent approach to deal with the additional time dimension. While 3D convolutions typically lead to higher accuracies, the inner workings of the trained models are more difficult to interpret. We focus on creating human-understandable visual explanations that represent the hierarchical parts of spatio-temporal networks. We introduce Class Feature Pyramids, a method that traverses the entire network structure and incrementally discovers kernels at different network depths that are informative for a specific class. Our method does not depend on the network's architecture or the type of 3D convolutions, supporting grouped and depth-wise convolutions, convolutions in fibers, and convolutions in branches. We demonstrate the method on six state-of-the-art 3D convolution neural networks (CNNs) on three action recognition (Kinetics-400, UCF-101, and HMDB-51) and two egocentric action recognition datasets (EPIC-Kitchens and EGTEA Gaze+). ",Class Feature Pyramids for Video Explanation
49,1175379596432367616,73389062,S Ramamoorthy,"['How hard is it to cleanly scoop a slice of grapefruit? It can be a hard task for a robot, but we are teaching our robot new tricks! Read more about this in our paper: <LINK>\n\nAlso, watch our robot in action here:\n<LINK>\n\n@EDINrobotics @InfAtEd']",https://arxiv.org/abs/1909.07247,"Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory fit using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control strategy is successful in 72 % of attempts in contrast to control using a nominal trajectory, which only succeeds in 50 % of attempts. ","Surfing on an uncertain edge: Precision cutting of soft tissue using
  torque-based medium classification"
50,1175154839791296512,3030169341,Pedro Machado,['My new paper about convolution spiking neural networks <LINK>'],https://arxiv.org/abs/1909.08288,"Biological image processing is performed by complex neural networks composed of thousands of neurons interconnected via thousands of synapses, some of which are excitatory and others inhibitory. Spiking neural models are distinguished from classical neurons by being biological plausible and exhibiting the same dynamics as those observed in biological neurons. This paper proposes a Natural Convolutional Neural Network (NatCSNN) which is a 3-layer bio-inspired Convolutional Spiking Neural Network (CSNN), for classifying objects extracted from natural images. A two-stage training algorithm is proposed using unsupervised Spike Timing Dependent Plasticity (STDP) learning (phase 1) and ReSuMe supervised learning (phase 2). The NatCSNN was trained and tested on the CIFAR-10 dataset and achieved an average testing accuracy of 84.7% which is an improvement over the 2-layer neural networks previously applied to this dataset. ","NatCSNN: A Convolutional Spiking Neural Network for recognition of
  objects extracted from natural images"
51,1175053119790600192,573729628,"Steve Taylor, PhD","[""Fantastic new paper led by NANOGrav postdoc Jeff Hazboun studying the temporal evolution of pulsar-timing array GW background upper-limits and detection statistics. A vital piece of work to show that we're on the right track and getting super close! <LINK>"", '@JeffreyHazboun']",https://arxiv.org/abs/1909.08644,"An ensemble of inspiraling supermassive black hole binaries should produce a stochastic background of very low frequency gravitational waves. This stochastic background is predicted to be a power law, with a spectral index of -2/3, and it should be detectable by a network of precisely timed millisecond pulsars, widely distributed on the sky. This paper reports a new ""time slicing"" analysis of the 11-year data release from the North American Nanohertz Observatory for Gravitational Waves (NANOGrav) using 34 millisecond pulsars. Methods to flag potential ""false positive"" signatures are developed, including techniques to identify responsible pulsars. Mitigation strategies are then presented. We demonstrate how an incorrect noise model can lead to spurious signals, and show how independently modeling noise across 30 Fourier components, spanning NANOGrav's frequency range, effectively diagnoses and absorbs the excess power in gravitational-wave searches. This results in a nominal, and expected, progression of our gravitational-wave statistics. Additionally we show that the first interstellar medium event in PSR J1713+0747 pollutes the common red noise process with low-spectral index noise, and use a tailored noise model to remove these effects. ","The NANOGrav 11-Year Data Set: Evolution of Gravitational Wave
  Background Statistics"
52,1174941146692218881,1968365508,Samaya Nissanke (she/her) üíô,"['New paper by incoming @GRAPPAInstitute fellow Mukherjee on velocity debiasing for gravitational wave standard sirens \n<LINK> critical re precision cosmology for these systems \nCongrats Suvodip!', '@DScol @GRAPPAInstitute Thank you! Hopefully Mukerjee will be here for your visit!']",https://arxiv.org/abs/1909.08627,"Gravitational wave (GW) sources are an excellent probe of the luminosity distance and offer a novel measure of the Hubble constant, $H_0$. This estimation of $H_0$ from standard sirens requires an accurate estimation of the cosmological redshift of the host galaxy of the GW source, after correcting for its peculiar velocity. Absence of an accurate peculiar velocity correction affects both the precision and accuracy of the measurement of $H_0$, particularly for nearby sources. We propose a framework to incorporate such a peculiar velocity correction for GW sources. A first implementation of our method to the event GW170817 combined with the Very Large Baseline Interferometry (VLBI) observation leads to a revised value of $H_0= 68.3^{+ 4.6}_{-4.5}$ km/s/Mpc. While this revision is minor, it demonstrates that our method makes it possible for obtaining an unbiased and accurate measurements of $H_0$ at the precision required for the standard siren cosmology. ","Velocity correction for Hubble constant measurements from standard
  sirens"
53,1174836911900413953,830355049,Mohit Bansal,['New @EMNLP2019 paper by @EasonNie (+@wang_songhe) revealing importance of joint semantic retrieval for downstream machine-reading-at-scale (SotA gains on #HotpotQA/#FEVER + extensive analysis)!üôÇ\n\n<LINK>\n<LINK>\n<LINK> (full-wiki) <LINK>'],https://arxiv.org/abs/1909.08041,"Machine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by ""reading"" information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling. Code/data made publicly available at: this https URL ","Revealing the Importance of Semantic Retrieval for Machine Reading at
  Scale"
54,1174362919405936640,793285241515323392,Matthew Wilson,"['How to learn policies that can reason about and manipulate many objects simultaneously?\nNew sim-to-real manipulation paper, accepted to Conference on Robot Learning (CoRL 2019)!\n\nw/ Tucker Hermans\n\npdf: <LINK>\nwebsite: <LINK> <LINK>', 'By using raw objects poses from a simulator, we learn representations that accurately capture the state information of a multi-object environment.\n\nWe use these representations in an asymmetric actor critic style to learn manipulation policies that work in the real world. https://t.co/pOqSG6co8B', 'Unlike prior work in this area, we handle a variably-sized set of objects\n\nOur loss function directly incentivizes the representations to capture multi-object information\n\nWe train two encoders: (1) CNN that maps from RGB (2) graph neural network that maps directly from raw state', 'The CNN is used in a policy network that transfers to the real world (via domain randomization).\nThe graph neural network (GNN) representation is more stable and accurate so is used in value estimation and goal/reward specification. https://t.co/uP1gk28VzF', ""As a side note, we use the GNN formalism, as defined in this DeepMind paper (https://t.co/Xn4QfrYN16). \n \nBut really we just use a Transformer's MHDPA, which can be interpreted as a special type of GNN.  This is similar to what DeepMind and OpenAI have done in recent work."", 'We find that our approach learns effective multi-object manipulation policies. And by applying domain randomization, we can enable them to transfer well to the real world.', 'We think this is an interesting direction, in part, because it allows precise goal specification. Agent understands world in terms of œï, where œï is a representation built from only relevant state info.\n\nGoals are then simply to navigate from œï_current to œï_desired.', 'This avoids issues of ambiguity from specifying concepts in image space or having to collect goal images (where there is e.g., occlusion).\n\nRobot, instead, is given a desired raw state of the world to achieve and it learns a goal-conditioned policy for how to get there.', 'This approach could also be extended to incorporate more detailed simulator state information, like object meshes or object configs (such as if a bottle is opened or closed).']",https://arxiv.org/abs/1909.07876,"We propose a method for sim-to-real robot learning which exploits simulator state information in a way that scales to many objects. We first train a pair of encoder networks to capture multi-object state information in a latent space. One of these encoders is a CNN, which enables our system to operate on RGB images in the real world; the other is a graph neural network (GNN) state encoder, which directly consumes a set of raw object poses and enables more accurate reward calculation and value estimation. Once trained, we use these encoders in a reinforcement learning algorithm to train image-based policies that can manipulate many objects. We evaluate our method on the task of pushing a collection of objects to desired tabletop regions. Compared to methods which rely only on images or use fixed-length state encodings, our method achieves higher success rates, performs well in the real world without fine tuning, and generalizes to different numbers and types of objects not seen during training. ","Learning to Manipulate Object Collections Using Grounded State
  Representations"
55,1174351372549943296,243126428,Jaemin Cho,"['New #emnlp2019 paper on diverse sequence generation with @seo_minjoon @HannaHajishirzi\nTL;DR) Separating diversification from generation improves both diversity &amp; accuracy in question generation and summarization\n‚Ä¢ Paper: <LINK>\n‚Ä¢ Code: <LINK> <LINK>', '@seo_minjoon @HannaHajishirzi We explicitly separate diversification from generation using a mixture-of-experts content selection module (called Selector) that guides an encoder-decoder model.', '@seo_minjoon @HannaHajishirzi Two-stage method:\n(1) Diversification: Selector samples different binary masks (called focus; m1, m2, and m3 in the figure) on a source sequence.\n\n(2) Generation: an encoder-decoder model generates different sequences from the source sequence guided by different masks.', '@seo_minjoon @HannaHajishirzi Not only does this improve the diversity of the generated sequences, but also improves accuracy (high fidelity) of them, since conventional MLE models often learn suboptimal mapping that is in the middle of the targets but not near any of them.']",https://arxiv.org/abs/1909.01953,"Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state of the art model. Our code is publicly available at this https URL ",Mixture Content Selection for Diverse Sequence Generation
56,1174245428013019136,1002942405157507073,Avik Pal,['Excited to release a new version of TorchGAN! It now contains a larger set of (new) GAN models and losses in @PyTorch.\n<LINK>. Install via `pip install torchgan`.\nPaper - <LINK>'],https://arxiv.org/abs/1909.03410,"TorchGAN is a PyTorch based framework for writing succinct and comprehensible code for training and evaluation of Generative Adversarial Networks. The framework's modular design allows effortless customization of the model architecture, loss functions, training paradigms, and evaluation metrics. The key features of TorchGAN are its extensibility, built-in support for a large number of popular models, losses and evaluation metrics, and zero overhead compared to vanilla PyTorch. By using the framework to implement several popular GAN models, we demonstrate its extensibility and ease of use. We also benchmark the training time of our framework for said models against the corresponding baseline PyTorch implementations and observe that TorchGAN's features bear almost zero overhead. ",TorchGAN: A Flexible Framework for GAN Training and Evaluation
57,1174124249910599680,1015053310603284480,Stephen Kane,"['A new paper by my student Colby Ostberg highlights the most promising Venus analog candidates among TESS planets. You can read it here: <LINK>', '@itsatrappist You sure about that Cayman? They might have water vapor!! üòÅ']",https://arxiv.org/abs/1909.07456,"The transit method is biased toward short orbital period planets that are interior to their host star's Habitable Zone (HZ). These planets are particularly interesting from the perspective of exploring runaway greenhouse scenarios and the possibility of potential Venus analogs. Here, we conduct an analysis of predicted TESS planet yield estimates produced by Huang et al. (2018), as well as the TESS Object of Interest (TOI) list resulting from the observations of sectors 1 - 13 during Cycle 1 of the TESS primary mission. In our analysis we consider potential terrestrial planets that lie within their host star's Venus Zone (Kane et al. 2014). These requirements are then applied to a predicted planetary yield from the TESS primary mission (Huang et al. 2018) and the TOI list, which results in an estimated 259 Venus analogs by the end of the TESS primary mission, and 46 Venus analogs in the TOI list for sectors 1 - 13. We also calculate the estimated transmission spectroscopy signal-to-noise ratio (S/N) for Venus analogs from the predicted yield and TOI list if they were to be observed by the Near-Infrared Imager and Slitless Spectrograph (NIRISS) on the James Webb Space Telescope (JWST), as well as update the S/N cutoff values determined by Kempton et al. (2018). Our findings show that the best estimated Venus analogs and TOI Venus analogs with $R_{p} < 1.5 \, R_\odot$ have an estimated transmission spectroscopy S/N $> 40$ while planets with radii $2 \, R_\oplus < R_p < 4 \, R_\oplus$ can achieve S/N $> 100$ ","Predicting the Yield of Potential Venus Analogs from TESS and their
  Potential for Atmospheric Characterization"
58,1173995339092779008,383107127,Jonathan A. Michaels,"['I love this paper (<LINK>), but I get slightly nervous when I see figures likes this. All models in this chart were created before Brain-Score was published. What can we conclude from a new architecture that does extremely well using that metric? <LINK>', 'I\'m definitely guilty of this kind of ""circularity"" as well. Maybe others like @recursus and @dyamins can comment on how we can better address the similarity between models and the brain going forward.']",https://arxiv.org/abs/1909.06161,"Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S ""IT"" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream. ","Brain-Like Object Recognition with High-Performing Shallow Recurrent
  ANNs"
59,1173958739898118144,2361934191,Louigi Addario-Berry,"['Just posted a new paper on ""Hipster random walks"" to the arXiv. Joint work with 4 coauthors, all hipper than me. <LINK>']",https://arxiv.org/abs/1909.07367,"We introduce and study a family of random processes on trees we call hipster random walks, special instances of which we heuristically connect to the min-plus binary trees introduced by Robin Pemantle and studied by Auffinger and Cable (2017; arXiv:1709.07849), and to the critical random hierarchical lattice studied by Hambly and Jordan (2004). We prove distributional convergence for the processes by showing that their evolutions can be understood as a discrete analogues of certain convection-diffusion equations, then using a combination of coupling arguments and results from the numerical analysis literature on convergence of numerical approximations of PDEs. ",Hipster random walks
60,1173925680452706304,336358884,Swati Gupta,"['How to find a tree in an electricity distribution network to send power to various houses so that the power loss is minimized? Check out our new paper for the first provable approximation guarantees! Joint work w/ A. Khodabakhsh, H. Mortagy, E. Nikolova: <LINK>']",https://arxiv.org/abs/1909.04759,"The network reconfiguration problem seeks to find a rooted tree $T$ such that the energy of the (unique) feasible electrical flow over $T$ is minimized. The tree requirement on the support of the flow is motivated by operational constraints in electricity distribution networks. The bulk of existing results on convex optimization over vertices of polytopes and on the structure of electrical flows do not easily give guarantees for this problem, while many heuristic methods have been developed in the power systems community as early as 1989. Our main contribution is to give the first provable approximation guarantees for the network reconfiguration problem. We provide novel lower bounds and corresponding approximation factors for various settings ranging from $\min\{O(m-n), O(n)\}$ for general graphs, to $O(\sqrt{n})$ over grids with uniform resistances on edges, and $O(1)$ for grids with uniform edge resistances and demands. To obtain the result for general graphs, we propose a new method for (approximate) spectral graph sparsification, which may be of independent interest. Using insights from our theoretical results, we propose a general heuristic for the network reconfiguration problem that is orders of magnitude faster than existing methods in the literature, while obtaining comparable performance. ",Electrical Flows over Spanning Trees
61,1173866678700191744,864970046,Falk Herwig,"['Great talks @NPA_IX and fittingly our new @jina_cee #nugrid paper on weak i-process nuclear physics uncertainty impact for metal-poor star abundance predictions today on #arXiv <LINK> @ArtemisSpyrou @Cec_gRESONANT measure 75Ga(n,g) and Ni66(n,g) üòÄ @PHASTatUVIC', '@NPA_IX @jina_cee @ArtemisSpyrou @Cec_gRESONANT @PHASTatUVIC In addition to Ga75 and Ni66 there are four more (n,g) measurements needed most urgently for i-process simulation predictions for metal-poor stars: Kr88, Rb89, Cs137 and I135  https://t.co/7Q1yoPj9E5 @jina_cee #nugrid @TRIUMFLab']",https://arxiv.org/abs/1909.07011,"Several anomalous elemental abundance ratios have been observed in the metal-poor star HD94028. We assume that its high [As/Ge] ratio is a product of a weak intermediate (i) neutron-capture process. Given that observational errors are usually smaller than predicted nuclear physics uncertainties, we have first set up a benchmark one-zone i-process nucleosynthesis simulation results of which provide the best fit to the observed abundances. We have then performed Monte Carlo simulations in which 113 relevant (n,$\gamma$) reaction rates of unstable species were randomly varied within Hauser-Feshbach model uncertainty ranges for each reaction to estimate the impact on the predicted stellar abundances. One of the interesting results of these simulations is a double-peaked distribution of the As abundance, which is caused by the variation of the $^{75}$Ga (n,$\gamma$) cross section. This variation strongly anti-correlates with the predicted As abundance, confirming the necessity for improved theoretical or experimental bounds on this cross section. The $^{66}$Ni (n,$\gamma$) reaction is found to behave as a major bottleneck for the i-process nucleosynthesis. Our analysis finds the Pearson product-moment correlation coefficient $r_\mathrm{P} > 0.2$ for all of the i-process elements with $32 \leq Z \leq 42$, with significant changes in their predicted abundances showing up when the rate of this reaction is reduced to its theoretically constrained lower bound. Our results are applicable to any other stellar nucleosynthesis site with the similar i-process conditions, such as Sakurai's object (V4334 Sagittarii) or rapidly-accreting white dwarfs. ","The impact of (n,$\gamma$) reaction rate uncertainties on the predicted
  abundances of i-process elements with $32\leq Z\leq 48$ in the metal-poor
  star HD94028"
62,1173770942255980548,577537524,Pete Florence,['Having robots learn dexterous tasks requiring real-time hand-eye coordination is hard.\nCan learning visual correspondence make it easier?\nNew paper: ‚ÄúSelf-Supervised Correspondence in Visuomotor Policy Learning‚Äù\nPdf: <LINK>\nVideo: <LINK> <LINK>'],http://arxiv.org/abs/1909.06933,"In this paper we explore using self-supervised correspondence for improving the generalization performance and sample efficiency of visuomotor policy learning. Prior work has primarily used approaches such as autoencoding, pose-based losses, and end-to-end policy optimization in order to train the visual portion of visuomotor policies. We instead propose an approach using self-supervised dense visual correspondence training, and show this enables visuomotor policy learning with surprisingly high generalization performance with modest amounts of data: using imitation learning, we demonstrate extensive hardware validation on challenging manipulation tasks with as few as 50 demonstrations. Our learned policies can generalize across classes of objects, react to deformable object configurations, and manipulate textureless symmetrical objects in a variety of backgrounds, all with closed-loop, real-time vision-based policies. Simulated imitation learning experiments suggest that correspondence training offers sample complexity and generalization benefits compared to autoencoding and end-to-end training. ",Self-Supervised Correspondence in Visuomotor Policy Learning
63,1173769361707999237,93301074,Harry Altman,"[""New paper with Andreas Weiermann: <LINK> For if you like well partial orders, or Young's lattice in m dimensions""]",https://arxiv.org/abs/1909.06719,"We compute the type (maximum linearization) of the well partial order of bounded lower sets in $\mathbb{N}^m$, ordered under inclusion, and find it is $\omega^{\omega^{m-1}}$; we give two proofs of this statement. Moreover we compute the type of the set of all lower sets in $\mathbb{N}^m$, a topic studied by Aschenbrenner and Pong, and find that it is equal to \[ \omega^{\sum_{k=1}^{m} \omega^{m-k}\binom{m}{k-1} }+ 1. \] As a consequence we deduce corresponding bounds on sequences of monomial ideals. ","Maximum linearizations of lower sets in $\mathbb{N}^m$ with application
  to monomial ideals"
64,1173660449432162305,1036997113274662920,Siddharth Karamcheti,"['How do reading comprehension models select supporting evidence? How does this evidence compare to those chosen by human users? \n\nVery excited to share our new #emnlp2019 paper (<LINK>) w/ @EthanJPerez, Rob Fergus, @jaseweston, @douwekiela, and @kchonyc! <LINK>']",https://arxiv.org/abs/1909.05863,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions. ",Finding Generalizable Evidence by Learning to Convince Q&A Models
65,1173629581116542976,147411178,spencer woody,['New paper on arXiv from my collaboration with @SandiaLabs <LINK>'],https://arxiv.org/abs/1909.05428,"The current standard Bayesian approach to model calibration, which assigns a Gaussian process prior to the discrepancy term, often suffers from issues of unidentifiability and computational complexity and instability. When the goal is to quantify uncertainty in physical parameters for extrapolative prediction, then there is no need to perform inference on the discrepancy term. With this in mind, we introduce Gibbs posteriors as an alternative Bayesian method for model calibration, which updates the prior with a loss function connecting the data to the parameter. The target of inference is the physical parameter value which minimizes the expected loss. We propose to tune the loss scale of the Gibbs posterior to maintain nominal frequentist coverage under assumptions of the form of model discrepancy, and present a bootstrap implementation for approximating coverage rates. Our approach is highly modular, allowing an analyst to easily encode a wide variety of such assumptions. Furthermore, we provide a principled method of combining posteriors calculated from data subsets. We apply our methods to data from an experiment measuring the material properties of tantalum. ","Bayesian Model Calibration for Extrapolative Prediction via Gibbs
  Posteriors"
66,1173399892032667648,382008009,Miles Cranmer,"['New paper with Rui Xu (Princeton), @PeterWBattaglia (DeepMind), @cosmo_shirley (Flatiron)! <LINK>\n\nWe demonstrate how you can extract physics from trained graph networks and improve zero-shot generalization. <LINK>', ""This paper is special to me because Schmidt &amp; Lipson (2009) is the very first paper I read in full. Their paper single-handedly drove me into my particular niche of science; I've been waiting to use it in my research for years and years and I finally got a chance to cite it here.""]",https://arxiv.org/abs/1909.05862,"We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning. ",Learning Symbolic Physics with Graph Networks
67,1173326153991380992,2577596593,Chelsea Finn,"['New paper: Hierarchical visual foresight learns to generate visual subgoals to break down a goal into smaller pieces. Accomplishes long-horizon vision-based tasks, without *any* supervision.\n\nw/ Suraj Nair @GoogleAI\n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1909.05829,"Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. Project page: this https URL ","Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks
  via Visual Subgoal Generation"
68,1172948326037282817,830355049,Mohit Bansal,"['Check out this new @EMNLP2019 ""Modularity-for-Interpretable-MultihopQA"" paper by @YichenJiang9 w. detailed interpr analysis on ques decompsn, bridge entity predn, learned dynm layout match wrt humans, &amp; robustness in adv setting üôÇ\n\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/1909.05803,"Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire network. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our model via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the controller can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller. ",Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning
69,1172765956348743681,18850305,Zachary Lipton,"['Congrats to my departed (from CMU, not Earth) MS students @alankarjain91 &amp; Bhargavi Paranjape on this new #EMNLP2019 paper addressing ***Cross Lingual NER to medium-source languages*** exploiting MT both for translating corpora &amp; projecting entities. <LINK> (1/2)', 'Many languages lack solid corpora for NER but enjoy high-quality MT to-from English. For these medium-resource languages, we combine MT to translate corpus &amp; project entities together w. orthography- lexicon-, &amp; statistics-based heuristics for handling unmatched entities (2/2)']",https://arxiv.org/abs/1909.05356,"Although over 100 languages are supported by strong off-the-shelf machine translation systems, only a subset of them possess large annotated corpora for named entity recognition. Motivated by this fact, we leverage machine translation to improve annotation-projection approaches to cross-lingual named entity recognition. We propose a system that improves over prior entity-projection methods by: (a) leveraging machine translation systems twice: first for translating sentences and subsequently for translating entities; (b) matching entities based on orthographic and phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Our approach improves upon current state-of-the-art methods for cross-lingual named entity recognition on 5 diverse languages by an average of 4.1 points. Further, our method achieves state-of-the-art F_1 scores for Armenian, outperforming even a monolingual model trained on Armenian source data. ",Entity Projection via Machine Translation for Cross-Lingual NER
70,1172493544369639425,1526099316,Ben Spar,['Check out our new paper! \n\n‚ÄúSubdiffusion and heat transport in a tilted 2D Fermi-Hubbard system‚Äù <LINK>'],https://arxiv.org/abs/1909.05848,"Using quantum gas microscopy we study the late-time effective hydrodynamics of an isolated cold-atom Fermi-Hubbard system subject to an external linear potential (a ""tilt""). The tilt is along one of the principal directions of the two-dimensional (2D) square lattice and couples mass transport to local heating through energy conservation. We study transport and thermalization in our system by observing the decay of prepared initial density waves as a function of wavelength $\lambda$ and tilt strength and find that the associated decay time $\tau$ crosses over as the tilt strength is increased from characteristically diffusive to subdiffusive with $\tau\propto\lambda^4$. In order to explain the underlying physics we develop a hydrodynamic model that exhibits this crossover. For strong tilts, the subdiffusive transport rate is set by a thermal diffusivity, which we are thus able to measure as a function of tilt in this regime. We further support our understanding by probing the local inverse temperature of the system at strong tilts, finding good agreement with our theoretical predictions. Finally, we discuss the relation of the strongly tilted limit of our system to recently studied 1D models which may exhibit nonergodic dynamics. ",Subdiffusion and heat transport in a tilted 2D Fermi-Hubbard system
71,1172466801319526401,1117093805499355136,Marilena Loverde,"['New paper today studying cosmic voids in the separate universe framework led by my PhD student Drew Jamieson <LINK>. I love the figures. <LINK>', '@nafshordi ha ha. Of course you only get to make nice pictures if you have good work! (The work, of course, is of the highest quality too!)', '@DScol thanks!']",https://arxiv.org/abs/1909.05313,"Voids have emerged as a novel probe of cosmology and large-scale structure. These regions of extreme underdensity are sensitive to physics beyond the standard model of cosmology, and can potentially be used as a testing ground to constrain new physics. We present the first determination of the linear void bias measured in separate universe simulations. Our methods are validated by comparing the separate universe response bias with the clustering bias of voids. We find excellent agreement between the two methods for voids identified in the halo field and the down-sampled dark matter field. For voids traced by halos, we identify two different contributions to the bias. The first is due to the bias of the underlying halo field used to identify voids, while the second we attribute to the dynamical impact of long-wavelength density perturbations on void formation and expansion. By measuring these contributions individually, we demonstrate that their sum is consistent with the total void bias. We also measure the void profiles in our simulations, and determine their separate universe response. These can be interpreted as the sensitivity of the profiles to the background density of the Universe. ",Separate Universe Void Bias
72,1172322854328659974,101810581,Animesh Garg,"['Our new work at  #Corl2019 will present RL with Ensemble of Suboptimal Teachers -aka- specify as much as you can easily, let learning handle the rest.\nBlog: <LINK>\nPaper: <LINK>\nw\\ @andrey_kurenkov, A. Mandlekar, @RobobertoMM, @silviocinguetta <LINK>']",https://arxiv.org/abs/1909.04121,"The exploration mechanism used by a Deep Reinforcement Learning (RL) agent plays a key role in determining its sample efficiency. Thus, improving over random exploration is crucial to solve long-horizon tasks with sparse rewards. We propose to leverage an ensemble of partial solutions as teachers that guide the agent's exploration with action suggestions throughout training. While the setup of learning with teachers has been previously studied, our proposed approach - Actor-Critic with Teacher Ensembles (AC-Teach) - is the first to work with an ensemble of suboptimal teachers that may solve only part of the problem or contradict other each other, forming a unified algorithmic solution that is compatible with a broad range of teacher ensembles. AC-Teach leverages a probabilistic representation of the expected outcome of the teachers' and student's actions to direct exploration, reduce dithering, and adapt to the dynamically changing quality of the learner. We evaluate a variant of AC-Teach that guides the learning of a Bayesian DDPG agent on three tasks - path following, robotic pick and place, and robotic cube sweeping using a hook - and show that it improves largely on sampling efficiency over a set of baselines, both for our target scenario of unconstrained suboptimal teachers and for easier setups with optimal or single teachers. Additional results and videos at this https URL ","AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an
  Ensemble of Suboptimal Teachers"
73,1172150025369133056,344361113,Manuel Gomez-Rodriguez,"['""Can a social media user guess what her followers want?"" In our new paper, we address this question from a theoretical perspective and then provide practical algorithms to detect strategic posting <LINK> #research #machinelearning #hcml']",https://arxiv.org/abs/1909.00440,"Whenever a social media user decides to share a story, she is typically pleased to receive likes, comments, shares, or, more generally, feedback from her followers. As a result, she may feel compelled to use the feedback she receives to (re-)estimate her followers' preferences and decides which stories to share next to receive more (positive) feedback. Under which conditions can she succeed? In this work, we first look into this problem from a theoretical perspective and then provide a set of practical algorithms to identify and characterize such behavior in social media. More specifically, we address the above problem from the viewpoint of sequential decision making and utility maximization. For a wide variety of utility functions, we first show that, to succeed, a user needs to actively trade off exploitation-- sharing stories which lead to more (positive) feedback--and exploration-- sharing stories to learn about her followers' preferences. However, exploration is not necessary if a user utilizes the feedback her followers provide to other users in addition to the feedback she receives. Then, we develop a utility estimation framework for observation data, which relies on statistical hypothesis testing to determine whether a user utilizes the feedback she receives from each of her followers to decide what to post next. Experiments on synthetic data illustrate our theoretical findings and show that our estimation framework is able to accurately recover users' underlying utility functions. Experiments on several real datasets gathered from Twitter and Reddit reveal that up to 82% (43%) of the Twitter (Reddit) users in our datasets do use the feedback they receive to decide what to post next. ",Can A User Anticipate What Her Followers Want?
74,1172091991699664897,953616889,Justin Read,"['New EDGE paper out today led by Martin Rey! \n\nWe show how different assembly histories lead to broad scatter in stellar mass and surface brightness for ultra-faint dwarfs. And we predict the existence of new class of extremely diffuse ultra-faints!\n\n<LINK> <LINK>', '@lellifede @CWRUastro Thanks! We are beginning to be able to make predictions now. It will be interesting to see if these come to pass, and to see if these simulations continue to match reality as we model more massive galaxies at this resolution. Watch this space!']",https://arxiv.org/abs/1909.04664,"We demonstrate how the least luminous galaxies in the Universe, ultra-faint dwarf galaxies, are sensitive to their dynamical mass at the time of cosmic reionization. We select a low-mass ($\sim \text{1.5} \times 10^{9} \, \text{M}_{\odot}$) dark matter halo from a cosmological volume, and perform zoom hydrodynamical simulations with multiple alternative histories using ""genetically modified"" initial conditions. Earlier forming ultra-faints have higher stellar mass today, due to a longer period of star formation before their quenching by reionization. Our histories all converge to the same final dynamical mass, demonstrating the existence of extended scatter ($\geq$ 1 dex) in stellar masses at fixed halo mass due to the diversity of possible histories. One of our variants builds less than 2 % of its final dynamical mass before reionization, rapidly quenching in-situ star formation. The bulk of its final stellar mass is later grown by dry mergers, depositing stars in the galaxy's outskirts and hence expanding its effective radius. This mechanism constitutes a new formation scenario for highly diffuse ($\text{r}_{1 /2} \sim 820 \, \text{pc}$, $\sim 32 \, \text{mag arcsec}^2$), metal-poor ($\big[ \mathrm{Fe}\, / \mathrm{H} \big]= -2.9$), ultra-faint ($\mathcal{M}_V= -5.7$) dwarf galaxies within the reach of next-generation low surface brightness surveys. ","EDGE: The origin of scatter in ultra-faint dwarf stellar masses and
  surface brightnesses"
75,1172065047193051136,626859202,Kevin Pimbblet,"['Our new paper out today on the ""Heterogeneity of Inverted Calcium II H:K Ratio Cluster Galaxies"" <LINK>', 'With @amelia_fmc and Jake Crossett, we present a new method for H:K ratio determination (an indicator of stellar populations younger than 200 Myr), and suggest there is a plurality of pathways that can achieve strong H:K inversion in galaxies.', '@Lawrious @amelia_fmc No, but thanks for the thought!']",https://arxiv.org/abs/1909.04956,"The ratio of calcium II H plus H$\epsilon$ to calcium II K inverts as a galaxy stellar population moves from being dominated by older stars to possessing more A and B class stars. This ratio - the H:K ratio - can serve as an indicator of stellar populations younger than 200 Myr. In this work, we provide a new method to determine H:K, and apply it to spectra taken of cluster galaxies in Abell~3888. Although H:K is on average systematically lower for the cluster than for a wider field sample, we show that H:K does not have a simple relationship with other indices such as the equivalent widths of H$\delta$ and [OII]. Moreover, strongly inverted galaxies with H:K>1.1 have no preferred location within the cluster and are only slightly lower in their velocity dispersions around the cluster compared to strongly emitting [OII] galaxies. Our results indicate that selecting galaxies on H:K inversion results in a heterogeneous sample formed via a mixture of pathways that likely includes, but may not be limited to, merging spiral galaxies, and quiescent galaxies accreting lower mass, gas rich companions. In concert with other selection criteria, H:K can provide a means to select a more `pure' passive sample or to aid in the selection of highly star-forming galaxies, especially where other spectral line indicators such as H$\alpha$ may not have been observed. ",Heterogeneity of Inverted Calcium II H:K Ratio Cluster Galaxies
76,1171878632211587074,153384371,Jonas Schuett,"['Check out my new paper ""A Legal Definition of AI""\n<LINK>\nWhy policy makers should not use the term #AI for regulatory purposes and what they should do instead \n@RoboticsEU @aimeevanrobot @frossi_t @profAndreaRenda @CompetitionProf @MullerCatelijne @BoboKroplewski <LINK>', '@BoboKroplewski @RoboticsEU @aimeevanrobot @frossi_t @profAndreaRenda @CompetitionProf @MullerCatelijne @BoboKroplewski Are the sectoral definitions by #AIHLEG public? I only know this document https://t.co/3GPM5IPM51']",http://arxiv.org/abs/1909.01095,"The paper argues that policy makers should not use the term artificial intelligence (AI) to define the material scope of AI regulations. The argument is developed by proposing a number of requirements for legal definitions, surveying existing AI definitions, and then discussing the extent to which they meet the proposed requirements. It is shown that existing definitions of AI do not meet the most important requirements for legal definitions. Next, the paper suggests that policy makers should instead deploy a risk-based definition of AI. Rather than using the term AI, they should focus on the specific risks they want to reduce. It is shown that the requirements for legal definitions can be better met by considering the main causes of relevant risks: certain technical approaches (e.g. reinforcement learning), applications (e.g. facial recognition), and capabilities (e.g. the ability to physically interact with the environment). Finally, the paper discusses the extent to which this approach can also be applied to more advanced AI systems. ",Defining the scope of AI regulations
77,1171820911638368256,36653441,Johan Ugander,"['""An Experimental Study of Structural Diversity in Social Networks,"" new paper with @jessicatysu, @krishna_kamath, @aneeshs, and @5harad, to appear at ICWSM 2020: <LINK> Several years in the making, I\'m really excited to share this work! 1/n', 'In adoption decisions based on social referrals, what about graph structure? Classic modeling of such decisions study ""influence response functions"", adoption as a function of # of adopted friends, see e.g. this classic by @duncanjwatts/@peterdodds:\nhttps://t.co/VMemhEfL4W 2/n https://t.co/o2Gn2pDlrC', 'In 2010-12 I had the opportunity to look very closely at the adoption of Facebook. With that rich data we saw significant differences in adoption probability as a function of graph structure. Paper, ""Structural diversity in social contagion"", in PNAS: https://t.co/98BJjiLVJY 3/n', 'The paper examined both Facebook adoption and engagement as a function of contact neighborhood structure. In both cases, ""diverse"" networks *predicted* (ahem, foreshadowing) much more likely adoption/engagement. Adoption left, engagement right: 4/n https://t.co/A7Js8zD1uA', 'For adoption, ""diversity"" could be easily operationalized in terms of the number of connected components (gray bands in figure above). For engagement, where neighborhoods were much larger, we looked at the number of ""large"" components. 5/n', 'This was all super interesting, and adoption was consistent with causal mechanisms like learning on networks: you discount information from connected sources as redundant. Two isolated sources of info &gt; two redundant sources, etc. But the analysis was far from causal. 6/n', 'In 2015 I joined Stanford and @5harad and I quickly started talking about the causal story that may/may not underly my structural diversity results. @jessicatysu was headed to Twitter and together with @krishna_kamath &amp; @aneeshs we started to sketch experimental designs 7/n', ""It's a tricky experiment because you need to randomize people to different contact neighborhoods. One obviously can't randomize actual connections, but with the recommendation system we could randomize what connections were suggested. 8/n"", 'Twitter was already experimenting with recs regularly. There is lots of work on pros/cons of diversity in rec systems; one interpretation of our new work is as a causal study of the long-run effects of diverse recommendations (engagement, not just acceptance of recs). 9/n', 'There are important details of the design, but essentially we had three separate recommendation algorithms running in parallel, serving low/medium/high diversity recommendations based on the similarity+component structure of the set of recommendations 10/n https://t.co/FTDuf6k9Fl', 'Most people pick-and-choose from recs, but it turns out a sizable fraction of users ""accept all"", taking our slate of 20 recommendations with them (left). Thus, we were able to induce a lasting difference (right) in the diversity of the accounts they follow. 11/n https://t.co/adeDnWOgYx', 'Interlude: diversity on Twitter means something slightly different than diversity on FB. Diversity in twitter recs often means topical diversity (mixing sports, arts, and professional recs) vs. FB which is more about social circles. 12/n', 'So what happened to these users who accepted our high/medium/low diversity (l/m/h pairwise similarity) recommendations? After 3 months, their retention rates were nearly identical. No causal effect of structural diversity. 13/n https://t.co/ihOqkUSDfd', 'Now, a very important kink in the design: for many users it was _not possible_ to form high diversity recs. The three treatment populations we studied were a randomization of only those users (12%) that were eligible for all recs (high diversity was the bottleneck)  14/n', 'Without this conditioning (conditioning on it being possible to serve high diversity recs), across all new users you see a positive relationship between engagement and diversity, consistent with the earlier FB work. 15/n', 'Going back to the FB findings, basically, the types of people who have high diversity networks are different than the types of people who have low diversity networks. But based on the Twitter findings, if you could randomize their network ""keeping them the same"", no effect. 16/n https://t.co/MHqTaYB30D', 'High diversity definitely *predicts* higher adoption/engagement in all these contexts (the Twitter study replicated this aspect of the FB study), but the new twitter experiment suggests that diversity doesn\'t ""cause"" engagement. 17/n', 'This work involved many lengthy discussions about what is being causally tested in this experiment, among us co-authors and with colleagues who were very generous with their time. See the paper for our best efforts to report our findings: https://t.co/wuMxzodIx9  18/n']",https://arxiv.org/abs/1909.03543,"Several recent studies of online social networking platforms have found that adoption rates and engagement levels are positively correlated with structural diversity, the degree of heterogeneity among an individual's contacts as measured by network ties. One common theory for this observation is that structural diversity increases utility, in part because there is value to interacting with people from different network components on the same platform. While compelling, evidence for this causal theory comes from observational studies, making it difficult to rule out non-causal explanations. We investigate the role of structural diversity on retention by conducting a large-scale randomized controlled study on the Twitter platform. We first show that structural diversity correlates with user retention on Twitter, corroborating results from past observational studies. We then exogenously vary structural diversity by altering the set of network recommendations new users see when joining the platform; we confirm that this design induces the desired changes to network topology. We find, however, that low, medium, and high structural diversity treatment groups in our experiment have comparable retention rates. Thus, at least in this case, the observed correlation between structural diversity and retention does not appear to result from a causal relationship, challenging theories based on past observational studies. ",An Experimental Study of Structural Diversity in Social Networks
78,1171810804867272704,369569444,Takahiro TERADA (ÂØ∫Áî∞ ÈöÜÂ∫É),"['Our new paper is available: ""Clustering of primordial black holes formed in a matter-dominated epoch,"" <LINK> (Non)Clustering of PBHs is an important topic since it affects e.g. constraints on the abundance of PBHs and the spin distribution of PBHs. <LINK>']",https://arxiv.org/abs/1909.04053,"In the presence of the local-type primordial non-Gaussianity, it is known that the clustering of primordial black holes (PBHs) emerges even on super-horizon scales at the formation time. This effect has been investigated in the high-peak limit of the PBH formation in the radiation-dominated epoch in the literature. There is another possibility that the PBH formation takes place in the early matter-dominated epoch. In this scenario, the high-peak limit is not applicable because even initially small perturbations grow and can become a PBH. We first derive a general formula to estimate the clustering of PBHs with primordial non-Gaussianity without assuming the high-peak limit, and then apply this formula to a model of PBH formation in a matter-dominated epoch. Clustering is less significant in the case of the PBH formation in the matter-dominated epoch than that in the radiation-dominated epoch. Nevertheless, it is much larger than the Poisson shot noise in many cases. Relations to the constraints of the isocurvature perturbations by the cosmic microwave background radiation are quantitatively discussed. ",Clustering of primordial black holes formed in a matter-dominated epoch
79,1171548832393572353,2785337469,Sebastian Ruder,"[""Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction\nNew @emnlp2019 paper led by @p_czarnowska, with @EXGRV @ryandcotterell @anncopestake\n<LINK> <LINK>"", '@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake We conduct a comprehensive analysis of morphological generalization in bilingual lexicon induction (BLI). As expected, BLI performance decreases on infrequent words that are not well represented in lexicons. We introduce morphologically complete lexicons with better coverage. https://t.co/4cRFfAWgGm', '@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake Furthermore, controlling for different factors, we obtain a more nuanced picture: BLI models *can* generalize for frequent morphosyntactic categories, even of infrequent words, but fail to generalize for rare categories. https://t.co/ccDGgbTIaz', '@emnlp2019 @p_czarnowska @EXGRV @ryandcotterell @anncopestake We finally propose a simple morphological constraint during training time that ameliorates this to some extent and improves the morphological generalization ability of BLI models. https://t.co/ZRHYBQsPvs']",https://arxiv.org/abs/1909.02855,"Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl\'aramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the state-of-the-art models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology. ","Don't Forget the Long Tail! A Comprehensive Analysis of Morphological
  Generalization in Bilingual Lexicon Induction"
80,1171449086743990273,1033928440645382149,Ben Zhou,"[""Check out our new #emnlp2019 paper where we studied temporal commonsense: <LINK>. We collected a QA dataset MC-TACOüåÆ(leaderboard coming soon) and showed that it's a new challenge to existing systems. Co-author with @DanielKhashabi, Qiang Ning and Dan Roth.""]",https://arxiv.org/abs/1909.03065,"Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic. ","""Going on a vacation"" takes longer than ""Going for a walk"": A Study of
  Temporal Commonsense Understanding"
81,1171412800058662913,97707247,Gautam Kamath,"['New paper on arXiv, to appear in #NeurIPS2019: ""Differentially Private Algorithms for Learning Mixtures of Separated Gaussians,"" with Or Sheffet, Vikrant Singhal, and Jon Ullman. <LINK> 1/n', 'We give a differentially private algorithm which privately learns mixtures of k Gaussians under separation condition sqrt(k)*sigma (i.e., a differentially private version of the algo of Achlioptas &amp; McSherry) 2/n', 'The non-private algo is roughly k-PCA + clustering. We provide new tools and analysis to privately perform both. Big challenge is avoiding paying a lot in terms of the bounds on mixture components. We pay loglog, instead of poly. 3/n', ""We also give a modern sample-and-aggregate-based algorithm. More expensive and can only handle spherical Gaussians, but it's a black box reduction that lets us port recent advances in learning GMMs with less than k^1/4 separation. Check out the paper for more details! 4/4""]",https://arxiv.org/abs/1909.03951,"Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work, we give new algorithms for learning the parameters of a high-dimensional, well separated, Gaussian mixture model subject to the strong constraint of differential privacy. In particular, we give a differentially private analogue of the algorithm of Achlioptas and McSherry. Our algorithm has two key properties not achieved by prior work: (1) The algorithm's sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm does not require strong a priori bounds on the parameters of the mixture components. ","Differentially Private Algorithms for Learning Mixtures of Separated
  Gaussians"
82,1171410080324997121,1039002616426983424,Zakaria Al Balushi,['New joint review paper ‚ÄúEpitaxial Growth of 2D Layered Transition Metal Dichalcogenides‚Äù on arxiv: <LINK>'],http://arxiv.org/abs/1909.03502,"Transition metal dichalcogenide (TMD) monolayers and heterostructures have emerged as a compelling class of materials with transformative new science that may be harnessed for novel device technologies. These materials are commonly fabricated by exfoliation of flakes from bulk crystals, but wafer-scale epitaxy of single crystal films is required to advance the field. This article reviews the fundamental aspects of epitaxial growth of van der Waals bonded crystals specific to TMD films. The structural and electronic properties of TMD crystals are initially described along with sources and methods used for vapor phase deposition. Issues specific to TMD epitaxy are critically reviewed including substrate properties and film-substrate orientation and bonding. The current status of TMD epitaxy on different substrate types is discussed along with characterization techniques for large area epitaxial films. Future directions are proposed including developments in substrates, in situ and full wafer characterization techniques and heterostructure growth. ",Epitaxial Growth of 2D Layered Transition Metal Dichalcogenides
83,1171394391879163904,5850692,Aaron Roth,"['We came up with a new and improved proof of the ""transfer theorem"", showing that differentially private analyses generalize. The paper is here: <LINK> and I wrote a blog post about it here: <LINK>.  Continue on for the TL;DR. 1/ <LINK>', 'It is an old observation (I think originally made by @frankmcsherry) that differentially private analyses generalize in expectation. If you (say) privately learn a classifier, then in expectation its test error will be close to its training error. 2/', 'This is true of many stability notions, but differential privacy is special because it composes adaptively. So you could actually train a sequence of classifiers on the same dataset, using previous outcomes to guide your search, and still not overfit in expectation. 3/', 'But these ""in expecation"" bounds aren\'t enough to derive confidence intervals, and so a line of work has emerged lifting these to high probability bounds. Our original paper (DFHPRR in the plot) got the first such bound via a direct moment calculation, but it wasn\'t tight. 4/', 'The BNSSSU paper (see the plot) introduced the ingenious ""monitor technique"" which let them prove the asymptotically optimal bound. This idea has proven useful elsewhere. But it also seems to lead to large constant overhead, even though the asymptotics are tight. We avoid it. 5/', ""Here's our proof: Pretend that after all analyses are complete, the dataset is resampled from its posterior distribution conditioned on the output of the mechanism. This doesn't change the joint distribution on datasets and outputs. So empirical accuracy guarantees still hold. 6/"", 'But the resampled dataset will take query values that are with high probability that close to their posterior means. So the only way the mechanism can promise empirical accuracy is if it promises accuracy with respect to this posterior distribution with high probability. 7/', ""But differential privacy implies that the posterior expectation of query values must be close to their prior (true) values. And thats it. We get high probability bounds for free, from the high probability sample accuracy bounds. Thats why we don't lose large constants. 8/"", 'This is joint work with terrific co-authors @chrisjung93, Katrina Ligett, @privatealgos, Saeed Sharifi, and Moshe Shenfeld. (So lets call it JLNRSS). It was really fun to work on. And consistent with prior work, we see that transfer theorems require six authors. 9/9']",https://arxiv.org/abs/1909.03577,"We give a new proof of the ""transfer theorem"" underlying adaptive data analysis: that any mechanism for answering adaptively chosen statistical queries that is differentially private and sample-accurate is also accurate out-of-sample. Our new proof is elementary and gives structural insights that we expect will be useful elsewhere. We show: 1) that differential privacy ensures that the expectation of any query on the posterior distribution on datasets induced by the transcript of the interaction is close to its true value on the data distribution, and 2) sample accuracy on its own ensures that any query answer produced by the mechanism is close to its posterior expectation with high probability. This second claim follows from a thought experiment in which we imagine that the dataset is resampled from the posterior distribution after the mechanism has committed to its answers. The transfer theorem then follows by summing these two bounds, and in particular, avoids the ""monitor argument"" used to derive high probability bounds in prior work. An upshot of our new proof technique is that the concrete bounds we obtain are substantially better than the best previously known bounds, even though the improvements are in the constants, rather than the asymptotics (which are known to be tight). As we show, our new bounds outperform the naive ""sample-splitting"" baseline at dramatically smaller dataset sizes compared to the previous state of the art, bringing techniques from this literature closer to practicality. ",A New Analysis of Differential Privacy's Generalization Guarantees
84,1171392384778809344,1728419371,Dr Joanna Barstow,"['Check out our cool new paper on 2.5-D retrievals from exoplanet phase curves by Pat Irwin, coauthors @V_Parmentier, @AstroJake, @AirborneGrain, @GleeAstro and Ryan Garland, + me of course! <LINK>', ""@ryanrgarland @V_Parmentier @AstroJake @AirborneGrain @GleeAstro Ah drat, I tried typing your name and it didn't come up! Sorry!"", ""@V_Parmentier @WASPplanets @AstroJake @AirborneGrain @GleeAstro It certainly will. It's a dense paper and we've all been staring at it a long time - so this omission was an honest mistake that will no doubt be picked up in review also. Please apologise to WASP-43b for its lack of formal introduction!"", '@AirborneGrain @V_Parmentier @AstroJake @GleeAstro @PatrickIrwin1 Pat is a first-class lurker - no tweets at all!']",https://arxiv.org/abs/1909.03233,"We present a novel retrieval technique that attempts to model phase curve observations of exoplanets more realistically and reliably, which we call the 2.5-dimension (2.5-D) approach. In our 2.5-D approach we retrieve the vertical temperature profile and mean gaseous abundance of a planet at all longitudes and latitudes \textbf{simultaneously}, assuming that the temperature or composition, $x$, at a particular longitude and latitude $(\Lambda,\Phi)$ is given by $x(\Lambda,\Phi) = \bar{x} + (x(\Lambda,0) - \bar{x})\cos^n\Phi$, where $\bar{x}$ is the mean of the morning and evening terminator values of $x(\Lambda,0)$, and $n$ is an assumed coefficient. We compare our new 2.5-D scheme with the more traditional 1-D approach, which assumes the same temperature profile and gaseous abundances at all points on the visible disc of a planet for each individual phase observation, using a set of synthetic phase curves generated from a GCM-based simulation. We find that our 2.5-D model fits these data more realistically than the 1-D approach, confining the hotter regions of the planet more closely to the dayside. We then apply both models to WASP-43b phase curve observations of HST/WFC3 and Spitzer/IRAC. We find that the dayside of WASP-43b is apparently much hotter than the nightside and show that this could be explained by the presence of a thick cloud on the nightside with a cloud top at pressure $< 0.2$ bar. We further show that while the mole fraction of water vapour is reasonably well constrained to $(1-10)\times10^{-4}$, the abundance of CO is very difficult to constrain with these data since it is degenerate with temperature and prone to possible systematic radiometric differences between the HST/WFC3 and Spitzer/IRAC observations. Hence, it is difficult to reliably constrain C/O. ","2.5-D retrieval of atmospheric properties from exoplanet phase curves:
  Application to WASP-43b observations"
85,1171385645920595968,180451291,Uri Shalit,"['In a new paper with Shie Mannor and led by @guytenn, we work at the intersection of RL and causal inference. We give a new method to perform off-policy evaluation in the presence of dynamic unmeasured confounders, or a POMDP to use RL terminology.  \n<LINK> <LINK>', 'Imagine learning a policy offline from hospital data. We know the actions we learn from were conducted by doctors who have some information our RL agent can‚Äôt see: unmeasured confounders. Gottesman et al. https://t.co/IIEH1xZrcd showed how bad things can go in this scenario.', 'In our paper, we give a way to evaluate agent policies in this difficult scenario. \nMain assumption: the unknown cond. prob. matrix between unmeasured and measured is invertible, as are the transition matrices btw unmeasured. Inspired by Miao et al. \nhttps://t.co/mgz65P9uxM', 'While we believe this is an important theoretical advance, the resulting method requires estimating inverses of big matrices. So we introduce a ‚Äúdecoupled POMDP‚Äù model which we consider sensible in its causal assumptions and leads to an easier estimation problem. https://t.co/52snAPT6h3', 'In the figures: u_t are unobserved confounders/states, z_t and o_t are observations, a_t are actions, r_t rewards, œÄ_b behavior policy (e.g. human doctor), œÄ_e agent policy we want to evaluate']",https://arxiv.org/abs/1909.03739,"This work studies the problem of batch off-policy evaluation for Reinforcement Learning in partially observable environments. Off-policy evaluation under partial observability is inherently prone to bias, with risk of arbitrarily large errors. We define the problem of off-policy evaluation for Partially Observable Markov Decision Processes (POMDPs) and establish what we believe is the first off-policy evaluation result for POMDPs. In addition, we formulate a model in which observed and unobserved variables are decoupled into two dynamic processes, called a Decoupled POMDP. We show how off-policy evaluation can be performed under this new model, mitigating estimation errors inherent to general POMDPs. We demonstrate the pitfalls of off-policy evaluation in POMDPs using a well-known off-policy method, Importance Sampling, and compare it with our result on synthetic medical data. ",Off-Policy Evaluation in Partially Observable Environments
86,1171370115897667584,230698303,Matt Darnley,"['New paper just submitted, led by my collaborator Paul Kuin @MSSLSpaceLab - all about the January 2016 eruption of one of the 4 known recurrent novae in the Large Magellanic Cloud <LINK> - this work showcases a fantastic collection of ground and space based data!', '@MSSLSpaceLab Also includes a lot of others, including @przemroz @starsumner @vrib_ast @heads0rtai1s']",https://arxiv.org/abs/1909.03281,"We present a comprehensive review of all observations of the eclipsing recurrent Nova LMC 1968 in the Large Magellanic Cloud which was previously observed in eruption in 1968, 1990, 2002, 2010, and most recently in 2016. We derive a probable recurrence time of $6.2 \pm 1.2$ years and provide the ephemerides of the eclipse. In the ultraviolet-optical-IR photometry the light curve shows high variability right from the first observation around two days after eruption. Therefore no colour changes can be substantiated. Outburst spectra from 2016 and 1990 are very similar and are dominated by H and He lines longward of 2000 Angstrom. Interstellar reddening is found to be E(B-V) = $0.07\pm0.01$. The super soft X-ray luminosity is lower than the Eddington luminosity and the X-ray spectra suggest the mass of the WD is larger than 1.3 M$_\odot$. Eclipses in the light curve suggest that the system is at high orbital inclination. On day four after the eruption a recombination wave was observed in Fe II ultraviolet absorption lines. Narrow line components are seen after day 6 and explained as being due to reionisation of ejecta from a previous eruption. The UV spectrum varies with orbital phase, in particular a component of the He II 1640 Angstrom emission line, which leads us to propose that early-on the inner WD Roche lobe might be filled with a bound opaque medium prior to the re-formation of an accretion disk. Both this medium and the ejecta can cause the delay in the appearance of the soft X-ray source. ",The January 2016 eruption of recurrent nova LMC 1968
87,1171345519907491840,733646548370919433,P·µât…òr W·µâil8acher,"['New paper by Ventou, Contini, et al., using MUSE data, on merger fraction evolution: <LINK>\nFigures about the fractions in different MUSE fields and comparison with simulations. #musevlt <LINK>']",https://arxiv.org/abs/1909.03706,"It is still a challenge to assess the merger fraction of galaxies at different cosmic epochs in order to probe the evolution of their mass assembly. Using the Illustris cosmological simulations, we investigate the relation between the separation of galaxies in a pair, both in velocity and projected spatial separation space, and the probability that these interacting galaxies will merge in the future. From this analysis, we propose a new set of criteria to select close pairs of galaxies along with a new corrective term to be applied to the computation of the galaxy merger fraction. We then probe the evolution of the major and minor merger fraction using the latest MUSE deep observations over the HUDF, HDFS, COSMOS-Gr30 and Abell 2744 regions. From a parent sample of 2483 galaxies with spectroscopic redshifts, we identify 366 close pairs spread over a large range of redshifts ($0.2<z<6$) and stellar masses ($10^7-10^{11}M_{\odot}$). Using the stellar mass ratio between the secondary and primary galaxy as a proxy to split the sample into major, minor and very minor mergers, we found a total of 183 major, 142 minor and 47 very minor close pairs corresponding to a mass ratio range of 1:1-1:6, 1:6-1:100 and lower than 1:100, respectively. Due to completeness issues, we do not consider the very minor pairs in the analysis. Overall, the major merger fraction increases up to $z\approx 2-3$ reaching 25% for pairs with the most massive galaxy with a stellar mass $M^*\geq 10^{9.5}M_{\odot}$. Beyond this redshift, the fraction decreases down to $\sim 5$% at $z\approx 6$. The evolution of the minor merger fraction is roughly constant with cosmic time, with a fraction of 20% at $z<3$ and a slow decrease between $3\leq z \leq6$ to 8-13%. ","New criteria for the selection of galaxy close pairs from cosmological
  simulations: evolution of the major and minor merger fraction in MUSE deep
  fields"
88,1171326547569106944,1153187867897860096,Nikita Nikolaev,"['New Paper: I construct Levelt filtration for singularly perturbed linear systems of #ODE (in rank 2 at a regular singular point) maintaining very tight #asymptotic control by upper-triangularising such system in a singular #perturbation families.\n\n<LINK>', 'I put this paper out whilst attending the @NCCRSwissMAP General Meeting in Villars-sur-Ollon. Hit submit with the Alps in my view.']",https://arxiv.org/abs/1909.04011,"We study singularly perturbed linear systems of rank two of ordinary differential equations of the form $\hbar x\partial_x \psi (x, \hbar) + A (x, \hbar) \psi (x, \hbar) = 0$, with a regular singularity at $x = 0$, and with a fixed asymptotic regularity in the perturbation parameter $\hbar$ of Gevrey type in a fixed sector. We show that such systems can be put into an upper-triangular form by means of holomorphic gauge transformations which are also Gevrey in the perturbation parameter $\hbar$ in the same sector. We use this result to construct a family in $\hbar$ of Levelt filtrations which specialise to the usual Levelt filtration for every fixed nonzero value of $\hbar$; this family of filtrations recovers in the $\hbar \to 0$ limit the eigen-decomposition for the $\hbar$-leading-order of the matrix $A (x, \hbar)$, and also recovers in the $x \to 0$ limit the eigen-decomposition of the residue matrix $A (0, \hbar)$. ","Triangularisation of Singularly Perturbed Logarithmic Differential
  Systems of Rank 2"
89,1171317643304931328,4249537197,Christian Wolf,"['New paper out, work by Th√©o Jaunet and with Romain Vuillemot @romson on ""Understanding Decisions and Memory\nin Deep Reinforcement Learning"": an interactive tool visualizing the hidden recurrent state of an RL agent and its effect on affordances and policy. <LINK> <LINK>']",https://arxiv.org/abs/1909.02982,"We present DRLViz, a visual analytics interface to interpret the internal memory of an agent (e.g. a robot) trained using deep reinforcement learning. This memory is composed of large temporal vectors updated when the agent moves in an environment and is not trivial to understand due to the number of dimensions, dependencies to past vectors, spatial/temporal correlations, and co-correlation between dimensions. It is often referred to as a black box as only inputs (images) and outputs (actions) are intelligible for humans. Using DRLViz, experts are assisted to interpret decisions using memory reduction interactions, and to investigate the role of parts of the memory when errors have been made (e.g. wrong direction). We report on DRLViz applied in the context of video games simulators (ViZDoom) for a navigation scenario with item gathering tasks. We also report on experts evaluation using DRLViz, and applicability of DRLViz to other scenarios and navigation problems beyond simulation games, as well as its contribution to black box models interpretability and explainability in the field of visual analytics. ","DRLViz: Understanding Decisions and Memory in Deep Reinforcement
  Learning"
90,1171307578825068545,961972787077373952,Aykut Erdem,"[""New paper w/ @erkuterdem -- a collaboration w/ @emreugur__'s Cognition, Learning and Robotics group at @Bogazici_CmpE is now out. Here, we extend PropNet (Li et al.) to reason about action effects on articulated multi-part objects.\n\n<LINK>\n<LINK> <LINK>"", 'Ahmet (Tekden) is the lead author of this paper, and of course, @emreugur__ and his students deserve most of the credit for this work. There is so much potential in applying graph neural networks to robotics.']",https://arxiv.org/abs/1909.03785,"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot's knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","Belief Regulated Dual Propagation Nets for Learning Action Effects on
  Groups of Articulated Objects"
91,1171290477997547520,2372296831,Nidhi Seethapathi,"['New preprint out --&gt; an extension of our 2015 paper on walking with changing speed to amputees walking while starting, stopping, and turning. A collaboration with SDMH Hospital in Jaipur studying amputees wearing low-cost passive prostheses\n<LINK> 1/2', 'author list: me, Dr. Anil Jain from SDMH and @manojmovement \n\nlink to the 2015 paper: \nhttps://t.co/T38DJl0oRI 2/2']",https://arxiv.org/abs/1909.03139,"Preferred walking speed is a widely-used performance measure for people with mobility issues, but is usually measured in straight line walking for fixed distances or durations. However, daily walking involves walking for bouts of different distances and walking with turning. Here, we studied walking for short distances and walking in circles in unilateral lower-limb amputees wearing an above or below-knee passive prosthesis, specifically, a Jaipur foot prosthesis. Analogous to earlier results in non-amputees, we found that their preferred walking speeds are lower for short distances and lower for circles of smaller radii. Using inverse optimization, we estimated the cost of changing speeds and turning such that the observed preferred walking speeds in our experiments minimizes the total energy cost. The inferred costs of changing speeds and turning were much larger than for non-amputees. These findings could inform prosthesis design and rehabilitation therapy to better assist changing speeds and turning tasks in amputee walking. Further, measuring the preferred speed for a range of distances and radii is a more robust subject-specific measure of walking performance. ","Walking for short distances and turning in lower-limb amputees: a study
  in low-cost prosthesis users"
92,1171284980745428992,4313989761,Dr. Andreas Faisst,['Do the first #galaxies form their stars smoothly or in bursts? Check out my new paper using data on @cosmosastro and from @NASAspitzer of @caltechipac\n‚û°Ô∏è<LINK>'],https://arxiv.org/abs/1909.03076,"The redshift range z=4-6 marks a transition phase between primordial and mature galaxy formation in which galaxies considerably increase their stellar mass, metallicity, and dust content. The study of galaxies in this redshift range is therefore important to understand early galaxy formation and the fate of galaxies at later times. Here, we investigate the burstiness of the recent star-formation history (SFH) of 221 $z\sim4.5$ main-sequence galaxies at log(M) > 9.7 by comparing their ultra-violet (UV) continuum, H$\alpha$ luminosity, and H$\alpha$ equivalent-width (EW). The H$\alpha$ properties are derived from the Spitzer [3.6$\mu$m]-[4.5$\mu$m] broad-band color, thereby properly taking into account model and photometric uncertainties. We find a significant scatter between H$\alpha$ and UV-derived luminosities and star-formation rates (SFRs). About half of the galaxies show a significant excess in H$\alpha$ compared to expectations from a constant smooth SFH. We also find a tentative anti-correlation between H$\alpha$ EW and stellar mass, ranging from 1000$\r{A}$ at log(M) < 10 to below 100$\r{A}$ at log(M) > 11. Consulting models suggests that most $z\sim4.5$ galaxies had a burst of star-formation within the last 50 Myrs, increasing their SFRs by a factor of > 5. The most massive galaxies on the other hand might decrease their SFRs, and may be transitioning to a quiescent stage by z=4. We identify differential dust attenuation (f) between stars and nebular regions as the main contributor to the uncertainty. With local galaxies selected by increasing H$\alpha$ EW (reaching values similar to high-z galaxies), we predict that f approaches unity at $z>4$ consistent with the extrapolation of measurements out to z=2. ","The Recent Burstiness of Star Formation in Galaxies at z~4.5 from
  H$\alpha$ Measurements"
93,1171227319807037440,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper on hidden monopole DM via axion portal appeared on arXiv today. We showed that there are interesting parameter regions where monopole DM and the axion are within the reach of PICO-500 and SHiP, respectively. Hidden photons contribute to DR.\n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.03627,"Hidden monopole is a plausible dark matter candidate due to its stability, but its direct experimental search is extremely difficult due to feeble interactions with the standard model particles in the minimal form. Then, we introduce an axion, $a$, connecting the hidden monopole and the standard model particles and examine the current limits and future prospects of direct dark matter searches and beam-dump experiments. We find two parameter regions around $m_a = {\cal O}(10)$ MeV, $f_a = {\cal O}(10^{5})$ GeV and $m_a = {\cal O}(100)$ MeV, $f_a = {\cal O}(10^{4})$ GeV where monopole dark matter and the axion are respectively within the reach of the future experiments such as PICO-500 and SHiP. We also note that the hidden photons mainly produced by the axion decay contribute to dark radiation with $\Delta N_{\rm eff} \simeq 0.6$ which can relax the $H_0$ tension. ","Hidden Monopole Dark Matter via Axion Portal and its Implications for
  Direct Detection Searches, Beam-Dump Experiments, and the $H_0$ Tension"
94,1171079441684062208,338526004,Sam Bowman,"['New #emnlp2019 paper alert: So, there are quite a few methods for trying to uncover what an NN model _knows_ about some task. If you ask the same question several different ways, will you get the same qualitative conclusion? (1/N)\n<LINK>', ""This paper starts with a fairly narrow question‚ÄîWhat do sentence acceptability models know about where words like 'ever' (NPIs) can appear?‚Äîbut focuses on what it means to give a satisfactory answer. https://t.co/F6JSrTdqHZ"", 'So, do you look at raw model accuracy on test data that highlights this phenomenon? Do you look at the relative label probabilities of paired examples that vary in a way that isolates the phenomenon? ...', ""Do you probe model hidden states for features that reflect the knowledge you're interested in? For LM-style models like BERT, do you look at word predictions/likelihoods that are relevant to what you're interested in? https://t.co/UcIO516a36"", ""These are all reasonable ways of operationalizing the research question we're interested in and we tried them all. They don't give strictly contradictory answers, but the qualitative conclusions that they suggest point in substantially different directions."", 'We urge researchers studying similar questions not to put too much weight on the qualitative conclusions from any one experiment of this kind, and to use more than one method whenever possible.', ""By the way, why does this paper have fifteen 'equal contribution' authors? This paper is the result of a little course design experiment, loosely inspired by a course of Joan Bresnan's that I participated in. ..."", 'I put together a graduate seminar with a fairly even mix of ML/NLP students and Syntax/Semantics students, and set the sole goal of collectively writing an analysis paper that everyone would find interesting and productive. ...', ""We got a big group, everyone put in a bunch of serious, thoughtful work building and critiquing our experiments, and I didn't need to offer much guidance on any stage of the project."", '(In fact, much of the seminar group is still meeting weekly without me, four months after the term ended.)', '(Twitter-active authors include @a_stadt @phu_pmh @zhansheng.)', '@wzuidema @a_stadt @phu_pmh @zhansheng @JumeletJ @_dieuwke_ Eek‚Äîsorry we missed that!']",https://arxiv.org/abs/1909.02597,"Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing in English, as a case study for our experiments. NPIs like ""any"" are grammatical only if they appear in a licensing environment like negation (""Sue doesn't have any cats"" vs. ""Sue has any cats""). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model's grammatical knowledge in a given domain. ","Investigating BERT's Knowledge of Language: Five Analysis Methods with
  NPIs"
95,1171018557586649091,4240865308,Fabio Galasso,['Our new @bmvc2019 paper is on @arxiv_org now: <LINK>\nWe use knowledge distillation to train a multi-task model for best detection and re-identification performance (person search).\nThis additionally applies to model compression'],https://arxiv.org/abs/1909.01058,"We introduce knowledge distillation for end-to-end person search. End-to-End methods are the current state-of-the-art for person search that solve both detection and re-identification jointly. These approaches for joint optimization show their largest drop in performance due to a sub-optimal detector. We propose two distinct approaches for extra supervision of end-to-end person search methods in a teacher-student setting. The first is adopted from state-of-the-art knowledge distillation in object detection. We employ this to supervise the detector of our person search model at various levels using a specialized detector. The second approach is new, simple and yet considerably more effective. This distills knowledge from a teacher re-identification technique via a pre-computed look-up table of ID features. It relaxes the learning of identification features and allows the student to focus on the detection task. This procedure not only helps fixing the sub-optimal detector training in the joint optimization and simultaneously improving the person search, but also closes the performance gap between the teacher and the student for model compression in this case. Overall, we demonstrate significant improvements for two recent state-of-the-art methods using our proposed knowledge distillation approach on two benchmark datasets. Moreover, on the model compression task our approach brings the performance of smaller models on par with the larger models. ",Knowledge Distillation for End-to-End Person Search
96,1170950582988922880,820031736914513925,Fabrizio Leisen,"['NEW PAPER: ""A P√≥lya-Gamma Sampler for a Generalized Logistic Regression"" written by Luciana Dalla Valle, Luca Rossini, Weixuan Zhu and myself. Code available on Luca\'s github page <LINK>']",https://arxiv.org/abs/1909.02989,"In this paper we introduce a novel Bayesian data augmentation approach for estimating the parameters of the generalised logistic regression model. We propose a P\'olya-Gamma sampler algorithm that allows us to sample from the exact posterior distribution, rather than relying on approximations. A simulation study illustrates the flexibility and accuracy of the proposed approach to capture heavy and light tails in binary response data of different dimensions. The methodology is applied to two different real datasets, where we demonstrate that the P\'olya-Gamma sampler provides more precise estimates than the empirical likelihood method, outperforming approximate approaches. ",A P\'olya-Gamma Sampler for a Generalized Logistic Regression
97,1170926863214137344,985110481345155072,Eren M. El√ßi,"['Often it‚Äôs a useful strategy to embed a math problem into a larger space to solve it. In this paper we use this idea and construct a new coupling of ‚Äûgeometric‚Äú representations of the random-cluster model to design a new MCMC algorithm for the RC-model. <LINK>', 'Beyond it‚Äôs application to design a new MCMC algorithm, we can use our coupling to construct a perfect sampling algorithm for the q-flow representation of the Potts model. This uses the idea of coupling of MCMC chains, see here our work here: https://t.co/msiDh1kWvl', 'https://t.co/kLljH3R3Qm']",http://arxiv.org/abs/1909.02719,"Potts spin systems play a fundamental role in statistical mechanics and quantum field theory, and can be studied within the spin, the Fortuin-Kasteleyn (FK) bond or the $q$-flow (loop) representation. We introduce a Loop-Cluster (LC) joint model of bond-occupation variables interacting with $q$-flow variables, and formulate a LC algorithm that is found to be in the same dynamical universality as the celebrated Swendsen-Wang algorithm. This leads to a theoretical unification for all the representations, and numerically, one can apply the most efficient algorithm in one representation and measure physical quantities in others. Moreover, by using the LC scheme, we construct a hierarchy of geometric objects that contain as special cases the $q$-flow clusters and the backbone of FK clusters, the exact values of whose fractal dimensions in two dimensions remain as an open question. Our work not only provides a unified framework and an efficient algorithm for the Potts model, but also brings new insights into rich geometric structures of the FK clusters. ",Loop-Cluster Coupling and Algorithm for Classical Statistical Models
98,1169999520190599171,3066116999,Adam Barrett,"['Our new paper on decomposing integrated information! Consciousness, and integrated information, are multi-dimensional! With @PedroMediano @_fernando_rosas @RCarhartHarris @anilkseth  <LINK>']",https://arxiv.org/abs/1909.02297,"Most information dynamics and statistical causal analysis frameworks rely on the common intuition that causal interactions are intrinsically pairwise -- every 'cause' variable has an associated 'effect' variable, so that a 'causal arrow' can be drawn between them. However, analyses that depict interdependencies as directed graphs fail to discriminate the rich variety of modes of information flow that can coexist within a system. This, in turn, creates problems with attempts to operationalise the concepts of 'dynamical complexity' or `integrated information.' To address this shortcoming, we combine concepts of partial information decomposition and integrated information, and obtain what we call Integrated Information Decomposition, or $\Phi$ID. We show how $\Phi$ID paves the way for more detailed analyses of interdependencies in multivariate time series, and sheds light on collective modes of information dynamics that have not been reported before. Additionally, $\Phi$ID reveals that what is typically referred to as 'integration' is actually an aggregate of several heterogeneous phenomena. Furthermore, $\Phi$ID can be used to formulate new, tailored measures of integrated information, as well as to understand and alleviate the limitations of existing measures. ","Beyond integrated information: A taxonomy of information dynamics
  phenomena"
99,1169982203117408258,1070344375979438082,Josh Izaac,"['Cool new paper with @james27182 @gppcarleo and Nathan Killoran @XanaduAI @FlatironCCQ: generalization of natural gradient descent to variational quantum circuits, reducing optimization steps required. <LINK> #QuantumComputing', '@james27182 @gppcarleo @XanaduAI @FlatironCCQ Even better: this new optimization technique is already implemented in PennyLane if you want to check it out! https://t.co/EYWQrdZWUL', 'A comparison of the quantum natural gradient descent optimizer vs. vanilla gradient descent and Adam for a 9 qubit circuit with L layers: https://t.co/mJzk5799Qq']",https://arxiv.org/abs/1909.02108,"A quantum generalization of Natural Gradient Descent is presented as part of a general-purpose optimization framework for variational quantum circuits. The optimization dynamics is interpreted as moving in the steepest descent direction with respect to the Quantum Information Geometry, corresponding to the real part of the Quantum Geometric Tensor (QGT), also known as the Fubini-Study metric tensor. An efficient algorithm is presented for computing a block-diagonal approximation to the Fubini-Study metric tensor for parametrized quantum circuits, which may be of independent interest. ",Quantum Natural Gradient
100,1169923445724868608,993462863112089600,Lyndon Emsley,"['Our second paper posted to arXiv with @COSMO_EPFL. Let us know what you think about our new approach to determining confidence in chemical shift driven powder #NMR #crystallography, before we get the reviews in.\n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.00870,"Nuclear Magnetic Resonance (NMR) spectroscopy is particularly well-suited to determine the structure of molecules and materials in powdered form. Structure determination usually proceeds by finding the best match between experimentally observed NMR chemical shifts and those of candidate structures. Chemical shifts for the candidate configurations have traditionally been computed by electronic-structure methods, and more recently predicted by machine learning. However, the reliability of the determination depends on the errors in the predicted shifts. Here we propose a Bayesian framework for determining the confidence in the identification of the experimental crystal structure, based on knowledge of the typical error in the electronic structure methods. We also extend the recently-developed ShiftML machine-learning model, including the evaluation of the uncertainty of its predictions. We demonstrate the approach on the determination of the structures of six organic molecular crystals. We critically assess the reliability of the structure determinations, facilitated by the introduction of a visualization of the of similarity between candidate configurations in terms of their chemical shifts and their structures. We also show that the commonly used values for the errors in calculated $^{13}$C shifts are underestimated, and that more accurate, self-consistently determined uncertainties make it possible to use $^{13}$C shifts to improve the accuracy of structure determinations. ",A Bayesian approach to NMR crystal structure determination
101,1169902290293014530,3423739275,Felix Leditzky,"['New @JILAscience paper with M. Alhejji, J. Levin and @quantum_graeme: for classical multiple access channels we consider entanglement assistance between senders leading to cap. region increase, and we show NP-hardness for the unassisted cap. region.\n<LINK>']",https://arxiv.org/abs/1909.02479,"Communication networks have multiple users, each sending and receiving messages. A multiple access channel (MAC) models multiple senders transmitting to a single receiver, such as the uplink from many mobile phones to a single base station. The optimal performance of a MAC is quantified by a capacity region of simultaneously achievable communication rates. We study the two-sender classical MAC, the simplest and best-understood network, and find a surprising richness in both a classical and quantum context. First, we find that quantum entanglement shared between senders can substantially boost the capacity of a classical MAC. Second, we find that optimal performance of a MAC with bounded-size inputs may require unbounded amounts of entanglement. Third, determining whether a perfect communication rate is achievable using finite-dimensional entanglement is undecidable. Finally, we show that evaluating the capacity region of a two-sender classical MAC is in fact NP-hard. ",Playing Games with Multiple Access Channels
102,1169874865391669250,2999702157,Anton Ilderton,"['New paper on the @arxiv! By #me. Using a simplified #laser field model I give exact results for two #scattering processes and contrast their high-intensty limits with conjectured behaviour in the literature.... also, pretty pictures. \n\n<LINK>\n\n@plym_math <LINK>']",https://arxiv.org/abs/1909.02484,"We give exact results for the emission spectra of both nonlinear Breit-Wheeler pair production and nonlinear Compton scattering in ultra-intense, ultra-short duration plane wave backgrounds, modelled as delta-function pulses. This includes closed form expressions for total scattering probabilities. We show explicitly that these probabilities do not exhibit the power-law scaling with intensity associated with the conjectured breakdown of (Furry picture) perturbation theory, instead scaling logarithmically in the high-intensity limit. ",Exact results for scattering on ultra-short plane wave backgrounds
103,1169784720017637376,745778402930503681,Gyuwan Kim,"['#emnlp2019 paper ""Subword Language Model for Query Auto-Completion"" for faster decoding while maintaining close accuracy compared to character LM. + new evaluation metric for QAC.  For more details, please check paper (<LINK>) and code (<LINK>).']",https://arxiv.org/abs/1909.00599,"Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with subwords shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our model achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the model could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component. ",Subword Language Model for Query Auto-Completion
104,1169784255477477378,2337598033,Geraint F. Lewis,['A new paper on arXiv (with @bigticketdw @nfmartin1980 @michelle_lmc @dougalmackey) <LINK> <LINK>'],https://arxiv.org/abs/1909.02017,"We present new horizontal branch (HB) distance measurements to 17 of the faintest known M31 satellites ($-6 \lesssim M_{V} \lesssim -13$) based on deep Hubble Space Telescope (HST) imaging. The color-magnitude diagrams extend $\sim$1-2 magnitudes below the HB, which provides for well-defined HBs, even for faint galaxies in which the tip of the red giant branch (TRGB) is sparsely populated. We determine distances across the sample to an average precision of 4% ($\sim 30$~kpc at $800$~kpc). We find that the majority of these galaxies are in good agreement, though slightly farther (0.1-0.2 mag) when compared to recent ground-based TRGB distances. Two galaxies (And~IX and And~XVII) have discrepant HST and ground-based distances by $\sim 0.3$ mag ($\sim 150$~kpc), which may be due to contamination from Milky Way foreground stars and/or M31 halo stars in sparsely populated TRGB regions. We use the new distances to update the luminosities and structural parameters for these 17 M31 satellites. The new distances do not substantially change the spatial configuration of the M31 satellite system. We comment on future prospects for precise and accurate HB distances for faint galaxies in the Local Group and beyond. ","A rogues gallery of Andromeda's dwarf galaxies II. Precise Distances to
  17 Faint Satellites"
105,1169775264034779136,35031140,Bas Hofstra,"['Hello #metascience2019 &amp; other friends, we just posted our new paper on arXiv: <LINK>. Title basically covers content: ""Diversity Breeds Innovation With Discounted Impact and Recognition."" Studied through records and texts from nearly all US PhDs across 30years.', 'Elaborates and extends prior work on innovation through text analyses and asks where the diversity-paradox in science (diversity breeds innovation, whereas diverse folks have less science careers) comes from.', 'With my great colleagues Sebastian Munoz-Najar Galvez, Bryan He, @viveksc, and Daniel A. McFarland', ""@viveksc I'm very glad to tentatively share this piece, it integrates many things/processes that are/were rewarding: an important question and answer, integration of multiple data sets, novel metrics and computation, interdisciplinary set of authors.""]",https://arxiv.org/abs/1909.02063,"Prior work finds a diversity paradox: diversity breeds innovation, and yet, underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-population of ~1.2 million US doctoral recipients from 1977-2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: e.g., novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity's role in innovation and partly explains the underrepresentation of some groups in academia. ",The Diversity-Innovation Paradox in Science
106,1169773697730846722,1556664198,Kyle Cranmer,"['New paper: ‚ÄúMining for Dark Matter Substructure:\nInferring subhalo population properties from strong lenses with machine learning‚Äù with Johann Brehmer, Siddharth Mishra-Sharma (@kdqg1) @joeri_hermans, and @glouppe and @KyleCranmer \n<LINK> <LINK>', 'Dark Matter‚Äôs gravitational effects can bend light making a gravitational lens. There are many things about dark matter (DM) we don‚Äôt know, but we do know it interacts gravitationally. \nhttps://t.co/8IrwEfWMl0 https://t.co/woOqWxg2Dp', 'The distribution of DM is clumpy. Big clumps of DM seed the formation of galaxies and make DM halos. But the halos have substructure ‚Äî smaller clumps called subhalos.\nhttps://t.co/sgvWGkQQS9', 'The subtle &amp; unique imprint of DM substructure on extended arcs in strong lensing systems contains a wealth of information about the properties &amp; distribution of DM on small scales &amp;, consequently, about the underlying particle physics. https://t.co/D59Xmz0HX0', 'We use the simulation-based inference (or likelihood-free inference) techniques we originally developed for the Large Hadron Collider to gravitational lensing from dark matter. \n(https://t.co/Deb0gfk3dp)\n\nhttps://t.co/0qN6tHFj6U', 'Here are some example simulations of a lensed system. The markers indicate location of subhalos, which distort the ring. (This corresponds to a complicated latent space and non-differentiable components to the simulation) https://t.co/Wma8lcOkhq', 'Ultimately, we are interested in inferring the parameters that describe the population of subhalos. The middle plot shows the sub halo mass function, the right shows the posterior over the 2-parameter model as we collect several lensed images (left). \nhttps://t.co/fiWgrmmQIm https://t.co/iUeTcZbhzi', 'The code for the project is on GitHub and each figure has an icon linking to a notebook to reproduce the figure\nGitHub: https://t.co/im8Re9hXkr https://t.co/KB1WO63Jxs', ""I learned a ton doing this project. It's my first astro paper and working on Dark Matter is always fun. As always, a pleasure to work with such a fantastic team. @glouppe @joeri_hermans, @kdqg1, and Johann Brehmer. @NYUDataScience @NYUScience"", '@glouppe @joeri_hermans @kdqg1 @NYUDataScience @NYUScience I should also say that this paper discusses a bit some of the issues around ""probabilistic catalogs"" (likelihood functions vs. posteriors), which @davidwhogg will probably enjoy. It\'s not about choosing between frequentist vs. Bayesian, but about what information is published.']",https://arxiv.org/abs/1909.02005,"The subtle and unique imprint of dark matter substructure on extended arcs in strong lensing systems contains a wealth of information about the properties and distribution of dark matter on small scales and, consequently, about the underlying particle physics. However, teasing out this effect poses a significant challenge since the likelihood function for realistic simulations of population-level parameters is intractable. We apply recently-developed simulation-based inference techniques to the problem of substructure inference in galaxy-galaxy strong lenses. By leveraging additional information extracted from the simulator, neural networks are efficiently trained to estimate likelihood ratios associated with population-level parameters characterizing substructure. Through proof-of-principle application to simulated data, we show that these methods can provide an efficient and principled way to simultaneously analyze an ensemble of strong lenses, and can be used to mine the large sample of lensing images deliverable by near-future surveys for signatures of dark matter substructure. ","Mining for Dark Matter Substructure: Inferring subhalo population
  properties from strong lenses with machine learning"
107,1169698492316405760,503452360,William Wang,"['BLEU/ROUGE only captures exact lexical overlap. Can one optimize distributional semantic rewards using REINFORCE for abstractive summarization? In our #EMNLP2019 paper, we show that optimizing BERTscore is a viable new solution for deep RL in #NLProc. <LINK>']",https://arxiv.org/abs/1909.00141,"Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language. ","Deep Reinforcement Learning with Distributional Semantic Rewards for
  Abstractive Summarization"
108,1169680305160163328,134765219,Stefanie Haustein,"[""If you're interested in Facebook and science communication, read our new preprint: <LINK>\nIf you're interested in Facebook, science communication and too busy/lazy to read a paper, read @juancommander's thread üëá <LINK>""]",http://arxiv.org/abs/1909.01476,"Despite its undisputed position as the biggest social media platform, Facebook has never entered the main stage of altmetrics research. In this study, we argue that the lack of attention by altmetrics researchers is due, in part, to the challenges in collecting Facebook data regarding activity that takes place outside of public pages and groups. We present a new method of collecting aggregate counts of shares, reactions, and comments across the platform-including users' personal timelines-and use it to gather data for all articles published between 2015 to 2017 in the journal PLOS ONE. We compare the gathered data with altmetrics collected and aggregated by Altmetric. The results show that 58.7% of papers shared on Facebook happen outside of public spaces and that, when collecting all shares, the volume of activity approximates patterns of engagement previously only observed for Twitter. Both results suggest that the role and impact of Facebook as a medium for science and scholarly communication has been underestimated. Furthermore, they emphasise the importance of openness and transparency around the collection and aggregation of altmetrics. ","How much research shared on Facebook happens outside of public pages and
  groups? A comparison of public and private online activity around PLOS ONE
  papers"
109,1169524930645544963,421251112,Cesar Pereida Garcia,"['Did you know that key formats impact the security against side-channel attacks? Check our new paper describing  certified #SCA! EM, timing and cache-timing attacks!\nShout out to the team @Sohaibuh @nictuv Iaroslav Gridin @acaldaya and Billy Bob Brumley.\n<LINK>']",https://arxiv.org/abs/1909.01785,"We demonstrate that the format in which private keys are persisted impacts Side Channel Analysis (SCA) security. Surveying several widely deployed software libraries, we investigate the formats they support, how they parse these keys, and what runtime decisions they make. We uncover a combination of weaknesses and vulnerabilities, in extreme cases inducing completely disjoint multi-precision arithmetic stacks deep within the cryptosystem level for keys that otherwise seem logically equivalent. Exploiting these vulnerabilities, we design and implement key recovery attacks utilizing signals ranging from electromagnetic (EM) emanations, to granular microarchitecture cache timings, to coarse traditional wall clock timings. ",Certified Side Channels
110,1169449526437875714,1576235694,Michael Brown,"["".@amelia_fmc has a new paper on Milky Way analogues. Her SDSS images of these galaxies immediately highlight the Milky Way's relatively low specific star formation rate. <LINK>\n\n(Bonus: who can spot the Seyfert 1?) <LINK>""]",https://arxiv.org/abs/1909.01654,"The Milky Way has been described as an anaemic spiral, but is its star formation rate (SFR) unusually low when compared to its peers? To answer this question, we define a sample of Milky Way Analogues (MWAs) based on stringent cuts on the best literature estimates of non-transient structural features for the Milky Way. This selection yields only 176 galaxies from the whole of the SDSS DR7 spectroscopic sample which have morphological classifications in GZ2, from which we infer SFRs from two separate indicators. The mean SFRs found are $\log(\rm{SFR}_{SED}/\rm{M}_{\odot}~\rm{yr}^{-1})=0.53$ with a standard deviation of 0.23 dex from SED fits, and $\log(\rm{SFR}_{W4}/\rm{M}_{\odot}~\rm{yr}^{-1})=0.68$ with a standard deviation of 0.41 dex from a mid-infrared calibration. The most recent estimate for the Milky Way's star formation rate of $\log(\rm{SFR}_{MW}/\rm{M}_{\odot}~\rm{yr}^{-1})=0.22$ fits well within 2$\sigma$ of these values, where $\sigma$ is the standard deviation of each of the SFR indicator distributions. We infer that the Milky Way, while being a galaxy with a somewhat low SFR, is not unusual when compared to similar galaxies. ","From the Outside Looking in: What can Milky Way Analogues Tell us About
  the Star Formation Rate of Our Own Galaxy?"
111,1169290756353351682,88519494,Mike Pritchard,"['New paper ""Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems"" <LINK>. Tom Beucler unveils elegant way to conserve enthalpy, mass and radiation in  NNs of convection for climate simulation! @uciess @PierreGentine @raspstephan #oxmlwxcl']",https://arxiv.org/abs/1909.00912,"Neural networks can emulate nonlinear physical systems with high accuracy, yet they may produce physically-inconsistent results when violating fundamental constraints. Here, we introduce a systematic way of enforcing nonlinear analytic constraints in neural networks via constraints in the architecture or the loss function. Applied to convective processes for climate modeling, architectural constraints enforce conservation laws to within machine precision without degrading performance. Enforcing constraints also reduces errors in the subsets of the outputs most impacted by the constraints. ","Enforcing Analytic Constraints in Neural-Networks Emulating Physical
  Systems"
112,1169170851289292800,1138299346351403008,Andrew Foong,"[""Check out our new paper: We prove single hidden layer Bayesian NNs with any mean-field Gaussian or MC dropout posterior can't represent uncertainty in between separated clusters of data. More flexible approximations needed? <LINK> <LINK>""]",https://arxiv.org/abs/1909.00719,"While Bayesian neural networks (BNNs) hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs. ","On the Expressiveness of Approximate Inference in Bayesian Neural
  Networks"
113,1169145914872778752,179893276,Sinan Kalkan,"['New paper with @KemLoksz, @camcanbaris and @eakbas2 on Imbalance Problems in Object Detection, which provides a taxonomy, identifies open challenges as well as introduces new imbalance problems. A must read for people working in object detection.  <LINK>']",https://arxiv.org/abs/1909.00169,"In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: this https URL . ",Imbalance Problems in Object Detection: A Review
114,1169072521510977536,6642132,Ikuya Yamada,['Our new CoNLL paper ‚ÄúNeural Attentive Bag-of-Entities Model for Text Classification‚Äù with @haplotyper is now available on ArXiv: <LINK>'],https://arxiv.org/abs/1909.01259,"This study proposes a Neural Attentive Bag-of-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at this https URL ",Neural Attentive Bag-of-Entities Model for Text Classification
115,1181896282195058689,2776073211,Tomasz Kusmierczyk,"['Our new paper on ""Correcting Predictions for Approximate Bayesian Inference"" is already available on  arXiv: <LINK>.']",https://arxiv.org/abs/1909.04919,"Bayesian models quantify uncertainty and facilitate optimal decision-making in downstream applications. For most models, however, practitioners are forced to use approximate inference techniques that lead to sub-optimal decisions due to incorrect posterior predictive distributions. We present a novel approach that corrects for inaccuracies in posterior inference by altering the decision-making process. We train a separate model to make optimal decisions under the approximate posterior, combining interpretable Bayesian modeling with optimization of direct predictive accuracy in a principled fashion. The solution is generally applicable as a plug-in module for predictive decision-making for arbitrary probabilistic programs, irrespective of the posterior inference strategy. We demonstrate the approach empirically in several problems, confirming its potential. ",Correcting Predictions for Approximate Bayesian Inference
116,1179041597645680640,1115946897431244800,Darsh J Shah,"['Check-out our new paper - <LINK>\nAutomatic Fact-guided sentence modification.\n\nMethod to automatically modify the factual information in a sentence. \n\nJoint work with @str_t5 , Prof. Regina Barzilay.']",https://arxiv.org/abs/1909.13838,"Online encyclopediae like Wikipedia contain large amounts of text that need frequent corrections and updates. The new information may contradict existing content in encyclopediae. In this paper, we focus on rewriting such dynamically changing articles. This is a challenging constrained generation task, as the output must be consistent with the new information and fit into the rest of the existing document. To this end, we propose a two-step solution: (1) We identify and remove the contradicting components in a target text for a given claim, using a neutralizing stance model; (2) We expand the remaining text to be consistent with the given claim, using a novel two-encoder sequence-to-sequence model with copy attention. Applied to a Wikipedia fact update dataset, our method successfully generates updated sentences for new claims, achieving the highest SARI score. Furthermore, we demonstrate that generating synthetic data through such rewritten sentences can successfully augment the FEVER fact-checking training dataset, leading to a relative error reduction of 13%. ",Automatic Fact-guided Sentence Modification
117,1178452182280134657,1074533138825732097,Michael Posa,"['We have a new pre-print, by lead author Alp Aydinoglu, ""Contact-Aware Controller Design for Complementarity Systems."" The paper presents an algorithm to design provably stable controllers for multi-contact systems using both state and tactile sensing.\n<LINK>']",https://arxiv.org/abs/1909.11221,"While many robotic tasks, like manipulation and locomotion, are fundamentally based in making and breaking contact with the environment, state-of-the-art control policies struggle to deal with the hybrid nature of multi-contact motion. Such controllers often rely heavily upon heuristics or, due to the combinatoric structure in the dynamics, are unsuitable for real-time control. Principled deployment of tactile sensors offers a promising mechanism for stable and robust control, but modern approaches often use this data in an ad hoc manner, for instance to guide guarded moves. In this work, by exploiting the complementarity structure of contact dynamics, we propose a control framework which can close the loop on rich, tactile sensors. Critically, this framework is non-combinatoric, enabling optimization algorithms to automatically synthesize provably stable control policies. We demonstrate this approach on three different underactuated, multi-contact robotics problems. ",Contact-Aware Controller Design for Complementarity Systems
118,1177741374755524608,40156543,Grace Hui Yang,['Check out our new paper on dialogue acts #cikm2019 <LINK>'],https://arxiv.org/abs/1909.00521,"In dialogues, an utterance is a chain of consecutive sentences produced by one speaker which ranges from a short sentence to a thousand-word post. When studying dialogues at the utterance level, it is not uncommon that an utterance would serve multiple functions. For instance, ""Thank you. It works great."" expresses both gratitude and positive feedback in the same utterance. Multiple dialogue acts (DA) for one utterance breeds complex dependencies across dialogue turns. Therefore, DA recognition challenges a model's predictive power over long utterances and complex DA context. We term this problem Concurrent Dialogue Acts (CDA) recognition. Previous work on DA recognition either assumes one DA per utterance or fails to realize the sequential nature of dialogues. In this paper, we present an adapted Convolutional Recurrent Neural Network (CRNN) which models the interactions between utterances of long-range context. Our model significantly outperforms existing work on CDA recognition on a tech forum dataset. ",Modeling Long-Range Context for Concurrent Dialogue Acts Recognition
119,1177418279834533888,125399960,Jaap Jumelet,"[""How does information flow in neural language models? Which inputs are causally related to which outputs? What biases does such a network have?\n\nCheck my new CoNLL paper on 'Generalized Contextual Decomposition' for LSTMs w/ \xa0@_dieuwke_ and @wzuidema!\n\n<LINK>"", 'We build on ‚ÄòContextual Decomposition‚Äô of @jamieMurdoch2 (Murdoch et al., 2018). Our generalization, applied to LSTM-based language models, allows us to not only quantify how much each input words contributes to the prediction of subsequent words... https://t.co/dIKQ3dw74Y', '‚Ä¶ but also to systematically test hypotheses on how words interact to shape those predictions. We focus on two different linguistic phenomena: number agreement and pronoun resolution, and find that in both cases, the model uses a *default reasoning* strategy.', 'For number agreement a model only predicts a plural verb when provided sufficient evidence from a plural noun, but defaults to singular without requiring a singular noun. This effect is not only found in the word embeddings, but the model intercepts as well. https://t.co/eVlN42h2qY', 'Key to our approach is that we can vary, and experimentally validate, which interactions between model components are considered crucial for the final prediction; by letting GCD solely focus on the intercepts we are able to determine the implicit bias that they encode. https://t.co/iHPiOYqWFF', ""GCD also allows us to study pronoun resolution, for which 'behavioural experiments' for language models are more difficult to design. We compare cases with unambiguously gendered referents (son, queen) and stereotypically gendered referents (doctor, nurse)."", 'Also here we find a default reasoning effect: male pronouns are predicted by default, whereas female pronouns require actual evidence. Moreover, we find a strong asymmetry between the effects of stereotypically male and stereotypically female referents on the model.', 'As expected, stereotypically male referents contribute to the generation of male pronouns. But surprisingly, stereotypically female referents do not contribute to the generation of female pronouns! https://t.co/j7369xLlvN', ""The code for our experiments can be found on Github: https://t.co/DEp0ynSdv4\n\nDon't hesitate to reach out if anything is unclear, I'd love to discuss this further online (or face-to-face in Hong Kong!)"", '@JamieMurdoch2 Great, thanks a lot!']",https://arxiv.org/abs/1909.08975,"Extensive research has recently shown that recurrent neural language models are able to process a wide range of grammatical phenomena. How these models are able to perform these remarkable feats so well, however, is still an open question. To gain more insight into what information LSTMs base their decisions on, we propose a generalisation of Contextual Decomposition (GCD). In particular, this setup enables us to accurately distil which part of a prediction stems from semantic heuristics, which part truly emanates from syntactic cues and which part arise from the model biases themselves instead. We investigate this technique on tasks pertaining to syntactic agreement and co-reference resolution and discover that the model strongly relies on a default reasoning effect to perform these tasks. ","Analysing Neural Language Models: Contextual Decomposition Reveals
  Default Reasoning in Number and Gender Assignment"
120,1177395165566078976,1084300516543381505,endo_suguru,"['Our new paper (collaboration work with Qunasys) appeared!\n\nOwing to Nakagawa-san and Kurata-san (Nakagawa Yuya and Iori Kurata), we could write such a nice paper!\n\n<LINK>']",https://arxiv.org/abs/1909.12250,"The Green's function plays a crucial role when studying the nature of quantum many-body systems, especially strongly-correlated systems. Although the development of quantum computers in the near future may enable us to compute energy spectra of classically-intractable systems, methods to simulate the Green's function with near-term quantum algorithms have not been proposed yet. Here, we propose two methods to calculate the Green's function of a given Hamiltonian on near-term quantum computers. The first one makes use of a variational dynamics simulation of quantum systems and computes the dynamics of the Green's function in real time directly. The second one utilizes the Lehmann representation of the Green's function and a method which calculates excited states of the Hamiltonian. Both methods require shallow quantum circuits and are compatible with near-term quantum computers. We numerically simulated the Green's function of the Fermi-Hubbard model and demonstrated the validity of our proposals. ",Calculation of the Green's function on near-term quantum computers
121,1176471158771916800,44446416,Fabr√≠cio Benevenuto,"['In this new paper we evaluated the effectiveness of reducing whatsapp forward limits, using empirical data from three countries (Brazil, India, and Indonesia) and epidemiological models. <LINK>', 'Our results suggest that the current efforts deployed by WhatsApp can offer significant delays on the information spread, but they are ineffective in blocking the propagation of misinformation campaigns through public groups when the content has a high viral nature.']",https://arxiv.org/abs/1909.08740,"WhatsApp is the most popular messaging app in the world. The closed nature of the app, in addition to the ease of transferring multimedia and sharing information to large-scale groups make WhatsApp unique among other platforms, where an anonymous encrypted messages can become viral, reaching multiple users in a short period of time. The personal feeling and immediacy of messages directly delivered to the user's phone on WhatsApp was extensively abused to spread unfounded rumors and create misinformation campaigns during recent elections in Brazil and India. WhatsApp has been deploying measures to mitigate this problem, such as reducing the limit for forwarding a message to at most five users at once. Despite the welcomed effort to counter the problem, there is no evidence so far on the real effectiveness of such restrictions. In this work, we propose a methodology to evaluate the effectiveness of such measures on the spreading of misinformation circulating on WhatsApp. We use an epidemiological model and real data gathered from WhatsApp in Brazil, India and Indonesia to assess the impact of limiting virality features in this kind of network. Our results suggest that the current efforts deployed by WhatsApp can offer significant delays on the information spread, but they are ineffective in blocking the propagation of misinformation campaigns through public groups when the content has a high viral nature. ",Can WhatsApp Counter Misinformation by Limiting Message Forwarding?
122,1175942530183127040,25507283,Bruno Salcedo,"['Brand new (and improved) version of my paper on the identification of discrete games, joint with Nail Kashaev <LINK> #econtwitter <LINK>']",https://arxiv.org/abs/1909.09320,"The empirical analysis of discrete complete-information games has relied on behavioral restrictions in the form of solution concepts, such as Nash equilibrium. Choosing the right solution concept is crucial not just for identification of payoff parameters, but also for the validity and informativeness of counterfactual exercises and policy implications. We say that a solution concept is discernible if it is possible to determine whether it generated the observed data on the players' behavior and covariates. We propose a set of conditions that make it possible to discern solution concepts. In particular, our conditions are sufficient to tell whether the players' choices emerged from Nash equilibria. We can also discern between rationalizable behavior, maxmin behavior, and collusive behavior. Finally, we identify the correlation structure of unobserved shocks in our model using a novel approach. ",Discerning Solution Concepts
123,1175230775186415616,449107555,Tom Goffrey,['New paper on numerical methods for planetary science. Lead by people significantly more mathematically talented than me.\n\nPaper: <LINK>\nECLIPS3D code available at: <LINK> <LINK>'],https://arxiv.org/abs/1909.03722,"Context. The study of linear waves and instabilities is necessary to understand the physical evolution of an atmosphere, and can provide physical interpretation of the complex flows found in simulations performed using Global Circulation Models (GCM). In particular, the acceleration of superrotating flow at the equator of hot Jupiters has mostly been studied under several simplifying assumptions, the relaxing of which may impact final results. Aims. We develop and benchmark a publicly available algorithm to identify the eigenmodes of an atmosphere around any initial steady state. We also solve for linear steady states. Methods. We linearise the hydrodynamical equations of a planetary atmosphere in a steady state with arbitrary velocities and thermal profile. We then discretise the linearised equations on an appropriate staggered grid, and solve for eigenvectors and linear steady solutions with the use of a parallel library for linear algebra: ScaLAPACK. We also implement a posteriori calculation of an energy equation in order to obtain more information on the underlying physics of the mode. Results. Our code is benchmarked against classical wave and instability test cases in multiple geometries. The steady linear circulation calculations also reproduce expected results for the atmosphere of hot Jupiters. We finally show the robustness of our energy equation, and its power to obtain physical insight into the modes. Conclusions. We have developed and benchmarked a code for the study of linear processes in planetary atmospheres, with an arbitrary steady state. The calculation of an a posteriori energy equation provides both increased robustness and physical meaning to the obtained eigenmodes. This code can be applied to various problems, and notably to further study the initial spin up of superrotation of GCM simulations of hot Jupiter. ","Eigenvectors, Circulation and Linear Instabilities for Planetary Science
  in 3 Dimensions (ECLIPS3D)"
124,1175128024200749058,564888272,Pierre Marchand,['New paper accepted in A&amp;A !\nWhere I talk about angular momentum and the Hall effect in star formation.\n<LINK>'],https://arxiv.org/abs/1909.09025,"We present here a minor modification of our numerical implementation of the Hall effect for the 2D Riemann solver used in Constrained Transport schemes, as described in Marchand et al. (2018). In the previous work, the tests showed that the angular momentum was not conserved during protostellar collapse simulations, with significant impact. By removing the whistler waves speed from the characteristic speeds of non-magnetic variables in the 1D Riemann solver, we are able to improve the angular momentum conservation in our test-case by one order of magnitude, while keeping the second-order numerical convergence of the scheme. We also reproduce the simulations of Tsukamoto et al. (2015) with consistent resistivities, the three non-ideal MHD effects and initial rotation, and agree with their results. In this case, the violation of angular momentum conservation is negligible in regard to the total angular momentum and the angular momentum of the disk. ","Impact of the Hall effect in star formation : improving the angular
  momentum conservation"
125,1173564714305171456,897534330079072256,Aishwarya Kamath,"['Departing from work on detecting annotation artifacts in datasets, here we take a different approach which gave insights on what deep networks like to read. Check out our new paper! <LINK> <LINK>']",https://arxiv.org/abs/1909.04547,"Recent research towards understanding neural networks probes models in a top-down manner, but is only able to identify model tendencies that are known a priori. We propose Susceptibility Identification through Fine-Tuning (SIFT), a novel abstractive method that uncovers a model's preferences without imposing any prior. By fine-tuning an autoencoder with the gradients from a fixed classifier, we are able to extract propensities that characterize different kinds of classifiers in a bottom-up manner. We further leverage the SIFT architecture to rephrase sentences in order to predict the opposing class of the ground truth label, uncovering potential artifacts encoded in the fixed classification model. We evaluate our method on three diverse tasks with four different models. We contrast the propensities of the models as well as reproduce artifacts reported in the literature. ",What do Deep Networks Like to Read?
126,1172580007547301888,2835427044,Sneha Kudugunta,"['New EMNLP paper ‚ÄúInvestigating Multilingual NMT Representation at Scale‚Äù w/ @ankurbpn, @orf_bnw, @caswell_isaac, @naveenariva. We study transfer in massively multilingual NMT @GoogleAI from the perspective of representational similarity.\n\nPaper: <LINK> 1/n <LINK>', '@ankurbpn @orf_bnw @caswell_isaac @naveenariva @GoogleAI We discuss the challenges of comparing misaligned sequences, and use a variant of SVCCA.  2/n', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We find that encoder representations of different languages cluster according to linguistic similarity... 3/n https://t.co/lHFtSIdilm', '@ankurbpn @orf_bnw @naveenariva @GoogleAI ‚Ä¶ Even at a more fine-grained level. 4/n https://t.co/iiwfV8MaoO', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We look at how our similarity measure capturing linguistic similarity vs script. 5/n https://t.co/V5qqInb1CH', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We also find that representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. 6/n https://t.co/IX4HYmYOSd', '@ankurbpn @orf_bnw @naveenariva @GoogleAI Colab to play with coming soon! 7/n', '@ankurbpn @orf_bnw @naveenariva @GoogleAI Huge thanks to my collaborators at @GoogleAI, without whom this work would not have been possible. This work was done as a part of the Google AI Residency - applications open soon, so definitely check it out!\n\nhttps://t.co/gFZs0tnWTA 8/8']",https://arxiv.org/abs/1909.02197,"Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major conclusions from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning. ",Investigating Multilingual NMT Representations at Scale
127,1172545382984761345,467069651,Arvind Neelakantan,"['We are excited to release Taskmaster-1, a new task-oriented dialog dataset.  We explore two methods for data collection, two-person and self-dialogs. Surprisingly self-dialogs are an effective way to collect dialog. \n\nPaper accepted to @emnlp2019 : <LINK> <LINK>', '@emnlp2019 Data: https://t.co/ld81l2vUJP\n\nWork done with many awesome colleagues at Google Assistant team and\n@GoogleAI along with student researcher Chinnadhurai Shankar']",https://arxiv.org/abs/1909.05358,"A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken ""Wizard of Oz"" (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is ""self-dialog"" in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design. ",Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset
128,1171836162408927233,8105842,Stephen Kinsella,"[""Just put out a new paper looking the structure of the network of inter-industry worker flows in the Irish economy. This is joint work with Dr Neave O'Clery from @oxunimaths and Eoin Slattery from @CSOIreland. Working paper is here <LINK>. Threaaaaad üëá"", ""We start from the observation that skills are embedded in workers, and when workers move job, they take those skills with them. There are three literatures we're speaking to with this paper. First, the evolutionary economic geography literature, which discusses embedded knowledge"", 'Second, the economic resilience literature, looking at the ability of a region to adapt to a shock. Third, the Porter/Delgado literature on the role of knowledge flows within industrial clusters, which they see as key to innovation and labour pooling.', 'What we do is use network science techniques to uncover industry clusters in inter-industry labour networks. Why does this matter? First, groups of tightly connected industries with high internal labour mobility and skill-sharing benefit from concentration effects.', ""But second, there's a risk: If there's a shock that reduces the ability of firms in some sector (eg construction in '08) to hire relevant skills &amp; gain new know-how, then that harms the ability of workers to seek new employment in unrelated sectors-their skills aren't marketable."", 'We study inter-industry labour flows from 2005 to 2014, using a unique dataset created from annual tax returns filed by employers on their employees to the Irish tax authorities. We have all labour market transitions in the Irish economy at the 4-digit NACE code level.', 'We find the Irish skill network exhibits a highly modular, nested structure. This helps us understand the ability of industries to benefit from similar skills. Finance/computing/public sector are tightly knit. Workers from these sectors rarely transition into the extended economy', ""We use some fairly fancy network modelling (thank you Dr O'Clery) to figure out what this dataset tells us. We also propose a new metric to predict industry employment growth based on the size of employment within an industry‚Äôs own community."", ""We also think we've found a useful way to think about (and identify) the optimal scale at which labour pooling operates, finding that the model performs best for relatively small industry clusters (mean size of about 20 industries)."", 'So: Pictures! Any network is a set of nodes with connections between them of varying strength. This shows the number of edges present as a ratio of those possible within and between 1-digit NACE sectors. Several sectors exhibit high levels of internal edges, including finance. https://t.co/N7f4dR9QFX', 'Interestingly most other sectors, and particularly manufacturing, retail and wholesale trade, transport, real estate, community work and agriculture, do not exhibit such a structure.', ""I'm excited to share this. It's the network of labour flows for the entire Irish economy from 2005-2014. We size the nodes by employment. What this shows is construction workers are much more likely to transition to mining, metals, and agriculture than public sector or finance. https://t.co/NRG6u0BgnN"", 'We observe a large degree of clustering of related industries, with services broadly located on the left-hand side, retail and farming center-bottom, complex manufacturing center-top and basic manufacturing and construction on the right-hand side.', ""(We can also break this data out by multinational/domestic firm, but that's a separate paper. We're also working on productivity differences between sectors as well, we think this line of research is really promising, but it is not ready to show you.)"", ""We try to figure out what the 'right' resolution is for this model. With data like this, a bit like google maps, you can 'zoom' in and out of the data. At the highest level, it's just the macroeconomy. At the lowest, it's a tiny cluster of firms."", ""To see this, look at this picture. It shows different versions of the graph above, with different resolutions tuned by the 'tao' parameter you see (7, 10, 13). It turns out the right 'level' to think about the Irish economy is about 5. https://t.co/qzN0QOOEkz"", ""This next picture is called a dendrogram. It shows which industries merge as you 'zoom out', and when they merge. Finance/aircraft leasing merge quickly. In the end, the Irish economy's inter industry network partitions into services &amp; manufacturing. https://t.co/CmRTkstoCI"", 'So, say you want to know which industries are more vulnerable than others. The picture on the left shows employment in adjacent industries vs total employment in 2016. On the right, employment in adjacent industries as a ratio of total employment vs total employment. https://t.co/y298zW921U', 'Why is that helpful? This metric gives us a sense of how many skilled workers are available in adjacent industries for each current worker in the sector. Firms can hire from neighbouring similar industries-but workers have the skills to move into these industries if laid off.', ""We run some regressions and use them to figure out if we can predict employment levels using a new community-related measure, and it turns out we can. But these regressions are fairly simple exercises, I wouldn't bet the house on them. What they really are are clues to help us-&gt;"", ""-&gt;figure out the right resolution to 'look' at the economy. We run a load of experiments to see which resolution is better, and it turns out to be 5. Why does that matter? There's a good chance 5 is a fairly good level of detail to understand the movement of workers. https://t.co/HOz84geO4k"", ""And that's most of the paper. So next steps. As I said above, we want to look at domestic vs multinational firms, we want to scale the network by productivity, and we'd love to see if the same relationships hold true at the firm level."", ""This project took quite a long time, and it is not finished by a long shot. This is the first output. We are looking for feedback, comments and questions, and of course discussion. We'll submit this to a journal later on this year once we've revised it a bit."", ""The main thing to do is to thank Neave and Eoin, my colleagues @BusinessAtUL and @Government_UoM who listened, read, and commented on an early draft, and to look forward to any feedback you've got on this: https://t.co/5JTrazUBNf https://t.co/tOhADlQBxa"", '@GerardBrady100 @OxUniMaths @CSOIreland Thanks, it‚Äôs just the start of this collaboration. Looking forward to the next version. It‚Äôs been a pleasure to work on this.', '@BusinessAtUL @Government_UoM Just realised Twitter autocorrected Eoin‚Äôs name, it‚Äôs Flaherty, apols Eoin!', ""@NollaigMul Take the ability to run a regression. It's something some people know how to do, and certain industries need those skills. So people with regression skills tend to fish up at and move between unis, finance places, statistical agencies."", '@nina_econ Thanks, it was a real case of learning by doing Dr Eichacker, especially when it came to the limitations of big data and how cool visualisation could be when thoughtfully applied. Also, just how limiting regression analysis is.', '@ronanlyons @GerardBrady100 @OxUniMaths @CSOIreland @JDelaRocaUSC @ProfDiegoPuga Brilliant, thanks Ronan. Our paper doesn‚Äôt have geographical variables but this literature is certainly close In spirit (and method). Will check it out.']",https://arxiv.org/abs/1909.03379,"There is an emerging consensus in the literature that locally embedded capabilities and industrial know-how are key determinants of growth and diversification processes. In order to model these dynamics as a branching process, whereby industries grow as a function of the availability of related or relevant skills, industry networks are typically employed. These networks, sometimes referred to as industry spaces, describe the complex structure of the capability or skill overlap between industry pairs, measured here via inter-industry labour flows. Existing models typically deploy a local or 'nearest neighbour' approach to capture the size of the labour pool available to an industry in related sectors. This approach, however, ignores higher order interactions in the network, and the presence of industry clusters or groups of industries which exhibit high internal skill overlap. We argue that these clusters represent skill basins in which workers circulate and diffuse knowledge, and delineate the size of the skilled labour force available to an industry. By applying a multi-scale community detection algorithm to this network of flows, we identify industry clusters on a range of scales, from many small clusters to few large groupings. We construct a new variable, cluster employment, which captures the workforce available to an industry within its own cluster. Using UK data we show that this variable is predictive of industry-city employment growth and, exploiting the multi-scale nature of the industrial clusters detected, propose a methodology to uncover the optimal scale at which labour pooling operates. ",Modular structure in labour networks reveals skill basins
129,1171555903952150529,503452360,William Wang,"['In our new #EMNLP2019 paper, we relax the fully-factored mean-field assumption, and propose a new Gaussian Copula Variational Autoencoder (VAE) to deal with posterior collapse. Neural Gaussian Copula for Variational Autoencoder: <LINK>  #NLProc @kingofspace0wzz']",https://arxiv.org/abs/1909.03569,"Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches. ",Neural Gaussian Copula for Variational Autoencoder
130,1169267243173253120,4187869443,Elisa Costantini,['New paper of our dusty group on arxiv today: D. Rogantini et al. ‚ÄúInterstellar dust along the line of sight of GX 3+1‚Äù <LINK> and stay tuned for the follow up paper on more sources! @kitakos @IoannaPsaradaki @SRON_Space'],https://arxiv.org/abs/1909.00652,"Studying absorption and scattering of X-ray radiation by interstellar dust grains allows us to access the physical and chemical properties of cosmic grains even in the densest regions of the Galaxy. We aim at characterising the dust silicate population which presents clear absorption features in the energy band covered by the Chandra X-ray Observatory. Through these absorption features, in principle, it is possible to infer the size distribution, composition, and structure of silicate in the interstellar medium. In particular, in this work, we investigate the magnesium and silicon K-edges. By using newly acquired synchrotron measurements, we build X-ray extinction models for fifteen dust candidates. These models, adapted for astrophysical analysis, and implemented in the Spex spectral fitting program, are used to reproduce the dust absorption features observed in the spectrum of the bright low mass X-ray binary GX 3+1 which is used as a background source. With the simultaneous analysis of the two edges we test two different size distributions of dust: one corresponding to the standard Mathis-Rumpl-Nordsieck model and one considering larger grains ($n(a) \propto a_i^{-3.5}$ with $0.005<a_1<0.25$ and $0.05<a_2<0.5$, respectively, with $a$ the grain size). These distributions may be representative of the complex Galactic region towards this source. We find that up to $70\%$ of dust is constituted by amorphous olivine. We discuss the crystallinity of the cosmic dust found along this line of sight. Both magnesium and silicon are highly depleted into dust ($\delta_{Z} = 0.89\ \rm{and}\ 0.94$, respectively) while their total abundance does not depart from solar values. ",Interstellar dust along the line of sight of GX 3+1
131,1178978980847919105,797433864,Danilo J. Rezende,"['We had a ""Hamiltonian extravaganza"" at @DeepMind. We show how to learn Hamiltonian gen models from pixels <LINK> and propose a general method for combining symmetry Lie groups with ODE-Flow generative models in <LINK> #physics #ML #symmetries <LINK> <LINK>', '@Deepmind Great working with @irinavlh,  S√©bastien Racani√®re, Peter Toth, @drew_jaegle and many others!  Thanks all for the hard work!', ""@timudk @Deepmind indeed vol-preservation is for the joint (q,p) space. This is less restrictive if we marginalize p to define a density on q. We can also treat the p at each step as a latent var. (instead of only the last one) In these cases, it doesn't look like there are obvious constraints.""]",https://arxiv.org/abs/1909.13789,"The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning. ",Hamiltonian Generative Networks
132,1178869323349118976,22399655,Ryota Kanaiüí°,['Our new paper on Information Closure Theory of Consciousness where we propose conscious processes form non-trivial information closure (NTIC). \n<LINK>'],https://arxiv.org/abs/1909.13045,"Information processing in neural systems can be described and analysed at multiple spatiotemporal scales. Generally, information at lower levels is more fine-grained and can be coarse-grained in higher levels. However, information processed only at specific levels seems to be available for conscious awareness. We do not have direct experience of information available at the level of individual neurons, which is noisy and highly stochastic. Neither do we have experience of more macro-level interactions such as interpersonal communications. Neurophysiological evidence suggests that conscious experiences co-vary with information encoded in coarse-grained neural states such as the firing pattern of a population of neurons. In this article, we introduce a new informational theory of consciousness: Information Closure Theory of Consciousness (ICT). We hypothesise that conscious processes are processes which form non-trivial informational closure (NTIC) with respect to the environment at certain coarse-grained levels. This hypothesis implies that conscious experience is confined due to informational closure from conscious processing to other coarse-grained levels. ICT proposes new quantitative definitions of both conscious content and conscious level. With the parsimonious definitions and a hypothesise, ICT provides explanations and predictions of various phenomena associated with consciousness. The implications of ICT naturally reconciles issues in many existing theories of consciousness and provides explanations for many of our intuitions about consciousness. Most importantly, ICT demonstrates that information can be the common language between consciousness and physical reality. ",Information Closure Theory of Consciousness
133,1178547420726206464,307880202,Blai Vidiella üéó,"['If we want to study the ecosystems, we should use all the possible knowledge available. Discrete systems (ie insects) are a source of interesting and rich dynamics (#chaos &amp; üëª) that can be confused by #randomness. I am really fascinated by our teamwork! <LINK> <LINK>']",https://arxiv.org/abs/1909.12501,"Ecological systems are complex dynamical systems. Modelling efforts on ecosystems' dynamical stability have revealed that population dynamics, being highly nonlinear, can be governed by complex fluctuations. Indeed, experimental and field research has provided mounting evidence of chaos in species' abundances, especially for discrete-time systems. Discrete-time dynamics, mainly arising in boreal and temperate ecosystems for species with non-overlapping generations, have been largely studied to understand the dynamical outcomes due to changes in relevant ecological parameters. The local and global dynamical behaviour of many of these models is difficult to investigate analytically in the parameter space and, typically, numerical approaches are employed when the dimension of the phase space is large. In this article we provide topological and dynamical results for a map modelling a discrete-time, three-species food chain with two predator species interacting on the same prey population. The domain where dynamics live is characterized, as well as the so-called escaping regions, for which the species go rapidly to extinction after surpassing the carrying capacity. We also provide a full description of the local stability of equilibria within a volume of the parameter space given by the prey's growth rate and the predation rates. We have found that the increase of the pressure of predators on the prey results in chaos. The entry into chaos is achieved via a supercritical Neimarck-Sacker bifurcation followed by period-doubling bifurcations of invariant curves. Interestingly, an increasing predation directly on preys can shift the extinction of top predators to their survival, allowing an unstable persistence of the three species by means of periodic and strange chaotic attractors. ","Dynamics in a time-discrete food-chain model with strong pressure on
  preys"
134,1177926432736997376,1111079114255921155,Takeuchi Lab,"['Posted a preprint!\nWe studied KPZ universal fluctuations of interfaces growing from curved shapes, which were accounted for by the KPZ variational formula. We believe our method for evaluating the variational formula is useful for various KPZ problems!\n<LINK>']",https://arxiv.org/abs/1909.11920,"We study fluctuations of interfaces in the Kardar-Parisi-Zhang (KPZ) universality class with curved initial conditions. By simulations of a cluster growth model and experiments of liquid-crystal turbulence, we determine the universal scaling functions that describe the height distribution and the spatial correlation of the interfaces growing outward from a ring. The scaling functions, controlled by a single dimensionless time parameter, show crossover from the statistical properties of the flat interfaces to those of the circular interfaces. Moreover, employing the KPZ variational formula to describe the case of the ring initial condition, we find that the formula, which we numerically evaluate, reproduces the numerical and experimental results precisely without adjustable parameters. This demonstrates that precise numerical evaluation of the variational formula is possible at all, and underlines the practical importance of the formula, which is able to predict the one-point distribution of KPZ interfaces for general initial conditions. ","Kardar-Parisi-Zhang Interfaces with Curved Initial Shapes and
  Variational Formula"
135,1177122509348638720,426509606,Yamir Moreno,"['In our latest work, out today on the arXiv, we study the dynamics of social contagion on hypergraphs and develop an analytical framework amenable of numerical solutions for arbitrary hypergraphs. Check it out at <LINK> w/ @GuiFdeArruda &amp; @lordgrilo <LINK>']",https://arxiv.org/abs/1909.11154,"Our understanding of the dynamics of complex networked systems has increased significantly in the last two decades. However, most of our knowledge is built upon assuming pairwise relations among the system's components. This is often an oversimplification, for instance, in social interactions that occur frequently within groups. To overcome this limitation, here we study the dynamics of social contagion on hypergraphs. We develop an analytical framework and provide numerical results for arbitrary hypergraphs, which we also support with Monte Carlo simulations. Our analyses show that the model has a vast parameter space, with first and second-order transitions, bi-stability, and hysteresis. Phenomenologically, we also extend the concept of latent heat to social contexts, which might help understanding oscillatory social behaviors. Our work unfolds the research line of higher-order models and the analytical treatment of hypergraphs, posing new questions and paving the way for modeling dynamical processes on these networks. ",Social contagion models on hypergraphs
136,1176542190744391680,1010536067886387200,Ananya Kumar,"[""In our NeurIPS 2019 (spotlight) paper, we explain why methods like Platt scaling / temperature scaling are less calibrated than reported, propose a way to overcome this issue, and describe how to measure a model's calibration error with fewer samples: <LINK>"", 'If you want to measure the calibration error of your model, try the debased estimator (Section 5). It‚Äôs easy to use, and we show (theoretically and experimentally) that it estimates the calibration error with fewer samples.', 'Past work on multiclass calibration only measures calibration on the most confident prediction. We look at ‚Äúmarginal calibration‚Äù (probability output for each class should be calibrated) like https://t.co/y4DU4MQW8e. We hope future work also reports marginal calibration scores.', 'Calibration background: Besides accuracy, we should measure the quality of a model‚Äôs uncertainty estimates. If a weather model says it is going to rain with 80% probability on 1000 days, it should rain on about 800 of them. We can quantify this using calibration error metrics.', 'Joint work with @tengyuma and @percyliang which we will present at #NeurIPS2019! Many exciting research directions remain in uncertainty calibration, as we discuss in Section 7 (calibration under dataset shifts, multiclass calibration, better metrics for measuring calibration).']",https://arxiv.org/abs/1909.10155,"Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates---those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient---it requires $O(B/\epsilon^2)$ samples, compared to $O(1/\epsilon^2)$ for scaling methods, where $B$ is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function to reduce variance and then bins the function values to actually ensure calibration. This requires only $O(1/\epsilon^2 + B)$ samples. Next, we show that we can estimate a model's calibration error more accurately using an estimator from the meteorological community---or equivalently measure its calibration error with fewer samples ($O(\sqrt{B})$ instead of $O(B)$). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration. In these experiments, we also estimate the calibration error and ECE more accurately than the commonly used plugin estimators. We implement all these methods in a Python library: this https URL ",Verified Uncertainty Calibration
137,1176527041807769601,246621369,Giovanni Beltrame,['How do you keep robots connected when exploring a large environment? We study field communication performance with quadcopters @ESA_CAVES @polymtl #Robotics @MIST_lab\n\nPaper:\n<LINK>\nVideo:\n<LINK>\nCode:\n<LINK>'],https://arxiv.org/abs/1909.10378,"Redundancy and parallelism make decentralized multi-robot systems appealing solutions for the exploration of extreme environments. However, effective cooperation often requires team-wide connectivity and a carefully designed communication strategy. Several recently proposed decentralized connectivity maintenance approaches exploit elegant algebraic results drawn from spectral graph theory. Yet, these proposals are rarely taken beyond simulations or laboratory implementations. In this work, we present two major contributions: (i) we describe the full-stack implementation---from hardware to software---of a decentralized control law for robust connectivity maintenance; and (ii) we assess, in the field, our setup's ability to correctly exchange all the necessary information required to maintain connectivity in a team of quadcopters. ","Decentralized Connectivity Control in Quadcopters: a Field Study of
  Communication Performance"
138,1176039359772053505,304669884,J Fern√°ndez Rossier,"['Our first preprint using @IBMResearch  #quantum computers is now in the arXiv: <LINK>\nWe propose  Berry phase estimation in gate based adiabatic quantum simulation and we apply for topological classification of the SSH and the dimerized Heisenberg model.', 'We carry out actual quantum computations on the IBM machines to successfully carry out the topological characterization of the SSH model.   It is great fun to combine our passion for topological phases of matter with the new ""toys"", the amazing @IBM quantum computers.']",https://arxiv.org/abs/1909.09415,"Gate-based quantum computers can in principle simulate the adiabatic dynamics of a large class of Hamiltonians. Here we consider the cyclic adiabatic evolution of a parameter in the Hamiltonian. We propose a quantum algorithm to estimate the Berry phase and use it to classify the topological order of both single-particle and interacting models, highlighting the differences between the two. This algorithm is immediately extensible to any interacting topological system. Our results evidence the potential of near-term quantum hardware for the topological classification of quantum matter. ",Berry Phase Estimation in Gate-Based Adiabatic Quantum Simulation
139,1175517530502422530,2280952807,Rada Mihalcea,['Misinformation is everywhere including‚Äîworryingly!‚Äîin health. We introduced a dataset &amp; algo with ling+speech+engagement features to find misinformation in medical videos. Guess what videos are more engaging?\n\nWork with R.Hui @vperez_r @LoebStacy @ ICMI19\n\n<LINK> <LINK>'],https://arxiv.org/abs/1909.01543,"Recent years have witnessed a significant increase in the online sharing of medical information, with videos representing a large fraction of such online sources. Previous studies have however shown that more than half of the health-related videos on platforms such as YouTube contain misleading information and biases. Hence, it is crucial to build computational tools that can help evaluate the quality of these videos so that users can obtain accurate information to help inform their decisions. In this study, we focus on the automatic detection of misinformation in YouTube videos. We select prostate cancer videos as our entry point to tackle this problem. The contribution of this paper is twofold. First, we introduce a new dataset consisting of 250 videos related to prostate cancer manually annotated for misinformation. Second, we explore the use of linguistic, acoustic, and user engagement features for the development of classification models to identify misinformation. Using a series of ablation experiments, we show that we can build automatic models with accuracies of up to 74%, corresponding to a 76.5% precision and 73.2% recall for misinformative instances. ",Towards Automatic Detection of Misinformation in Online Medical Videos
140,1174908875977265155,961972787077373952,Aykut Erdem,"['In our recent study, appearing in @CoNLL2019, we set out to understand how important is to track entities and their state changes for comprehending multimodal procedures, e.g. cooking recipes with pictures.\n\nPaper: üëâ <LINK>\nBlog: üëâ <LINK>\n[1/5] <LINK>', 'We present Procedural Reasoning Networks ‚Äî a BiDAF model extended with an extra module for tracking entity states. It outperforms others by a large margin on visual reasoning tasks of RecipeQA. So glad to be a part of this work w/ @AmacSercan, @semihyagcioglu &amp; @erkuterdem [2/5] https://t.co/NmO10HPMdF', 'We formalize this with Relational RNNs by @santoroAI et al., which allow for interactions between entities when updating their states as the model reads the recipe. We show that the resulting dynamic entity embeddings generalize well across recipes with similar flavors. [3/5] https://t.co/6tUJgL6b6q', 'Code for Procedural Reasoning Networks (PRNs) is available at https://t.co/CQ1FXuNx88. \n\nFeel free to get in touch with us about using or adopting our model. Lastly, we would like to thank @ai2_allennlp for providing an excellent code base that make this work possible. [4/5]', 'For those who are interested, here are some prior works most related to ours:\n- EntNets by @henaffmikael et al.: https://t.co/vMpwMHvaSp\n- RelNets by @trapitbansal et al.: https://t.co/3v1M390g6e\n- Working Memory Networks by @juanpavez et al.: https://t.co/Oy7ViehOHh\n[5/5]']",https://arxiv.org/abs/1909.08859,"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states. ",Procedural Reasoning Networks for Understanding Multimodal Procedures
141,1174119222026428416,116784416,Dr. Rachael Beaton,"['Scott Carlsten, @johnnypgreco, myself, and the incomparable Jenny Greene present a Wide Field Survey of Satellite Systems for 10 Hosts in the Local Volume.\nWe find 153 objects. 93 are new. Many will be background, buy some will be golden tickets.\n<LINK>', ""@johnnypgreco Here are our hosts -- 6 to 7 are Milky Way-like the others are not, but that's okay. \nWe use archival data in the @CFHTelescope + MegaCam that Scott carefully processed.\n\nWe recover all previously detected floof around them. https://t.co/l8J2sbWffC"", ""@johnnypgreco @CFHTelescope The archival area for most galaxies covers ~150 kpc in projected radius, meaning that we get a reasonable volume around the host. \n\nSome galaxies have lots of friends üòä! but some don't ü§î. Some galaxies have background groups behind them -- tricksy galaxies üòì. https://t.co/aPlSP1W84D"", '@johnnypgreco @CFHTelescope Unfortunately, some of them have lots of foreground stars in front of them -- like NGC1023 below -- usually, this is a ü§© for me because bright stars are awesome! but for this these are ü§Æ. \nScott had to search this area by hand -- but we find unambiguous satellite candidates. https://t.co/URFe5CDVEX', ""@johnnypgreco @CFHTelescope We find three kinds of objects: \ndwarf Irregulars, dwarf Ellipticals, and nucleated dwarf Ellipticals. \nAll are beautiful and special in their own way. \nWe give them all names based on their coordinates because there's more work to do to determine if they are satellites! https://t.co/conB30fjcN"", ""@johnnypgreco @CFHTelescope Here's some examples of things we reject at being unlikely to be dwarfs at the distance of our hosts. These are probably large galaxies that are far far away -- they hold their own mysteries and fascinations, but not the ones we are working on right now. https://t.co/9Lblyg3veX"", ""@johnnypgreco @CFHTelescope This is just the start of this project! We wanted the catalog paper out as quickly as possible so that the community of like-minded investigators could do follow-up! \nMeanwhile ... we're gonna estimate some distances. #BacktoWork"", '@johnnypgreco @CFHTelescope Most of this paper talks about completeness -- how well we are able to detect objects with different properties like size and total brightness ... because that will be important later ... in Paper III. üòâ', ""@johnnypgreco @CFHTelescope Matched to the same completeness limits, our objects tend to be redder than the galaxies in SAGA. We don't know what this means yet, because we don't know if this is an effect of background contamination. https://t.co/INW1XvvCnj"", '@johnnypgreco @CFHTelescope If you assume all 153 detections are at the distance of their host (which they are not!), then here\'s what their parameters look like. We generally have excellent completeness for the ""Classical Satellites"". https://t.co/6p3Cy0LAM6', '@johnnypgreco @CFHTelescope The fraction of dwarf Ellipticals and the nucleation fraction as a function of the mass of the host (assuming Vcirc is a reasonable proxy) is also interesting -- again, this is all preliminary until we measure distances and determine which of these candidates are satellites! https://t.co/3JH7fuy4X6', '@johnnypgreco @CFHTelescope @mgeha And @eteq !', '@MBKplus @johnnypgreco @CFHTelescope @mgeha @eteq !! I was gonna say I gotta get out my cauldron of dEs !!', '@yaoyuanmao @johnnypgreco @CFHTelescope :-) we are excited to collaborate!! We have to clean our sample of background, but scott is well on his way to that!', '@yaoyuanmao @johnnypgreco @CFHTelescope And Jenny just mentioned having you down!']",https://arxiv.org/abs/1909.07389,"We present the results of an extensive search for dwarf satellite galaxies around 10 primary host galaxies in the Local Volume (D$<$12 Mpc) using archival CFHT/MegaCam imaging data. The hosts span a wide range in properties, with stellar masses ranging from that of the LMC to ${\sim}3$ times that of the Milky Way (MW). The surveyed hosts are: NGC 1023, NGC 1156, NGC 2903, NGC 4258, NGC 4565, NGC 4631, NGC 5023, M51, M64, and M104. We detect satellite candidates using a consistent semi-automated detection algorithm that is optimized for the detection of low surface brightness objects. Depending on the host, our completeness limit is $M_g{\sim}-8$ to $-10$ (assuming the distance of the host). We detect objects with surface brightness down to $\mu_{0,g}{\sim}26$ mag arcsec$^{-2}$ at $\gtrsim90\%$ completeness. The survey areas of the six best-surveyed hosts cover most of the inner projected $R<150$ kpc area, which roughly doubles the number of MW-mass hosts surveyed at this level of area and luminosity completeness. The number of detected candidates range from 1 around M64 to 33 around NGC 4258. In total, 153 candidates are found, of which 93 are new. While we defer an analysis of the satellite luminosity functions of the hosts until distance information is available for the candidates, we do show that the candidates are primarily red, spheroid systems with properties roughly consistent with known satellites in the Local Group. ","Wide-Field Survey of Dwarf Satellite Systems Around 10 Hosts in the
  Local Volume"
142,1173977511069462529,3433220662,Anthony Bonato,"['Now up on arXiv: ""Centrality in dynamic competition networks,"" joint work with @NicoleEikmeier @dgleich and Rehan Malik. We study a new centrality score as a predictor of importance in competition networks such as food webs. \n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.06810,"Competition networks are formed via adversarial interactions between actors. The Dynamic Competition Hypothesis predicts that influential actors in competition networks should have a large number of common out-neighbors with many other nodes. We empirically study this idea as a centrality score and find the measure predictive of importance in several real-world networks including food webs, conflict networks, and voting data from Survivor. ",Centrality in dynamic competition networks
143,1173951756646125574,1132257519148126209,electron dynamiX group (Julia St√§hler),"['as cold as ice\nIn a collaborative study, using STM, tr-2PPE, and ab initio theory, we showed that energy dissipation during solvation leads to permanent rearrangements of molecules in frozen water, involving energy-costly hydrogen bond breakage. \n\n<LINK> <LINK>']",https://arxiv.org/abs/1909.03844,"We determine the impact of electron solvation on D$_2$O structures adsorbed on Cu(111) with low temperature scanning tunneling microscopy, two-photon photoemission, and ab initio theory. UV photons generating solvated electrons lead not only to transient, but also to permanent structural changes through the rearrangement of individual molecules. The persistent changes occur near sites with a high density of dangling OH groups that facilitate electron solvation. We conclude that energy dissipation during solvation triggers permanent molecular rearrangement via vibrational excitation. ",Impact of electron solvation on ice structures at the molecular scale
144,1172474586329493504,73090248,Prof. Danushka Bollegala,['We propose a method to provably anonymise search queries by ‚Äúhiding in plain sight‚Äù (aka semantic decomposition).  Word embedding spaces can be used to find disjunctive meaning spaces for reconstructing results Joint work with T.Machide and K.Kawarabayashi <LINK>'],https://arxiv.org/abs/1909.05819,"We propose a method to protect the privacy of search engine users by decomposing the queries using semantically \emph{related} and unrelated \emph{distractor} terms. Instead of a single query, the search engine receives multiple decomposed query terms. Next, we reconstruct the search results relevant to the original query term by aggregating the search results retrieved for the decomposed query terms. We show that the word embeddings learnt using a distributed representation learning method can be used to find semantically related and distractor query terms. We derive the relationship between the \emph{obfuscity} achieved through the proposed query anonymisation method and the \emph{reconstructability} of the original search results using the decomposed queries. We analytically study the risk of discovering the search engine users' information intents under the proposed query obfuscation method, and empirically evaluate its robustness against clustering-based attacks. Our experimental results show that the proposed method can accurately reconstruct the search results for user queries, without compromising the privacy of the search engine users. ",Query Obfuscation Semantic Decomposition
145,1171900619252105216,2286503947,Changhan Wang,"['Can we build machine translation models on finer-grained vocabularies than characters? Can they be compacter and faster? Can they be generic and transferable to any languages?\n\nIn <LINK> with @kchonyc and @thoma_gu, we study byte-level BPE for these questions. <LINK>']",http://arxiv.org/abs/1909.03341,"Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice. In this paper, we investigate byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets. ",Neural Machine Translation with Byte-Level Subwords
146,1171600474224713729,70874545,Josh Lothringer,"['So we found water vapor in the atmosphere of a 9 Earth-mass exoplanet in the habitable zone! After observing 8 transits of K2-18b with HST/WFC3, we (led by Bj√∂rn Benneke) find a significant water feature:\n\n<LINK> <LINK>', ""K2-18b receives only 5% more radiation than the Earth, leaving it with an equilibrium temperature of 265 K. This is the coolest exoplanet that we've detected water in. While not a true Earth-analogue due to its size, this bodes well for our exploration of small planets. https://t.co/SrAXA5yPgz"", ""One of the most intriguing parts to me is how clear the atmosphere is! K2-18b appears cool enough that cloud species like KCl and ZnS don't form, while also being far enough away from its moderately active M-dwarf host star to not have significant haze production."", ""That's not to say there aren't *any* clouds: at pressures around 100 mbar, we find some water clouds: the first water clouds in an exoplanet! (Turns out we've found some water clouds in a chilly brown dwarf though: https://t.co/G7zPTbizWC)"", 'I should emphasize that it was mostly Bj√∂rn doing the heavy lifting on this work. I just happen to be the highest co-author with a twitter account and wanted to share this exciting result!üî≠üååüõ∏', 'Keep your eyes peeled later today for results using some of the same data by a different team...!']",https://arxiv.org/abs/1909.04642,"Results from the Kepler mission indicate that the occurrence rate of small planets ($<3$ $R_\oplus$) in the habitable zone of nearby low-mass stars may be as high as 80%. Despite this abundance, probing the conditions and atmospheric properties on any habitable-zone planet is extremely difficult and has remained elusive to date. Here, we report the detection of water vapor and the likely presence of liquid and icy water clouds in the atmosphere of the $2.6$ $R_\oplus$ habitable-zone planet K2-18b. The simultaneous detection of water vapor and clouds in the mid-atmosphere of K2-18b is particularly intriguing because K2-18b receives virtually the same amount of total insolation from its host star ($1368_{-107}^{+114}$ W m$^{-2}$) as the Earth receives from the Sun (1361 W m$^{-2}$), resulting in the right conditions for water vapor to condense and explain the detected clouds. In this study, we observed nine transits of K2-18b using HST/WFC3 in order to achieve the necessary sensitivity to detect the water vapor, and we supplement this data set with Spitzer and K2 observations to obtain a broader wavelength coverage. While the thick hydrogen-dominated envelope we detect on K2-18b means that the planet is not a true Earth analog, our observations demonstrate that low-mass habitable-zone planets with the right conditions for liquid water are accessible with state-of-the-art telescopes. ","Water Vapor and Clouds on the Habitable-Zone Sub-Neptune Exoplanet
  K2-18b"
147,1171456628148752388,232287209,Dallas Card,"['I\'m excited about this one! In ""Show Your Work"" (to appear at EMNLP) we advocate for better reporting of experimental results and propose budget-aware evaluation of models. \n<LINK> by @JesseDodge, @ssgrn, @dallascard, @royschwartz02, &amp; @nlpnoah (1/5)', '@JesseDodge @ssgrn @royschwartz02 @nlpnoah In particular, from a practical perspective, imagine you want to choose an existing method to use on a new dataset. Which should you try? Whatever is state-of-the-art might have great potential, but might also require a lot of hyperparameter tuning. (2/5)', '@JesseDodge @ssgrn @royschwartz02 @nlpnoah As such, what you really care about is the expected performance of a method for a given computational budget. Depending on the resources you have available, you might be better off trying a method which is more likely to give a good result with less effort. (3/5)', '@JesseDodge @ssgrn @royschwartz02 @nlpnoah Most published results are based on a large amount of experimentation that often goes unreported. \nIn this paper, we show how validation results from hyperparameter tuning can be reused to facilitate such comparisons, without requiring any additional experimental work. (4/5)', '@JesseDodge @ssgrn @royschwartz02 @nlpnoah We also demonstrate a number of cases where the answer to ""which model performs best"" is a function of the computational budget, provide a checklist for future work, and attempt to estimate the amount of effort that went into obtaining previously published results. (5/5)', ""@brendan642 @JesseDodge @ssgrn @royschwartz02 @nlpnoah Yes, that's absolutely true! A different hyperparameter search space could simultaneously lead to both a higher max performance but lower expected performance, for example."", '@brendan642 @JesseDodge @ssgrn @royschwartz02 @nlpnoah For the purpose of comparisons, I think it actually makes sense to consider a model to be somewhat underspecified without an accompanying hyperparameter search space.', '@brendan642 @JesseDodge @ssgrn @royschwartz02 @nlpnoah I think the most systematic thing would be for model developers to define a hyperparam search space (e.g. bounds or a grid) and procedure (e.g. random search), and use it consistently for evaluation on multiple datasets. This would also encourage focus on the most important HPs.']",https://arxiv.org/abs/1909.03004,"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique. ",Show Your Work: Improved Reporting of Experimental Results
148,1171449086743990273,1033928440645382149,Ben Zhou,"[""Check out our new #emnlp2019 paper where we studied temporal commonsense: <LINK>. We collected a QA dataset MC-TACOüåÆ(leaderboard coming soon) and showed that it's a new challenge to existing systems. Co-author with @DanielKhashabi, Qiang Ning and Dan Roth.""]",https://arxiv.org/abs/1909.03065,"Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic. ","""Going on a vacation"" takes longer than ""Going for a walk"": A Study of
  Temporal Commonsense Understanding"
149,1171407879791894528,373332154,Vivien Parmentier,"['Exoplanets are 3D and we do get 3D information by looking at the change of the planet brightness with orbital phase. Today in <LINK> we find a new way to convert these observations into a 3D temperature map.  ? 1/n \n<LINK> (anim by @TomLouden_b )', 'For simplicity people have often assumed that at each phase the visible hemisphere is homogeneous and that at each phase the mean hemispheric vertical temperature profile is not related to the one at the previous phase.  That would lead to a planet looking like this: https://t.co/hmCgbnA9CY', 'However when considering the temperature should change smoothly with longitude, we obtain a much more realistic map. https://t.co/4jfY8OhcOk', 'But that was known, the novelty here is to retrieve not only a horizontal map but also use the spectral information to *jointly* retrieve the vertical structure at all longitudes. We tested this ""2.5D"" retrieval with fake HST/Spitzer data from a WASP-43b GCM... see by yourself ! https://t.co/tOOxeDmEax', 'On real data the different approaches lead to a very different temperature map. The 1D approach *overestimates* the nightside temperature of WASP-43b significantly because we never see *only* the nightside but always are contaminated by a small but much hotter dayside part. https://t.co/odAPw69eJx', 'But as always, science is harder than that and several assumptions on the smoothness of the temperature map can lead to different retrieved temperature maps. As usual, we are all waiting for the same big telescope to make our planets shine with flying colors. https://t.co/ysWPfcMjUL', 'For more details see the paper https://t.co/Qpnc5x6r1y by a Oxford + UCL team: Pat Irwin, myself, @AstroJake, @GleeAstro, @DrJoVian, @AirborneGrain  and R. Garland.  And more info and nice gifs shared by Pat irwin on his website https://t.co/I8bMNp6KRa n/n', '@AstroJake @GleeAstro @DrJoVian @AirborneGrain @OxfordAOPP @OxfordPhysics @UniofOxford', '@AstroJake @GleeAstro @DrJoVian @AirborneGrain @PatrickIrwin1 &amp; @ryanrgarland, all the team is on tweeter then !']",http://arxiv.org/abs/1909.03233,"We present a novel retrieval technique that attempts to model phase curve observations of exoplanets more realistically and reliably, which we call the 2.5-dimension (2.5-D) approach. In our 2.5-D approach we retrieve the vertical temperature profile and mean gaseous abundance of a planet at all longitudes and latitudes \textbf{simultaneously}, assuming that the temperature or composition, $x$, at a particular longitude and latitude $(\Lambda,\Phi)$ is given by $x(\Lambda,\Phi) = \bar{x} + (x(\Lambda,0) - \bar{x})\cos^n\Phi$, where $\bar{x}$ is the mean of the morning and evening terminator values of $x(\Lambda,0)$, and $n$ is an assumed coefficient. We compare our new 2.5-D scheme with the more traditional 1-D approach, which assumes the same temperature profile and gaseous abundances at all points on the visible disc of a planet for each individual phase observation, using a set of synthetic phase curves generated from a GCM-based simulation. We find that our 2.5-D model fits these data more realistically than the 1-D approach, confining the hotter regions of the planet more closely to the dayside. We then apply both models to WASP-43b phase curve observations of HST/WFC3 and Spitzer/IRAC. We find that the dayside of WASP-43b is apparently much hotter than the nightside and show that this could be explained by the presence of a thick cloud on the nightside with a cloud top at pressure $< 0.2$ bar. We further show that while the mole fraction of water vapour is reasonably well constrained to $(1-10)\times10^{-4}$, the abundance of CO is very difficult to constrain with these data since it is degenerate with temperature and prone to possible systematic radiometric differences between the HST/WFC3 and Spitzer/IRAC observations. Hence, it is difficult to reliably constrain C/O. ","2.5-D retrieval of atmospheric properties from exoplanet phase curves:
  Application to WASP-43b observations"
150,1171384117847580672,989603062112284672,Paul M. Zeiger,['Uploaded my first first author #preprint to #ArXiv yesterday. We introduce a #versatile and #efficient method to simulate vibrational #EELS experiments. Find out more under:\n\n<LINK>'],https://arxiv.org/abs/1909.03982,We introduce a novel method for the simulation of the impact scattering in vibrational scanning transmission electron microscopy electron energy loss spectroscopy (STEM-EELS) simulations. The phonon-loss process is modeled by a combination of molecular dynamics and elastic multislice calculations within a modified frozen phonon approximation. The key idea is thereby to use a so-called $\delta$-thermostat in the classical molecular dynamics simulation to generate frequency dependent configurations of the vibrating specimen's atomic structure. The method includes correlated motion of atoms and provides vibrational spectrum images at the cost comparable to standard frozen phonon calculations. We demonstrate good agreement of our method with simulations and experiments for a 15nm flake of hexagonal boron-nitride (hBN). ,Efficient and versatile model for vibrational STEM-EELS
151,1171358518491975681,187655958,Fragkiskos Malliaros,"['Our work on ""Kernel Node Embeddings"" w\' A. Celikkanat has been accepted to #IEEEGlobalSIP! We propose KernelNE, a model that learns node representations based on random walks and kernelized matrix factorization.  #MachineLearning #NetworkScience\n\nPaper: <LINK>', '@metoxos_ Thank you Giorgo! :)']",https://arxiv.org/abs/1909.03416,"Learning representations of nodes in a low dimensional space is a crucial task with many interesting applications in network analysis, including link prediction and node classification. Two popular approaches for this problem include matrix factorization and random walk-based models. In this paper, we aim to bring together the best of both worlds, towards learning latent node representations. In particular, we propose a weighted matrix factorization model which encodes random walk-based information about the nodes of the graph. The main benefit of this formulation is that it allows to utilize kernel functions on the computation of the embeddings. We perform an empirical evaluation on real-world networks, showing that the proposed model outperforms baseline node embedding algorithms in two downstream machine learning tasks. ",Kernel Node Embeddings
152,1169775264034779136,35031140,Bas Hofstra,"['Hello #metascience2019 &amp; other friends, we just posted our new paper on arXiv: <LINK>. Title basically covers content: ""Diversity Breeds Innovation With Discounted Impact and Recognition."" Studied through records and texts from nearly all US PhDs across 30years.', 'Elaborates and extends prior work on innovation through text analyses and asks where the diversity-paradox in science (diversity breeds innovation, whereas diverse folks have less science careers) comes from.', 'With my great colleagues Sebastian Munoz-Najar Galvez, Bryan He, @viveksc, and Daniel A. McFarland', ""@viveksc I'm very glad to tentatively share this piece, it integrates many things/processes that are/were rewarding: an important question and answer, integration of multiple data sets, novel metrics and computation, interdisciplinary set of authors.""]",https://arxiv.org/abs/1909.02063,"Prior work finds a diversity paradox: diversity breeds innovation, and yet, underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-population of ~1.2 million US doctoral recipients from 1977-2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: e.g., novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity's role in innovation and partly explains the underrepresentation of some groups in academia. ",The Diversity-Innovation Paradox in Science
153,1169549838054301696,1141772501472727040,Lena Voita,"['Our @emnlp2019 paper on context-aware NMT is out! After our ACL paper, we come further in using less of document-level data and propose an approach for using only monolingual document-level data.\n<LINK>\n@lena_voita, @RicoSennrich, @iatitov <LINK>']",https://arxiv.org/abs/1909.01383,"Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only. ",Context-Aware Monolingual Repair for Neural Machine Translation
154,1169535676171378688,389221031,Pavel Mancera Pi√±a,"[""Paper day!! My first PhD paper is out! <LINK>\nWe study the gas kinematicsüì° of HI-rich ultra-diffuse galaxies (UDGs). I'm really enjoying doing this: I have an amazing PhD supervisor, great co-authors, and cool and interesting results! I open thread! [1/N]"", 'The paper is actually a Letter, accepted for publication in ApJL, here a picture of title and the list of my co-authors. What did we do? As you know UDGs are puzzling, large, LSB galaxies. Lots of work has done in the optical, but little is known about HI kinematics... [2/N] https://t.co/SiH5YBPtkO', ""Yet, HI-kinematics is crucial to understand galaxy formation &amp; evolution (hey, we are from Groningenüá≥üá±), so we really wanted to investigate the kinematics of HI-rich UDGs. We got interferometric and deep optical data of six ISOLATED HI-rich UDGs from Leisman+17' catalogue [3/N]"", 'Our six galaxies have log(MHI/Msun)~9, are gas dominated (mean MHI/Mstar = 15), isolated (mean distance to nearest neighbor ~1 Mpc) and distant (mean distance ~90 Mpc). This all ensures that the baryonic mass (mainly given by the HI) can be measured with good accuracy [4/N]', 'Here an example of our data: HI contours on top of stellar image, PV diagram and velocity field (with major axis and beam). These are low spatial resolution data, strongly affected by beam smearing, e.g. you tend to see solid-body rotation where there is none [5/N] https://t.co/71VO7uS8tm', 'To deal with this we use a 3D approach, using the software 3DBarolo (3DB, Di Teodoro &amp; Fraternali 15), that gives circular speeds unaffected by beam smearing. Inclinations are determined independently, from HI maps, with an error of +-5 deg based on tests using simulations [6/N]', '3DB finds good and realistic models. How good? ""3DB puts the points exactly where my eyes would put them"" Renzo Sancisiüòéüòú. In the Fig. you see major-axis PV diagrams: black and red contours show data and model, respectively; yellow points show the rotation velocities. [7/N] https://t.co/hiCIGDSie5', 'Very important: PVs are very extended, Rout~8-18 kpc depending on the UDG, and consistent with having reached the flat part of the rotation curve. The circular speeds cannot increase significantly after such large radii. Also, the PVs are very narrow -&gt; low vel. dispersion [8/N]', ""So we have Vcirc and Mbar... let's look at the baryonic Tully-Fisher relation (BTFR)! We expect UDGs to follow the relation, as many galaxies of different types do, and remarkably tightly btw. No one expects surprises, right? Well, we have some puzzling news for you... [9/N]"", 'Our HI-rich UDGs do NOT follow the BTFR! They rotate too slowly for their baryonic masses (or are too massive for their circular speeds). But wait -we said at first- something should be wrong. Two options: are we overestimating Mbar or underestimating Vcirc? [10/N] https://t.co/sO3R57lpp9', 'We check for systematics in Mbar and Virc, meaning: in flux, mass, distance, rotation curves and inclinations. We find impossible to reconcile our new observations with the BTFR: Mbar and Vcirc are rather robust. It looks like HI-rich UDGs are real outliers of the BTFR!! [11/N]', 'There is something else: our UDGs lie near the curve where galaxies with a baryon fraction equal to the comic mean are expected to lie! This means that their position w.r.t. the BTFR imply they have the cosmic baryon fractions: seems like they have no ""missing baryons""! [12/N] https://t.co/aZlbAuwhA8', 'But they dwarfs, with shallow potential wells, and mostly made by gas. How do they retain all of their baryons!? We have an idea: maybe feedback processes have been rather weak, and they managed to retain their gas, or promptly re-accrete it. This may be related with the...[13/N]', 'low gas velocity dispersions we find, that suggest a currently weak heating of the gas. If this is the case, seems like there is something we do not understand yet about feedback implementations in the LCDM framework for these UDGs. Would be nice to test with simulations! [14/N]', 'Finally: these HI-rich UDGs have no ""missing baryons"", and still they rotate to slow for their Mbar... what is going on with the dark matter? We checked this by comparing Mbar with Mdyn, the later estimated from the rotation curve, so we measure only INSIDE the HI-disc [15/N]', 'In contrast w/ typical dwarfs/LSB galaxies (although they seem to have some companions in our figure), dark-matter dominated at all radii, we find that our UDGs have dark matter fractions smaller than 50%, and consistent with no/very little dark matter inside their discs!! [16/N] https://t.co/YC4swSGjFj', 'And that\'s it basically! Maybe I got too excited, I hope it is not too longüòÖ. We find our HI-rich UDGs to be outliers of the baryonic Tully-Fisher relation, and their positions suggest that they have no ""missing baryons"" and little room for dark matter inside their discs! [17/N]', 'In a work in preparation we will show in detail the kinematic models, and will discuss more about what do our results imply for proposed formation models of UDGs, keep tuned! Sorry I did not include references in the thread, but you can find them in the paper of course! [18/N]', 'Btw, I had very interesting conversations with many people, specially @TusyaPonomareva, @lellifede, A. di Cintio and J. Rom√°n, thank you guysüëèüèΩ! And if you, anonymous referee, are out there, thank you very much to you too!! [19/N]', 'So please read the paper, and if you have questions/comments let us know! I am flying in a few minutes to a conference in China, but I hope to read emails soonüòÖ [20/20]', '@DanieliShany Thank you very very much, Shany!! I also look forward to discussing it with youüòå', ""@MichaelTremmel Hi, Michael! I'm very glad you liked the thread and I that I managed to transmit some of my enthusiasmüòÖ. \nAbout your question, we don't estimate directly fbar, but we will in the forthcoming paper and indeed assuming different profiles. In the case of this paper, our..."", '@MichaelTremmel Our statement comes from the comparison with the dashed line in Fig. 3, which is the expectation for galaxies with fbar=fbar,cosmic. You can see the motivation behind this McGaugh 2012 (see picture). Many thanks for the interest and questions! https://t.co/8M0pvOpaEm', '@fdmtweets Ohhh, thank you very much for staying tuned:) And await for the paper from @anchwr too!']",http://arxiv.org/abs/1909.01363,"We study the gas kinematics traced by the 21-cm emission of a sample of six HI$-$rich low surface brightness galaxies classified as ultra-diffuse galaxies (UDGs). Using the 3D kinematic modelling code $\mathrm{^{3D}}$Barolo we derive robust circular velocities, revealing a startling feature: HI$-$rich UDGs are clear outliers from the baryonic Tully-Fisher relation, with circular velocities much lower than galaxies with similar baryonic mass. Notably, the baryon fraction of our UDG sample is consistent with the cosmological value: these UDGs are compatible with having no ""missing baryons"" within their virial radii. Moreover, the gravitational potential provided by the baryons is sufficient to account for the amplitude of the rotation curve out to the outermost measured point, contrary to other galaxies with similar circular velocities. We speculate that any formation scenario for these objects will require very inefficient feedback and a broad diversity in their inner dark matter content. ","Off the baryonic Tully-Fisher relation: a population of baryon-dominated
  ultra-diffuse galaxies"
155,1169179555476725761,859294200,Eric Malmi,"['seq2seq has become the de facto approach for text editing tasks. We propose LaserTagger which casts editing into a tagging task, making it more data efficient, faster and less prone to hallucination than seq2seq and yields SOTA results\n<LINK> #emnlp2019 @GoogleAI <LINK>', '@GoogleAI As a sidenote: I thought that author rebuttals don\'t make a difference, so a big shout-out to the reviewer who wrote ""thanks for clarifying my confusions"" and gave us a slightly positive score after an initial negative review!']",https://arxiv.org/abs/1909.01187,"We propose LaserTagger - a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment. ","Encode, Tag, Realize: High-Precision Text Editing"
156,1169130873901453312,3716338821,Mikko Tuomi,"['Together with a bunch of colleagues, we studied the system of planets around GJ 357 that I originally discovered.\n\nOur dynamical investigation reveals probable co-planarity of the system, implying that transit of GJ 357 d might be detectable. <LINK> <LINK>', 'We know that GJ 357 d, the outermost planet in the system, is located within the stellar habitable zone.\n\nIt would thus be interesting to observe its transit, giving information regarding its composition and, possibly, its atmosphere. https://t.co/czr4k2mSZW', 'The planet GJ 357 d is listed in the Habitable Exoplanets cataloque maintained by @ProfAbelMendez. https://t.co/g0TCH7aidT', '@ProfAbelMendez Although it has a mass of some 7 times that of the Earth, GJ 357 d might be a habitable planet given that it does not have a thick gaseous envelope.\n\nCertainly, its distance from the star is suitable for the existence of liquid water on its surface. https://t.co/3dHBSAITa4']",https://arxiv.org/abs/1909.00831,"We report the detection of a new planetary system orbiting the nearby M2.5V star GJ357, using precision radial-velocities from three separate echelle spectrographs, HARPS, HiRES, and UVES. Three small planets have been confirmed in the system, with periods of 9.125+/-0.001, 3.9306+/-0.0003, and 55.70+/-0.05 days, and minimum masses of 3.33+/-0.48, 2.09+/-0.32, and 6.72+/-0.94 Me, respectively. The second planet in our system, GJ357c, was recently shown to transit by the Transiting Exoplanet Survey Satellite (TESS; Luque et al. 2019), but we could find no transit signatures for the other two planets. Dynamical analysis reveals the system is likely to be close to coplanar, is stable on Myrs timescales, and places strong upper limits on the masses of the two non-transiting planets b and d of 4.25 and 11.20 Me, respectively. Therefore, we confirm the system contains at least two super-Earths, and either a third super-Earth or mini-Neptune planet. GJ357b & c are found to be close to a 7:3 mean motion resonance, however no libration of the orbital parameters was found in our simulations. Analysis of the photometric lightcurve of the star from the TESS, when combined with our radial-velocities, reveal GJ357c has an absolute mass, radius, and density of 2.248+0.117-0.120 Me, 1.167+0.037-0.036 Re, and 7.757+0.889-0.789 g/cm3, respectively. Comparison to super-Earth structure models reveals the planet is likely an iron dominated world. The GJ357 system adds to the small sample of low-mass planetary systems with well constrained masses, and further observational and dynamical follow-up is warranted to better understand the overall population of small multi-planet systems in the solar neighbourhood. ","GJ357: A low-mass planetary system uncovered by precision
  radial-velocities and dynamical simulations"
157,1178978980847919105,797433864,Danilo J. Rezende,"['We had a ""Hamiltonian extravaganza"" at @DeepMind. We show how to learn Hamiltonian gen models from pixels <LINK> and propose a general method for combining symmetry Lie groups with ODE-Flow generative models in <LINK> #physics #ML #symmetries <LINK> <LINK>', '@Deepmind Great working with @irinavlh,  S√©bastien Racani√®re, Peter Toth, @drew_jaegle and many others!  Thanks all for the hard work!', ""@timudk @Deepmind indeed vol-preservation is for the joint (q,p) space. This is less restrictive if we marginalize p to define a density on q. We can also treat the p at each step as a latent var. (instead of only the last one) In these cases, it doesn't look like there are obvious constraints.""]",https://arxiv.org/abs/1909.13789,"The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning. ",Hamiltonian Generative Networks
158,1177767274154872832,65876824,Jascha Sohl-Dickstein,"['Neural reparameterization improves structural optimization! By parameterizing physical design in terms of the (constrained) output of a neural network, we propose stronger and more elegant bridges, skyscrapers, and cantilevers. <LINK> With shoyer@ samgreydanus@ <LINK>']",https://arxiv.org/abs/1909.04240,"Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produces the best design 50% more often than the best baseline method. ",Neural reparameterization improves structural optimization
159,1176894109589467136,4850927243,Grusha Prasad,"[""The preprint is up on arxiv! <LINK>\n\nBrief summary: We propose a new paradigm to probe how syntactic representations in neural LMs are organized. We find that LSTM LMs' representations of relative clauses are organized in a linguistically interpretable manner. <LINK> <LINK>"", 'We also created templates with over 200 verbs, 150 nouns, 20 adverbs and 70 adjectives that can generate semantically plausible sentences: https://t.co/D184UvacgX']",https://arxiv.org/abs/1909.10579,"Neural language models (LMs) perform well on tasks that require sensitivity to syntactic structure. Drawing on the syntactic priming paradigm from psycholinguistics, we propose a novel technique to analyze the representations that enable such success. By establishing a gradient similarity metric between structures, this technique allows us to reconstruct the organization of the LMs' syntactic representational space. We use this technique to demonstrate that LSTM LMs' representations of different types of sentences with relative clauses are organized hierarchically in a linguistically interpretable manner, suggesting that the LMs track abstract properties of the sentence. ","Using Priming to Uncover the Organization of Syntactic Representations
  in Neural Language Models"
160,1173569474089410560,382302464,Goran Glava≈°,"['Our attempt to systematize the heterogeneous body of work on debiasing (static) word embeddings: we formalize bias specifications, measure different bias aspects (new measures!) and propose new general-purpose (mutually composable) debiasing algorithms: \n<LINK>', 'Joint work with @anne_lauscher @licwu @spponzius']",https://arxiv.org/abs/1909.06092,"Distributional word vectors have recently been shown to encode many of the human biases, most notably gender and racial biases, and models for attenuating such biases have consequently been proposed. However, existing models and studies (1) operate on under-specified and mutually differing bias definitions, (2) are tailored for a particular bias (e.g., gender bias) and (3) have been evaluated inconsistently and non-rigorously. In this work, we introduce a general framework for debiasing word embeddings. We operationalize the definition of a bias by discerning two types of bias specification: explicit and implicit. We then propose three debiasing models that operate on explicit or implicit bias specifications and that can be composed towards more robust debiasing. Finally, we devise a full-fledged evaluation framework in which we couple existing bias metrics with newly proposed ones. Experimental findings across three embedding methods suggest that the proposed debiasing models are robust and widely applicable: they often completely remove the bias both implicitly and explicitly without degradation of semantic information encoded in any of the input distributional spaces. Moreover, we successfully transfer debiasing models, by means of cross-lingual embedding spaces, and remove or attenuate biases in distributional word vector spaces of languages that lack readily available bias specifications. ","A General Framework for Implicit and Explicit Debiasing of
  Distributional Word Vector Spaces"
161,1172580007547301888,2835427044,Sneha Kudugunta,"['New EMNLP paper ‚ÄúInvestigating Multilingual NMT Representation at Scale‚Äù w/ @ankurbpn, @orf_bnw, @caswell_isaac, @naveenariva. We study transfer in massively multilingual NMT @GoogleAI from the perspective of representational similarity.\n\nPaper: <LINK> 1/n <LINK>', '@ankurbpn @orf_bnw @caswell_isaac @naveenariva @GoogleAI We discuss the challenges of comparing misaligned sequences, and use a variant of SVCCA.  2/n', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We find that encoder representations of different languages cluster according to linguistic similarity... 3/n https://t.co/lHFtSIdilm', '@ankurbpn @orf_bnw @naveenariva @GoogleAI ‚Ä¶ Even at a more fine-grained level. 4/n https://t.co/iiwfV8MaoO', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We look at how our similarity measure capturing linguistic similarity vs script. 5/n https://t.co/V5qqInb1CH', '@ankurbpn @orf_bnw @naveenariva @GoogleAI We also find that representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. 6/n https://t.co/IX4HYmYOSd', '@ankurbpn @orf_bnw @naveenariva @GoogleAI Colab to play with coming soon! 7/n', '@ankurbpn @orf_bnw @naveenariva @GoogleAI Huge thanks to my collaborators at @GoogleAI, without whom this work would not have been possible. This work was done as a part of the Google AI Residency - applications open soon, so definitely check it out!\n\nhttps://t.co/gFZs0tnWTA 8/8']",https://arxiv.org/abs/1909.02197,"Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major conclusions from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning. ",Investigating Multilingual NMT Representations at Scale
162,1171555903952150529,503452360,William Wang,"['In our new #EMNLP2019 paper, we relax the fully-factored mean-field assumption, and propose a new Gaussian Copula Variational Autoencoder (VAE) to deal with posterior collapse. Neural Gaussian Copula for Variational Autoencoder: <LINK>  #NLProc @kingofspace0wzz']",https://arxiv.org/abs/1909.03569,"Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches. ",Neural Gaussian Copula for Variational Autoencoder
