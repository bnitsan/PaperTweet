,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,959125984774623232,16434310,chrislintott,"['Great new paper out, led by Gareth Martin - bulgeless galaxies have normal black holes in simulations too!  <LINK>', '@KnudJahnke Read on. You‚Äôll like the last two figures']",https://arxiv.org/abs/1801.09699,"Understanding the processes that drive the formation of black holes (BHs) is a key topic in observational cosmology. While the observed $M_{\mathrm{BH}}$--$M_{\mathrm{Bulge}}$ correlation in bulge-dominated galaxies is thought to be produced by major mergers, the existence of a $M_{\mathrm{BH}}$--$M_{\star}$ relation, across all galaxy morphological types, suggests that BHs may be largely built by secular processes. Recent evidence that bulge-less galaxies, which are unlikely to have had significant mergers, are offset from the $M_{\mathrm{BH}}$--$M_{\mathrm{Bulge}}$ relation, but lie on the $M_{\mathrm{BH}}$--$M_{\star}$ relation, has strengthened this hypothesis. Nevertheless, the small size and heterogeneity of current datasets, coupled with the difficulty in measuring precise BH masses, makes it challenging to address this issue using empirical studies alone. Here, we use Horizon-AGN, a cosmological hydrodynamical simulation to probe the role of mergers in BH growth over cosmic time. We show that (1) as suggested by observations, simulated bulge-less galaxies lie offset from the main $M_{\mathrm{BH}}$--$M_{\mathrm{Bulge}}$ relation, but on the $M_{\mathrm{BH}}$--$M_{\star}$ relation, (2) the positions of galaxies on the $M_{\mathrm{BH}}$--$M_{\star}$ relation are not affected by their merger histories and (3) only $\sim$35 per cent of the BH mass in today's massive galaxies is directly attributable to merging -- the majority ($\sim$65 per cent) of BH growth, therefore, takes place gradually, via secular processes, over cosmic time. ","Normal black holes in bulge-less galaxies: the largely quiescent,
  merger-free growth of black holes over cosmic time"
1,958877125297098752,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper on the enhanced axion-photon coupling appeared on arxiv today. Such enhancement is motivated by GUT with hidden photon, and it is advantageous for many axion-search experiments! \n<LINK>']",https://arxiv.org/abs/1801.10344,"We show that the axion coupling to photons can be enhanced in simple models with a single Peccei-Quinn field, if the gauge coupling unification is realized by a large kinetic mixing $\chi = {\cal O}(0.1)$ between hypercharge and unbroken hidden U(1)$_H$. The key observation is that the U(1)$_H$ gauge coupling should be rather strong to induce such large kinetic mixing, leading to enhanced contributions of hidden matter fields to the electromagnetic anomaly. We find that the axion-photon coupling is enhanced by about a factor of 10-100 with respect to the GUT-axion models with $E/N = 8/3$. ",Enhanced axion-photon coupling in GUT with hidden photon
2,958747382635642881,2654165034,Jason M Pittman,['First paper out of my new #reseachinstitute <LINK> #agi #containment #ontology #arxiv'],https://arxiv.org/abs/1801.09317,"The development of artificial general intelligence is considered by many to be inevitable. What such intelligence does after becoming aware is not so certain. To that end, research suggests that the likelihood of artificial general intelligence becoming hostile to humans is significant enough to warrant inquiry into methods to limit such potential. Thus, containment of artificial general intelligence is a timely and meaningful research topic. While there is limited research exploring possible containment strategies, such work is bounded by the underlying field the strategies draw upon. Accordingly, we set out to construct an ontology to describe necessary elements in any future containment technology. Using existing academic literature, we developed a single domain ontology containing five levels, 32 codes, and 32 associated descriptors. Further, we constructed ontology diagrams to demonstrate intended relationships. We then identified humans, AGI, and the cyber world as novel agent objects necessary for future containment activities. Collectively, the work addresses three critical gaps: (a) identifying and arranging fundamental constructs; (b) situating AGI containment within cyber science; and (c) developing scientific rigor within the field. ","A Cyber Science Based Ontology for Artificial General Intelligence
  Containment"
3,958693080894459904,471996981,Jonathan Bright üá∫üá¶,['Understanding news story chains using\ninformation retrieval and network clustering\ntechniques - new draft paper out with @pmyteh <LINK> <LINK>'],https://arxiv.org/abs/1801.07988,"Content analysis of news stories (whether manual or automatic) is a cornerstone of the communication studies field. However, much research is conducted at the level of individual news articles, despite the fact that news events (especially significant ones) are frequently presented as ""stories"" by news outlets: chains of connected articles covering the same event from different angles. These stories are theoretically highly important in terms of increasing public recall of news items and enhancing the agenda-setting power of the press. Yet thus far, the field has lacked an efficient method for detecting groups of articles which form stories in a way that enables their analysis. In this work, we present a novel, automated method for identifying linked news stories from within a corpus of articles. This method makes use of techniques drawn from the field of information retrieval to identify textual closeness of pairs of articles, and then clustering techniques taken from the field of network analysis to group these articles into stories. We demonstrate the application of the method to a corpus of 61,864 articles, and show how it can efficiently identify valid story clusters within the corpus. We use the results to make observations about the prevalence and dynamics of stories within the UK news media, showing that more than 50% of news production takes place within stories. ","Understanding news story chains using information retrieval and network
  clustering techniques"
4,958671311475179520,40639812,Colin Cotter,['New paper on 2D Euler equation with stochastic Lie transport parameterisation of unresolved dynamics. <LINK>'],https://arxiv.org/abs/1801.09729,"We present a numerical investigation of stochastic transport in ideal fluids. According to Holm (Proc Roy Soc, 2015) and Cotter et al. (2017), the principles of transformation theory and multi-time homogenisation, respectively, imply a physically meaningful, data-driven approach for decomposing the fluid transport velocity into its drift and stochastic parts, for a certain class of fluid flows. In the current paper, we develop new methodology to implement this velocity decomposition and then numerically integrate the resulting stochastic partial differential equation using a finite element discretisation for incompressible 2D Euler fluid flows. The new methodology tested here is found to be suitable for coarse graining in this case. Specifically, we perform uncertainty quantification tests of the velocity decomposition of Cotter et al. (2017), by comparing ensembles of coarse-grid realisations of solutions of the resulting stochastic partial differential equation with the ""true solutions"" of the deterministic fluid partial differential equation, computed on a refined grid. The time discretization used for approximating the solution of the stochastic partial differential equation is shown to be consistent. We include comprehensive numerical tests that confirm the non-Gaussianity of the stream function, velocity and vorticity fields in the case of incompressible 2D Euler fluid flows. ",Numerically Modelling Stochastic Lie Transport in Fluid Dynamics
5,958518209144999936,171674815,Mark Marley,"['New paper (Zhou+) from Apai group on cloud modulation in L/T dwarf, HN Peg B: <LINK>']",https://arxiv.org/abs/1801.09757,"Time-resolved observations of brown dwarfs' rotational modulations provide powerful insights into the properties of condensate clouds in ultra-cool atmospheres. Multi-wavelength light curves reveal cloud vertical structures, condensate particle sizes, and cloud morphology, which directly constrain condensate cloud and atmospheric circulation models. We report results from Hubble Space Telescope/Wide Field Camera 3 near-infrared G141 taken in six consecutive orbits observations of HN Peg B, an L/T transition brown dwarf companion to a G0V type star. The best-fit sine wave to the $1.1-1.7\mu$m broadband light curve has the amplitude of $1.206\pm0.025\%$ and period of $15.4\pm0.5$ hr. The modulation amplitude has no detectable wavelength dependence except in the 1.4 $\mu$m water absorption band, indicating that the characteristic condensate particle sizes are large ($>1\mu$m). We detect significantly ($4.4\sigma$) lower modulation amplitude in the 1.4$\mu$m water absorption band, and find that HN Peg B's spectral modulation resembles those of early T type brown dwarfs. We also describe a new empirical interpolation method to remove spectral contamination from the bright host star. This method may be applied in other high-contrast time-resolved observations with WFC3. ","Cloud Atlas: Rotational Modulations in the L/T Transition Brown Dwarf
  Companion HN Peg B"
6,958149395534446592,43560393,Maurice Herlihy,['I have a new paper on atomic cross-chain swaps: <LINK>'],https://arxiv.org/abs/1801.09515,"An atomic cross-chain swap is a distributed coordination task where multiple parties exchange assets across multiple blockchains, for example, trading bitcoin for ether. An atomic swap protocol guarantees (1) if all parties conform to the protocol, then all swaps take place, (2) if some coalition deviates from the protocol, then no conforming party ends up worse off, and (3) no coalition has an incentive to deviate from the protocol. A cross-chain swap is modeled as a directed graph ${\cal D}$, whose vertexes are parties and whose arcs are proposed asset transfers. For any pair $({\cal D},L)$, where ${\cal D} = (V,A)$ is a strongly-connected directed graph and $L \subset V$ a feedback vertex set for ${\cal D}$, we give an atomic cross-chain swap protocol for ${\cal D}$, using a form of hashed timelock contracts, where the vertexes in $L$ generate the hashlocked secrets. We show that no such protocol is possible if ${\cal D}$ is not strongly connected, or if ${\cal D}$ is strongly connected but $L$ is not a feedback vertex set. The protocol has time complexity $O(diam({\cal D}))$ and space complexity (bits stored on all blockchains) $O(|A|^2)$. ",Atomic Cross-Chain Swaps
7,958121152441876481,2956121356,Russ Salakhutdinov,"['New #iclr2018 paper on Active Neural Localization, a fully differentiable neural network that learns to localize efficiently using deep RL (with @dchaplot and Emilio Parisotto):\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1801.08214,"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize accurately and efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to localize accurately while minimizing the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ",Active Neural Localization
8,957899194328600577,16434310,chrislintott,"[""A new Galaxy Zoo paper out today - <LINK>. Simulated galaxies below 10^11 solar masses don't look like their counterparts in the real Universe. <LINK>"", ""@Space_Mog Yeah, in cosmological simulations (Illustris specifically). I'm not too surprised either but finding the actual mass at which things break is worthwhile, I think."", ""@AnalyticD @memcculloch @PeterMadelaine Isn't that the point of doing this sort of thing? Testing the model?"", ""@memcculloch @PeterMadelaine I've had a quick look, and the claim that there aren't any disk galaxies with M_baryon &lt; 10^9 is confusing; I don't see any such claim in a quick reading of @DudeDarkmatter's paper, which is cited. Am I missing something?"", ""@AnalyticD I'm not on the Illustris team who did the simulations. But in my opinion they have done a good job of opening up their data (https://t.co/hXbyvyDTa6). Not much point opening code when you need months of supercomputer time to run it, though."", '@DudeDarkmatter Thanks, I appreciate it (&amp; will pass comments to rest of the team). I confess part of the motivation was the number of talks which started with two images and the claim that they were indistinguishable.', '@DudeDarkmatter I would have bet beforehand though that (1) the threshold would have been at lower mass (though our Illustris colleagues seem not surprised) and (2) that it would have manifested in more detailed morphological classification.', ""@memcculloch @DudeDarkmatter @PeterMadelaine I think we should check we're talking about the same Universe first. You started by saying there were no disk galaxies &lt; 10^9 M_sol. Stacy says there are plenty (as does literature). Do we agree or disagree here?"", '@memcculloch We‚Äôre talking past each other which is no fun. Do we agree disk galaxies with M_baryon&lt;10^9 m_sun like Leo P exist?', '@memcculloch For example, the mass here https://t.co/LV8aq7LJuh contradicts your claim in the paper you linked to that no such systems exist,', '@memcculloch So this paper is wrong? https://t.co/DbeXiR30pE it seems to claim to predict a sharp cutoff', '@memcculloch Sorry. But ‚ÄòDon‚Äôt worry about the details, look at the big picture‚Äô doesn‚Äôt work here. Maybe twitter doesn‚Äôt help. Thanks for the chat']",https://arxiv.org/abs/1801.08541,"Modern cosmological simulations model the universe with increasing sophistication and at higher spatial and temporal resolutions. These enhancements permit detailed comparisons between the simulation outputs and real observational data. Recent projects such as Illustris are capable of producing simulated images that are comparable to those obtained from local surveys. This paper tests how well Illustris achieves this goal across a diverse population of galaxies using visual morphologies derived from Galaxy Zoo citizen scientists. Morphological classifications provided by volunteers for simulated galaxies are compared with similar data for a compatible sample of images drawn from the SDSS Legacy Survey. This paper investigates how simple morphological characterization by human volunteers asked to distinguish smooth from featured systems differs between simulated and real galaxy images. Differences are identified, which are likely due to the limited resolution of the simulation, but which could be revealing real differences in the dynamical evolution of populations of galaxies in the real and model universes. Specifically, for stellar masses $M_{\star}\lesssim10^{11}M_{\odot}$, a larger proportion of Illustris galaxies that exhibit disk-like morphology or visible substructure, relative to their SDSS counterparts. Toward higher masses, simulated and observed galaxies converge and exhibit similar morphology distributions. The stellar mass threshold indicated by this divergent behavior confirms recent works using parametric measures of morphology from Illustris simulated images. When $M_{\star}\gtrsim10^{11}M_{\odot}$, the Illustris dataset contains fewer galaxies that classifiers regard as unambiguously featured. These results suggest that comparison between the detailed properties of observed and simulated galaxies, even when limited to reasonably massive systems, may be misleading. ","Galaxy Zoo: Morphological classification of galaxy images from the
  Illustris simulation"
9,957817292540149763,2337598033,Geraint F. Lewis,"['Excellent new paper with @lukebarnesastro is on the arrive <LINK> <LINK>', '@lukebarnesastro Here‚Äôs a lovely figure explaining the paper! https://t.co/7hD7pCjsaH']",https://arxiv.org/abs/1801.08781,"Models of the very early universe, including inflationary models, are argued to produce varying universe domains with different values of fundamental constants and cosmic parameters. Using the cosmological hydrodynamical simulation code from the eagle collaboration, we investigate the effect of the cosmological constant on the formation of galaxies and stars. We simulate universes with values of the cosmological constant ranging from Lambda = 0 to Lambda_0 = 300, where Lambda_0 is the value of the cosmological constant in our Universe. Because the global star formation rate in our Universe peaks at t = 3.5 Gyr, before the onset of accelerating expansion, increases in Lambda of even an order of magnitude have only a small effect on the star formation history and efficiency of the universe. We use our simulations to predict the observed value of the cosmological constant, given a measure of the multiverse. Whether the cosmological constant is successfully predicted depends crucially on the measure. The impact of the cosmological constant on the formation of structure in the universe does not seem to be a sharp enough function of Lambda to explain its observed value alone. ","Galaxy Formation Efficiency and the Multiverse Explanation of the
  Cosmological Constant with EAGLE Simulations"
10,957297537374924800,14853083,Andre Luckow,"['New paper: Task-parallel Analysis Analysis of Molecular Dynamics Trajectories: <LINK> #bigdata #HPC #Spark #Dask #xsede', '@dask_dev Thanks for the feedback. We will include these in the next revision. Particularly, we will have another look on map_block.']",https://arxiv.org/abs/1801.07630,"Different parallel frameworks for implementing data analysis applications have been proposed by the HPC and Big Data communities. In this paper, we investigate three task-parallel frameworks: Spark, Dask and RADICAL-Pilot with respect to their ability to support data analytics on HPC resources and compare them with MPI. We investigate the data analysis requirements of Molecular Dynamics (MD) simulations which are significant consumers of supercomputing cycles, producing immense amounts of data. A typical large-scale MD simulation of a physical system of O(100k) atoms over {\mu}secs can produce from O(10) GB to O(1000) GBs of data. We propose and evaluate different approaches for parallelization of a representative set of MD trajectory analysis algorithms, in particular the computation of path similarity and leaflet identification. We evaluate Spark, Dask and RADICAL-Pilot with respect to their abstractions and runtime engine capabilities to support these algorithms. We provide a conceptual basis for comparing and understanding different frameworks that enable users to select the optimal system for each application. We also provide a quantitative performance analysis of the different algorithms across the three frameworks. ",Task-parallel Analysis of Molecular Dynamics Trajectories
11,956864080458997761,63441844,David Roberson,['New paper on the arXiv. This one took quite a while to get there. \n<LINK>'],https://arxiv.org/abs/1801.08243,"A vector $t$-coloring of a graph is an assignment of real vectors $p_1, \ldots, p_n$ to its vertices such that $p_i^Tp_i = t-1$ for all $i=1, \ldots, n$ and $p_i^Tp_j \le -1$ whenever $i$ and $j$ are adjacent. The vector chromatic number of $G$ is the smallest real number $t \ge 1$ for which a vector $t$-coloring of $G$ exists. For a graph $H$ and a vector $t$-coloring $p_1,\ldots,p_n$ of a graph $G$, the assignment $(i,\ell) \mapsto p_i$ is a vector $t$-coloring of the categorical product $G \times H$. It follows that the vector chromatic number of $G \times H$ is at most the minimum of the vector chromatic numbers of the factors. We prove that equality always holds, constituting a vector coloring analog of the famous Hedetniemi Conjecture from graph coloring. Furthermore, we prove a necessary and sufficient condition for when all of the optimal vector colorings of the product can be expressed in terms of the optimal vector colorings of the factors. The vector chromatic number is closely related to the well-known Lov\'{a}sz theta function, and both of these parameters admit formulations as semidefinite programs. This connection to semidefinite programming is crucial to our work and the tools and techniques we develop could likely be of interest to others in this field. ",Vector Coloring the Categorical Product of Graphs
12,956775793631219714,377049708,Eddie Lee,['Did you ever want to use maxent (maximum entropy) for your research? Too much work to do it? We made it easy in our new Python package: <LINK> with the paper <LINK>.'],http://arxiv.org/abs/1801.08216,"ConIII (pronounced CON-ee) is an open-source Python project providing a simple interface to solving the pairwise and higher order Ising model and a base for extension to other maximum entropy models. We describe the maximum entropy problem and give an overview of the algorithms that are implemented as part of ConIII (this https URL) including Monte Carlo histogram, pseudolikelihood, minimum probability flow, a regularized mean field method, and a cluster expansion method. Our goal is to make a variety of maximum entropy techniques accessible to those unfamiliar with the techniques and accelerate workflow for users. ","Convenient Interface to Inverse Ising (ConIII): A Python 3 Package for
  Solving Ising-Type Maximum Entropy Models"
13,956417691824394240,1710697381,Diego F. Torres,"['New paper: ""Simultaneous obs &amp; X-ray\n spectroscopy of the transitional MSP J1023+0038"" <LINK> <LINK>']",https://arxiv.org/abs/1801.07794,"We report on the first simultaneous XMM-Newton, NuSTAR and Swift observations of the transitional millisecond pulsar PSR J1023+0038 in the X-ray active state. Our multi-wavelength campaign allowed us to investigate with unprecedented detail possible spectral variability over a broad energy range in the X-rays, as well as correlations and lags among emissions in different bands. The soft and hard X-ray emissions are significantly correlated, with no lags between the two bands. On the other hand, the X-ray emission does not correlate with the UV emission. We refine our model for the observed mode switching in terms of rapid transitions between a weak propeller regime and a rotation-powered radio pulsar state, and report on a detailed high-resolution X-ray spectroscopy using all XMM-Newton Reflection Grating Spectrometer data acquired since 2013. We discuss our results in the context of the recent discoveries on the system and of the state of the art simulations on transitional millisecond pulsars, and show how the properties of the narrow emission lines in the soft X-ray spectrum are consistent with an origin within the accretion disc. ","Simultaneous broadband observations and high-resolution X-ray
  spectroscopy of the transitional millisecond pulsar PSR J1023+0038"
14,956358180560191488,930764003277643777,Matias Quiroz,"['Officially in the VB-business after our (with David Nott and Robert Kohn) new paper  on arXiV: <LINK>, Gaussian variational inference for high-dimensional state space models.']",https://arxiv.org/abs/1801.07873,"Our article considers a Gaussian variational approximation of the posterior density in a high-dimensional state space model. The variational parameters to be optimized are the mean vector and the covariance matrix of the approximation. The number of parameters in the covariance matrix grows as the square of the number of model parameters, so it is necessary to find simple yet effective parameterizations of the covariance structure when the number of model parameters is large. We approximate the joint posterior distribution over the high-dimensional state vectors by a dynamic factor model, having Markovian time dependence and a factor covariance structure for the states. This gives a reduced description of the dependence structure for the states, as well as a temporal conditional independence structure similar to that in the true posterior. The usefulness of the approach is illustrated for prediction in two high-dimensional applications that are challenging for Markov chain Monte Carlo sampling. The first is a spatio-temporal model for the spread of the Eurasian Collared-Dove across North America; the second is a Wishart-based multivariate stochastic volatility model for financial returns. ","Gaussian variational approximation for high-dimensional state space
  models"
15,956235654492377088,748528569064710145,MMitchell,"['New paper up on mitigating unwanted biases with adversarial learning, using a projection term. We walk through some fairness notions &amp; show theoretical guarantees. To be presented @ the Artificial Intelligence, Ethics, and Society conference. <LINK>', '@evanmiltenburg Good point, editing, thanks!']",https://arxiv.org/abs/1801.07593,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks. ",Mitigating Unwanted Biases with Adversarial Learning
16,955752580017217536,4383380472,Robert Gorwa,"[""New paper! @DzGuilbeault and I unpack the conceptual ambiguity in the 'bot' and media manipulation literature, and discuss other major challenges for policy and research in the space (data, detection, etc). Have a look! \n\n<LINK>""]",https://arxiv.org/abs/1801.06863,"Amidst widespread reports of digital influence operations during major elections, policymakers, scholars, and journalists have become increasingly interested in the political impact of social media 'bots.' Most recently, platform companies like Facebook and Twitter have been summoned to testify about bots as part of investigations into digitally-enabled foreign manipulation during the 2016 US Presidential election. Facing mounting pressure from both the public and from legislators, these companies have been instructed to crack down on apparently malicious bot accounts. But as this article demonstrates, since the earliest writings on bots in the 1990s, there has been substantial confusion as to exactly what a 'bot' is and what exactly a bot does. We argue that multiple forms of ambiguity are responsible for much of the complexity underlying contemporary bot-related policy, and that before successful policy interventions can be formulated, a more comprehensive understanding of bots --- especially how they are defined and measured --- will be needed. In this article, we provide a history and typology of different types of bots, provide clear guidelines to better categorize political automation and unpack the impact that it can have on contemporary technology policy, and outline the main challenges and ambiguities that will face both researchers and legislators concerned with bots in the future. ",Unpacking the Social Media Bot: A Typology to Guide Research and Policy
17,955451778501406720,1545756036,Mike Boylan-Kolchin,"['New paper from @Alex_Fitts ++: low-mass dwarf galaxies get virtually all of their stellar mass from in situ star formation, not mergers. Great job, Alex! <LINK> <LINK>']",https://arxiv.org/abs/1801.06187v1,"We investigate the merger histories of isolated dwarf galaxies based on a suite of 15 high-resolution cosmological zoom-in simulations, all with masses of $M_{\rm halo} \approx 10^{10}\,{\rm M}_{\odot}$ (and M$_\star\sim10^5-10^7\,{\rm M}_{\odot}$) at $z=0$, from the Feedback in Realistic Environments (FIRE) project. The stellar populations of these dwarf galaxies at $z=0$ are formed essentially entirely ""in situ"": over 90$\%$ of the stellar mass is formed in the main progenitor in all but two cases, and all 15 of the galaxies have >70$\%$ of their stellar mass formed in situ. Virtually all galaxy mergers occur prior to $z\sim3$, meaning that accreted stellar populations are ancient. On average, our simulated dwarfs undergo 5 galaxy mergers in their lifetimes, with typical pre-merger galaxy mass ratios that are less than 1:10. This merger frequency is generally comparable to what has been found in dissipationless simulations when coupled with abundance matching. Two of the simulated dwarfs have a luminous satellite companion at $z=0$. These ultra-faint dwarfs lie at or below current detectability thresholds but are intriguing targets for next-generation facilities. The small contribution of accreted stars make it extremely difficult to discern the effects of mergers in the vast majority of dwarfs either photometrically or using resolved-star color-magnitude diagrams (CMDs). The important implication for near-field cosmology is that star formation histories of comparably massive galaxies derived from resolved CMDs should trace the build-up of stellar mass in one main system across cosmic time as opposed to reflecting the contributions of many individual star formation histories of merged dwarfs. ","] No Assembly Required: Mergers are Mostly Irrelevant for the Growth of
  Low-mass Dwarf Galaxies"
18,955310867234672640,24115949,„ÄåCristi√°n„Äç,"[""Hey, check out my new paper <LINK> yeah, it's the first on today's arxiv list üòé <LINK>""]",http://arxiv.org/abs/1801.06179,"Massive black hole binaries (MBHBs) represent an unavoidable outcome of hierarchical galaxy formation, but their dynamical evolution at sub-parsec scales is poorly understood, due to a combination of uncertainties in theoretical models and lack of firm observational evidence. In gas rich environments, it has been shown that a putative extended, steady circumbinary gaseous disc plays an important role in the MBHB evolution, facilitating its coalescence. How gas on galactic scales is transported to the nuclear region to form and maintain such a stable structure is, however, unclear. If, following a galaxy merger, turbulent gas is condenses in cold clumps and filaments that are randomly scattered, gas is naturally transported on parsec scales and interacts with the MBHB in discrete incoherent pockets. The aim of this work is to investigate the gaseous structures arising from this interaction. We employ a suite of smoothed-particle-hydrodynamic simulations to study the formation and evolution of gaseous structures around a MBHB constantly perturbed by the incoherent infall of molecular clouds. We investigate the influence of the infall rate and angular momentum distribution of the clouds on the geometry and stability of the arising structures. We find that the continuous supply of incoherent clouds is a double-edge sword, resulting in the intermittent formation and disruption of circumbinary structures. Anisotropic cloud distributions featuring an excess of co-rotating events generate more prominent co-rotating circumbinary discs. Similar structures are seen when mostly counter-rotating clouds are fed to the binary, even though they are more compact and less stable. In general, our simulations do not show the formation of extended smooth and stable circumbinary discs, typically assumed in analytical and numerical investigations of the the long term evolution of MBHBs. (Abridged) ","Accretion of clumpy cold gas onto massive black holes binaries: the
  challenging formation of extended circumbinary structures"
19,954389689003401216,115805003,Matteo Cantiello,"['Stars like the Sun don‚Äôt last forever. They eventually expand and engulf the inner members of their planetary systems. ""Planetary Engulfment in the Hertzsprung--Russell Diagram"" New paper published on ApJL #FlatironCCA <LINK> <LINK>']",https://arxiv.org/abs/1801.04274,"Planets accompany most sun-like stars. The orbits of many are sufficiently close that they will be engulfed when their host stars ascend the giant branch. This Letter compares the power generated by orbital decay of an engulfed planet to the intrinsic stellar luminosity. Orbital decay power is generated by drag on the engulfed companion by the surrounding envelope. As stars ascend the giant branch their envelope density drops and so does the power injected through orbital decay, scaling approximately as $L_{\rm decay} \propto R_*^{-9/2}$. Their luminosity, however, increases along the giant branch. These opposed scalings indicate a crossing, where $L_{\rm decay}= L_*$. We consider the engulfment of planets along isochrones in the Hertzsprung-Russell (H-R) diagram. We find that the conditions for such a crossing occur around $L_*\approx 10^2$~$L_\odot$ (or $a\approx 0.1$~au) for Jovian planetary companions. The consumption of closer-in giant planets, such as hot Jupiters, leads to $L_{\rm decay}\gg L_*$, while more distant planets such as warm Jupiters, $a\approx 0.5$~au, lead to minor perturbations of their host stars with $L_{\rm decay} \ll L_*$. Our results map out the parameter space along the giant branch in the H-R Diagram where interaction with planetary companions leads to significant energetic disturbance of host stars. ",Planetary Engulfment in the Hertzsprung--Russell Diagram
20,954282357850288128,481539448,Richard Alexander,"['New paper, led by @PhysicsUoL postdoc Giovanni Dipierro (with @cassidentprone, me and many others). We use ALMA to observe gaps and rings in the planet-forming disc Elias 24, and show that these structures can be explained by a giant planet in the disc.\n<LINK> <LINK>', ""What's particularly neat is that Giovanni's simulations show one single planet (plus differential dust/gas motion) can account for both of the observed gaps in the emission, and the bright rings in between. https://t.co/kEGQvKdby1""]",https://arxiv.org/abs/1801.05812,"We present Atacama Large Millimeter/sub-millimeter Array (ALMA) Cycle 2 observations of the 1.3 mm dust continuum emission of the protoplanetary disc surrounding the T Tauri star Elias 24 with an angular resolution of $\sim 0.2""$ ($\sim 28$ au). The dust continuum emission map reveals a dark ring at a radial distance of $0.47""$ ($\sim 65$ au) from the central star, surrounded by a bright ring at $0.58""$ ($\sim 81$ au). In the outer disc, the radial intensity profile shows two inflection points at $0.71""$ and $0.87""$ ($\sim 99$ and $121$ au respectively). We perform global three-dimensional smoothed particle hydrodynamic gas/dust simulations of discs hosting a migrating and accreting planet. Combining the dust density maps of small and large grains with three dimensional radiative transfer calculations, we produce synthetic ALMA observations of a variety of disc models in order to reproduce the gap- and ring-like features observed in Elias 24. We find that the dust emission across the disc is consistent with the presence of an embedded planet with a mass of $\sim 0.7\, \mathrm{M_{\mathrm{J}}}$ at an orbital radius of $\sim$ 60 au. Our model suggests that the two inflection points in the radial intensity profile are due to the inward radial motion of large dust grains from the outer disc. The surface brightness map of our disc model provides a reasonable match to the gap- and ring-like structures observed in Elias 24, with an average discrepancy of $\sim$ 5% of the observed fluxes around the gap region. ",Rings and gaps in the disc around Elias 24 revealed by ALMA
21,953826512041541633,47125300,Jorge Zuluaga,"['‚ÄúTowards a theoretical determination of the geographical probability distribution of meteoroid impacts on Earth‚Äù our new paper on asteroid impacts <LINK> BTW: yesterday America was in the antapex hemisphere  #meteorite #Detroit @ProfAbelMendez @MarioSucerquia <LINK>', 'And here is the companion annual map... https://t.co/ak8nychr6m https://t.co/PUS0xfPfmm']",https://arxiv.org/abs/1801.05720,"Tunguska and Chelyabinsk impact events occurred inside a geographical area of only 3.4\% of the Earth's surface. Although two events hardly constitute a statistically significant demonstration of a geographical pattern of impacts, their spatial coincidence is at least tantalizing. To understand if this concurrence reflects an underlying geographical and/or temporal pattern, we must aim at predicting the spatio-temporal distribution of meteoroid impacts on Earth. For this purpose we designed, implemented and tested a novel numerical technique, the ""Gravitational Ray Tracing"" (GRT) designed to compute the relative impact probability (RIP) on the surface of any planet. GRT is inspired by the so-called ray-casting techniques used to render realistic images of complex 3D scenes. In this paper we describe the method and the results of testing it at the time of large impact events. Our findings suggest a non-trivial pattern of impact probabilities at any given time on Earth. Locations at $60-90\deg$ from the apex are more prone to impacts, especially at midnight. Counterintuitively, sites close to apex direction have the lowest RIP, while in the antapex RIP are slightly larger than average. We present here preliminary maps of RIP at the time of Tunguska and Chelyabinsk events and found no evidence of a spatial or temporal pattern, suggesting that their coincidence was fortuitous. We apply the GRT method to compute theoretical RIP at the location and time of 394 large fireballs. Although the predicted spatio-temporal impact distribution matches marginally the observed events, we successfully predict their impact speed distribution. ","Towards a theoretical determination of the geographical probability
  distribution of meteoroid impacts on Earth"
22,953800657567444992,232294292,Gary Marcus üá∫üá¶,"['Innateness, AlphaZero, and AI: AlphaGo &amp; its successors have been presented as an argument that a ""even in the most challenging of domains: it is possible to train to superhuman level, without human ... guidance"".  I evaluate this claim. new paper on arXiv <LINK>']",https://arxiv.org/abs/1801.05667,"The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series of papers by Silver et al (Silver et al., 2017a) on AlphaGo and its successors that have been presented as an argument that a ""even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance"", ""starting tabula rasa."" I argue that these claims are overstated, for multiple reasons. I close by arguing that artificial intelligence needs greater attention to innateness, and I point to some proposals about what that innateness might look like. ","Innateness, AlphaZero, and Artificial Intelligence"
23,953731086894366720,835680883,Jason Wright,"[""Thank you @arxiv for publishing our @eprv2017 @AAS_ResNotes! \nHere's the link to the paper, including the pretty table of 23 new EPRV instruments!\n<LINK>\n#EPRVIII""]",https://arxiv.org/abs/1801.05383,"The Third Workshop on Extremely Precise Radial Velocities was held at the Penn Stater Conference Center and Hotel in State College, Pennsylvania, USA from 2016 August 14 to 17, and featured over 120 registrants from around the world. Here we provide a brief description of the conference, its format, and its session topics and chairs. 23 instrument teams were represented in plenary talks, and we present a table containing the basic characteristics of their new precise Doppler velocimeters. ","The Third Workshop on Extremely Precise Radial Velocities: The New
  Instruments"
24,953599022870261760,27047369,C√©sar A. Hidalgo,"['New paper exploring (i) how individual countries jump in the product space, (ii) when they jump far (answer: at an intermediate level of development), and (iii) whether jumping far is beneficial for economic growth (it provides a slight short term boost). <LINK> <LINK>']",https://arxiv.org/abs/1801.05352,"Countries tend to diversify their exports by entering products that are related to their current exports. Yet this average behavior is not representative of every diversification path. In this paper, we introduce a method to identify periods when countries enter unrelated products. We analyze the economic diversification paths of 93 countries between 1965 and 2014 and find that countries enter unrelated products in only about 7.2% of all observations. We find that countries enter more unrelated products when they are at an intermediate level of economic development, and when they have higher levels of human capital. Finally, we ask whether countries entering more unrelated products grow faster than those entering only related products. The data shows that countries that enter more unrelated activities experience a small but significant increase in future economic growth, compared to countries with a similar level of income, human capital, capital stock per worker, and economic complexity. ","Shooting High or Low: Do Countries Benefit from Entering Unrelated
  Activities?"
25,953580985370767360,939772046615236609,Ed Tarleton,['Preprint of our new paper: Consistent determination of geometrically necessary dislocation density @DasSuchandrima @FHofmannGroup on arXiv <LINK> <LINK>'],https://arxiv.org/abs/1801.05292,"The use of Nye's dislocation tensor for calculating the density of geometrically necessary dislocations (GND) is widely adopted in the study of plastically deformed materials. The curl operation involved in finding the Nye tensor, while conceptually straightforward has been marred with inconsistencies and several different definitions are in use. For the three most common definitions, we show that their consistent application leads to the same result. To eliminate frequently encountered confusion, a summary of expressions for Nye's tensor in terms of elastic and plastic deformation gradient, and for both small and large deformations, is presented. A further question when estimating GND density concerns the optimization technique used to solve the under-determined set of equations linking Nye's tensor and GND density. A systematic comparison of the densities obtained by two widely used techniques, L1 and L2 minimisation, shows that both methods yield remarkably similar total GND densities. Thus the mathematically simpler, L2, may be preferred over L1 except when information about the distribution of densities on specific slip systems is required. To illustrate this, we compare experimentally measured lattice distortions beneath nano-indents in pure tungsten, probed using 3D-resolved synchrotron X-ray micro-diffraction, with those predicted by 3D strain-gradient crystal plasticity finite element calculations. The results are in good agreement and show that the volumetric component of the elastic strain field has a surprisingly small effect on the determined Nye tensor. This is important for experimental techniques, such as micro-beam Laue measurements and HR-EBSD, where only the deviatoric strain component is measured. ","Consistent determination of geometrically necessary dislocation density
  from simulations and experiments"
26,953436837648875520,21611239,Sean Carroll,"['New paper: talking about the multiverse is perfectly ordinary science, despite what an overly naive version of ‚Äúfalsifiability‚Äù might suggest.\n<LINK>', '@john95683 Nope.', '@ElyseHargreaves Nope... it‚Äôs explained in the article.']",https://arxiv.org/abs/1801.05016,"Cosmological models that invoke a multiverse - a collection of unobservable regions of space where conditions are very different from the region around us - are controversial, on the grounds that unobservable phenomena shouldn't play a crucial role in legitimate scientific theories. I argue that the way we evaluate multiverse models is precisely the same as the way we evaluate any other models, on the basis of abduction, Bayesian inference, and empirical success. There is no scientifically respectable way to do cosmology without taking into account different possibilities for what the universe might be like outside our horizon. Multiverse theories are utterly conventionally scientific, even if evaluating them can be difficult in practice. ",Beyond Falsifiability: Normal Science in a Multiverse
27,953172099513962496,1710697381,Diego F. Torres,['New paper (ApJL sub.) ‚Äú1st continuous optical monitoring of the transitional millisecond PSR J1023+0038 with Kepler‚Äù <LINK> <LINK>'],http://arxiv.org/abs/1801.04736,"We report on the first continuous, 80 day optical monitoring of the transitional millisecond pulsar PSR J1023+0038 carried out in mid-2017 with Kepler in the K2 configuration, when an X-ray subluminous accretion disk was present in the binary. Flares lasting from minutes to 14 hr were observed for 15.6% of the time, which is a larger fraction than previously reported on the basis of X-ray and past optical observations, and more frequently when the companion was at the superior conjunction of the orbit. A sinusoidal modulation at the binary orbital period was also present with an amplitude of ~16%, which varied by a few percent over timescales of days, and with a maximum that took place 890 +/- 85 s earlier than the superior conjunction of the donor. We interpret these phenomena in terms of reprocessing of the X-ray emission by an asymmetrically heated companion star surface and/or a non-axisymmetric outflow possibly launched close to the inner Lagrangian point. Furthermore, the non-flaring average emission varied by up to ~ 40% over a time scale of days in the absence of correspondingly large variations of the irradiating X-ray flux. The latter suggests that the observed changes in the average optical luminosity might be due to variations of the geometry, size, and/or mass accretion rate in the outer regions of the accretion disk. ","The first continuous optical monitoring of the transitional millisecond
  pulsar PSR J1023+0038 with Kepler"
28,952893524806160384,582161546,Alessandro  Vespignani,['New paper analyze the ‚Äúconnection between the way in which individuals explore new resources and exploit known assets in the social and spatial spheres.‚Äù\n <LINK>\nBy @suneman @a_baronca @lau_retti <LINK>'],https://arxiv.org/abs/1801.03962,"According to personality psychology, personality traits determine many aspects of human behaviour. However, validating this insight in large groups has been challenging so far, due to the scarcity of multi-channel data. Here, we focus on the relationship between mobility and social behaviour by analysing trajectories and mobile phone interactions of $\sim 1,000$ individuals from two high-resolution longitudinal datasets. We identify a connection between the way in which individuals explore new resources and exploit known assets in the social and spatial spheres. We show that different individuals balance the exploration-exploitation trade-off in different ways and we explain part of the variability in the data by the big five personality traits. We point out that, in both realms, extraversion correlates with the attitude towards exploration and routine diversity, while neuroticism and openness account for the tendency to evolve routine over long time-scales. We find no evidence for the existence of classes of individuals across the spatio-social domains. Our results bridge the fields of human geography, sociology and personality psychology and can help improve current models of mobility and tie formation. ",Understanding the interplay between social and spatial behaviour
29,952865519442460672,3716338821,Mikko Tuomi,"['Our new paper led by @MatiasDiazM, @ExoplanetJJ (also @phillippro, @johannateske et al.) is out! We study the radial velocities of #HD26965 and find a consistent periodic signal. But is it caused by an #exoplanet orbiting the star with P = 42.36d? <LINK> <LINK>', 'We find that the signal is there beyond any reasonable doubt, supported by RV data from #HARPS, #HIRES, #PFS, and #CHIRON. If it is caused by a planet, which is what I think, it corresponds to a hot mini-Neptune with a minimum mass of 7Me. https://t.co/Od0LaIoAja', 'The planet, should it exist, is ""hot"" because it is located interior to the inner edge of the liquid-water habitable zone of #HD26965, i.e. the it is too hot for liquid water to exist on the planet\'s surface. Here is the current detection threshold for additional planets. https://t.co/CdBD8zQUw4', 'But there is a catch - #HD26965 shows evidence for activity-induced variations. There is a long-period magnetic activity cycle (CaII H&amp;K emission data from Mount Wilson) and possible rotation period of 37d, or 43d coinciding with the RV signal. https://t.co/qV27MjtnR8', 'Evidence for activity-induced periodicity of 43d is rather weak. However, because it is very close to the RV period, it is plausible that they are caused by the same process, i.e. stellar rotation. Literature gives rotation period of 37 - 38d. So have we found a planet at all?', 'Read more at about it in our paper accepted for publication in AJ. https://t.co/qQrY0ee4vG\n\n""This study serves as an excellent test case for future works that aim to detect small planets orbiting ‚ÄòSun-like‚Äô stars using radial velocity measurements."" https://t.co/UBH97hpc0X', ""#HD26965 is rather similar to the Sun, a bit less massive and less luminous. But we could not detect planets smaller than 10Me in the star's habitable zone. This is the typical and so unfortunate state-of-the-art precision with radial velocities. https://t.co/mT6TfEHNKj"", '@TomimPA @MatiasDiazM @ExoplanetJJ @phillippro @johannateske Who knows? I certainly am not aware anymore, and the concept of ""group"" is very vague as well. Not to mention, there is no such thing as ""certain"". So this is a very simple question to ask, but almost impossible to answer. The best answer I can give is ""dozens"".']",https://arxiv.org/abs/1801.03970,"We report the discovery of a radial velocity signal that can be interpreted as a planetary-mass candidate orbiting the K dwarf HD26965, with an orbital period of 42.364$\pm$0.015 days, or alternatively, as the presence of residual, uncorrected rotational activity in the data. Observations include data from HIRES, PFS, CHIRON, and HARPS, where 1,111 measurements were made over 16 years. Our best solution for HD26965 $b$ is consistent with a super-Earth that has a minimum mass of 6.92$\pm$0.79 M$_{\oplus}$ orbiting at a distance of 0.215$\pm$0.008 AU from its host star. We have analyzed the correlation between spectral activity indicators and the radial velocities from each instrument, showing moderate correlations that we include in our model. From this analysis, we recover a $\sim$38 day signal, which matches some literature values of the stellar rotation period. However, from independent Mt. Wilson HK data for this star, we find evidence for a significant 42 day signal after subtraction of longer period magnetic cycles, casting doubt on the planetary hypothesis for this period. Although our statistical model strongly suggests that the 42-day signal is Doppler in origin, we conclude that the residual effects of stellar rotation are difficult to fully model and remove from this dataset, highlighting the difficulties to disentangle small planetary signals and photospheric noise, particularly when the orbital periods are close to the rotation period of the star. This study serves as an excellent test case for future works that aim to detect small planets orbiting `Sun-like' stars using radial velocity measurements. ","The test case of HD26965: difficulties disentangling weak Doppler
  signals from stellar activity"
30,952831304088936448,16174436,Sune Lehmann,"['New paper with @lau_retti and @a_baronca out on arXiv. ""Individual mobility and social behaviour: Two sides of the same coin"". <LINK>']",https://arxiv.org/abs/1801.03962,"According to personality psychology, personality traits determine many aspects of human behaviour. However, validating this insight in large groups has been challenging so far, due to the scarcity of multi-channel data. Here, we focus on the relationship between mobility and social behaviour by analysing trajectories and mobile phone interactions of $\sim 1,000$ individuals from two high-resolution longitudinal datasets. We identify a connection between the way in which individuals explore new resources and exploit known assets in the social and spatial spheres. We show that different individuals balance the exploration-exploitation trade-off in different ways and we explain part of the variability in the data by the big five personality traits. We point out that, in both realms, extraversion correlates with the attitude towards exploration and routine diversity, while neuroticism and openness account for the tendency to evolve routine over long time-scales. We find no evidence for the existence of classes of individuals across the spatio-social domains. Our results bridge the fields of human geography, sociology and personality psychology and can help improve current models of mobility and tie formation. ",Understanding the interplay between social and spatial behaviour
31,952715673880244225,2337598033,Geraint F. Lewis,['Superb new paper on the arxiv with @pythonomer <LINK> <LINK>'],https://arxiv.org/abs/1801.03949,"Our nearest large cosmological neighbour, the Andromeda galaxy (M31), is a dynamical system, and an accurate measurement of its total mass is central to our understanding of its assembly history, the life-cycles of its satellite galaxies, and its role in shaping the Local Group environment. Here, we apply a novel approach to determine the dynamical mass of M31 using high velocity Planetary Nebulae (PNe), establishing a hierarchical Bayesian model united with a scheme to capture potential outliers and marginalize over tracers unknown distances. With this, we derive the escape velocity run of M31 as a function of galacto-centric distance, with both parametric and non-parametric approaches. We determine the escape velocity of M31 to be $470\pm{40}$ km s$^{-1}$ at a galacto-centric distance of 15 kpc, and also, derive the total potential of M31, estimating the virial mass and radius of the galaxy to be $0.8\pm{0.1}\times10^{12}\,M_\odot$ and $240\pm{10}$ kpc, respectively. Our M31 mass is on the low-side of the measured range, this supports the lower expected mass of the M31-Milky Way system from the timing and momentum arguments, satisfying the HI constraint on circular velocity between $10\lesssim R/\textrm{kpc}<35$, and agreeing with the stellar mass Tully-Fisher relation. To place these results in a broader context, we compare them to the key predictions of the $\Lambda{\rm CDM}$ cosmological paradigm, including the stellar-mass-halo-mass and the dark matter halo concentration-virial mass correlation, and finding it to be an outlier to this relation. ","The Need for Speed: Escape velocity and dynamical mass measurements of
  the Andromeda galaxy"
32,952154463388798977,2791978125,Christian J√§h,['Our new paper on Hyperbolic systems tackles well-posedness under no conditions on the multiplicities #Maths #PDE #Analysis #Science\n<LINK>'],https://arxiv.org/abs/1801.03573,"In this paper we analyse the well-posedness of the Cauchy problem for a rather general class of hyperbolic systems with space-time dependent coefficients and with multiple characteristics of variable multiplicity. First, we establish a well-posedness result in anisotropic Sobolev spaces for systems with upper triangular principal part under interesting natural conditions on the orders of lower order terms below the diagonal. Namely, the terms below the diagonal at a distance $k$ to it must be of order $-k$. This setting also allows for the Jordan block structure in the system. Second, we give conditions for the Schur type triangularisation of general systems with variable coefficients for reducing them to the form with an upper triangular principal part for which the first result can be applied. We give explicit details for the appearing conditions and constructions for $2\times 2$ and $3\times 3$ systems, complemented by several examples. ","Hyperbolic systems with non-diagonalisable principal part and variable
  multiplicities, I. Well-posedness"
33,951899780065587200,792761013749870592,Nick Rubin,['New paper is out!  Geometric constraints on fermionic marginals + quantum computation\n\n<LINK>'],https://arxiv.org/abs/1801.03524,"Many quantum algorithms, including recently proposed hybrid classical/quantum algorithms, make use of restricted tomography of the quantum state that measures the reduced density matrices, or marginals, of the full state. The most straightforward approach to this algorithmic step estimates each component of the marginal independently without making use of the algebraic and geometric structure of the marginals. Within the field of quantum chemistry, this structure is termed the fermionic $n$-representability conditions, and is supported by a vast amount of literature on both theoretical and practical results related to their approximations. In this work, we introduce these conditions in the language of quantum computation, and utilize them to develop several techniques to accelerate and improve practical applications for quantum chemistry on quantum computers. We show that one can use fermionic $n$-representability conditions to reduce the total number of measurements required by more than an order of magnitude for medium sized systems in chemistry. We also demonstrate an efficient restoration of the physicality of energy curves for the dilation of a four qubit diatomic hydrogen system in the presence of three distinct one qubit error channels, providing evidence these techniques are useful for pre-fault tolerant quantum chemistry experiments. ","Application of fermionic marginal constraints to hybrid quantum
  algorithms"
34,951844845500461058,57793813,Teppei Katori (È¶ôÂèñÂì≤Âπ≥),"['The new paper from #MiniBooNE is about the first observation of kaon decay-at-rest #KDAR #neutrinos , potential source for high-precision #nuxsec measurement and sterile neutrino search! @Fermilab <LINK> <LINK>']",https://arxiv.org/abs/1801.03848,"We report the first measurement of monoenergetic muon neutrino charged current interactions. MiniBooNE has isolated 236 MeV muon neutrino events originating from charged kaon decay at rest ($K^+ \rightarrow \mu^+ \nu_\mu$) at the NuMI beamline absorber. These signal $\nu_\mu$-carbon events are distinguished from primarily pion decay in flight $\nu_\mu$ and $\overline{\nu}_\mu$ backgrounds produced at the target station and decay pipe using their arrival time and reconstructed muon energy. The significance of the signal observation is at the 3.9$\sigma$ level. The muon kinetic energy, neutrino-nucleus energy transfer ($\omega=E_\nu-E_\mu$), and total cross section for these events is extracted. This result is the first known-energy, weak-interaction-only probe of the nucleus to yield a measurement of $\omega$ using neutrinos, a quantity thus far only accessible through electron scattering. ","First Measurement of Monoenergetic Muon Neutrino Charged Current
  Interactions"
35,951579934614589440,2284142012,Brigitta Sip≈ëcz,"['Exciting times, the new Astropy paper is on arxiv including significant updates since the previous paper (0.2 release), the status of the project and some future plans. <LINK>', ""Oh, and let's hope we'll have enough to say for a new paper in a few years time and don't have to wait for version v20.0 :) (First paper was v0.2, this brand new second one is v2.0)""]",http://arxiv.org/abs/1801.02634,"The Astropy project supports and fosters the development of open-source and openly-developed Python packages that provide commonly-needed functionality to the astronomical community. A key element of the Astropy project is the core package Astropy, which serves as the foundation for more specialized projects and packages. In this article, we provide an overview of the organization of the Astropy project and summarize key features in the core package as of the recent major release, version 2.0. We then describe the project infrastructure designed to facilitate and support development for a broader ecosystem of inter-operable packages. We conclude with a future outlook of planned new features and directions for the broader Astropy project. ","The Astropy Project: Building an inclusive, open-science project and
  status of the v2.0 core package"
36,951510760521285633,1653127615,Chris Monahan,"['New paper ""Parton distribution functions from reduced Ioffe-time distributions"" on the #arXiv at <LINK>. Ioffe-time distributions are the best distributions. #physics #latticeQCD']",https://arxiv.org/abs/1801.03023,"We show that the correct way to extract parton distribution functions from the reduced Ioffe-time distribution (RITD), a ratio of the Ioffe-time distribution for a moving hadron and a hadron at rest, is through a factorization formula. This factorization exists because, at small distances, forming the ratio does not change the infrared behavior of the numerator, which is factorizable. We illustrate the effect of such a factorization by applying it to results in the literature. ",Parton distribution functions from reduced Ioffe-time distributions
37,951312545209569281,16714100,Cayman Unterborn,"['New white paper alert! If we ever want to understand anything with certainty about an exoplanet, we need to learn more about Venus ASAP. <LINK>']",https://arxiv.org/abs/1801.03146,"The goals of the astrobiology community are focussed on developing a framework for the detection of biosignatures, or evidence thereof, on objects inside and outside of our solar system. A fundamental aspect of understanding the limits of habitable environments and detectable signatures is the study of where the boundaries of such environments can occur. Thus, the need to study the creation, evolution, and frequency of hostile environments for habitability is an integral part of the astrobiology story. These provide the opportunity to understand the bifurcation, between habitable and uninhabitable. The archetype of such a planet is the Earth's sister planet, Venus, and provides a unique opportunity to explore the processes that created a completely uninhabitable environment and thus define the conditions that can rule out bio-related signatures. We advocate a continued comprehensive study of our sister planet, including models of early atmospheres, compositional abundances, and Venus-analog frequency analysis from current and future exoplanet data. Moreover, new missions to Venus to provide in-situ data are necessary. ",Venus: The Making of an Uninhabitable World
38,951266384763346944,5850692,Aaron Roth,"['We have a new paper, giving a smoothed analysis of the greedy algorithm in linear contextual bandit problems: <LINK> . I wrote a blog post about why exploration can be ""unfair"" and why you should care about exploration-free algorithms: <LINK>']",https://arxiv.org/abs/1801.03423,"Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. This raises a fairness concern. In such settings, one might like to run a ""greedy"" algorithm, which always makes the (myopically) optimal decision for the individuals at hand - but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm. We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve ""no regret"", perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that ""generically"" (i.e. in slightly perturbed environments), exploration and exploitation need not be in conflict in the linear setting. ","A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual
  Bandit Problem"
39,951261838452256769,29000998,Debashis Ghosh,['A new paper on the potential restrictiveness of the treatment positivity assumption in causal inference for high-dimensional covariates: \n<LINK>'],http://arxiv.org/abs/1801.03185,"A powerful tool for the analysis of nonrandomized observational studies has been the potential outcomes model. Utilization of this framework allows analysts to estimate average treatment effects. This article considers the situation in which high-dimensional covariates are present and revisits the standard assumptions made in causal inference. We show that by employing a flexible Gaussian process framework, the assumption of strict overlap leads to very restrictive assumptions about the distribution of covariates, results for which can be characterized using classical results from Gaussian random measures as well as reproducing kernel Hilbert space theory. In addition, we propose a strategy for data-adaptive causal effect estimation that does not rely on the strict overlap assumption. These findings reveal the stringency that accompanies the use of the treatment positivity assumption in high-dimensional settings. ","A Gaussian process framework for overlap and causal effect estimation
  with high-dimensional covariates"
40,951102888016863233,339412676,Astropy,"['On the #arxiv today: a new #astropy paper describing the status of the project, and key updates since our original v0.2 paper! <LINK>']",https://arxiv.org/abs/1801.02634,"The Astropy project supports and fosters the development of open-source and openly-developed Python packages that provide commonly-needed functionality to the astronomical community. A key element of the Astropy project is the core package Astropy, which serves as the foundation for more specialized projects and packages. In this article, we provide an overview of the organization of the Astropy project and summarize key features in the core package as of the recent major release, version 2.0. We then describe the project infrastructure designed to facilitate and support development for a broader ecosystem of inter-operable packages. We conclude with a future outlook of planned new features and directions for the broader Astropy project. ","The Astropy Project: Building an inclusive, open-science project and
  status of the v2.0 core package"
41,950915404880228352,70985850,Anthony Horton,"['New @astropy paper now on the arXiv: <LINK>\n\nThere are overviews of this community Python library for astronomy project, the key features of the core package as of the recent v2.0 release, &amp; the affiliated package ecosystem. <LINK>', ""@astropy Everyone who's contributed to the astropy project was invited to be a co-author of the paper. I got on the list due to some (very small) contributions to both the core package &amp; ccdproc affiliated package some time back. My 1st software paper!"", '@astropy ""As of version 2.0, astropy contains 212244 lines of code\ncontributed by 232 unique\ncontributors over 19270 git commits.""\n\nIt\'s a true community effort. https://t.co/wCdips0CwV', '@astropy Astronomical coordinate systems. I wonder how many professional astronomers could give the definitions for *all* of these, without looking anything up? https://t.co/mosJj4QJFP']",https://arxiv.org/abs/1801.02634,"The Astropy project supports and fosters the development of open-source and openly-developed Python packages that provide commonly-needed functionality to the astronomical community. A key element of the Astropy project is the core package Astropy, which serves as the foundation for more specialized projects and packages. In this article, we provide an overview of the organization of the Astropy project and summarize key features in the core package as of the recent major release, version 2.0. We then describe the project infrastructure designed to facilitate and support development for a broader ecosystem of inter-operable packages. We conclude with a future outlook of planned new features and directions for the broader Astropy project. ","The Astropy Project: Building an inclusive, open-science project and
  status of the v2.0 core package"
42,950750788845961216,23000769,Christopher Conselice,['Our new paper is out on a VLT/MUSE study of a new compact lensing massive cluster - the Clio Cluster  - which is one of our JWST GTO targets.  Has low intracluster light - making high-z galaxies easier to study in the next generation of deep searches:\n\n<LINK>'],https://arxiv.org/abs/1801.01140,"We present the results of a VLT MUSE/FORS2 and Spitzer survey of a unique compact lensing cluster CLIO at z = 0.42, discovered through the GAMA survey using spectroscopic redshifts. Compact and massive clusters such as this are understudied, but provide a unique prospective on dark matter distributions and for finding background lensed high-z galaxies. The CLIO cluster was identified for follow up observations due to its almost unique combination of high mass and dark matter halo concentration, as well as having observed lensing arcs from ground based images. Using dual band optical and infra-red imaging from FORS2 and Spitzer, in combination with MUSE optical spectroscopy we identify 89 cluster members and find background sources out to z = 6.49. We describe the physical state of this cluster, finding a strong correlation between environment and galaxy spectral type. Under the assumption of a NFW profile, we measure the total mass of CLIO to be M$_{200} = (4.49 \pm 0.25) \times 10^{14}$ M$_\odot$. We build and present an initial strong-lensing model for this cluster, and measure a relatively low intracluster light (ICL) fraction of 7.21 $\pm$ 1.53% through galaxy profile fitting. Due to its strong potential for lensing background galaxies and its low ICL, the CLIO cluster will be a target for our 110 hour JWST 'Webb Medium-Deep Field' (WMDF) GTO program. ","MUSE spectroscopy and deep observations of a unique compact JWST target,
  lensing cluster CLIO"
43,950669410120945664,939772046615236609,Ed Tarleton,['Preprint of new paper: Interstitial-mediated dislocation climb and the weakening of  particle-reinforced alloys under irradiation  <LINK>'],https://arxiv.org/abs/1801.02535,"Dislocations can climb out of their glide plane by absorbing (or emitting) point defects (vacancies and self-interstitial atoms (SIAs)). In contrast with conservative glide motion, climb relies on the point defects' thermal diffusion and hence operates on much longer timescales, leading to some forms of creep. Whilst equilibrium point defect concentrations allow dislocations to climb to relieve non-glide stresses, point defect supersaturations also lead to osmotic forces, driving dislocation motion even in the absence of external stresses. Self-interstitial atoms typically have significantly higher formation energies than vacancies, so their contribution to climb is usually ignored. However, under irradiation conditions, both types of defect are athermally created in equal numbers. In this letter, we use simple thermodynamic arguments to show that the contribution of interstitials cannot be neglected in irradiated materials, and that the osmotic force they induce on dislocations is many orders of magnitude larger than that caused by vacancies. This explains why the prismatic dislocation loops observed by {\it in situ} transmission electron microscope irradiations are more often of interstitial rather than vacancy character. Using discrete dislocation dynamics simulations, we investigate the effect on dislocation-obstacle interactions, and find reductions in the depinning time of many orders of magnitude. This has important consequences for the strength of particle-reinforced alloys under irradiation. ","Interstitial-mediated dislocation climb and the weakening of
  particle-reinforced alloys under irradiation"
44,950228403214483456,715587065526808577,Yannis Paschalidis,['How can the US save billions of $ in preventable hospitalizations? Predict &amp; prevent! A new paper to appear in the @ProceedingsIEEE. Preprint <LINK>'],https://arxiv.org/abs/1801.01204,"Urban living in modern large cities has significant adverse effects on health, increasing the risk of several chronic diseases. We focus on the two leading clusters of chronic disease, heart disease and diabetes, and develop data-driven methods to predict hospitalizations due to these conditions. We base these predictions on the patients' medical history, recent and more distant, as described in their Electronic Health Records (EHR). We formulate the prediction problem as a binary classification problem and consider a variety of machine learning methods, including kernelized and sparse Support Vector Machines (SVM), sparse logistic regression, and random forests. To strike a balance between accuracy and interpretability of the prediction, which is important in a medical setting, we propose two novel methods: K-LRT, a likelihood ratio test-based method, and a Joint Clustering and Classification (JCC) method which identifies hidden patient clusters and adapts classifiers to each cluster. We develop theoretical out-of-sample guarantees for the latter method. We validate our algorithms on large datasets from the Boston Medical Center, the largest safety-net hospital system in New England. ","Predicting Chronic Disease Hospitalizations from Electronic Health
  Records: An Interpretable Classification Approach"
45,950223832673083392,17121984,Max Bannach,['We have a new paper: ‚ÄûComputing Hitting Set Kernels By AC^0-Circuits‚Äú <LINK>'],https://arxiv.org/abs/1801.00716,"Given a hypergraph $H = (V,E)$, what is the smallest subset $X \subseteq V$ such that $e \cap X \neq \emptyset$ holds for all $e \in E$? This problem, known as the hitting set problem, is a basic problem in parameterized complexity theory. There are well-known kernelization algorithms for it, which get a hypergraph $H$ and a number $k$ as input and output a hypergraph $H'$ such that (1) $H$ has a hitting set of size $k$ if, and only if, $H'$ has such a hitting set and (2) the size of $H'$ depends only on $k$ and on the maximum cardinality $d$ of edges in $H$. The algorithms run in polynomial time, but are highly sequential. Recently, it has been shown that one of them can be parallelized to a certain degree: one can compute hitting set kernels in parallel time $O(d)$ -- but it was conjectured that this is the best parallel algorithm possible. We refute this conjecture and show how hitting set kernels can be computed in constant parallel time. For our proof, we introduce a new, generalized notion of hypergraph sunflowers and show how iterated applications of the color coding technique can sometimes be collapsed into a single application. ",Computing Hitting Set Kernels By AC^0-Circuits
46,950179881551212544,2337598033,Geraint F. Lewis,['New paper on the arxiv with @galahsurvey <LINK> <LINK>'],https://arxiv.org/abs/1801.01514,"Using data from the GALAH pilot survey, we determine properties of the Galactic thin and thick disks near the solar neighbourhood. The data cover a small range of Galactocentric radius ($7.9 \leq R_\mathrm{GC} \leq 9.5$ kpc), but extend up to 4 kpc in height from the Galactic plane, and several kpc in the direction of Galactic anti-rotation (at longitude $260 ^\circ \leq \ell \leq 280^\circ$). This allows us to reliably measure the vertical density and abundance profiles of the chemically and kinematically defined `thick' and `thin' disks of the Galaxy. The thin disk (low-$\alpha$ population) exhibits a steep negative vertical metallicity gradient, at d[M/H]/d$z=-0.18 \pm 0.01$ dex kpc$^{-1}$, which is broadly consistent with previous studies. In contrast, its vertical $\alpha$-abundance profile is almost flat, with a gradient of d[$\alpha$/M]/d$z$ = $0.008 \pm 0.002$ dex kpc$^{-1}$. The steep vertical metallicity gradient of the low-$\alpha$ population is in agreement with models where radial migration has a major role in the evolution of the thin disk. The thick disk (high-$\alpha$ population) has a weaker vertical metallicity gradient d[M/H]/d$z = -0.058 \pm 0.003$ dex kpc$^{-1}$. The $\alpha$-abundance of the thick disk is nearly constant with height, d[$\alpha$/M]/d$z$ = $0.007 \pm 0.002$ dex kpc$^{-1}$. The negative gradient in metallicity and the small gradient in [$\alpha$/M] indicate that the high-$\alpha$ population experienced a settling phase, but also formed prior to the onset of major SNIa enrichment. We explore the implications of the distinct $\alpha$-enrichments and narrow [$\alpha$/M] range of the sub-populations in the context of thick disk formation. ","The GALAH survey: properties of the Galactic disk(s) in the solar
  neighbourhood"
47,949216719540838401,3807291922,Aram Harrow,"['Excited to post a new paper on when XOR games have entangled value = 1, together with Adam Bene Watts, Gurtej Kanwar and Anand Natarajan.\n<LINK>\nMy favorite parts are the linear algebra formulation and the NPA lower bounds.']",https://arxiv.org/abs/1801.00821,"We study the complexity of computing the commuting-operator value $\omega^*$ of entangled XOR games with any number of players. We introduce necessary and sufficient criteria for an XOR game to have $\omega^* = 1$, and use these criteria to derive the following results: 1. An algorithm for symmetric games that decides in polynomial time whether $\omega^* = 1$ or $\omega^* < 1$, a task that was not previously known to be decidable, together with a simple tensor-product strategy that achieves value 1 in the former case. The only previous candidate algorithm for this problem was the Navascu\'{e}s-Pironio-Ac\'{i}n (also known as noncommutative Sum of Squares or ncSoS) hierarchy, but no convergence bounds were known. 2. A family of games with three players and with $\omega^* < 1$, where it takes doubly exponential time for the ncSoS algorithm to witness this (in contrast with our algorithm which runs in polynomial time). 3. A family of games achieving a bias difference $2(\omega^* - \omega)$ arbitrarily close to the maximum possible value of $1$ (and as a consequence, achieving an unbounded bias ratio), answering an open question of Bri\""{e}t and Vidick. 4. Existence of an unsatisfiable phase for random (non-symmetric) XOR games: that is, we show that there exists a constant $C_k^{\text{unsat}}$ depending only on the number $k$ of players, such that a random $k$-XOR game over an alphabet of size $n$ has $\omega^* < 1$ with high probability when the number of clauses is above $C_k^{\text{unsat}} n$. 5. A lower bound of $\Omega(n \log(n)/\log\log(n))$ on the number of levels in the ncSoS hierarchy required to detect unsatisfiability for most random 3-XOR games. This is in contrast with the classical case where the $n$-th level of the sum-of-squares hierarchy is equivalent to brute-force enumeration of all possible solutions. ","Algorithms, Bounds, and Strategies for Entangled XOR Games"
48,948937365640265728,29000998,Debashis Ghosh,['New paper out from our group on arxiv about a new data-adaptive approach to causal inference:    \n<LINK>'],https://arxiv.org/abs/1801.00816,"In most nonrandomized observational studies, differences between treatment groups may arise not only due to the treatment but also because of the effect of confounders. Therefore, causal inference regarding the treatment effect is not as straightforward as in a randomized trial. To adjust for confounding due to measured covariates, a variety of methods based on the potential outcomes framework are used to estimate average treatment effects. One of the key assumptions is treatment positivity, which states that the probability of treatment is bounded away from zero and one for any possible combination of the confounders. Methods for performing causal inference when this assumption is violated are relatively limited. In this article, we discuss a new balance-related condition involving the convex hulls of treatment groups, which I term relaxed covariate overlap. An advantage of this concept is that it can be linked to a concept from machine learning, termed the margin. Introduction of relaxed covariate overlap leads to an approach in which one can perform causal inference in a three-step manner. The methodology is illustrated with two examples. ",Relaxed covariate overlap and margin-based causal effect estimation
49,948823828398071809,10046412,fwilhelm,"['New paper: If you want to measure parity with low overhead, turn up the volume (and have an aspiring rock star as first author).\n<LINK>']",https://arxiv.org/abs/1801.00713,"Robust high-fidelity parity measurment is an important operation in many applications of quantum computing. In this work we show how in a circuit-QED architecture, one can measure parity in a single shot at very high contrast by taking advantage of the nonlinear behavior of a strongly driven microwave cavity coupled to one or multiple qubits. We work in a nonlinear dispersive regime treated in an exact dispersive transformation. We show that appropriate tuning of experimental parameters leads to very high contrast in the cavity and therefore to a high efficiency parity readout with a microwave photon counter or another amplitude detector. These tuning conditions are based on nonlinearity and are hence more robust than previously described linear tuning schemes. In the first part of the paper we show in detail how to achieve this for two qubit parity measurements and extend this to $N$ qubits in the second part of the paper. We also study the QNDness of the protocol. ",Nonlinear Parity Readout with a Microwave Photodetector
50,948555945394786305,735386827578875904,siegfried Vanaverbek,"[""I am happy to be one of the 200 authors of the new paper on Boyajian's star which is released on archiv today:\n\n<LINK>""]",https://arxiv.org/abs/1801.00732,"We present a photometric detection of the first brightness dips of the unique variable star KIC 8462852 since the end of the Kepler space mission in 2013 May. Our regular photometric surveillance started in October 2015, and a sequence of dipping began in 2017 May continuing on through the end of 2017, when the star was no longer visible from Earth. We distinguish four main 1-2.5% dips, named ""Elsie,"" ""Celeste,"" ""Skara Brae,"" and ""Angkor"", which persist on timescales from several days to weeks. Our main results so far are: (i) there are no apparent changes of the stellar spectrum or polarization during the dips; (ii) the multiband photometry of the dips shows differential reddening favoring non-grey extinction. Therefore, our data are inconsistent with dip models that invoke optically thick material, but rather they are in-line with predictions for an occulter consisting primarily of ordinary dust, where much of the material must be optically thin with a size scale <<1um, and may also be consistent with models invoking variations intrinsic to the stellar photosphere. Notably, our data do not place constraints on the color of the longer-term ""secular"" dimming, which may be caused by independent processes, or probe different regimes of a single process. ",The First Post-Kepler Brightness Dips of KIC 8462852
51,948552259343876097,21011627,"John Bochanski, Ph.D.",['Looking for fundamental properties of co-moving stars in #Gaia ? Check out our new paper! <LINK>'],http://arxiv.org/abs/1801.00537,"We have estimated fundamental parameters for a sample of co-moving stars observed by $Gaia$ and identified by Oh et al. (2017). We matched the $Gaia$ observations to the 2MASS and WISE catalogs and fit MIST isochrones to the data, deriving estimates of the mass, radius, [Fe/H], age, distance and extinction to 9,754 stars in the original sample of 10,606 stars. We verify these estimates by comparing our new results to previous analyses of nearby stars, examining fiducial cluster properties, and estimating the power-law slope of the local present-day mass function. A comparison to previous studies suggests that our mass estimates are robust, while metallicity and age estimates are increasingly uncertain. We use our calculated masses to examine the properties of binaries in the sample, and show that separation of the pairs dominates the observed binding energies and expected lifetimes. ",Fundamental Properties of Co-Moving Stars observed by $Gaia$
52,948468167038390273,40639812,Colin Cotter,['New paper on how to extend the PV-conserving compatible finite element approach for shallow water equations to domains with boundaries. <LINK>'],https://arxiv.org/abs/1801.00691,"We describe an energy-enstrophy conserving discretisation for the rotating shallow water equations with slip boundary conditions. This relaxes the assumption of boundary-free domains (periodic solutions or the surface of a sphere, for example) in the energy-enstrophy conserving formulation of McRae and Cotter (2014). This discretisation requires extra prognostic vorticity variables on the boundary in addition to the prognostic velocity and layer depth variables. The energy-enstrophy conservation properties hold for any appropriate set of compatible finite element spaces defined on arbitrary meshes with arbitrary boundaries. We demonstrate the conservation properties of the scheme with numerical solutions on a rotating hemisphere. ","Energy-enstrophy conserving compatible finite element schemes for the
  rotating shallow water equations with slip boundary conditions"
53,948386675293646848,3132910891,Delia Milliron,"['New paper on the arxiv, collaboration with @UMNews engineering:\nTuning Nanocrystal Surface Depletion Toward Enhanced Film Conductivity \n<LINK>']",http://arxiv.org/abs/1801.00731,"Electron conduction through bare metal oxide nanocrystal (NC) films is hindered by surface depletion regions resulting from the presence of surface states. We control the radial dopant distribution in tin-doped indium oxide (ITO) NCs as a means to manipulate the NC depletion width. We find in films of ITO NCs of equal overall dopant concentration that those with dopant-enriched surfaces show decreased depletion width and increased conductivity. Variable temperature conductivity data shows electron localization length increases and associated depletion width decreases monotonically with increased density of dopants near the NC surface. We calculate band profiles for NCs of differing radial dopant distributions and, in agreement with variable temperature conductivity fits, find NCs with dopant-enriched surfaces have narrower depletion widths and longer localization lengths than those with dopant-enriched cores. Following amelioration of NC surface depletion by atomic layer deposition of alumina, all films of equal overall dopant concentration have similar conductivity. Variable temperature conductivity measurements on alumina-capped films indicate all films behave as granular metals. Herein, we conclude that dopant-enriched surfaces decrease the near-surface depletion region, which directly increases the electron localization length and conductivity of NC films. ","Tuning Nanocrystal Surface Depletion by Controlling Dopant Distribution
  as a Route Toward Enhanced Film Conductivity"
54,963732234661490688,311082384,Robert M. Gower üá∫üá¶,"['What the wave? Check out our new paper on ""Learning about random media  from near-surface backscattering: using machine learning to measure  particle size and concentration""  <LINK>. Congrats to #arturgower, Jonathan Deakin, WilliamJ Parnell, and I David Abrahams <LINK>', 'Comes with a Julia package for generating fancy multiple scattering simulations: https://t.co/Bm2ZJolmEe']",https://arxiv.org/abs/1801.05490,"To what extent can particulate random media be characterised using direct wave backscattering from a single receiver/source? Here, in a two dimensional setting, we show using a machine learning approach that both the particle radius and concentration can be accurately measured when the boundary condition on the particles is of Dirichlet type. Although the methods we introduce could be applied to any particle type. In general backscattering is challenging to interpret for a wide range of particle concentrations, because multiple scattering cannot be ignored, except in the very dilute range. Across the concentration range from 1% to 20% we find that the mean backscattered wave field is sufficient to accurately determine the concentration of particles. However, to accurately determine the particle radius, the second moment, or average intensity, of the backscattering is necessary. We are also able to determine what is the ideal frequency range to measure a broad range of particles sizes. To get rigorous results with supervised machine learning requires a large, highly precise, dataset of backscattered waves from an infinite half-space filled with particles. We are able to create this dataset by introducing a numerical approach which accurately approximates the backscattering from an infinite half-space. ","Characterising particulate random media from near-surface
  backscattering: a machine learning approach to predict particle size and
  concentration"
55,963316526920159232,215360052,Jonas L. Juul,['New paper online: Isospectral discrete and quantum graphs with the same flip counts and nodal counts <LINK> with Chris Joyner (QMUL). We show that isospectral graphs cannot be resolved using only their (Laplacian) nodal counts and flip counts.'],https://arxiv.org/abs/1801.05246,"The existence of non-isomorphic graphs which share the same Laplace spectrum (to be referred to as isospectral graphs) leads naturally to the following question: What additional information is required in order to resolve isospectral graphs? It was suggested by Band, Shapira and Smilansky that this might be achieved by either counting the number of nodal domains or the number of times the eigenfunctions change sign (the so-called flip count). Recently examples of (discrete) isospectral graphs with the same flip count and nodal count have been constructed by K. Ammann by utilising Godsil-McKay switching. Here we provide a simple alternative mechanism that produces systematic examples of both discrete and quantum isospectral graphs with the same flip and nodal counts. ","Isospectral discrete and quantum graphs with the same flip counts and
  nodal counts"
56,959830283838197765,150595475,Hussain Gohar,"['Our recent research paper, <LINK>, got featured in New Scientist science magazine.\n<LINK>']",https://arxiv.org/abs/1801.09660,"We investigate the generalized uncertainty principle (GUP) corrections to the entropy content and the information flux of black holes, as well as the corrections to the sparsity of the Hawking radiation at the late stages of evaporation. We find that due to these quantum gravity motivated corrections, the entropy flow per particle reduces its value on the approach to the Planck scale due to a better accuracy in counting the number of microstates. We also show that the radiation flow is no longer sparse when the mass of a black hole approaches Planck mass which is not the case for non-GUP calculations. ","Generalized uncertainty principle impact onto the black holes
  information flux and the sparsity of Hawking radiation"
57,956991416122728451,505807051,Licheng Yu,"['Our new work ""MAttNet: Modular Attention Network for Referring Expression Comprehension"" by @mohitban47 @unccs @AdobeResearch \nPaper: <LINK>\nDemo: <LINK> <LINK>']",https://arxiv.org/abs/1801.08186,"In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo and code are provided. ","MAttNet: Modular Attention Network for Referring Expression
  Comprehension"
58,955616388722212865,3160301736,Andr√©s A. Plazas Malag√≥n,"[""New paper out! JPL's Precision Projector Laboratory at Caltech: <LINK>""]",https://arxiv.org/abs/1801.06599,"As astronomical observations from space benefit from improved sensitivity, the effectiveness of scientific programs is becoming limited by systematics that often originate in poorly understood image sensor behavior. Traditional, bottom-up detector characterization methods provide one way to model underlying detector physics, and generate ever more faithful numerical simulations, but this approach is vulnerable to preconceptions and over-simplification. The alternative top-down approach is laboratory emulation, which enables observation, calibration, and analysis scenarios to be tested without relying on a complete understanding of the underlying physics. This complements detector characterization and simulation efforts by testing their validity. We describe a laboratory facility and experimental testbed that supports the emulation of a wide range of mission concepts such as gravitational weak lensing measurements by WFIRST and high precision spectrophotometry of transiting exoplanets by JWST. An Offner relay projects readily customizable ""scenes"" (e.g. stars, galaxies, spectra) with very low optical aberration over the full area of a typical optical or near infrared image sensor. f/8 and slower focal ratios may be selected, spanning those of most proposed space missions and approximating the point spread function (PSF) size of seeing limited ground based surveys. Diffraction limited PSFs are projected over a wide field of view and wavelength range to deliver highly predictable image patterns down to sub-pixel scales with stable intensity and fine motion control. The testbed enables realistic validation of detector performance on science-like images, which aids mission design and survey strategy, as well as targeted investigations of various detector effects. ","Precision Projector Laboratory: Detector Characterization with an
  Astronomical Emulation Testbed"
59,957957752357187584,4639078397,John Wise,"['JHU postdoc L. Corlies led a study where we compared first galaxy simulation data to ultra-faint dwarfs, in particular how metallicities can tell us about its early SF history.\n<LINK> <LINK>']",https://arxiv.org/abs/1801.08569,"Ultrafaint dwarf galaxies (UFDs) are typically assumed to have simple, stellar populations with star formation ending at reionization. Yet as the observations of these galaxies continue to improve, their star formation histories (SFHs) are revealed to be more complicated than previously thought. In this paper, we study how star formation, chemical enrichment, and mixing proceed in small, dark matter halos at early times using a high-resolution, cosmological, hydrodynamical simulation. The goals are to inform the future use of analytic models and to explore observable properties of the simulated halos in the context of UFD data. Specifically, we look at analytic approaches that might inform metal enrichment within and beyond small galaxies in the early Universe. We find that simple assumptions for modeling the extent of supernova-driven winds agree with the simulation on average whereas inhomogeneous mixing and gas flows have a large effect on the spread in simulated stellar metallicities. In the context of the UFDs, this work demonstrates that simulations can form halos with a complex SFH and a large spread in the metallicity distribution function within a few hundred Myr in the early Universe. In particular, bursty and continuous star formation are seen in the simulation and both scenarios have been argued from the data. Spreads in the simulated metallicities, however remain too narrow and too metal-rich when compared to the UFDs. Future work is needed to help reduce these discrepancies and advance our interpretation of the data. ","Exploring Simulated Early Star Formation in the Context of the
  Ultrafaint Dwarf Galaxies"
60,956590679580463107,3321159693,rediet abebe,['Our work on Opinion Dynamics with Varying Susceptibility to Persuasion is on the arXiv! We find that targeting agents at the level of susceptibility leads to an interesting family of questions and can effectively optimize network opinions <LINK> @NetSciPhDs'],https://arxiv.org/abs/1801.07863,"A long line of work in social psychology has studied variations in people's susceptibility to persuasion -- the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation by interacting parties in a network: in addition to considering interventions that directly modify people's intrinsic opinions, it is also natural to consider interventions that modify people's susceptibility to persuasion. In this work, we adopt a popular model for social opinion dynamics, and we formalize the opinion maximization and minimization problems where interventions happen at the level of susceptibility. We show that modeling interventions at the level of susceptibility lead to an interesting family of new questions in network opinion dynamics. We find that the questions are quite different depending on whether there is an overall budget constraining the number of agents we can target or not. We give a polynomial-time algorithm for finding the optimal target-set to optimize the sum of opinions when there are no budget constraints on the size of the target-set. We show that this problem is NP-hard when there is a budget, and that the objective function is neither submodular nor supermodular. Finally, we propose a heuristic for the budgeted opinion optimization and show its efficacy at finding target-sets that optimize the sum of opinions compared on real world networks, including a Twitter network with real opinion estimates. ",Opinion Dynamics with Varying Susceptibility to Persuasion
61,954018861208109056,881959726958862337,Yuhuai (Tony) Wu,"['If you use K-FAC you only need to do 1 update (ACKTR), but if you use first order optimizer, you need to do 320 updates (PPO). AND 1 update by K-FAC still wins. This is what we (with @baaadas) find by comparing ACKTR vs. PPO vs. PPOKFAC. <LINK> <LINK>']",https://arxiv.org/abs/1801.05566,"In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR. ","An Empirical Analysis of Proximal Policy Optimization with
  Kronecker-factored Natural Gradients"
62,952865519442460672,3716338821,Mikko Tuomi,"['Our new paper led by @MatiasDiazM, @ExoplanetJJ (also @phillippro, @johannateske et al.) is out! We study the radial velocities of #HD26965 and find a consistent periodic signal. But is it caused by an #exoplanet orbiting the star with P = 42.36d? <LINK> <LINK>', 'We find that the signal is there beyond any reasonable doubt, supported by RV data from #HARPS, #HIRES, #PFS, and #CHIRON. If it is caused by a planet, which is what I think, it corresponds to a hot mini-Neptune with a minimum mass of 7Me. https://t.co/Od0LaIoAja', 'The planet, should it exist, is ""hot"" because it is located interior to the inner edge of the liquid-water habitable zone of #HD26965, i.e. the it is too hot for liquid water to exist on the planet\'s surface. Here is the current detection threshold for additional planets. https://t.co/CdBD8zQUw4', 'But there is a catch - #HD26965 shows evidence for activity-induced variations. There is a long-period magnetic activity cycle (CaII H&amp;K emission data from Mount Wilson) and possible rotation period of 37d, or 43d coinciding with the RV signal. https://t.co/qV27MjtnR8', 'Evidence for activity-induced periodicity of 43d is rather weak. However, because it is very close to the RV period, it is plausible that they are caused by the same process, i.e. stellar rotation. Literature gives rotation period of 37 - 38d. So have we found a planet at all?', 'Read more at about it in our paper accepted for publication in AJ. https://t.co/qQrY0ee4vG\n\n""This study serves as an excellent test case for future works that aim to detect small planets orbiting ‚ÄòSun-like‚Äô stars using radial velocity measurements."" https://t.co/UBH97hpc0X', ""#HD26965 is rather similar to the Sun, a bit less massive and less luminous. But we could not detect planets smaller than 10Me in the star's habitable zone. This is the typical and so unfortunate state-of-the-art precision with radial velocities. https://t.co/mT6TfEHNKj"", '@TomimPA @MatiasDiazM @ExoplanetJJ @phillippro @johannateske Who knows? I certainly am not aware anymore, and the concept of ""group"" is very vague as well. Not to mention, there is no such thing as ""certain"". So this is a very simple question to ask, but almost impossible to answer. The best answer I can give is ""dozens"".']",https://arxiv.org/abs/1801.03970,"We report the discovery of a radial velocity signal that can be interpreted as a planetary-mass candidate orbiting the K dwarf HD26965, with an orbital period of 42.364$\pm$0.015 days, or alternatively, as the presence of residual, uncorrected rotational activity in the data. Observations include data from HIRES, PFS, CHIRON, and HARPS, where 1,111 measurements were made over 16 years. Our best solution for HD26965 $b$ is consistent with a super-Earth that has a minimum mass of 6.92$\pm$0.79 M$_{\oplus}$ orbiting at a distance of 0.215$\pm$0.008 AU from its host star. We have analyzed the correlation between spectral activity indicators and the radial velocities from each instrument, showing moderate correlations that we include in our model. From this analysis, we recover a $\sim$38 day signal, which matches some literature values of the stellar rotation period. However, from independent Mt. Wilson HK data for this star, we find evidence for a significant 42 day signal after subtraction of longer period magnetic cycles, casting doubt on the planetary hypothesis for this period. Although our statistical model strongly suggests that the 42-day signal is Doppler in origin, we conclude that the residual effects of stellar rotation are difficult to fully model and remove from this dataset, highlighting the difficulties to disentangle small planetary signals and photospheric noise, particularly when the orbital periods are close to the rotation period of the star. This study serves as an excellent test case for future works that aim to detect small planets orbiting `Sun-like' stars using radial velocity measurements. ","The test case of HD26965: difficulties disentangling weak Doppler
  signals from stellar activity"
63,951717524122492928,749437811015704577,Ted Mackereth,"['My latest efforts now on the arXiv (<LINK>, with @rcrain_astro and other fine folks), in which we study the origins of element abundance patterns like that of our Galaxy in the EAGLE simulations #MilkyWay <LINK>', '@rcrain_astro We show that galaxies with these patterns are rare in the sims (~5%), which is important to understand as we start to understand MW better with surveys like #Gaia', 'In the paper, we relate this to an atypical dark matter accretion history in the galaxies that have these abund. trends - a prediction that can be tested in the MW']",https://arxiv.org/abs/1801.03593,"Spectroscopic surveys of the Galaxy reveal that its disc stars exhibit a spread in $\mathrm{[\alpha/Fe]}$ at fixed $\mathrm{[Fe/H]}$, manifest at some locations as a bimodality. The origin of these diverse, and possibly distinct, stellar populations in the Galactic disc is not well understood. We examine the Fe and $\alpha$-element evolution of 133 Milky Way-like galaxies from the EAGLE simulation, to investigate the origin and diversity of their $\mathrm{[\alpha/Fe]}$-$\mathrm{[Fe/H]}$ distributions. We find that bimodal $\mathrm{[\alpha/Fe]}$ distributions arise in galaxies whose gas accretion histories exhibit episodes of significant infall at both early and late times, with the former fostering more intense star formation than the latter. The shorter characteristic consumption timescale of gas accreted in the earlier episode suppresses its enrichment with iron synthesised by Type Ia SNe, resulting in the formation of a high-$\mathrm{[\alpha/Fe]}$ sequence. We find that bimodality in $\mathrm{[\alpha/Fe]}$ similar to that seen in the Galaxy is rare, appearing in approximately 5 percent of galaxies in our sample. We posit that this is a consequence of an early gas accretion episode requiring the mass accretion history of a galaxy's dark matter halo to exhibit a phase of atypically-rapid growth at early epochs. The scarcity of EAGLE galaxies exhibiting distinct sequences in the $\mathrm{[\alpha/Fe]}$-$\mathrm{[Fe/H]}$ plane may therefore indicate that the Milky Way's elemental abundance patterns, and its accretion history, are not representative of the broader population of $\sim L^\star$ disc galaxies. ",The origin of diverse $\alpha$-element abundances in galaxy discs
64,951178244757233664,775449094739197953,Ian Goodfellow,"['To gain some idea of the far future of ML security, we studied a simple toy problem called ""adversarial spheres,"" simulating a future where advanced ML models are extremely accurate. We find that even then, an adversary can still easily fool them. <LINK>', '@NicoChauvin74 We\'ve tried to do that, and so far we\'ve always been able to fool the ""garbage"" class detector']",https://arxiv.org/abs/1801.02774,"State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples. ",Adversarial Spheres
65,958443393394335746,51169895,Gianluca Stringhini,"['In our latest paper we had a look at the Russian Troll accounts published as part of the US congress investigation. We find that these accounts did not play a significant role in ""pushing"" news URLs on other platform, with the exception of RT links <LINK> 1/ <LINK>', 'We also find that many accounts were created right before important events such as the Brussels attacks and that many were used to spread divisive and hateful messages 2/2', '@tvidas There are two sides if the coin, the messages they post and the influence they have in spreading news. The negative result is more surprising in my opinion']",https://arxiv.org/abs/1801.09288,"Over the past couple of years, anecdotal evidence has emerged linking coordinated campaigns by state-sponsored actors with efforts to manipulate public opinion on the Web, often around major political events, through dedicated accounts, or ""trolls."" Although they are often involved in spreading disinformation on social media, there is little understanding of how these trolls operate, what type of content they disseminate, and most importantly their influence on the information ecosystem. In this paper, we shed light on these questions by analyzing 27K tweets posted by 1K Twitter users identified as having ties with Russia's Internet Research Agency and thus likely state-sponsored trolls. We compare their behavior to a random set of Twitter users, finding interesting differences in terms of the content they disseminate, the evolution of their account, as well as their general behavior and use of Twitter. Then, using Hawkes Processes, we quantify the influence that trolls had on the dissemination of news on social platforms like Twitter, Reddit, and 4chan. Overall, our findings indicate that Russian trolls managed to stay active for long periods of time and to reach a substantial number of Twitter users with their tweets. When looking at their ability of spreading news content and making it viral, however, we find that their effect on social platforms was minor, with the significant exception of news published by the Russian state-sponsored news outlet RT (Russia Today). ","Disinformation Warfare: Understanding State-Sponsored Trolls on Twitter
  and Their Influence on the Web"
66,952474710356488192,1896711848,Herbert Bos,"['New study shows systems security community does a *bad* job in benchmarking its research:  <LINK>\nIf benchmarking is bad, how can we trust the results?  Benchmarking crimes threaten the validity of the research. <LINK>']",https://arxiv.org/abs/1801.02381,"Properly benchmarking a system is a difficult and intricate task. Unfortunately, even a seemingly innocuous benchmarking mistake can compromise the guarantees provided by a given systems security defense and also put its reproducibility and comparability at risk. This threat is particularly insidious as it is generally not a result of malice and can easily go undetected by both authors and reviewers. Moreover, as modern defenses often trade off security for performance in an attempt to find an ideal design point in the performance-security space, the damage caused by benchmarking mistakes is increasingly worrisome. To analyze the magnitude of the phenomenon, we identify a set of 22 ""benchmarking crimes"" that threaten the validity of systems security evaluations and perform a survey of 50 defense papers published in top venues. To ensure the validity of our results, we perform the complete survey twice, with two independent readers. We find only a very small number of disagreements between readers, showing that our assessment of benchmarking crimes is highly reproducible. We show that benchmarking crimes are widespread even in papers published at tier-1 venues. We find that tier-1 papers commit an average of five benchmarking crimes and we find only a single paper in our sample that committed no benchmarking crimes. Moreover, we find that the scale of the problem is constant over time, suggesting that the community is not yet addressing it despite the problem being now more relevant than ever. This threatens the scientific process, which relies on reproducibility and comparability to ensure that published research advances the state of the art. We hope to raise awareness of these issues and provide recommendations to improve benchmarking quality and safeguard the scientific process in our community. ",Benchmarking Crimes: An Emerging Threat in Systems Security
