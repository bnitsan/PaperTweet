,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1161661512156684289,1158385581476515840,Allyson Ettinger,['New paper up! ‚ÄúWhat BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models‚Äù <LINK>. Tests that have elicited informative patterns in the brain prove quite useful for clarifying strengths and limitations of BERT pre-training.'],https://arxiv.org/abs/1907.13528,"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation. ","What BERT is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models"
1,1161465413215096832,28847789,Zsolt Kira,"['How can you deal with spatio-temporal domain shift in videos? \nAccepted #ICCV19 paper on video domain adaptation at a scale much larger than before, including new dataset and effective temporal alignment method.  \n\narXiv: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/1907.12743,"Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over ""Source only"" from 73.9% to 81.8% on ""HMDB --> UCF"", and 10.3% gain on ""Kinetics --> Gameplay""). The code and data are released at this http URL ",Temporal Attentive Alignment for Large-Scale Video Domain Adaptation
2,1161260162100936704,3393842567,"Shane Ross, PhD",['Cylinder transit boundaries close to become ellipsoids when friction is present. New paper summarizes geometry of transition &amp; escape across saddles in physical systems of many degrees of freedom w/ dissipation <LINK>\n\n#AppliedMath #DynamicalSystems #TubeDynamics <LINK>'],https://arxiv.org/abs/1907.10728,"Escape from a potential well can occur in different physical systems, such as capsize of ships, resonance transitions in celestial mechanics, and dynamic snap-through of arches and shells, as well as molecular reconfigurations in chemical reactions. The criteria and routes of escape in one-degree of freedom systems has been well studied theoretically with reasonable agreement with experiment. The trajectory can only transit from the hilltop of the one-dimensional potential energy surface. The situation becomes more complicated when the system has higher degrees of freedom since it has multiple routes to escape through an equilibrium of saddle-type, specifically, an index-1 saddle. This paper summarizes the geometry of escape across a saddle in some widely known physical systems with two degrees of freedom and establishes the criteria of escape providing both a methodology and results under the conceptual framework known as tube dynamics. These problems are classified into two categories based on whether the saddle projection and focus projection in the symplectic eigenspace are coupled or not when damping and/or gyroscopic effects are considered. To simplify the process, only the linearized system around the saddle points are analyzed. We define a transition region, $\mathcal{T}_h$, as the region of initial conditions of a given initial energy $h$ which transit from one side of a saddle to the other. We find that in conservative systems, the boundary of the transition region, $\partial \mathcal{T}_h$, is a cylinder, while in dissipative systems, $\partial \mathcal{T}_h$ is an ellipsoid. ","Geometry of escape and transition dynamics in the presence of
  dissipative and gyroscopic forces in two degree of freedom systems"
3,1160181701064167424,807530274,Morteza Karimzadeh,['Our new IEEE VAST/TVCG paper on a visual analytics system for social spambot identification &amp; labeling by leveraging group &amp; dynamic behaviour:\n <LINK>'],https://arxiv.org/abs/1907.13319,"Social media platforms such as Twitter are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve and evade traditional detection techniques. In this work, we propose VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling techniques, which offer new insights that enable the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We conducted a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool. ",VASSL: A Visual Analytics Toolkit for Social Spambot Labeling
4,1159090667940921344,3199605543,Afonso S. Bandeira,"['A new paper (together with Tim Kunisky) proving a lower bound on the\ndegree 4 sum-of-squares lower bound for Sherrington-Kirkpatrick\nHamiltonian, something we have been working on for a while!\n<LINK>']",https://arxiv.org/abs/1907.11686,"We show that, if $\mathbf{W} \in \mathbb{R}^{N \times N}_{\mathsf{sym}}$ is drawn from the gaussian orthogonal ensemble, then with high probability the degree 4 sum-of-squares relaxation cannot certify an upper bound on the objective $N^{-1} \cdot \mathbf{x}^\top \mathbf{W} \mathbf{x}$ under the constraints $x_i^2 - 1 = 0$ (i.e. $\mathbf{x} \in \{ \pm 1 \}^N$) that is asymptotically smaller than $\lambda_{\max}(\mathbf{W}) \approx 2$. We also conjecture a proof technique for lower bounds against sum-of-squares relaxations of any degree held constant as $N \to \infty$, by proposing an approximate pseudomoment construction. ","A Tight Degree 4 Sum-of-Squares Lower Bound for the
  Sherrington-Kirkpatrick Hamiltonian"
5,1159090432443375616,3199605543,Afonso S. Bandeira,"['A new paper (with Yunzi Ding, Tim Kunisky, and Alex Wein) on\nsubexponential-time algorithms for Sparse PCA, with matching low\ndegree likelihood ratio evidence of hardness!\n<LINK>']",https://arxiv.org/abs/1907.11635,"We study the computational cost of recovering a unit-norm sparse principal component $x \in \mathbb{R}^n$ planted in a random matrix, in either the Wigner or Wishart spiked model (observing either $W + \lambda xx^\top$ with $W$ drawn from the Gaussian orthogonal ensemble, or $N$ independent samples from $\mathcal{N}(0, I_n + \beta xx^\top)$, respectively). Prior work has shown that when the signal-to-noise ratio ($\lambda$ or $\beta\sqrt{N/n}$, respectively) is a small constant and the fraction of nonzero entries in the planted vector is $\|x\|_0 / n = \rho$, it is possible to recover $x$ in polynomial time if $\rho \lesssim 1/\sqrt{n}$. While it is possible to recover $x$ in exponential time under the weaker condition $\rho \ll 1$, it is believed that polynomial-time recovery is impossible unless $\rho \lesssim 1/\sqrt{n}$. We investigate the precise amount of time required for recovery in the ""possible but hard"" regime $1/\sqrt{n} \ll \rho \ll 1$ by exploring the power of subexponential-time algorithms, i.e., algorithms running in time $\exp(n^\delta)$ for some constant $\delta \in (0,1)$. For any $1/\sqrt{n} \ll \rho \ll 1$, we give a recovery algorithm with runtime roughly $\exp(\rho^2 n)$, demonstrating a smooth tradeoff between sparsity and runtime. Our family of algorithms interpolates smoothly between two existing algorithms: the polynomial-time diagonal thresholding algorithm and the $\exp(\rho n)$-time exhaustive search algorithm. Furthermore, by analyzing the low-degree likelihood ratio, we give rigorous evidence suggesting that the tradeoff achieved by our algorithms is optimal. ",Subexponential-Time Algorithms for Sparse PCA
6,1158936724476555270,794346991627010048,Lauren Oakden-Rayner (Dr.Dr. ü•≥),"['New preprint: Exploring large scale public medical image datasets.\n\nThis paper analyses several large, famous datasets (CXR14 and MURA), and looks at the various problems created by labelling strategies, schemas, and lack of documentation.\n\n<LINK>', ""@VidurMahajan1 This paper is already submitted to a journal, but I'd like to read yours!""]",https://arxiv.org/abs/1907.12720,"Rationale and Objectives: Medical artificial intelligence systems are dependent on well characterised large scale datasets. Recently released public datasets have been of great interest to the field, but pose specific challenges due to the disconnect they cause between data generation and data usage, potentially limiting the utility of these datasets. Materials and Methods: We visually explore two large public datasets, to determine how accurate the provided labels are and whether other subtle problems exist. The ChestXray14 dataset contains 112,120 frontal chest films, and the MURA dataset contains 40,561 upper limb radiographs. A subset of around 700 images from both datasets was reviewed by a board-certified radiologist, and the quality of the original labels was determined. Results: The ChestXray14 labels did not accurately reflect the visual content of the images, with positive predictive values mostly between 10% and 30% lower than the values presented in the original documentation. There were other significant problems, with examples of hidden stratification and label disambiguation failure. The MURA labels were more accurate, but the original normal/abnormal labels were inaccurate for the subset of cases with degenerative joint disease, with a sensitivity of 60% and a specificity of 82%. Conclusion: Visual inspection of images is a necessary component of understanding large image datasets. We recommend that teams producing public datasets should perform this important quality control procedure and include a thorough description of their findings, along with an explanation of the data generating procedures and labelling rules, in the documentation for their datasets. ",Exploring large scale public medical image datasets
7,1158858183634558976,1003652696723873792,Max Gaspari,"[""New paper published with @esortom (excellent PhD student!) on detecting the imprints of 'SMBH shadows' with ALMA. Exciting new direct evidences of Chaotic Cold Accretion (CCA) in several massive galaxies! #alma #blackholes #CCA_rain\n<LINK> <LINK>""]",http://arxiv.org/abs/1907.13526,"To advance our understanding of the fuelling and feedback processes which power the Universe's most massive black holes, we require a significant increase in our knowledge of the molecular gas which exists in their immediate surroundings. However, the behaviour of this gas is poorly understood due to the difficulties associated with observing it directly. We report on a survey of 18 brightest cluster galaxies lying in cool cores, from which we detect molecular gas in the core regions of eight via carbon monoxide (CO), cyanide (CN) and silicon monoxide (SiO) absorption lines. These absorption lines are produced by cold molecular gas clouds which lie along the line of sight to the bright continuum sources at the galaxy centres. As such, they can be used to determine many properties of the molecular gas which may go on to fuel supermassive black hole accretion and AGN feedback mechanisms. The absorption regions detected have velocities ranging from -45 to 283 km s$^{-1}$ relative to the systemic velocity of the galaxy, and have a bias for motion towards the host supermassive black hole. We find that the CN N = 0 - 1 absorption lines are typically 10 times stronger than those of CO J = 0 - 1. This is due to the higher electric dipole moment of the CN molecule, which enhances its absorption strength. In terms of molecular number density CO remains the more prevalent molecule with a ratio of CO/CN $\sim 10$, similar to that of nearby galaxies. Comparison of CO, CN and HI observations for these systems shows many different combinations of these absorption lines being detected. ","Constraining cold accretion onto supermassive black holes: molecular gas
  in the cores of eight brightest cluster galaxies revealed by joint CO and CN
  absorption"
8,1158827811521667072,374186349,Nantia Makrynioti,"[""Our new paper with Ruy Ley-Wild and @vassalos 'sql4ml\nA declarative end-to-end workflow for machine learning' is now available on arXiv: (<LINK>)\nThe prototype implementation used in the paper is available here: <LINK>""]",https://arxiv.org/abs/1907.12415,"We present sql4ml, a system for expressing supervised machine learning (ML) models in SQL and automatically training them in TensorFlow. The primary motivation for this work stems from the observation that in many data science tasks there is a back-and-forth between a relational database that stores the data and a machine learning framework. Data preprocessing and feature engineering typically happen in a database, whereas learning is usually executed in separate ML libraries. This fragmented workflow requires from the users to juggle between different programming paradigms and software systems. With sql4ml the user can express both feature engineering and ML algorithms in SQL, while the system translates this code to an appropriate representation for training inside a machine learning framework. We describe our translation method, present experimental results from applying it on three well-known ML algorithms and discuss the usability benefits from concentrating the entire workflow on the database side. ",sql4ml A declarative end-to-end workflow for machine learning
9,1158698643597266945,1959874248,Jean Rizk,"['Our new paper, An Alternative Formulation of Coxian Phase-type Distributions: Application to Emergency Department Length of Stay, is available on <LINK>.']",https://arxiv.org/abs/1907.13489,"In this paper we present a new methodology to model patient transitions and length of stay in the emergency department using a series of conditional Coxian phase-type distributions, with covariates. We reformulate the Coxian models (standard Coxian, Coxian with multiple absorbing states, joint Coxian, and conditional Coxian) to take into account heterogeneity in patient characteristics such as arrival mode, time of admission and age. The approach differs from previous research in that it reduces the computational time, and it allows the inclusion of patient covariate information directly into the model. The model is applied to emergency department data from University Hospital Limerick in Ireland. ","An Alternative Formulation of Coxian Phase-type Distributions with
  Covariates: Application to Emergency Department Length of Stay"
10,1158391881388048384,959094316323885057,Murium,['Check out the preprint for our new RecSys paper Style Conditioned Recommendations to learn how to apply style transfer to recs for diversification <LINK> @KAryafar @ACMRecSys @anderton_tim #RecSys #MachineLearning <LINK>'],http://arxiv.org/abs/1907.12388,"We propose Style Conditioned Recommendations (SCR) and introduce style injection as a method to diversify recommendations. We use Conditional Variational Autoencoder (CVAE) architecture, where both the encoder and decoder are conditioned on a user profile learned from item content data. This allows us to apply style transfer methodologies to the task of recommendations, which we refer to as injection. To enable style injection, user profiles are learned to be interpretable such that they express users' propensities for specific predefined styles. These are learned via label-propagation from a dataset of item content, with limited labeled points. To perform injection, the condition on the encoder is learned while the condition on the decoder is selected per explicit feedback. Explicit feedback can be taken either from a user's response to a style or interest quiz, or from item ratings. In the absence of explicit feedback, the condition at the encoder is applied to the decoder. We show a 12% improvement on NDCG@20 over the traditional VAE based approach and an average 22% improvement on AUC across all classes for predicting user style profiles against our best performing baseline. After injecting styles we compare the user style profile to the style of the recommendations and show that injected styles have an average +133% increase in presence. Our results show that style injection is a powerful method to diversify recommendations while maintaining personal relevance. Our main contribution is an application of a semi-supervised approach that extends item labels to interpretable user profiles. ",Style Conditioned Recommendations
11,1157205825657720834,944291984675614721,Tobias de Jong,"['Our new paper on applying sub-pixel image registration, dimension reduction and cluster analysis to analyse and visualize spectroscopic (LEEM) data is now on arXiv!üòÉ  <LINK>\nWith @j_jobst and @sensemolen; @LeidenPhysics <LINK>', '@j_jobst @sensemolen @LeidenPhysics We show that using @dask_dev and #scipy, we can successfully (and fast!) register hundreds of spectroscopic electron microscopy images, despite noise and contrast inversions complicating the process.', ""Secondly, we apply #dask_ml's and @scikit_learn's PCA to reduce these hundreds of images a few color (!) images containing almost all information in the entire stack, enabling easy visualization."", 'Finally, we show that using a simple clustering algorithm, we can classify each pixel on the sample to different classes of spectra. Here, we show we can separate bi-, tri- and quadlayer graphene, as well as domain boundaries and step edges in bright field LEEM data. https://t.co/eSGIEOaWHj', 'All this, without any prior information on the spectra or spatial distribution! For Dark Field LEEM data, we can even distinguish different stacking orders (c.f. https://t.co/3WAT7rFIaz) https://t.co/rARyqpSzm6', 'A preliminary version of the code, using @dask_dev, @scikit_learn, @xarray_dev and some other parts of the SciPy stack in @ProjectJupyter notebooks is available: https://t.co/iSSDDgKEh9. The data used to showcase will still follow. #openscience']",https://arxiv.org/abs/1907.13510,"For many complex materials systems, low-energy electron microscopy (LEEM) offers detailed insights into morphology and crystallography by naturally combining real-space and reciprocal-space information. Its unique strength, however, is that all measurements can easily be performed energy-dependently. Consequently, one should treat LEEM measurements as multi-dimensional, spectroscopic datasets rather than as images to fully harvest this potential. Here we describe a measurement and data analysis approach to obtain such quantitative spectroscopic LEEM datasets with high lateral resolution. The employed detector correction and adjustment techniques enable measurement of true reflectivity values over four orders of magnitudes of intensity. Moreover, we show a drift correction algorithm, tailored for LEEM datasets with inverting contrast, that yields sub-pixel accuracy without special computational demands. Finally, we apply dimension reduction techniques to summarize the key spectroscopic features of datasets with hundreds of images into two single images that can easily be presented and interpreted intuitively. We use cluster analysis to automatically identify different materials within the field of view and to calculate average spectra per material. We demonstrate these methods by analyzing bright-field and dark-field datasets of few-layer graphene grown on silicon carbide and provide a high-performance Python implementation. ","Quantitative analysis of spectroscopic Low Energy Electron Microscopy
  data: High-dynamic range imaging, drift correction and cluster analysis"
12,1156802754662338560,901142962758758400,Hang-Hyun Jo,"['Our paper ""Burst-tree decomposition of time series reveals the structure of temporal correlations"" (with @takayukihir &amp; @bolozna) is available at <LINK> | We propose a new method of analyzing the bursty event sequences that turns event sequences into trees. <LINK>']",https://arxiv.org/abs/1907.13556,"Comprehensive characterization of non-Poissonian, bursty temporal patterns observed in various natural and social processes is crucial to understand the underlying mechanisms behind such temporal patterns. Among them bursty event sequences have been studied mostly in terms of interevent times (IETs), while the higher-order correlation structure between IETs has gained very little attention due to the lack of a proper characterization method. In this paper we propose a method of decomposing an event sequence into a set of IETs and a burst tree, which exactly captures the structure of temporal correlations that is entirely missing in the analysis of IET distributions. We apply the burst-tree decomposition method to various datasets and analyze the structure of the revealed burst trees. In particular, we observe that event sequences show similar burst-tree structure, such as heavy-tailed burst size distributions, despite of very different IET distributions. The burst trees allow us to directly characterize the preferential and assortative mixing structure of bursts responsible for the higher-order temporal correlations. We also show how to use the decomposition method for the systematic investigation of such higher-order correlations captured by the burst trees in the framework of randomized reference models. Finally, we devise a simple kernel-based model for generating event sequences showing appropriate higher-order temporal correlations. Our method is a tool to make the otherwise overwhelming analysis of higher-order correlations in bursty time series tractable by turning it into the analysis of a tree structure. ","Burst-tree decomposition of time series reveals the structure of
  temporal correlations"
13,1156721631089373184,4905145794,Pranav Gokhale,"['Our new @EPiQCExpedition paper, ‚ÄúMinimizing State Preparations in Variational Quantum Eigensolver by Partitioning into Commuting Families‚Äù is now on arXiv! <LINK>. We apply a systems perspective to reduce the cost of VQE.']",https://arxiv.org/abs/1907.13623,"Variational quantum eigensolver (VQE) is a promising algorithm suitable for near-term quantum machines. VQE aims to approximate the lowest eigenvalue of an exponentially sized matrix in polynomial time. It minimizes quantum resource requirements both by co-processing with a classical processor and by structuring computation into many subproblems. Each quantum subproblem involves a separate state preparation terminated by the measurement of one Pauli string. However, the number of such Pauli strings scales as $N^4$ for typical problems of interest--a daunting growth rate that poses a serious limitation for emerging applications such as quantum computational chemistry. We introduce a systematic technique for minimizing requisite state preparations by exploiting the simultaneous measurability of partitions of commuting Pauli strings. Our work encompasses algorithms for efficiently approximating a MIN-COMMUTING-PARTITION, as well as a synthesis tool for compiling simultaneous measurement circuits. For representative problems, we achieve 8-30x reductions in state preparations, with minimal overhead in measurement circuit cost. We demonstrate experimental validation of our techniques by estimating the ground state energy of deuteron on an IBM Q 20-qubit machine. We also investigate the underlying statistics of simultaneous measurement and devise an adaptive strategy for mitigating harmful covariance terms. ","Minimizing State Preparations in Variational Quantum Eigensolver by
  Partitioning into Commuting Families"
14,1156470182472224768,2999702157,Anton Ilderton,['Our new paper is on the @arXiv! Written with @bkingspeaking and Alexander MacLeod as part of our @EPSRC grant. #lasers #physics #lasers \n\n<LINK> <LINK>'],https://arxiv.org/abs/1907.12835,"We consider the absorption of probe photons by electrons in the presence of an intense, pulsed, background field. Our analysis reveals an interplay between regularisation and gauge invariance which distinguishes absorption from its crossing-symmetric processes, as well as a physical interpretation of absorption in terms of degenerate processes in the weak field limit. In the strong field limit we develop a locally constant field approximation (LCFA) for absorption which also exhibits new features. We benchmark the LCFA against exact analytical calculations and explore its regime of validity. Pulse shape effects are also investigated, as well as infra-red and collinear limits of the absorption process. ",Absorption cross section in an intense plane wave background
15,1156215996664942592,204261944,Matthew Kenworthy,"[""Awesome to see Sam Mellon's paper on ~70 new variable stars discovered in #bRing data! No new #exoring systems though... <LINK> <LINK>""]",https://arxiv.org/abs/1907.11927,"Besides monitoring the bright star $\beta$ Pic during the near transit event for its giant exoplanet, the $\beta$ Pictoris b Ring (bRing) observatories at Siding Springs Observatory, Australia and Sutherland, South Africa have monitored the brightnesses of bright stars ($V$ $\simeq$ 4--8 mag) centered on the south celestial pole ($\delta$ $\leq$ -30$^{\circ}$) for approximately two years. Here we present a comprehensive study of the bRing time series photometry for bright southern stars monitored between 2017 June and 2019 January. Of the 16762 stars monitored by bRing, 353 of them were found to be variable. Of the variable stars, 80% had previously known variability and 20% were new variables. Each of the new variables was classified, including 3 new eclipsing binaries (HD 77669, HD 142049, HD 155781), 26 $\delta$ Scutis, 4 slowly pulsating B stars, and others. This survey also reclassified four stars based on their period of pulsation, light curve, spectral classification, and color-magnitude information. The survey data were searched for new examples of transiting circumsecondary disk systems, but no candidates were found. ",Bright Southern Variable Stars in the bRing Survey
16,1156031244225789953,28535459,Dougal Mackey,"['An exciting new S5 paper on arXiv today, led by Sergey Koposov - discovery of a 1700 km/s (!) star ejected from the Galactic centre. This lets us place quite precise constraints on the geometry and kinematics of the Milky Way.\n\n<LINK>', 'A big cheer for Australian facilities - the star was discovered with the 4m Anglo-Australian Telescope and verified with the 2.3m ANU telescope, both at Siding Spring Observatory.']",https://arxiv.org/abs/1907.11725,"We present the serendipitous discovery of the fastest Main Sequence hyper-velocity star (HVS) by the Southern Stellar Stream Spectroscopic Survey (S5). The star S5-HVS1 is a $\sim 2.35$ M$_\odot$ A-type star located at a distance of $\sim 9$ kpc from the Sun and has a heliocentric radial velocity of $1017\pm 2.7$ km/s without any signature of velocity variability. The current 3-D velocity of the star in the Galactic frame is $1755\pm50$ km/s. When integrated backwards in time, the orbit of the star points unambiguously to the Galactic Centre, implying that S5-HVS1 was kicked away from Sgr A* with a velocity of $\sim 1800$ km/s and travelled for $4.8$ Myr to its current location. This is so far the only HVS confidently associated with the Galactic Centre. S5-HVS1 is also the first hyper-velocity star to provide constraints on the geometry and kinematics of the Galaxy, such as the Solar motion $V_{y,\odot}= 246.1\pm 5.3$ km/s or position $R_0=8.12\pm 0.23$ kpc. The ejection trajectory and transit time of S5-HVS1 coincide with the orbital plane and age of the annular disk of young stars at the Galactic centre, and thus may be linked to its formation. With the S5-HVS1 ejection velocity being almost twice the velocity of other hyper-velocity stars previously associated with the Galactic Centre, we question whether they have been generated by the same mechanism or whether the ejection velocity distribution has been constant over time. ","The Great Escape: Discovery of a nearby 1700 km/s star ejected from the
  Milky Way by Sgr A*"
17,1155870529242333184,307826617,Kev Abazajian ‚§∑‚è≥üåé,"['New paper today‚Äî Hidden Treasures: Sterile Neutrino #darkmatter can be cold or warm, or a fraction of the dark matter through several production mechanisms. 3.55 keV signal could be CDM, WDM or CWDM &amp; œÉ8 problem could be due to ~80 eV steriles <LINK> <LINK>']",https://arxiv.org/abs/1907.11696,"We discuss numerous mechanisms for production of sterile neutrinos, which can account for all or a fraction of dark matter, and which can range from warm to effectively cold dark matter, depending on the cosmological scenario. We investigate production by Higgs boson decay, $(B-L)$ gauge boson production at high temperature, as well as production via resonant and nonresonant neutrino oscillations. We calculate the effects on structure formation in these models, some for the first time. If two populations of sterile neutrinos, one warm and one cold, were produced by different mechanisms, or if sterile neutrinos account for only a fraction of dark matter, while the remainder is some other cold dark matter particle, the resulting multi-component dark matter may alleviate some problems in galaxy formation. We examine the X-ray constraints and the candidate signal at 3.5 keV. Finally, we also show that the $\sigma_8$ problem can be a signature of fractional dark matter in the form of sterile neutrinos in several mechanisms. ","Hidden Treasures: sterile neutrinos as dark matter with miraculous
  abundance, structure formation for different production mechanisms, and a
  solution to the sigma-8 problem"
18,1155849806063030272,1030005672090451969,Catherine Walsh,['New paper by @chreistrup on the chemical origin of comets ‚òÑÔ∏è the CO snowline is a sweetspot for cooking üç≥ interstellar ices ‚ùÑ\n#astrochemistry\n#comets\n#planetformation\n<LINK>'],https://arxiv.org/abs/1907.11255,"[Abridged] With a growing number of molecules observed in many comets, and an improved understanding of chemical evolution in protoplanetary disk midplanes, comparisons can be made between models and observations that could potentially constrain the formation histories of comets. A $\chi^{2}$-method was used to determine maximum likelihood surfaces for 14 different comets that formed at a given time (up to 8 Myr) and place (out to beyond the CO iceline) in the pre-solar nebula midplane. This was done using observed volatile abundances for the 14 comets and the evolution of volatile abundances from chemical modelling of disk midplanes. Considering all parent species (ten molecules) in a scenario that assumed reset initial chemistry, the $\chi^{2}$ likelihood surfaces show a characteristic trail in the parameter space with high likelihood of formation around 30 AU at early times and 12 AU at later times for ten comets. This trail roughly traces the vicinity of the CO iceline in time. The formation histories for all comets were thereby constrained to the vicinity of the CO iceline, assuming that the chemistry was partially reset early in the pre-solar nebula. This is found, both when considering carbon-, oxygen-, and sulphur-bearing molecules (ten in total), and when only considering carbon- and oxygen-bearing molecules (seven in total). Since these 14 comets did not previously fall into the same taxonomical categories together, this chemical constraint may be proposed as an alternative taxonomy for comets. Based on the most likely time for each of these comets to have formed during the disk chemical evolution, a formation time classification for the 14 comets is suggested. ","Cometary compositions compared with protoplanetary disk midplane
  chemical evolution. An emerging chemical evolution taxonomy for comets"
19,1155426141278232579,1055171991425241094,Dr.A.G.Askitop,['New paper on optically programmable coupling in opticak lattices of polariton condensates\n<LINK>'],https://arxiv.org/abs/1907.08580,"We demonstrate deterministic control of the nearest and next-nearest neighbor coupling in the unit cell of a square lattice of microcavity exciton-polariton condensates. We tune the coupling in a continuous and reversible manner by optically imprinting potential barriers of variable height, in the form of spatially localized incoherent exciton reservoirs that modify the particle flow between condensates. By controlling the couplings in a $2\times2$ polariton cluster, we realize ferromagnetic, anti-ferromagnetic and paired ferromagnetic phases. Our approach paves the way towards simulating complex condensed matter phases through precise control of the individual couplings in networks of optical nonlinear oscillators. ",Optical control of synchronous phases in a programmable polariton cell
20,1155150440914460672,900607970794491904,Eyal Bairey,['An open quantum system reaches a steady state. Can measurements of this steady state reveal the Hamiltonian and dissipation leading to it? Check out our new paper: <LINK>'],https://arxiv.org/abs/1907.11154,"Recent works have shown that generic local Hamiltonians can be efficiently inferred from local measurements performed on their eigenstates or thermal states. Realistic quantum systems are often affected by dissipation and decoherence due to coupling to an external environment. This raises the question whether the steady states of such open quantum systems contain sufficient information allowing for full and efficient reconstruction of the system's dynamics. We find that such a reconstruction is possible for generic local Markovian dynamics. We propose a recovery method that uses only local measurements; for systems with finite-range interactions, the method recovers the Lindbladian acting on each spatial domain using only observables within that domain. We numerically study the accuracy of the reconstruction as a function of the number of measurements, type of open-system dynamics and system size. Interestingly, we show that couplings to external environments can in fact facilitate the reconstruction of Hamiltonians composed of commuting terms. ",Learning the dynamics of open quantum systems from their steady states
21,1155101427582275584,2441942726,Aleksas Mazeliauskas,['My new paper with Vytautas Vislavicius on blast wave fits with resonance decay feed down:\n<LINK>'],https://arxiv.org/abs/1907.11059,"We present a new approach to take into account resonance decays in the blast-wave model fits of identified hadron spectra. Thanks to pre-calculated decayed particle spectra, we are able to extract, in a matter of seconds, the multiplicity dependence of the single freeze-out temperature $T_{\rm fo}$, average fluid velocity $\left<\beta_{\rm T}\right>$, velocity exponent $n$, and the volume $dV/dy$ of an expanding fireball. In contrast to blast-wave fits without resonance feed-down, our approach results in a freeze-out temperature of $T_{\rm fo}\approx 150\,\text{MeV}$, which has only weak dependence on multiplicity and collision system. Finally, we discuss separate chemical and kinetic freeze-outs separated by partial chemical equilibrium. ","Temperature and fluid velocity on the freeze-out surface from $\pi$,
  $K$, $p$ spectra in pp, p--Pb and Pb--Pb collisions"
22,1154730150854049792,1556664198,Kyle Cranmer,"['New paper describing our software tool: MadMiner: Machine learning-based inference for particle physics.\nWith Johann Brehmer, Felix Kling, and Irina Espejo #HEPML\n<LINK> <LINK>', 'And there are some physics results. EFT constraints in ttH. Comparison with rate only, pT,yy distribution, the leading interference effects on fully differential x-sec (Sally), and the full story with non-linear effects (Alices). Also demonstrates incorporating systematics. https://t.co/eyHXFzlCPL']",https://arxiv.org/abs/1907.10621,"Precision measurements at the LHC often require analyzing high-dimensional event data for subtle kinematic signatures, which is challenging for established analysis methods. Recently, a powerful family of multivariate inference techniques that leverage both matrix element information and machine learning has been developed. This approach neither requires the reduction of high-dimensional data to summary statistics nor any simplifications to the underlying physics or detector response. In this paper we introduce MadMiner, a Python module that streamlines the steps involved in this procedure. Wrapping around MadGraph5_aMC and Pythia 8, it supports almost any physics process and model. To aid phenomenological studies, the tool also wraps around Delphes 3, though it is extendable to a full Geant4-based detector simulation. We demonstrate the use of MadMiner in an example analysis of dimension-six operators in ttH production, finding that the new techniques substantially increase the sensitivity to new physics. ",MadMiner: Machine learning-based inference for particle physics
23,1154660048309772289,187837457,Konstantinos Drossos,['Happy to share our new paper on language modelling for sound event detection! \n\nPaper on arXiv: <LINK>\nCode on GitHub: <LINK>\n\n#soundevents #PyTorch #audio #soundeventdetection #dsp #signalprocessing #sound'],https://arxiv.org/abs/1907.08506,"A sound event detection (SED) method typically takes as an input a sequence of audio frames and predicts the activities of sound events in each frame. In real-life recordings, the sound events exhibit some temporal structure: for instance, a ""car horn"" will likely be followed by a ""car passing by"". While this temporal structure is widely exploited in sequence prediction tasks (e.g., in machine translation), where language models (LM) are exploited, it is not satisfactorily modeled in SED. In this work we propose a method which allows a recurrent neural network (RNN) to learn an LM for the SED task. The method conditions the input of the RNN with the activities of classes at the previous time step. We evaluate our method using F1 score and error rate (ER) over three different and publicly available datasets; the TUT-SED Synthetic 2016 and the TUT Sound Events 2016 and 2017 datasets. The obtained results show an increase of 9% and 2% at the F1 (higher is better) and a decrease of 7% and 2% at ER (lower is better) for the TUT Sound Events 2016 and 2017 datasets, respectively, when using our method. On the contrary, with our method there is a decrease of 4% at F1 score and an increase of 7% at ER for the TUT-SED Synthetic 2016 dataset. ","Language Modelling for Sound Event Detection with Teacher Forcing and
  Scheduled Sampling"
24,1154549893253353472,1376287471,Tharindu Jayasinghe,"['New paper day! We have completed a variability analysis of the southern sky using archival V-band data from @SuperASASSN. Here we took a look at ~30 million V&lt;17 mag sources in the South and systematically searched them for variability: <LINK>', 'We came up with a catalog of ~220,000 variable stars (not including the TESS CVZ which we covered in a previous paper), out of which ~88,000 sources were new discoveries (shown below)! Look at the abundance of variables towards the Galactic plane üòç https://t.co/D8xtqjmRAS', 'When we compare the newly discovered variables to those that were discovered by previous surveys, we find that most high-amplitude variables (e.g., Miras!) and strongly periodic sources (e.g., Cepheids, RR Lyrae) have already been discovered! https://t.co/D1wGmNvsz1', 'Most of our new discoveries are low amplitude LPVs (~48,000). We also find ~23,000 new eclipsing binaries, ~10,200 rotational variables and ~2,200 Delta-scuti variables! https://t.co/fawMdDPeTd', 'You can get more information on these variables (and their light curves) from our newly revamped variable stars database (https://t.co/bBskmhsyE0) and query the database of ~30 million southern sources on our photometry database (https://t.co/FIiNn2waIn).', ""What's next: we are working on carrying out this analysis for all the sources in the north, as well as to fill some of the gaps in the APASS catalog which we used as our input source list! Stay tuned for more all-sky variability goodness in our next paper :)""]",https://arxiv.org/abs/1907.10609,"The All-Sky Automated Survey for Supernovae (ASAS-SN) provides long baseline (${\sim}4$ yrs) light curves for sources brighter than V$\lesssim17$ mag across the whole sky. As part of our effort to characterize the variability of all the stellar sources visible in ASAS-SN, we have produced ${\sim}30.1$ million V-band light curves for sources in the southern hemisphere using the APASS DR9 catalog as our input source list. We have systematically searched these sources for variability using a pipeline based on random forest classifiers. We have identified ${\sim} 220,000$ variables, including ${\sim} 88,300$ new discoveries. In particular, we have discovered ${\sim}48,000$ red pulsating variables, ${\sim}23,000$ eclipsing binaries, ${\sim}2,200$ $\delta$-Scuti variables and ${\sim}10,200$ rotational variables. The light curves and characteristics of the variables are all available through the ASAS-SN variable stars database (this https URL). The pre-computed ASAS-SN V-band light curves for all the ${\sim}30.1$ million sources are available through the ASAS-SN photometry database (this https URL). This effort will be extended to provide ASAS-SN light curves for sources in the northern hemisphere and for V$\lesssim17$ mag sources across the whole sky that are not included in APASS DR9. ","The ASAS-SN Catalog of Variable Stars V: Variables in the Southern
  Hemisphere"
25,1154508881285902336,1132258856594223104,devistuia,"['Into network compression for lifelong learning? You can learn the compression rate and save capacity for new tasks! Check out our #bmvc2019 spotlight paper on arxiv! Work with @bermanmaxim, Shivangi Srivastava and Matthew Blaschko. @bmvc2019 <LINK>']",https://arxiv.org/abs/1907.09695,"The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting. There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones. The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore. Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity. Such approaches allow learning small networks while making redundant parameters available for the next tasks. The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset. We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning. Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data. Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity. ",Adaptive Compression-based Lifelong Learning
26,1154445615159480326,804374251931402241,Tiancheng Zhao (Tony),"['Our new work in evaluating open-domain dialog system, a very challenging problem! We show that multiple reference is an easy, reproducible way to improve the effectiveness of existing response generation metrics. Check out the paper <LINK>']",http://arxiv.org/abs/1907.10568,"The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multi-reference evaluation. Existing metrics have been shown to correlate poorly with human judgement, particularly in open-domain dialog. One alternative is to collect human annotations for evaluation, which can be expensive and time consuming. To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references. A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the quality and the diversity of system output. ","Investigating Evaluation of Open-Domain Dialogue Systems With Human
  Generated Multiple References"
27,1154421163444953088,1068545181576773632,Kenneth Brown,"[""New paper about co-trapping laser-cooled K and Ca+ on the arXiv today.  <LINK> Just in time for Jyothi's talk @nacti2019 .  A future paper looking at charge exchange is on the way. Stay tuned. <LINK>""]",https://arxiv.org/abs/1907.10455,"In this article we describe the design, construction and implementation of our ion-atom hybrid system incorporating a high resolution time of flight mass spectrometer (TOFMS). Potassium atoms ($^{39}$K) in a Magneto Optical Trap (MOT) and laser cooled calcium ions ($^{40}$Ca$^+$) in a linear Paul trap are spatially overlapped and the combined trap is integrated with a TOFMS for radial extraction and detection of reaction products. We also present some experimental results showing interactions between $^{39}$K$^+$ and $^{39}$K, $^{40}$Ca$^+$ and $^{39}$K$^+$ as well as $^{40}$Ca$^+$ and $^{39}$K pairs. Finally, we discuss prospects for cooling CaH$^+$ molecular ions in the hybrid ion-atom system. ",A hybrid ion-atom trap with integrated high resolution mass spectrometer
28,1154365639710126081,1020951700067241984,Daoudi Mohamed,['A preprint of our new paper: Dynamic Facial Expression Generation on Hilbert Hypersphere with Conditional Wasserstein Generative Adversarial Nets #GAN\n<LINK>\nWe propose to generate the six  facial expressions given a single neutral face image. <LINK>'],https://arxiv.org/abs/1907.10087,"In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral face image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a hypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion generation on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize new facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by editing the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first work that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We evaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic appearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation, dynamic facial expression transfer and data augmentation for training improved emotion recognition models. ","Dynamic Facial Expression Generation on Hilbert Hypersphere with
  Conditional Wasserstein Generative Adversarial Nets"
29,1154341655518035969,3807291922,Aram Harrow,"[""Adaptive simulated annealing and quantum simulated annealing are both known. My new paper with Annie Wei combines adaptive &amp; quantum, part of the larger goal of sqrt'ing all the classical algorithms.\n<LINK>""]",http://arxiv.org/abs/1907.09965,"Markov chain Monte Carlo algorithms have important applications in counting problems and in machine learning problems, settings that involve estimating quantities that are difficult to compute exactly. How much can quantum computers speed up classical Markov chain algorithms? In this work we consider the problem of speeding up simulated annealing algorithms, where the stationary distributions of the Markov chains are Gibbs distributions at temperatures specified according to an annealing schedule. We construct a quantum algorithm that both adaptively constructs an annealing schedule and quantum samples at each temperature. Our adaptive annealing schedule roughly matches the length of the best classical adaptive annealing schedules and improves on nonadaptive temperature schedules by roughly a quadratic factor. Our dependence on the Markov chain gap matches other quantum algorithms and is quadratically better than what classical Markov chains achieve. Our algorithm is the first to combine both of these quadratic improvements. Like other quantum walk algorithms, it also improves on classical algorithms by producing ""qsamples"" instead of classical samples. This means preparing quantum states whose amplitudes are the square roots of the target probability distribution. In constructing the annealing schedule we make use of amplitude estimation, and we introduce a method for making amplitude estimation nondestructive at almost no additional cost, a result that may have independent interest. Finally we demonstrate how this quantum simulated annealing algorithm can be applied to the problems of estimating partition functions and Bayesian inference. ","Adaptive Quantum Simulated Annealing for Bayesian Inference and
  Estimating Partition Functions"
30,1154319126048980993,762010710633250816,Samuel Wiqvist,"['New paper out! In this paper, we develop an efficient and generic correlated pseudo-marginal MCMC scheme for stochastic differential equation mixed-effects models (SDEMEMs). Work with Andrew Golightly, @Ashleig00117209, and @uPicchini. \n\nLink to paper: <LINK> <LINK>', '@Ashleig00117209 @uPicchini Our correlated pseudo-marginal MCMC method is 10-40 times more efficient compared to ordinary pseudo-marginal MCMC, depending on which application we consider. https://t.co/BPc8JKkvGP', '@Ashleig00117209 @uPicchini We think our work will help push forward the use of SDEMEMs for applied problems. Find the full story in the paper. :)']",https://arxiv.org/abs/1907.09851,"Stochastic differential equation mixed-effects models (SDEMEMs) are flexible hierarchical models that are able to account for random variability inherent in the underlying time-dynamics, as well as the variability between experimental units and, optionally, account for measurement error. Fully Bayesian inference for state-space SDEMEMs is performed, using data at discrete times that may be incomplete and subject to measurement error. However, the inference problem is complicated by the typical intractability of the observed data likelihood which motivates the use of sampling-based approaches such as Markov chain Monte Carlo. A Gibbs sampler is proposed to target the marginal posterior of all parameter values of interest. The algorithm is made computationally efficient through careful use of blocking strategies and correlated pseudo-marginal Metropolis-Hastings steps within the Gibbs scheme. The resulting methodology is flexible and is able to deal with a large class of SDEMEMs. The methodology is demonstrated on three case studies, including tumor growth dynamics and neuronal data. The gains in terms of increased computational efficiency are model and data dependent, but unless bespoke sampling strategies requiring analytical derivations are possible for a given model, we generally observe an efficiency increase of one order of magnitude when using correlated particle methods together with our blocked-Gibbs strategy. ","Efficient inference for stochastic differential equation mixed-effects
  models using correlated particle pseudo-marginal algorithms"
31,1154276074110554112,75249390,Axel Maas,"['I have published a new paper on subtle issues in gauge theories at <LINK> - I will write sometime a blog entry to explain it contents, as it pretty involved. However, it is a testimony that after half a century we not yet fully understand such theories. #np3', '@philipthrift Not yet - once the paper is accepted for publication, the code will become available from my homepage.', 'I have published the promised blog entry on this paper at https://t.co/41xXPvCrwc']",https://arxiv.org/abs/1907.10435,"A continuum formulation of gauge-fixing resolving the Gribov-Singer ambiguity remains a challenge. Finding a Lagrangian formulation of operational resolutions in numerical lattice calculations, like minimal Landau gauge, would be one possibility. Such a formulation will here be constrained by reconstructing the Dyson-Schwinger equation for which the lattice minimal-Landau-gauge ghost propagator is a solution. It is found that this requires an additional term. As a by-product new, high precision lattice results for the ghost-gluon vertex in three and four dimensions are obtained. ",Constraining the gauge-fixed Lagrangian in minimal Landau gauge
32,1154249988806807552,486160321,Nitin Agarwal,"[""We present 'Quadric loss', a new loss function to preserve high frequency  information (edges &amp; corners) for reconstructing 3D models. No edge/corner annotations required. (oral @bmvc2019)\n\nPaper: <LINK>\n\n#3dvision #ML #DeepLearning #geometry #computervision <LINK>""]",https://arxiv.org/abs/1907.10250,"Sharp features such as edges and corners play an important role in the perception of 3D models. In order to capture them better, we propose quadric loss, a point-surface loss function, which minimizes the quadric error between the reconstructed points and the input surface. Computation of Quadric loss is easy, efficient since the quadric matrices can be computed apriori, and is fully differentiable, making quadric loss suitable for training point and mesh based architectures. Through extensive experiments we show the merits and demerits of quadric loss. When combined with Chamfer loss, quadric loss achieves better reconstruction results as compared to any one of them or other point-surface loss functions. ",Learning Embedding of 3D models with Quadric Loss
33,1154084867455377408,2162872302,Mingxing Tan,['Introducing MixNet: AutoML + a new mixed depthwise conv (MDConv).  SOTA results for mobile: 78.9% ImageNet top-1 accuracy under typical mobile settings (&lt;600M FLOPS).\n\nPaper: <LINK>\nCode &amp; models: <LINK> <LINK>'],https://arxiv.org/abs/1907.09595,"Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at this https URL tensorflow/tpu/tree/master/models/official/mnasnet/mixnet ",MixConv: Mixed Depthwise Convolutional Kernels
34,1154010821162348544,743028716457070592,Franco Vazza,"['New paper out on MNRAS, <LINK>  led by S.Hackstein and coauthored by authors all on twitter: @HambObs @franco_vazza @SciBry @VolkerHeesen  It introduces a Bayesian framework to use Fast Radio Bursts to probe the signature and origin of cosmic magnetic fields 1/N <LINK>', '@HambObs @SciBry @VolkerHeesen Stephan first designed a nice framework, containing models for the FRB local environment (e.g. wind vs SN vs uniform), for the FRB host galaxy (starburst vs Milky Way), and for intergalactic magnetic fields (primordial vs astrophysical from  #magcow simulations). https://t.co/OtJjA2S2JW', '@HambObs @SciBry @VolkerHeesen He than ""glued"" all realization together, allowing a very large exploration of model parameters leading to observable Dispersion and Rotation Measurements, as a function of the (arbitrary) distance of FRBs.', '@HambObs @SciBry @VolkerHeesen The result is a python package (still under development) called PreFRBle (‚ÄúPredictions of Fast Radio Burst models and their Likelihood Estimates‚Äù)', '@HambObs @SciBry @VolkerHeesen This is the estimated contribution of all model components to the final DM and RM, based on the redshift where the FRB is located. So not a clear situation, and the IGM contribution almost always is the sub-dominant one (unless you can integrate over a very large redshift). https://t.co/pmCoRVPxnv', '@HambObs @SciBry @VolkerHeesen In general, to **hope** distinguishing whether the residual IGM signal in RM suggests a primordial or astrophysical origin of seed fields, one needs to focus on z&gt;&gt;1 events (many!). https://t.co/HLQjdfw5sc', '@HambObs @SciBry @VolkerHeesen Basically, the contribution of the progenitor/host will remain for a long time the biggest source of uncertainty, which will be hard to remove with ""simple"" large statistics - although the PREFRBLE now offers a powerful way to go.', '@HambObs @SciBry @VolkerHeesen Through the analysis of the Bayes factor, it follows that a magnetar progenitor in a windy starburst host galaxy would in general provide the lowest ""pollution"" to the RM, thus making the detection of the IGM signal highest, even for z~0.5 FRBs.', '@HambObs @SciBry @VolkerHeesen In all other cases, useful signatures of IGM fields can only be found in very long lines of sight, probed by FRBs at z&gt;5, which if of course not a nice message. https://t.co/kfPa0B8H0G', '@HambObs @SciBry @VolkerHeesen So the joint progress of our knowledge of progenitors and their local environment can at the same time solve the FRB mystery, as well as make them a useful probe of IGM magnetic fields. This is supposed just to be the first, of a few works on the subject, so stay tuned.  N/N']",https://arxiv.org/abs/1907.09650,"We investigate the possibility of measuring intergalactic magnetic fields using the dispersion measures and rotation measures of fast radio bursts. With Bayesian methods, we produce probability density functions for values of these measures. We distinguish between contributions from the intergalactic medium, the host galaxy and the local environment of the progenitor. To this end, we use constrained, magnetohydrodynamic simulations of the local Universe to compute lines-of-sight integrals from the position of the Milky Way. In particular, we differentiate between predominantly astrophysical and primordial origins of magnetic fields in the intergalactic medium. We test different possible types of host galaxies and probe different distribution functions of fast radio burst progenitor locations inside the host galaxy. Under the assumption that fast radio bursts are produced by magnetars, we use analytic predictions to account for the contribution of the local environment. We find that less than 100 fast radio bursts from magnetars in stellar-wind environments hosted by starburst dwarf galaxies at redshift $z \gtrsim 0.5$ suffice to discriminate between predominantly primordial and astrophysical origins of intergalactic magnetic fields. However, this requires the contribution of the Milky Way to be removed with a precision of $\approx 1 \rm~rad~m^{-2}$. We show the potential existence of a subset of fast radio bursts whose rotation measure carry information on the strength of the intergalactic magnetic field and its origins. ","Fast Radio Burst dispersion measures and rotation measures and the
  origin of intergalactic magnetic fields"
35,1153836209908547585,131782092,Jeffrey Simpson,"['New paper that I helped write on the arXiv today:\n\n""The Southern Stellar Stream Spectroscopic Survey (S‚Åµ): Overview, Target Selection, Data Reduction, Validation, and Early Science‚Äù\n\n<LINK> <LINK>', ""We have been observing with the Anglo-Australian Telescope's AAOmega spectrograph stars in recently identified streams within the footprint of the Dark Energy Survey. So far we have mapped 12 streams, observed about 35000 stars, 3000 nearby dwarf galaxies, and 1700 quasars!"", 'Our website can be found at:\n\nhttps://t.co/dPme3TOqRP\n\n(I made the website thanks to a template from @templatedco)', 'We also have a second paper today from Nora Ship that measures the proper motions of these streams\n\nhttps://t.co/D4nJUCbIls', 'We also have an upcoming paper on a ‚ñà‚ñà‚ñà‚ñà‚ñà star that is ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà and with which we were able to ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà!!']",https://arxiv.org/abs/1907.09481,"We introduce the Southern Stellar Stream Spectroscopy Survey (${S}^5$), an on-going program to map the kinematics and chemistry of stellar streams in the Southern Hemisphere. The initial focus of ${S}^5$ has been spectroscopic observations of recently identified streams within the footprint of the Dark Energy Survey (DES), with the eventual goal of surveying streams across the entire southern sky. Stellar streams are composed of material that has been tidally striped from dwarf galaxies and globular clusters and hence are excellent dynamical probes of the gravitational potential of the Milky Way, as well as providing a detailed snapshot of its accretion history. Observing with the 3.9-m Anglo-Australian Telescope's 2-degree-Field fibre positioner and AAOmega spectrograph, and combining the precise photometry of DES DR1 with the superb proper motions from $Gaia$ DR2, allows us to conduct an efficient spectroscopic survey to map these stellar streams. So far ${S}^5$ has mapped 9 DES streams and 3 streams outside of DES; the former are the first spectroscopic observations of these recently discovered streams. In addition to the stream survey, we use spare fibres to undertake a Milky Way halo survey and a low-redshift galaxy survey. This paper presents an overview of the ${S}^5$ program, describing the scientific motivation for the survey, target selection, observation strategy, data reduction and survey validation. Finally, we describe early science results on stellar streams and Milky Way halo stars drawn from the survey. Updates on ${S}^5$, including future public data release, can be found at \url{this http URL}. ","The Southern Stellar Stream Spectroscopic Survey (${S}^5$): Overview,
  Target Selection, Data Reduction, Validation, and Early Science"
36,1153710160168148992,60893773,James Bullock,"['Excited about new paper by @DarthLazar that presents a new mass estimator for spheroidal galaxies from plane-of-the sky velocity dispersion that is accurate for any velocity dispersion anisotropy <LINK> @UCIPhysAstro <LINK>', ""Measuring masses in spheroidal galaxies is difficult bc we usually only have 1D velocities; don't know how velocities add up in other 2 dimensions.  We show analytically that at one radius we can determine mass independent of anisotropy using velocities in the plane of the sky https://t.co/SRHywtFjxR"", 'This extends earlier work from (former) @UCIPhysAstro PhD students J. Wolf &amp; G. Martinez, who derived a similar mass for line-of-sight velocities. Importantly, the two radii are different! Together =&gt; radial mass profile.  We find Draco &amp; Sculptor consistent with NFW https://t.co/0AJGBHp4J4', ""Here are the implied rotation curves.  As proper motion measurements get better, we'll be able to constrain inner dark matter density profiles better and perhaps test dark matter models that predict core vs. cusp https://t.co/k4RbCFD66S""]",https://arxiv.org/abs/1907.08841,"Starting with the spherical Jeans equation, we show that there exists a radius where the mass enclosed depends only on the projected tangential velocity dispersion, assuming that the anisotropy profile slowly varies. This is well-approximated at the radius where the log-slope of the stellar tracer profile is $-2$: $r_{-2}$. The associated mass is $M(r_{-2}) = 2 G^{-1} \langle \sigma_{\mathcal{T}}^{2}\rangle^{*} r_{-2}$ and the circular velocity is $V^{2}({r_{-2}}) = 2\langle \sigma_{\mathcal{T}}^{2}\rangle^{*}$. For a Plummer profile $r_{-2} \simeq 4R_e/5$. Importantly, $r_{-2}$ is smaller than the characteristic radius for line-of-sight velocities derived by Wolf et al. 2010. Together, the two estimators can constrain the mass profiles of dispersion-supported galaxies. We illustrate its applicability using published proper motion measurements of dwarf galaxies Draco and Sculptor, and find that they are consistent with inhabiting cuspy NFW subhalos of the kind predicted in CDM but we cannot rule out a core. We test our combined mass estimators against previously-published, non-spherical cosmological dwarf galaxy simulations done in both CDM and SIDM. For CDM, the estimates for the dynamic rotation curves are found to be accurate to $10\%$ while SIDM are accurate to $15\%$. Unfortunately, this level of accuracy is not good enough to measure slopes at the level required to distinguish between cusps and cores of the type predicted in viable SIDM models without stronger priors. However, we find that this provides good enough accuracy to distinguish between the normalization differences predicted at small radii ($r \simeq r_{-2} < r_{\rm core}$) for interesting SIDM models. As the number of galaxies with internal proper motions increases, mass estimators of this kind will enable valuable constraints on SIDM and CDM models. ","Accurate mass estimates from the proper motions of dispersion-supported
  galaxies"
37,1153691366796517376,20164453,Mark Pearce,"['It may be summer, but we have a new paper out! <LINK>. A prototype X-ray polarimeter designed for future post-PoGO+ space missions.']",https://arxiv.org/abs/1907.09030,"In recent years, a number of purpose-built scintillator-based polarimeters have studied bright astronomical sources for the first time in the hard X-ray band (tens to hundreds of keV). The addition of polarimetry can help data interpretation by resolving model-dependent degeneracies. The typical instrument approach is that incident X-rays scatter off a plastic scintillator into an adjacent scintillator cell. In all missions to date, the scintillators are read out using traditional vacuum tube photo-multipliers (PMTs). The advent of solid-state PMTs (""silicon PM"" or ""MPPC"") is attractive for space-based instruments since the devices are compact, robust and require a low bias voltage. We have characterised the plastic scintillator, EJ-248M, optically coupled to a multi-pixel photon counter (MPPC) and read out with the Citiroc ASIC. A light-yield of 1.6 photoelectrons/keV has been obtained, with a low energy detection threshold of $\lesssim$5 keV at room temperature. We have also constructed an MPPC-based polarimeter-demonstrator in order to investigate the feasibility of such an approach for future instruments. Incident X-rays scatter from a plastic-scintillator bar to surrounding cerium-doped GAGG (Gadolinium Aluminium Gallium Garnet) scintillators yielding time-coincident signals in the scintillators. We have determined the polarimetric response of this set-up using both unpolarised and polarised $\sim$50 keV X-rays. We observe a clear asymmetry in the GAGG counting rates for the polarised beam. The low-energy detection threshold in the plastic scintillator can be further reduced using a coincidence technique. The demonstrated polarimeter design shows promise as a space-based Compton polarimeter and we discuss ways in which our polarimeter can be adapted for such a mission. ","A Compton polarimeter using scintillators read out with MPPCs through
  Citiroc ASIC"
38,1153644419465146370,1701409680,Suhail Dhawan,"['Our new paper on magnification, dust and time-delays of the first resolved multiply-imaged lensed SNIa. <LINK>']",https://arxiv.org/abs/1907.06756,"We report lensing magnifications, extinction, and time-delay estimates for the first resolved, multiply-imaged Type Ia supernova iPTF16geu, at $z = 0.409$, using $Hubble\,Space\,Telescope$ ($HST$) observations in combination with supporting ground-based data. Multi-band photometry of the resolved images provides unique information about the differential dimming due to dust in the lensing galaxy. Using $HST$ and Keck AO reference images taken after the SN faded, we obtain a total lensing magnification for iPTF16geu of $\mu = 67.8^{+2.6}_{-2.9}$, accounting for extinction in the host and lensing galaxy. As expected from the symmetry of the system, we measure very short time-delays for the three fainter images with respect to the brightest one: -0.23 $\pm$ 0.99, -1.43 $\pm$ 0.74 and 1.36 $\pm$ 1.07 days. Interestingly, we find large differences between the magnifications of the four supernova images, even after accounting for uncertainties in the extinction corrections: $\Delta m_1 = -3.88^{+0.07}_{-0.06}$, $\Delta m_2 = -2.99^{+0.09}_{-0.08}$, $\Delta m_3 = -2.19^{+0.14}_{-0.15}$ and $\Delta m_4 = -2.40^{+0.14}_{-0.12}$ mag, discrepant with model predictions suggesting similar image brightnesses. A possible explanation for the large differences is gravitational lensing by substructures, micro- or millilensing, in addition to the large scale lens causing the image separations. We find that the inferred magnification is insensitive to the assumptions about the dust properties in the host and lens galaxy. ","Magnification, dust and time-delay constraints from the first resolved
  strongly lensed Type Ia supernova"
39,1153555839669678081,268337552,Nicolas Kourtellis,"['Our new ACM TWEB paper on Detecting Cyberbullying and Cyberaggression in Social Media is finally here!\nCheck out the preprint: <LINK>\nwith @dchatzakou, @jhblackb, @emilianoucl, @gianluca_string, @iliasl, @athenavakali &amp; the support of @ENCASE_H2020, @TEFresearch']",https://arxiv.org/abs/1907.08873,"Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across all demographics. More than half of young social media users worldwide have been exposed to such prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions, with negative consequences such as embarrassment, depression, isolation from other community members, which embed the risk to lead to even more critical consequences, such as suicide attempts. In this work, we take the first concrete steps to understand the characteristics of abusive behavior in Twitter, one of today's largest social media platforms. We analyze 1.2 million users and 2.1 million tweets, comparing users participating in discussions around seemingly normal topics like the NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering text, user, and network-based attributes. Using various state-of-the-art machine learning algorithms, we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status of Twitter user accounts marked as abusive by our methodology, and study the performance of potential mechanisms that can be used by Twitter to suspend users in the future. ",Detecting Cyberbullying and Cyberaggression in Social Media
40,1153520603598602241,315718949,Cl√©ment Canonne,"['New paper out: ""Domain Compression and its Application to Randomness-Optimal Distributed Goodness-of-Fit"" (<LINK>), with J. Acharya (@AcharyaJayadev), Y. Han, Z. Sun, and H. Tyagi! (as teased in <LINK>). 1/14', 'We consider a similar setting as in previous work [1,2,3]: n users, each gets an i.i.d. from some unknown discrete distribution p on k elements. The server wants to perform goodness-of-fit on p: is p=q (known reference q), or is p Œµ-far  from q in TV distance? 2/14', 'Now for the twist: the server cannot see the n samples, because each user has a ""local information constraint"": cannot communicate much (bandwidth), or wants to preserve local #privacy, for instance. So each sample needs to be ""compressed"" before sending it to the server. 3/14', 'For instance, communication (all I say below works for local privacy too): each user can send only ‚Ñì‚â™ log k bits (non-interactively) to the server. How many users (samples) are now necessary as a function of k, ‚Ñì, Œµ?\n\n*Without* comm. constraints, well-known: Œò(‚àök/Œµ¬≤).\n4/14', ""In previous work, we pinpointed the optimal sample complexity when (i) the users share in advance a common random seed (public-coin protocols), and (ii) they don't (private-coin protocols). There is a *big* gap! 5/14 https://t.co/NSEIGV1D6e"", '(Intuitively, with a shared random seed (O(log k) shared bits suffice), the users can ""hash"" the domain to get a new distribution on 2^‚Ñì ‚â™ k elements, and the distance Œµ shrinks but not ""too much"" -- now, no more communication constraint, they can send the full ‚Ñì bits!) 6/14', 'In this new work, we ask the following: *what if they share some randomness... but very few bits? What if the users share a random seed of length s ‚â™ log k?* Does that even help? Is there a sharp transition at log k?\n\nWe show a full *optimal* trade-off in all parameters. 7/14', 'Namely, we show the following (picture below). Interestingly... ""one bit of bandwidth (increase ‚Ñì) is worth 2 bits of shared randomness (increase s)!\n\n(again, similar statement for œÅ-LDP instead of ‚Ñì-bit comm. constraints) 8/14 https://t.co/vfrwZvrQxL', 'Techniques are (IMO) quite interesting too: \n(i) we introduce ""domain compression"" (some sort of L1 dimensionality reduction from and to the probability simplex) and give a randomness-efficient optimal domain reduction protocol 9/14', '(ii) we show that (domain reduction with s random bits)+(private-coin testing protocol under local constraints XYZ) ‚áí (s-random bit testing protocol under local constraints XYZ) in a black-box way. 10 /14', '(iii) we significantly extend the lower bound framework of [1] to handle this ""limited shared randomness"" setting -- giving a unified way to obtain sample complexity lower bounds for many statistical inference task in this model, under various constraints. 11/14', 'Fun fact: for (ii), we need to do success amplif. from a low-success-probability test to get prob. say 2/3. But... cannot amplify by repetition, since our base test ""burnt"" all the s common random bits already! So we got to use that amazing result: https://t.co/uo5WNyysmH 12/14', 'Long story short: many ideas I really like, and hopefully a nice read. Check the paper! https://t.co/CAO0ePaKdc 13/14', ""PS: I talked about it at WoLA'19 last weekend. Slides (handwritten) available here: https://t.co/xs94wlMoKp (https://t.co/HCxs1sMZt2)\n\nRefs:\n[1] https://t.co/H4Nd3QKPBl (COLT'19)\n[2] https://t.co/39xSK9A7TS (ICML'19)\n[3] https://t.co/rimFLwhSOY (AISTATS'19) 14/14""]",https://arxiv.org/abs/1907.08743,"We study goodness-of-fit of discrete distributions in the distributed setting, where samples are divided between multiple users who can only release a limited amount of information about their samples due to various information constraints. Recently, a subset of the authors showed that having access to a common random seed (i.e., shared randomness) leads to a significant reduction in the sample complexity of this problem. In this work, we provide a complete understanding of the interplay between the amount of shared randomness available, the stringency of information constraints, and the sample complexity of the testing problem by characterizing a tight trade-off between these three parameters. We provide a general distributed goodness-of-fit protocol that as a function of the amount of shared randomness interpolates smoothly between the private- and public-coin sample complexities. We complement our upper bound with a general framework to prove lower bounds on the sample complexity of this testing problems under limited shared randomness. Finally, we instantiate our bounds for the two archetypal information constraints of communication and local privacy, and show that our sample complexity bounds are optimal as a function of all the parameters of the problem, including the amount of shared randomness. A key component of our upper bounds is a new primitive of domain compression, a tool that allows us to map distributions to a much smaller domain size while preserving their pairwise distances, using a limited amount of randomness. ","Domain Compression and its Application to Randomness-Optimal Distributed
  Goodness-of-Fit"
41,1153207399462948864,1136974925699457025,Katrin F√ºrsich,['Coming up soon! My new paper about Raman scattering from current-stabilized nonequilibrium phases in Ca2RuO4. Already available on @arxiv and in press for publication in @PhysRevB. Stay tuned üòÉ\n<LINK> <LINK>'],https://arxiv.org/abs/1907.07905,"We used Raman light scattering to study the current-stabilized nonequilibrium semimetallic and metallic phases in Ca$_2$RuO$_4$. By determining the local temperature through careful analysis of the Stokes and anti-Stokes intensities, we find that Joule heating can be completely avoided by supplying sufficient cooling power in a helium-flow cryostat, and that the current induces the semimetallic state without inducing any significant heating. We further investigate the current-induced semimetallic state as a function of temperature and current. We confirm the absence of long-range antiferromagnetic order and identify a substantial Fano broadening of several phonons, which suggests coupling to charge and orbital fluctuations. Our results demonstrate that the semimetallic state is a genuine effect of the applied electrical current and that the current-induced phases have characteristics distinct from the equilibrium ones. ","Raman scattering from current-stabilized nonequilibrium phases in
  Ca$_2$RuO$_4$"
42,1153010519076483074,3432298840,Ye Yuan,['Excited to share our new paper on diverse trajectory forecasting with DPP. We show that diverse and likely samples can be obtained from a generative model even when the data distribution is unbalanced.\n<LINK> <LINK>'],https://arxiv.org/abs/1907.04967,"The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a single outcome. While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode that has most data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse and likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as a parameter estimation of the DSF. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn move the latent codes of the sample set to find an optimal diverse and likely set of trajectories. Our method is a novel application of DPPs to optimize a set of items (trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data. ",Diverse Trajectory Forecasting with Determinantal Point Processes
43,1152274369747439617,1010214960352223232,Igor (Gary) Rubinov,"['For #AI to benefit people everywhere, we must understand its effect on communities globally. Check out the results of our new paper on ‚ÄúAI Ethics: A Global Perspective‚Äù co-written with @anthroptimist   <LINK> <LINK>']",https://arxiv.org/abs/1907.07892,"The ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry, researchers in academia, and the public. However, current analyses of AI in a global context are biased toward perspectives held in the U.S., and limited by a lack of research, especially outside the U.S. and Western Europe. This article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of AI and related technologies in five global regions. Our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages. Our review of the literature suggests that AI is likely to have markedly different social impacts depending on geographical setting. Likewise, perceptions and understandings of AI are likely to be profoundly shaped by local cultural and social context. Recent research in U.S. settings demonstrates that AI-driven technologies have a pattern of entrenching social divides and exacerbating social inequality, particularly among historically-marginalized groups. Our literature review indicates that this pattern exists on a global scale, and suggests that low- and middle-income countries may be more vulnerable to the negative social impacts of AI and less likely to benefit from the attendant gains. We call for rigorous ethnographic research to better understand the social impacts of AI around the world. Global, on-the-ground research is particularly critical to identify AI systems that may amplify social inequality in order to mitigate potential harms. Deeper understanding of the social impacts of AI in diverse social settings is a necessary precursor to the development, implementation, and monitoring of responsible and beneficial AI technologies, and forms the basis for meaningful regulation of these technologies. ","Global AI Ethics: A Review of the Social Impacts and Ethical
  Implications of Artificial Intelligence"
44,1152044099358167040,2416760538,Peter Gao,"['New co-authored paper! \n\n<LINK>\n\n@ThorngrenDaniel led this one (@jjfplanet and I helped), detailing how observed hot Jupiter radii constrains the location of the radiative-convective boundary and the intrinsic temperature.', ""@ThorngrenDaniel @jjfplanet Long story short: If you're modeling hot Jupiter atmospheres in 1D or 3D and you're still assuming that the intrinsic temperature is 100 K, assume again.""]",https://arxiv.org/abs/1907.07777,"In giant planet atmosphere modelling, the intrinsic temperature $T_\mathrm{int}$ and radiative-convective boundary (RCB) are important lower boundary conditions. Often in one-dimensional radiative-convective models and in three-dimensional general circulation models it is assumed that $T_\mathrm{int}$ is similar to that of Jupiter itself, around 100 K, which yields a RCB around 1 kbar for hot Jupiters. In this work, we show that the inflated radii, and hence high specific entropy interiors, of hot Jupiters suggest much higher $T_\mathrm{int}$ values. Assuming the effect is primarily due to current heating (rather than delayed cooling), we derive an equilibrium relation between $T_\mathrm{eq}$ and $T_\mathrm{int}$, showing that the latter can take values as high as 700 K. In response, the RCB moves upward in the atmosphere. Using one-dimensional radiative-convective atmosphere models, we find RCBs of only a few bars, rather than the kilobar typically supposed. This much shallower RCB has important implications for the atmospheric structure, vertical and horizontal circulation, interpretations of phase curves, and the effect of deep cold traps on cloud formation. ","The Intrinsic Temperature and Radiative-Convective Boundary Depth in the
  Atmospheres of Hot Jupiters"
45,1152030965872513024,1019760963569049601,Almog Yalinewich,"['Our new paper is on the arxiv. We study the optical transient from an explosion close to the surface of a star. Such an explosion can occur due to super Eddington accretion of a compact companion in a common envelope, and can be a precursor to a supernova <LINK> <LINK>']",https://arxiv.org/abs/1907.07689,"We study the hydrodynamic evolution of an explosion close to the stellar surface, and give predictions for the radiation from such an event. We show that such an event will give rise to a multi-wavelength transient. We apply this model to describe a precursor burst to the peculiar supernova iPTF14hls, which occurred in 1954, sixty year before the supernova. We propose that the new generation of optical surveys might detect similar transients, and they can be used to identify supernova progenitors well before the explosion. ",Optical Transient from an Explosion Close to the Stellar Surface
46,1151866162025660416,70775884,Andrea Tacchetti,"['Economists have been investigating how to design auction rules for decades. In our new paper, we cast auction design as a function approximation problem, and show that Deep Learning and modern AI can help. As usual, robust data representations are key! <LINK> <LINK>']",https://arxiv.org/abs/1907.05181,"Auctions are protocols to allocate goods to buyers who have preferences over them, and collect payments in return. Economists have invested significant effort in designing auction rules that result in allocations of the goods that are desirable for the group as a whole. However, for settings where participants' valuations of the items on sale are their private information, the rules of the auction must deter buyers from misreporting their preferences, so as to maximize their own utility, since misreported preferences hinder the ability for the auctioneer to allocate goods to those who want them most. Manual auction design has yielded excellent mechanisms for specific settings, but requires significant effort when tackling new domains. We propose a deep learning based approach to automatically design auctions in a wide variety of domains, shifting the design work from human to machine. We assume that participants' valuations for the items for sale are independently sampled from an unknown but fixed distribution. Our system receives a data-set consisting of such valuation samples, and outputs an auction rule encoding the desired incentive structure. We focus on producing truthful and efficient auctions that minimize the economic burden on participants. We evaluate the auctions designed by our framework on well-studied domains, such as multi-unit and combinatorial auctions, showing that they outperform known auction designs in terms of the economic burden placed on participants. ",A Neural Architecture for Designing Truthful and Efficient Auctions
47,1151667755474673664,2800204849,Andrew Gordon Wilson,['Subspace Inference for Bayesian Deep Learning. In our new #UAI2019 paper we construct low dimensional subspaces for scalable Bayesian inference on modern deep nets (with code)! Mode connectivity makes a guest appearance. <LINK> <LINK>'],https://arxiv.org/abs/1907.07504,"Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification. ",Subspace Inference for Bayesian Deep Learning
48,1151646424754466816,1024926454923112448,Dominik Schleicher,"['Our new paper: \nResolving accretion flows in nearby active galactic nuclei with the Event Horizon Telescope, <LINK> @ehtelescope @AnilloBlackHole Astronomy #UdeC']",https://arxiv.org/abs/1907.05879,"The Event Horizon Telescope (EHT), now with its first ever image of the photon ring around the supermassive black hole of M87, provides a unique opportunity to probe the physics of supermassive black holes through Very Long Baseline Interferometry (VLBI), such as the existence of the event horizon, the accretion processes as well as jet formation in Low Luminosity AGN (LLAGN). We build a theoretical model which includes an Advection Dominated Accretion Flow (ADAF) with emission from thermal and non-thermal electrons in the flow and a simple radio jet outflow. The predicted spectral energy distribution (SED) of this model is compared to sub-arcsec resolution observations to get the best estimates of the model parameters. The model-predicted radial emission profiles at different frequency bands are used to predict whether the inflow can be resolved by the EHT or with telescopes such as the Global 3-mm VLBI array (GMVA). In this work the model is initially tested with high resolution SED data of M87 and then applied to our sample of 5 galaxies (Cen A, M84, NGC 4594, NGC 3998 and NGC 4278). The model then allows us to predict if one can detect and resolve the inflow for any of these galaxies using the EHT or GMVA within an 8 hour integration time. ","Resolving accretion flows in nearby active galactic nuclei with the
  Event Horizon Telescope"
49,1151515352121004032,68538286,Dan Hendrycks,"['Natural Adversarial Examples are real-world and unmodified examples which cause classifiers to be consistently confused. The new dataset has 7,500 images, which we personally labeled over several months.\nPaper: <LINK>\nDataset and code: <LINK> <LINK>', '@neuroecology @rbhar90 From the introduction, @goodfellow_ian et al. define adversarial examples as ‚Äúinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.‚Äù', '@gwern @neuroecology @rbhar90 Yes, this is hard example mining, just as adversarial l_p perturbations are perturbations mined from the l_p ball.']",https://arxiv.org/abs/1907.07174,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models. ",Natural Adversarial Examples
50,1151466278340747265,2742282828,Andreea Font,['New paper led by Egidijus Kukstas introducing a new approach to environmental physics: exploring the cross-correlations between galaxy properties and their local environment (characterised by tSZ effect and diffuse X-rays) and comparing them to simulations <LINK>'],https://arxiv.org/abs/1907.06645,"The observable properties of galaxies depend on both internal processes and the external environment. In terms of the environmental role, we still do not have a clear picture of the processes driving the transformation of galaxies. The use of proxies for environment (e.g., host halo mass, distance to the N^th nearest neighbour, etc.), as opposed to the real physical conditions (e.g., hot gas density) may bear some responsibility for this. Here we propose a new method that directly links galaxies to their local environments, by using spatial cross-correlations of galaxy catalogues with maps from large-scale structure surveys (e.g., thermal Sunyaev-Zel'dovich [tSZ] effect, diffuse X-ray emission, weak lensing of galaxies or the CMB). We focus here on the quenching of galaxies and its link to local hot gas properties. Maps of galaxy overdensity and quenched fraction excess are constructed from volume-limited SDSS catalogs, which are cross-correlated with tSZ effect and X-ray maps from Planck and ROSAT, respectively. Strong signals out to Mpc scales are detected for most cross-correlations and are compared to predictions from the EAGLE and BAHAMAS cosmological hydrodynamical simulations. The simulations successfully reproduce many, but not all, of the observed power spectra, with an indication that environmental quenching may be too efficient in the simulations. We demonstrate that the cross-correlations are sensitive to both the internal (e.g., AGN and stellar feedback) and external processes (e.g., ram pressure stripping, harassment, strangulation, etc.) responsible for quenching. The methods outlined in this paper can be adapted to other observables and, with upcoming surveys, will provide a stringent test of physical models for environmental transformation. ","Environment from cross-correlations: connecting hot gas and the
  quenching of galaxies"
51,1151460006698594304,2425754287,Roman Orus,['New paper out! Simulation methods for open quantum many-body systems. With A. Kshetrimayum and H. Weimer. <LINK>'],https://arxiv.org/abs/1907.07079?fbclid=IwAR1wvSffZjIcRsyPYYhXSJLXC_8gb2FBi79j9WCL5mv_UHsKZsbcwh3nyIs,"Coupling a quantum many-body system to an external environment dramatically changes its dynamics and offers novel possibilities not found in closed systems. Of special interest are the properties of the steady state of such open quantum many-body systems, as well as the relaxation dynamics towards the steady state. However, new computational tools are required to simulate open quantum many-body systems, as methods developed for closed systems cannot be readily applied. We review several approaches to simulate open many-body systems and point out the advances made in recent years towards the simulation of large system sizes. ",Simulation methods for open quantum many-body systems
52,1151388886200999937,3041506707,Nicolas Vandewalle,"['Our paper on ""capillary assemblies in a rotating magnetic field"" is available on @arxiv  <LINK> we evidence complex rotation modes. This work is the starting point of a new way to propel magneto capillary systems @UniversiteLiege #grasp <LINK>']",http://arxiv.org/abs/1907.06904,"Small objects floating on a fluid have a tendency to aggregate due to capillary forces. This effect has been used, with the help of a magnetic induction field, to assemble submillimeter metallic spheres into a variety of structures, whose shape and size can be tuned. Under time-varying fields, these assemblies can propel themselves due to a breaking of time reversal symmetry in their adopted shapes. In this article, we study the influence of an in-plane rotation of the magnetic field on these structures. Various rotational modes have been observed with different underlying mechanisms. The magnetic properties of the particles cause them to rotate individually. Dipole-dipole interactions in the assembly can cause the whole structure to align with the field. Finally, non-reciprocal deformations can power the rotation of the assembly. Symmetry plays an important role in the dynamics, as well as the frequency and amplitude of the applied field. Understanding the interplay of these effects is essential, both to explain previous observations and to develop new functions for these assemblies. ",Capillary assemblies in a rotating magnetic field
53,1151378374587289600,797858947344568320,Balazs Tarjan,['Our new paper about the utilization of approximated recurrent neural language models for Hungarian speech recognition has been accepted to SLSP 2019! Full paper can be found here: <LINK>'],https://arxiv.org/abs/1907.06407,"Recognition of Hungarian conversational telephone speech is challenging due to the informal style and morphological richness of the language. Recurrent Neural Network Language Model (RNNLM) can provide remedy for the high perplexity of the task; however, two-pass decoding introduces a considerable processing delay. In order to eliminate this delay we investigate approaches aiming at the complexity reduction of RNNLM, while preserving its accuracy. We compare the performance of conventional back-off n-gram language models (BNLM), BNLM approximation of RNNLMs (RNN-BNLM) and RNN n-grams in terms of perplexity and word error rate (WER). Morphological richness is often addressed by using statistically derived subwords - morphs - in the language models, hence our investigations are extended to morph-based models, as well. We found that using RNN-BNLMs 40% of the RNNLM perplexity reduction can be recovered, which is roughly equal to the performance of a RNN 4-gram model. Combining morph-based modeling and approximation of RNNLM, we were able to achieve 8% relative WER reduction and preserve real-time operation of our conversational telephone speech recognition system. ","Investigation on N-gram Approximated RNNLMs for Recognition of
  Morphologically Rich Speech"
54,1151326890331971585,4750366201,Bryan Ostdiek,"['Very pleased to announce our new paper using #MachineLearning to tell what stars are accreted onto the Milky Way vs those that were born in situ in the \n@ESAGaia\n data. See what we find in the data tomorrow!  <LINK>', '@ESAGaia With @linoush95 @SheaGKosmo @AndrewWetzel @astrorobyn @PFHopkins_Astro']",https://arxiv.org/abs/1907.06652,"The goal of this study is to present the development of a machine learning based approach that utilizes phase space alone to separate the Gaia DR2 stars into two categories: those accreted onto the Milky Way from those that are in situ. Traditional selection methods that have been used to identify accreted stars typically rely on full 3D velocity, metallicity information, or both, which significantly reduces the number of classifiable stars. The approach advocated here is applicable to a much larger portion of Gaia DR2. A method known as ""transfer learning"" is shown to be effective through extensive testing on a set of mock Gaia catalogs that are based on the FIRE cosmological zoom-in hydrodynamic simulations of Milky Way-mass galaxies. The machine is first trained on simulated data using only 5D kinematics as inputs and is then further trained on a cross-matched Gaia/RAVE data set, which improves sensitivity to properties of the real Milky Way. The result is a catalog that identifies around 767,000 accreted stars within Gaia DR2. This catalog can yield empirical insights into the merger history of the Milky Way and could be used to infer properties of the dark matter distribution. ",Cataloging Accreted Stars within Gaia DR2 using Deep Learning
55,1151316229371547654,4827295586,Kevin Wagner,"[""Here's the spiral protoplanetary disk around MWC 758 for the first time imaged in the M' filter with the Large Binocular Telescope! See the paper (<LINK>) for more details, including an investigation into new and existing planet candidates. @LBTObs @danielapai <LINK>""]",https://arxiv.org/abs/1907.06655,"Theoretical studies suggest that a giant planet around the young star MWC 758 could be responsible for driving the spiral features in its circumstellar disk. Here, we present a deep imaging campaign with the Large Binocular Telescope with the primary goal of imaging the predicted planet. We present images of the disk in two epochs in the $L^{\prime}$ filter (3.8 $\mu m$) and a third epoch in the $M^{\prime}$ filter (4.8 $\mu m$). The two prominent spiral arms are detected in each observation, which constitute the first images of the disk at $M^\prime$, and the deepest yet in $L^\prime$ ($\Delta L^\prime=$12.1 exterior to the disk at 5$\sigma$ significance). We report the detection of a S/N$\sim$3.9 source near the end of the Sourthern arm, and, from the source's detection at a consistent position and brightness during multiple epochs, we establish a $\sim$90% confidence-level that the source is of astrophysical origin. We discuss the possibilities that this feature may be a) an unresolved disk feature, and b) a giant planet responsible for the spiral arms, with several arguments pointing in favor of the latter scenario. We present additional detection limits on companions exterior to the spiral arms, which suggest that a $\lesssim$4 M$_{Jup}$ planet exterior to the spiral arms could have escaped detection. Finally, we do not detect the companion candidate interior to the spiral arms reported recently by Reggiani et al. (2018), although forward modelling suggests that such a source would have likely been detected. ","Thermal Infrared Imaging of MWC 758 with the Large Binocular Telescope:
  Planetary Driven Spiral Arms?"
56,1151164490395856897,2352463609,Carlos Blanco,"[""In our new paper with Dan Hooper @DanHooperAstro , Miguel Escudero, and Sam Witte, we take a deep look at the explored and explorable parameter space of Z'-mediated WIMPs. Are they dead? find out!  \n\n<LINK>""]",https://arxiv.org/abs/1907.05893,"Although weakly interacting massive particles (WIMPs) have long been among the most studied and theoretically attractive classes of candidates for the dark matter of our universe, the lack of their detection in direct detection and collider experiments has begun to dampen enthusiasm for this paradigm. In this study, we set out to appraise the status of the WIMP paradigm, focusing on the case of dark matter candidates that interact with the Standard Model through a new gauge boson. After considering a wide range of $Z'$ mediated dark matter models, we quantitatively evaluate the fraction of the parameter space that has been excluded by existing experiments, and that is projected to fall within the reach of future direct detection experiments. Despite the existence of stringent constraints, we find that a sizable fraction of this parameter space remains viable. More specifically, if the dark matter is a Majorana fermion, we find that an order one fraction of the parameter space is in many cases untested by current experiments. Future direct detection experiments with sensitivity near the irreducible neutrino floor will be able to test a significant fraction of the currently viable parameter space, providing considerable motivation for the next generation of direct detection experiments. ","$Z'$ Mediated WIMPs: Dead, Dying, or Soon to be Detected?"
57,1151101532491997187,887514666,Babak Ehteshami Bejnordi,"['See our new arXiv paper on channel-gated nets for conditional compute. Our batch-shaping loss matches the marginal aggregated posterior of a feature in NN to any prior PDF and helps to learn more conditional features: <LINK> (B.E., T. Blankevoort, M. Welling) <LINK>']",http://arxiv.org/abs/1907.06627,"We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool $batch$-$shaping$ that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples. ",Batch-Shaping for Learning Conditional Channel Gated Networks
58,1149646895377076224,806058672619212800,Guillaume Lample,"['Our new paper: Large Memory Layers with Product Keys <LINK>\nWe created a key-value memory layer that can increase model capacity for a negligible computational cost. A 12-layer transformer with a memory outperforms a 24-layer transformer, and is 2x faster! 1/2 <LINK>', 'The memory is based on a product-key parametrization that enables fast and exact nearest neighbor search over millions of values. It improves model performance without using larger dimension or adding more layers that would slow the model. @alexsablay @LudovicDenoyer @hjegou 2/2', '@arankomatsuzaki Yes, all models were trained in the same setting, with the same number of iterations. With more iterations, results would have been even more favorable to memory augmented models, as the sparse updates in the memory require more iterations to fully converge.']",https://arxiv.org/abs/1907.05242,"This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes. ",Large Memory Layers with Product Keys
59,1149549986092163072,972555245179064320,Jordy de Vries,"['Very happy with a new paper today, but it is a rather technical one <LINK> . Anomalous dimensions play a big role in renormalization-group equations that determine how coupling constants depend on the energy scale where they are probed.', 'We used a method, developed for the QCD beta function, to get higher-loop anomalous dimensions of Standard Model EFT operators. As an example, we calculated 2 and 3 loop anomalous dimension of the so- called CP-odd Weinberg operator, where only one-loop results were known.', 'The calculation seemed impossible to me (10^4 highly nontrivial diagrams) but the algorithm developed by my collaborators did the trick. The method can be extended to high-loop renormalization of a much larger class of effective operators.', 'We found that 2- and 3-loop results are rather big, and that perturbation theory can fail already for not-that-small values of the QCD coupling. Also the 3-loop result has a piece that also appears in the 4-loop beta function. Pretty interesting, but we don‚Äôt know why....', 'We started this work some years ago when we were all at Nikhef in Amsterdam. Now 3 of us are elsewhere in the US, Scotland, and Switzerland. So it‚Äôs extra nice the calculation was completed.']",https://arxiv.org/abs/1907.04923,"We apply a fully automated extension of the $R^*$-operation capable of calculating higher-loop anomalous dimensions of n-point Green's functions of arbitrary, possibly non-renormalisable, local Quantum Field Theories. We focus on the case of the CP-violating Weinberg operator of the Standard Model Effective Field Theory whose anomalous dimension is so far known only at one loop. We calculate the two-loop anomalous dimension in full QCD and the three-loop anomalous dimensions in the limit of pure Yang-Mills theory. We find sizeable two-loop and large three-loop corrections, due to the appearance of a new quartic group invariant. We discuss phenomenological implications for electric dipole moments and future applications of the method. ","Two- and three-loop anomalous dimensions of Weinberg's dimension-six
  CP-odd gluonic operator"
60,1149482137281679362,140287694,Anowar J Shajib,['New paper from the H0LiCOW collaboration.\n\nWe report that the early- and late-universe measurements of H0 are at 5.3œÉ tension!!\n\n<LINK> <LINK>'],https://arxiv.org/abs/1907.04869,"We present a measurement of the Hubble constant ($H_{0}$) and other cosmological parameters from a joint analysis of six gravitationally lensed quasars with measured time delays. All lenses except the first are analyzed blindly with respect to the cosmological parameters. In a flat $\Lambda$CDM cosmology, we find $H_{0} = 73.3_{-1.8}^{+1.7}$, a 2.4% precision measurement, in agreement with local measurements of $H_{0}$ from type Ia supernovae calibrated by the distance ladder, but in $3.1\sigma$ tension with $Planck$ observations of the cosmic microwave background (CMB). This method is completely independent of both the supernovae and CMB analyses. A combination of time-delay cosmography and the distance ladder results is in $5.3\sigma$ tension with $Planck$ CMB determinations of $H_{0}$ in flat $\Lambda$CDM. We compute Bayes factors to verify that all lenses give statistically consistent results, showing that we are not underestimating our uncertainties and are able to control our systematics. We explore extensions to flat $\Lambda$CDM using constraints from time-delay cosmography alone, as well as combinations with other cosmological probes, including CMB observations from $Planck$, baryon acoustic oscillations, and type Ia supernovae. Time-delay cosmography improves the precision of the other probes, demonstrating the strong complementarity. Allowing for spatial curvature does not resolve the tension with $Planck$. Using the distance constraints from time-delay cosmography to anchor the type Ia supernova distance scale, we reduce the sensitivity of our $H_0$ inference to cosmological model assumptions. For six different cosmological models, our combined inference on $H_{0}$ ranges from $\sim73$-$78~\mathrm{km~s^{-1}~Mpc^{-1}}$, which is consistent with the local distance ladder constraints. ","H0LiCOW XIII. A 2.4% measurement of $H_{0}$ from lensed quasars:
  $5.3\sigma$ tension between early and late-Universe probes"
61,1149481089108332545,48144742,Gill Verdon ‚öõÔ∏èüé≤ü§ñ,"['Happy to release my very first paper with @GoogleAI Quantum: \n\n""Learning to learn with quantum neural networks via classical neural networks""\n\nAs it turns out, classical machine learning can discover new ways to rapidly optimize quantum neural nets\n\n<LINK> <LINK>']",https://arxiv.org/abs/1907.05415,"Quantum Neural Networks (QNNs) are a promising variational learning paradigm with applications to near-term quantum processors, however they still face some significant challenges. One such challenge is finding good parameter initialization heuristics that ensure rapid and consistent convergence to local minima of the parameterized quantum circuit landscape. In this work, we train classical neural networks to assist in the quantum learning process, also know as meta-learning, to rapidly find approximate optima in the parameter landscape for several classes of quantum variational algorithms. Specifically, we train classical recurrent neural networks to find approximately optimal parameters within a small number of queries of the cost function for the Quantum Approximate Optimization Algorithm (QAOA) for MaxCut, QAOA for Sherrington-Kirkpatrick Ising model, and for a Variational Quantum Eigensolver for the Hubbard model. By initializing other optimizers at parameter values suggested by the classical neural network, we demonstrate a significant improvement in the total number of optimization iterations required to reach a given accuracy. We further demonstrate that the optimization strategies learned by the neural network generalize well across a range of problem instance sizes. This opens up the possibility of training on small, classically simulatable problem instances, in order to initialize larger, classically intractably simulatable problem instances on quantum devices, thereby significantly reducing the number of required quantum-classical optimization iterations. ","Learning to learn with quantum neural networks via classical neural
  networks"
62,1149477660013162499,890966966726479874,Jesse Thomason,"['Check out our new preprint, ""Vision-and-Dialog Navigation"", where we extend the VLN paradigm with a corpus of two-sided, cooperative human-human navigation dialogs! Work with @mmurz, @mayacakmak, and @LukeZettlemoyer.\npaper: <LINK>\ncode: <LINK> <LINK>', 'The Cooperative Vision-and-Language Navigation (CVDN) corpus adds a dynamic, dialog-based language component to the already-dynamic visual context of VLN, further contrasting tasks like VisDial that have dynamic, dialog-based language context but static visual context. https://t.co/hDKdYJ9qcK', 'CVDN is a resource for training agents that navigate, ask questions, and answer navigation questions. We first focus on navigation, and demonstrate an initial seq2seq model that benefits from longer dialog histories as language context for navigating towards a goal location. https://t.co/nSbzQ9jUGA', 'Feel free to get in touch with us about using CVDN! The full CVDN dialog data as well as the Navigation from Dialog History task data are available along with the code linked above.\nYou crowdsourcing interface too: https://t.co/AOey4IvBRa (use two tabs to connect with yourself)']",https://arxiv.org/abs/1907.04957,"Robots navigating in human environments should use language to ask for assistance and be able to understand human responses. To study this challenge, we introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k embodied, human-human dialogs situated in simulated, photorealistic home environments. The Navigator asks questions to their partner, the Oracle, who has privileged access to the best next steps the Navigator should take according to a shortest path planner. To train agents that search an environment for a goal location, we define the Navigation from Dialog History task. An agent, given a target object and a dialog history between humans cooperating to find that object, must infer navigation actions towards the goal in unexplored environments. We establish an initial, multi-modal sequence-to-sequence model and demonstrate that looking farther back in the dialog history improves performance. Sourcecode and a live interface demo can be found at this https URL ",Vision-and-Dialog Navigation
63,1149305627350454272,872274950,Tim Dettmers,['My new work with @LukeZettlemoyer on accelerated training of sparse networks from random weights to dense performance levels ‚Äî no retraining required!\nPaper: <LINK>\nBlog post: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1907.04840,"We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use. ",Sparse Networks from Scratch: Faster Training without Losing Performance
64,1149143023671889920,769188626324398080,Heiga Zen (ÂÖ® ÁÇ≥Ê≤≥),['Our new paper: \n\nLearning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning\n\n<LINK>'],https://arxiv.org/abs/1907.04448,"We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages. Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish speech using an English speaker's voice, without training on any bilingual or parallel examples. Such transfer works across distantly related languages, e.g. English and Mandarin. Critical to achieving this result are: 1. using a phonemic input representation to encourage sharing of model capacity across languages, and 2. incorporating an adversarial loss term to encourage the model to disentangle its representation of speaker identity (which is perfectly correlated with language in the training data) from the speech content. Further scaling up the model by training on multiple speakers of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize intelligible speech for training speakers in all languages seen during training, and in native or foreign accents. ","Learning to Speak Fluently in a Foreign Language: Multilingual Speech
  Synthesis and Cross-Language Voice Cloning"
65,1149132781651292160,580031141,"James Davenport, PhD","['New paper on the arXiv from me: SETI with optical surveys. I hope this is the first of many such fun interrogations of data we‚Äôre gathering!\n<LINK>', 'This isn‚Äôt the magical ultimate way to carry out technosignature searches. As the workshop last year outlined, big surveys don‚Äôt necessarily probe the ‚Äúbest‚Äù idea types. BUT it does fill important niches... https://t.co/O6zIAvlgAd', 'Importantly, it‚Äôs ‚Äúeasy‚Äù, with only the cost of computer runtime (v. cheap) and human coding time (not cheap), and highly extensible! Also great for students, b/c many approaches/methods/signal types unexplored!', 'The false positives are likely to be either very strange systematics, or very rare things. As with all outlier hunting, that means we‚Äôre able to do ‚Äúgood‚Äù astronomy research too, and students time is not ‚Äúwasted‚Äù', 'What I‚Äôm not seeing out of big projects like @ztfsurvey or @LSST is SETI working groups -YET! I know many in these surveys are interested in this topic, and I‚Äôve started an informal email group with ZTF folks, but I see this as a tangible organizational need', '@Miquai YES! Just tell me when!', '@astronomeara I think so too! https://t.co/NAmqOMo82e', 'And for a tl;dr discussion, here‚Äôs a Vlog I shot a couple months ago as I was finishing this paper:\nhttps://t.co/WzXyXLhxsv']",https://arxiv.org/abs/1907.04443,"Traditional searches for extraterrestrial intelligence (SETI) or ""technosignatures"" focus on dedicated observations of single stars or regions in the sky to detect excess or transient emission from intelligent sources. The newest generation of synoptic time domain surveys enable an entirely new approach: spatio-temporal SETI, where technosignatures may be discovered from spatially resolved sources or multiple stars over time. Current optical time domain surveys such as ZTF and the Evryscope can probe 10-100 times more of the ""Cosmic Haystack"" parameter space volume than many radio SETI investigations. Small-aperture, high cadence surveys like Evryscope can be comparable in their Haystack volume completeness to deeper surveys including LSST. Investigations with these surveys can also be conducted at a fraction of the cost of dedicated SETI surveys, since they make use of data already being gathered. However, SETI methodology has not widely utilized such surveys, and the field is in need of new search algorithms that can account for signals in both the spatial and temporal domains. Here I describe the broad potential for modern wide-field time domain optical surveys to revolutionize our search for technosignatures, and illustrate some example SETI approaches using transiting exoplanets to form a distributed beacon. ",SETI in the Spatio-Temporal Survey Domain
66,1149118095836925952,19510090,Julian Togelius,"['How could AI help you design games? Perhaps by giving you helpful suggestions as you design? In a new paper, we introduce Pitako, a recommender system for game design. The system learns from existing games to suggest design elements.\n<LINK> <LINK>', ""Thin of this as something akin to Amazon's, Netflix's, or Spotify's recommendations, but for game design. The system has a library of more than a hundred games, and when it recognizes a design pattern from existing games, it suggests elements to include, and from which game. https://t.co/f3nwXR2c9s"", 'So, if you are creating a space-themed shooter and have created spaceships and aliens, it suggests that you may want a missile and a firing mechanic. Of course, you may not want that - you may want to make a nonviolent game, so feel free to ignore the suggestion.', 'But Pitako takes its cues from existing games in its database, and those are mostly reimplemented versions of classic arcade games. To make all this possible, we build on the @gvgai framework, which includes loads of games implemented in the Video Game Description Language.', 'The nice thing about building on VGDL is that we can recommend rules as well as sprites and behavior, and we can abstract from specific implementations in individual games to general rules. At the core, we use the classic Apriori algorithm for this. https://t.co/KuRTaLisKq', 'Finally, Pitako also recommends where to place sprites in levels. Again, this is based on patterns extracted from the levels of existing games, but augmented with a few heuristics. https://t.co/ChEOs1hjlZ', 'The paper, by @Jaspier @dgopstein @nealen and myself, will be presented at @cog2019ieee, which is gearing up to be a really fantastic conference. You should be there.', 'Technically, Pitako is a sub-system of Cicero, a versatile tool for AI-assisted design built on the GVGAI framework and the VGDL language. Some previous work on Cicero can be found here:\nhttps://t.co/j2Ap9G3jBZ']",https://arxiv.org/abs/1907.03877,"Recommender Systems are widely and successfully applied in e-commerce. Could they be used for design? In this paper, we introduce Pitako1, a tool that applies the Recommender System concept to assist humans in creative tasks. More specifically, Pitako provides suggestions by taking games designed by humans as inputs, and recommends mechanics and dynamics as outputs. Pitako is implemented as a new system within the mixed-initiative AI-based Game Design Assistant, Cicero. This paper discusses the motivation behind the implementation of Pitako as well as its technical details and presents usage examples. We believe that Pitako can influence the use of recommender systems to help humans in their daily tasks. ",Pitako -- Recommending Game Design Elements in Cicero
67,1149116995893420032,450326907,Robert Serrano,['My first co-authored paper on kicked black holes in rotating star clusters.\nWe find a new phase in the orbital decay of an orbiting black hole due to angular momentum exchange from the cluster to the black hole. <LINK>'],https://arxiv.org/abs/1907.04330,"In this paper, we continue our study on the evolution of black holes (BHs) that receive velocity kicks at the origin of their host star cluster potential. We now focus on BHs in rotating clusters that receive a range of kick velocities in different directions with respect to the rotation axis. We perform N-body simulations to calculate the trajectories of the kicked BHs and develop an analytic framework to study their motion as a function of the host cluster and the kick itself. Our simulations indicate that for a BH that is kicked outside of the cluster's core, as its orbit decays in a rotating cluster the BH will quickly gain angular momentum as it interacts with stars with high rotational frequencies. Once the BH decays to the point where its orbital frequency equals that of local stars, its orbit will be circular and dynamical friction becomes ineffective since local stars will have low relative velocities. After circularization, the BH's orbit decays on a longer timescale than if the host cluster was not rotating. Hence BHs in rotating clusters will have longer orbital decay times. The timescale for orbit circularization depends strongly on the cluster's rotation rate and the initial kick velocity, with kicked BHs in slowly rotating clusters being able to decay into the core before circularization occurs. The implication of the circularization phase is that the probability of a BH undergoing a tidal capture event increases, possibly aiding in the formation of binaries and high-mass BHs. ","The evolution of kicked stellar-mass black holes in star cluster
  environments II. Rotating star clusters"
68,1149115977273098240,1011309405273513986,NarangLab,['Excited to share a new paper from the group on arXiv on excited-state #nanophotonics and #polaritonic #chemistry \n<LINK>'],https://arxiv.org/abs/1907.04646,"Advances in nanophotonics, quantum optics, and low-dimensional materials have enabled precise control of light-matter interactions down to the nanoscale. Combining concepts from each of these fields, there is now an opportunity to create and manipulate photonic matter via strong coupling of molecules to the electromagnetic field. Towards this goal, here we introduce a first principles framework to calculate polaritonic excited-state potential-energy surfaces for strongly coupled light-matter systems. In particular, we demonstrate the applicability of our methodology by calculating the polaritonic excited-state manifold of a Formaldehyde molecule strongly coupled to an optical cavity. This proof-of-concept calculation shows how strong coupling can be exploited to alter photochemical reaction pathways by influencing avoided crossings. Therefore, by introducing an ab initio method to calculate excited-state potential-energy surfaces, our work opens a new avenue for the field of polaritonic chemistry. ","Excited-State Nanophotonic and Polaritonic Chemistry with Ab initio
  Potential-Energy Surfaces"
69,1149091535289540608,109394186,Rodrigo Canaan,"['In Hanabi, if you want to collaborate with arbitrary partners (including humans), you need to make sure your agent plays well when paired with players using diverse strategies. Our new paper at @cog2019ieee addresses this issue through quality-diversity <LINK> <LINK>', 'We put forward two metrics we can use to characterize an agent - Communicativeness and Risk Aversion - and use Map Elites to find the best agent we can at each point of the behavior space defined by these metrics https://t.co/X3dQV543cc']",https://arxiv.org/abs/1907.03840,"In complex scenarios where a model of other actors is necessary to predict and interpret their actions, it is often desirable that the model works well with a wide variety of previously unknown actors. Hanabi is a card game that brings the problem of modeling other players to the forefront, but there is no agreement on how to best generate a pool of agents to use as partners in ad-hoc cooperation evaluation. This paper proposes Quality Diversity algorithms as a promising class of algorithms to generate populations for this purpose and shows an initial implementation of an agent generator based on this idea. We also discuss what metrics can be used to compare such generators, and how the proposed generator could be leveraged to help build adaptive agents for the game. ",Diverse Agents for Ad-Hoc Cooperation in Hanabi
70,1149081656520822784,19510090,Julian Togelius,"[""Hanabi is a card game where you must collaborate. You know everyone else's cards, but not your own. How do you collaborate when you don't know your partners' strategy?\nIn a new paper, we address the challenge of finding sets of collaborative strategies.\n<LINK> <LINK>"", 'The key to doing this is quality-diversity algorithms, which are evolutionary-like algorithms that can find diverse sets of solutions. We define the metrics of Communicativeness and Risk Aversion, and find as-good-as-possible strategies for each parameter combination. https://t.co/vFbjNL4j6U', 'How does this help us collaborate? Well, we can then measure which strategy collaborates best with other strategies, and find the best collaborating strategy pairs. Some strategies are better at collaborating overall, but others collaborate well with very specific strategies. https://t.co/F3gTQdD6Yx', 'Once you have a library of collaborative strategies, you can create an agent which recognizes the strategy of its teammate and tries to match it for optimal collaboration. This is part of our ongoing project on collaborative agents, with applications far beyond Hanabi.', 'Our paper, by @rocanaan, me, @nealen, Stefan Menzel, will be presented at @cog2019ieee. We will also participate in the Hanabi competition arranged at the conference, which has a track for ad-hoc playing.', 'How does this relate to the work by @DeepMindAI (@j_foerst, @sharky6000, @marcgbellemare, @MichaelHBowling and others) on Hanabi playing? They show that strategies that perform very well in self-play do not necessarily perform well with unknown team-mates.\nhttps://t.co/ptf7ABkPKR https://t.co/d4TWTH3H4x', 'In comparison, our agents are less strong in self-play, but we manage to find sets of agents that perform well across a large range of playstyles, and investigate which of them play well with each other. The goal is a meta- or ensemble agent that plays well with any partner.', '@j_foerst Thanks! We hope we can get stronger agents in the future with a better policy representation. Even with the relatively limited representation we used in this paper we found one region of space with good (not great) agents in self-play. Importantly though, they collaborate well :)', '@j_foerst I wonder that too. And if so, what the scope is for adapting / switching between strategies within a single game. I think it is very likely that if you play (say, 10) repeated games with another agent, or watch that agent play, adaptation will be beneficial.']",https://arxiv.org/abs/1907.03840,"In complex scenarios where a model of other actors is necessary to predict and interpret their actions, it is often desirable that the model works well with a wide variety of previously unknown actors. Hanabi is a card game that brings the problem of modeling other players to the forefront, but there is no agreement on how to best generate a pool of agents to use as partners in ad-hoc cooperation evaluation. This paper proposes Quality Diversity algorithms as a promising class of algorithms to generate populations for this purpose and shows an initial implementation of an agent generator based on this idea. We also discuss what metrics can be used to compare such generators, and how the proposed generator could be leveraged to help build adaptive agents for the game. ",Diverse Agents for Ad-Hoc Cooperation in Hanabi
71,1149037407192453124,831519512214245377,Dr. Fran Concha-Ram√≠rez,"['My new paper with @Martijn_Wilhelm, S. Portegies Zwart and T. Haworth is out on arXiv today: \n<LINK>\n\nWe run simulations of star clusters of different densities, where some of the stars have protoplanetary disks around them. Short thread about our results:', 'The disks are subject to intrinsic viscous growth, truncations due to nearby encounters with other stars, and external photoevaporation caused by bright, massive stars in the vicinity. We look to find out how these processes affect disk survival (and subsequent planet formation).', 'We find that external photoevaporation is very efficient in depleting disk mass: between 60% and 80% of disks get destroyed before 2 Myr, in the early dynamical stages of cluster evolution. https://t.co/wMMeQvsb5k', ""Mass lost due to encounters with other stars is negligible compared to the effects of photoevaporation. The scope of these encounters is also very local, whereas photoevaporation affects all stars in the cluster all through the cluster's evolution (though in different degrees). https://t.co/8A8DvyecLS"", 'Our results are in good agreement with observational estimates of disk lifetimes. They also support observational evidence that suggests that rocky gas giant cores/planets must start forming very early on (before 1 Myr of disk evolution).', 'Our biggest caveat: our results are strongly dependent on initial conditions (the bane of simulations). Initial stellar densities can strongly affect to what degree each of these disk dispersal mechanisms is important.', 'We will continue investigating the effect of initial conditions in future work. \nFor now, comments and questions are very welcome!', '@nespinozap @Martijn_Wilhelm Thanks! It does impose such constraints, which is v interesting because most stars are formed in clusters. So it also constrains the regions where stars w/ planets might have formed!', ""@nespinozap @Martijn_Wilhelm We have not taken stellar properties such as metallicity into account, and also we don't treat the dust and gas components of the disk separately. It could definitely affect our results!"", '@MikeGrudic @Martijn_Wilhelm https://t.co/b1qTj38Sto']",https://arxiv.org/abs/1907.03760,"Planet-forming circumstellar disks are a fundamental part of the star formation process. Since stars form in a hierarchical fashion in groups of up to hundreds or thousands, the UV radiation environment that these disks are exposed to can vary in strength by at least six orders of magnitude. This radiation can limit the masses and sizes of the disks. Diversity in star forming environments can have long lasting effects in disk evolution and in the resulting planetary populations. We perform simulations to explore the evolution of circumstellar disks in young star clusters. We include viscous evolution, as well as the impact of dynamical encounters and external photoevaporation. We find that photoevaporation is an important process in destroying circumstellar disks: in regions of stellar density $\rho \sim 100 \mathrm{\ M}_\odot \mathrm{\ pc}^{-3}\mathrm{\ }$ around 80% of disks are destroyed before 2 Myr of cluster evolution. Our findings are in agreement with observed disk fractions in young star forming regions and support previous estimations that planet formation must start in timescales < 0.1 - 1 Myr. ","External photoevaporation of circumstellar disks constrains the
  timescale for planet formation"
72,1148944337700610048,2180768821,Erik Hoel,"['Do networks have an intrinsic scale? A new paper by myself &amp; @jkbren on causal emergence shows they do, and how ""scale-free"" networks are the critical point for developing scale. I\'m particularly proud of this paper, which is packed with interesting stuff. <LINK> <LINK>', 'Some of this interesting stuff: how random networks contain a fixed amount of effective information, how to construct accurate macroscales using higher-order dependencies, biological networks being more causally emergent than technological networks (continued)', 'more interesting stuff: random networks contain effectively no causal emergence, causal emergence is basically a clustering algorithm for noise in a network, effective information grows only when structure does, how to categorize all networks in terms of determinism/degeneracy...', ""... and I could go on. Possibly this could have been several papers but I'm happy to have something so dense and enjoyable and hope that others find these techniques to accurately model higher scales in networks useful. This is a preprint - it's under submission at a good journal""]",https://arxiv.org/abs/1907.03902,"The connectivity of a network contains information about the relationships between nodes, which can denote interactions, associations, or dependencies. We show that this information can be analyzed by measuring the uncertainty (and certainty) contained in paths along nodes and links in a network. Specifically, we derive from first principles a measure known as effective information and describe its behavior in common network models. Networks with higher effective information contain more information in the relationships between nodes. We show how subgraphs of nodes can be grouped into macro-nodes, reducing the size of a network while increasing its effective information (a phenomenon known as causal emergence). We find that informative higher scales are common in simulated and real networks across biological, social, informational, and technological domains. These results show that the emergence of higher scales in networks can be directly assessed and that these higher scales offer a way to create certainty out of uncertainty. ",The emergence of informative higher scales in complex networks
73,1148836374063325184,398987599,Andrea Rapisarda,"['New paper online ""On the origins of extreme wealth inequality in the Talent vs Luck Model"" <LINK>']",https://arxiv.org/abs/1907.04237,"While wealth distribution in the world is highly skewed and heavy-tailed, human talent - as the majority of individual features - is normally distributed. In a recent computational study by Pluchino et al [Talent vs luck: The role of randomness in success and failure, Adv. Complex Syst. 21 (03-04) (2018) 1850014], it has been shown that the combined effects of both random external factors (lucky and unlucky events) and multiplicative dynamics in capital accumulation are able to clarify this apparent contradiction. We introduce here a simplified version (STvL) of the original Talent versus Luck (TvL) model, where only lucky events are present, and verify that its dynamical rules lead to the same very large wealth inequality as the original model. We also derive some analytical approximations aimed to capture the mechanism responsible for the creation of such wealth inequality from a Gaussian-distributed talent. Under these approximations, our analysis is able to reproduce quite well the results of the numerical simulations of the simplified model in special cases. On the other hand, it also shows that the complexity of the model lies in the fact that lucky events are transformed into an increase of capital with heterogeneous rates, which yields a non-trivial generalization of the role of multiplicative processes in generating wealth inequality, whose fully generic case is still not amenable to analytical computations. ",On the origins of extreme wealth inequality in the Talent vs Luck Model
74,1148767008353337352,942850262145863680,Leonardo Andreta de Castro,"['New paper on arXiv. In it, @SwarnadeepMaju3 , @kenbrownquantum and I discuss how some qubits could be used as probes to recalibrate a quantum computer in real time and prevent the error rate from crossing a threshold \n<LINK>']",https://arxiv.org/abs/1907.03864,"Accurate control of quantum systems requires precise measurement of the parameters that govern the dynamics, including control fields and interactions with the environment. Parameters will drift in time and experiments interleave protocols that perform parameter estimation with protocols that measure the dynamics of interest. Here we specialize to a system made of qubits where the dynamics correspond to a quantum computation. We propose setting aside some qubits, which we call spectator qubits, to be measured periodically during the computation, to act as probes of the changing experimental and environmental parameters. By using control strategies that minimize the sensitivity of the qubits involved in the computation, we can acquire sufficient information from the spectator qubits to update our estimates of the parameters and improve our control. As a result, we can increase the length of experiment where the dynamics of the data qubits are highly reliable. In particular, we simulate how spectator qubits can keep the error level of operations on data qubits below a $10^{-4}$ threshold in two scenarios involving coherent errors: a classical magnetic field gradient dynamically decoupled with sequences of two or four $\pi$-pulses, and laser beam instability detected via crosstalk with neighboring atoms in an ion trap. ",Real-time calibration with spectator qubits
75,1148667412453236738,1556664198,Kyle Cranmer,"['New paper:\nEtalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale. @atilimgunes, Shao, Bhimji, @lukasheinrich_ , Meadows, Liu, Munk, Naderiparizi, Gram-Hansen, @glouppe , Ma, Zhao, Torr, Lee, @KyleCranmer, Prabhat, &amp; Wood <LINK> <LINK>', 'A tour de force by the team. üëèüëèüëè\n@PyTorch+MPI model trained on 1024 32-core CPU nodes reaching 450 Tflop/s üî•\n\nTrivia: Etalumis is a a joke... it\'s ""simulate"" backwards, and that\'s what we are trying to do... invert the simulation.']",https://arxiv.org/abs/1907.03382,"Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN--LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global minibatch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL. ","Etalumis: Bringing Probabilistic Programming to Scientific Simulators at
  Scale"
76,1148631297247588354,706175245976248321,Oscar Viyuela,"['Our new paper on the arXiv: Simulating gauge theories in the lab! (<LINK>) with Alessio Celi, Benoit Vermersch, Hannes Pichler, Mikhail Lukin &amp; Peter Zoller!']",https://arxiv.org/abs/1907.03311,"Solving strongly coupled gauge theories in two or three spatial dimensions is of fundamental importance in several areas of physics ranging from high-energy physics to condensed matter. On a lattice, gauge invariance and gauge invariant (plaquette) interactions involve (at least) four-body interactions that are challenging to realize. Here we show that Rydberg atoms in configurable arrays realized in current tweezer experiments are the natural platform to realize scalable simulators of the Rokhsar-Kivelson Hamiltonian --a 2D U(1) lattice gauge theory that describes quantum dimer and spin-ice dynamics. Using an electromagnetic duality, we implement the plaquette interactions as Rabi oscillations subject to Rydberg blockade. Remarkably, we show that by controlling the atom arrangement in the array we can engineer anisotropic interactions and generalized blockade conditions for spins built of atom pairs. We describe how to prepare the resonating valence bond and the crystal phases of the Rokhsar-Kivelson Hamiltonian adiabatically, and probe them and their quench dynamics by on-site measurements of their quantum correlations. We discuss the potential applications of our Rydberg simulator to lattice gauge theory and exotic spin models. ",Emerging 2D Gauge theories in Rydberg configurable arrays
77,1148594342384295937,1091815542334337024,Scott Niekum,"['Want to get up to speed on what‚Äôs  happening in robot learning for manipulation? Oliver Kroemer, George Konidaris, and I have just released a new survey paper: <LINK>. And please let us know if there is important work we missed ‚Äî it is a huge and growing field!', '@animesh_garg Yes, quite a bit longer than we‚Äôd like to admit! Thanks for your feedback in early ideas!', '@RezaAhmadzadeh_ That was fast!  We will update it if we missed anything but this is essentially the final version that we are submitting to a journal.']",http://arxiv.org/abs/1907.03146,"A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges. ","A Review of Robot Learning for Manipulation: Challenges,
  Representations, and Algorithms"
78,1148569570833313792,1608529771,Michael W. Cole,"['Lab\'s new theory paper unifying biological &amp; artificial neural nets via activity flow. Network coding (encoding/decoding) models simulate how cognition emerges via connectivity. ""Brain network organization as the computational architecture of cognition‚Äù <LINK> <LINK>']",https://arxiv.org/abs/1907.03612,"Understanding neurocognitive computations will require not just localizing cognitive information distributed throughout the brain but also determining how that information got there. We review recent advances in linking empirical and simulated brain network organization with cognitive information processing. Building on these advances, we offer a new framework for understanding the role of connectivity in cognition - network coding (encoding/decoding) models. These models utilize connectivity to specify the transfer of information via neural activity flow processes, successfully predicting the formation of cognitive representations in empirical neural data. The success of these models supports the possibility that localized neural functions mechanistically emerge (are computed) from distributed activity flow processes that are specified primarily by connectivity patterns. ",Discovering the Computational Relevance of Brain Network Organization
79,1148547363985928195,839104540985151490,IPPP Durham,"['New IPPP paper: ""Realistic simulations of galaxy formation in f(R) modified gravity"" by Christian Arnold et al. <LINK>']",http://arxiv.org/abs/1907.02977,"We have carried out a set of cosmological hydrodynamical simulations that follow galaxy formation in $f(R)$ modified gravity models. Our simulations employ the Illustris-TNG full physics model and a new modified gravity solver in the AREPO code. For the first time we are able to investigate the degeneracy in the matter power spectrum between the effects of $f(R)$-gravity and feedback from active galactic nuclei (AGN), and the imprint of modified gravity on the properties of galaxies and on the distribution of dark matter, gas and stars in the universe. $f(R)$-gravity has an observable effect on the neutral hydrogen power spectrum at high redshift at a level of 20%. For both the F6 and F5 models, this is significantly larger than the predicted errors for the SKA1-MID survey, making this probe a powerful test of gravity on large scales. A similar effect is present in the power spectrum of the stars at high redshift. We also show that rotationally supported disc galaxies can form in $f(R)$-gravity, even in the partially screened regime. Our simulations indicate that there might be more disc galaxies in F6 compared to GR, and fewer in F5. Finally, we show that the back reaction between AGN feedback and modified gravity in the matter power spectrum is not important in the F6 model but has a sizeable effect in F5. ",Realistic simulations of galaxy formation in f(R) modified gravity
80,1148547362673057792,839104540985151490,IPPP Durham,"['New IPPP paper: ""A high-redshift test of gravity using enhanced growth of small structures..."" by Matteo Leo et al. <LINK>']",http://arxiv.org/abs/1907.02981,"Future 21 cm intensity mapping surveys such as SKA can provide precise information on the spatial distribution of the neutral hydrogen (HI) in the post-reionization epoch. This information will allow to test the standard LCDM paradigm and with that the nature of gravity. In this work, we employ the SHYBONE simulations, which model galaxy formation in f(R) modified gravity using the IllustrisTNG model, to study the effects of modified gravity on HI abundance and power spectra. We find that the enhanced growth low-mass dark matter halos experience in f(R) gravity at high redshifts alters the HI power spectrum and can be observable through 21 cm intensity mapping. Our results suggest that the HI power spectrum is suppressed by 13\% on scales $k\lesssim2\,h\,\mathrm{Mpc}^{-1}$ at z=2 for F6, a f(R) model which passes most observational constraints. We show that this suppression can be detectable by SKA1-MID with 1000 hours of exposure time, making HI clustering a novel test of gravity at high redshift. ","High-redshift test of gravity using enhanced growth of small structures
  probed by the neutral hydrogen distribution"
81,1148529673946705920,1601296094,David Bowler,['New paper from my PhD student Jack Poulton on Al dopants in Si: pairs of dopants are inactive unless in a high spin state\n\n<LINK>'],https://arxiv.org/abs/1907.03636,"We have used density functional theory to study the energetics and electronic structure of aluminium dopants in crystalline silicon. We present data regarding the atomic and electronic structure and properties of pairs of substitutional aluminium dopants. We find that pairs of dopants, when occupying nearest neighbouring subsitutional sites in a high spin state, can bond to form aluminium pairs. This suggests that such a configuration of dopants will be electrically active when made to occupy a high spin state, whereas in the low spin state the neighbouring dopant pairs are found to be self compensating. ",An Ab Initio Study of Aluminium self-compensation in Bulk Silicon
82,1148509903763509248,752502333720911877,Niels Justesen,"['In our new paper (with Miguel Duque, Daniel Jaramillo, @jb_mouret &amp; @risi1979) we present BRIL: Behavioral Repertoire Imitation Learning. It learns a policy from demonstrations that can be modulated with behavioral features to express different behaviors. <LINK> <LINK>']",https://arxiv.org/abs/1907.03046,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach. ",Learning a Behavioral Repertoire from Demonstrations
83,1148320485886873600,19510090,Julian Togelius,"['How can you use AI methods to balance a game? In particular, a complex game such as Hearthstone, with hundreds of cards? Through multiobjective evolutionary algorithms. In our new paper, we show how to rebalance the game while making minimal card changes.\n<LINK> <LINK>', 'In our experiments, we use an existing game-playing agent, which has different performance for different classes. We then nerf and buff cards so the agent plays all classes equally well. This can be seen as a form of handicapping.', 'The key is to change the cards as little as possible, so the player does not have to relearn the game. That is why we use multiobjective evolution, with one of the objectives being to minimize changes. https://t.co/vhCLxqhizY', 'This procedure could, of course, be applied to many other games, as well as to non-game processes. We chose Hearthstone because it is so complex - most games would probably be easier to balance.', 'The paper, which is written by @EuMyself, @rocanaan, @scotchkorean27, Matthew Fontaine, myself, and @amykhoover, will be presented at @cog2019ieee. Looking forward to it!', '@jacobmbuckman @hardmaru Quite possible. In general, the competition between players, other players, and game configurations is what forms the meta. So if we rebalanced the meta for a particular player or set of players, it makes sense that we should also be able to find a new player to break the meta.']",https://arxiv.org/abs/1907.01623,"Balancing an ever growing strategic game of high complexity, such as Hearthstone is a complex task. The target of making strategies diverse and customizable results in a delicate intricate system. Tuning over 2000 cards to generate the desired outcome without disrupting the existing environment becomes a laborious challenge. In this paper, we discuss the impacts that changes to existing cards can have on strategy in Hearthstone. By analyzing the win rate on match-ups across different decks, being played by different strategies, we propose to compare their performance before and after changes are made to improve or worsen different cards. Then, using an evolutionary algorithm, we search for a combination of changes to the card attributes that cause the decks to approach equal, 50% win rates. We then expand our evolutionary algorithm to a multi-objective solution to search for this result, while making the minimum amount of changes, and as a consequence disruption, to the existing cards. Lastly, we propose and evaluate metrics to serve as heuristics with which to decide which cards to target with balance changes. ",Evolving the Hearthstone Meta
84,1147835644372226050,730069366205779970,Chen Chen,"['Getting tired of labelling data for finetuning/adapting a neural network to segment CMR images acquired from a new site?  Check our latest paper for improving the model generalisability for the cross-site, cross-scanner CMR image segmentation task! <LINK>']",http://arxiv.org/abs/1907.01268,"Convolutional neural network (CNN) based segmentation methods provide an efficient and automated way for clinicians to assess the structure and function of the heart in cardiac MR images. While CNNs can generally perform the segmentation tasks with high accuracy when training and test images come from the same domain (e.g. same scanner or site), their performance often degrades dramatically on images from different scanners or clinical sites. We propose a simple yet effective way for improving the network generalization ability by carefully designing data normalization and augmentation strategies to accommodate common scenarios in multi-site, multi-scanner clinical imaging data sets. We demonstrate that a neural network trained on a single-site single-scanner dataset from the UK Biobank can be successfully applied to segmenting cardiac MR images across different sites and different scanners without substantial loss of accuracy. Specifically, the method was trained on a large set of 3,975 subjects from the UK Biobank. It was then directly tested on 600 different subjects from the UK Biobank for intra-domain testing and two other sets for cross-domain testing: the ACDC dataset (100 subjects, 1 site, 2 scanners) and the BSCMR-AS dataset (599 subjects, 6 sites, 9 scanners). The proposed method produces promising segmentation results on the UK Biobank test set which are comparable to previously reported values in the literature, while also performing well on cross-domain test sets, achieving a mean Dice metric of 0.90 for the left ventricle, 0.81 for the myocardium and 0.82 for the right ventricle on the ACDC dataset; and 0.89 for the left ventricle, 0.83 for the myocardium on the BSCMR-AS dataset. The proposed method offers a potential solution to improve CNN-based model generalizability for the cross-scanner and cross-site cardiac MR image segmentation task. ","Improving the generalizability of convolutional neural network-based
  segmentation on CMR images"
85,1147511877104295936,1655032190,Enrico Corsaro,['Check out our new paper on asteroseismology and dynamo action of the sub-giant star EK Eridani! <LINK>'],https://arxiv.org/abs/1907.01338,"We present further evidence of the presence of acoustic oscillations on the slowly-rotating, over-active G8 sub-giant EK Eri. This star was observed with the 1-m Hertzsprung SONG telescope, at the Observatorio del Teide for two different runs of 8 and 13 nights, respectively, and separated by about a year. We determined a significant excess of power around $\nu_\mathrm{max} = 253 \pm 3\,\mu$Hz in the first observing run and we were able to determine the large separation $\Delta\nu = 16.43 \pm 0.22\,\mu$Hz. No significant excess of power was instead detected in a subsequent SONG observing season, as also supported by our analysis of the simultaneous TESS photometric observations. We propose a new amplitude-luminosity relation in order to account for the missing power in the power spectrum. Based on the evolutionary stage of this object we argue that standard $\alpha^2\Omega$ dynamo cannot be excluded as a possible origin for the observed magnetic field. ",Acoustic oscillations and dynamo action in the G8 sub-giant EK Eri
86,1147217655171936257,503452360,William Wang,"[""The order in a dialogue can be a crucial supervised signal for learning. @_jiaweiwu's #acl2019nlp paper Self-Supervised Dialogue Learning introduces a new auxiliary task, inconsistent order detection, to improve dialogue systems. <LINK> #NLProc #acl2019italy <LINK>""]",http://arxiv.org/abs/1907.00448,"The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. ",Self-Supervised Dialogue Learning
87,1147217081789812736,1133088397273313282,francesco croce,"[""Excited to share our new adversarial attack for L1, L2 and Linf!\nOur Fast Adaptive Boundary (FAB) attack achieves the best results on Madry's CIFAR-10 Challenge and MNIST &amp; CIFAR-10 TRADES Challenges!\nPaper <LINK>\nCode <LINK> <LINK>""]",https://arxiv.org/abs/1907.02044,"The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient masking. ","Minimally distorted Adversarial Examples with a Fast Adaptive Boundary
  Attack"
88,1147214248344465408,804069495253962752,David Mart√≠nez Delgado,"['Our new paper on the nature of a shell of young stars in the SMC is now available in <LINK>.  Figure 1 shows our Canon 50 telephoto mosaic of the Magellanic Clouds taken by @YBeletsky , inspired in the photographic plates work made by de Vaucouleurs in the 1950s. <LINK>']",https://arxiv.org/abs/1907.02264,"Understanding the evolutionary history of the Magellanic Clouds requires an in-depth exploration and characterization of the stellar content in their outer regions, which ultimately are key to tracing the epochs and nature of past interactions. We present new deep images of a shell-like over-density of stars in the outskirts of the Small Magellanic Cloud (SMC). The shell, also detected in photographic plates dating back to the fifties, is located at ~1.9 degr from the center of the SMC in the north-east direction.The structure and stellar content of this feature were studied with multi-band, optical data from the Survey of the MAgellanic Stellar History (SMASH) carried out with the Dark Energy Camera on the Blanco Telescope at Cerro Tololo Inter-American Observatory. We also investigate the kinematic of the stars in the shell using the Gaia Data Release 2. The shell is composed of a young population with an age ~ 150 Myr, with no contribution from an old population. Thus, it is hard to explain its origin as the remnant of a tidally disrupted stellar system. The spatial distribution of the young main-sequence stars shows a rich sub-structure, with a spiral arm-like feature emanating from the main shell and a separated small arc of young stars close to the globular cluster NGC 362. We find that the absolute g-band magnitude of the shell is M_{g,shell} = -10.78+/- 0.02, with a surface brightness of mu_{g,shell} = 25.81+/- 0.01 mag/arcsec^{2}. We have not found any evidence that this feature is of tidal origin or a bright part of a spiral arm-like structure. Instead, we suggest that the shell formed in a recent star formation event, likely triggered by an interaction with the Large Magellanic Cloud and/or the Milky Way, ~150 Myr ago. ","On the nature of a shell of young stars in the outskirts of the Small
  Magellanic Cloud"
89,1147098993593372676,852856132632797184,Carl Doersch,"['Training on only synthetic data, our new paper gets near the SOTA on the real-world 3DPW human pose dataset by focusing on motion. Simulation is underappreciated for representation learning; you just have to keep the nets from overfitting to the domain. <LINK> <LINK>']",https://arxiv.org/abs/1907.02499,"Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset. ","Sim2real transfer learning for 3D human pose estimation: motion to the
  rescue"
90,1147010995845586944,2541941466,Alba Cervera-Lierta,"['New paper! üìÑ\n""Data re-uploading for a universal quantum classifier"", Adri√°n P√©rez-Garc√≠a, Alba Cervera-Lierta, Elies Gil-Ferrer and Jos√© Ignacio Latorre.\n<LINK>\nScirate: <LINK>\n@adpersa @EliesMiquel @j_i_latorre @QUANTIC_BSC', '*Adri√°n P√©rez-Salinas @adpersa', ""*Elies Gil-Fuster @EliesMiquel \nüôà what's wrong with me today?""]",https://arxiv.org/abs/1907.02085,"A single qubit provides sufficient computational capabilities to construct a universal quantum classifier when assisted with a classical subroutine. This fact may be surprising since a single qubit only offers a simple superposition of two states and single-qubit gates only make a rotation in the Bloch sphere. The key ingredient to circumvent these limitations is to allow for multiple data re-uploading. A quantum circuit can then be organized as a series of data re-uploading and single-qubit processing units. Furthermore, both data re-uploading and measurements can accommodate multiple dimensions in the input and several categories in the output, to conform to a universal quantum classifier. The extension of this idea to several qubits enhances the efficiency of the strategy as entanglement expands the superpositions carried along with the classification. Extensive benchmarking on different examples of the single- and multi-qubit quantum classifier validates its ability to describe and classify complex data. ",Data re-uploading for a universal quantum classifier
91,1146993240576593920,939498802767044608,Stephan,"['New paper on much faster RL by learning graph representations of the world <LINK>! World graphs capture the structure of the world and can be used to focus exploration: <LINK> w/ Wendy Shang, Alex Trott, @CaimingXiong @RichardSocher @SFResearch <LINK>']",http://arxiv.org/abs/1907.00664,"In many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency. ",Learning World Graphs to Accelerate Hierarchical Reinforcement Learning
92,1146911476017324033,3269288695,Archit Sharma,"['1/ Can we use model-based planning in behavior space rather than action space? DADS can discover skills without any rewards, which can later be composed zero-shot via planning in the behavior space for new tasks.\n\nPaper: <LINK>\nWebsite: <LINK> <LINK>', '2/ DADS discovers meaningful behaviors without any supervision using mutual information based exploration. It simultaneously models the transition dynamics for the learnt skills. We adapt MPC to plan in behavior spaces, which enables the zero-shot skill-composition on new tasks.', '3/ Just like the MuJoCo Ant environment, we can discover locomotion skills for Humanoid agent as well. Again, these skills can be composed using MPC to navigate the agent to different goals: https://t.co/X9TdL8h7se', '4/ This work could not have been possible without my amazing collaborators @GoogleAI: Shixiang Gu, @svlevine, @Vikashplus, @hausman_k.']",https://arxiv.org/abs/1907.01657,"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery. ",Dynamics-Aware Unsupervised Discovery of Skills
93,1146740578614480896,1604541715,"Paul Beck, PhD","['New paper out, lead by Dr. Savita Mathur on ""Revisiting the impact of stellar magnetic activity on the detectability of solar-like oscillations by Kepler"".  <LINK>']",https://arxiv.org/abs/1907.01415,"Over 2,000 stars were observed for one month with a high enough cadence in order to look for acoustic modes during the survey phase of the Kepler mission. Solar-like oscillations have been detected in about 540 stars. The question of why no oscillations were detected in the remaining stars is still open. Previous works explained the non-detection of modes with the high level of magnetic activity. However, the studied stars contained some classical pulsators and red giants that could have biased the results. In this work, we revisit this analysis on a cleaner sample of 1,014 main-sequence solar-like stars. First we compute the predicted amplitude of the modes. We find that the stars with detected modes have an amplitude to noise ratio larger than 0.94. We measure reliable rotation periods and the associated photometric magnetic index for 684 stars and in particular for 323 stars where the mode amplitude is predicted to be high enough to be detected. We find that among these 323 stars 32% have a magnetic activity level larger than the Sun at maximum activity, explaining the non-detection of p modes. Interestingly, magnetic activity cannot be the primary reason responsible for the absence of detectable modes in the remaining 68% of the stars without p modes detected and with reliable rotation periods. Thus, we investigate metallicity, inclination angle, and binarity as possible causes of low mode amplitudes. Using spectroscopic observations for a subsample, we find that a low metallicity could be the reason for suppressed modes. No clear correlation with binarity nor inclination is found. We also derive the lower limit for our photometric activity index (of 20-30 ppm) below which rotation and magnetic activity are not detected. Finally with our analysis we conclude that stars with a photometric activity index larger than 2,000 ppm have 98.3% probability of not having oscillations detected. ","Revisiting the impact of stellar magnetic activity on the detectability
  of solar-like oscillations by Kepler"
94,1146735850136449024,1192577568,Daniel Worrall,"['NEW PAPER @miccai2019! ""Supervised Uncertainty Quantification for Segmentation with Multiple Annotations"". We adapt Prob Unet to output epistemic &amp; CALIBRATED aleatoric uncertainties <LINK>. Work w. Shi Hu, Stefan Knegt, @BasVeeling, Henkjan Huisman &amp; @wellingmax <LINK>', '@stefanknegt']",https://arxiv.org/abs/1907.01949,"The accurate estimation of predictive uncertainty carries importance in medical scenarios such as lung node segmentation. Unfortunately, most existing works on predictive uncertainty do not return calibrated uncertainty estimates, which could be used in practice. In this work we exploit multi-grader annotation variability as a source of 'groundtruth' aleatoric uncertainty, which can be treated as a target in a supervised learning problem. We combine this groundtruth uncertainty with a Probabilistic U-Net and test on the LIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find that we are able to improve predictive uncertainty estimates. We also find that we can improve sample accuracy and sample diversity. ","Supervised Uncertainty Quantification for Segmentation with Multiple
  Annotations"
95,1146677258851106816,3368978577,James Kuszlewicz,"['Our new paper is out on arXiv! We show how to infer the asteroseismic inclination angle of a star using a hierarchical Bayesian model! <LINK>. Check out <LINK> and <LINK> for the code!', 'A huge thank you also to all my coauthors who made this work possible, including @Thomas_S_North @astrokeat @farrwill @CitationWarrior!', '@simon4nine Cheers mate! How are you doing in Oz?']",https://arxiv.org/abs/1907.01565,"The stellar inclination angle-the angle between the rotation axis of a star and our line of sight-provides valuable information in many different areas, from the characterisation of the geometry of exoplanetary and eclipsing binary systems, to the formation and evolution of those systems. We propose a method based on asteroseismology and a Bayesian hierarchical scheme for extracting the inclination angle of a single star. This hierarchical method therefore provides a means to both accurately and robustly extract inclination angles from red giant stars. We successfully apply this technique to an artificial dataset with an underlying isotropic inclination angle distribution to verify the method. We also apply this technique to 123 red giant stars observed with $\textit{Kepler}$. We also show the need for a selection function to account for possible population-level biases, that are not present in individual star-by-star cases, in order to extend the hierarchical method towards inferring underlying population inclination angle distributions. ",Bayesian hierarchical inference of asteroseismic inclination angles
96,1146641497212764160,345177338,Mark Burgess,['Koalja: from Data Plumbing to Smart Workspaces in the Extended Cloud\n\nNew paper about @AljabrIO hybrid data pipes and workspaces\n<LINK>'],https://arxiv.org/abs/1907.01796,"Koalja describes a generalized data wiring or `pipeline' platform, built on top of Kubernetes, for plugin user code. Koalja makes the Kubernetes underlay transparent to users (for a `serverless' experience), and offers a breadboarding experience for development of data sharing circuitry, to commoditize its gradual promotion to a production system, with a minimum of infrastructure knowledge. Enterprise grade metadata are captured as data payloads flow through the circuitry, allowing full tracing of provenance and forensic reconstruction of transactional processes, down to the versions of software that led to each outcome. Koalja attends to optimizations for avoiding unwanted processing and transportation of data, that are rapidly becoming sustainability imperatives. Thus one can minimize energy expenditure and waste, and design with scaling in mind, especially with regard to edge computing, to accommodate an Internet of Things, Network Function Virtualization, and more. ",Koalja: from Data Plumbing to Smart Workspaces in the Extended Cloud
97,1146501535586816003,2780460254,Dr Deanna C. Hooper,"[""I have a new paper out today! A lot of work has gone in to this paper, I'm so glad it's finished! We studied dark matter - dark radiation interactions using Lyman-alpha data. Open access version here: <LINK>""]",https://arxiv.org/abs/1907.01496v1,"Several interesting Dark Matter (DM) models invoke a dark sector leading to two types of relic particles, possibly interacting with each other: non-relativistic DM, and relativistic Dark Radiation (DR). These models have interesting consequences for cosmological observables, and could in principle solve problems like the small-scale cold DM crisis, Hubble tension, and/or low $\sigma_8$ value. Their cosmological behaviour is captured by the ETHOS parametrisation, which includes a DR-DM scattering rate scaling like a power-law of the temperature, $T^n$. Scenarios with $n=0$, $2$, or $4$ can easily be realised in concrete dark sector set-ups. Here we update constraints on these three scenarios using recent CMB, BAO, and high-resolution Lyman-$\alpha$ data. We introduce a new Lyman-$\alpha$ likelihood that is applicable to a wide range of cosmological models with a suppression of the matter power spectrum on small scales. For $n=2$ and $4$, we find that Lyman-$\alpha$ data strengthen the CMB+BAO bounds on the DM-DR interaction rate by many orders of magnitude. However, models offering a possible solution to the missing satellite problem are still compatible with our new bounds. For $n=0$, high-resolution Lyman-$\alpha$ data bring no stronger constraints on the interaction rate than CMB+BAO data, except for extremely small values of the DR density. Using CMB+BAO data and a theory-motivated prior on the minimal density of DR, we find that the $n=0$ model can reduce the Hubble tension from $4.1\sigma$ to $2.7\sigma$, while simultaneously accommodating smaller values of the $\sigma_8$ and $S_8$ parameters hinted by cosmic shear data. ","] Constraining Dark Matter -- Dark Radiation interactions with CMB, BAO,
  and Lyman-$\alpha$"
98,1146459699841134595,2445765961,Macartan Humphreys,"['Happy to share a new working paper with Philip Dawid and Monica Musio on when you can use information on causal processes to make claims about ""causes of effects"" \n\n(is Y due to X? / process tracing / causal attribution)\n\n<LINK>\n\nHighlights:\n\n1/n', ""* Experiments focus on treatment effects but we often care about whether an outcome is due to a cause\n* That's a harder question and answer usually not identified by experimental data\n* But don't obsess about identification: you can learn lots even if estimands are not identified"", '* Knowledge of mediation processes can tighten bounds even if you cannot observe the mediators.\n\nBut:\n\n1 Getting arbitrarily ""close"" to a causal process does not render causal effects observable\n\n2 Process data better for disconfirming causal relations than for confirming them', ""3. Max learning arises from short processes (when X is a necessary condition for a sufficient condit'n for Y)\n\n4. Understanding conditional fx can tighten bounds more than knowledge of mediation\n\nReminder for me how much to learn working with great people outside your discipline""]",https://arxiv.org/abs/1907.00399,"Suppose X and Y are binary exposure and outcome variables, and we have full knowledge of the distribution of Y, given application of X. From this we know the average causal effect of X on Y. We are now interested in assessing, for a case that was exposed and exhibited a positive outcome, whether it was the exposure that caused the outcome. The relevant ""probability of causation"", PC, typically is not identified by the distribution of Y given X, but bounds can be placed on it, and these bounds can be improved if we have further information about the causal process. Here we consider cases where we know the probabilistic structure for a sequence of complete mediators between X and Y. We derive a general formula for calculating bounds on PC for any pattern of data on the mediators (including the case with no data). We show that the largest and smallest upper and lower bounds that can result from any complete mediation process can be obtained in processes with at most two steps. We also consider homogeneous processes with many mediators. PC can sometimes be identified as 0 with negative data, but it cannot be identified at 1 even with positive data on an infinite set of mediators. The results have implications for learning about causation from knowledge of general processes and of data on cases. ",Bounding Causes of Effects with Mediators
99,1146325984267898881,1064069189143601153,Stephan Rasp,"['Here is my new paper on online learning, a method to tackle instabilities and biases in neural network cloud parameterizations: <LINK>\n\nIt also comes with a @mybinderteam notebook containing all the experiments: <LINK> <LINK>']",https://arxiv.org/abs/1907.01351,"Over the last couple of years, machine learning parameterizations have emerged as a potential way to improve the representation of sub-grid processes in Earth System Models (ESMs). So far, all studies were based on the same three-step approach: first a training dataset was created from a high-resolution simulation, then a machine learning algorithm was fitted to this dataset, before the trained algorithm was implemented in the ESM. The resulting online simulations were frequently plagued by instabilities and biases. Here, coupled online learning is proposed as a way to combat these issues. Coupled learning can be seen as a second training stage in which the pretrained machine learning parameterization, specifically a neural network, is run in parallel with a high-resolution simulation. The high-resolution simulation is kept in sync with the neural network-driven ESM through constant nudging. This enables the neural network to learn from the tendencies that the high-resolution simulation would produce if it experienced the states the neural network creates. The concept is illustrated using the Lorenz 96 model, where coupled learning is able to recover the ""true"" parameterizations. Further, detailed algorithms for the implementation of coupled learning in 3D cloud-resolving models and the super parameterization framework are presented. Finally, outstanding challenges and issues not resolved by this approach are discussed. ","Coupled online learning as a way to tackle instabilities and biases in
  neural network parameterizations"
100,1146294050376560640,1138762581164855298,Christoph Ternes,"['New paper on @arxiv, <LINK> with @MariamTortola. We study the sensitivity of DUNE (@DUNEScience) and JUNO to Quasi-Dirac neutrino oscillations, which can occur when neutrinos obtain both Dirac and Majorana masses, with the latter ones sufficiently small.']",https://arxiv.org/abs/1907.00980,"Quasi-Dirac neutrinos are obtained when the Lagrangian density of a neutrino mass model contains both Dirac and Majorana mass terms, and the Majorana terms are sufficiently small. This type of neutrinos introduces new mixing angles and mass splittings into the Hamiltonian, which will modify the standard neutrino oscillation probabilities. In this paper, we focus on the case where the new mass splittings are too small to be measured, but new angles and phases are present. We perform a sensitivity study for this scenario for the upcoming experiments DUNE and JUNO, finding that they will improve current bounds on the relevant parameters. Finally, we also explore the discovery potential of both experiments, assuming that neutrinos are indeed quasi-Dirac particles. ",Quasi-Dirac neutrino oscillations at DUNE and JUNO
101,1146289136342589440,202069135,laurent besacier,['New paper accepted at Interspeech @interspeech2019: Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-resource Settings (PhD work of Marcely Zanon-Boito). On this task RNN outperforms Transformer and CNN! See <LINK> <LINK>'],https://arxiv.org/abs/1907.00184,"Since Bahdanau et al. [1] first introduced attention for neural machine translation, most sequence-to-sequence models made use of attention mechanisms [2, 3, 4]. While they produce soft-alignment matrices that could be interpreted as alignment between target and source languages, we lack metrics to quantify their quality, being unclear which approach produces the best alignments. This paper presents an empirical evaluation of 3 main sequence-to-sequence models (CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme sequences. This task consists in aligning word sequences in a source language with phoneme sequences in a target language, inferring from it word segmentation on the target side [5]. Evaluating word segmentation quality can be seen as an extrinsic evaluation of the soft-alignment matrices produced during training. Our experiments in a low-resource scenario on Mboshi and English languages (both aligned to French) show that RNNs surprisingly outperform CNNs and Transformer for this task. Our results are confirmed by an intrinsic evaluation of alignment quality through the use of Average Normalized Entropy (ANE). Lastly, we improve our best word discovery model by using an alignment entropy confidence measure that accumulates ANE over all the occurrences of a given alignment pair in the collection. ","Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery
  in Low-resource Settings"
102,1146050901565743105,10547982,"Orad Reshef, PhD","['New paper out on arXiv:\n<LINK>\n\nWe demonstrate a high-Q multi-resonant plasmonic metasurface. Also provide a quick analytic code to help you design it!\n\nFeaturing @MdSaadBinAlam1, @RWBoyd12 and our industrial collaborators at @IridianSpectral Technologies. <LINK>', '@MdSaadBinAlam1 @RWBoyd12 @IridianSpectral Special thanks to everyone at twitter with the great advice on how to include our code with the article, notably @NanoStAndrews @j_bertolotti @johnmdudley @mickeykats @LeeCBassett @gonuke']",https://arxiv.org/abs/1907.00458,"Resonant metasurfaces are devices composed of nanostructured sub-wavelength scatterers that generate narrow optical resonances, enabling applications in filtering, nonlinear optics, and molecular fingerprinting. It is highly desirable for these applications to incorporate such devices with multiple, high-quality-factor resonances; however, it can be challenging to obtain more than a pair of narrow resonances in a single plasmonic surface. Here, we demonstrate a multi-resonant metasurface that operates by extending the functionality of surface lattice resonances, which are the collective responses of arrays of metallic nanoparticles. This device features a series of resonances with high quality factors (Q ~ 40), an order of magnitude larger than what is typically achievable with plasmonic nanoparticles, as well as a narrow free spectral range. This design methodology can be used to better tailor the transmission spectrum of resonant metasurfaces and represents an important step towards the miniaturization of optical devices. ",Multi-resonant high-Q plasmonic metasurfaces
103,1156904131149357056,1095087149932822534,Will Smith,"['New paper accepted to ICCV 2019: ""Orientation-aware Semantic Segmentation on Icosahedron Spheres"" <LINK> Nice to publish something with a former PhD student, now in industry, and on a topic different to their PhD work. Great work @kelp0308!']",https://arxiv.org/abs/1907.12849,"We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art. ",Orientation-aware Semantic Segmentation on Icosahedron Spheres
104,1156828143451136001,1025202283288489984,Sumeet Singh,"['New paper (under review) on learning stabilizable dynamics for model-based RL, using control-theoretic regularization: \n\nPaper: <LINK>\nVideos: <LINK>\nCode: <LINK>\n\n1/2', 'We provide compelling motivation, from both a stability and data efficiency standpoint, for incorporating control-theoretic notions within learning as a means of context-driven hypothesis pruning.\n\nw/ Spencer M. Richards @vikassindhwani J-J Slotine @MarcoPavoneSU\n\n2/2']",https://arxiv.org/abs/1907.13122,"We propose a novel framework for learning stabilizable nonlinear dynamical systems for continuous control tasks in robotics. The key contribution is a control-theoretic regularizer for dynamics fitting rooted in the notion of stabilizability, a constraint which guarantees the existence of robust tracking controllers for arbitrary open-loop trajectories generated with the learned system. Leveraging tools from contraction theory and statistical learning in Reproducing Kernel Hilbert Spaces, we formulate stabilizable dynamics learning as a functional optimization with convex objective and bi-convex functional constraints. Under a mild structural assumption and relaxation of the functional constraints to sampling-based constraints, we derive the optimal solution with a modified Representer theorem. Finally, we utilize random matrix feature approximations to reduce the dimensionality of the search parameters and formulate an iterative convex optimization algorithm that jointly fits the dynamics functions and searches for a certificate of stabilizability. We validate the proposed algorithm in simulation for a planar quadrotor, and on a quadrotor hardware testbed emulating planar dynamics. We verify, both in simulation and on hardware, significantly improved trajectory generation and tracking performance with the control-theoretic regularized model over models learned using traditional regression techniques, especially when learning from small supervised datasets. The results support the conjecture that the use of stabilizability constraints as a form of regularization can help prune the hypothesis space in a manner that is tailored to the downstream task of trajectory generation and feedback control, resulting in models that are not only dramatically better conditioned, but also data efficient. ","Learning Stabilizable Nonlinear Dynamics with Contraction-Based
  Regularization"
105,1156397400451272706,1359972247,Jay Thiagarajan,['Distill-to-Label: Excited to share new paper on using knowledge distillation with multiple instance learning for instance segmentation of histopathology images of breast and colon cancer. Weak supervision FTW. #AI4Health \n\nLink: <LINK> <LINK>'],https://arxiv.org/abs/1907.12926,"Weakly supervised instance labeling using only image-level labels, in lieu of expensive fine-grained pixel annotations, is crucial in several applications including medical image analysis. In contrast to conventional instance segmentation scenarios in computer vision, the problems that we consider are characterized by a small number of training images and non-local patterns that lead to the diagnosis. In this paper, we explore the use of multiple instance learning (MIL) to design an instance label generator under this weakly supervised setting. Motivated by the observation that an MIL model can handle bags of varying sizes, we propose to repurpose an MIL model originally trained for bag-level classification to produce reliable predictions for single instances, i.e., bags of size $1$. To this end, we introduce a novel regularization strategy based on virtual adversarial training for improving MIL training, and subsequently develop a knowledge distillation technique for repurposing the trained MIL model. Using empirical studies on colon cancer and breast cancer detection from histopathological images, we show that the proposed approach produces high-quality instance-level prediction and significantly outperforms state-of-the MIL methods. ","Distill-to-Label: Weakly Supervised Instance Labeling Using Knowledge
  Distillation"
106,1156307951050797058,730104103121125376,"Kirsten R. Hall, PhD","['My new paper is on the arXiv: ""Quantifying the Thermal Sunyaev-Zel\'dovich Effect and Excess Millimeter Emission in Quasar Environments"" using data from @TheNRAO #VLA, @ACT_Pol, and @ESAHerschel. You can read it here! <LINK> <LINK>']",https://arxiv.org/abs/1907.11731,"In this paper we probe the hot, post-shock gas component of quasar-driven winds through the thermal Sunyaev-Zel'dovich (tSZ) effect. Combining datasets from the Atacama Cosmology Telescope, the $\textit{Herschel}$ Space Observatory, and the Very Large Array, we measure average spectral energy distributions (SEDs) of 109,829 optically-selected, radio quiet quasars from 1.4~GHz to 3000~GHz in six redshift bins between $0.3<z<3.5$. We model the emission components in the radio and far-infrared, plus a spectral distortion from the tSZ effect. At $z>1.91$, we measure the tSZ effect at $3.8\sigma$ significance with an amplitude corresponding to a total thermal energy of $3.1\times10^{60}$ ergs. If this energy is due to virialized gas, then our measurement implies quasar host halo masses are $\sim6\times10^{12}~h^{-1}$M$_\odot$. Alternatively, if the host dark matter halo masses are $\sim2\times10^{12}~h^{-1}$M$_\odot$ as some measurements suggest, then we measure a $>$90 per cent excess in the thermal energy over that expected due to virialization. If the measured SZ effect is primarily due to hot bubbles from quasar-driven winds, we find that $(5^{+1.2}_{-1.3}$) per cent of the quasar bolometric luminosity couples to the intergalactic medium over a fiducial quasar lifetime of 100 Myr. An additional source of tSZ may be correlated structure, and further work is required to separate the contributions. At $z\leq1.91$, we detect emission at 95 and 148~GHz that is in excess of thermal dust and optically thin synchrotron emission. We investigate potential sources of this excess emission, finding that CO line emission and an additional optically thick synchrotron component are the most viable candidates. ","Quantifying the Thermal Sunyaev-Zel'dovich Effect and Excess Millimeter
  Emission in Quasar Environments"
107,1156171945207062528,192826908,Jorge Lillo-Box,"['A mistery lasting several decades is now resolved: HR10 is a binary star with both components having their own circumstellar envelopes. No falling evaporating bodies! We put together 32 years of RVs and new PIONEER observations from @ESO in our new paper <LINK> <LINK>', '@ESO Great job by the team lead by Benjam√≠n Montesinos  from the @C_Astrobiologia  and also including Isa Rebollido (@Yollevoreloj ), and @villavrr from @UAM_Madrid and Steve Ertel among many others. Check out this PIONEER (@ESO_Chile) orbit of the two components of the binary! https://t.co/1VBPOeSK2O']",https://arxiv.org/abs/1907.12441,"This paper is framed within a large project devoted to studying the presence of circumstellar material around main sequence stars, and looking for exocometary events. The work concentrates on HR 10 (A2 IV/V), known for its conspicuous variability in the circumstellar narrow absorption features of Ca II K and other lines, so far interpreted as $\beta$ Pic-like phenomena, within the falling evaporating body scenario. The main goal of this paper is to carry out a thorough study of HR 10 to find the origin of the observed variability, determine the nature of the star, its absolute parameters, and evolutionary status. Interferometric near-infrared (NIR) observations, multi-epoch high-resolution optical spectra spanning a time baseline of more than 32 years, and optical and NIR photometry, together with theoretical modelling, were used to tackle the above objectives. Our results reveal that HR 10 is a binary. The narrow circumstellar absorption features superimposed on the photospheric Ca II K lines -- and lines of other species -- can be decomposed into two or more components, the two deep ones tracing the radial velocity of the individual stars, which implies that their origin cannot be ascribed to transient exocometary events, their variability being fully explained by the binarity of the object. There does not appear to be transient events associated with potential exocomets. Each individual star holds its own circumstellar shell and there are no traces of a circumbinary envelope. The combined use of the interferometric and radial velocity data leads to a complete spectrometric and orbital solution for the binary, the main parameters being: an orbital period of 747.6 days, eccentricities of the orbits around the centre of mass 0.25 (HR 10-A), 0.21 (HR 10-B) and a mass ratio of q=M$_{\rm B}$/M$_{\rm A}$=0.72-0.84. The stars are slightly off the main sequence, the binary being $\sim530$ Myr old. ","HR 10: A main-sequence binary with circumstellar envelopes around both
  components. Discovery and analysis"
108,1155915495046078465,4887259509,Mohammadreza Mousaei,['Checkout our new paper about autonomous aerial vehicle fault detection @arXiv_Daily @arxiv @CMU_Robotics @airlabcmu #uav #Robotics (data for this paper is open sourced and will be published soon) \n<LINK>'],https://arxiv.org/abs/1907.00511,"The recent increase in the use of aerial vehicles raises concerns about the safety and reliability of autonomous operations. There is a growing need for methods to monitor the status of these aircraft and report any faults and anomalies to the safety pilot or to the autopilot to deal with the emergency situation. In this paper, we present a real-time approach using the Recursive Least Squares method to detect anomalies in the behavior of an aircraft. The method models the relationship between correlated input-output pairs online and uses the model to detect anomalies. The result is an easy-to-deploy anomaly detection method that does not assume a specific aircraft model and can detect many types of faults and anomalies in a wide range of autonomous aircraft. The experiments on this method show a precision of 88.23%, recall of 88.23%, and 86.36% accuracy for over 22 flight tests. The other contribution is providing a new fault detection open dataset for autonomous aircraft, which contains complete data and the ground truth for 22 fixed-wing flights with eight different types of mid-flight actuator failures to help future fault detection research for aircraft. The source code and the dataset can be accessed from this https URL ",Automatic Real-time Anomaly Detection for Autonomous Aerial Vehicles
109,1155189343348240389,49160998,Mike McConnell,"[""Here's a surprising topic for a new paper from my group.\nImplementation of a Hamming-Distance-Like Genomic Quantum Classifier Using Inner Products on... - <LINK> #ScholarAlerts""]",https://arxiv.org/abs/1907.08267,"Motivated by the problem of classifying individuals with a disease versus controls using functional genomic attributes as input, we encode the input as a string of 1s (presence) or 0s (absence) of the genomic attribute across the genome. Blocks of physical regions in the subdivided genome serve as the feature dimensions, which takes full advantage of remaining in the computational basis of a quantum computer. Given that a natural distance between two binary strings is the Hamming distance and that this distance shares properties with an inner product between qubits, we developed two Hamming-distance-like classifiers which apply two different kinds of inner products (""active"" and ""symmetric"") to directly and efficiently classify a test input into either of two training classes. To account for multiple disease and control samples, each train class can be composed of an arbitrary number of bit strings (i.e., number of samples) that can be compressed into one input string per class. Thus, our circuits require the same number of qubits regardless of the number of training samples. These algorithms, which implement a training bisection decision plane, were simulated and implemented on IBMQX4 and IBMQX16. The latter allowed for encoding of $64$ training features across the genome for $2$ (disease and control) training classes. ","Implementation of a Hamming-Distance-Like Genomic Quantum Classifier
  Using Inner Products on IBMQX4 and IBMQX16"
110,1155186750605496320,18262687,Rushil,"['An exciting new paper from our group : How do you compute the scalar field #topology of a massive dataset ( &gt; 10M), &amp; use this info in evaluating the deep learning model? \nCheck out the paper which will appear soon in @ieeevis #VIS2019 \npaper: <LINK> <LINK>']",https://arxiv.org/abs/1907.08325,"With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous datasets that require techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, namely topology aware datacubes, we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications. ","Scalable Topological Data Analysis and Visualization for Evaluating
  Data-Driven Models in Scientific Applications"
111,1151698877449297920,837412258150035457,Dr. Burcin Mutlu-Pakdil,"['Our new paper on ultra-faint dwarf galaxies (UFD) is out on arXiv today: <LINK> \n\nThis is the first comprehensive case study of signatures of tidal distribution in UFDs. We focused on LeoV and found very cool results!!  Check them out!!', 'Our team includes \n@sand_dave @caprastro @JeffCarlinastro @BethWillman @denija83 @michelle_lmc @anilcseth M. Walker, N. Caldwell, M.Mateo, E. Olszewski and D. Zaritsky of @azstewobs. @UAresearch', 'Leo V has shown both photometric overdensities and kinematic members at large radii, along with a tentative kinematic gradient, suggesting that it may have undergone a close encounter with the Milky Way. We critically assessed these signs in combination with new observations üí™üèª', 'Our comprehensive analysis includes i) HST photometry @NASAHubble, ii) two epochs of spectra from Hectochelle on @mmtobservatory, and iii) @ESAGaia measurements.', 'Using the #HST data, we examine one of the reported stream-like overdensities at large radii, and conclude that it is not a true stellar stream, but instead a clump of foreground stars and background galaxies.', 'Our spectroscopic analysis shows that one known member star is likely a binary, and challenges the membership status of three others, including two distant candidates that had formerly provided evidence for overall stellar mass loss.', 'We also find evidence that the proposed kinematic gradient across Leo V might be due to small number statistics.', 'Using #Gaia DR2, we update the systemic proper motion of Leo V, which is consistent with its reported orbit that did not put Leo V at risk of being disturbed by the Milky Way.', 'Our findings remove most of the observational clues that suggested Leo V was disrupting, but we also find new plausible member stars, two of which are located &gt;5 half-light radii from the main body. ü§∑üèª\u200d‚ôÄÔ∏è These stars require further investigation.', 'The nature of Leo V still remains an open question. The true nature of UFDs of #MilkyWay is indeed hard to understand!', '@Claire_Simeone Thank you Claire!! ü§©', '@listen2spacepod Yes! We should do it. Dwarf galaxies are so cool ü§©']",https://arxiv.org/abs/1907.07233,"The ultra-faint dwarf galaxy Leo V has shown both photometric overdensities and kinematic members at large radii, along with a tentative kinematic gradient, suggesting that it may have undergone a close encounter with the Milky Way. We investigate these signs of disruption through a combination of i) high-precision photometry obtained with the Hubble Space Telescope (HST), ii) two epochs of stellar spectra obtained with the Hectochelle Spectrograph on the MMT, and iii) measurements from the Gaia mission. Using the HST data, we examine one of the reported stream-like overdensities at large radii, and conclude that it is not a true stellar stream, but instead a clump of foreground stars and background galaxies. Our spectroscopic analysis shows that one known member star is likely a binary, and challenges the membership status of three others, including two distant candidates that had formerly provided evidence for overall stellar mass loss. We also find evidence that the proposed kinematic gradient across Leo V might be due to small number statistics. We update the systemic proper motion of Leo V, finding $(\mu_\alpha \cos\delta, \mu_\delta)= (0.009\pm0.560$, $-0.777\pm0.314)$ mas yr$^{-1}$, which is consistent with its reported orbit that did not put Leo V at risk of being disturbed by the Milky Way. These findings remove most of the observational clues that suggested Leo V was disrupting, however, we also find new plausible member stars, two of which are located >5 half-light radii from the main body. These stars require further investigation. Therefore, the nature of Leo V still remains an open question. ","Signatures of Tidal Disruption in Ultra-Faint Dwarf Galaxies: A Combined
  HST, Gaia, and MMT/Hectochelle Study of Leo V"
112,1150933578248704000,110836003,Y. Kawamoto,"['Submitted a paper ""Local Distribution Obfuscation via Probability Coupling"", dealing with distribution privacy with f-divergence (separated from an old version of ESORICS\'19 paper) and probability coupling (new part). <LINK>']",https://arxiv.org/abs/1907.05991,"We introduce a general model for the local obfuscation of probability distributions by probabilistic perturbation, e.g., by adding differentially private noise, and investigate its theoretical properties. Specifically, we relax a notion of distribution privacy (DistP) by generalizing it to divergence, and propose local obfuscation mechanisms that provide divergence distribution privacy. To provide f-divergence distribution privacy, we prove that probabilistic perturbation noise should be added proportionally to the Earth mover's distance between the probability distributions that we want to make indistinguishable. Furthermore, we introduce a local obfuscation mechanism, which we call a coupling mechanism, that provides divergence distribution privacy while optimizing the utility of obfuscated data by using exact/approximate auxiliary information on the input distributions we want to protect. ",Local Distribution Obfuscation via Probability Coupling
113,1150801626178830338,3557386648,Gabriel Ilharco,"['New paper on evaluation metrics for instruction conditioned navigation (e.g. VLN) on arXiv! #NLProc #Robotics #VLN \n\nPaper üëâ <LINK>\n\nThread üëá <LINK>', 'Evaluating the performance is challenging. For instance, the increasingly popular Vision and Language Navigation (VLN) task has seen a succession of progressively better evaluation measures since it was introduced (e.g. SR, NE, SPL, SED, CLS).', 'Natural language instructions in this task are generally sufficiently specific for agents to perfectly follow the intended trajectory. Yet, most measures fail to properly compare an observed path with a reference trajectory.', 'For instance, a common property is invariance to intermediary nodes in the reference trajectory, masking out unwanted and potentially dangerous behaviors, as explored in depth by https://t.co/YWFpECse9Y', 'Further, even the more recent measure CLS has its flaws, by not dealing well with the order\xa0in which nodes compose the paths.', 'In our recent paper, we introduce a similarity measure (nDTW) between two trajectories by adapting Dynamic Time Warping to the context of navigation. https://t.co/xEnBuHoWae', 'nDTW softly penalizes deviations from the path, is sensitive to the order of the elements in the paths but not to scale and density of nodes, is well suited for both continuous and graph-based environments, and can be efficiently computed.', 'Along with those and other desired properties, we demonstrate the pragmatic value of the measure, by showing that nDTW can also be used as a reward signal for RL agents, improving their performance on both Room-to-Room (R2R) and Room-for-Room (R4R) datasets.', 'Further, we ask humans to rank pairs of paths by asking them ""If you instructed a robot to take the blue path, which of these orange paths would you prefer it to take?"". Human judgments correlate better with our measure than with all previous ones. https://t.co/YgaZKd9aRW', 'Similarly to SPL and SED, we introduce SDTW by constraining nDTW to only successful episodes, making it ideal for evaluating tasks where success is central for performance.', 'Given its theoretical and pragmatic advantages over previous metrics, we hope authors will adopt SDTW for instruction conditioned navigation tasks in future work.']",https://arxiv.org/abs/1907.05446,"In instruction conditioned navigation, agents interpret natural language and their surroundings to navigate through an environment. Datasets for studying this task typically contain pairs of these instructions and reference trajectories. Yet, most evaluation metrics used thus far fail to properly account for the latter, relying instead on insufficient similarity comparisons. We address fundamental flaws in previously used metrics and show how Dynamic Time Warping (DTW), a long known method of measuring similarity between two time series, can be used for evaluation of navigation agents. For such, we define the normalized Dynamic Time Warping (nDTW) metric, that softly penalizes deviations from the reference path, is naturally sensitive to the order of the nodes composing each path, is suited for both continuous and graph-based evaluations, and can be efficiently calculated. Further, we define SDTW, which constrains nDTW to only successful paths. We collect human similarity judgments for simulated paths and find nDTW correlates better with human rankings than all other metrics. We also demonstrate that using nDTW as a reward signal for Reinforcement Learning navigation agents improves their performance on both the Room-to-Room (R2R) and Room-for-Room (R4R) datasets. The R4R results in particular highlight the superiority of SDTW over previous success-constrained metrics. ","General Evaluation for Instruction Conditioned Navigation using Dynamic
  Time Warping"
114,1149117159844409344,537861776,Kevin Clark,"['BAM! Our new #ACL2019 paper presents ""Born-Again Multi-Task Networks,"" a simple way to improve multi-task learning using knowledge distillation. With @lmthang @ukhndlwl @quocleix @chrmanning. Paper: <LINK> Code: <LINK>']",https://arxiv.org/abs/1907.04829,"It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training. ",BAM! Born-Again Multi-Task Networks for Natural Language Understanding
115,1148875221614694400,926551285788233728,Vincent Fortuin,"['Our new paper on using VAEs to deal with missing data in time series is online! This is joint work with @gxr and @s_mandt. Find the link here: <LINK>\nDetails in the thread.', 'To model the temporal dynamics of the time series, we use a Gaussian process prior in the latent space of the VAE. The latent GP uses a Cauchy kernel, to model dynamics across different time scales. We then perform amortized variational inference on the latent GP posterior. https://t.co/XpU8MFPdsB', 'To make the inference efficient, we parameterize the precision matrix of the variational posterior as a tridiagonal matrix (product of two bidiagonal matrices). This allows sampling and inference in linear time, while still modeling potential longe-range interactions in time. https://t.co/KMKb7cNjj2', 'To deal with the missingness in the data, we optimize the ELBO only over observed features. Since we assume that all features will be observed at least at some time points, all parts of the model will eventually get trained. https://t.co/qMfT5Tr365', 'We assess our model on different benchmark and real-world data sets and find that it yields a superior imputation performance to baseline methods. https://t.co/9NWbQo4N7q', ""@flwenz Not yet, but we're working on it!""]",https://arxiv.org/abs/1907.04155,"Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates. ",GP-VAE: Deep Probabilistic Time Series Imputation
116,1148791856559525893,1012125662117851136,Edward Kennedy,"['really excited about this new paper w/great student kwangho kim &amp; @ashley_naimi:\n\n<LINK>\n\nwe extend incremental effects (<LINK>) to studies w/multiple outcomes+dropout &amp; do a super interesting analysis of infinite-horizon setup w/many timepoints', '@ashley_naimi Even with ~100 timepoints you can summarize interesting causal effects with simple curves like: https://t.co/DT431CKPFh']",https://arxiv.org/abs/1907.04004,"Modern longitudinal studies collect feature data at many timepoints, often of the same order of sample size. Such studies are typically affected by {dropout} and positivity violations. We tackle these problems by generalizing effects of recent incremental interventions (which shift propensity scores rather than set treatment values deterministically) to accommodate multiple outcomes and subject dropout. We give an identifying expression for incremental intervention effects when dropout is conditionally ignorable (without requiring treatment positivity), and derive the nonparametric efficiency bound for estimating such effects. Then we present efficient nonparametric estimators, showing that they converge at fast parametric rates and yield uniform inferential guarantees, even when nuisance functions are estimated flexibly at slower rates. We also study the variance ratio of incremental intervention effects relative to more conventional deterministic effects in a novel infinite time horizon setting, where the number of timepoints can grow with sample size, and show that incremental intervention effects yield near-exponential gains in statistical precision in this setup. Finally we conclude with simulations and apply our methods in a study of the effect of low-dose aspirin on pregnancy outcomes. ","Incremental Intervention Effects in Studies with Dropout and Many
  Timepoints"
117,1148778925734150150,734677275216470016,Guodong Zhang,"['New paper on studying how the critical batch size changes based on properties of the optimization algorithm (including momentum and preconditioning), through two different lenses: large scale experiments, and analysis of a simple noisy quadratic model.\n\n<LINK>', 'In the paper, we show that a simple noisy quadratic model (NQM) is remarkably consistent with the batch size effects observed in real neural networks, while allowing us to run experiments in seconds.', 'The NQM successfully predicts that momentum should speed up training relative to plain SGD at larger batch sizes, but do nothing at small batch sizes. https://t.co/U9HEYKyHAK', 'Through large scale experiments, we confirm that, as predicted by the NQM, preconditioning extends perfect batch size scaling to larger batch sizes than are possible with momentum SGD. Furthermore, unlike momentum, preconditioning can help at small batch sizes as well.', 'Joint work with Lala Li, @zacharynado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue and @RogerGrosse.']",https://arxiv.org/abs/1907.04164,"Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments, and analysis of a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization. ","Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a
  Noisy Quadratic Model"
118,1147102629920530441,30142130,Meredith Ringel Morris,['New position paper written for the #assets2019 workshop on #AI fairness and #disability (w/ @AnhongGuo @ecekamar @hannawallach @jennwvaughan) #FATML #a11y - <LINK>'],https://arxiv.org/abs/1907.02227,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms. ",Toward Fairness in AI for People with Disabilities: A Research Roadmap
119,1146504589782507520,993896114565791744,"Doug Rennehan, PhD","['I have a new paper out on arXiv! We predict massive blue elliptical BCGs at z &gt; 3, and provide an explanation for the observation of highly star forming BCGs from SpARCs. Check it out: <LINK> #astronomy #astrophysics @AstroCanada', ""@AstroCanada Starting from obs. in Miller+'18 of high-redshift overdense protoclusters we produce mock observations and show JWST can clearly see the proto-BCGs (left JWST mock obs., right simulation without CCD scale) https://t.co/L8fPQvqMOI"", '@AstroCanada We find a downsizing trend on cluster scale -- higher z=0 mass clusters have a higher prob. of assembling their BCGs early. Graph shows the first assembly redshift probability for a given cluster z=0 mass (from MDPL2, https://t.co/dr01jMwr8b) https://t.co/h5zYBRYUYM', '@AstroCanada Assembly redshift continually increases with increasing final cluster mass. BCGs should assemble throughout cosmic time, and those at higher redshift should undergo rapid star formation.']",https://arxiv.org/abs/1907.00977,"The current consensus on the formation and evolution of the brightest cluster galaxies is that their stellar mass forms early ($z \gtrsim 4$) in separate galaxies that then eventually assemble the main structure at late times ($z \lesssim 1$). However, advances in observational techniques have led to the discovery of protoclusters out to $z \sim 7$, suggesting that the late-assembly picture may not be fully complete. If these protoclusters assemble rapidly in the early universe, they should form the brightest cluster galaxies much earlier than suspected by the late-assembly picture. Using a combination of observationally constrained hydrodynamical and dark-matter-only simulations, we show that the stellar assembly time of a sub-set of the brightest cluster galaxies occurs at high redshifts ($z > 3$) rather than at low redshifts ($z < 1$), as is commonly thought. We find, using isolated non-cosmological hydrodynamical simulations, that highly overdense protoclusters assemble their stellar mass into brightest cluster galaxies within $\sim 1$ $\mathrm{Gyr}$ of evolution -- producing massive blue elliptical galaxies at high redshifts ($z \gtrsim 1.5$). We argue that there is a downsizing effect on the cluster scale wherein some of the brightest cluster galaxies in the cores of the most-massive clusters assemble earlier than those in lower-mass clusters. In those clusters with $z = 0$ virial mass $\geqslant 5\times10^{14}$ M$_\mathrm{\odot}$, we find that $9.8$% have their cores assembly early, and a higher fraction of $16.4$% in those clusters above $10^{15}$ M$_\mathrm{\odot}$. The James Webb Space Telescope will be able to detect and confirm our prediction in the near future, and we discuss the implications to constraining the value of $\sigma_\mathrm{8}$. ","Rapid early coeval star formation and assembly of the most massive
  galaxies in the universe"
120,1157567138216984576,808750328241799168,Rachel Bawden,"['Two days ago, we presented the University of Edinburgh‚Äôs submissions to the #WMT2019 shared task on news translation. You can find all details in the latest version of the paper available at <LINK> #acl2019nlp <LINK>']",https://arxiv.org/abs/1907.05854,"The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English-to-Gujarati, Gujarati-to-English, English-to-Chinese, Chinese-to-English, German-to-English, and English-to-Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English-Gujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German-to-English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English-to-Czech, we compared different pre-processing and tokenisation regimes. ","The University of Edinburgh's Submissions to the WMT19 News Translation
  Task"
121,1156802754662338560,901142962758758400,Hang-Hyun Jo,"['Our paper ""Burst-tree decomposition of time series reveals the structure of temporal correlations"" (with @takayukihir &amp; @bolozna) is available at <LINK> | We propose a new method of analyzing the bursty event sequences that turns event sequences into trees. <LINK>']",https://arxiv.org/abs/1907.13556,"Comprehensive characterization of non-Poissonian, bursty temporal patterns observed in various natural and social processes is crucial to understand the underlying mechanisms behind such temporal patterns. Among them bursty event sequences have been studied mostly in terms of interevent times (IETs), while the higher-order correlation structure between IETs has gained very little attention due to the lack of a proper characterization method. In this paper we propose a method of decomposing an event sequence into a set of IETs and a burst tree, which exactly captures the structure of temporal correlations that is entirely missing in the analysis of IET distributions. We apply the burst-tree decomposition method to various datasets and analyze the structure of the revealed burst trees. In particular, we observe that event sequences show similar burst-tree structure, such as heavy-tailed burst size distributions, despite of very different IET distributions. The burst trees allow us to directly characterize the preferential and assortative mixing structure of bursts responsible for the higher-order temporal correlations. We also show how to use the decomposition method for the systematic investigation of such higher-order correlations captured by the burst trees in the framework of randomized reference models. Finally, we devise a simple kernel-based model for generating event sequences showing appropriate higher-order temporal correlations. Our method is a tool to make the otherwise overwhelming analysis of higher-order correlations in bursty time series tractable by turning it into the analysis of a tree structure. ","Burst-tree decomposition of time series reveals the structure of
  temporal correlations"
122,1156652385709842433,238073516,Erik Wittern,"['üéÜ Our paper ""An Empirical Study of GraphQL Schemas"" was just accepted @Icsoc2. We collected schemas from commercial APIs and GitHub and analyzed feature use, naming conventions, and worst-case query complexities. Read the preprint at <LINK> üéÜ']",https://arxiv.org/abs/1907.13012,"GraphQL is a query language for APIs and a runtime to execute queries. Using GraphQL queries, clients define precisely what data they wish to retrieve or mutate on a server, leading to fewer round trips and reduced response sizes. Although interest in GraphQL is on the rise, with increasing adoption at major organizations, little is known about what GraphQL interfaces look like in practice. This lack of knowledge makes it hard for providers to understand what practices promote idiomatic, easy-to-use APIs, and what pitfalls to avoid. To address this gap, we study the design of GraphQL interfaces in practice by analyzing their schemas - the descriptions of their exposed data types and the possible operations on the underlying data. We base our study on two novel corpuses of GraphQL schemas, one of 16 commercial GraphQL schemas and the other of 8,399 GraphQL schemas mined from GitHub projects. We make both corpuses available to other researchers. Using these corpuses, we characterize the size of schemas and their use of GraphQL features and assess the use of both prescribed and organic naming conventions. We also report that a majority of APIs are susceptible to denial of service through complex queries, posing real security risks previously discussed only in theory. We also assess ways in which GraphQL APIs attempt to address these concerns. ",An Empirical Study of GraphQL Schemas
123,1156474827919769600,131264897,Iacopo Iacopini,"['New preprint where we study a multilayer model of adoption dynamics with ""complex"" recovery | with B. Sch√§fer, @ElsaArcaute, C. Beck &amp; V. Latora | <LINK> | @QMULMaths @CASAUCL @turinginst <LINK>']",http://arxiv.org/abs/1907.13096,"Due to the emergence of new technologies, the whole electricity system is undergoing transformations on a scale and pace never observed before. The decentralization of energy resources and the smart grid have forced utility services to rethink their relationships with customers. Demand response (DR) seeks to adjust the demand for power instead of adjusting the supply. However, DR business models rely on customer participation and can only be effective when large numbers of customers in close geographic vicinity, e.g., connected to the same transformer, opt in. Here, we introduce a model for the dynamics of service adoption on a two-layer multiplex network: the layer of social interactions among customers and the power-grid layer connecting the households. While the adoption process - based on peer-to-peer communication - runs on the social layer, the time-dependent recovery rate of the nodes depends on the states of their neighbors on the power-grid layer, making an infected node surrounded by infectious ones less keen to recover. Numerical simulations of the model on synthetic and real-world networks show that a strong local influence of the customers' actions leads to a discontinuous transition where either none or all the nodes in the network are infected, depending on the infection rate and social pressure to adopt. We find that clusters of early adopters act as points of high local pressure, helping maintaining adopters, and facilitating the eventual adoption of all nodes. This suggests direct marketing strategies on how to efficiently establish and maintain new technologies such as DR schemes. ",Multilayer modeling of adoption dynamics in energy demand management
124,1156466481833676800,22392129,Jasmijn Bastings,"['We released Joey NMT üê® a minimalist NMT toolkit for novices. Small &amp; easy to understand code in @PyTorch with top quality translations. Paper includes a user study! <LINK> #NLProc @StatNLP_HD @AmsterdamNLP #nmt @KreutzerJulia #pytorch <LINK>', 'GitHub: https://t.co/JaBanwwbRQ']",https://arxiv.org/abs/1907.12484,"We present Joey NMT, a minimalist neural machine translation toolkit based on PyTorch that is specifically designed for novices. Joey NMT provides many popular NMT features in a small and simple code base, so that novices can easily and quickly learn to use it and adapt it to their needs. Despite its focus on simplicity, Joey NMT supports classic architectures (RNNs, transformers), fast beam search, weight tying, and more, and achieves performance comparable to more complex toolkits on standard benchmarks. We evaluate the accessibility of our toolkit in a user study where novices with general knowledge about Pytorch and NMT and experts work through a self-contained Joey NMT tutorial, showing that novices perform almost as well as experts in a subsequent code quiz. Joey NMT is available at this https URL . ",Joey NMT: A Minimalist NMT Toolkit for Novices
125,1154365639710126081,1020951700067241984,Daoudi Mohamed,['A preprint of our new paper: Dynamic Facial Expression Generation on Hilbert Hypersphere with Conditional Wasserstein Generative Adversarial Nets #GAN\n<LINK>\nWe propose to generate the six  facial expressions given a single neutral face image. <LINK>'],https://arxiv.org/abs/1907.10087,"In this work, we propose a novel approach for generating videos of the six basic facial expressions given a neutral face image. We propose to exploit the face geometry by modeling the facial landmarks motion as curves encoded as points on a hypersphere. By proposing a conditional version of manifold-valued Wasserstein generative adversarial network (GAN) for motion generation on the hypersphere, we learn the distribution of facial expression dynamics of different classes, from which we synthesize new facial expression motions. The resulting motions can be transformed to sequences of landmarks and then to images sequences by editing the texture information using another conditional Generative Adversarial Network. To the best of our knowledge, this is the first work that explores manifold-valued representations with GAN to address the problem of dynamic facial expression generation. We evaluate our proposed approach both quantitatively and qualitatively on two public datasets; Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the effectiveness of our approach in generating realistic videos with continuous motion, realistic appearance and identity preservation. We also show the efficiency of our framework for dynamic facial expressions generation, dynamic facial expression transfer and data augmentation for training improved emotion recognition models. ","Dynamic Facial Expression Generation on Hilbert Hypersphere with
  Conditional Wasserstein Generative Adversarial Nets"
126,1154295187121876997,841703218979733505,Andrei Igoshev,['In our recent research <LINK> using @ESAGaia we find that the timescale of common envelope differs for hot subdwarf stars and CVs. Mass ejection happens very fast (~100 years) for sdBs and it takes much longer for cataclysmic variables (~10000 years).'],https://arxiv.org/abs/1907.10068,"Evolution of close binaries often proceeds through the common envelope stage. The physics of the envelope ejection (CEE) is not yet understood, and several mechanisms were suggested to be involved. These could give rise to different timescales for the CEE mass-loss. In order to probe the CEE-timescales we study wide companions to post-CE binaries. Faster mass-loss timescales give rise to higher disruption rates of wide binaries and result in larger average separations. We make use of data from Gaia DR2 to search for ultra-wide companions (projected separations $10^3$-$2\times 10^5$ a.u. and $M_2 > 0.4$ M$_\odot$) to several types of post-CEE systems, including sdBs, white-dwarf post-common binaries, and cataclysmic variables. We find a (wide-orbit) multiplicity fraction of $1.4\pm 0.2$ per cent for sdBs to be compared with a multiplicity fraction of $5.0\pm 0.2$ per cent for late-B/A/F stars which are possible sdB progenitors. The distribution of projected separations of ultra-wide pairs to main sequence stars and sdBs differs significantly and is compatible with prompt mass loss (upper limit on common envelope ejection timescale of $10^2$ years). The smaller statistics of ultra-wide companions to cataclysmic variables and post-CEE binaries provide weaker constraints. Nevertheless, the survival rate of ultra-wide pairs to the cataclysmic variables suggest much longer, $\sim10^4$ years timescales for the CEE in these systems, possibly suggesting non-dynamical CEE in this regime. ","Inferred timescales for common envelope ejection using wide astrometric
  companions"
127,1153348461913346049,364909780,David Orden,['A paper where we use Delaunay triangulations to study wave groups: Estimation of spatio-temporal wave grouping properties using Delaunay triangulation and spline techniques <LINK> Open access version at <LINK>'],https://arxiv.org/abs/1907.02714,"Wave groups can be detected and studied by using the wave envelope. So far, the method used to compute the wave envelope employs the Riesz transform. However, such a technique always produces symmetric envelopes, which is only realistic in the case of linear waves. In this paper we present a new method to compute the wave envelope providing more realistic results. In particular, the method allows to detect non-symmetry in the wave envelope, something useful, for instance, when detecting groups of high waves. The method computes first the local maxima and minima of the sea surface, and then determines the wave envelope by combining discrete methods, namely the use of the Delaunay triangulation, and tensor-product splines. The proposed method has been applied to simulated wave fields, and also to wave elevations data measured by an X-band radar. The obtained results correctly reproduce the behavior of the simulated waves. ","Estimation of spatio-temporal wave grouping properties using Delaunay
  triangulation and spline techniques"
128,1152264272191377414,2302304521,Dr Andra Stroe üè≥Ô∏è‚Äçüåàüá∑üá¥,"['Our paper on APEX observations of galaxies in the Antlia cluster is accepted to ApJ and on the Arxiv today! We find that star forming galaxies in this disturbed galaxy cluster have large molecular gas reservoirs! Great work by my student @cairns_jd ! <LINK>', 'Fingers crossed for our @almaobs ACA proposal! If accepted, we will be able to study these interesting galaxies at high spatial resolution!', 'Thanks to @ESO and @robivison for approving the summer student funding which made all of this possible!']",https://arxiv.org/abs/1907.07691v1,"We present CO(2-1) observations of 72 galaxies in the nearby, disturbed Antlia galaxy cluster with the Atacama Pathfinder Experiment (APEX) telescope. The galaxies in our sample are selected to span a wide range of stellar masses ($10^{8}M_{\odot}\lesssim M_{\star} \lesssim 10^{10}M_{\odot}$) and star formation rates ($0.0005M_{\odot}\text{yr}^{-1}<\text{SFR}<0.3M_{\odot}\text{yr}^{-1}$). Reaching a depth of $23\text{mJy}$ in $50\text{km}\text{s}^{-1}$ channels, we report a total CO detection rate of $37.5\%$ and a CO detection rate of $86\%$ for sources within 1 dex of the main sequence. We compare our sample with a similar sample of galaxies in the field, finding that, for a fixed stellar mass and SFR, galaxies in the Antlia cluster have comparable molecular gas reservoirs to field galaxies. We find that $\sim41\%$ (11/27) of our CO detections display non-Gaussian CO(2-1) emission line profiles, and a number of these sources display evidence of quenching in their optical images. We also find that the majority of our sample lie either just below, or far below the main sequence of field galaxies, further hinting at potential ongoing quenching. We conclude that the Antlia cluster represents an intermediate environment between fields and dense clusters, where the gentler intracluster medium (ICM) allows the cluster members to retain their reservoirs of molecular gas, but in which the disturbed ICM is just beginning to influence the member galaxies, resulting in high SFRs and possible ongoing quenching. ",] Large Molecular Gas Reservoirs in Star Forming Cluster Galaxies
129,1152030965872513024,1019760963569049601,Almog Yalinewich,"['Our new paper is on the arxiv. We study the optical transient from an explosion close to the surface of a star. Such an explosion can occur due to super Eddington accretion of a compact companion in a common envelope, and can be a precursor to a supernova <LINK> <LINK>']",https://arxiv.org/abs/1907.07689,"We study the hydrodynamic evolution of an explosion close to the stellar surface, and give predictions for the radiation from such an event. We show that such an event will give rise to a multi-wavelength transient. We apply this model to describe a precursor burst to the peculiar supernova iPTF14hls, which occurred in 1954, sixty year before the supernova. We propose that the new generation of optical surveys might detect similar transients, and they can be used to identify supernova progenitors well before the explosion. ",Optical Transient from an Explosion Close to the Stellar Surface
130,1151514837438087168,600819785,Shruthi Chari,"['Our recently accepted  ISWC\'19 paper on ""Making Study Populations Visible through Knowledge Graphs"" is now on arXiv (<LINK>).  Give it a read to get an idea of work we are doing to standardize representations of populations to further evidence-based medicine.']",https://arxiv.org/abs/1907.04358,"Treatment recommendations within Clinical Practice Guidelines (CPGs) are largely based on findings from clinical trials and case studies, referred to here as research studies, that are often based on highly selective clinical populations, referred to here as study cohorts. When medical practitioners apply CPG recommendations, they need to understand how well their patient population matches the characteristics of those in the study cohort, and thus are confronted with the challenges of locating the study cohort information and making an analytic comparison. To address these challenges, we develop an ontology-enabled prototype system, which exposes the population descriptions in research studies in a declarative manner, with the ultimate goal of allowing medical practitioners to better understand the applicability and generalizability of treatment recommendations. We build a Study Cohort Ontology (SCO) to encode the vocabulary of study population descriptions, that are often reported in the first table in the published work, thus they are often referred to as Table 1. We leverage the well-used Semanticscience Integrated Ontology (SIO) for defining property associations between classes. Further, we model the key components of Table 1s, i.e., collections of study subjects, subject characteristics, and statistical measures in RDF knowledge graphs. We design scenarios for medical practitioners to perform population analysis, and generate cohort similarity visualizations to determine the applicability of a study population to the clinical population of interest. Our semantic approach to make study populations visible, by standardized representations of Table 1s, allows users to quickly derive clinically relevant inferences about study populations. ",Making Study Populations Visible through Knowledge Graphs
131,1151326890331971585,4750366201,Bryan Ostdiek,"['Very pleased to announce our new paper using #MachineLearning to tell what stars are accreted onto the Milky Way vs those that were born in situ in the \n@ESAGaia\n data. See what we find in the data tomorrow!  <LINK>', '@ESAGaia With @linoush95 @SheaGKosmo @AndrewWetzel @astrorobyn @PFHopkins_Astro']",https://arxiv.org/abs/1907.06652,"The goal of this study is to present the development of a machine learning based approach that utilizes phase space alone to separate the Gaia DR2 stars into two categories: those accreted onto the Milky Way from those that are in situ. Traditional selection methods that have been used to identify accreted stars typically rely on full 3D velocity, metallicity information, or both, which significantly reduces the number of classifiable stars. The approach advocated here is applicable to a much larger portion of Gaia DR2. A method known as ""transfer learning"" is shown to be effective through extensive testing on a set of mock Gaia catalogs that are based on the FIRE cosmological zoom-in hydrodynamic simulations of Milky Way-mass galaxies. The machine is first trained on simulated data using only 5D kinematics as inputs and is then further trained on a cross-matched Gaia/RAVE data set, which improves sensitivity to properties of the real Milky Way. The result is a catalog that identifies around 767,000 accreted stars within Gaia DR2. This catalog can yield empirical insights into the merger history of the Milky Way and could be used to infer properties of the dark matter distribution. ",Cataloging Accreted Stars within Gaia DR2 using Deep Learning
132,1151164490395856897,2352463609,Carlos Blanco,"[""In our new paper with Dan Hooper @DanHooperAstro , Miguel Escudero, and Sam Witte, we take a deep look at the explored and explorable parameter space of Z'-mediated WIMPs. Are they dead? find out!  \n\n<LINK>""]",https://arxiv.org/abs/1907.05893,"Although weakly interacting massive particles (WIMPs) have long been among the most studied and theoretically attractive classes of candidates for the dark matter of our universe, the lack of their detection in direct detection and collider experiments has begun to dampen enthusiasm for this paradigm. In this study, we set out to appraise the status of the WIMP paradigm, focusing on the case of dark matter candidates that interact with the Standard Model through a new gauge boson. After considering a wide range of $Z'$ mediated dark matter models, we quantitatively evaluate the fraction of the parameter space that has been excluded by existing experiments, and that is projected to fall within the reach of future direct detection experiments. Despite the existence of stringent constraints, we find that a sizable fraction of this parameter space remains viable. More specifically, if the dark matter is a Majorana fermion, we find that an order one fraction of the parameter space is in many cases untested by current experiments. Future direct detection experiments with sensitivity near the irreducible neutrino floor will be able to test a significant fraction of the currently viable parameter space, providing considerable motivation for the next generation of direct detection experiments. ","$Z'$ Mediated WIMPs: Dead, Dying, or Soon to be Detected?"
133,1149321787991306241,859104497976700928,Daniele Gravina,"['How to produce a large set of diverse and high-quality artifacts for games? We propose procedural content generation through quality-diversity (PCG-QD) as a subset of search-based procedural content generation\n\n<LINK> <LINK>', 'Quality-diversity is perfectly suited for generating content autonomously, as it can produce a large set of diverse and high-quality artifacts in one run, or with a human designer, as it can explain and express its artifacts‚Äô desirable properties.', 'This paper presents the components of quality-diversity algorithms, identifies the strengths of PCG-QD over popular alternatives, surveys recent work in this vein and attempts to map out the road ahead.', 'This paper, which is written by myself, @Amidos2006, @SentientDesigns, @togelius, and @yannakakis will be presented at @cog2019ieee #CoG2019 #PCG']",http://arxiv.org/abs/1907.04053,"Quality-diversity (QD) algorithms search for a set of good solutions which cover a space as defined by behavior metrics. This simultaneous focus on quality and diversity with explicit metrics sets QD algorithms apart from standard single- and multi-objective evolutionary algorithms, as well as from diversity preservation approaches such as niching. These properties open up new avenues for artificial intelligence in games, in particular for procedural content generation. Creating multiple systematically varying solutions allows new approaches to creative human-AI interaction as well as adaptivity. In the last few years, a handful of applications of QD to procedural content generation and game playing have been proposed; we discuss these and propose challenges for future work. ",Procedural Content Generation through Quality Diversity
134,1149116995893420032,450326907,Robert Serrano,['My first co-authored paper on kicked black holes in rotating star clusters.\nWe find a new phase in the orbital decay of an orbiting black hole due to angular momentum exchange from the cluster to the black hole. <LINK>'],https://arxiv.org/abs/1907.04330,"In this paper, we continue our study on the evolution of black holes (BHs) that receive velocity kicks at the origin of their host star cluster potential. We now focus on BHs in rotating clusters that receive a range of kick velocities in different directions with respect to the rotation axis. We perform N-body simulations to calculate the trajectories of the kicked BHs and develop an analytic framework to study their motion as a function of the host cluster and the kick itself. Our simulations indicate that for a BH that is kicked outside of the cluster's core, as its orbit decays in a rotating cluster the BH will quickly gain angular momentum as it interacts with stars with high rotational frequencies. Once the BH decays to the point where its orbital frequency equals that of local stars, its orbit will be circular and dynamical friction becomes ineffective since local stars will have low relative velocities. After circularization, the BH's orbit decays on a longer timescale than if the host cluster was not rotating. Hence BHs in rotating clusters will have longer orbital decay times. The timescale for orbit circularization depends strongly on the cluster's rotation rate and the initial kick velocity, with kicked BHs in slowly rotating clusters being able to decay into the core before circularization occurs. The implication of the circularization phase is that the probability of a BH undergoing a tidal capture event increases, possibly aiding in the formation of binaries and high-mass BHs. ","The evolution of kicked stellar-mass black holes in star cluster
  environments II. Rotating star clusters"
135,1149037159875170304,1046598080,Brennan Klein,"['Almost 2 years ago to the day, @erikphoel and I met in NY to chat about his terrific paper on causal emergence <LINK>\n\nSince then, we‚Äôve been building a formalism to study causal emergence in networks. Today we posted our first paper on it <LINK> <LINK>', ""Networks are such powerful objects. They've changed how we study complex systems. But I‚Äôve always been struck by how nontrivial the ‚Äúwhat is a node?‚Äù question can be.\n\nWe provide a framework for identifying the most informative *scale* to describe interdependencies in a system... https://t.co/FgxIUkQu36"", '...which is to say, we find that compressed or coarse-grained or macroscale descriptions of networks often have more *effective information* than the original microscale network (e.g. your raw network data).\n\nThis noise-minimizing process is known as causal emergence.\n\nSo what? https://t.co/WQ39KyHs5B', 'It\'s a question of zoom: what\'s the right scale to represent brain networks, given what we want from our model? What\'s the best scale to model economic systems? What counts as a ""node"" in a genome?\n\nThey\'re rich, tough, fun problems. And there\'s a long way to go. https://t.co/ZKnNUvmxF9', ""This research fascinates me, and there's a bunch of directions to go with it.\n\nFeel free to send feedback or questions, and stay tuned for the release of some tutorials / open python code.\n\nhttps://t.co/Til7g7LCEk""]",https://arxiv.org/abs/1907.03902,"The connectivity of a network contains information about the relationships between nodes, which can denote interactions, associations, or dependencies. We show that this information can be analyzed by measuring the uncertainty (and certainty) contained in paths along nodes and links in a network. Specifically, we derive from first principles a measure known as effective information and describe its behavior in common network models. Networks with higher effective information contain more information in the relationships between nodes. We show how subgraphs of nodes can be grouped into macro-nodes, reducing the size of a network while increasing its effective information (a phenomenon known as causal emergence). We find that informative higher scales are common in simulated and real networks across biological, social, informational, and technological domains. These results show that the emergence of higher scales in networks can be directly assessed and that these higher scales offer a way to create certainty out of uncertainty. ",The emergence of informative higher scales in complex networks
136,1148588150630973441,1578756350,Laura V. Sales,['Grad student Ethan Jahn first paper is out! With some FIRE folks we study the predicted satellites of the LMC within LCDM. Several more LMC-associated ultrafaints should be there says FIRE! @MBKplus @kjb_astro @jbprime @AndrewWetzel  <LINK> <LINK>'],https://arxiv.org/abs/1907.02979,"Within $\Lambda$CDM, dwarf galaxies like the Large Magellanic Cloud (LMC) are expected to host numerous dark matter subhalos, several of which should host faint dwarf companions. Recent Gaia proper motions confirm new members of the LMC-system in addition to the previously known SMC, including two classical dwarf galaxies ($M_\ast$ > $10^5$ M$_{\odot}$; Carina and Fornax) as well as several ultra-faint dwarfs (Car2, Car3, Hor1, and Hyd1). We use the Feedback In Realistic Environments (FIRE) simulations to study the dark and luminous (down to ultrafaint masses, $M_\ast$ ~ $6\times10^3$ M$_{\odot}$) substructure population of isolated LMC-mass hosts ($M_\text{200}$ = $1-3\times10^{11}$ M$_{\odot}$) and place the Gaia + DES results in a cosmological context. By comparing number counts of subhalos in simulations with and without baryons, we find that, within 0.2 $r_\text{200}$, LMC-mass hosts deplete ~30% of their substructure, significantly lower than the ~70% of substructure depleted by Milky Way (MW) mass hosts. For our highest resolution runs ($m_\text{bary}$ = 880 M$_{\odot}$), ~5-10 subhalos form galaxies with $M_\ast$ > $10^4$ M$_{\odot}$, in agreement with the 7 observationally inferred pre-infall LMC companions. However, we find steeper simulated luminosity functions than observed, hinting at observation incompleteness at the faint end. The predicted DM content for classical satellites in FIRE agrees with observed estimates for Carina and Fornax, supporting the case for an LMC association. We predict that tidal stripping within the LMC potential lowers the inner dark matter density of ultra faint companions of the LMC. Thus, in addition to their orbital consistency, the low densities of dwarfs Car2, Hyd1, and Hyd2 reinforce their likelihood of Magellanic association. ","Dark and luminous satellites of LMC-mass galaxies in the FIRE
  simulations"
137,1148488256876679168,1059276096150003712,Jan Drugowitsch,"['We are out with a new, slightly more technical than usual, preprint on how to find closed-form solutions to Fokker-Planck equations of two-dimensional correlated race-to-boundary models <LINK>. Congratulations to Haozhe Shan and @MorenoBote! [1/4]', 'These kind of models can be used to model decision-making, or neural activity, with two correlated sources of evidence/inputs. In the limit of perfectly anti-correlated sources, these models become equivalent to drift diffusion models. [2/4] https://t.co/f545W5bucw', 'We provide closed-form expressions for the probability density function describing the races for a large set of correlation coefficients, and show that those are all solutions that can be found with the used Method of Images. [3/4]', 'These results can in turn be used to find expressions for other properties, like bound hitting probabilities and first-passage-time distributions. For more details, check out the preprint! [4/4]']",http://arxiv.org/abs/1907.03341,"Diffusion processes with boundaries are models of transport phenomena with wide applicability across many fields. These processes are described by their probability density functions (PDFs), which often obey Fokker-Planck equations (FPEs). While obtaining analytical solutions is often possible in the absence of boundaries, obtaining closed-form solutions to the FPE is more challenging once absorbing boundaries are present. As a result, analyses of these processes have largely relied on approximations or direct simulations. In this paper, we studied two-dimensional, time-homogeneous, spatially-correlated diffusion with linear, axis-aligned, absorbing boundaries. Our main result is the explicit construction of a full family of closed-form solutions for their PDFs using the method of images (MoI). We found that such solutions can be built if and only if the correlation coefficient $\rho$ between the two diffusing processes takes one of a numerable set of values. Using a geometric argument, we derived the complete set of $\rho$'s where such solutions can be found. Solvable $\rho$'s are given by $\rho = - \cos \left( \frac{\pi}{k} \right)$, where $k \in \mathbb{Z}^+ \cup \{ +\infty\}$. Solutions were validated in simulations. Qualitative behaviors of the process appear to vary smoothly over $\rho$, allowing extrapolation from our solutions to cases with unsolvable $\rho$'s. ","Family of closed-form solutions for two-dimensional correlated diffusion
  processes"
138,1148404319076544512,836417100864344064,Takuma Udagawa,"['Our #AAAI2019 paper ‚ÄúA Natural Language Corpus of Common Grounding under Continuous and Partially-Observable Context‚Äù is up on arXiv: <LINK>! We proposed a minimal dialogue task to study advanced common grounding under continuous and partially-observable context.', 'Dataset is available here: https://t.co/LIEcs46rBA']",https://arxiv.org/abs/1907.03399,"Common grounding is the process of creating, repairing and updating mutual understandings, which is a critical aspect of sophisticated human communication. However, traditional dialogue systems have limited capability of establishing common ground, and we also lack task formulations which introduce natural difficulty in terms of common grounding while enabling easy evaluation and analysis of complex models. In this paper, we propose a minimal dialogue task which requires advanced skills of common grounding under continuous and partially-observable context. Based on this task formulation, we collected a largescale dataset of 6,760 dialogues which fulfills essential requirements of natural language corpora. Our analysis of the dataset revealed important phenomena related to common grounding that need to be considered. Finally, we evaluate and analyze baseline neural models on a simple subtask that requires recognition of the created common ground. We show that simple baseline models perform decently but leave room for further improvement. Overall, we show that our proposed task will be a fundamental testbed where we can train, evaluate, and analyze dialogue system's ability for sophisticated common grounding. ","A Natural Language Corpus of Common Grounding under Continuous and
  Partially-Observable Context"
139,1148110079562006528,850306776352395265,Mario Reig,"['""4321... axion!""\nWith J. Fuentes-Martin and A. Vicente @AvelinoQuantum \n\nWe study the implications for the strong CP problem and axion physics of two theories explaining the B-anomalies. We show that, if confirmed, they can change the BSM paradigm. \n<LINK> <LINK>']",https://arxiv.org/abs/1907.02550,"We analyze the strong CP problem and the implications for axion physics in the context of $U_1$ vector leptoquark models, recently put forward as an elegant solution to the hints of lepton flavor universality violation in B-meson decays. It is shown that in minimal gauge models containing the $U_1$ as a gauge boson, the Peccei-Quinn solution of the strong CP problem requires the introduction of two axions. Characteristic predictions for the associated axions can be deduced from the model parameter space hinted by B-physics, allowing the new axion sector to account for the dark matter of the Universe. We also provide a specific ultraviolet completion of the axion sector that connects the Peccei-Quinn mechanism to the generation of neutrino masses. ",Strong CP problem with low-energy emergent QCD: The 4321 case
140,1146960044266676224,118592197,Tsuyoshi Okubo,"['Our new preprint: ""Abelian and Non-Abelian Chiral Spin Liquids in a Compact Tensor Network Representation""  Hyun-Yong Lee, Ryui Kaneko, Tsuyoshi Okubo, and Naoki Kawashima, We propose a good and compact TNS for chiral spin liquid on the star lattice. <LINK>', 'We extend the loop gas and the string gas states for Kitaev spin liquid (https://t.co/gt17AjuVi6) to the case of chiral spin liquids of Kitaev model on the star lattice. We show that those tensor network states can express gapped chiral spin liquids.', 'Kiatev „Çπ„Éî„É≥Ê∂≤‰Ωì„ÅÆ„Ç≥„É≥„Éë„ÇØ„Éà„Å™„ÉÜ„É≥„ÇΩ„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØË°®Áèæ„Åß„ÅÇ„Çãloop gas„ÄÅstring gas Áä∂ÊÖã„Çí„Ç´„Ç§„É©„É´„Çπ„Éî„É≥Ê∂≤‰Ωì„Å´Êã°Âºµ„Åó„Åæ„Åó„Åü„ÄÇarXiv:1901.05786„Å®ÂêåÊßò„ÄÅ„Å®„Å¶„ÇÇËâØ„ÅÑ‰ªï‰∫ã„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇarXiv:1901.05786„Å®ÊØî„Åπ„Çã„Å®ÂÉïËá™Ë∫´„ÅÆÂØÑ‰∏é„ÅØÂ∞è„Åï„ÅÑ„Åß„Åô„Åå...']",https://arxiv.org/abs/1907.02268,"We provide new insights into the Abelian and non-Abelian chiral Kitaev spin liquids on the star lattice using the recently proposed loop gas (LG) and string gas (SG) states [H.-Y. Lee, R. Kaneko, T. Okubo, N. Kawashima, Phys. Rev. Lett. 123, 087203 (2019)]. Those are compactly represented in the language of tensor network. By optimizing only one or two variational parameters, accurate ansatze are found in the whole phase diagram of the Kitaev model on the star lattice. In particular, the variational energy of the LG state becomes exact(within machine precision) at two limits in the model, and the criticality at one of those is analytically derived from the LG feature. It reveals that the Abelian CSLs are well demonstrated by the short-ranged LG while the non-Abelian CSLs are adiabatically connected to the critical LG where the macroscopic loops appear. Furthermore, by constructing the minimally entangled states and exploiting their entanglement spectrum and entropy, we identify the nature of anyons and the chiral edge modes in the non-Abelian phase with the Ising conformal field theory. ","Abelian and non-Abelian chiral spin liquids in a compact tensor network
  representation"
141,1146798627953598464,1176044852,Aurelien Lucchi,['New work to appear at #UAI2019 on the role of memory in stochastic optimization:\n<LINK>\nWe study how the way of retaining information about past gradients affects the convergence properties of an algorithm and make connections to Adam and Nesterov momentum.'],https://arxiv.org/abs/1907.01678,"The choice of how to retain information about past gradients dramatically affects the convergence properties of state-of-the-art stochastic optimization methods, such as Heavy-ball, Nesterov's momentum, RMSprop and Adam. Building on this observation, we use stochastic differential equations (SDEs) to explicitly study the role of memory in gradient-based algorithms. We first derive a general continuous-time model that can incorporate arbitrary types of memory, for both deterministic and stochastic settings. We provide convergence guarantees for this SDE for weakly-quasi-convex and quadratically growing functions. We then demonstrate how to discretize this SDE to get a flexible discrete-time algorithm that can implement a board spectrum of memories ranging from short- to long-term. Not only does this algorithm increase the degrees of freedom in algorithmic choice for practitioners but it also comes with better stability properties than classical momentum in the convex stochastic setting. In particular, no iterate averaging is needed for convergence. Interestingly, our analysis also provides a novel interpretation of Nesterov's momentum as stable gradient amplification and highlights a possible reason for its unstable behavior in the (convex) stochastic setting. Furthermore, we discuss the use of long term memory for second-moment estimation in adaptive methods, such as Adam and RMSprop. Finally, we provide an extensive experimental study of the effect of different types of memory in both convex and nonconvex settings. ",The Role of Memory in Stochastic Optimization
142,1146501535586816003,2780460254,Dr Deanna C. Hooper,"[""I have a new paper out today! A lot of work has gone in to this paper, I'm so glad it's finished! We studied dark matter - dark radiation interactions using Lyman-alpha data. Open access version here: <LINK>""]",https://arxiv.org/abs/1907.01496v1,"Several interesting Dark Matter (DM) models invoke a dark sector leading to two types of relic particles, possibly interacting with each other: non-relativistic DM, and relativistic Dark Radiation (DR). These models have interesting consequences for cosmological observables, and could in principle solve problems like the small-scale cold DM crisis, Hubble tension, and/or low $\sigma_8$ value. Their cosmological behaviour is captured by the ETHOS parametrisation, which includes a DR-DM scattering rate scaling like a power-law of the temperature, $T^n$. Scenarios with $n=0$, $2$, or $4$ can easily be realised in concrete dark sector set-ups. Here we update constraints on these three scenarios using recent CMB, BAO, and high-resolution Lyman-$\alpha$ data. We introduce a new Lyman-$\alpha$ likelihood that is applicable to a wide range of cosmological models with a suppression of the matter power spectrum on small scales. For $n=2$ and $4$, we find that Lyman-$\alpha$ data strengthen the CMB+BAO bounds on the DM-DR interaction rate by many orders of magnitude. However, models offering a possible solution to the missing satellite problem are still compatible with our new bounds. For $n=0$, high-resolution Lyman-$\alpha$ data bring no stronger constraints on the interaction rate than CMB+BAO data, except for extremely small values of the DR density. Using CMB+BAO data and a theory-motivated prior on the minimal density of DR, we find that the $n=0$ model can reduce the Hubble tension from $4.1\sigma$ to $2.7\sigma$, while simultaneously accommodating smaller values of the $\sigma_8$ and $S_8$ parameters hinted by cosmic shear data. ","] Constraining Dark Matter -- Dark Radiation interactions with CMB, BAO,
  and Lyman-$\alpha$"
143,1146294050376560640,1138762581164855298,Christoph Ternes,"['New paper on @arxiv, <LINK> with @MariamTortola. We study the sensitivity of DUNE (@DUNEScience) and JUNO to Quasi-Dirac neutrino oscillations, which can occur when neutrinos obtain both Dirac and Majorana masses, with the latter ones sufficiently small.']",https://arxiv.org/abs/1907.00980,"Quasi-Dirac neutrinos are obtained when the Lagrangian density of a neutrino mass model contains both Dirac and Majorana mass terms, and the Majorana terms are sufficiently small. This type of neutrinos introduces new mixing angles and mass splittings into the Hamiltonian, which will modify the standard neutrino oscillation probabilities. In this paper, we focus on the case where the new mass splittings are too small to be measured, but new angles and phases are present. We perform a sensitivity study for this scenario for the upcoming experiments DUNE and JUNO, finding that they will improve current bounds on the relevant parameters. Finally, we also explore the discovery potential of both experiments, assuming that neutrinos are indeed quasi-Dirac particles. ",Quasi-Dirac neutrino oscillations at DUNE and JUNO
144,1145883134572896256,2929934263,Blaise Cruz,"[""Hi! We've released Filipino versions of BERT &amp; ULMFiT, plus a large unlabeled preprocessed text corpora in Filipino. We're open sourcing it so everyone can use it in their projects :) Cite us if you find it useful!\n\nLink: <LINK>\nPreprint: <LINK>""]",https://arxiv.org/abs/1907.00409,"Unlike mainstream languages (such as English and French), low-resource languages often suffer from a lack of expert-annotated corpora and benchmark resources that make it hard to apply state-of-the-art techniques directly. In this paper, we alleviate this scarcity problem for the low-resourced Filipino language in two ways. First, we introduce a new benchmark language modeling dataset in Filipino which we call WikiText-TL-39. Second, we show that language model finetuning techniques such as BERT and ULMFiT can be used to consistently train robust classifiers in low-resource settings, experiencing at most a 0.0782 increase in validation error when the number of training examples is decreased from 10K to 1K while finetuning using a privately-held sentiment dataset. ","Evaluating Language Model Finetuning Techniques for Low-resource
  Languages"
145,1159542966224134145,10834752,Arvind Narayanan,"[""A couple of major dark patterns updates:\n- The study has been peer reviewed and @aruneshmathur will present it at ACM CSCW in Austin, TX <LINK>\n- We've released our source code and full dataset <LINK>\nDirect link to paper: <LINK> <LINK>""]",https://arxiv.org/abs/1907.07032,"Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns. ",Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites
146,1156123734895292416,426509606,Yamir Moreno,"['Out today on the arXiv: ""Collective dynamics of random Janus oscillator networks"". Here we analytically and numerically study the rich dynamics of ensembles of these oscillators coupled on large homogeneous and heterogeneous networks. <LINK> <LINK>']",https://arxiv.org/abs/1907.12065,"Janus oscillators have been recently introduced as a remarkably simple phase oscillator model that exhibits non-trivial dynamical patterns -- such as chimeras, explosive transitions, and asymmetry-induced synchronization -- that once were only observed in specifically tailored models. Here we study ensembles of Janus oscillators coupled on large homogeneous and heterogeneous networks. By virtue of the Ott-Antonsen reduction scheme, we find that the rich dynamics of Janus oscillators persists in the thermodynamic limit of random regular, Erd\H{o}s-R\'enyi and scale-free random networks. We uncover for all these networks the coexistence between partially synchronized state and a multitude of states displaying global oscillations. Furthermore, abrupt transitions of the global and local order parameters are observed for all topologies considered. Interestingly, only for scale-free networks, it is found that states displaying global oscillations vanish in the thermodynamic limit. ",Collective dynamics of random Janus oscillator networks
147,1151698877449297920,837412258150035457,Dr. Burcin Mutlu-Pakdil,"['Our new paper on ultra-faint dwarf galaxies (UFD) is out on arXiv today: <LINK> \n\nThis is the first comprehensive case study of signatures of tidal distribution in UFDs. We focused on LeoV and found very cool results!!  Check them out!!', 'Our team includes \n@sand_dave @caprastro @JeffCarlinastro @BethWillman @denija83 @michelle_lmc @anilcseth M. Walker, N. Caldwell, M.Mateo, E. Olszewski and D. Zaritsky of @azstewobs. @UAresearch', 'Leo V has shown both photometric overdensities and kinematic members at large radii, along with a tentative kinematic gradient, suggesting that it may have undergone a close encounter with the Milky Way. We critically assessed these signs in combination with new observations üí™üèª', 'Our comprehensive analysis includes i) HST photometry @NASAHubble, ii) two epochs of spectra from Hectochelle on @mmtobservatory, and iii) @ESAGaia measurements.', 'Using the #HST data, we examine one of the reported stream-like overdensities at large radii, and conclude that it is not a true stellar stream, but instead a clump of foreground stars and background galaxies.', 'Our spectroscopic analysis shows that one known member star is likely a binary, and challenges the membership status of three others, including two distant candidates that had formerly provided evidence for overall stellar mass loss.', 'We also find evidence that the proposed kinematic gradient across Leo V might be due to small number statistics.', 'Using #Gaia DR2, we update the systemic proper motion of Leo V, which is consistent with its reported orbit that did not put Leo V at risk of being disturbed by the Milky Way.', 'Our findings remove most of the observational clues that suggested Leo V was disrupting, but we also find new plausible member stars, two of which are located &gt;5 half-light radii from the main body. ü§∑üèª\u200d‚ôÄÔ∏è These stars require further investigation.', 'The nature of Leo V still remains an open question. The true nature of UFDs of #MilkyWay is indeed hard to understand!', '@Claire_Simeone Thank you Claire!! ü§©', '@listen2spacepod Yes! We should do it. Dwarf galaxies are so cool ü§©']",https://arxiv.org/abs/1907.07233,"The ultra-faint dwarf galaxy Leo V has shown both photometric overdensities and kinematic members at large radii, along with a tentative kinematic gradient, suggesting that it may have undergone a close encounter with the Milky Way. We investigate these signs of disruption through a combination of i) high-precision photometry obtained with the Hubble Space Telescope (HST), ii) two epochs of stellar spectra obtained with the Hectochelle Spectrograph on the MMT, and iii) measurements from the Gaia mission. Using the HST data, we examine one of the reported stream-like overdensities at large radii, and conclude that it is not a true stellar stream, but instead a clump of foreground stars and background galaxies. Our spectroscopic analysis shows that one known member star is likely a binary, and challenges the membership status of three others, including two distant candidates that had formerly provided evidence for overall stellar mass loss. We also find evidence that the proposed kinematic gradient across Leo V might be due to small number statistics. We update the systemic proper motion of Leo V, finding $(\mu_\alpha \cos\delta, \mu_\delta)= (0.009\pm0.560$, $-0.777\pm0.314)$ mas yr$^{-1}$, which is consistent with its reported orbit that did not put Leo V at risk of being disturbed by the Milky Way. These findings remove most of the observational clues that suggested Leo V was disrupting, however, we also find new plausible member stars, two of which are located >5 half-light radii from the main body. These stars require further investigation. Therefore, the nature of Leo V still remains an open question. ","Signatures of Tidal Disruption in Ultra-Faint Dwarf Galaxies: A Combined
  HST, Gaia, and MMT/Hectochelle Study of Leo V"
148,1146367776308809729,4111874585,Manuel Rigger,"['A preprint of our @FSEconf paper ""Understanding GCC Builtins to Develop Better Tools"" is now online at <LINK>.  We took care to make the results replicable; check out the repository at <LINK>. Work with @smarr, Bram Adams, and @moessenboeck. <LINK>', '@FSEconf @smarr @moessenboeck CC @gnutools, @llvmweekly, @llvmorg']",https://arxiv.org/abs/1907.00863,"C programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we analyzed the evolution of builtin use over time and found that the majority of projects mostly added builtins. This suggests that builtins are not a legacy feature and must be supported in future tools. Systematic testing of builtin support in existing tools revealed that many lacked support for builtins either partially or completely; we also discovered incorrect implementations in various tools, including the formally verified CompCert compiler. ",Understanding GCC Builtins to Develop Better Tools
