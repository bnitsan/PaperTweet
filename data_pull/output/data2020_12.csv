,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1349458668253818882,294188323,Will Zeng,['Want to learn about our recent work on resource estimation for quantum advantage in derivative pricing?\n\nMy colleague @nikitasstam is giving a public talk at the virtual @NYCQuantum Meetup on Jan 26th 18.00 ET\n\nPaper ref: <LINK>\n\nSignup: <LINK> <LINK>'],https://arxiv.org/abs/2012.03819,"We give an upper bound on the resources required for valuable quantum advantage in pricing derivatives. To do so, we give the first complete resource estimates for useful quantum derivative pricing, using autocallable and Target Accrual Redemption Forward (TARF) derivatives as benchmark use cases. We uncover blocking challenges in known approaches and introduce a new method for quantum derivative pricing - the re-parameterization method - that avoids them. This method combines pre-trained variational circuits with fault-tolerant quantum computing to dramatically reduce resource requirements. We find that the benchmark use cases we examine require 8k logical qubits and a T-depth of 54 million. We estimate that quantum advantage would require executing this program at the order of a second. While the resource requirements given here are out of reach of current systems, we hope they will provide a roadmap for further improvements in algorithms, implementations, and planned hardware architectures. ",A Threshold for Quantum Advantage in Derivative Pricing
1,1348522633923477505,38824024,Aman Gupta,"['New paper out on Arxiv! We propose a video embedding technique - deep smoothed gaussian mixture model (DSGMM). We improve on NetVLAD using smoothing for low count clusters. We show classification improvements on YouTube-8M and a LinkedIn dataset. Link - <LINK>', 'Joint work with Sirjan Kafle, Xue Xia, Ananth Sankar, Xi Chen, Di Wen and Liang Zhang']",https://arxiv.org/abs/2012.11673,"Cluster-and-aggregate techniques such as Vector of Locally Aggregated Descriptors (VLAD), and their end-to-end discriminatively trained equivalents like NetVLAD have recently been popular for video classification and action recognition tasks. These techniques operate by assigning video frames to clusters and then representing the video by aggregating residuals of frames with respect to the mean of each cluster. Since some clusters may see very little video-specific data, these features can be noisy. In this paper, we propose a new cluster-and-aggregate method which we call smoothed Gaussian mixture model (SGMM), and its end-to-end discriminatively trained equivalent, which we call deep smoothed Gaussian mixture model (DSGMM). SGMM represents each video by the parameters of a Gaussian mixture model (GMM) trained for that video. Low-count clusters are addressed by smoothing the video-specific estimates with a universal background model (UBM) trained on a large number of videos. The primary benefit of SGMM over VLAD is smoothing which makes it less sensitive to small number of training samples. We show, through extensive experiments on the YouTube-8M classification task, that SGMM/DSGMM is consistently better than VLAD/NetVLAD by a small but statistically significant margin. We also show results using a dataset created at LinkedIn to predict if a member will watch an uploaded video. ","Smoothed Gaussian Mixture Models for Video Classification and
  Recommendation"
2,1347217988613767168,96854493,Meike Zehlike,['Check out our new paper (<LINK>) that presents a modified significance adjustment algorithm for FA*IR and the math behind it. This new version is also the one that is implement in #FairSearch (<LINK>).\nJoint work with @tom_suhr and @ChaToX'],http://arxiv.org/abs/2012.12795,"In this report we provide an improvement of the significance adjustment from the FA*IR algorithm of Zehlike et al., which did not work for very short rankings in combination with a low minimum proportion $p$ for the protected group. We show how the minimum number of protected candidates per ranking position can be calculated exactly and provide a mapping from the continuous space of significance levels ($\alpha$) to a discrete space of tables, which allows us to find $\alpha_c$ using a binary search heuristic. ","A Note on the Significance Adjustment for FA*IR with Two Protected
  Groups"
3,1346842343018819584,2308964947,Gábor Melis,['Our new paper (<LINK>) about improving the tradeoff behind posterior collapse combines constraints on mutual information with Monte-Carlo objectives.'],https://arxiv.org/abs/2012.00708,"A common failure mode of density models trained as variational autoencoders is to model the data without relying on their latent variables, rendering these variables useless. Two contributing factors, the underspecification of the model and the looseness of the variational lower bound, have been studied separately in the literature. We weave these two strands of research together, specifically the tighter bounds of Monte-Carlo objectives and constraints on the mutual information between the observable and the latent variables. Estimating the mutual information as the average Kullback-Leibler divergence between the easily available variational posterior $q(z|x)$ and the prior does not work with Monte-Carlo objectives because $q(z|x)$ is no longer a direct approximation to the model's true posterior $p(z|x)$. Hence, we construct estimators of the Kullback-Leibler divergence of the true posterior from the prior by recycling samples used in the objective, with which we train models of continuous and discrete latents at much improved rate-distortion and no posterior collapse. While alleviated, the tradeoff between modelling the data and using the latents still remains, and we urge for evaluating inference methods across a range of mutual information values. ",Mutual Information Constraints for Monte-Carlo Objectives
4,1346609649479520263,15751681,Irene Celino,['#DBLP added a new #paper <LINK> to my page <LINK>'],https://arxiv.org/abs/2012.11936,"One of the grand challenges discussed during the Dagstuhl Seminar ""Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web"" and described in its report is that of a: ""Public FAIR Knowledge Graph of Everything: We increasingly see the creation of knowledge graphs that capture information about the entirety of a class of entities. [...] This grand challenge extends this further by asking if we can create a knowledge graph of ""everything"" ranging from common sense concepts to location based entities. This knowledge graph should be ""open to the public"" in a FAIR manner democratizing this mass amount of knowledge."" Although linked open data (LOD) is one knowledge graph, it is the closest realisation (and probably the only one) to a public FAIR Knowledge Graph (KG) of everything. Surely, LOD provides a unique testbed for experimenting and evaluating research hypotheses on open and FAIR KG. One of the most neglected FAIR issues about KGs is their ongoing evolution and long term preservation. We want to investigate this problem, that is to understand what preserving and supporting the evolution of KGs means and how these problems can be addressed. Clearly, the problem can be approached from different perspectives and may require the development of different approaches, including new theories, ontologies, metrics, strategies, procedures, etc. This document reports a collaborative effort performed by 9 teams of students, each guided by a senior researcher as their mentor, attending the International Semantic Web Research School (ISWS 2019). Each team provides a different perspective to the problem of knowledge graph evolution substantiated by a set of research questions as the main subject of their investigation. In addition, they provide their working definition for KG preservation and evolution. ","Knowledge Graphs Evolution and Preservation -- A Technical Report from
  ISWS 2019"
5,1345301528027025408,90131577,Noam Slonim 🟢,"['If you have interest in *targeted* sentiment analysis, we just released YASO - a new carefully curated benchmark data, with a cool name. The paper includes SOTA res, and lots of interesting analyses. #ProjectDebater @IBMResearch #SentimentAnalysis #NLP -- <LINK> <LINK>']",https://arxiv.org/abs/2012.14541,"Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO - a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at this https URL ","YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain
  Reviews"
6,1345300312974909443,158659687,Sibylle Anderl,"[""Another piece in the puzzle of low-mass star formation: We have a new paper coming out in A&amp;A, led by Linda Podio, on protostellar jets and outflows. It's based on PdBI-observations of all 21 CALYPSO-Class-0-sources in CO, SiO and SO.  <LINK>""]",https://arxiv.org/abs/2012.15379,"As a part of the CALYPSO large programme, we constrain the properties of protostellar jets and outflows in a sample of 21 Class 0 protostars with internal luminosities, Lint, from 0.035 to 47 Lsun. We analyse high angular resolution (~0.5""-1"") IRAM PdBI observations in CO (2-1), SO ($5_6-4_5$), and SiO (5-4). CO (2-1), which probes outflowing gas, is detected in all the sources (for the first time in SerpS-MM22 and SerpS-MM18b). Collimated high-velocity jets in SiO (5-4) are detected in 67% of the sources (for the first time in IRAS4B2, IRAS4B1, L1448-NB, SerpS-MM18a), and 77% of these also show jet/outflow emission in SO ($5_6-4_5$). In 5 sources (24% of the sample) SO ($5_6-4_5$) probes the inner envelope and/or the disk. The CALYPSO survey shows that the outflow phenomenon is ubiquitous and that the detection rate of high-velocity jets increases with protostellar accretion, with at least 80% of the sources with Lint>1 Lsun driving a jet. The protostellar flows exhibit an onion-like structure, where the SiO jet (opening angle ~10$^o$) is nested into a wider angle SO (~15$^o$) and CO (~25$^o$) outflow. On scales >300 au the SiO jets are less collimated than atomic jets from Class II sources (~3$^o$). Velocity asymmetry between the two jet lobes are detected in one third of the sources, similarly to Class II atomic jets, suggesting that the same launching mechanism is at work. Most of the jets are SiO rich (SiO/H2 from >2.4e-7 to >5e-6), which indicates efficient release of >1%-10% of silicon in gas phase likely in dust-free winds, launched from inside the dust sublimation radius. The mass-loss rates (from ~7e-8 to ~3e-6 Msun/yr) are larger than what was measured for Class II jets. Similarly to Class II sources, the mass-loss rates are ~1%-50% of the mass accretion rates suggesting that the correlation between ejection and accretion in young stars holds from 1e4 yr up to a few Myr. ","The CALYPSO IRAM-PdBI survey of jets from Class 0 protostars. Are jets
  ubiquitous in young stars ?"
7,1345161894189928448,1489278174,Claire Edmunds,"['Starting off 2021 right with a new paper on arXiv from @ARC_EQUS and @Sydney_Science! Exciting new work reducing the measurement error in Yb 171 ions using electron shelving 1/5\n\n#quantum #trappedions \n<LINK>', 'Yb ions are awesome for #quantumcomputing as they store info for a long time and aren\'t too badly affected by magnetic field noise (both @honeywell @IonQ_Inc use Yb!)\n\nBut measurement error tends to be larger as the qubit states can mix during detection(""off-resonant scattering"")', 'By shelving the |1&gt; qubit state to a metastable D5/2 level before detection, we achieve single-qubit detection errors of 1.8e-3 on an APD (6x lower than our best efforts previously!) and 7.7e-3 on an EMCCD camera (4x lower)  3/5', 'We record a detection error as low as 6e-6 and 6.3e-4 on the APD and EMCCD if we shelve to the long-lived F7/2 state (a procedure that is currently shelving-rate limited)\n\nAn amazing team effort with @MJBiercuk, Ting Rei Tan, Alistair Milne and Ashwin Singh, led by @CHQuant! 4/5 https://t.co/CRAauC8OME', 'This project goes hand in hand with a precision characterisation on the 411nm transition in Yb that we use for the electron shelving, up on arXiv a few days ago\n\nhttps://t.co/k4GSIGnahu\n\nhttps://t.co/Upo8VhGlQw', 'ping @Berkeley_ions @Ion_busters @IonQ_Inc @iqoqi @SandiaLabs']",https://arxiv.org/abs/2012.14606,"Qubits encoded in hyperfine states of trapped ions are ideal for quantum computation given their long lifetimes and low sensitivity to magnetic fields, yet they suffer from off-resonant scattering during detection often limiting their measurement fidelity. In ${}^{171}$Yb$^{+}$ this is exacerbated by a low fluorescence yield, which leads to a need for complex and expensive hardware - a problematic bottleneck especially when scaling up the number of qubits. We demonstrate a detection routine based on electron shelving to address this issue in ${}^{171}$Yb$^{+}$ and achieve a 5.6$\times$ reduction in single-ion detection error on an avalanche photodiode to $1.8(2)\times10^{-3}$ in a 100 $\mu$s detection period, and a 4.3$\times$ error reduction on an electron multiplying CCD camera, with $7.7(2)\times10^{-3}$ error in 400 $\mu$s. We further improve the characterization of a repump transition at 760 nm to enable a more rapid reset of the auxiliary $^2$F$_{7/2}$ states populated after shelving. Finally, we examine the detection fidelity limit using the long-lived $^2$F$_{7/2}$ state, achieving a further 300$\times$ and 12$\times$ reduction in error to $6(7)\times10^{-6}$ and $6.3(3)\times10^{-4}$ in 1 ms on the respective detectors. While shelving-rate limited in our setup, we suggest various techniques to realize this detection method at speeds compatible with quantum information processing, providing a pathway to ultra-high fidelity detection in ${}^{171}$Yb$^{+}$. ","Scalable hyperfine qubit state detection via electron shelving in the
  ${}^2$D$_{5/2}$ and ${}^2$F$_{7/2}$ manifolds in ${}^{171}$Yb$^{+}$"
8,1345125497328422917,1068545181576773632,Kenneth Brown,"['Happy New Year! Please check out ""Between Shor and Steane: A unifying construction for measuring error syndromes ""<LINK> my new paper with @Huang_Shilin.', 'Key result 1: We have a method for creating new syndrome extraction circuits by decomposing a set of parity checks into the product of two other matrices. https://t.co/QoBtQUbvYV', 'Key result 2: In the context of circuit-level error correction, we can choose different compositions each time we measure the error syndrome. This limits the correlations between different measurements.', 'As an example, we consider the l*l toric code where we use m*m patches to check the syndrome. We find that by offsetting the m*m patches in space over measurement rounds we can increase the threshold. https://t.co/MM4tM9MwRP', 'When @Huang_Shilin first explained this to me, he had a great analogy of hopping mall escalators. You go up the escalator from 2 to 3 but then to get to the escalator to go from 3 to 4 you need to walk by all the shops. It reminded me of these escalators in San Francisco. https://t.co/k1QbuqvYBH', 'Our motivation for starting this project is that Steane style syndrome extraction is much better than Shor style extraction, if you can afford the cost of making a more complicated encoded state for measuring the error.', 'The reason is simple. Steane only needs to touch each data qubit twice. Shor methods tend to touch the data qubits more.  For an l*l toric code, it needs to touch each data qubit O(l) times.', 'Whether or not this helps in practice depends on the connectivity of the quantum computer and the relative strength between gate errors and memory errors.']",https://arxiv.org/abs/2012.15403,"Fault-tolerant quantum error correction requires the measurement of error syndromes in a way that minimizes correlated errors on the quantum data. Steane and Shor ancilla are two well-known methods for fault-tolerant syndrome extraction. In this paper, we find a unifying construction that generates a family of ancilla blocks that interpolate between Shor and Steane. This family increases the complexity of ancilla construction in exchange for reducing the rounds of measurement required to fault-tolerantly measure the error. We then apply this construction to the toric code of size $L\times L$ and find that blocks of size $m\times m$ can be used to decode errors in $O(L/m)$ rounds of measurements. Our method can be applied to any Calderbank-Shor-Steane codes and presents a new direction for optimizing fault-tolerant quantum computation. ","Between Shor and Steane: A unifying construction for measuring error
  syndromes"
9,1345109123042603010,1077995761487568896,Jon Miller,"['New paper day!  \nA year and 100+ observations of the black hole GRS 1915+105 by @mayuishungry reveals its faint state is driven by variable, sometimes Compton-thick obscuration.\nThe pic shows before &amp; after spectra at the same source luminosity.  Madness.\n<LINK> <LINK>']",https://arxiv.org/abs/2012.15033,"GRS 1915$+$105 is a stellar-mass black hole that is well known for exhibiting at least 12 distinct classes of X-ray variability and correlated multi-wavelength behavior. Despite such extraordinary variability, GRS 1915$+$105 remained one of the brightest sources in the X-ray sky. However, in early 2019, the source became much fainter, apparently entering a new accretion state. Here, we report the results of an extensive, year-long monitoring campaign of GRS 1915$+$105 with the Neil Gehrels Swift Observatory. During this interval, the flux of GRS 1915$+$105 gradually diminished; the observed count rate eventually dropped by two orders of magnitude. Simple but robust spectral fits to these monitoring observations show that this new state results from the combination of a dramatic and persistent increase in internal obscuration, and a reduced mass accretion rate. The internal obscuration is the dominant effect, with a median value of $N_{H} = 7\times 10^{23}~{\rm cm}^{-2}$. In a number of observations, the source appears to be Compton-thick. We suggest that this state should be identified as the ""obscured state,"" and discuss the implications of this new (or rarely observed) accretion mode for black holes across the mass scale. ",The Novel Obscured State of Stellar-mass Black Hole GRS 1915+105
10,1345098779926671360,892059194240532480,Mikel Artetxe,"['We have a new paper on cross-lingual word embeddings! Instead of aligning fixed monolingual embeddings under the isometry assumption, our method fixes the target language embeddings, and learns aligned embeddings in the source language from scratch.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2012.15715,"Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. ","Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through
  Context Anchoring"
11,1345068435177353216,1138704090106646528,Aitor Ormazabal,"['Check out our new paper ""Beyond offline mapping: Learning Cross Lingual Word Embeddings through Context Anchoring"". We propose a new method to learn word embeddings aligned in a target space without a mapping step, outperforming mapping methods in BLI. <LINK>', 'w/ @artetxem, @Aitor57 , @glabaka and @eagirre']",http://arxiv.org/abs/2012.15715,"Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. ","Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through
  Context Anchoring"
12,1345057650279518208,96779364,Arnab Bhattacharyya,"['To round off 2020, a new paper out on arXiv (<LINK>) on testing product distributions. This is joint work with Sutanu Gayen, @Saravanan_CU and Vinod Variyam, and will appear at ALT 2021.', 'We take a second look at identity testing and closeness testing of distributions that are promised to be product. Arguably, the simplest type of structure in high-dimensions is full independence of components.', ""The primary goal of the paper is to explore how the complexity of testing identity of product distributions differs with respect to the *metric* that's used to measure closeness. Our results extend prior work on identity testing binary product dists wrt total variation distance."", 'A fantastic related open question is to give tight bounds for identity testing of bounded-degree Bayes nets. In a different vein, one could also ask for testing identity wrt f-divergences or integral probability metrics.']",https://arxiv.org/abs/2012.14632,"We study the problems of identity and closeness testing of $n$-dimensional product distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart (COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample complexity bounds for non-tolerant testing over a binary alphabet: given two product distributions $P$ and $Q$ over a binary alphabet, distinguish between the cases $P = Q$ and $d_{\mathrm{TV}}(P, Q) > \epsilon$. We build on this prior work to give a more comprehensive map of the complexity of testing of product distributions by investigating tolerant testing with respect to several natural distance measures and over an arbitrary alphabet. Our study gives a fine-grained understanding of how the sample complexity of tolerant testing varies with the distance measures for product distributions. In addition, we also extend one of our upper bounds on product distributions to bounded-degree Bayes nets. ",Testing Product Distributions: A Closer Look
13,1345047774635708416,893109085,Edouard Grave,"['New paper on memory efficient open domain question answering. We show that combining dimension reduction, vector quantization and passage filtering greatly reduces the memory footprint of retrieval based systems, without hurting accuracy too much.\nPaper: <LINK> <LINK>', 'Joint work with @gizacard, @Fabio_Petroni, @lucas_hosseini, @nicola_decao and @riedelcastro. This was part of our winning entry to the 6Gb track of the EfficientQA NeurIPS competition.']",https://arxiv.org/abs/2012.15156,"Recently, retrieval systems based on dense representations have led to important improvements in open-domain question answering, and related tasks. While very effective, this approach is also memory intensive, as the dense vectors for the whole knowledge source need to be kept in memory. In this paper, we study how the memory footprint of dense retriever-reader systems can be reduced. We consider three strategies to reduce the index size: dimension reduction, vector quantization and passage filtering. We evaluate our approach on two question answering benchmarks: TriviaQA and NaturalQuestions, showing that it is possible to get competitive systems using less than 6Gb of memory. ",A Memory Efficient Baseline for Open Domain Question Answering
14,1345016902289125376,179222978,Yoav Len,"[""A new paper with Dmitry Zakharov, presenting a new version of the classical Kirchhoff's matrix tree theorem!  1/n\n<LINK>"", ""Kirchhoff's theorem can be interpreted as #{spanning trees of a graph G} = #{elements in Jac(G)}.  With this new formula, we found an analogous combinatorial description for the number of elements in another group that one can naturally associate to a graph, the Prym group.  2/n."", ""There's a continuous version for metric graphs as well, which implies an interesting decomposition of the Prym variety.\nn/n.""]",https://arxiv.org/abs/2012.15235,"We prove an analogue of Kirchhoff's matrix tree theorem for computing the volume of the tropical Prym variety for double covers of metric graphs. We interpret the formula in terms of a semi-canonical decomposition of the tropical Prym variety, via a careful study of the tropical Abel-Prym map. In particular, we show that the map is harmonic, determine its degree at every cell of the decomposition, and prove that its global degree is $2^{g-1}$. Along the way, we use the Ihara zeta function to provide a new proof of the analogous result for finite graphs. As a counterpart, the appendix by Sebastian Casalaina-Martin shows that the degree of the algebraic Abel-Prym map is $2^{g-1}$ as well. ",Kirchhoff's theorem for Prym varieties
15,1344910044085645312,467433945,Xiangci Li,"['Happy new year!\nI\'m happy to share that my paper ""A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification"" will appear at  #aaai2021 SDU workshop! #NLP  <LINK>']",https://arxiv.org/abs/2012.14500,"Even for domain experts, it is a non-trivial task to verify a scientific claim by providing supporting or refuting evidence rationales. The situation worsens as misinformation is proliferated on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes crucial for combating the spread of misinformation. In this work, we propose a novel, paragraph-level, multi-task learning model for the SciFact task by directly computing a sequence of contextualized sentence embeddings from a BERT model and jointly training the model on rationale selection and stance prediction. ","A Paragraph-level Multi-task Learning Model for Scientific
  Fact-Verification"
16,1344346476617592833,825458566689648645,Sajad Sotudeh,"['I\'m excited to announce our new paper: ""On Generating Extended Summaries of Long Documents"" w/ @armancohan and Nazli Goharian. \n\narXiv: <LINK>\nCode and Datasets: <LINK>\n(1/4) <LINK>', 'We propose a novel multi-tasking approach that exploits the hierarchical structure of long scientific documents to aid the extractive summarization model in selecting summary-worthy sentences, and finally, form the ""extended"" summaries of the given long documents. (2/4)', 'In order to support this task, we additionally collect two extended summarization datasets: arXiv-Long, and PubMed-Long. The experimental results indicate that the multi-tasking model either outperforms or matches the performance of the prior baseline (i.e., BertSumExt). (3/4)', 'The extrinsic analysis: 1) our model improves consistently as the summary length increases; 2) our model adjusts the extraction probability of sentences toward salient sentences across diverse sections of the source document and pick those with higher confidence. (4/4) https://t.co/otpKom9Lov']",https://arxiv.org/abs/2012.14136,"Prior work in document summarization has mainly focused on generating short summaries of a document. While this type of summary helps get a high-level view of a given document, it is desirable in some cases to know more detailed information about its salient points that can't fit in a short summary. This is typically the case for longer documents such as a research paper, legal document, or a book. In this paper, we present a new method for generating extended summaries of long papers. Our method exploits hierarchical structure of the documents and incorporates it into an extractive summarization model through a multi-task learning approach. We then present our results on three long summarization datasets, arXiv-Long, PubMed-Long, and Longsumm. Our method outperforms or matches the performance of strong baselines. Furthermore, we perform a comprehensive analysis over the generated results, shedding insights on future research for long-form summary generation task. Our analysis shows that our multi-tasking approach can adjust extraction probability distribution to the favor of summary-worthy sentences across diverse sections. Our datasets, and codes are publicly available at this https URL ",On Generating Extended Summaries of Long Documents
17,1344052849991270400,3007703358,Yixin Nie,"['Happy to share our new paper on addressing contradictions in dialogue modeling.\n\n<LINK>\n\nWe introduce DialoguE COntradiction DEtection (DECODE) task and a new dataset with contradictory dialogues to study how well NLU models can capture consistency in dialogues. <LINK>', 'We then compare a structured utterance-based approach with a typical unstructured approach of using pre-trained Transformer models for dialogue contradiction detection. \n\n2/N', 'The result reveals that: \n(1). Our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain (e.g. DNLI); \n\n3/N', '(2). The structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. \n\n4/N', 'The finding argues for the development of more robust and generalizable NLU modeling and challenges a belief amongst some practitioners that they can just use a standard Transformer and it will learn all the structure correctly on its own. \n\n5/N', 'We also show that our best contradiction detector correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. \n\n6/N', 'Going forward, we envision complementary progress on both the modeling of NLU and NLG and the integration of the two. We hope our work could facilitate and provide guidelines for future work on incorporating NLU modeling into dialogue systems. \n\n7/N', 'Check out more details and the data on our project page in @parlai_parley.\n\nhttps://t.co/V0juczVC2o\nhttps://t.co/AcbMz8VKFz\n\n8/N', 'A huge Thank You to my co-authors: Mary Williamson, @mohitban47, @douwekiela, @jaseweston, and the ParlAI team.\n\nAlso, thanks to @wellecks, Arthur Szlam, @kchonyc for the prior work (Dialogue NLI).\n\n9/N']",https://arxiv.org/abs/2012.13391,"To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. ","I like fish, especially dolphins: Addressing Contradictions in Dialogue
  Modeling"
18,1344016432850468867,985933610174812162,Artur d'Avila Garcez,['And this new paper on Logic Tensor Networks with many examples now implemented in Tensorflow 2:\n<LINK> <LINK>'],https://arxiv.org/abs/2012.13635,"Artificial Intelligence agents are required to learn from their surroundings and to reason about the knowledge that has been learned in order to make decisions. While state-of-the-art learning from data typically uses sub-symbolic distributed representations, reasoning is normally useful at a higher level of abstraction with the use of a first-order logic language for knowledge representation. As a result, attempts at combining symbolic AI and neural computation into neural-symbolic systems have been on the increase. In this paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism and computational model that supports learning and reasoning through the introduction of a many-valued, end-to-end differentiable first-order logic called Real Logic as a representation language for deep learning. We show that LTN provides a uniform language for the specification and the computation of several AI tasks such as data clustering, multi-label classification, relational learning, query answering, semi-supervised learning, regression and embedding learning. We implement and illustrate each of the above tasks with a number of simple explanatory examples using TensorFlow 2. Keywords: Neurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic. ",Logic Tensor Networks
19,1343928977195479041,1299830240823455745,Matteo Agostini,"['What does Cosmology say about #Majorana #neutrinos? The discovery potential of next-generation double-beta decay experiments has never been higher! Here is our new paper with a Christmas-color palette: <LINK> #Physics #Science <LINK>', 'Uhm... it does resemble the Italian flag 🇮🇹. Is this just a coincidence @FrancescoVissa1?']",http://arxiv.org/abs/2012.13938,"We discuss the impact of the cosmological measurements on the predictions of the Majorana mass of the neutrinos, the parameter probed by neutrinoless double-beta decay experiments. Using a minimal set of assumptions, we quantify the probabilities of discovering neutrinoless double-beta decay and introduce a new graphical representation that could be of interest for the community. ",Discovery probabilities of Majorana neutrinos based on cosmological data
20,1343698488039677954,1515424688,Armen Aghajanyan,"[""I'm delighted to present a new exploratory paper, Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning (<LINK>), which explores fine-tuning through the lens of intrinsic dimensionality."", 'In particular we show there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. We make deeper connections between intrinsic dimension, pre-training and generalization.', 'As described in the seminal paper of Li et al., 2018, the intrinsic dimension (ID) of an objective function represents the minimal amount of parameters needed to solve the objective function to a satisfactory solution.', 'We compute this dimension by projecting the optimization problem into a smaller subspace and finding the subspace dimension that gives us 90% of the full solution (satisfactory solution). Surpassing engineering challenges, we’re able to compute the ID of large pre-trained models.', 'For example, we analyze BERT and RoBERTa’s intrinsic dimensions on MRPC and QQP datasets. We see that BERT-Large requires roughly 1100 parameters to reach 90% of the full parameter model on MRPC, while RoBERTa-Large requires 200. https://t.co/3EOJPXKxWh', 'The low dimensionality of NLP tasks within the framework provided by pre-trained representations leads us to further analyze the dynamics of pre-training and intrinsic dimensionality. We retrained RoBERTa-Base and measured the intrinsic dimensions of checkpoints across 6 tasks. https://t.co/M3TSIE0YK1', 'We interpret pre-training as implicitly minimizing the ID of the “average” NLP task. Under this interpretation, pre-trained models provide a framework of compressing downstream tasks; alternatively the ID represents the minimal description length of a task wrt to pretrained model', 'Well what about the effect of pre-trained model parameter count and intrinsic dimensionality? We follow up by doing an empirical study over pre-trained models available in the @huggingface library. https://t.co/0L2AYcEw5g', 'Perhaps surprisingly, larger models tend to have lower ID after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. The final connection we make is between generalization and ID, showing that models with smaller ID generalize better. https://t.co/JK7Me2aN5k', 'Lastly, we connect intrinsic dimensionality with compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count. https://t.co/7mh84ck9fT', 'We hope that intrinsic dimensionality has shined a light on the efficacy on fine-tuning pre-trained models. This was joint work with great co-authors @LukeZettlemoyer, @sonalsgupta. And thank you @AkshatS07, @ml_perception for the in-depth conversations.', ""@yoavgo We wanted to stay consistent with the previous definition of ID, from Li et al.. Still, if you look at the MRPC/QQP graphs, we're able to achieve the full solution accuracy with a couple thousand parameters. Also this 90% ID allowed us to compute ID in a much more stable fashion."", ""@AlexTamkin @LukeZettlemoyer We actually did a similar experiment; we did see models with smaller ID needed less data almost weakly monotonically. But some other dataset-specific phenomena were going on, e.g., large areas of plateaus where lowering ID didn't reduce data need. We're looking into this!""]",https://arxiv.org/abs/2012.13255,"Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count. ","Intrinsic Dimensionality Explains the Effectiveness of Language Model
  Fine-Tuning"
21,1342929850353344512,2376146880,Hoifung Poon,"['Our new paper in AAAI-21: ""Self-Supervised Self-Supervision by Combining Deep Learning and Probabilistic Logic"" (<LINK>). 1/n', 'Self-supervision essentially hallucinates noisy training data at scale. In self-supervised self-supervision (S4), we further attempt to hallucinate hallucination schemes at scale. 2/n', 'Self-supervision can be task-agnostic, such as masked language model in pretraining. Here, we consider task-specific self-supervision, i.e., directly hallucinate training examples for an end task. 3/n', 'There are many forms of self-supervision, including distant supervision (e.g. using known relations to annotate co-occurring entity mentions), data programming (expert-derived rules), joint inference (domain-specific constraints). 4/n', 'In Deep Probabilistic Logic (DPL: https://t.co/zyqvByXU6Y), we use probabilistic logic as a unifying framework to combine noisy and potentially conflicting self-supervision, and train a deep neural network for the end task with variational EM. 5/n', ""The learning objective generalizes Judea Pearl's virtual evidence from random variables to arbitrary potential functions. 6/n"", 'In self-supervised self-supervision, we extend deep probabilistic logic with structured learning and active learning capability. Starting from seed self-supervision, the S4 system automatically proposes new self-supervision. 7/n', 'We show promising results on text classification with unary &amp; binary potentials, demonstrating superiority over state-of-the-art self-supervision systems such as DPL and Snorkel. 8/n', 'Next, would be fascinating to explore more complex tasks and self-supervision. Joint work with Hunter Lang (https://t.co/OcyzibEEh6). Paper available on arxiv: https://t.co/0fWoIuriSA. n/n']",https://arxiv.org/abs/2012.12474,"Labeling training examples at scale is a perennial challenge in machine learning. Self-supervision methods compensate for the lack of direct supervision by leveraging prior knowledge to automatically generate noisy labeled examples. Deep probabilistic logic (DPL) is a unifying framework for self-supervised learning that represents unknown labels as latent variables and incorporates diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. While DPL is successful at combining pre-specified self-supervision, manually crafting self-supervision to attain high accuracy may still be tedious and challenging. In this paper, we propose Self-Supervised Self-Supervision (S4), which adds to DPL the capability to learn new self-supervision automatically. Starting from an initial ""seed,"" S4 iteratively uses the deep neural network to propose new self supervision. These are either added directly (a form of structured self-training) or verified by a human expert (as in feature-based active learning). Experiments show that S4 is able to automatically propose accurate self-supervision and can often nearly match the accuracy of supervised methods with a tiny fraction of the human effort. ","Self-supervised self-supervision by combining deep learning and
  probabilistic logic"
22,1342832409310588931,63441844,David Roberson,['My Christmas gift to the world is a new paper with the excellent Simon Schmidt: <LINK>'],https://arxiv.org/abs/2012.13328,"We introduce the notion of nonlocal symmetry of a graph $G$, defined as a winning quantum correlation for the $G$-automorphism game that cannot be produced classically. Recent connections between quantum group theory and quantum information show that quantum correlations for this game correspond to tracial states on $C(\text{Qut}(G))$ -- the algebra of functions on the quantum automorphism group of $G$. This allows us to also define nonlocal symmetry for any quantum permutation group. We investigate the differences and similarities between this and the notion of quantum symmetry, defined as non-commutativity of $C(\text{Qut}(G))$. Roughly speaking, quantum symmetry vs nonlocal symmetry can be viewed respectively as non-classicality of our model of reality vs non-classicality of our observation of reality. We show that quantum symmetry is necessary but not sufficient for nonlocal symmetry. In particular, we show that the complete graph on five vertices is the only connected graph on five or fewer vertices with nonlocal symmetry, despite a dozen others having quantum symmetry. In particular this shows that the quantum symmetric group on four points, $S_4^+$, does not exhibit nonlocal symmetry, answering a question from the literature. In contrast to quantum symmetry, we show that two disjoint classical automorphisms do not guarantee nonlocal symmetry. However, three disjoint automorphisms do suffice. We also give a construction of quantum permutation matrices built from a finite abelian group $\Gamma$ and a permutation $\pi$ on $|\Gamma|$ elements. Computational evidence suggests that for cyclic groups of increasing size almost all permutations $\pi$ result in nonlocal symmetry. Surprisingly, the construction never results in nonlocal symmetry when $\mathbb{Z}_2^3$ is used. We also investigate under what conditions nonlocal symmetry arises when taking unions or products of graphs. ",Quantum symmetry vs nonlocal symmetry
23,1342545826690850818,1206622352777392146,Daniel Varjas,['Ever thought that amorphous matter is more symmetric than crystals? In our new paper with @helene_spring and @AkhmerovAnton we find amorphous phases where average mirror symmetry protects topological edge states on any edge orientation <LINK> <LINK>'],https://arxiv.org/abs/2012.12909,"Protection of topological surface states by reflection symmetry breaks down when the boundary of the sample is misaligned with one of the high symmetry planes of the crystal. We demonstrate that this limitation is removed in amorphous topological materials, where the Hamiltonian is invariant on average under reflection over any axis due to continuous rotation symmetry. While the local disorder caused by the amorphous structure weakens the topological protection, we demonstrate that the edge remains protected from localization. In order to classify such phases we perform a systematic search over all the possible symmetry classes in two dimensions and construct the example models realizing each of the proposed topological phases. Finally, we compute the topological invariant of these phases as an integral along a meridian of the spherical Brillouin zone of an amorphous Hamiltonian. ",Amorphous topological phases protected by continuous rotation symmetry
24,1342326299872911363,1197510168717725696,Tal Daniel,"['Excited to share our new work on (introspective) VAEs! We introduce Soft-IntroVAE. With @AvivTamar1 \nPaper: <LINK>\nWebsite: <LINK>\nCode: <LINK>\n[1/7] <LINK>', ""Soft-IntroVAE is a VAE that is trained adversarially without a discriminator. If you use VAEs in your research, give S-IntroVAE a try, you'll likely get much better results. \n[2/7]"", 'S-IntroVAE replaces the hinge-loss terms of IntroVAE with a smooth exponential loss on generated samples and utilizes the complete ELBO for the adversarial signal.\n[3/7] https://t.co/YLX6O78OJz', 'This results in improved training stability and enables theoretical analysis of the complete algorithm. We discover exciting insights on the introspective training that explain the model’s behavior.\n[4/7]', 'We perform an extensive set of experiments on 2D datasets, image generation and reconstruction, image translation and out-of-distribution detection, demonstrating compelling results.\n[5/7] https://t.co/0jHGyNJbIy', 'In the GitHub repo you can find code tutorials in a Jupyter Notebook format which can run on Google Colab, along with the rest of the code. We hope to see integration of S-IntroVAE in other VAE-based projects.\n[6/7] https://t.co/hDrBUrYlNP', 'You reached the final tweet, so here is a treat – we curated a “Digital Monsters” dataset, composed of ~4000 images of Pokemon, Digimon and Nexomon (it’s a thing), and trained S-IntroVAE. We hope these don’t give you nightmares.\n[7/7] https://t.co/2g5Z3ezShd']",https://arxiv.org/abs/2012.13253,"The recently introduced introspective variational autoencoder (IntroVAE) exhibits outstanding image generations, and allows for amortized inference using an image encoder. The main idea in IntroVAE is to train a VAE adversarially, using the VAE encoder to discriminate between generated and real data samples. However, the original IntroVAE loss function relied on a particular hinge-loss formulation that is very hard to stabilize in practice, and its theoretical convergence analysis ignored important terms in the loss. In this work, we take a step towards better understanding of the IntroVAE model, its practical implementation, and its applications. We propose the Soft-IntroVAE, a modified IntroVAE that replaces the hinge-loss terms with a smooth exponential loss on generated samples. This change significantly improves training stability, and also enables theoretical analysis of the complete algorithm. Interestingly, we show that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribution and an entropy term. We discuss the implications of this result, and demonstrate that it induces competitive image generation and reconstruction. Finally, we describe two applications of Soft-IntroVAE to unsupervised image translation and out-of-distribution detection, and demonstrate compelling results. Code and additional information is available on the project website -- this https URL ","Soft-IntroVAE: Analyzing and Improving the Introspective Variational
  Autoencoder"
25,1342302053993857025,3413370493,Manjari Bagchi ✨ 🌟 💫 📡 🔭 🛰 🌌,"[""Our new work on population synthesis of Galactic normal pulsars. Anirban replaces the 'death-line' of undead stars with a 'death-condition' .. Perhaps the paper was more suitable for the Halloween :D <LINK>""]",https://arxiv.org/abs/2012.13243,"We revisit the population of normal pulsars in the Galactic field in an `evolutionary' approach. By comparing the distributions of various parameters of synthetic pulsars detectable by the Parkes Multibeam Pulsar Survey, the Pulsar Arecibo L-band Feed Array Survey, and two Swinburne Multibeam surveys with those of the real pulsars detected by the same surveys, we find that a good and physically realistic model can be obtained by using a uniform distribution of the braking index in the range of 2.5 to 3.0, a uniform distribution of the cosine of the angle between the spin and the magnetic axis in the range of 0 to 1, a log-normal birth distribution of the surface magnetic field with the mean and the standard deviation as 12.85 and 0.55 respectively while keeping the distributions of other parameters unchanged from the ones most commonly used in the literature. We have also replaced the universal 'death-line' by a `death-condition' specific to each individual pulsar. We find that our model is better than the most popular model. With our improved model, we predict that an all-sky pulsar survey with phase-I SKA-MID will detect about nine thousand normal pulsars in the Galactic field. Among these pulsars, a considerable number will produce continuous gravitational waves in the operational range of the future ground-based gravitational waves detectors like LIGO A$+$, and in certain cases, the spin-down limit of the gravitational wave strains will be well below the detection sensitivity limit. We also provide a fit for the present-day distributions of the spin periods and 1400 MHz luminosities of the whole normal pulsar population in the Galactic field (which are potentially observable) and those can be used in future population studies under the snapshot approach. ",Understanding the Galactic population of normal pulsars: A leap forward
26,1342280309077921792,780129186153426944,Manki,"['Look at our new paper! We crosschecked our previous paper on the EE of topological string theory via a dual Chern-Simons theory computation\n\n<LINK>', ""don't mind the comment that we forgot to delete :)""]",https://arxiv.org/abs/2012.13397,"This is the second in a two-part paper devoted to studying entanglement entropy and edge modes in the A model topological string theory. This theory enjoys a gauge-string (Gopakumar-Vafa) duality which is a topological analogue of AdS/CFT. In part 1, we defined a notion of generalized entropy for the topological closed string theory on the resolved conifold. We provided a canonical interpretation of the generalized entropy in terms of the q-deformed entanglement entropy of the Hartle-Hawking state. We found string edge modes transforming under a quantum group symmetry and interpreted them as entanglement branes. In this work, we provide the dual Chern-Simons gauge theory description. Using Gopakumar-Vafa duality, we map the closed string theory Hartle-Hawking state to a Chern-Simons theory state containing a superposition of Wilson loops. These Wilson loops are dual to closed string worldsheets that determine the partition function of the resolved conifold. We show that the undeformed entanglement entropy due to cutting these Wilson loops reproduces the bulk generalized entropy and therefore captures the entanglement underlying the bulk spacetime. Finally, we show that under the Gopakumar-Vafa duality, the bulk entanglement branes are mapped to a configuration of topological D-branes, and the non-local entanglement boundary condition in the bulk is mapped to a local boundary condition in the gauge theory dual. This suggests that the geometric transition underlying the gauge-string duality may also be responsible for the emergence of entanglement branes. ","Entanglement entropy and edge modes in topological string theory II: The
  dual gauge theory story"
27,1342162683223822337,1196243170188824576,Javier F Acevedo,"['New paper with @josephbramante and Alan Goodman on nuclear fusion inside composite dark matter!\n\n<LINK> <LINK>', 'Large composite dark matter states accelerate nuclei in their interior to large energies, producing ionization, collisional radiation and thermonuclear fusion. Such heavy composite states could be observed at neutrino observatories like IceCube or SNO+', 'We also show that this kind of dark matter can be efficiently captured on Earth and catalyze nuclear reactions in massive white dwarfs, producing Type Ia supernovae.']",https://arxiv.org/abs/2012.10998,"A new dynamic is identified between dark matter and nuclei. Nuclei accelerated to MeV energies by the internal potential of composite dark matter can undergo nuclear fusion. This effect arises in simple models of composite dark matter made of heavy fermions bound by a light scalar field. Cosmologies and detection prospects are explored for composites that catalyze nuclear reactions in underground detectors and stars, including bremsstrahlung radiation from nuclei scattering against electrons in hot plasma formed in the composite interior. If discovered and collected, this kind of composite dark matter could in principle serve as a ready-made, compact nuclear fusion generator. ",Nuclear Fusion Inside Dark Matter
28,1341969087841370113,1974233137,Tushar Sharma,"['New article - ""YALCOM - Yet Another LCOM Metric"" based on our Arxiv paper (Do We Need Improved Code Quality Metrics? - <LINK>). /cc @CoolSWEng  <LINK>']",https://arxiv.org/abs/2012.12324,"The software development community has been using code quality metrics for the last five decades. Despite their wide adoption, code quality metrics have attracted a fair share of criticism. In this paper, first, we carry out a qualitative exploration by surveying software developers to gauge their opinions about current practices and potential gaps with the present set of metrics. We identify deficiencies including lack of soundness, i.e., the ability of a metric to capture a notion accurately as promised by the metric, lack of support for assessing software architecture quality, and insufficient support for assessing software testing and infrastructure. In the second part of the paper, we focus on one specific code quality metric-LCOM as a case study to explore opportunities towards improved metrics. We evaluate existing LCOM algorithms qualitatively and quantitatively to observe how closely they represent the concept of cohesion. In this pursuit, we first create eight diverse cases that any LCOM algorithm must cover and obtain their cohesion levels by a set of experienced developers and consider them as a ground truth. We show that the present set of LCOM algorithms do poorly w.r.t. these cases. To bridge the identified gap, we propose a new approach to compute LCOM and evaluate the new approach with the ground truth. We also show, using a quantitative analysis using more than 90 thousand types belonging to 261 high-quality Java repositories, the present set of methods paint a very inaccurate and misleading picture of class cohesion. We conclude that the current code quality metrics in use suffer from various deficiencies, presenting ample opportunities for the research community to address the gaps. ",Do We Need Improved Code Quality Metrics?
29,1341968643899478019,1974233137,Tushar Sharma,"['New article - ""Issues with Code Quality Metrics"" based on our Arxiv paper (Do We Need Improved Code Quality Metrics? - <LINK>). /cc @CoolSWEng  <LINK>']",https://arxiv.org/abs/2012.12324,"The software development community has been using code quality metrics for the last five decades. Despite their wide adoption, code quality metrics have attracted a fair share of criticism. In this paper, first, we carry out a qualitative exploration by surveying software developers to gauge their opinions about current practices and potential gaps with the present set of metrics. We identify deficiencies including lack of soundness, i.e., the ability of a metric to capture a notion accurately as promised by the metric, lack of support for assessing software architecture quality, and insufficient support for assessing software testing and infrastructure. In the second part of the paper, we focus on one specific code quality metric-LCOM as a case study to explore opportunities towards improved metrics. We evaluate existing LCOM algorithms qualitatively and quantitatively to observe how closely they represent the concept of cohesion. In this pursuit, we first create eight diverse cases that any LCOM algorithm must cover and obtain their cohesion levels by a set of experienced developers and consider them as a ground truth. We show that the present set of LCOM algorithms do poorly w.r.t. these cases. To bridge the identified gap, we propose a new approach to compute LCOM and evaluate the new approach with the ground truth. We also show, using a quantitative analysis using more than 90 thousand types belonging to 261 high-quality Java repositories, the present set of methods paint a very inaccurate and misleading picture of class cohesion. We conclude that the current code quality metrics in use suffer from various deficiencies, presenting ample opportunities for the research community to address the gaps. ",Do We Need Improved Code Quality Metrics?
30,1341948284324069377,1103008101618409472,Liming Jiang,"['Our new work regarding image generation in the frequency domain is now available on arXiv! A generic and complementary loss for diverse generative models.\n\nFocal Frequency Loss for Generative Models\nPaper: <LINK>\nGitHub: <LINK> <LINK>', 'With @doubledaibo @ccloy']",https://arxiv.org/abs/2012.12821,"Image reconstruction and synthesis have witnessed remarkable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthesis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency components that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both perceptual quality and quantitative performance. We further show its potential on StyleGAN2. ",Focal Frequency Loss for Image Reconstruction and Synthesis
31,1341788644760113153,2377407248,Daniel Whiteson,"['New paper:  enhancing jet images *beyond* *the* *physical*  *resolution*\n\n""How to GAN Higher Jet Resolution"" with many friends\n\n<LINK> <LINK>', 'You know thing in TV shows, where they have 3 pixels of a license plate and someone says \n\n    ""Can you enhance that"", \n\nand the tech says \n\n      ""Sure!"" \n\nand then turns it into a crisp image?\n\nNeural networks can kinda do that now.  It\'s called ""super resolution"".', 'It\'s not movie magic. It uses general trends about how high-resolution information has been downgraded to low-resolution to make a statistical guess about what the high-res should look like.\n\nFor a license plate, it means knowing how ""8""s get blurry vs ""B""s.', 'We used it for jets at the LHC, where we are limited by the pixels (calorimeter cells) but would love to be able to ENHANCE our jets.  \n\nSo we did. And it worked! \n\n""HR"" is the high-res truth we want\n""LR"" is the low-res obs we made\n""SR"" is our ENHANCED version of LR https://t.co/V5pbxem33T', 'So by seeing lots of examples, the network learns how jets get blurred and can do a statistical unblurring! https://t.co/om11awmnuC', '@dangaristo Great question!  \n\nTLDR; yes, we think so.\n\nWe tried it on jets from different kinds of processes and found that networks trained on one process could reliably up-sample jets from another.\n\nSo as long as the new physics produces normal quarks/gluons, it should work.']",https://arxiv.org/abs/2012.11944,"QCD-jets at the LHC are described by simple physics principles. We show how super-resolution generative networks can learn the underlying structures and use them to improve the resolution of jet images. We test this approach on massless QCD-jets and on fat top-jets and find that the network reproduces their main features even without training on pure samples. In addition, we show how a slim network architecture can be constructed once we have control of the full network performance. ",How to GAN Higher Jet Resolution
32,1341766513234206720,1215310334,Timo Schick,['🎄 New paper 🎄 Looking for something to do over the Xmas holidays? Check out our preprint (w/@HinrichSchuetze) on text generation with PET. We show that providing short task descriptions greatly improves text generation perf. in few-shot settings: <LINK> #NLProc <LINK>'],https://arxiv.org/abs/2012.11926,"Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GenPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GenPET gives consistent improvements over strong baselines in few-shot settings. ",Few-Shot Text Generation with Pattern-Exploiting Training
33,1341642191672885250,1140222123006472194,Kasper Elm Heintz,"['New paper on @arxiv today! This one is lead by Alex Mannings, examining @NASAHubble images of the host galaxies of #FastRadioBursts — and the images are magnificent! Here shown are residual images, making the spiral structures stand out.\nCheck it out: <LINK> ! <LINK>']",https://arxiv.org/abs/2012.11617,"We present Hubble Space Telescope (HST/WFC3) ultraviolet and infrared observations of eight fast radio burst (FRB) host galaxies with sub-arcsecond localizations, including the hosts of three known repeating FRBs. We quantify their spatial distributions and locations with respect to their host galaxy light distributions, finding that they occur at moderate host normalized-offsets of 1.4 $r_e$ ([0.6,2.1] $r_e$; 68% interval), occur on fainter regions of their hosts in terms of IR light, but overall trace the radial distribution of IR light in their galaxies. The FRBs in our tested distribution do not clearly trace the distributions of any other transient population with known progenitors, and are statistically distinct from the locations of LGRBs, H-poor SLSNe, SGRBs, and Ca-rich transients. We further find that most FRBs are not in regions of elevated local star formation rate and stellar mass surface densities in comparison to the mean global values of their hosts. We also place upper limits to the IR flux at the FRB positions of $m_{\rm IR}\gtrsim\!24.8-27.6$~AB~mag, constraining both satellite and background galaxies to luminosities well below the host luminosity of FRB121102. We find that 5/8 FRB hosts exhibit clear spiral arm features in IR light, and that the positions of all well-localized FRBs located in such hosts are consistent with their spiral arms, although not on their brightest regions. Our results do not strongly support the primary progenitor channel of FRBs being connected either with the most massive (stripped-envelope) stars, or with events which require kicks and long delay times (neutron star mergers). ",A High-Resolution View of Fast Radio Burst Host Environments
34,1341615441697894400,1185977761032110080,Kazumasa Ohno (大野 和正),"[""A new paper posted! Though the paper is still under review, we have studied haze formation on Triton, an ultra-cold moon of Neptune, using microphysical models. We have discussed what physical processes are going on in Triton's cold and hazy atmosphere.\n<LINK>"", 'We developed a bin-scheme microphysical model that can trace size and porosity distributions of haze particles in a self-consistent manner. We tested several possible nature of Triton hazes, namely Titan-like sphere and aggregate haze, as well as the hazes coated by C2H4 ices.', 'In a nutshell, our main conclusion is that condensation of C2H4 ices likely play crucial role to control physical and optical properties of Triton hazes. It is hard to explain existing observations of Triton hazes by Voyager 2 assuming Titan-like haze without condensation.', 'Both sphere and aggregate icy hazes can reasonably explain observations, so it is currently difficult to conclude which is true. As the spectral behavior and scattering properties are different, future mission on outer solar system will help to unveil the morphological nature.', 'In both sphere and aggregate cases, total haze mass flux (tholin+ice) is about an order of magnitude lower than that on Triton. Given predominant icy composition, Titan-like haze production rate on Triton is likely much lower than that on Titan.', 'Our findings are bloadly consistent with the results and expectation of a Pluto (+Triton) haze paper that appeared in recent Nature Astronomy. I hope our works can complement their paper.\nhttps://t.co/rQEbeARDv2', 'The paper is my first solar system paper. The results highlight a deep connection between haze and cloud formation: vapor condensation considerably alter the haze properties, depending on thermal structure. Hopefully, I would like to explore this phenomena in exoplanet context.']",https://arxiv.org/abs/2012.11932,"The largest moon of Neptune, Triton, possess a cold and hazy atmosphere. Since the discovery of near-surface haze layer during the Voyager fly in 1989, the haze formation mechanism has not been investigated in detail. Here, we provide the first haze microphysical model on Triton. Our model solves the evolution of both size and porosity distributions of haze particles in a self-consistent manner. We simulated the formation of sphere and aggregate hazes with and without condensation of the C$_2$H$_4$ ice. The haze particles can grow into fractal aggregates with mass-equivalent sphere sizes of $\sim0.1$--$1~{\rm {\mu}m}$ and fractal dimension of $D_{\rm f} = 1.8$--$2.2$. The ice-free hazes cannot simultaneously explain both UV and visible observations of Voyager 2, while including the condensation of C$_2$H$_4$ ices provides two better solutions. For ice aggregates, the required total haze mass flux is $\sim2\times{10}^{-15}~{\rm g~{cm}^{-2}~s^{-1}}$. For the icy sphere scenario, the column integrated C$_2$H$_4$ production rate is $\sim8\times{10}^{-15}~{\rm g~{cm}^{-2}~s^{-1}}$, and the ice-free mass flux of $\sim6\times{10}^{-17}~{\rm g~{cm}^{-2}~s^{-1}}$. The UV occultation observations at short wavelength $<0.15~{\rm {\mu}m}$ may slightly favor the icy aggregates. Observations of the haze optical depth and the degree of forward scattering in UV and visible should be able to distinguish whether Triton's hazes are icy spheres or ice aggregates in future Triton missions. ",Haze Formation on Triton
35,1341588625708892163,1286105864114339841,Venta Terauds,"['New paper with Jeremy Sumner @jezlurch, finally at the preprint stage! We construct a new algebraic framework for modelling genome rearrangements: a ""genome algebra"" that incorporates the genomes\' physical symmetries into each element. \n<LINK>']",http://arxiv.org/abs/2012.11665,"We present a unified framework for modelling genomes and their rearrangements in a genome algebra, as elements that simultaneously incorporate all physical symmetries. Building on previous work utilising the group algebra of the symmetric group, we explicitly construct the genome algebra for the case of unsigned circular genomes with dihedral symmetry and show that the maximum likelihood estimate (MLE) of genome rearrangement distance can be validly and more efficiently performed in this setting. We then construct the genome algebra for the general case, that is, for genomes represented by elements of an arbitrary group and symmetry group, and show that the MLE computations can be performed entirely within this framework. There is no prescribed model in this framework; that is, it allows any choice of rearrangements with arbitrary weights. Further, since the likelihood function is built from path probabilities -- a generalisation of path counts -- the framework may be utilised for any distance measure that is based on path probabilities. ",A new algebraic approach to genome rearrangement models
36,1341463012755054593,1002538331039846400,Nikolaos Syrrakos,['New paper on arXiv today! Interested in LHC physics? Check it out: <LINK>'],https://arxiv.org/abs/2012.10635,"We analytically calculate one-loop five-point Master Integrals, \textit{pentagon integrals}, with up to one off-shell leg to arbitrary order in the dimensional regulator in $d=4-2\epsilon$ space-time dimensions. A pure basis of Master Integrals is constructed for the pentagon family with one off-shell leg, satisfying a single-variable canonical differential equation in the Simplified Differential Equations approach. The relevant boundary terms are given in closed form, including a hypergeometric function which can be expanded to arbitrary order in the dimensional regulator using the \texttt{Mathematica} package \texttt{HypExp}. Thus one can obtain solutions of the canonical differential equation in terms of Goncharov Polylogartihms of arbitrary transcendental weight. As a special limit of the one-mass pentagon family, we obtain a fully analytic result for the massless pentagon family in terms of pure and universally transcendental functions. For both families we provide explicit solutions in terms of Goncharov Polylogartihms up to weight four. ",Pentagon integrals to arbitrary order in the dimensional regulator
37,1341443451422986241,1292966965288546304,Joshua Agterberg,"['I\'m happy to say that my new paper (with Minh Tang and Carey Priebe) entitled ""Nonparametric Two-Sample Hypothesis Testing for Random Graphs with Negative and Repeated Eigenvalues"" was posted on @arxiv last week: <LINK>  1/n', 'We tackle the problem of determining whether two random graphs on a different number of vertices have the same distribution assuming only that their edge probability generating matrices are low-rank.  2/n', 'The test can be viewed as the network analogue of the classical nonparametric test of equality of distribution.  We use the maximum mean discrepancy applied to the rows of rotated scaled eigenvectors of the adjacency matrices (the adjacency spectral embeddings) 3/n', 'In particular, if the edge probability matrices have both negative and arbitrarily close eigenvalues (meaning there is no asymptotic eigengap), we show that one need only worry about the arbitrarily close eigenvalues.  4/n', 'It turns out that close eigenvalues corresponds to a block-orthogonal matrix, and we propose an optimal-transport based algorithm to estimate this (block) orthogonal matrix.  5/n', 'We provide finite-sample guarantees and almost sure convergence in a natural asymptotic regime, and we study the performance under various degrees of sparsity on the graphs.  6/n', 'This paper showcases the subtle interplay between different types of geometry in the latent space for random graphs, and it provides a practical test statistic to determine if two graphs both come from e.g. a stochastic blockmodel.', 'I should also mention that this test allows for any low-rank model, including your favorite stochastic blockmodel variants (e.g. degree-corrected SBM, mixed-membership SBM)\n\n#NetworkAnalysis  #Statistics #DataScience #MachineLearning #Probability']",https://arxiv.org/abs/2012.09828,"We propose a nonparametric two-sample test statistic for low-rank, conditionally independent edge random graphs whose edge probability matrices have negative eigenvalues and arbitrarily close eigenvalues. Our proposed test statistic involves using the maximum mean discrepancy applied to suitably rotated rows of a graph embedding, where the rotation is estimated using optimal transport. We show that our test statistic, appropriately scaled, is consistent for sufficiently dense graphs, and we study its convergence under different sparsity regimes. In addition, we provide empirical evidence suggesting that our novel alignment procedure can perform better than the na\""ive alignment in practice, where the na\""ive alignment assumes an eigengap. ","Nonparametric Two-Sample Hypothesis Testing for Random Graphs with
  Negative and Repeated Eigenvalues"
38,1341365348403179520,17629607,Andrei Bursuc,"['New work spearheaded by S. Gidaris on self-supervised learning: OBoW - Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning\n\nPaper: <LINK> \nCode: <LINK> \n🧵👇\n1/N <LINK>', 'Self-supervised methods have progressed significantly, in particular due to contrastive methods narrowing the gap with supervised methods. Contrastive methods focus on learning embeddings invariant to intra-image variations while being discriminative w.r.t. different images. 2/N', 'However they care less about contextual reasoning, where reconstruction approaches are more suitable. Early context-encoders focused too much on non-essential details, while feature reconstruction methods (e.g. BoWNet, BYOL) have shown encouraging results 3/N', 'With OBoW we design an online teacher-student scheme, revisiting BoWNet. The student reconstructs Bag-of-Words (BoW) vectors computed from teacher’s features. BoW captures multiple local visual concepts, i.e. a richer target representation (local classification in disguise). 4/N https://t.co/UZWJLGxp8T', 'The online setting allows the teacher to be continuously updated via momentum-based updates as the student progresses. This is nice but poses a few challenges as the vocabulary for building BoW is also on the move now. We propose three effective solutions to this effect: 5/N', '(i) A queue-based vocabulary where the oldest entry is replaced by a randomly sampled local feature from the current batch. (ii) BoW targets computed with soft-assignments instead of hard-assignments. 6/N https://t.co/o502yX5aLC', '(iii) A dynamic BoW-prediction head that can adapt to the evolving vocabulary by computing weights for each visual word before mapping the student features to the corresponding BoW vector. You may recall this from few-shot learning for generating weights of novel classes. 7/N https://t.co/DdGT161s4T', 'We evaluate OBoW extensively on ImageNet, Places205, VOC07 for classification and on VOC07+12 for detection. OBoW delivers strong results that surpass prior state-of-the-art approaches on most evaluation protocols. 8/N https://t.co/b923JGjGlo', 'We argue that BoW targets expose fewer learning shortcuts and prevent teacher-student collapse and overfitting: reducing momentum does not collapse if we use a lower learning rate, performance degrades gracefully with simple data augmentation strategies. 9/N https://t.co/OaY2KxtJ1D', 'We release our implementation code and pre-trained models https://t.co/Z3cYeHcSly\nLet us know your questions or comments 10/N=10', '@ducha_aiki AFAIR, FROST has been withdrawn because of a coding error that impacted results', '@ducha_aiki Otherwise, the protocols are different. In self-supervised we pre-train in self-supervised manner unlabeled ImageNet images and then we fine-tune over 1-10% labeled images comparing against a supervised model trained from scratch on the same labeled images. Naming could be better', '@ducha_aiki I forgot to add the link https://t.co/PmnP85uZ3Q', '@ducha_aiki Yeah, I can relate to that :) We have several tables and results (6 tables, 4 figures), some with very detailed captions, and varying protocols. Ultimately we compromised over some of them to fit space.']",https://arxiv.org/abs/2012.11552,"Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited. With this in mind, we propose a teacher-student scheme to learn representations by training a convolutional net to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. We provide the implementation code at this https URL ",OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning
39,1341288063079751680,1191024791869894656,Matthew J. Smith,"['New Paper! <LINK>  Tutorial: Introduction to computational causal inference using reproducible Stata, R and Python code @WATZILEI @camaringe @PausalZ @CausalInferBot @epi_twit @statstwitbot #epitwitter #tmle #stats #causalinference <LINK>']",https://arxiv.org/abs/2012.09920,"The purpose of many health studies is to estimate the effect of an exposure on an outcome. It is not always ethical to assign an exposure to individuals in randomised controlled trials, instead observational data and appropriate study design must be used. There are major challenges with observational studies, one of which is confounding that can lead to biased estimates of the causal effects. Controlling for confounding is commonly performed by simple adjustment for measured confounders; although, often this is not enough. Recent advances in the field of causal inference have dealt with confounding by building on classical standardisation methods. However, these recent advances have progressed quickly with a relative paucity of computational-oriented applied tutorials contributing to some confusion in the use of these methods among applied researchers. In this tutorial, we show the computational implementation of different causal inference estimators from a historical perspective where different estimators were developed to overcome the limitations of the previous one. Furthermore, we also briefly introduce the potential outcomes framework, illustrate the use of different methods using an illustration from the health care setting, and most importantly, we provide reproducible and commented code in Stata, R and Python for researchers to apply in their own observational study. The code can be accessed at this https URL ","Tutorial: Introduction to computational causal inference using
  reproducible Stata, R and Python code"
40,1341002203008528385,916407589,Anders Kvellestad,"['In high-dimensional parameter space no one can hear you scream... \n\nSo, to avoid any unfortunate surprises when poking around in theory space for interesting new physics, you probably want to follow the simple recommendations in our new paper: <LINK> <LINK>', '@pstoecker Indeed!']",https://arxiv.org/abs/2012.09874,"Physical theories that depend on many parameters or are tested against data from many different experiments pose unique challenges to statistical inference. Many models in particle physics, astrophysics and cosmology fall into one or both of these categories. These issues are often sidestepped with statistically unsound ad hoc methods, involving intersection of parameter intervals estimated by multiple experiments, and random or grid sampling of model parameters. Whilst these methods are easy to apply, they exhibit pathologies even in low-dimensional parameter spaces, and quickly become problematic to use and interpret in higher dimensions. In this article we give clear guidance for going beyond these procedures, suggesting where possible simple methods for performing statistically sound inference, and recommendations of readily-available software tools and standards that can assist in doing so. Our aim is to provide any physicists lacking comprehensive statistical training with recommendations for reaching correct scientific conclusions, with only a modest increase in analysis burden. Our examples can be reproduced with the code publicly available at this https URL ","Simple and statistically sound recommendations for analysing physical
  theories"
41,1340885092160323586,22399655,Ryota Kanai💡,"['A new opinion paper on deep learning and global workspace theory with Rufin VanRullen. In this paper, we proposed a  possible way to implement the global workspace by interpreting it as a shared latent space.  \n<LINK>']",https://arxiv.org/abs/2012.10390,"Recent advances in deep learning have allowed Artificial Intelligence (AI) to reach near human-level performance in many sensory, perceptual, linguistic or cognitive tasks. There is a growing need, however, for novel, brain-inspired cognitive architectures. The Global Workspace theory refers to a large-scale system integrating and distributing information among networks of specialized modules to create higher-level forms of cognition and awareness. We argue that the time is ripe to consider explicit implementations of this theory using deep learning techniques. We propose a roadmap based on unsupervised neural translation between multiple latent spaces (neural networks trained for distinct tasks, on distinct sensory inputs and/or modalities) to create a unique, amodal global latent workspace (GLW). Potential functional advantages of GLW are reviewed, along with neuroscientific implications. ",Deep Learning and the Global Workspace Theory
42,1339979635359182850,2423179856,Edward Raff,"['Used MalConv and annoyed with the huge training cost? Looked at it with disdain for being unable to learn feature interactions? With @willcfleshman @rjzak @drhyrum &amp; @filar we have what you need: A new @RealAAAI #AAAI2021 paper📝<LINK>\n🧑\u200d💻 <LINK> <LINK>', ""MalConv up to 25x faster and over 100x+ more memory efficient with a new chunked approach to temporal max pooling. Max pooling's gradient is sparse, but no one ever exploits it because their problems are too small. With this we can now train on files with T=100,000,000+ steps! https://t.co/FFtxMs0i8W"", 'We want feature interactions, but 100M+ steps is still too much for transformers. So we develop a new attention based gating mechanism ""Global Channel Gating"" (GCG) that allows learning interactions over 100M+ steps! https://t.co/NXkJxQFOy8', 'Using GCG we have MalConv2, which has a feature extraction and context extraction sub-networks, which interact through GCG to modulate parts of the input based on other content. https://t.co/OCMzlyeqyh', 'All of this work isn\'t getting us to domain knowledge levels yet, but a nice improvement work learning from raw bytes! We are also circumventing a trivial attack, we can process the _entire_ file even if its hundreds of MB. No trivial ""just append to the end"" evasion. https://t.co/8Zcsw4kBKC', 'As mentioned, also way fafster! We are processing more data in less time and less RAM! The huge reductions is what allows us to re-invest those savings into MalConv2.0 https://t.co/RxL6pio2qc', 'Impossible without @willcfleshman in particular, we can also look at how the GCG attention impacts what is/is-not used to make decisions. The results are pretty good, and seems to learn kind of intuitive logic an analyst might use. Hard to scale these kinds of studies up though. https://t.co/5AnA6MBbuY', 'Happy to get this work out there, and would not have been possible without so many people. Especially all the wonderful people who have used and attacked MalConv! Hearing about the successes and troubles in using it drove a lot of the thought behind this work.', ""If MalConv2 is useful in anyway to you, please drop a line and let us know! Especially with this pandemic thing, my normal feedback network of conferences doesn't work as well as normal 🙃""]",https://arxiv.org/abs/2012.09390,"Recent works within machine learning have been tackling inputs of ever-increasing size, with cybersecurity presenting sequence classification problems of particularly extreme lengths. In the case of Windows executable malware detection, inputs may exceed $100$ MB, which corresponds to a time series with $T=100,000,000$ steps. To date, the closest approach to handling such a task is MalConv, a convolutional neural network capable of processing up to $T=2,000,000$ steps. The $\mathcal{O}(T)$ memory of CNNs has prevented further application of CNNs to malware. In this work, we develop a new approach to temporal max pooling that makes the required memory invariant to the sequence length $T$. This makes MalConv $116\times$ more memory efficient, and up to $25.8\times$ faster to train on its original dataset, while removing the input length restrictions to MalConv. We re-invest these gains into improving the MalConv architecture by developing a new Global Channel Gating design, giving us an attention mechanism capable of learning feature interactions across 100 million time steps in an efficient manner, a capability lacked by the original MalConv CNN. Our implementation can be found at this https URL ","Classifying Sequences of Extreme Length with Constant Memory Applied to
  Malware Detection"
43,1339961797747630080,1299830240823455745,Matteo Agostini,['Did you know that that we can search for light exotic fermions with double-beta decays experiments? Check out our new paper: <LINK> #neutrinos #physics  @LEGEND_Science  @UCLHEP <LINK>'],http://arxiv.org/abs/2012.09281,"The Standard Model of Particle Physics predicts the double-$\beta$ decay of certain nuclei with the emission of two active neutrinos. In this letter, we argue that double-$\beta$ decay experiments could be used to probe models with light exotic fermions through the search for spectral distortions in the electron spectrum with respect to the Standard Model expectations. We consider two concrete examples: models with light sterile neutrinos, singly produced in the double-$\beta$ decay, and models with a light $Z_2$-odd fermion, pair produced due to a $Z_2$ symmetry. We estimate the discovery potential of a selection of double-$\beta$ decay experiments and find that future searches will test for the first time a new part of the parameter space of interest at the MeV-mass scale. ",Search for Light Exotic Fermions in Double-Beta Decays
44,1339954853175541762,836333446872051713,Mikhail Tikhonov,"['First paper from the new group! ""Improve it or lose it: evolvability costs of competition for expression"". Thanks to Jake and Devon for their hard work! <LINK>']",https://arxiv.org/abs/2012.09325,"Expression level is known to be a strong determinant of a protein's rate of evolution. But the converse can also be true: evolutionary dynamics can affect expression levels of proteins. Having implications in both directions fosters the possibility of a feedback loop, where higher expressed systems are more likely to improve and be expressed even higher, while those that are expressed less are eventually lost to drift. Using a minimal model to study this in the context of a changing environment, we demonstrate that one unexpected consequence of such a feedback loop is that a slow switch to a new environment can allow genotypes to reach higher fitness sooner than a direct exposure to it. ",Improve it or lose it: evolvability costs of competition for expression
45,1339934552261865475,981128363279572992,Victor Chernozhukov #peace 🇺🇦,"['<LINK>\n\nWith the amazing Denis Chetverikov and Yuta Koike, we  \njust finished up a new paper that derives a high-dimensional CLT with optimal dependence  on log d (log of dimension), envelope B, and (up to log n) optimal dependence on n (sample size). <LINK>']",https://arxiv.org/abs/2012.09513,"In this paper, we derive new, nearly optimal bounds for the Gaussian approximation to scaled averages of $n$ independent high-dimensional centered random vectors $X_1,\dots,X_n$ over the class of rectangles in the case when the covariance matrix of the scaled average is non-degenerate. In the case of bounded $X_i$'s, the implied bound for the Kolmogorov distance between the distribution of the scaled average and the Gaussian vector takes the form $$C (B^2_n \log^3 d/n)^{1/2} \log n,$$ where $d$ is the dimension of the vectors and $B_n$ is a uniform envelope constant on components of $X_i$'s. This bound is sharp in terms of $d$ and $B_n$, and is nearly (up to $\log n$) sharp in terms of the sample size $n$. In addition, we show that similar bounds hold for the multiplier and empirical bootstrap approximations. Moreover, we establish bounds that allow for unbounded $X_i$'s, formulated solely in terms of moments of $X_i$'s. Finally, we demonstrate that the bounds can be further improved in some special smooth and zero-skewness cases. ","Nearly optimal central limit theorem and bootstrap approximations in
  high dimensions"
46,1339904650183634944,4866589137,Dr. Nathan Adams,"['It’s Christmas come early! A new paper is up today, led by South African PhD student Eliab Malefahlo and featuring the combination of my photo-z catalogues and VLA data to probe the radio luminosity function below the detection threshold. <LINK>']",https://arxiv.org/abs/2012.09797,"We present the 1.4GHz radio luminosity functions (RLFs) of galaxies in the COSMOS field, measured above and below the $5\sigma$ detection threshold, using a Bayesian model-fitting technique. The radio flux-densities from VLA-COSMOS 3-GHz data, are extracted at the position of stellar mass-limited near-infrared (NIR) galaxies. We fit a local RLF model, which is a combination of active galactic nuclei (AGN) and star-forming galaxy (SFG), in 10 redshift bins with a pure luminosity evolution (PLE) model. We show that the evolution strength is similar to literature values up to $z\sim 1.6$. Beyond $z\sim 2$, we find that the SFG RLF exhibits a negative evolution ($L^*$ moves to lower luminosities) due to the decrease in low stellar-mass sources in our stellar mass-limited sample at high redshifts. From the RLF for SFGs, we determine the evolution in the cosmic star-formation-rate density (SFRD), which we find to be consistent with the established behaviour up to $z\sim 1$. Beyond $z\sim 1$ cosmic SFRD declines if one assumes an evolving infrared--radio correlation (IRRC), whereas it stays relatively higher if one adopts a constant IRRC. We find that the form of the relation between radio luminosity and SFR is therefore crucial in measuring the cosmic SFRD from radio data. We investigate the effects of stellar mass on the total RLF by splitting our sample into low ($10^{8.5} \leq M/\mathrm{M}_{\odot} \leq 10^{10}$) and high ($M>10^{10}\,\mathrm{M}_{\odot}$) stellar-mass subsets. We find that the SFRD is dominated by sources in the high stellar masses bin, at all redshifts. ","A deep radio view of the evolution of the cosmic star-formation rate
  density from a stellar-mass selected sample in VLA-COSMOS"
47,1339839789399465984,125145779,Noam Libeskind,"['Gaia eDR3 says that the LG still doesn’t have that much angular momentum. \n\n<LINK>\n\nNew paper by Salomon et al with @benfamaey <LINK>', 'Deleted my other tweet because of a typo in the link!']",https://arxiv.org/abs/2012.09204,"We present an analysis of the proper motion of the Andromeda galaxy (M31), based on the Early Third Data Release of the Gaia mission. We use the Gaia photometry to select young blue main sequence stars, and apply several quality cuts to obtain clean samples of these tracers. After correcting the proper motion measurements for the internal rotation of the M31 disk motion, we derive an apparent motion of 52.5 +/- 5.8 muas/yr with respect to the Gaia reference frame, or 61.9 +/- 9.7 muas/yr after applying a zero-point correction determined from quasars within 20 degrees from M31 and a correction from systemic biases. Accounting for the Solar reflex motion we deduce a relative velocity between Andromeda and the Milky way (in a non-rotating frame at the current location of the Sun) of 42.2 +/- 39.3 km/s along right ascension (40.0 +/- 39.3 km/s along galactic longitude) and -59.4 +/- 30.3 km/s along declination (-60.9 +/- 30.3 km/s along galactic latitude), with a total transverse velocity of V_trans = 82.4 +/- 31.2 km/s. These values are consistent with (but more accurate than) earlier Hubble Space Telescope measurements that predict a future merger between the two galaxies. We also note a surprisingly large difference in the derived proper motion between the blue stars in M31 and samples of red stars that appear to lie in that galaxy. We propose several hypotheses to explain the discrepancy but found no clear evidence with the current data to privilege any one of them. ","The proper motion of Andromeda from Gaia eDR3: confirming a nearly
  radial orbit"
48,1339787095603376130,590175311,Benedict Guttman-Kenney,"['👋New paper on uneven🇬🇧 regional recovery using @fable_data\n\nPaper <LINK>\n\nBlog @EconObservatory <LINK>\n\nThread⬇️\n\n@johngathergood @ChicagoBooth @UKRI_News @UniofNottingham @WarwickBSchool @econromesh @FT @ChrisGiles_ @valentinaromei #econtwitter <LINK>', 'This is our second paper using @fable_data : a new source of real-time European consumption data. #householdfinance\n\nResearch is with a fantastic team: @johngathergood, Fabian Gunzinger, Edika Quispe-Torreblanca + Neil Stewart. We didn’t plan to write more than a blog but... 2/n', '...to put it plainly, we just couldn’t stop. To try and put it more poetically, it feels like we were standing on the edge and had a choice: to turn and go back or to travel further into these data. We chose to wander deeper in (to paraphrase a more productive 31 year old) 3/n https://t.co/t5I5SZEzs0', 'We show Fable to be a highly correlated, leading indicator of ONS and Bank of England data. Its transaction-level nature enables us to disaggregate this national picture across spending types and regions... 4/n https://t.co/7MfuixHVeO', '...We show the national recovery has been heavily driven by online spending. Offline spending contracted further in November during the second national lockdown... 5/n https://t.co/1GQssN0g2r', ""...UK’s recovery is heavily weighted towards the “home counties” around outer London and the South.\nAgain here there's a stark contrast: strong online spending growth while offline spending contracts... 6/n https://t.co/IEj2uXq9A9"", '...The strongest recovery in spending is seen in the ""commuter belt"" in + around outer London, popular domestic tourist destinations, and also in areas of high second home ownership... 7/n https://t.co/ZEyVtTs1pW', '...Localities facing the UK\'s new tighter ""Tier 3"" restrictions at the start of December (mostly the midlands and northern areas) had 38.4% lower spending than areas facing the less restrictive ""Tier 2"" (mostly southern areas)... 8/n https://t.co/s0J2s5G7bs', ""Such growing regional inequalities are a challenge to the UK government's aim to 'level-up' UK regions that historically had lower productivity + living standards (especially outside London and the South East). How can policymakers address growing regional inequalities? 9/n"", 'To prevent such COVID-19-driven regional inequalities from becoming persistent we propose governments introduce temporary, regionally-targeted interventions in 2021 once virus outbreaks are under control + vaccinations more broadly rolled out...\n\n 10/n', 'By utilizing real-time, regional data policymakers can efficiently decide when, where and how to implement such regional interventions and be able to rapidly evaluate their effectiveness to consider whether to expand, modify or remove them. 11/n', 'What kinds of regional measures? We list some ideas in the paper (e.g. business rate relief, VAT cuts, spending vouchers, transfers to local governments). But we hope our paper prompts public discussion on what measures could be implemented and their relative merits. 12/12', ""p.s.  if you missed it before - here's our first paper evaluating the first set of UK local lockdowns on consumer spending and covid cases https://t.co/I9NsJowKl7"", '@threadreaderapp unroll', '@threader_app please compile']",https://arxiv.org/abs/2012.09336,"We show the recovery in consumer spending in the United Kingdom through the second half of 2020 is unevenly distributed across regions. We utilise Fable Data: a real-time source of consumption data that is a highly correlated, leading indicator of Bank of England and Office for National Statistics data. The UK's recovery is heavily weighted towards the ""home counties"" around outer London and the South. We observe a stark contrast between strong online spending growth while offline spending contracts. The strongest recovery in spending is seen in online spending in the ""commuter belt"" areas in outer London and the surrounding localities and also in areas of high second home ownership, where working from home (including working from second homes) has significantly displaced the location of spending. Year-on-year spending growth in November 2020 in localities facing the UK's new tighter ""Tier 3"" restrictions (mostly the midlands and northern areas) was 38.4% lower compared with areas facing the less restrictive ""Tier 2"" (mostly London and the South). These patterns had been further exacerbated during November 2020 when a second national lockdown was imposed. To prevent such COVID-19-driven regional inequalities from becoming persistent we propose governments introduce temporary, regionally-targeted interventions in 2021. The availability of real-time, regional data enables policymakers to efficiently decide when, where and how to implement such regional interventions and to be able to rapidly evaluate their effectiveness to consider whether to expand, modify or remove them. ","Levelling Down and the COVID-19 Lockdowns: Uneven Regional Recovery in
  UK Consumer Spending"
49,1339752653681610753,1920417332,Ryan Glasser,['Our new paper on the feasibility of #MachineLearning in experimental #quantum state reconstruction is on the arxiv!  Fun using  #ibmq quantum computer!\n\n<LINK>\n\n@slohani_ai @ProfTSearles @ArmyResearchLab @USArmy @Tulane @TulaneSSE @HowardUniv @IBMResearch @qiskit <LINK>'],https://arxiv.org/abs/2012.09432,"We determine the resource scaling of machine learning-based quantum state reconstruction methods, in terms of inference and training, for systems of up to four qubits when constrained to pure states. Further, we examine system performance in the low-count regime, likely to be encountered in the tomography of high-dimensional systems. Finally, we implement our quantum state reconstruction method on an IBM Q quantum computer, and compare against both unconstrained and constrained MLE state reconstruction. ","On the experimental feasibility of quantum state reconstruction via
  machine learning"
50,1339751364465975299,1290487197645201409,Jake Moran,"['Excited to announce my first first-author paper! We show in a minimal model that slow exposure to a new environment can allow genotypes to reach higher fitness than direct exposure to it, stemming from a competition for expression.\n<LINK>', '@KevinWoodUM Thank you!']",https://arxiv.org/abs/2012.09325,"Expression level is known to be a strong determinant of a protein's rate of evolution. But the converse can also be true: evolutionary dynamics can affect expression levels of proteins. Having implications in both directions fosters the possibility of a feedback loop, where higher expressed systems are more likely to improve and be expressed even higher, while those that are expressed less are eventually lost to drift. Using a minimal model to study this in the context of a changing environment, we demonstrate that one unexpected consequence of such a feedback loop is that a slow switch to a new environment can allow genotypes to reach higher fitness sooner than a direct exposure to it. ",Improve it or lose it: evolvability costs of competition for expression
51,1339751231724482560,1053730346930384897,Jeff Carlin,"['New MADCASH paper! <LINK> In this one, we present Hubble Space Telescope observations of the first two dwarf galaxies found in MADCASH data. Both are dwarf spheroidals at the right distances to be associated with their ~LMC-mass hosts. 1/8', 'The properties of these dwarfs (sizes, metallicities, etc.) are fairly typical of dSphs at similar luminosities. We could measure their properties because these data are beautiful! Check out this comparison of ground-based (top) vs. HST (bottom) data: 2/8 https://t.co/wdx4ct6d1t', '“MADCASH-2” is unusual: most of its stars are ancient and metal-poor, but ~10% of its stars formed in the past ~1.5 Gyr (~1% in the past 500 Myr), even though it doesn’t seem to have any gas remaining. 3/8 https://t.co/i5IwaZNs0Y', 'This suggests that its host (NGC 4214) may be responsible for the recent rejuvenation of star formation in MADCASH-2 and its quenching. 4/8', 'Because they are less massive, and they aren’t expected to have significant CGM, we might expect low-mass hosts to have less effect on their satellites than more massive (Milky Way-like) galaxies. 5/8', 'But we’re seeing evidence of quenching among dwarf satellites of low-mass hosts, suggesting that environmental processing may be more important than we thought for satellites of LMC-mass hosts. 6/8', ""With Rubin Observatory/LSST, we’ll likely find many more dwarfs similar to we're uncovering with MADCASH. This “precursor LSST science” is important to develop tools for analysis and interpretation so we’re ready for the massive data set that will come! 7/8"", 'Hope you enjoy the paper! Thanks to @bmutlupakdil, @ProfAnnikaPeter, @denija83, @eteq, @sand_dave, @jrh_astro, @caprastro, and all the other non-Twitter contributors! 8/8']",https://arxiv.org/abs/2012.09174,"We present a deep Hubble Space Telescope (HST) imaging study of two dwarf galaxies in the halos of Local Volume Large Magellanic Cloud (LMC) analogs. These dwarfs were discovered as part of our Subaru+Hyper Suprime-Cam MADCASH survey: MADCASH-1, which is a satellite of NGC 2403 (D~3.2 Mpc), and MADCASH-2, a previously unknown dwarf galaxy near NGC 4214 (D~3.0 Mpc). Our HST data reach >3.5 mag below the tip of the red giant branch (TRGB) of each dwarf, allowing us to derive their structural parameters and assess their stellar populations. We measure TRGB distances ($D=3.41^{+0.24}_{-0.23}$ Mpc for MADCASH-1, and $D=3.00^{+0.13}_{-0.15}$ Mpc for MADCASH-2), and confirm their associations with their host galaxies. MADCASH-1 is a predominantly old, metal-poor stellar system (age ~13.5 Gyr, [M/H] ~ -2.0), similar to many Local Group dwarfs. Modelling of MADCASH-2's CMD suggests that it contains mostly ancient, metal-poor stars (age ~13.5 Gyr, [M/H] ~ -2.0), but that ~10% of its stellar mass was formed 1.1--1.5 Gyr ago, and ~1% was formed 400--500 Myr ago. Given its recent star formation, we search MADCASH-2 for neutral hydrogen using the Green Bank Telescope, but find no emission and estimate an upper limit on the HI mass of $<4.8\times10^4 M_{\odot}$. These are the faintest dwarf satellites known around host galaxies of LMC mass outside the Local Group ($M_{V,\text{MADCASH-1}}=-7.81\pm0.18$, $M_{V,\text{MADCASH-2}}=-9.15\pm0.12$), and one of them shows signs of recent environmental quenching by its host. Once the MADCASH survey for faint dwarf satellites is complete, our census will enable us to test CDM predictions for hierarchical structure formation, and discover the physical mechanisms by which low-mass hosts influence the evolution of their satellites. ","Hubble Space Telescope Observations of Two Faint Dwarf Satellites of
  Nearby LMC Analogs from MADCASH"
52,1339672597764730880,1075406968087592960,Ji Hou,"['Sharing our new work Contrastive Scene Contexts, a new pre-training method for data-efficient learning in 3D. \n\nNew ScanNet benchmark coming up soon!\n\n<LINK>\nProject: <LINK>\nPaper: <LINK>\n\n(w/ BenGraham @MattNiessner @sainingxie)']",http://arxiv.org/abs/2012.09165,"The rapid progress in 3D scene understanding has come with growing demand for data; however, collecting and annotating 3D scenes (e.g. point clouds) are notoriously hard. For example, the number of scenes (e.g. indoor rooms) that can be accessed and scanned might be limited; even given sufficient data, acquiring 3D labels (e.g. instance masks) requires intensive human labor. In this paper, we explore data-efficient learning for 3D point cloud. As a first step towards this direction, we propose Contrastive Scene Contexts, a 3D pre-training method that makes use of both point-level correspondences and spatial contexts in a scene. Our method achieves state-of-the-art results on a suite of benchmarks where training data or labels are scarce. Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmentation) of the baseline performance that uses full annotations. ","Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene
  Contexts"
53,1339640488400400385,53836928,Tarun Chitra,"['🚨⚠️ Paper Alert ⚠️🚨\n\nQ: Have you wondered about math for the following?\n\na) Optimal token qty to emit for yield farming incentives\nb) Hedging impermanent loss w/ options\nc) When do LPs not get rekt?\n\nA: New paper from moi, @alexhevans, @GuilleAngeris \n\n<LINK>', 'We highlight how to relate hedging costs and optimal yield farming in our final blog post\n\n1. Greeks (∆, Γ, not @gakonst) are bounded by the curvature of the impact function\n\n2. Curvature of CFMM controls how much you need to pay to avoid 🧛 attacks\n\nhttps://t.co/WI9fbvuARv', 'This gives a quantitative answer to ""how much does @SushiSwap need to pay in $SUSHI to convince @UniswapProtocol liquidity for a particular pool to migrate?""\n\nThis cost varies from pair to pair and effectively decays to zero when you have a really gigantic pool https://t.co/ePovkIMqwD', 'Exercise for the reader (really @MartinTassy and @_Dave__White_): \n\nWe generalize the profit conditions for Uniswap to arbitrary CFMMs. Find the proper renormalization (e.g. variance rescaling) to get a similar Kelly-style, continuous limit result for @CurveFinance https://t.co/MNbrk3hPMJ', 'This fact lets you adjust a CFMM\'s curvature in order to change the interval for LP profitability\n\nAlmost all ""limit order"" approximations in CFMMs that I\'ve seen effectively end up being curvature adjustments to adjust the conditions in this tweet 👇🏾\n\nhttps://t.co/AQpS3JRLRZ', 'Which brings me to my final challenge:\n\nThe @danrobinson (CFMMs) v. @SBF_Alameda (CLOBs) fight would be a infinitely more compelling if you compared\n\n💣 continuous time, discrete price LOB \n🧨 discrete time, continuous price CFMMs https://t.co/LMFujC3jkY', 'For LOBs: Market makers adjust liquidity by adding/removing *discrete* price/quantity orders\n\nFor CFMMs: MMs adjust liquidity by adjusting curvature (e.g. bounds for where they want liquidity to be used, adding/removing liquidity)\n\nAll to replicate the famous Glosten drawing 👇🏾 https://t.co/wVRHwkhqxj', '👏🏾👏🏾👏🏾 for feedback/comments over the last 6mo:\n@ciamac @theyisun @adamlerer @ChiangRei @teo_leibowitz + some folks not on Twitter', '@CryptoCobain @alexhevans @GuilleAngeris Cobie, your brain is nuts, you never sleep and suck in heaps of information to form some dank, deep tweets \n\nI salute you https://t.co/GotXV6NQVE']",https://arxiv.org/abs/2012.08040,"Liquidity and trading activity on constant function market makers (CFMMs) such as Uniswap, Curve, and Balancer has grown significantly in the second half of 2020. Much of the growth of these protocols has been driven by incentivized pools or 'yield farming', which reward participants in crypto assets for providing liquidity to CFMMs. As a result, CFMMs and associated protocols, which were historically very small markets, now constitute the most liquid trading venues for a large number of crypto assets. But what does it mean for a CFMM to be the most liquid market? In this paper, we propose a basic definition of price sensitivity and liquidity. We show that this definition is tightly related to the curvature of a CFMM's trading function and can be used to explain a number of heuristic results. For example, we show that low-curvature markets are good for coins whose market value is approximately fixed and that high-curvature markets are better for liquidity providers when traders have an informational edge. Additionally, the results can also be used to model interacting markets and explain the rise of incentivized liquidity provision, also known as 'yield farming.' ",When does the tail wag the dog? Curvature and market making
54,1339627688554852352,2331035372,Yuan Luo,['New paper accepted by #AAAI2021 - PANTHER: Pathway Augmented Nonnegative Tensor factorization for\nHighER-order feature learning. PANTHER builds more accurate and better interpretable ML models for genetic medicine.\nPaper: <LINK>. Code: <LINK>. <LINK>'],https://arxiv.org/abs/2012.08580,"Genetic pathways usually encode molecular mechanisms that can inform targeted interventions. It is often challenging for existing machine learning approaches to jointly model genetic pathways (higher-order features) and variants (atomic features), and present to clinicians interpretable models. In order to build more accurate and better interpretable machine learning models for genetic medicine, we introduce Pathway Augmented Nonnegative Tensor factorization for HighER-order feature learning (PANTHER). PANTHER selects informative genetic pathways that directly encode molecular mechanisms. We apply genetically motivated constrained tensor factorization to group pathways in a way that reflects molecular mechanism interactions. We then train a softmax classifier for disease types using the identified pathway groups. We evaluated PANTHER against multiple state-of-the-art constrained tensor/matrix factorization models, as well as group guided and Bayesian hierarchical models. PANTHER outperforms all state-of-the-art comparison models significantly (p<0.05). Our experiments on large scale Next Generation Sequencing (NGS) and whole-genome genotyping datasets also demonstrated wide applicability of PANTHER. We performed feature analysis in predicting disease types, which suggested insights and benefits of the identified pathway groups. ","PANTHER: Pathway Augmented Nonnegative Tensor factorization for
  HighER-order feature learning"
55,1339627662348840960,68156248,Nick Hawes,"[""In the @GOALS_oxford lab we're quite excited about uncertain MDPs for representing the results of learning in a model that can be planned with. Our forthcoming #AAAI21 paper introduces a new method for regret-based SSP planning in such models.  \n<LINK>""]",https://arxiv.org/abs/2012.04626,"The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines. ","Minimax Regret Optimisation for Robust Planning in Uncertain Markov
  Decision Processes"
56,1339600369014296577,89709541,Anke Arentsen,"['New paper out today from the XSL collaboration, led by Ariane Lançon (@ObsStrasbourg): ""A comparison between X-shooter spectra and PHOENIX models across the HR-diagram"". [1/3] Link: <LINK>', 'Synthetic stellar spectra are super useful, but there are certainly ways in which they can be improved. Comparing them to observed spectra is crucial for identifying issues! In this paper, Ariane fits flux-calibrated XSL spectra against PHOENIX model spectra. [2/3]', 'The paper shows that there can be significant differences when only the absorption line spectra are fitted vs. when the SED is included (especially for cool stars, but it is a complex function of position on the HR diagram). More work like this is needed in the future! [3/3]']",https://arxiv.org/abs/2012.09129,"The path towards robust near-infrared extensions of stellar population models involves the confrontation between empirical and synthetic stellar spectral libraries across the wavelength ranges of photospheric emission. [...] With its near-UV to near-IR coverage, the X-shooter Spectral Library (XSL) allows us to examine to what extent models succeed in reproducing stellar energy distributions (SEDs) and stellar absorption line spectra simultaneously. This study compares the stellar spectra of XSL with the PHOENIX spectra of the G\""ottingen Spectral Library. The comparison is carried out both separately in the three arms of the X-shooter spectrograph, and jointly across the whole spectrum. When adopting the stellar parameters published with data release DR2 of XSL, we find that the SEDs of the models are consistent with those of the data at Teff > 5000 K. Below 5000 K, there are significant discrepancies in the SEDs. When leaving the stellar parameters free to adjust, satisfactory representations of the SEDs are obtained down to about 4000 K. However, in particular below 5000 K and in the UVB spectral range, strong local residuals associated with intermediate resolution spectral features are then seen; the necessity of a compromise between reproducing the line spectra and reproducing the SEDs leads to dispersion between the parameters favored by various spectral ranges. We describe the main trends observed and we point out localized offsets between the parameters preferred in this global fit to the SEDs and the parameters in DR2. These depend in a complex way on position in the HR diagram (HRD). We estimate the effect of the offsets on bolometric corrections as a function of position in the HRD and use this for a brief discussion of their impact on the studies of stellar populations. [abridged] ","A comparison between X-shooter spectra and PHOENIX models across the
  HR-diagram"
57,1339550155738128384,1189378867,Clément Moulin-Frier,"[""We've just written a first position paper as a kick-off of the ORIGINS project! \n\nGrounding #AI in the origins of human behavior\n<LINK>\n\nThis is an Exploratory Action funded by @Inria, allowing the recruitment of @nisioti_eleni as a new post-doc @FlowersINRIA <LINK>""]",https://arxiv.org/abs/2012.08564,"Recent advances in Artificial Intelligence (AI) have revived the quest for agents able to acquire an open-ended repertoire of skills. However, although this ability is fundamentally related to the characteristics of human intelligence, research in this field rarely considers the processes that may have guided the emergence of complex cognitive capacities during the evolution of the species. Research in Human Behavioral Ecology (HBE) seeks to understand how the behaviors characterizing human nature can be conceived as adaptive responses to major changes in the structure of our ecological niche. In this paper, we propose a framework highlighting the role of environmental complexity in open-ended skill acquisition, grounded in major hypotheses from HBE and recent contributions in Reinforcement learning (RL). We use this framework to highlight fundamental links between the two disciplines, as well as to identify feedback loops that bootstrap ecological complexity and create promising research directions for AI researchers. ",Grounding Artificial Intelligence in the Origins of Human Behavior
58,1339533065127923712,890132360397803520,Christoph Salge,"['New paper: Robust Multi-Agent Reinforcement Learning with Social Empowerment for Coordination and Communication\n\nwith @heiden_tessa @herke and E. Gavves, H. van Hoof\n\n<LINK>\n\n<LINK>', 'The core idea is that transfer empowerment provides a gradient towards those policies that remain reactive towards other agents - reducing the risks of non-robust policies that assume they know how another agent acts (based on learning) rather than reacting to it.', 'We compare to recent, great work by @natashajaques who used mutual information as a social influence measure. Approaches are very similar, but empowerment as potential MI, vs actual MI, has less of a conflict with exploitation, once the agent gets good.  \n\nhttps://t.co/11DZXvSeDl https://t.co/srjvsn9IwF', ""Empowerment leads to faster and better results in MLRL, compared to DDPG and MADDPG and SI.\n\nMore videos and code for the works can be found on Tessa's GitHub: \n\nhttps://t.co/0Hth6BQSnS"", 'Silly typo, this should have been MARL, as in multi-agent reinforcement learning.']",https://arxiv.org/abs/2012.08255,"We consider the problem of robust multi-agent reinforcement learning (MARL) for cooperative communication and coordination tasks. MARL agents, mainly those trained in a centralized way, can be brittle because they can adopt policies that act under the expectation that other agents will act a certain way rather than react to their actions. Our objective is to bias the learning process towards finding strategies that remain reactive towards others' behavior. Social empowerment measures the potential influence between agents' actions. We propose it as an additional reward term, so agents better adapt to other agents' actions. We show that the proposed method results in obtaining higher rewards faster and a higher success rate in three cooperative communication and coordination tasks. ","Robust Multi-Agent Reinforcement Learning with Social Empowerment for
  Coordination and Communication"
59,1339495894031413248,1162081,Carlos Baquero,"['Distributed systems causality is only hard if you miss the right visual abstraction. Posted a new ArXiv tutorial/dissemination paper on making causality graphically easy. <LINK> <LINK>', '@sudomax @dittolive Yep, and Russell also disseminated similar graphical representations in his many talks on Riak and causality.']",https://arxiv.org/abs/2012.09086,"Events in distributed systems include sending or receiving messages, or changing some state in a node. Not all events are related, but some events can cause and influence how other, later events, occur. For instance, a reply to a received mail message is influenced by that message, and maybe by other prior messages also received. This article brings an introduction to classic causality tracking mechanisms and covers some more recent developments. The presentation is supported by a new graphical notation that allows an intuitive interpretation of the causality relations described. ",Causality is Graphically Simple
60,1339493459309309954,1968365508,Samaya Nissanke (she/her) 💙,"['New @GRAPPAinstitute paper on prospects of Hubble constant measurements with a population of neutron-star -- black hole mergers (end to end simulation with gravitational wave and electromagnetic selection effects) with yours truly, led by UCL Feeney + <LINK>', 'Using end-to-end simulations, we anticipate unbiased 1.5-2.4% precision measures in H0 dependent on underlying properties of populations (spin alignment of black hole) &amp; waveform systematics.', 'Really fun collaboration as always with Feeney, Peiris, Mortlock &amp; fun to return &amp; to NS-BH mergers and H0 simulations after 10 years+ (&amp; pause for reflection): https://t.co/kNvFmqXrmN']",https://arxiv.org/abs/2012.06593,"Gravitational wave (GW) and electromagnetic (EM) observations of neutron-star-black-hole (NSBH) mergers can provide precise local measurements of the Hubble constant ($H_0$), ideal for resolving the current $H_0$ tension. We perform end-to-end analyses of realistic populations of simulated NSBHs, incorporating both GW and EM selection for the first time. We show that NSBHs could achieve unbiased 1.5-2.4% precision $H_0$ estimates by 2030. The achievable precision is strongly affected by the details of spin precession and tidal disruption, highlighting the need for improved modeling of NSBH mergers. ","Prospects for Measuring the Hubble Constant with Neutron-Star-Black-Hole
  Mergers"
61,1339484903524712448,1968365508,Samaya Nissanke (she/her) 💙,"['New @GRAPPAinstitute paper led by my PhD student Banafsheh Shiralilou on computing gravitational waveforms in subset of quadratic gravity theories beyond General Relativity. <LINK>', 'Working in the post-Newtonian approximation, we produce GW waveforms where curvature non-linearities beyond a scalar field first appear for the inspiral phase. Instead of null tests of GR, this approach takes forward modelling. A big step &amp; congratulations to Banafsheh!!!', 'Banafsheh has just started her second year of PhD (most of it has been during the pandemic), her family is in Iran&amp;Turkey, we are so proud  of her hard work and determination.', 'Huge kudos to former group member Tanja Hinderer, who just left, it was such a fun collaboration (some photos attached of last night submission)! https://t.co/tVeyeUTmrQ', '@anwesh05 @GRAPPAinstitute Thank you! And of course Tanja, Néstor and Helvi! It was one of those papers where the team spirit was lovely and calm from start to end 🙌🏾 but the science very hard and required true strength and grit and perseverance from Banafsheh (speaking as a former post Newtonian dudette)']",https://arxiv.org/abs/2012.09162,"Gravitational waves (GWs) from merging black holes allow for unprecedented probes of strong-field gravity. Testing gravity in this regime requires accurate predictions of gravitational waveform templates in viable extensions of General Relativity. We concentrate on scalar Gauss-Bonnet gravity, one of the most compelling classes of theories appearing as low-energy limit of quantum gravity paradigms, which introduces quadratic curvature corrections to gravity coupled to a scalar field and allows for black hole solutions with scalar-charge. Focusing on inspiralling black hole binaries, we compute the leading-order corrections due to curvature nonlinearities in the GW and scalar waveforms, showing that the new contributions, beyond merely the effect of scalar field, appear at first post-Newtonian order in GWs. We provide ready-to-implement GW polarizations and phasing. Computing the GW phasing in the Fourier domain, we perform a parameter-space study to quantify the detectability of deviations from General Relativity. Our results lay important foundations for future precision tests of gravity with both parametrized and theory-specific searches. ","Nonlinear curvature effects in gravitational waves from inspiralling
  black hole binaries"
62,1339473434267545601,2188294226,Josh Firth,"['Interested in how behaviours spread on social networks in natural populations? Our new paper outlines the importance of social learning strategies &amp; presents a new method for examining different forms of behavioural contagions. Online now:\n<LINK> <LINK>', 'w/ big thanks to the rest of the team @Gfalbery, @KristinaBeck_, @IvanJaric, L Spurgin, @Ben_Sheldon_EGI , W Hoppitt \xa0&amp; everyone else who has given feedback.\nReally hoping this framework will be useful to lots of people!', '@teddy_wilkin @Gfalbery @KristinaBeck_ @IvanJaric @Ben_Sheldon_EGI Thanks very much @teddy_wilkin 👍it has taken years of work to get it to this stage 😄']",http://arxiv.org/abs/2012.08925,"The spread of socially-learnt behaviours occurs in many animal species, and understanding how behaviours spread can provide novel insights into the causes and consequences of sociality. Within wild populations, behaviour spread is often assumed to occur as a ""simple contagion"". Yet, emerging evidence suggests behaviours may frequently spread as ""complex contagions"", and this holds significant ramifications for the modes and extent of transmission. We present a new framework enabling comprehensive examination of behavioural contagions by integrating social-learning strategies into network-based diffusion analyses. We show how our approach allows determination of the relationship between social bonds and behavioural transmission, identification of individual-level transmission rules, and examination of population-level social structure effects. We provide resources that allow general applications across diverse systems, and demonstrate how further study-specific developments can be made. Finally, we outline the new opportunities this framework facilitates, the conceptual contributions to understanding sociality, and its applications across fields. ","Analysing the Social Spread of Behaviour: Integrating Complex Contagions
  into Network Based Diffusions"
63,1339421633572827139,864023169212055552,Mojtaba Raouf,['Our new paper using the @SAMI_survey has been accepted for publication in ApJ.  This study offers the first evidence that the dynamical state of galaxy groups may influence the BGG’s stellar and gas kinematics. The version of arxiv can be found here: <LINK> <LINK>'],http://arxiv.org/abs/2012.08634,"We study the stellar and gas kinematics of the brightest group galaxies (BGGs) in dynamically relaxed and unrelaxed galaxy groups for a sample of 154 galaxies in the SAMI galaxy survey. We characterize the dynamical state of the groups using the luminosity gap between the two most luminous galaxies and the BGG offset from the luminosity centroid of the group. We find that the misalignment between the rotation axis of gas and stellar components is more frequent in the BGGs in unrelaxed groups, although with quite low statistical significance. Meanwhile galaxies whose stellar dynamics would be classified as `regular rotators' based on their kinemetry are more common in relaxed groups. We confirm that this dependency on group dynamical state remains valid at fixed stellar mass and Sersic index. The observed trend could potentially originate from a differing BGG accretion history in virialised and evolving groups. Amongst the halo relaxation probes, the group BGG offset appears to play a stronger role than the luminosity gap on the stellar kinematic differences of the BGGs. However, both the group BGG offset and luminosity gap appear to roughly equally drive the misalignment between the gas and stellar component of the BGGs in one direction. This study offers the first evidence that the dynamical state of galaxy groups may influence the BGG's stellar and gas kinematics and calls for further studies using a larger sample with higher signal-to-noise. ","The SAMI Galaxy Survey: Kinematics of stars and gas in brightest group
  galaxies; the role of group dynamics"
64,1339387932776513541,39640065,Dan Scolnic,"['One of coolest things in science is when signal goes from 🤔 to 💥. Huge congrats to @ESAGaia team, see this figure that shows huge difference between DR2 v. DR3.  Here is new paper by SH0ES team (<LINK>) that uses new Gaia data, gets same H0 with more precision. <LINK>']",https://arxiv.org/abs/2012.08534,"We present an expanded sample of 75 Milky Way Cepheids with Hubble Space Telescope (HST) photometry and Gaia EDR3 parallaxes which we use to recalibrate the extragalactic distance ladder and refine the determination of the Hubble constant. All HST observations were obtained with the same instrument (WFC3) and filters (F555W, F814W, F160W) used for imaging of extragalactic Cepheids in Type Ia supernova (SN Ia) hosts. The HST observations used the WFC3 spatial scanning mode to mitigate saturation and reduce pixel-to-pixel calibration errors, reaching a mean photometric error of 5 millimags per observation. We use new Gaia EDR3 parallaxes, vastly improved since DR2, and the Period-Luminosity (PL) relation of these Cepheids to simultaneously calibrate the extragalactic distance ladder and to refine the determination of the Gaia EDR3 parallax offset. The resulting geometric calibration of Cepheid luminosities has 1.0% precision, better than any alternative geometric anchor. Applied to the calibration of SNe~Ia, it results in a measurement of the Hubble constant of 73.0 +/- 1.4 km/sec/Mpc, in good agreement with conclusions based on earlier Gaia data releases. We also find the slope of the Cepheid PL relation in the Milky Way, and the metallicity dependence of its zeropoint, to be in good agreement with the mean values derived from other galaxies. In combination with the best complementary sources of Cepheid calibration, we reach 1.8% precision and find H_0=73.2 +/- 1.3 km/sec/Mpc, a 4.2 sigma difference with the prediction from Planck CMB observations under LambdaCDM. We expect to reach ~1.3% precision in the near term from an expanded sample of ~40 SNe Ia in Cepheid hosts. ","Cosmic Distances Calibrated to 1% Precision with Gaia EDR3 Parallaxes
  and Hubble Space Telescope Photometry of 75 Milky Way Cepheids Confirm
  Tension with LambdaCDM"
65,1339329005934104576,1210312444221935616,Cyrus Rashtchian,"['New paper on Approximate Trace Reconstruction with Sami Davies, Miki Racz, Ben Schiffer. You get samples of an unknown n-bit string subject to iid deletions. We study the complexity of finding a string that is close in *edit distance* to the original:\n\n<LINK> <LINK>', 'The motivation comes from practical applications (e.g., DNA data storage), where it would often be fine to get an approximation, as long as you are able to save in the number of samples needed. See our survey for background: https://t.co/HHajyaLORh', 'My favorite result is a black-box lower bound showing that we can take lower bounds for exact reconstruction and turn them into slightly weaker bounds for approximate reconstruction. For example, if you want to get within edit distance n^{1/4}, you need Omega(n^{9/8}) samples. https://t.co/qPxbbz1aiR', 'To complement this, we also provide a handful of new algorithms and upper bounds.  We show that for some large and non-trivial families of strings, there are ""inherently approximate"" algorithms that can get a good approximation using only polylog(n) samples. https://t.co/2ARhFKTgau', 'The main technique is to roughly identify very dense or sparse substrings and then approximate these portions with 1-runs or 0-runs. So we pushed this idea as far as we could, and this led to the families of strings that we can approximately reconstruct.', ""The holy grail would be to show that you can get within edit distance n/100 with poly(n) samples. I don't know how to prove this, but maybe you do! Let me know if you want to chat about this or if you end up solving it!"", 'While our upper bounds leave much room for improvement, we do at least get a separation where many strings need Omega(n) samples for exact reconstruction, but we can approximately reconstruct them with much fewer samples.']",https://arxiv.org/abs/2012.06713,"In the usual trace reconstruction problem, the goal is to exactly reconstruct an unknown string of length $n$ after it passes through a deletion channel many times independently, producing a set of traces (i.e., random subsequences of the string). We consider the relaxed problem of approximate reconstruction. Here, the goal is to output a string that is close to the original one in edit distance while using much fewer traces than is needed for exact reconstruction. We present several algorithms that can approximately reconstruct strings that belong to certain classes, where the estimate is within $n/\mathrm{polylog}(n)$ edit distance, and where we only use $\mathrm{polylog}(n)$ traces (or sometimes just a single trace). These classes contain strings that require a linear number of traces for exact reconstruction and which are quite different from a typical random string. From a technical point of view, our algorithms approximately reconstruct consecutive substrings of the unknown string by aligning dense regions of traces and using a run of a suitable length to approximate each region. To complement our algorithms, we present a general black-box lower bound for approximate reconstruction, building on a lower bound for distinguishing between two candidate input strings in the worst case. In particular, this shows that approximating to within $n^{1/3 - \delta}$ edit distance requires $n^{1 + 3\delta/2}/\mathrm{polylog}(n)$ traces for $0< \delta < 1/3$ in the worst case. ",Approximate Trace Reconstruction
66,1339245285151633408,60893773,James Bullock,['New paper w Victor Robles @YaleAstronomy :: 3D orbits of satellite galaxies (via @ESAGaia ) help constrain their dark matter halo structure &amp; past mass loss.    \n\nOne result: no MW sats have Vmax&gt;27km/s =&gt; Too Big to Fail will not die.  I blame @MBKplus \n\n<LINK> <LINK>'],https://arxiv.org/abs/2012.07865,"Using the phat-ELVIS suite of Milky Way-size halo simulations, we show that subhalo orbital pericenters, $r_{\rm peri}$, correlate with their dark matter halo structural properties. Specifically, at fixed maximum circular velocity, $V_{\rm max}$, subhalos with smaller $r_{\rm peri}$ are more concentrated (have smaller $r_{\rm max}$ values) and have lost more mass, with larger peak circular velocities, $V_{\rm peak}$, prior to infall. These trends provide information that can tighten constraints on the inferred $V_{\rm max}$ and $V_{\rm peak}$ values for known Milky Way satellites. We illustrate this using published pericenter estimates enabled by Gaia for the nine classical Milky Way dwarf spheroidal satellites. The two densest dSph satellites (Draco and Ursa Minor) have relatively small pericenters, and this pushes their inferred $r_{\rm max}$ and $V_{\rm max}$ values lower than they would have been without pericenter information. For Draco, we infer $V_{\rm max} = 23.5 \, \pm 3.3$ km s$^{-1}$ (compared to $27.3 \, \pm 7.1$ km s$^{-1}$ without pericenter information). Such a shift exacerbates the traditional Too Big to Fail problem. Draco's peak circular velocity range prior to infall narrows from $V_{\rm peak} = 21 - 49$ km s$^{-1}$ without pericenter information to $V_{\rm peak} = 25-37$ km s$^{-1}$ with the constraint. Over the full population of classical dwarf spheroidals, we find no correlation between $V_{\rm peak}$ and stellar mass today, indicative of a high level of stochasticity in galaxy formation at stellar masses below $\sim 10^7$ M$_\odot$. As proper motion measurements for dwarf satellites become more precise, they should enable useful priors on the expected structure and evolution of their host dark matter subhalos. ","Orbital pericenters and the inferred dark matter halo structure of
  satellite galaxies"
67,1339239475969478666,4037010501,Eric J. Zhang,"['Our new paper on laser-annealing of transmon qubits: <LINK>\n\nExpanding upon our prior work, we look at the precision of our laser-tuning method, and how it helps mitigate frequency crowding for high-performance quantum processors <LINK>']",https://arxiv.org/abs/2012.08475,"Scaling the number of qubits while maintaining high-fidelity quantum gates remains a key challenge for quantum computing. Presently, superconducting quantum processors with >50-qubits are actively available. For such systems, fixed-frequency transmons are attractive due to their long coherence and noise immunity. However, scaling fixed-frequency architectures proves challenging due to precise relative frequency requirements. Here we employ laser annealing to selectively tune transmon qubits into desired frequency patterns. Statistics over hundreds of annealed qubits demonstrate an empirical tuning precision of 18.5 MHz, with no measurable impact on qubit coherence. We quantify gate error statistics on a tuned 65-qubit processor, with median two-qubit gate fidelity of 98.7%. Baseline tuning statistics yield a frequency-equivalent resistance precision of 4.7 MHz, sufficient for high-yield scaling beyond 1000-qubit levels. Moving forward, we anticipate selective laser annealing to play a central role in scaling fixed-frequency architectures. ","High-fidelity superconducting quantum processors via laser-annealing of
  transmon qubits"
68,1339231323995713537,4365927557,Dr. Jake Turner 🌅,"['***New 1st author paper *****\n\n""The search for radio emission from the exoplanetary systems 55 Cancri, υ Andromedae, and τ Boötis using LOFAR beam-formed observations""\n\nSee the paper here: <LINK>\n\nTHREAD/\n@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro <LINK>', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro I would thank all my co-authors but especially Philippe Zarka and Jean-Mathias Grießmeier (my co-advisor for my PhD) who were instrumental in this work and my current postdoc advisor Ray Jayawardhana (@DrRayJay). \n\n@Obs_Paris @CNRS @Univ_Orleans @Cornell @UVaAstro \n1/n https://t.co/Zc2eMQwJMF', ""@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans Background on paper (1/3): \nI've been working on this project since 2015, the 3rd yr of my PhD. I've worked on this project during my PhD at @UVA &amp; at #LPC2E in France &amp; throughout my postdoc at @Cornell \n\nBut the story of this project began much sooner than that\n\n2/n"", '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA Background (2/3): \nMy 1st year of college in 2007, I read a paper (https://t.co/fz6ekLcu35) by Jean-Mathias that said we might be able to detect exoplanet radio emission using this future telescope called @LOFAR \n\nI was enthralled by this idea but had no idea where to start \n3/n', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA Background (3/3): \nIn 2014, during the 2nd year of grad school I meet Jean-Mathias at the Cool Stars 18 conference @LowellObs\n\nDuring that meeting he was looking for a student to work on some new data taken with LOFAR looking for exoplanet radio emission. The rest is history \n4/n', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs The paper can be found as a forthcoming articles in A&amp;A here: https://t.co/VSiDqm4Ekx\n\nIt can also be found for free on arXiv here:  https://t.co/PgdxgwqTK4\n\nNow to the paper\n5/n', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Why radio? \n\nObserving planetary auroral radio emission is the most promising method to detect exoplanetary magnetic fields. Other methods produce many false positives (eg. https://t.co/o7ivdcMH2s). \n\nSee Grießmeier 2015 for an overview: https://t.co/Z7L7WWJe6C\n6/n https://t.co/XSjWXo8I0l', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs The first proof of Jupiter’s magnetic field, the 1st measured magnetic field of a planet other than Earth, came from observing its radio emission (Burke &amp; Franklin 1955). \n\nAll the magnetized planets in our Solar System emit natural radio waves (see figure below)\n7/n https://t.co/WrU8NwhNle', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Why care about exoplanet magnetic fields? \n- Interior structure  --&gt; dynamo \n- Atmospheric escape &amp; evolution\n- Star-planet interactions\n- Atmospheric dynamics\n- Habitability  --&gt; deflecting stellar wind particles &amp; cosmic rays \n- Rotation period of the planet \n- ExoMoons\n..\n8/n https://t.co/8nJp8VWD2Q', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Looking for radio emission from exoplanets is not new. Astronomers have searched since the 1980s (Winglee et al. 1986, see below) \n\nMost studies since have resulted in clear non-detections. There are a few tentative detections but none have been confirmed by follow-up obs. \n9/n https://t.co/Erj0QbFsGA', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Most of these studies have involved imaging observations and only span a small fraction of the planetary orbit. They were also at higher sensitivities and frequencies than most theoretical predictions. \n10/n https://t.co/fKbmkmJNwn', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs The theoretical predictions by Grießmeier (2007, 2017) find that a handful of planets should be observable with modern low-frequency telescopes like LOFAR. This is what inspired our search. \n11/n https://t.co/MWsTiiCVhq', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs In our study, we obtained and analyzed LOFAR-LBA beam-formed circularly polarized (Stokes-V) observations of the exoplanetary systems 55 Cancri, υ Andromedae, and τ Boötis.\n\nAll three systems are predicted to be good candidates to search for exoplanetary radio emission.\n12/n https://t.co/GAUHYo6WKY', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs For our LOFAR beamformed observations, we observed in the low-band from 15-74 MHZ with three beams in full polarization (IQUV).  \n13/n https://t.co/7C3ib6O9Nb', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs The ON beam was pointed at the source &amp; 2 OFF-beams were pointed at nearby empty sky. \n\nA fundamental assumption of our method is that the OFF beams provide a good characterization of the ionospheric fluctuations, RFI, and any systematics present in the ON-beam.\n14/n https://t.co/EHsYcN6zAH', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs For this project, 22 exoplanet observations were taken with a total of 89 hours. \n\nWe tried to cover as much of the orbit of each planet as possible to compensate for the expected beaming effect of the radio emission. \n15/n https://t.co/P5qUpu0m4x', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs We used the BeamfOrmed Radio Emsision AnaLyIS (BOREALIS) Pipeline (see Vasylieva 2015; Turner et al. 2017, 2019) to analyze the data. \n\nWe showed previously that BOREALIS performs well at masking Radio Frequency interference (RFI) &amp; correctly the time-frequency response\n16/n https://t.co/Y6EUfEuRpP', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs More on BOREALIS and tests we have done with attenuated Jupiter emission can be found:\nTurner et al. 2017: https://t.co/YiecX10kmQ\nTurner et al. 2019: https://t.co/xbC716ajpj\n17/n', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs We searched through the data in two ways: \n1. Looking for slow emission after Integrating over all time or frequency  --- &gt; timescales of mins\n2.) Looking for burst emission after high-pass filtering the dynamic spectrum -- &gt; timescales of secs\n\n18/n https://t.co/hfCRqak6FG', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Results:\nFor most of the observations and frequency ranges explored we did not find any excess signal in the ON-beam when compared to the OFF beams. \n\nAn example of a non-detection of slow emission can be found below \n19/n https://t.co/Y6KJMN4MDv', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs In one observation L570725 (2017-03-06) of τ Boo, we detected slowly variable emission in the range 14-38 MHz.\n\nThe signals in the time-series and integrated spectrum correspond to a statistically significant signal of 6.9σ and 8.6σ, respectively.\n20/n https://t.co/TUQnIdVVKd', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs The slow detection can also be seen in the binned dynamic spectra (this was unexpected). \n\nThese structured features are very similar to the observed radio dynamic spectrum of Jupiter\n\n21/n https://t.co/8SkFvZEyI8', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs However, there is a faint replica signal of the ON-beam in the OFF-beams. \n\nThis replica signal in the OFF beam originates from the ON beam due to imperfect phasing. We detect the pulsar B0809+74 &amp; Jupiter in both the ON and OFF beams\n\n22/n https://t.co/zzMcMCAZBW', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs We could not identify an instrumental origin/systematic error for the excess ON-beam signal detected in observation L570725. \n\nBut, the amplitudes of ON &amp; OFF signal on that day compared to the other dates encourages us to be skeptical. Follow-up obs are needed to confirm. \n23/n https://t.co/kuItMaTwrf', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs We also detected Stokes-V burst emission in the frequency band 15-38 MHz in observation L569131 of τ Boo. \n\nThe ON-beam shows an excess of 1 sec burst emission compared to the OFF-beams and we find a statistically significant signal of 3.2σ.\n24/n https://t.co/Df4z3MAqjb', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs We do not find any potential false positives for the bursty signal, this is likely a real detection of celestial emission and not an instrumental effect. \n\nHowever, a detection at 3.2σ level is not highly significant &amp; calls for confirmation via follow-up observations.\n25/n', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Assuming the bursty and slowly variable signal are real and of celestial origin, we conclude it is indeed likely that the source of the detected signals is located within the τ Boo system.\n26/n https://t.co/WY62S1zNZc', '@CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Emission from the planet τ Boo b remains a possible cause for our detected radio signals. It is not the only possible source, the other being radio emission by stellar flares (however there is no evidence for these)\n\nTherefore, follow-up observations are needed. \n27/n', '""Cornell postdoc detects possible exoplanet radio emission""\n\nCornell Chronicle story on my detection\n\nhttps://t.co/7Anzi9KztJ\n\n28/28 END', '@sensitivsci @CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro Thanks Mark!', '@MikeM_UK_ @CSInst @Cornell @LOFAR @Obs_Paris @CNRS @UVaAstro @DrRayJay @Univ_Orleans @UVA @LowellObs Yes, Mike this is a phased array']",http://arxiv.org/abs/2012.07926,"Observing planetary auroral radio emission is the most promising method to detect exoplanetary magnetic fields, the knowledge of which will provide valuable insights into the planet's interior structure, atmospheric escape, and habitability. We present LOFAR-LBA circularly polarized beamformed observations of the exoplanetary systems 55 Cancri, $\upsilon$ Andromedae, and $\tau$ Bo\""{o}tis. We tentatively detect circularly polarized bursty emission from the $\tau$ Bo\""{o}tis system in the range 14-21 MHz with a flux density of $\sim$890 mJy and with a significance of $\sim$3$\sigma$. For this detection, no signal is seen in the OFF-beams, and we do not find any potential causes which might cause false positives. We also tentatively detect slowly variable circularly polarized emission from $\tau$ Bo\""{o}tis in the range 21-30 MHz with a flux density of $\sim$400 mJy and with a statistical significance of $>$8$\sigma$. The slow emission is structured in the time-frequency plane and shows an excess in the ON-beam with respect to the two simultaneous OFF-beams. Close examination casts some doubts on the reality of the slowly varying signal. We discuss in detail all the arguments for and against an actual detection. Furthermore, a $\sim$2$\sigma$ marginal signal is found from the $\upsilon$ Andromedae system and no signal is detected from the 55 Cancri system. Assuming the detected signals are real, we discuss their potential origin. Their source probably is the $\tau$ Bootis planetary system, and a possible explanation is radio emission from the exoplanet $\tau$ Bootis b via the cyclotron maser mechanism. Assuming a planetary origin, we derived limits for the planetary polar surface magnetic field strength, finding values compatible with theoretical predictions. Further low-frequency observations are required to confirm this possible first detection of an exoplanetary radio signal. [Abridged] ","The search for radio emission from the exoplanetary systems 55 Cancri,
  $\upsilon$ Andromedae, and $\tau$ Bo\""{o}tis using LOFAR beam-formed
  observations"
69,1339219770730799106,3308167500,"David C. Norris, MD 🌻","['NEW #DTAT paper: What Were They Thinking? Pharmacologic priors implicit in a choice of 3+3 dose-escalation design <LINK>\n\n“The unexamined trial is not worth conducting.”\n\n— Socrates 1/\n\n<LINK>', 'This latest #DTAT paper @arXiv sets out to reverse-engineer the unstated (∴ unexamined!) pharmacologic intuitions that underlie the #trialsafety claim implicit in the decision to conduct a dose-escalation trial. 2/ https://t.co/Fs3UXZbbwC', 'Prior elicitation from doctors has never been easy, especially about #pharmacology or the future of the #Daleks: 3/\n\nhttps://t.co/owrPUklQvu', 'So we need a mind-bending contest of sorts. Has dose escalation gone soft after all that time in the tank? Let’s find out, shall we?\n\nEn garde! 4/\n\nhttps://t.co/wXnX0KFjPP', 'Like Morbius sending #DoctorWho back to his earlier regenerations, we need somehow to drive a trial design back to the #priors that presumably justify it. In my view, this amounts to an #InverseProblem—not unlike that presented by computed tomography: 5/\n\nhttps://t.co/crnG4OPQqx', 'Given the full 3-D information from a CT scan, one can easily reconstruct a plain X-ray taken from any angle. Likewise, starting from a fully articulated set of priors about a drug’s pharmacology, we can easily project the safety characteristics of any trial design. 6/ https://t.co/tBjmj30k08', 'That’s the (easy) ‘forward problem’. What if you had to start from a bunch of 2-D X-ray images, and work out the full 3-D structure? THIS is an inverse problem, and it’s HARD—which is why we need the C in CT! 7/ https://t.co/dLnvjjeo60', 'We’ll need plenty of #computation here, too. Ultimately, we will obtain F⁻¹ by a graphical technique requiring thousands of (forwards) calculations of F. The usual approach of approximating F via discrete-event simulation is both too slow and too noisy for this purpose. 8/', 'Over the summer, I came across this point made by Daniel Sabanés Bové &amp; Wai Yin Yeung in a vignette for their #crmPack package — also authored by Giuseppe Palermo &amp; @thomas_jaki — which planted the seed of the necessary idea. 9/\n\nhttps://t.co/7gbK6JVjRD https://t.co/VQlaNH398e', 'For a task such as this, there is no better tool than Prolog. This definite clause grammar (DCG) — which at 526 characters would fit into 2 tweets! — is an EXECUTABLE SPECIFICATION that contains all the essential logic: 10/ https://t.co/D3tUv6nZIq', 'As a big fan of declarative programming, I’ve flirted with Prolog for almost 2 decades now … but never quite managed to make truly effective use of it in a practical application.\n\nAll that changed when I encountered @MarkusTriska’s teaching: 11/\n\nhttps://t.co/y7a23x7AEU', '@MarkusTriska What I’ve learned from Markus is that Prolog remains an active research area, with ongoing work to develop and incorporate new language constructs that expand the (logically) pure core of the language—which is its truly powerful aspect. 12/\n\nhttps://t.co/wLvsvVTscJ', '@MarkusTriska There is in fact a cutting-edge implementation, @mjt128’s Scryer Prolog, fully committed to the ISO Standard, that includes declarative integer arithmetic CLP(ℤ) and—uniquely at the moment—a pure if_/3 predicate that preserves generality. 13/\n\nhttps://t.co/aADhLknq8w', '@MarkusTriska @mjt128 Also notable is that Scryer is itself implemented in @rustlang, a language designed to support #safe, #reliable programming — the very same advantages of #LogicProgramming that so strongly recommend Prolog for #clinicaltrials applications. 14/\nhttps://t.co/Zox7SSyAZQ', '@MarkusTriska @mjt128 @rustlang Thanks to Markus’s online book https://t.co/zQo1lorOIj, videos https://t.co/ExmLORBTy3, and the new declarative #constructs (and pure #practices) he and others have labored to introduce, Prolog has at last become for me a PRACTICAL TOOL. 15/', '@MarkusTriska @mjt128 @rustlang What’s more, my current application barely scratches the surface of what could be done with Prolog in this problem domain.\n\nConstraint logic programming (CLP) might well support a unified treatment subsuming the whole field of dose escalation. 16/ https://t.co/irOAZpaNfo', '(But, I digress…)\n\nFor a 3+3 trial with D prespecified doses, each of the J paths can be represented as a 2×D matrix Tʲ, j ∈ {1,…,J} 17/ https://t.co/UuOymLIEek', 'In terms of these matrices, the J-vector π of path probabilities can be obtained from a simple matrix equation involving a J-vector b and J×2D matrix U that are *constants* for each value of D. 18/ https://t.co/0PdgdRbdUm', 'Although J grows exponentially as D increases, for trials of practical size the matrices remain puny. The latest release of R package #precautionary (v0.2) caches the b’s and U’s for D ranging 2 thru 8. 19/\n\nhttps://t.co/OAQj4uiIBk https://t.co/celJodac23', 'As I’ve done previously, I now ‘ordinalize’ the binary DLTs of the 3+3 trial, obtaining ordinal toxicities in terms of which safety outcomes such as severe or fatal toxicities may be explored. 20/\nhttps://t.co/Pd8hpatLYB', 'In the present analysis, a logarithmic scaling proves supremely helpful. So here I focus on the logarithm of the therapeutic index which I had previously denoted r₀. I denote this ‘log-therapeutic index’ by κ:\n\nκ ≡ log(r₀) 21/ https://t.co/Lp5ONKG58a', 'Crucially, the expected number of fatal toxicities in the trial now also reduces to matrix operations that R can perform almost instantaneously. 22/ https://t.co/Sfua0QXyVC', 'To complete our F function, we need to specify the underlying pharmacology and the trial’s prespecified doses. Again we adhere to our logarithmic theme, positing a lognormal MTDi distribution and a geometric sequence of prespecifed doses with logarithmic spacing δ. 23/ https://t.co/cgNBxyjqfK', 'Now complete, F apparently has many dimensions. But one of them can be factored out if we keep hammering the logarithmic theme.\n\nThe trick? Use our prespecified doses as a natural scale for dose measurement, effectively setting δ≡1.\n24/ https://t.co/HcROTQS2xr', 'This pares down the dimensionality of F enough that we can cram it into a plot like the paper’s Figure 1.\n\nBut as you can see, further simplification is needed!\n\nIf only we could remove just… one… more… dimension… 25/ https://t.co/q9WSklRlbt', 'We can! Through a #minimax framing of our #trialsafety question, we can ‘slice’ Figure 1 along a plausible worst-case scenario:\n\nWhat if our 2nd dose level coincided with median MTDᵢ? 26/\n\nhttps://t.co/Q7DOFUyMRh https://t.co/49JGgxhr79', 'As a bonus, it turns out not only μ (which we’ve effectively set equal to 2) but even D drops out* in this scenario, so we obtain Figure 3—a UNIVERSAL SAFETY SCHEMATIC for 3+3 trials, with axes we can interpret intuitively:\n\n*So long as D ≥ 3; see Fig 2 in the paper. 27/ https://t.co/Qta1hEJOSz', 'κ/σ characterizes the drug itself, according to how its dosing safety margin κ compares to inter-individual variability in optimal dosing.\n\nAs such, it measures how suited the drug is to 1-size-fits-all dosing—a #OneSizeFitsAllogist’s #TherapeuticIndex. 28/ https://t.co/ljNwx3laCd', 'δ/σ, by contrast, gauges an aspect of our design: the signal-to-noise ratio for the dose-escalation process. 29/ https://t.co/Ra9yjZxxQq', 'Phase 1 trialists are called upon, I think, to consider questions posed by the axes of this plot.\n\nFortunately, this is #pharmacology not #metaphysics—nothing here threatens to “transcend every faculty of the mind”! 30/\nhttps://t.co/y9ZFE9COKT', 'To illustrate, let’s reconsider that fatal #AFM11 trial analyzed in my April paper, and try to locate it on Figure 3. 31/\nhttps://t.co/6eLwPlRY2p', 'That trial had a dose-level ratio of 3, while our model gave posterior median estimates 1/σ²\u2004≡\u2004τ ≈ 1.31 and r₀ ≈ 1.33. Pulling out our desk calculator…\n\nσ = 1/√1.31 = 0.874\n\nδ = log 3 = 1.1\n\nκ = log 1.33 = 0.285      32/', 'These figures yield in turn κ/σ ≈ 0.33 and δ/σ ≈ 1.26, which you can see puts that trial—in retrospect—on the far-left edge of Fig 3, between the contours of 0.8 and 0.9 expected fatalities. 33/ https://t.co/ZLksGJwZ6t', 'Of course, a calculator-based ‘postmortem’ of this kind serves only to demonstrate a continuity with earlier work. The proper use of Figure 3 is to promote PROSPECTIVE thinking that yields #smarter and #safer trials. 34/ https://t.co/fSfMmNmvYG', 'Any trialist proposing a dose-escalation design owes trial participants at least reasonable guesses for the therapeutic index κ and optimal-dose heterogeneity parameter σ of the drug, and then thoughtful consideration of their #trialsafety implications. 35/', 'Other dose-escalation designs surely have their own Fig 3’s, perhaps more favorable than the one I’ve drawn for 3+3. The methods I used in this paper ought to be adaptable to any design (such as @koaeraser’s mTPI) for which rules can be pretabulated: 36/\n\nhttps://t.co/qWerGe9Vf6 https://t.co/DAB8qiyJgm', '@koaeraser And thanks to the dose transition pathways (DTP) of @ChristinaBYap et al, even CRM and EWOC should be amenable to this treatment.\n\n(Figure 3 in my paper was generated in mere seconds, so there’s ample headroom for enlarged matrices if needed.) 37/\nhttps://t.co/rk0XZ6DTqE https://t.co/Z4kQSInAcB', '2021 will be a year of so much renewal, and I hope this includes renewed attention to #pharmacologic thinking in #oncology #dosefinding, and the renewed commitment to #trialsafety this will make possible. 38/38', '@threadreaderapp unroll please.']",https://arxiv.org/abs/2012.05301,"If explicit, formal consideration of clinical pharmacology at all informs the design and conduct of modern oncology dose-finding trials, the designs themselves hardly attest to this. Yet in conducting a trial, investigators affirm that they hold reasonable expectations of participant safety - expectations that necessarily depend on beliefs about how certain pharmacologic parameters are distributed in the study population. Thus, these beliefs are implicit in a trial's presumed conformance to a community standard of safety, and may therefore to some extent be reverse-engineered from trial designs. For one popular form of dose-escalation trial design, I demonstrate here how this may be done. ","What Were They Thinking? Pharmacologic priors implicit in a choice of
  3+3 dose-escalation design"
70,1339196773743685632,10834752,Arvind Narayanan,"['Many online education platforms track and profit from student data, but universities are able to use their power to negotiate contracts with vendors to get much better privacy. That’s one of the findings in our new paper “Virtual Classrooms and Real Harms” <LINK>', 'We analyzed 23 popular tools used for online learning—their code, their privacy policies, and 50 “Data Protection Addenda” that they negotiated with universities. We studied 129 (!) U.S. state privacy laws that impact ed tech. We also surveyed 105 educators and 10 administrators.', 'A major reason for poor privacy by default is that the regulations around traditional educational records aren’t well suited to the ‘data exhaust’ of online communication, echoing arguments by @elanazeide &amp; @HNissenbaum here: https://t.co/kA9ucxLe1m', 'The good news is that universities can and do take steps to protect students and educators. We recommend they move away from a complex, expensive, and slow up-front vetting process before licensing software to a more nimble system for incorporating continual feedback.', 'Some backstory for our work: soon after covid-19 hit, we realized that many tech policy problems had acquired a new urgency. The privacy and security risks of the sudden shift to remote learning in higher ed seemed to be one where we could use our skills to uniquely contribute.', 'We’ve tried to find a balance between the speed and thoroughness of our research while battling the chaos of 2020. We hope our recommendations can help universities, including yours. https://t.co/MgIENSGA24\n\nWe also have a blog post about our work: https://t.co/LxhU42dIOH', 'I had a small role in this paper but credit really goes to @shaananc, @RossTeixeira, @akohlbre, @ynotez, @MrsMRS_PhD, and Mihir Kshirsagar, as well as the supportive, interdisciplinary environment at Princeton CITP that is conducive to this kind of research. 🙏']",https://arxiv.org/abs/2012.05867,"Universities have been forced to rely on remote educational technology to facilitate the rapid shift to online learning. In doing so, they acquire new risks of security vulnerabilities and privacy violations. To help universities navigate this landscape, we develop a model that describes the actors, incentives, and risks, informed by surveying 49 educators and 14 administrators at U.S. universities. Next, we develop a methodology for administrators to assess security and privacy risks of these products. We then conduct a privacy and security analysis of 23 popular platforms using a combination of sociological analyses of privacy policies and 129 state laws, alongside a technical assessment of platform software. Based on our findings, we develop recommendations for universities to mitigate the risks to their stakeholders. ",Virtual Classrooms and Real Harms: Remote Learning at U.S. Universities
71,1339170576397635585,131879500,John Ilee,"['We have a new paper out today on the really interesting protoplanetary disc of CI Tau - <LINK>\n\nThis one of (the only?) young stars that we know hosts both a Hot Jupiter and a protoplanetary disc. This means CI Tau can tell us a lot about planet formation 🪐', 'In 2018 we looked at CI Tau with ALMA, finding the dust disc shows many rings and gaps that could be explained by forming planets (in addition to the HJ). However, looking at only dust limits us, and to really understand what was going on, we needed to look at gas in the disc https://t.co/aASKQX9mka', 'We went back again with ALMA to look at the gas in the disc, and found a hint of a gap! However, after comparing carefully with the dust, the gap wasn’t in the “correct” place to be caused by a planet in any of the dust gaps... 🤔 https://t.co/omp4E9dcNW', 'It turns out that the “gap” could be explained by the disc shadowing itself from stellar irradiation. So, we need to take extra care when looking for gas gaps in discs and trying to explain them 🎯', 'We also found tentative evidence for a velocity perturbation that could be caused by a planet *outside* the dust disc. More data is needed to be sure, so we’re eager for @almaobs to resume operations next year 📡 https://t.co/uxkEMRL9Ij', 'This was a fun paper to work on, led by @GRosotti with help from the old IoA crowd (@StefanoFacchi14 @marcotazzari @kamatahvel and many more)', '@r_d_alexander @StefanoFacchi14 Ha, I’m sure he’s busy packing. It’s certainly being debated, but I think there are other lines of evidence that point to a HJ explanation (e.g. https://t.co/IIMuahm39L). Perhaps @joe_llama knows the latest info...?']",https://arxiv.org/abs/2012.07848,"Recent observations have revealed that most proto-planetary discs show a pattern of bright rings and dark gaps. However, most of the high-resolution observations have focused only on the continuum emission. In this Paper we present high-resolution ALMA band 7 (0.89mm) observations of the disc around the star CI Tau in the $^{12}$CO & $^{13}$CO $J=3$-2 and CS $J=7$-6 emission lines. Our recent work demonstrated that the disc around CI Tau contains three gaps and rings in continuum emission, and we look for their counterparts in the gas emission. While we find no counterpart of the third gap and ring in $^{13}$CO, the disc has a gap in emission at the location of the second continuum ring (rather than gap). We demonstrate that this is mostly an artefact of the continuum subtraction, although a residual gap still remains after accounting for this effect. Through radiative transfer modelling we propose this is due to the inner disc shadowing the outer parts of the disc and making them colder. This raises a note of caution in mapping high-resolution gas emission lines observations to the gas surface density - while possible, this needs to be done carefully. In contrast to $^{13}$CO, CS emission shows instead a ring morphology, most likely due to chemical effects. Finally, we note that $^{12}$CO is heavily absorbed by the foreground preventing any morphological study using this line. ","High resolution observations of molecular emission lines toward the CI
  Tau proto-planetary disc: planet-carved gaps or shadowing?"
72,1339160692440567809,216729597,Marcel S. Pawlowski,"['I\'m super excited for our new paper today (by @VoltarCH, @lellifede, @KatjaFah, @satellitegalaxy et al.): <LINK> \n\nThe title already states our main finding: ""The coherent motion of Cen A dwarf satellite galaxies remains a challenge for ΛCDM cosmology"" <LINK>', 'The study is a follow-up on our 2018 paper (https://t.co/hour7mFDYi) which cause some debate when we found that of the Centaurus A satellites galaxies which are arranged in a flattened structure, 14 out of 16 have velocities indicative of a rotating plane of satellites. https://t.co/dCaL47xDDy', ""This is highly unlikely to find in cosmological simulations; less than &lt;0.5% of host galaxies should be surrounded by such an extreme satellite galaxy structure. Baryonic physics doesn't offer an easy way out, so this constitutes a severe challenge to the cosmological model. https://t.co/eoR7QtdvVX"", 'We now test our past findings after almost doubling the number of satellites with velocity information. Guess what: \n\n1) the kinematic signal is still present (21/28 sats follow trend)!\n\n2) Its tension with cosmological simulations (we now use Illustris TNG) is at the 0.2% level! https://t.co/vV9MAPlguG', '@VoltarCH has a great thread with more details on the paper, so let me just add a couple outtakes.\n\nhttps://t.co/3xClZkUzxI', 'The orientation that maximizes the kinematic signal aligns with the spatial flattening of the satellite distribution, like a rotating satellite plane.\n\nDashed line: major axis of sat distr.\nThick green line: minor axis\nDotted lines: indicate where kinematic coherence is maximized https://t.co/eurZFFXcqk', ""Furthermore, while we select the most luminous (or massive) satellite galaxies in the simulations for our main analysis, the results don't change if we instead randomly pick 28 out of N top-ranked sats. The tension with LCDM is robust and not only present for special satellites. https://t.co/5g48bBT95R"", 'One of the concerns regarding the original study was that there were only 16 satellites with known velocities, i.e.: ""we need more data!""\n\nWe got more data. \n\nIt confirms our previous findings.', ""So, observations don't make this LCDM tension go away in Centaurus A. Combined with similar structures around the MW and M31, we really need to think hard about what we might be missing in our cosmological simulations, or even in the underlying model.""]",https://arxiv.org/abs/2012.08138,"The plane-of-satellites problem is one of the most severe small-scale challenges for the standard $\Lambda$CDM cosmological model: several dwarf galaxies around the Milky Way and Andromeda co-orbit in thin, planar structures. A similar case has been identified around the nearby elliptical galaxy Centaurus A (Cen A). In this Letter, we study the satellite system of Cen A adding twelve new galaxies with line-of-sight velocities from VLT/MUSE observations. We find 21 out of 28 dwarf galaxies with measured velocities share a coherent motion. Similarly flattened and coherently moving structures are found only in 0.2% of Cen A analogs in the Illustris-TNG100 cosmological simulation, independently of whether we use its dark-matter-only or hydrodynamical run. These analogs are not co-orbiting, and arise only by chance projection, thus they are short-lived structures in such simulations. Our findings indicate that the observed co-rotating planes of satellites are a persistent challenge for $\Lambda$CDM, which is largely independent from baryon physics. ","The coherent motion of Cen A dwarf satellite galaxies remains a
  challenge for $\Lambda$CDM cosmology"
73,1339134061424635904,1275184138664976384,Vittorio Ferrari,"['📢 New paper 📢\nDeFMO: Deblurring and Shape Recovery of Fast Moving Objects\n\nWe deblur videos of fast moving objects, re-rendering them as if they were captured by a high-speed camera.\n\n@DRozumnyi Martin R. Oswald @matas_jiri @mapo1\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2012.00595,"Objects moving at high speed appear significantly blurred when captured with cameras. The blurry appearance is especially ambiguous when the object has complex shape or texture. In such cases, classical methods, or even humans, are unable to recover the object's appearance and motion. We propose a method that, given a single image with its estimated background, outputs the object's appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed generative model embeds an image of the blurred object into a latent space representation, disentangles the background, and renders the sharp appearance. Inspired by the image formation model, we design novel self-supervised loss function terms that boost performance and show good generalization capabilities. The proposed DeFMO method is trained on a complex synthetic dataset, yet it performs well on real-world data from several datasets. DeFMO outperforms the state of the art and generates high-quality temporal super-resolution frames. ",DeFMO: Deblurring and Shape Recovery of Fast Moving Objects
74,1339016705591406597,1363049263,Chris Colose,"['For anyone interested, thread on a new paper I led (submitted and on arxiv), “Effects of Spin-Orbit Resonances and Tidal Heating on the Inner Edge of the Habitable Zone.” I talk climate, dynamics, astrophysics, tides, habitability, and more. \n<LINK>', 'The inner edge of the HZ can be defined in a few ways, such as a runaway greenhouse transition, or more conservatively (and more accommodating of what GCMs can handle), when the upper atmosphere is moist enough for H2O escape such that a reasonably sized ocean is lost to space.', 'There’s been a lot of attention to the inner edge of the HZ around low mass stars. That’s what astronomers like to look at (more transits, smaller star-to-planet contrast, etc.). There can be a bunch of non-radiative phenomena important in these cases, such as tidal effects.', 'The tidal effects can lead to evolution in rotation, as occurs in the solar system with moons tidally locked to host planets. Earth’s rotation is still far from equilibrium bc of the Moon; Venus rotates slowly (but a few different things are happening) https://t.co/f8UFmNfhR9', 'The rotation of a planet is known to impact HZ estimates, because slow rotators typically develop substellar cloud decks that help shield intense sunlight (at least in GCMs, we’ve never observed this!). Venus’ habitability history is tied up a bit in its rotation history.', 'A lot of tidally evolved exoplanets are expected to be tidally locked, or what many would think of as being synchronously rotating (in which the rotation and orbital period are equal and the same side of the planet always faces the star. A 1:1 resonance).', 'Synchronous rotation doesn’t necessarily mean slow rotation. For planets in the HZ around very low mass stars, the orbital period is short (a few Earth days), so the rotation period is short (dynamically, “fast”). So the flow is different &amp; those cloud decks don’t really develop.', 'The spectrum of a star is also important because even for a given stellar flux, redder stars interact with an atmosphere (or surface) different. Water vapor near-IR absorption, less Rayleigh scattering at long wavelengths. If ice is present, that is no longer that reflective.', 'Probing HZs around different stars thus means 1) sampling different rotation regimes because of Kepler’s 3rd law, e.g., you can’t have a habitable synchronous rotating planet around the Sun w a 30 day rotation period, but you can around a cooler star 2) Different stellar spectrum', 'Work on HZs around different stars and the different dynamical regimes associated with planets around them has been done before. We’ve added to this a bit using a different model, but extend the analysis in a number of ways.', 'One aspect is that synchronous rotation is not inevitable. The rotation can depart from 1:1 for a number of reasons, e.g., an eccentric orbit. In these cases, other states like trapped spin-orbit resonances are permitted (e.g., Mercury rotates 3 times for every 2 revolutions).', 'These spin-orbit configurations become more likely than the synchronous configuration at higher eccentricity, and lead to very different distributions of starlight on the planet. https://t.co/WP1jbXVy8o', 'They can also lead to really cool sunrise and sunsets. For example, on Mercury there are double sunrises and sunsets because the orbital velocity is faster than the rotation near the closest point in orbit\nhttps://t.co/7sTUHOgpHp', 'And one MORE thing (stuff is complicated) is that differential tidal distortions of a planet can generate heat on eccentric orbits. This can be an important energy source, in some cases similar to or larger than the total stellar flux! (diag from https://t.co/W6mP21YiU4 ) https://t.co/GFHxV2FCHi', 'In the paper, we put this stuff together &amp; sampled aquaplanets in 1:1, 3:2, &amp; 2:1 spin-orbit resonances orbiting seven star types ranging from 2600 K to the Sun (~5800 K) and different stellar fluxes using a NASA GISS GCM called ROCKE-3D. https://t.co/oBVeEmjjTj', 'We also calculated geothermal heating rates due to tidal forcing for planets orbiting two star types with a software package called VPlanet (@VPlanetCode), and used those values in a suite of ROCKE-3D simulations \nhttps://t.co/wCcQb6CEYL', 'The experimental setup therefore isn’t too idealized, but meant to be astrophysically consistent. The spectral distribution changes across stellar types; the orbital/rotation period, and (if present) geothermal heating varies with star and stellar flux (that is, distance).', 'Here are some maps of temperature for the different resonance states (here w/o tidal heating); these just by eye show the imprint of the distribution of stellar heating and “dynamical regime” associated w rotation rate, which gets systematically faster moving to lower mass stars https://t.co/vv3nPlD2SC', 'https://t.co/bdQz86E7lx', 'https://t.co/rQF4tfIvc2', 'We ran more sims for the 2600 K and 3000 K stars w tidal heating, the magnitude of which varies due to mass of the stars, planet-star distance, eccentricity, &amp; the rotation itself. So you get diff HZ boundaries depending on the resonance https://t.co/KkTnDapaoz', 'The bottom panel shows the upper atmospheric humidity for the two stars &amp; resonance configurations. There’s a bunch here but key point is tidal and stellar flux both increase as the planet moves closer to the star, so you go from temperate to uninhabitable over a narrow distance https://t.co/HyXk6ieIpu', ""The resonance doesn’t matter much for the IHZ w/o tidal heating, but does with. The stellar type &amp; eccentricity is important (below figure is w e=0.2); ocean configuration, &amp; also model differences (comparison with previous work for the 1:1 case below) matters some\n\nThat's all!"", 'co-authors on twitter @haqqmisra @storyofthewolf1 @VPLanetCode']",https://arxiv.org/abs/2012.07996,"Much attention has been given to the climate dynamics and habitable boundaries of synchronously rotating planets around low mass stars. However, other rotational states are possible, particularly when higher eccentricity orbits can be maintained in a system, including spin-orbit resonant configurations. Additionally, the oscillating strain as a planet moves from periastron to apoastron results in friction and tidal heating, which can be an important energy source. Here, we simulate the climate of ocean-covered planets near the inner edge of the habitable zone around M to solar stars with ROCKE-3D, and leverage the planetary evolution software package, VPLanet, to calculate tidal heating rates for Earth-sized planets orbiting 2600 K and 3000 K stars. This study is the first to use a 3-D General Circulation Model that implements tidal heating to investigate habitability for multiple resonant states. We find that in the absence of tidal heating, the resonant state has little impact on the inner edge, because for a given stellar flux, higher-order states tend to be warmer than synchronous rotators, but for a given temperature, have drier upper atmospheres. However, when strong tidal heating is present, the rotational component implies a strong dependence of habitable conditions on the system evolution and rotational state. Since tidal and stellar heating both decrease with orbital distance, this results in a compact orbital width separating temperate and uninhabitable climates. We summarize these results and also compare ROCKE-3D to previously published simulations of the inner edge that used a modified version of the NCAR CAM4 model. ","Effects of Spin-Orbit Resonances and Tidal Heating on the Inner Edge of
  the Habitable Zone"
75,1339012222811598848,837133583558987776,Colin Raffel,"['New preprint!\nWe demonstrate an attack that can extract non-trivial chunks of training data from GPT-2.\nShould we be worried about this? Probably!\n\nPaper: <LINK>\nBlog post: <LINK>', 'Incredible mega-collaboration with Nicholas Carlini, @florian_tramer, @Eric_Wallace_, @mcjagielski, @adversariel, @katherine1ee, @ada_rob, @nottombrown, @dawnsongtweets, @UlfarEr &amp; @AlinaMOprea']",https://arxiv.org/abs/2012.07805,"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models. ",Extracting Training Data from Large Language Models
76,1338873987842121730,119325534,Kayla Nguyen,['Just out here dropping a new paper :) <LINK>'],https://arxiv.org/abs/2012.04134,"Orbital angular momentum and torque transfer play central roles in a wide range of magnetic textures and devices including skyrmions and spin-torque electronics(1-4). Analogous topological structures are now also being explored in ferroelectrics, including polarization vortex arrays in ferroelectric/dielectric superlattices(5). Unlike magnetic toroidal order, electric toroidal order does not couple directly to linear external fields. To develop a mechanism that can control switching in polarization vortices, we utilize a high-energy electron beam and show that transverse currents are generated by polar order in the ballistic limit. We find that the presence of an electric toroidal moment in a ferro-rotational phase transfers a measurable torque and orbital angular momentum to the electron beam. Furthermore, we find that the complex polarization patterns, observed in these heterostructures, are microscopically chiral with a non-trivial axial component of the polarization. This chirality opens the door for the coupling of ferroelectric and optical properties. ","Transferring Orbital Angular Momentum to an Electron Beam Reveals
  Toroidal and Chiral Order"
77,1338773955822424065,1604541715,"Paul Beck, PhD","['A new paper, lead by Florian Koller, PhD student @UniGraz, reports on six possible candidates for Coronal Mass Ejections or #CME among late-type main-sequence stars from optical spectra @sdssurveys. Great job, Florian! \n<LINK> <LINK>']",https://arxiv.org/abs/2012.00786,"This work aims to detect and classify stellar flares and potential stellar coronal mass ejection (CME) signatures in optical spectra provided by the Sloan Digital Sky Survey (SDSS) data release 14. The sample is constrained to all F, G, K, and M main-sequence type stars, resulting in more than 630,000 stars. This work makes use of the individual spectral exposures provided by the SDSS. An automatic flare search was performed by detecting significant amplitude changes in the $H\alpha$ and $H\beta$ spectral lines after a Gaussian profile was fit to the line core. CMEs were searched for by identifying asymmetries in the Balmer lines caused by the Doppler effect of plasma motions in the line of sight. We identified 281 flares on late-type stars (spectral types K3 to M9). We identified six possible CME candidates showing excess flux in Balmer line wings. Flare energies in $H\alpha$ were calculated and masses of the CME candidates were estimated. The derived $H\alpha$ flare energies range from $3 \times 10^{28} - 2 \times 10^{33}$ erg. The $H\alpha$ flare energy increases with earlier types, while the fraction of flaring times increases with later types. Mass estimates for the CME candidates are in the range of $6 \times 10^{16} - 6 \times 10^{18}$ g, and the highest projected velocities are $\sim300 - 700\:$ km/s. The low detection rate of CMEs we obtained agrees with previous studies, suggesting that for late-type main-sequence stars the CME occurrence rate that can be detected with optical spectroscopy is low. ","Search for flares and associated CMEs on late-type main-sequence stars
  in optical SDSS spectra"
78,1338705690852900867,15520108,Rudi Podgornik 岳儒迪,"['Our new paper ""Electrostatic interaction between SARS-CoV-2 virus and charged electret fibre” explains the detailed nature of the nanoscale interaction between the virus and the electret fibre in N95 filters. <LINK>', 'Here is the local charge on the spike proteins of an isolated virus and a virus close to the electret (polypropylene) fibre. The charge patters are completely modified by the coupling between electric potential and S-protein amino acid dissociation equilibrium. https://t.co/1HuYhsyDVb', 'Almost any kind of face mask offers some protection against particles and pathogens of different sizes, but the most efficient ones make use of a layered structure where one or more layers are electrically charged. These are the electret layers in the filter material.', 'The exact nature of electrostatic capture with respect to both the charge on the particles and the electret fibres as well as the effect of immediate environment remains unclear. This is what our work elucidates by using a detailed spike protein charge regulation model.', 'We show how pH and salt concentration drastically change both the scale and the sign of the virus electret interaction. Surprisingly, configuration of only a few proximal spike proteins is as important for the strength of the interaction as total number on the virus.', 'We formulated the general understanding of efficient virus filtration mechanisms on th enanoscale level and specifically the nature of electrostatic interactions in an electret filter. The crucial element appears to be amino acid charge regulation.', 'A snapshot of the electrostatic potential around the spike protein studded SARS-SoV2 virus. https://t.co/T1zdxomU1b', '@fpaillus Not really. You put the fibers into the field of corona discharge and apparently there is some charge trapping that leads to an electret (same as magnet but for charge) state. The charge is very non-uniformly distributed and macroscopic charge is close to 0.', '@RalfBlossey Thanks. Much appreciated 😀']",https://arxiv.org/abs/2012.07160,"While almost any kind of face mask offers some protection against particles and pathogens of different sizes, the most efficient ones make use of a layered structure where one or more layers are electrically charged. This electret layer is essential to efficient filtration of difficult-to-capture small particles, yet the exact nature of electrostatic capture with respect to both the charge on the particles and the electret fibres as well as the effect of immediate environment remains unclear. Here, we explore in detail the electrostatic interaction between the surface of a single charged electret fibre and a model of SARS-CoV-2 virus. Using Poisson-Boltzmann electrostatics coupled to a detailed spike protein charge regulation model, we show how pH and salt concentration drastically change both the scale and the sign of the interaction. Furthermore, the configuration of the few spike proteins closest to the electret fibre turns out to be as important for the strength of the interaction as their total number on the virus, a direct consequence of spike protein charge regulation. The results of our work elucidate the details of virus electrostatics and contribute to the general understanding of efficient virus filtration mechanisms. ","Electrostatic interaction between SARS-CoV-2 virus and charged electret
  fibre"
79,1338685428212305921,2896282038,Vijay Varma,"['New paper time, which means new movie time! In this paper with Matt Mould and Davide Gerosa and members of the @SXSProject, we confirm the existence of unstable aligned-spin binary black holes in general relativity! Paper link:<LINK>. Settle in for a long thread! <LINK>', 'Most objects in nature are spinning; for example, the Earth, Sun, and our galaxy are all rotating. The same applies to black holes because they are formed from dying stars that are typically rotating. Now, when two spinning black holes get together, wild things can happen!', 'First, if the black holes orbit each other, they lose energy to gravitational radiation, causing the orbit to shrink. This leads to even stronger radiation. This runaway process culminates in a spectacular merger of the two black holes. Credit @SXSProject for this move. https://t.co/iYF0z17qYd', 'When the black holes are spinning, the simple scenario is when both spins are either ""aligned"" along or opposite the orbital angular momentum. In the movie below, the arrows on the BHs show the spins, while the pink arrow in the middle shows the orbital angular momentum. https://t.co/Nhln1l3GVK', 'For these systems, called ""aligned-spin"" systems, things are relatively simple: the black holes orbit in the same plane until they merge into a single black hole. The gravitational signal (shown below the black holes) also looks relatively simple.', 'On the other hand, if the spin directions are tilted with respect to the orbital angular momentum, it\'s a whole other story. In this case, the spins interact with the orbit, causing the orbital plane to wobble, or ""precess"". This also modulates the gravitational wave signal. https://t.co/LE0ZnXmxYp', 'You might be wondering why the final black hole is running away. You can find out all about it here: https://t.co/qJWDQyGmf1', 'Going back to precessing binaries, because the gravitational wave signal is so different from the aligned-spin case, @LIGO and @ego_virgo can use this to learn about the spins of the black holes.', 'Precessing binaries are likely to form in dense environments like globular clusters, while aligned-spin binaries are likely to form in more isolated environments like galactic fields. So, (not) observing precession can tell us how the black holes formed in the first place!', 'So, to sum up, there are two classes of spinning binaries: aligned-spin and precessing. Notably, aligned-spin binaries are also equilibrium points, which means that a system that starts with perfectly aligned-spins will continue to be in that configuration.', ""Ok, now I'm finally getting to the real part of the story. Back in 2015, Davide showed that some of these binaries are actually in an unstable equilibrium: https://t.co/zHgVYW8Nni"", 'Think of a golf ball placed on top of a basketball. If you work hard enough, you can get it to stay there. But even a small wind would make the golf ball fall off. The golf ball in this example is in an unstable equilibrium.', ""What Davide's work showed was that if the spin of the heavier black hole is aligned with the orbital angular momentum, while the spin of the lighter black hole is in the opposite direction, that system can suddenly start precessing if the spins are even slightly disturbed."", 'These are called ""up-down"" binaries, and have important implications: If LIGO/Virgo see a precessing binary, it doesn\'t necessarily have to come from a globular cluster anymore. It could have formed as an up-down binary in a different environment, and then started to precess!', ""Ok, now I'm really really getting to the real part of the story. Davide's calculation was based on an approximation to Einstein's general relativity called post-Newtonian theory. This method works well when the black holes are far away, but breaks down as they approach the merger"", ""At that point, the only thing one can do is to solve Einstein's equations on a supercomputer. This is called numerical relativity and has been very successful in solving Einstein's equations for black hole mergers."", 'So, in this paper, we use some super-long and expensive numerical relativity simulations to look for these weird up-down binaries. And...no surprise, they are the real deal! This is what is shown in the first movie in this thread, which I will repeat below.', 'The purple arrow shows the spin of the heavier BH, which is along the orbital angular momentum (""up""). The orange arrow shows the spin of the lighter BH, which is ""down"". We start the simulation with small tilts for the spins, but as the binary evolves they become very misaligned https://t.co/VxLmAX28kr', 'By the time the black holes merge (end of movie), the spins end up nearly in the orbital plane! For comparison, here are the other four configurations: up-up, down-down and down-up: https://t.co/WGO44z7xMA. Only in the up-down case, the spins go nuts!', 'To sum up: We can now confirm that these up-down aligned-spin binaries do indeed become unstable and start precessing, in full general relativity! These systems may be out there in nature, and we are all really rooting for @LIGO and @ego_virgo to catch these!', ""If you are interested in even more details, apart from Davide's paper, you might like Matt's follow up work: https://t.co/4ejuu9Kztw. Matt also gets full credit for these beautiful animations!"", ""@geodesicvoyager @LIGO @ego_virgo Thanks! For a more mathematical explanation, I would check out Matt's paper: https://t.co/4ejuu9Kztw, around Eq.16. The spin evolution is modeled as a harmonic oscillator, and the instability corresponds to the case where the frequency becomes complex (think exponential growth)."", '@geodesicvoyager @LIGO @ego_virgo Mixing fractions is a bit complicated. Apparently, in galactic fields, you only find up-up configurations because the BHs spins are determined by the angular momentum of the envelope. But in AGN disks, each of those four configurations may be equally likely. So, if you consider', '@geodesicvoyager @LIGO @ego_virgo mixing fractions between AGN disks, globular clusters, and galactic fields, this can become important.', ""@ObinnaUmeh1 @LIGO @ego_virgo Thanks! I'm not a radio astronomy expert, so take this with a grain of salt. Indeed if the black hole has a spin with a component in the orbital plane, the binary would precess. I would think that it's unlikely to have the instability we talk about in this paper, however."", '@ObinnaUmeh1 @LIGO @ego_virgo The reason is, this instability is a two-spin effect, meaning both objects need to have substantial spins. I believe pulsar spins, even for millisecond pulsars, are typically small.', '@ObinnaUmeh1 @LIGO @ego_virgo But that system does sound very interesting. I wonder if they tried to extract the properties of a possible companion and/or the orbit from the modulations they observe.', ""@ObinnaUmeh1 @LIGO @ego_virgo Another possibility, I would guess, is that there is a mountain on the pulsar, which might cause it to precess as well (I think). So, it doesn't necessarily need to be in a binary."", ""@BluColarBluMage That's right! Although, unlike a star-planet system, we can't think of it as one object orbiting a heavier, stationary object. Both black holes are moving at about half the speed of light by the time they merge!"", '@BluColarBluMage Indeed, one very interesting effect of the in-plane spins interacting with each other is the superkick: https://t.co/5g7y9alu1V', '@BluColarBluMage Yep, and people have found some candidates: https://t.co/xO2QSEjNTr. But these are not conclusive as far as I know.', '@BluColarBluMage Superkicks happen for very fine-tuned configuration; the spins need to line up in some specific ways right before the merger. This makes them quite rare. But more moderate kicks like 500-1000 km/s are more common. These may not be enough to eject the final black hole.', ""@BluColarBluMage That's right, about 5000 km/s, which is a little larger than 1%. Yes, smaller kicks could cause the supermassive black hole to be shifted from the galactic center. I believe this one of the things astronomers look for when searching for recoiling supermassive black holes.""]",https://arxiv.org/abs/2012.07147,"Binary black holes with spins that are aligned with the orbital angular momentum do not precess. However, post-Newtonian calculations predict that ""up-down"" binaries, in which the spin of the heavier (lighter) black hole is aligned (antialigned) with the orbital angular momentum, are unstable when the spins are slightly perturbed from perfect alignment. This instability provides a possible mechanism for the formation of precessing binaries in environments where sources are preferentially formed with (anti) aligned spins. In this paper, we present the first full numerical relativity simulations capturing this instability. These simulations span $\sim 100$ orbits and $\sim 3$-$5$ precession cycles before merger, making them some of the longest numerical relativity simulations to date. Initialized with a small perturbation of $1^{\circ}$-$10^{\circ}$, the instability causes a dramatic growth of the spin misalignments, which can reach $\sim 90^{\circ}$ near merger. We show that this leaves a strong imprint on the subdominant modes of the gravitational wave signal, which can potentially be used to distinguish up-down binaries from other sources. Finally, we show that post-Newtonian and effective-one-body approximants are able to reproduce the unstable dynamics of up-down binaries extracted from numerical relativity. ",Up-down instability of binary black holes in numerical relativity
80,1338667757701713920,2337598033,Geraint F. Lewis,"['Why was our universe born in a low entropy state? Has this got anything to do with matter-antimatter symmetry? \n@lukebarnesastro and I explore this in a new paper submitted to JCAP\n\n“Under an Iron Sky: On the Entropy at the Start of the Universe""\n\n<LINK> <LINK>']",https://arxiv.org/abs/2012.06975,"Curiously, our Universe was born in a low entropy state, with abundant free energy to power stars and life. The form that this free energy takes is usually thought to be gravitational: the Universe is almost perfectly smooth, and so can produce sources of energy as matter collapses under gravity. It has recently been argued that a more important source of low-entropy energy is nuclear: the Universe expands too fast to remain in nuclear statistical equilibrium (NSE), effectively shutting off nucleosynthesis in the first few minutes, providing leftover hydrogen as fuel for stars. Here, we fill in the astrophysical details of this scenario, and seek the conditions under which a Universe will emerge from early nucleosynthesis as almost-purely iron. In so doing, we identify a hitherto-overlooked character in the story of the origin of the second law: matter-antimatter asymmetry. ",Under an Iron Sky: On the Entropy at the Start of the Universe
81,1338667309804761090,1169068112177745922,Alexis Plascencia,"['Today @fileviez, @clamurgal and I have a new paper on the arXiv where we study the correlations between dark matter and the decays of a Higgs in a new sector that only couples to quarks  <LINK>\n\nBaryonic Higgs and Dark Matter 1/10 <LINK>', 'We study the theory with local Baryon number that needs the minimal number of fermionic representations to cancel the gauge anomalies (4 new rep’s). One of these representations is a good dark matter candidate since it is neutral and automatically stable 😀 2/10', 'We find three different regions in the parameter space depending on which dark matter annihilation channel gives the dominant contribution to the dark matter relic density. 3/10 https://t.co/tWAYTCe71y', 'We also computed all the tree-level and one-loop decays of the Higgs responsible for the breaking of the U(1)_B.  You can find all the details in Appendix B of the paper 😀 4/10 https://t.co/HjaqPli5xt', 'We find that the most interesting region is the one in which the dominant annihilation channel is DM DM → Z_B h_B since it does not rely on any resonance to obtain the correct relic abundance. 5/10', 'Furthermore, the decay of the new Higgs into a pair of photons can be a few percent! Much larger than the one for the SM Higgs of 0.2%. 6/10 https://t.co/V73vEHA14Z', 'Since constraints from Xenon-1T require the scalar mixing angle to be small, we study the p p → Z_B h_B associated production at the LHC which is not suppressed by the mixing angle.  7/10', 'This is the expected number of events in the g_B vs M_ZB plane for the  High Luminosity LHC. The regions in grey are ruled out from dijet searches by the ATLAS and CMS collaborations.  8/10 https://t.co/o4krnogn9v', 'For the search channel with missing energy in the final state things get more involved since we require the invisible decay of the new Higgs to be the dominant one. 9/10', 'This means the decay h_B → Z_B Z_B needs to be kinematically closed; on the other hand, to satisfy the dark matter relic density we need M_DM = M_ZB/2, so there is only a small window for the mass of the ZB gauge boson for this to take place. 10/10 https://t.co/RCcM8SBhS8']",https://arxiv.org/abs/2012.06599,"We discuss the correlation between dark matter and Higgs decays in gauge theories where the dark matter is predicted from anomaly cancellation. In these theories, the Higgs responsible for the breaking of the gauge symmetry generates the mass for the dark matter candidate. We investigate the Higgs decays in the minimal gauge theory for Baryon number. After imposing the dark matter density and direct detection constraints, we find that the new Higgs can have a large branching ratio into two photons or into dark matter. Furthermore, we discuss the production channels and the unique signatures at the Large Hadron Collider. ",Baryonic Higgs and Dark Matter
82,1338588941201657857,228069637,Kevis-Kokitsi Maninis,['Check out our new work!\n\nVid2CAD: CAD Model Alignment using Multi-View Constraints from Videos\n\nVideo: <LINK>\nProject: <LINK>\nPaper: <LINK>\n\n@StefanPopovCV @MattNiessner @VittoFerrariCV'],https://arxiv.org/abs/2012.04641,"We address the task of aligning CAD models to a video sequence of a complex scene containing multiple objects. Our method can process arbitrary videos and fully automatically recover the 9 DoF pose for each object appearing in it, thus aligning them in a common 3D coordinate frame. The core idea of our method is to integrate neural network predictions from individual frames with a temporally global, multi-view constraint optimization formulation. This integration process resolves the scale and depth ambiguities in the per-frame predictions, and generally improves the estimate of all pose parameters. By leveraging multi-view constraints, our method also resolves occlusions and handles objects that are out of view in individual frames, thus reconstructing all objects into a single globally consistent CAD representation of the scene. In comparison to the state-of-the-art single-frame method Mask2CAD that we build on, we achieve substantial improvements on the Scan2CAD dataset (from 11.6% to 30.7% class average accuracy). ",Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos
83,1337981482451742721,1227067477911310336,Ishan Mishra,"[""🚨!!! New paper alert !!!🚨\n\nMy first, first-author paper is out! ‘Bayesian analysis of Juno/JIRAM's NIR observations of Europa’; In press in Icarus and available on arxiv here: <LINK> \n\nA thread:"", ""In 2018/19, @NASAJuno 's JIRAM spectrometer made serendipitous observations of Europa. Around 500 high quality, high-res spectra were collected (2/n) https://t.co/RB7aSkBUd1"", 'Around the same time, I started developing a Bayesian inference framework for spectroscopic analysis of Europan reflectance data, with the popular Hapke model at its heart (3/n) https://t.co/AfrBA0LQqO', 'This kind of tool will be especially useful to analyze data from future missions like @EuropaClipper  and @JUICEmission,  that aim to pick out signatures of trace species like organics. (4/n) https://t.co/deKzG6ppja', 'But more immediately, we demonstrate an application to select spectra from the new @NASAJuno /JIRAM data-set! (5/n) https://t.co/jcseSgabZe', 'Unfortunately, only 4 spectra from a tiny patch on Europa were usable, as most of the JIRAM data of Europa have uncertain observation geometry parameters - important inputs to our modelling (6/n) https://t.co/eDNu0aFbhI', 'By the way, all the data are also normalized to 1 at 2.227 mu-m, due to uncertain photometric calibration (7/n) https://t.co/uEsYcPb6Uv', ""Anyway, there's still a lot to be learnt from normalized data, and here we study the amorphous v/s crystalline ice distribution of the mean of the four spectra using our Bayesian framework (8/n)"", 'We first validate our framework on synthetic am. ice and cr. ice mixture spectra and a laboratory cr. ice spectrum (courtesy Dr. Roger Clark)  (9/n) https://t.co/jNsDZTVfkX', 'Next, for the JIRAM data, we find strong evidence of small (~20 mu-m) am. ice grains intimately mixed with large (~600 mu-m) cr. ice grains. Am. ice utterly dominates the composition (&gt;99% number density fraction) (10/n) https://t.co/2DTYJROyOk', 'We also performed this analysis for a range of estimated SNR values (normalization factor for noise is unknown), and found that the results still hold. (11/n) https://t.co/6qEeT3bE9S', 'The biggest take-away is that a water-ice only model is clearly lacking as it cannot explain the data at around 2.5 and 3.6 microns. (12/n) https://t.co/T3pfq8CRpz', 'To improve our model and further investigate this mismatch, we need optical constants of non-water-ice Europan species, over the entire 2-5 mu-m range, which is sorely lacking in literature. (13/n) https://t.co/fCfntlMh33', 'But I will help fill the gap! Earlier this year, I got awarded the NASA FINESST grant that involves lab work to calculate Europa specific optical constants (14/n) https://t.co/1iJqb9SsXU', 'We’ll use this to improve our model and revisit the rich @NASAJuno / JIRAM dataset of Europa. Hopefully all the uncertainties in the complete JIRAM dataset will also be sorted out soon! (15/n) https://t.co/skI8BivJmp', 'Shout out to my wonderful advisors @NikoleKLewis and Jonathan Lunine, and another big one to the Bayesian insights of @MartianColonist (all are co-authors). 😊🙏 (16/n)', 'P.S. My open-source python code for the Bayesian framework will be arriving soon at @github . Keep an eye out for the announcement tweet! (17/17)']",https://arxiv.org/abs/2012.05240,"Juno spacecraft's spectrometer JIRAM recently observed the moon Europa in the 2-5 {\mu}m wavelength region. Here we present analysis of the average spectrum of a set of observations near 20{\deg}N and 40{\deg}W, focusing on the two forms of water-ice - amorphous and crystalline. We also take this as an opportunity to present a novel Bayesian spectral inversion framework for reflectance spectroscopy. We first validate this framework using simulated spectra of amorphous and crystalline ice mixtures and a laboratory spectrum of crystalline ice. We next analyze the JIRAM data and, through Bayesian model comparisons, find that a two-component intimately mixed model (TC-IM model) of amorphous and crystalline ice is strongly preferred (at 26{\sigma} confidence) over a two-component model of the same species but where their spectra are areally/linearly mixed. We also find that the TC-IM model is strongly preferred (at > 30{\sigma} confidence) over single-component models with only amorphous or crystalline ice, indicating the presence of both these phases of water ice in the data. For the highest SNR estimates of the JIRAM data, the TC-IM model solution corresponds to a mixture with a very large number density fraction (99.952 +/- 0.001 \%) of small (23.12 +/- 1.01 microns) amorphous ice grains, and a very small fraction (0.048 +/- 0.001 \%) of large (565.34 +/- 1.01 microns) crystalline ice grains. The overabundance of small amorphous ice grains we find is consistent with previous studies. The maximum-likelihood spectrum of the TC-IM model, however, is in tension with the data in the regions around 2.5 and 3.6 {\mu}m, and indicates the presence of non-ice components not currently included in our model, primarily due to the limited availability of cryogenic optical constants. ",Bayesian analysis of Juno/JIRAM's NIR observations of Europa
84,1337835893827235841,1149242181326180353,Lapkin Lab,['Our new paper introduces the first integration of a CO2 electroreduction (a carbon capture technique) into an existing bulk chemical process (ethylene oxide production)!\n\n<LINK>\n\n#Realtimechem  #ClimateChange <LINK>'],https://arxiv.org/abs/2012.03198,"Electrochemical conversion of CO2 (CO2R) into fuels and chemicals can both reduce CO2 emissions and allow for clean manufacturing in the scenario of significant expansion of renewable power generation. However, large-scale process deployment is currently limited by unfavourable process economics resulting from significant up- and down-stream costs for obtaining pure CO2, separation of reaction products and increased logistical effort. We have discovered a method for economically viable recycling of waste CO2 that addresses these challenges. Our approach is based on integration of a CO2R unit into an existing manufacturing process: ethylene oxide (EO) production, which emits CO2 as a by-product. The standard EO process separates waste CO2 from gas stream, hence the substrate for electroreduction is available at an EO plant at no additional cost. CO2 can be converted into an ethylene-rich stream and recycled on-site back to the EO reactor, which uses ethylene as a raw material, and also the anode product (oxygen) can be simultaneously valorized for the EO production reaction. If powered by a renewable electricity source, the process will significantly (ca. 80%) reduce the CO2 emissions of an EO manufacturing plant. A sensitivity analysis shows that the recycling approach can be economically viable in the short term and that its payback time could be as low as 1-2 years in the regions with higher carbon taxes and/or with access to low-cost electricity sources. ","Economically viable CO2 electroreduction embedded within ethylene oxide
  manufacturing"
85,1337520529612402689,201350518,Po-Wei Wang,"['1/ A new paper on unleashing the power of SDPs to ML problems! This time, we detect communities (clusters) by approximate modularity maximization with an efficient low-cardinality SDP. Joint work with @zicokolter.\n\npaper: <LINK>\ncode: <LINK> <LINK>', '2/ Core idea: Convert the Kronecker delta to the dot-products between standard basis, relax the domain, and control the cardinality/sparsity in the decomposed space! This leads to an efficient solver that scales linearly to the number of edges, applicable to millions of nodes. https://t.co/3FjEESHmCl', '3/ The SDP relaxation provides a significant boost over the greedy method, leading to a 30% improvement over the state-of-the-art discrete algorithms, at the same time being faster because requiring less random restarts!\n\nTry it at the Colab: https://t.co/nBivCCO8Wh']",https://arxiv.org/abs/2012.02676,"Modularity maximization has been a fundamental tool for understanding the community structure of a network, but the underlying optimization problem is nonconvex and NP-hard to solve. State-of-the-art algorithms like the Louvain or Leiden methods focus on different heuristics to help escape local optima, but they still depend on a greedy step that moves node assignment locally and is prone to getting trapped. In this paper, we propose a new class of low-cardinality algorithm that generalizes the local update to maximize a semidefinite relaxation derived from max-k-cut. This proposed algorithm is scalable, empirically achieves the global semidefinite optimality for small cases, and outperforms the state-of-the-art algorithms in real-world datasets with little additional time cost. From the algorithmic perspective, it also opens a new avenue for scaling-up semidefinite programming when the solutions are sparse instead of low-rank. ",Community detection using fast low-cardinality semidefinite programming
86,1337386427940802560,56113666,Mengye Ren,"['In standard few-shot learning (FSL), an elephant is always an elephant no matter which episode it is. Check out our new paper that extends FSL to more flexible classification criteria --&gt; <LINK> <LINK>', '@cimonisasi Thank you! We are still working on the release of the code base, hopefully in a couple months.']",https://arxiv.org/abs/2012.05895,"Semantic concepts are frequently defined by combinations of underlying attributes. As mappings from attributes to classes are often simple, attribute-based representations facilitate novel concept learning with zero or few examples. A significant limitation of existing attribute-based learning paradigms, such as zero-shot learning, is that the attributes are assumed to be known and fixed. In this work we study the rapid learning of attributes that were not previously labeled. Compared to standard few-shot learning of semantic classes, in which novel classes may be defined by attributes that were relevant at training time, learning new attributes imposes a stiffer challenge. We found that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. We further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model's generalization ability. ",Few-Shot Attribute Learning
87,1337385549271879681,1296555996303761408,Zemel Group,"['Check out our new paper Flexible Few-Shot Learning -- the same object can belong to different classes depending on context. We found unsupervised representation is better than supervised. A short version at NeurIPS metalearn workshop today at 10 EST. <LINK> <LINK>', 'Joint work by @mengyer @Eleni30fillou @kcjacksonwang @james_r_lucas Jake Snell @xaqlab @AToliasLab and Rich Zemel']",https://arxiv.org/abs/2012.05895,"Semantic concepts are frequently defined by combinations of underlying attributes. As mappings from attributes to classes are often simple, attribute-based representations facilitate novel concept learning with zero or few examples. A significant limitation of existing attribute-based learning paradigms, such as zero-shot learning, is that the attributes are assumed to be known and fixed. In this work we study the rapid learning of attributes that were not previously labeled. Compared to standard few-shot learning of semantic classes, in which novel classes may be defined by attributes that were relevant at training time, learning new attributes imposes a stiffer challenge. We found that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. We further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model's generalization ability. ",Few-Shot Attribute Learning
88,1337378119821488128,980524179739963392,James  Lucas,"['1/6 Check our new paper on Flexible Few-Shot Learning! <LINK>\n\nWe extend FSL to include flexible classification criterion in each episode. Unsupervised representation learning beats supervised approaches.\n\nShort version @ #NeurIPS2020 metalearn workshop, 10am EST <LINK>', '2/6 In the FFSL setting, each episode includes a support set, query set, AND a hidden context.\n\nThe context determines how each example is classified and varies across episodes.\n\nAt test time, we see novel contexts that weren’t included in the training set. https://t.co/fRqRs2vmGd', '3/6 We build two new benchmark datasets based on Celeb-A and Zappos-50k, where the context is determined by a choice of object attributes.\n\nExisting episodic training methods perform poorly on these tasks, but unsupervised representation learning gets closer to oracle performance https://t.co/z0Vrt9abNy', '4/6 Why do existing methods fail?\n\nWe analyze training a protonet in a toy FFSL problem and identify that, unlike in the FSL setting, the protonet is encouraged to destroy information relevant for test-time contexts. https://t.co/7GAiqlxvuB', '5/6 The unsupervised representation learning approaches (U/UFT) learn localized features relevant to the test-time context. This allows them to generalize well. https://t.co/zLNYuLhpjw', '6/6 Many additional results included in our paper: \n\nIncluding: careful evaluation of our proposed method, further Celeb-A + Zappos-50k experiments, evaluation on an ImageNet FFSL task, + much more! https://t.co/4zAhXIBFQp']",https://arxiv.org/abs/2012.05895,"Semantic concepts are frequently defined by combinations of underlying attributes. As mappings from attributes to classes are often simple, attribute-based representations facilitate novel concept learning with zero or few examples. A significant limitation of existing attribute-based learning paradigms, such as zero-shot learning, is that the attributes are assumed to be known and fixed. In this work we study the rapid learning of attributes that were not previously labeled. Compared to standard few-shot learning of semantic classes, in which novel classes may be defined by attributes that were relevant at training time, learning new attributes imposes a stiffer challenge. We found that supervised learning with training attributes does not generalize well to new test attributes, whereas self-supervised pre-training brings significant improvement. We further experimented with random splits of the attribute space and found that predictability of test attributes provides an informative estimate of a model's generalization ability. ",Few-Shot Attribute Learning
89,1337255175996596227,3013822602,Eric Michaud,"['Excited to share my new paper “Understanding Learned Reward Functions” with co-authors @ARGleave and Stuart Russell, presented at the Deep RL Workshop at #NeurIPS2020\n\nPaper: <LINK>\nCode: <LINK>\nPresentation: <LINK> <LINK>', 'How can you tell if a learned reward function captures user preferences? We apply some standard ML interpretability techniques towards understanding what learned reward functions are doing in a few RL environments.', 'For instance, in this gridworld environment, where the agent (blue) tries to get to the goal (green), we find that our learned reward function simply detects whether a goal block is visible, not whether the agent has reached it. https://t.co/NyiF5NnAwy', ""In Atari environments like Seaquest, our learned reward function here seems to pay the most attention to the game's score display (areas highlighted in green are most salient). To reliably predict reward, the model can simply learn to detect when the score changes. https://t.co/gAYDtRgTUE"", 'This is a good reminder that when benchmarking reward learning algorithms on games, the score should not be displayed in the environment -- it can make the task of predicting reward too easy.', 'Why does any of this matter? Well, for many real-world tasks, it is not possible to manually design a good reward function for an RL agent -- human desires, which the agent is tasked with realizing, are just too complicated. Reward functions must instead be *learned*.', ""However, current algorithms for reward learning can fail silently. Absent perfect reward learning, we therefore need techniques for auditing learned reward functions -- for scrutinizing a machine's understanding of human preferences."", 'Our paper is a tentative step in this direction. We hope that more advanced interpretability techniques will someday allow researchers to more comprehensively open up AI systems and verify that such systems understand and are aligned with human values.', 'As a closing thought, I also wonder whether future interpretability techniques, coupled with sophisticated reward learning, could be a kind of ""microscope AI"" for improving our understanding of human values and human well-being. @ch402 @nickcammarata @SamHarrisOrg', 'This work was done during my internship with @CHAI_Berkeley. Many thanks to everyone at CHAI for your support!']",http://arxiv.org/abs/2012.05862,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability. ",Understanding Learned Reward Functions
90,1337211008729968646,241532071,Nataniel Ruiz,"['Presenting our new AI/ML paper of work done at @Apple (w/ Barry Theobald, @anuragranj, Ahmed Abdelaziz and Nick Apostoloff). MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias. \nWe test face recognition networks by generating faces.\n<LINK> <LINK>']",https://arxiv.org/abs/2012.05225,"To detect bias in face recognition networks, it can be useful to probe a network under test using samples in which only specific attributes vary in some controlled way. However, capturing a sufficiently large dataset with specific control over the attributes of interest is difficult. In this work, we describe a simulator that applies specific head pose and facial expression adjustments to images of previously unseen people. The simulator first fits a 3D morphable model to a provided image, applies the desired head pose and facial expression controls, then renders the model into an image. Next, a conditional Generative Adversarial Network (GAN) conditioned on the original image and the rendered morphable model is used to produce the image of the original person with the new facial expression and head pose. We call this conditional GAN -- MorphGAN. Images generated using MorphGAN conserve the identity of the person in the original image, and the provided control over head pose and facial expression allows test sets to be created to identify robustness issues of a facial recognition deep network with respect to pose and expression. Images generated by MorphGAN can also serve as data augmentation when training data are scarce. We show that by augmenting small datasets of faces with new poses and expressions improves the recognition performance by up to 9% depending on the augmentation and data scarcity. ",MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias
91,1337064147645816832,1238925341747343361,Aaron Barth,"['New paper today by Ben Boizelle, on black hole mass measurements in radio galaxies NGC 315 and NGC 4261 using ALMA data:\n\n<LINK> <LINK>', ""NGC 315 is one of the best ALMA targets we've found so far, with a clear detection of high-velocity rotation from gas inside the sphere of influence of a 2 billion solar mass black hole.""]",https://arxiv.org/abs/2012.04669,"We present Atacama Large Millimeter/submillimeter Array (ALMA) Cycle 5 and Cycle 6 observations of CO(2$-$1) and CO(3$-$2) emission at 0.2''$-$0.3'' resolution in two radio-bright, brightest group/cluster early-type galaxies, NGC 315 and NGC 4261. The data resolve CO emission that extends within their black hole (BH) spheres of influence ($r_\mathrm{g}$), tracing regular Keplerian rotation down to just tens of parsecs from the BHs. The projected molecular gas speeds in the highly inclined ($i>60^\circ$) disks rises at least 500 km s$^{-1}$ near their galaxy centers. We fit dynamical models of thin-disk rotation directly to the ALMA data cubes, and account for the extended stellar mass distributions by constructing galaxy surface brightness profiles corrected for a range of plausible dust extinction values. The best-fit models yield $(M_\mathrm{BH}/10^9\,M_\odot)=2.08\pm0.01(\mathrm{stat})^{+0.32}_{-0.14}(\mathrm{sys})$ for NGC 315 and $(M_\mathrm{BH}/10^9\,M_\odot)=1.67\pm0.10(\mathrm{stat})^{+0.39}_{-0.24}(\mathrm{sys})$ for NGC 4261, the latter of which is larger than previous estimates by a factor of $\sim$3. The BH masses are broadly consistent with the relations between BH masses and host galaxy properties. These are among the first ALMA observations to map dynamically cold gas kinematics well within the BH-dominated regions of radio galaxies, resolving the respective $r_\mathrm{g}$ by factors of $\sim$5$-$10. The observations demonstrate ALMA's ability to precisely measure BH masses in active galaxies, which will enable more confident probes of accretion physics for the most massive galaxies. ","Black Hole Mass Measurements of Radio Galaxies NGC 315 and NGC 4261
  Using ALMA CO Observations"
92,1336970430192758785,786855300322172928,Alkistis Pourtsidou,"['Paper alert! A new @EC_Euclid paper led by Peter Taylor from @NASAJPL applies a method known as ""k-cut"" or ""nulling scheme"" to 3x2pt statistics to optimally filter out nonlinearities we cannot model to sufficient precision. <LINK>', 'The main problem the method solves is related to the fact that when having broad redshift kernels (as in weak lensing) an angular scale cut (ell_cut) does not really correspond to removing small physical scales because of mode mixing effects due to the projection.', 'By applying the BNT ""nulling"" transformation the bins become narrower limiting this mixing effect and allowing for a true cut of physical scales (k_cut) minimising information loss. https://t.co/wnvbtWRtyN', 'The gist of it is that with this transform and the corresponding k_cut you can get the same precision (FOM) but with much better accuracy wrt to an optimistic ell_cut that would lead to biased (wrong) parameter estimation. https://t.co/7DAb0qOaA1', 'As an extra bonus, the application of the method seems to come with zero extra computational cost to the data analysis pipeline. Exciting!!! Have a read of the paper for more details 🙂']",https://arxiv.org/abs/2012.04672,"Modelling uncertainties at small scales, i.e. high $k$ in the power spectrum $P(k)$, due to baryonic feedback, nonlinear structure growth and the fact that galaxies are biased tracers poses a significant obstacle to fully leverage the constraining power of the {\it Euclid} wide-field survey. $k$-cut cosmic shear has recently been proposed as a method to optimally remove sensitivity to these scales while preserving usable information. In this paper we generalise the $k$-cut cosmic shear formalism to $3 \times 2$ point statistics and estimate the loss of information for different $k$-cuts in a $3 \times 2$ point analysis of the {\it Euclid} data. Extending the Fisher matrix analysis of~\citet{blanchard2019euclid}, we assess the degradation in constraining power for different $k$-cuts. We work in the idealised case and assume the galaxy bias is linear, the covariance is Gaussian, while neglecting uncertainties due to photo-z errors and baryonic feedback. We find that taking a $k$-cut at $2.6 \ h \ {\rm Mpc} ^{-1}$ yields a dark energy Figure of Merit (FOM) of 1018. This is comparable to taking a weak lensing cut at $\ell = 5000$ and a galaxy clustering and galaxy-galaxy lensing cut at $\ell = 3000$ in a traditional $3 \times 2$ point analysis. We also find that the fraction of the observed galaxies used in the photometric clustering part of the analysis is one of the main drivers of the FOM. Removing $50 \% \ (90 \%)$ of the clustering galaxies decreases the FOM by $19 \% \ (62 \%)$. Given that the FOM depends so heavily on the fraction of galaxies used in the clustering analysis, extensive efforts should be made to handle the real-world systematics present when extending the analysis beyond the luminous red galaxy (LRG) sample. ",Euclid: Forecasts for $k$-cut $3 \times 2$ Point Statistics
93,1336930498887684096,1007218217633361920,Fredrik K. Gustafsson,"['New paper: Accurate 3D Object Detection using Energy-Based Models.\n\narXiv: <LINK>\nCode: <LINK>\nProject page: <LINK>\n\n<LINK>', 'We apply energy-based models p(y|x; theta) to the task of 3D bounding box regression, extending the recent energy-based regression approach from 2D to 3D object detection.', 'This is achieved by designing a differentiable pooling operator for 3D bounding boxes y, and adding an extra network branch to the state-of-the-art 3D object detector SA-SSD.', 'We evaluate our proposed detector on the KITTI dataset and consistently outperform the SA-SSD baseline, demonstrating the potential of energy-based models for 3D object detection.']",https://arxiv.org/abs/2012.04634,"Accurate 3D object detection (3DOD) is crucial for safe navigation of complex environments by autonomous robots. Regressing accurate 3D bounding boxes in cluttered environments based on sparse LiDAR data is however a highly challenging problem. We address this task by exploring recent advances in conditional energy-based models (EBMs) for probabilistic regression. While methods employing EBMs for regression have demonstrated impressive performance on 2D object detection in images, these techniques are not directly applicable to 3D bounding boxes. In this work, we therefore design a differentiable pooling operator for 3D bounding boxes, serving as the core module of our EBM network. We further integrate this general approach into the state-of-the-art 3D object detector SA-SSD. On the KITTI dataset, our proposed approach consistently outperforms the SA-SSD baseline across all 3DOD metrics, demonstrating the potential of EBM-based regression for highly accurate 3DOD. Code is available at this https URL ",Accurate 3D Object Detection using Energy-Based Models
94,1336868143189417984,2872512304,Dr. Elizabeth Hobson,"['New preprint! Figuring out how to best analyze social network data can be a brain breaker. In this paper, we discuss several considerations, potential pitfalls, and best practices for thinking through your analyses. <LINK> <LINK>', 'This was a great collaboration that started as a @NIMBioS working group. Careful readers may spot three interesting quirks / easter eggs hidden in the paper… https://t.co/ZslasOfsSf', 'Thanks to co-first-author @mattjsilk for a fantastic supplement (~80 pages of examples!) &amp; terrific co-authors Nina Fefferman, @DanLarremore, Puck Rombach, Saray Shai, &amp; @NoaPinter', ""Thanks also to our reviewers who provided a ton of useful feedback - it was a very positive review experience! We're revising now, so if anyone wants to catch the quirks/easter eggs they will likely be much harder to find in the future as we clean some things up...Any guesses?""]",https://arxiv.org/abs/2012.04720,"Analyzing social networks is challenging. Key features of relational data require the use of non-standard statistical methods such as developing system-specific null, or reference, models that randomize one or more components of the observed data. Here we review a variety of randomization procedures that generate reference models for social network analysis. Reference models provide an expectation for hypothesis-testing when analyzing network data. We outline the key stages in producing an effective reference model and detail four approaches for generating reference distributions: permutation, resampling, sampling from a distribution, and generative models. We highlight when each type of approach would be appropriate and note potential pitfalls for researchers to avoid. Throughout, we illustrate our points with examples from a simulated social system. Our aim is to provide social network researchers with a deeper understanding of analytical approaches to enhance their confidence when tailoring reference models to specific research questions. ","A guide to choosing and implementing reference models for social network
  analysis"
95,1336708304190361600,390426378,Dr. John Noonan,"['New paper alert!\nWe gazed into the hazy entrails of a past long forgotten seeking knowledge, and the oracle gave us riddles: <LINK>', ""@TheScripps5 tbh that's my reaction to most papers too, and thanks!""]",https://arxiv.org/abs/2012.04619,"Far ultraviolet observations of comets yield information about the energetic processes that dissociate the sublimated gases from their primitive surfaces. Understanding which emission processes are dominant, their effects on the observed cometary spectrum, and how to properly invert the spectrum back to composition of the presumably pristine surface ices of a comet nuclei are all critical components for proper interpretation and analysis of comets. The close approach of comet 46P/Wirtanen in 2018-2019 provided a unique opportunity to study the inner most parts of a cometary coma with the Hubble Space Telescope Cosmic Origins Spectrograph, rarely accessible with remote observations, at length scales (100's of km) and wavelengths (900-1430 Angstroms) previously probed only by the European Space Agency's Rosetta spacecraft. Our observations show a complex picture for the inner coma; atomic production rates for H and O that show water is the dominant source of both, an abundance of atomic sulfur that is difficult to explain with the lifetimes of common sulfur parent molecules, and a density distribution that is poorly fit with both Haser and vectorial models. ",FUV Observations of the Inner Coma of 46P/Wirtanen
96,1336692204929212420,804397363402067968,Timothy Raben,"['Super excited about this new paper <LINK>, that should excite all you #odderon fans!\n\nThis is my first collaboration with experimentalists from large collaborations (D0 &amp; TOTEM).', 'The goal of this paper was to look at the difference between proton-proton, and proton-antiproton scattering. Comparing these two processes at similar energies has the potential to reveal some fun physics!', ""Unfortunately we don't usually build a new machine/experiment to just switch from protons to anti-protons, we usually go to higher energies too."", 'So this study looked at several center of mass energies with the TOTEM experiment (proton-proton) and extrapolated down to the highest D0 center of mass energy (proton-antiproton).', 'The specific ""object"" of interest here is the differential cross section as a function of transverse momentum. I.e. how does  the probability of scattering change as the transverse momentum of the interaction change.', 'This dip and bump is a characteristic of this process. https://t.co/A5HeZ18y9h', 'It has long been suspected that the size/dip-ness depends on whether you look at proton-proton or proton-antiproton scattering.  As you can see from the picture, it certainly looks like it! In fact we claim the difference in the above graph leads to a 3.4sigma difference.', ""But wait, that's not all! Totem has already measured the total cross section for this process (and something called rho). This measurement is also dominated by the very same physics that we are measuring here. In fact, since the measurements take place in different..."", 'transverse momentum region (i.e. not measuring the exact same effect), we can combine the two measurements and say\n\nWe have &gt;5sigma evidence for the t-channel exchange of a colorless C-odd gluonic compound!', 'Woo! Above 5 sigma! That means we ""know"" there is some real physics going on here.\n\nWhat about all the jargon at the end of the previous tweet? That\'s about as specific as we can get about what exactly causes this 5 sigma discrepancy.', 'The leading idea is that this is an ""odderon"" effect (#teamodderon), but exactly what an odderon is, how many there are, what other effects there could be at play, and many more questions will have to wait to future studies.']",https://arxiv.org/abs/2012.03981,"We describe an analysis comparing the $p\bar{p}$ elastic cross section as measured by the D0 Collaboration at a center-of-mass energy of 1.96 TeV to that in $pp$ collisions as measured by the TOTEM Collaboration at 2.76, 7, 8, and 13 TeV using a model-independent approach. The TOTEM cross sections extrapolated to a center-of-mass energy of $\sqrt{s} =$ 1.96 TeV are compared with the D0 measurement in the region of the diffractive minimum and the second maximum of the $pp$ cross section. The two data sets disagree at the 3.4$\sigma$ level and thus provide evidence for the $t$-channel exchange of a colorless, $C$-odd gluonic compound, also known as the odderon. We combine these results with a TOTEM analysis of the same $C$-odd exchange based on the total cross section and the ratio of the real to imaginary parts of the forward elastic scattering amplitude in $pp$ scattering. The combined significance of these results is larger than 5$\sigma$ and is interpreted as the first observation of the exchange of a colorless, $C$-odd gluonic compound. ","Comparison of $pp$ and $p \bar{p}$ differential elastic cross sections
  and observation of the exchange of a colorless $C$-odd gluonic compound"
97,1336690769155870721,841858076193955841,Karan Desai (KD),"['📢📢 New paper (w/ @ramprs21 @jcjohnss @nikhil_ai) identifies two bottlenecks when scaling MoCo-like contrastive SSL methods to complex scene images, and proposes a general training method to fix them!\nPaper: <LINK>\nBlog: <LINK>\nMore Details 👇👇 <LINK>']",https://arxiv.org/abs/2012.04630,"Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretraining. Despite their success these methods have been primarily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene images with many objects. Analyzing contrastive SSL methods shows that they have poor visual grounding and receive poor supervisory signal when trained on scene images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome these limitations. CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST significantly improves the features learned by SSL methods on scene images, and further experiments show that CAST-trained models are more robust to changes in backgrounds. ","CASTing Your Model: Learning to Localize Improves Self-Supervised
  Representations"
98,1336688376502743042,785305217822842880,Ramprasaath,"['📢New Preprint📢 Introducing CAST--a generic training recipe to fix visual grounding of contrastive SSL models (eg MoCo) and make them work on complex scene images!\n\nPaper: <LINK>\nBlog: <LINK>\n(w/ @kdexd @jcjohnss @nikhil_ai) @SFResearch (1/3) <LINK>', 'We identify two major issues with MoCo (and similar models): (a) Poor visual grounding ability. It sometimes ""cheats"" during pretraining by looking at the background. CAST uses unsupervised saliency maps to force the model to look at relevant image regions (via Grad-CAM). (2/3)', '...(b) Noisy training signal. Randomly sampled crops from an input image may contain different (or no) objects. CAST samples random crops such that they overlap with salient image regions.\nResults on downstream tasks —&gt; better transfer &amp; better grounding! \nCode coming soon! (3/3)']",https://arxiv.org/abs/2012.04630,"Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretraining. Despite their success these methods have been primarily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene images with many objects. Analyzing contrastive SSL methods shows that they have poor visual grounding and receive poor supervisory signal when trained on scene images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome these limitations. CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST significantly improves the features learned by SSL methods on scene images, and further experiments show that CAST-trained models are more robust to changes in backgrounds. ","CASTing Your Model: Learning to Localize Improves Self-Supervised
  Representations"
99,1336655077755543563,610427323,Desika Narayanan,"[""hey astroroverse - we're putting out a paper and new simulation effort today that we're super pumped about!\n\n<LINK>\n\nled by amazing UF grad student Qi Li, we investigate, via cosmological simulation, the origin of the Milky Way's dust extinction curve [1/]"", ""extinction laws are *critical* to interpreting almost any UV-NIR observation in astronomy.  and yet, we don't have solid models for understanding their origin, why and how they vary.   \n\nunsurprisingly, the MW offers the best obs constraints on the curve - so we start there! [2/]"", 'in short: we find that progenitors of the MW start off with steeper ext. laws early in the Universe.  as the metal densities grow, grain growth processes overwhelm destruction, and you transfer power from small grains to large in the size distributions, flattening to MRN  [3/]', 'at the same time, the 2175 Angstrom UV bump strengths are much more complex.  these depend primarily on the graphite/silicate ratio, which is (in our model), dependent on the fraction of carbon-dominated dust grains vs ""others"". [4/]', ""the UV bump of the MW rises with cosmic time, but asymptotes as the graphite/silicate ratio doesn't change much.  at the end, you get model z=0 extinction laws in MW analogs that look a lot like the MW range (yellow show our 12 MW analogs, while dashed show obs. MW range) [5/N] https://t.co/sW3h4BcqXa"", 'but...*how* did we do this?  to do all of this, we (Qi Li) put in a model for active dust particles into gizmo, and ran cosmological sims with simba physics in it.  these ""super-dust"" particles feel dust-grain drag, and represent individually, a distribution of sizes. [6/N]', ""this represents the first paper of a larger campaign we're leading to understand grain size distributions and dust extinction laws across the Universe! some of our future work will be from gizmo, other from arepo and the smuggle feedback model. [7/N]"", ""finally, and most importantly: Qi Li is an unreal scientist and computationalist. i can't believe how lucky i've been to collaborate with him. and he's on the postdoc job market!   be on the look out for his application and forthcoming great work by him.""]",https://arxiv.org/abs/2012.03978,"We develop a cosmological model for the evolution of dust grains in galaxies with a distribution of sizes in order to understand the origin of the Milky Way dust extinction curve. Our model considers the formation of active dust in evolved stars, growth by accretion and coagulation, and destruction processes via shattering, sputtering, and astration in the ISM of galaxies over cosmic time. Our main results follow. Galaxies in our cosmological model with masses comparable to the Milky Way's at z~0 exhibit a diverse range of extinction laws, though with slopes and bump strengths comparable to the range observed in the Galaxy. The progenitors of the Milky Way have steeper slopes, and only flatten to slopes comparable to the Galaxy at $z \approx 1$. This owes to increased grain growth rates at late times/in high-metallicity environments driving up the ratio of large to small grains, with a secondary dependence on the graphite to silicate ratio evolution. The UV bump strengths depend primarily on the graphite to silicate ratio, and remain broadly constant in MW-like galaxies between z=3 and z=0, though show slight variability. Our models span comparable regions of bump-slope space as sightlines in the Galaxy do, though there is a lack of clear relationship between the model slopes and bump strengths owing to small scale fluctuations in the bump strength. Our models naturally produce slopes for some non-Milky Way analogs as steep as those of the LMC and SMC in metal poor galaxies, though notably the bump strengths are, on average, too large when comparing to the Magellanic clouds. This owes to the fact that we evolve the grain size distributions of graphites and silicates simultaneously, which is an oversimplification. Our model provides a novel framework to study the origins and variations of dust extinction curves in galaxies over cosmic time. ",The Origin of the Dust Extinction Curve in Milky Way-like Galaxies
100,1336614102458884103,1329722485806407681,Diego Ortego,['Interested in training DNNs with noisy labels? Check out our new work on Multi-Objective Interpolation Training to deal with synthetic and web noise.\n@ArazoEric  Paul Albert @oconnorn @kevinmcguinness @insight_centre\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2012.04462,"Deep neural networks trained with standard cross-entropy loss memorize noisy labels, which degrades their performance. Most research to mitigate this memorization proposes new robust classification loss functions. Conversely, we propose a Multi-Objective Interpolation Training (MOIT) approach that jointly exploits contrastive learning and classification to mutually help each other and boost performance against label noise. We show that standard supervised contrastive learning degrades in the presence of label noise and propose an interpolation training strategy to mitigate this behavior. We further propose a novel label noise detection method that exploits the robust feature representations learned via contrastive learning to estimate per-sample soft-labels whose disagreements with the original labels accurately identify noisy samples. This detection allows treating noisy samples as unlabeled and training a classifier in a semi-supervised manner to prevent noise memorization and improve representation learning. We further propose MOIT+, a refinement of MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation studies verify the key components of our method. Experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. Code is available at this https URL ",Multi-Objective Interpolation Training for Robustness to Label Noise
101,1336505646292799488,970803673155751936,Charline Le Lan,"['New insights about the limitations of density-based anomaly detection!\nWith @laurent_dinh, we show that perfect density model *cannot* guarantee anomaly detection.\n📜Paper: <LINK>\n(1/8) <LINK> <LINK>', 'We demonstrate that the widespread intuition that an anomaly should have low density does not quite apply in general, irrespective of the dimension of the data or the inductive bias of the model.\n(2/8)', 'We deconstruct this expectation through the lens of invertible reparametrizations (e.g., Cartesian to polar coordinates). While density changes with the underlying representation, the status of inlier/outlier *does not*.\n(3/8)', 'In particular, it is possible to impose an arbitrary score to any point in a new representation of the same problem, which can mislead density-based anomaly detection methods into wrong anomaly detection results.\n(4/8) https://t.co/1O1hpGoYBH', 'What if we consider a fixed distribution (eg. the 2d gaussian below) which regular regions are known?🤔\nEven in this case, we find that the status of inlier/outlier of two points can be swapped with a continuous invertible map.\n(5/8) https://t.co/SwrZPA3nXY', 'Density only conveys meaningful information for anomaly detection in a particular representation space. Density-based methods therefore need to make this underlying hypothesis explicit for reliable anomaly detection.  \n(6/8)', 'We cannot fix density-based anomaly detection with more data 📊 or capacity 🗄 but we might require additional *prior knowledge*, e.g., about the task or meaning of the data.\n(7/8)', 'Excited to talk about our work at ICBINB @ #NeurIPS2020 workshop this Saturday!🌟\nPoster: https://t.co/sNV17q4PP1\nSlides:  https://t.co/6fxhrZqBeW\nWorkshop: https://t.co/QDuWz96381\n(8/8)']",https://arxiv.org/abs/2012.03808,"Thanks to the tractability of their likelihood, several deep generative models show promise for seemingly straightforward but important applications like anomaly detection, uncertainty estimation, and active learning. However, the likelihood values empirically attributed to anomalies conflict with the expectations these proposed applications suggest. In this paper, we take a closer look at the behavior of distribution densities through the lens of reparametrization and show that these quantities carry less meaningful information than previously thought, beyond estimation issues or the curse of dimensionality. We conclude that the use of these likelihoods for anomaly detection relies on strong and implicit hypotheses, and highlight the necessity of explicitly formulating these assumptions for reliable anomaly detection. ",Perfect density models cannot guarantee anomaly detection
102,1336489909721296897,60893773,James Bullock,"['New paper by @UCIPhysAstro PhD student @DarthLazar Alex Lazar - how does clustering affect gravitational lensing searches for starless, low-mass dark matter halos?  Quite a bit!  w/ @MBKplus  Robert Feldmann, Onur Çatmabacak, &amp; Leonidas Moustakas\n\n<LINK> <LINK>', ""Orientation of the lens-host matters a lot too... Not accounting for this could potentially make the difference between thinking you've ruled out or confirmed predictions from standard dark matter model. https://t.co/r1pP7BQZ8F""]",https://arxiv.org/abs/2012.03958,"A promising route for revealing the existence of dark matter structures on mass scales smaller than the faintest galaxies is through their effect on strong gravitational lenses. We examine the role of local, lens-proximate clustering in boosting the lensing probability relative to contributions from substructure and unclustered line-of-sight (LOS) halos. Using two cosmological simulations that can resolve halo masses of $M_{\rm halo} \simeq 10^{9}\ M_{\odot}$ (in a simulation box of length $L_{\rm box}{\sim}100\,{\rm Mpc}$) and $10^{7}\ M_{\odot}$ ($L_{\rm box}\sim20\,{\rm Mpc}$), we demonstrate that clustering in the vicinity of the lens host produces a clear enhancement relative to an assumption of unclustered halos that persists to $> 20\,R_{\rm vir}$. This enhancement exceeds estimates that use a two-halo term to account for clustering, particularly within $2-5\,R_{\rm vir}$. We provide an analytic expression for this excess, clustered contribution. We find that local clustering boosts the expected count of $10^9 \ M_\odot$ perturbing halos by ${\sim}35\%$ compared to substructure alone, a result that will significantly enhance expected signals for low-redshift ($z_l \simeq 0.2$) lenses, where substructure contributes substantially compared to LOS halos. We also find that the orientation of the lens with respect to the line of sight (e.g., whether the line of sight passes through the major axis of the lens) can also have a significant effect on the lensing signal, boosting counts by an additional $\sim 50\%$ compared to a random orientations. This could be important if discovered lenses are biased to be oriented along their principal axis. ","Out of sight, out of mind? The impact of correlated clustering in
  substructure lensing"
103,1336488935489495040,951137934643814400,Lionel Briand,"['If you are interested in App testing, see this new, fresh out-of-the-press paper, just submitted paper: ""Automated, Cost-effective, and Update-driven App Testing"", from Chanh-Duc Ngo (his thesis), @FabrizioPastore, yours truly. \n\n<LINK>\n\nComments are welcome.', '""... a model-based approach that synthesizes App models with static analysis, integrates a dynamically-refined state abstraction function and combines complementary testing strategies ...""']",https://arxiv.org/abs/2012.02471,"Apps' pervasive role in our society led to the definition of test automation approaches to ensure their dependability. However, state-of-the-art approaches tend to generate large numbers of test inputs and are unlikely to achieve more than 50% method coverage. In this paper, we propose a strategy to achieve significantly higher coverage of the code affected by updates with a much smaller number of test inputs, thus alleviating the test oracle problem. More specifically, we present ATUA, a model-based approach that synthesizes App models with static analysis, integrates a dynamically-refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval. Its model-based strategy enables ATUA to generate a small set of inputs that exercise only the code affected by the updates. In turn, this makes common test oracle solutions more cost-effective as they tend to involve human effort. A large empirical evaluation, conducted with 72 App versions belonging to nine popular Android Apps, has shown that ATUA is more effective and less effort intensive than state-of-the-art approaches when testing App updates. ","Automated, Cost-effective, and Update-driven App Testing"
104,1336453971032981504,980073199332282369,Toshihiko Yamasaki,"['A new paper on arXiv.\n\nSparse Fooling Images: Fooling Machine Perception through Unrecognizable Images\nSoichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki\n\n<LINK>']",https://arxiv.org/abs/2012.03843,"Fooling images are a potential threat to deep neural networks (DNNs). These images are not recognizable to humans as natural objects, such as dogs and cats, but are misclassified by DNNs as natural-object classes with high confidence scores. Despite their original design concept, existing fooling images retain some features that are characteristic of the target objects if looked into closely. Hence, DNNs can react to these features. In this paper, we address the question of whether there can be fooling images with no characteristic pattern of natural objects locally or globally. As a minimal case, we introduce single-color images with a few pixels altered, called sparse fooling images (SFIs). We first prove that SFIs always exist under mild conditions for linear and nonlinear models and reveal that complex models are more likely to be vulnerable to SFI attacks. With two SFI generation methods, we demonstrate that in deeper layers, SFIs end up with similar features to those of natural images, and consequently, fool DNNs successfully. Among other layers, we discovered that the max pooling layer causes the vulnerability against SFIs. The defense against SFIs and transferability are also discussed. This study highlights the new vulnerability of DNNs by introducing a novel class of images that distributes extremely far from natural images. ",Are DNNs fooled by extremely unrecognizable images?
105,1336337255837749250,1439446945,Lav Varshney,"['New paper using reinforcement learning to edit infrastructure networks like bus routes in Chicago to improve equitable access by racial group to schools, grocery stores, voting booths, etc. @SFResearch @ivanbrugere @CaimingXiong @CSL_Illinois @ECEILLINOIS  <LINK>', 'There is also a part on augmenting social networks, e.g. in universities via ""buddy lunches"", to yield equitable access to people with valuable information within the network.', 'Rather than macroeconomic policy like equitable tax codes that @SFResearch @StephanZheng pursued via reinforcement learning in the AI Economist (https://t.co/jJ8yaG7rYy), here the idea is to consider more mesoscale problems of equity.', 'For #ReproducibleResearch, there is also code available @SFResearch https://t.co/AjbhXzIJVl']",https://arxiv.org/abs/2012.03900,"Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of $(1-\tfrac{1}{3e})$. We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups. ",GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning
106,1336215414582505473,138781058,Dr. André Gaul,"['🆕📜 Just published a new research paper on arXiv with Jörg Liesen: we developed mathematical models for measuring how (de-)centralized a Federated Byzantine Agreement System is (like @StellarOrg). ➡️ <LINK>', 'All algorithms are implemented in the Python package https://t.co/vh95QWOn6c and all examples in the paper can be reproduced by running https://t.co/SwWLo4lteI.', '@twig2noise jaa habe ich erst gegen Ende gemerkt 😂', '@twig2noise f strings sind echt super, wie in js die `Running ${name}` strings']",https://arxiv.org/abs/2012.03913,"The federated Byzantine agreement system (FBAS) is a consensus model introduced by Mazi\`eres in 2016 where the participating nodes conceptually form a network, with links between them being established by each node individually and thus in a decentralized way. An important question is whether these decentralized decisions lead to an overall decentralized network. The level of (de-)centralization in a network can be assessed using centrality measures. In this paper we consider three different approaches for obtaining centrality measures for the nodes in an FBAS. Two of them are based on adapting well-known measures based on graphs and hypergraphs to the FBAS context. Since the network structure of an FBAS can be more complex than (usual) graphs or hypergraphs, we also develop a new, problem-adapted centrality measure. This new measure is based on the intactness of nodes, which is an important ingredient of the FBAS model. We illustrate advantages and disadvantages of the three approaches on several computed examples. We have implemented all centrality measures and performed all computations in the Python package Stellar Observatory. ",Centrality of nodes in Federated Byzantine Agreement Systems
107,1336127022154706944,16252640,Matthew FL,"['New paper on arXiv about mFST, a Python library for working with Finite-State Machines with Custom Semirings\nPaper: <LINK>\nCode: <LINK>\nIn the paper, I demonstate how quickly get started with FSTs and how one could mix PyTorch+FSTs <LINK>', 'mFST was originally developed in 2018 for teaching advanced NLP methods with FSTs https://t.co/K2d0dyt5mN\nwith @adveisner', '@SouravDutta91 mFST is a wrapper around OpenFST with goodies that make FSTs in Python pleasant to work with. OpenFST is a very mature FST implementation that supports self-loop. Here is one of the figures from the paper with a self-loop https://t.co/p1drHSoSna', '@wellformedness @sjmielke Sorry about capitalizing it wrong.  Personally I think it looks a little nicer with it capitalized though 🙍']",https://arxiv.org/abs/2012.03437,"This paper introduces mFST, a new Python library for working with Finite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and exposes all of OpenFST's methods for manipulating FSTs. Additionally, mFST is the only Python wrapper for OpenFST that exposes OpenFST's ability to define a custom semirings. This makes mFST ideal for developing models that involve learning the weights on a FST or creating neuralized FSTs. mFST has been designed to be easy to get started with and has been previously used in homework assignments for a NLP class as well in projects for integrating FSTs and neural networks. In this paper, we exhibit mFST API and how to use mFST to build a simple neuralized FST with PyTorch. ","MFST: A Python OpenFST Wrapper With Support for Custom Semirings and
  Jupyter Notebooks"
108,1336104458669813760,385670040,Sutanay Choudhury,['Excited to release a new scientific ML benchmark dataset  and challenge tasks (<LINK>) for modeling intermolecular interactions.  Check out our paper at NeurIPS workshop on Physical Sciences at <LINK> #NeurIPS2020'],https://arxiv.org/abs/2012.00131,"Intermolecular and long-range interactions are central to phenomena as diverse as gene regulation, topological states of quantum materials, electrolyte transport in batteries, and the universal solvation properties of water. We present a set of challenge problems for preserving intermolecular interactions and structural motifs in machine-learning approaches to chemical problems, through the use of a recently published dataset of 4.95 million water clusters held together by hydrogen bonding interactions and resulting in longer range structural patterns. The dataset provides spatial coordinates as well as two types of graph representations, to accommodate a variety of machine-learning practices. ","HydroNet: Benchmark Tasks for Preserving Intermolecular Interactions and
  Structural Motifs in Predictive and Generative Models for Molecular Data"
109,1336048085840097282,4365927557,Dr. Jake Turner 🌅,"['**New 1st Author Paper***\n\n""Decaying Orbit of the Hot Jupiter WASP-12b: Confirmation with TESS Observations"" \n\nPaper can be found here: <LINK>\n\nCo-authors: @AstroAndrew123 @DrRayJay \n\nTHREAD/\n@CSInst @CornellAstro @Cornell @NASA_TESS \n#Exoplanets #Astronomy @NASA <LINK>', 'WASP-12b is an ultra-hot Jupiter (~2600 K) that has been studied extensively since 2009. It is thought to have clouds made of corundum. Also, the atmosphere of the planet is escaping &amp; will completely evaporate in 300 Myr. \n\nBut what about its orbit? \n1/ https://t.co/QNqnh6d2Lq', 'Maciejewski et al. 2016 (https://t.co/uWu5gge7ob) were the first to report evidence of a decreasing orbital period for WASP-12b. \n\nFollow-up observations by Yee et al. 2020\n(https://t.co/6bPk0hiZbe) determined conclusively that the orbit of WASP-12b was decaying. \n\n2/ https://t.co/PZyBYC8BYj', 'Observations of orbital decay on close-in planets  enhances our understanding of the hot Jupiter population and their evolution \n\n3/ https://t.co/ZqgJSEvJA5', 'Motivated by these indications of WASP-12b’s changing period, my team (@AstroAndrew123, @DrRayJay) decided to use observations from @NASA’s @NASA_TESS to verify its orbital decay, and derive updated orbital parameters and planetary properties. \n\n4/ https://t.co/QxLoEKm93c', 'We modeled each individual transit (the phase folded light curve is below). We also detected the occultation of the planet but only after binning the entire @NASA_TESS data \n\nThese data were modeled with the EXOplanet Modeling Package (EXOMOP) developed in my PhD\n\n5/ https://t.co/TKKGKPx2UO', 'We found that the timing data from the transits &amp; occultation from @NASA_TESS confirmed the decaying orbit of WASP-12b!  \n\nWe compared the decaying orbit model with an apsidal precession model and the decaying orbit model fit the data much better.  \n\n6/ https://t.co/NRbp7Nbamx', 'Our finding indicates an orbital decay lifetime of 2.9 Myr, shorter than the estimated mass-loss timescale of 300 Myr. \n\nWe also find a modified tidal quality factor of Q’⋆ = 1.39×10^5. The cause of such a low tidal quality factor requires additional theoretical work\n\n7/ https://t.co/DkGOqDdtoD', 'Our study highlights the power of long-term high-precision ground &amp; space-based transit &amp; occultation observations for understanding the orbital evolution of close-in giant planets.\n\nIt will be interesting to see what other discoveries @NASA_TESS will make in the future \n\n8/8', '@decaelus @NASA_TESS Thanks @decaelus!', '@wilson_cauley @NASA_TESS Thanks!', '@PlavchanPeter @decaelus @NASA_TESS Thanks Peter!']",https://arxiv.org/abs/2012.02211,"Theory suggests that the orbits of some close-in giant planets should decay due to tidal interactions with their host stars. To date, WASP-12b is the only hot Jupiter reported to have a decaying orbit, at a rate of 29$\pm$2 msec year$^{-1}$. We analyzed data from NASA's Transiting Exoplanet Survey Satellite (TESS) to verify that WASP-12b's orbit is indeed changing. We find that the TESS transit and occultation data are consistent with a decaying orbit with an updated period of 1.091420090$\pm$0.000000041 days and a decay rate of 32.53$\pm$1.62 msec year$^{-1}$. We find an orbital decay timescale of $\tau$ = P/$|\dot P|$ = 2.90$\pm$0.14 Myr. If the observed decay results from tidal dissipation, the modified tidal quality factor is Q'$_{\star}$ = 1.39$\pm$0.15 $\times 10^{5}$, which falls at the lower end of values derived for binary star systems and hot Jupiters. Our result highlights the power of space-based photometry for investigating the orbital evolution of short-period exoplanets. ","Decaying Orbit of the Hot Jupiter WASP-12b: Confirmation with TESS
  Observations"
110,1336014851882110976,20810416,Dr. Roman Yampolskiy,['New paper on AI observatories <LINK> #AIsafety'],https://arxiv.org/abs/2012.02592,"In the last years, AI safety gained international recognition in the light of heterogeneous safety-critical and ethical issues that risk overshadowing the broad beneficial impacts of AI. In this context, the implementation of AI observatory endeavors represents one key research direction. This paper motivates the need for an inherently transdisciplinary AI observatory approach integrating diverse retrospective and counterfactual views. We delineate aims and limitations while providing hands-on-advice utilizing concrete practical examples. Distinguishing between unintentionally and intentionally triggered AI risks with diverse socio-psycho-technological impacts, we exemplify a retrospective descriptive analysis followed by a retrospective counterfactual risk analysis. Building on these AI observatory tools, we present near-term transdisciplinary guidelines for AI safety. As further contribution, we discuss differentiated and tailored long-term directions through the lens of two disparate modern AI safety paradigms. For simplicity, we refer to these two different paradigms with the terms artificial stupidity (AS) and eternal creativity (EC) respectively. While both AS and EC acknowledge the need for a hybrid cognitive-affective approach to AI safety and overlap with regard to many short-term considerations, they differ fundamentally in the nature of multiple envisaged long-term solution patterns. By compiling relevant underlying contradistinctions, we aim to provide future-oriented incentives for constructive dialectics in practical and theoretical AI safety research. ","Transdisciplinary AI Observatory -- Retrospective Analyses and
  Future-Oriented Contradistinctions"
111,1335985080590864384,773986416,Nicholas Boardman,['New paper of mine out on @arxiv today. How do gas-phase metallicity gradients vary across the mass-size plane in galaxies? Find out at <LINK>'],https://arxiv.org/abs/2012.02362,"Gas-phase abundances and abundance gradients provide much information on past stellar generations, and are powerful probes of how galaxies evolve. Gas abundance gradients in galaxies have been studied as functions of galaxies' mass and size individually, but have largely not been considered across the galaxy mass--size plane. Thus, we investigate gas-phase abundance gradients across this plane, using a sample of over 1000 galaxies selected from the MApping Nearby Galaxies at APO (MaNGA) spectroscopic survey. We find that gradients vary systematically such that above $10^{10}M_{\odot}$, smaller galaxies display flatter gradients than larger galaxies at a given stellar mass. This mass--size behaviour cannot be explained by instrumental effects, nor is it simply a reflection of known trends between gradients and morphology. We explore multiple possibilities for a physical origin for this pattern, though further work is needed to establish a firm physical interpretation. ","SDSS-IV MaNGA: galaxy gas-phase metallicity gradients vary across the
  mass-size plane"
112,1335979963124576257,28840722,Phil Long,"['New paper with @niladrichat and Peter Bartlett, called ""When does gradient descent with logistic loss find interpolating two-layer networks?"": <LINK>.']",https://arxiv.org/abs/2012.02409,"We study the training of finite-width two-layer smoothed ReLU networks for binary classification using the logistic loss. We show that gradient descent drives the training loss to zero if the initial loss is small enough. When the data satisfies certain cluster and separation conditions and the network is wide enough, we show that one step of gradient descent reduces the loss sufficiently that the first result applies. ","When does gradient descent with logistic loss find interpolating
  two-layer networks?"
113,1335820995500896257,336253721,Allen Schmaltz,"['In the NEW paper in the series (<LINK>), ""Coarse-to-Fine Memory Matching for Joint Retrieval and Classification"", we extend non-parametric memory matching to the more general setting of retrieval-classification tasks. ... 1/3 <LINK>', 'Via a coarse-to-fine search procedure across dense representations, for learning and inference, we can train a SINGLE, SHARED Transformer LM for both retrieval and classification, and ... 2/3', '... at inference, we can leverage the distances and representations from coupled retrieval and classification to analyze, constrain, and update the data/model. 3/3']",http://arxiv.org/abs/2012.02287,"We present a novel end-to-end language model for joint retrieval and classification, unifying the strengths of bi- and cross- encoders into a single language model via a coarse-to-fine memory matching search procedure for learning and inference. Evaluated on the standard blind test set of the FEVER fact verification dataset, classification accuracy is significantly higher than approaches that only rely on the language model parameters as a knowledge base, and approaches some recent multi-model pipeline systems, using only a single BERT base model augmented with memory layers. We further demonstrate how coupled retrieval and classification can be leveraged to identify low confidence instances, and we extend exemplar auditing to this setting for analyzing and constraining the model. As a result, our approach yields a means of updating language model behavior through two distinct mechanisms: The retrieved information can be updated explicitly, and the model behavior can be modified via the exemplar database. ",Coarse-to-Fine Memory Matching for Joint Retrieval and Classification
114,1334962057964367873,753984416293216256,Michele Celebrano @sNOm Lab,"['This is the most challenging paper I wrote. Many results to be cross-checked and no knowledge about graphene to start. But also a great experience that allowed me to learn new things (I hope)... this is thanks to an incredible crew of people! <LINK> @GrapheneEU <LINK>', '@michebad @GrapheneEU @GrapheneUCam @CerulloLab @polini_marco Noooo... Diciamo che è stato un piacevole fuoripista. Anche se abbastanza difficoltoso data la mia ignoranza nel campo. Ma mercoledì già misuro un altro 2D. Ma sempre per passatempo... Mica roba seria! 😉']",https://arxiv.org/abs/2012.01779,"Graphene is an ideal material for integrated nonlinear optics thanks to its strong light-matter interaction and large nonlinear optical susceptibility. Graphene has been used in optical modulators, saturable absorbers, nonlinear frequency converters, and broadband light emitters. For the latter application, a key requirement is the ability to control and engineer the emission wavelength and bandwidth, as well as the electronic temperature of graphene. Here, we demonstrate that the emission wavelength of graphene$'$ s broadband hot carrier photoluminescence can be tuned by integration on photonic cavities, while thermal management can be achieved by out-of-plane heat transfer to hexagonal boron nitride. Our results pave the way to graphene-based ultrafast broadband light emitters with tunable emission. ",Tunable broadband light emission from graphene
115,1334947983306207239,924816072036904960,Adam Gleave,"['Have you ever had a reward or imitation learning algorithm not work as expected? Diagnostic tasks can help! In a new paper with Pedro Freire et al we develop a suite of tasks, DERAIL, to quickly test individual facets of algorithms. <LINK> <LINK>']",https://arxiv.org/abs/2012.01365,"The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at this https URL . ",DERAIL: Diagnostic Environments for Reward And Imitation Learning
116,1334942562486951936,52360091,Javier M. Duarte,['New paper led by @iris_hep fellows to be presented at #ML4PS Workshop at #NeurIPS2020 implementing graph neural networks on #FPGAs for co-processing and sub-microsecond trigger applications at the #LHC <LINK> <LINK> <LINK>'],http://arxiv.org/abs/2012.01563,"We develop and study FPGA implementations of algorithms for charged particle tracking based on graph neural networks. The two complementary FPGA designs are based on OpenCL, a framework for writing programs that execute across heterogeneous platforms, and hls4ml, a high-level-synthesis-based compiler for neural network to firmware conversion. We evaluate and compare the resource usage, latency, and tracking performance of our implementations based on a benchmark dataset. We find a considerable speedup over CPU-based execution is possible, potentially enabling such algorithms to be used effectively in future computing workflows and the FPGA-based Level-1 trigger at the CERN Large Hadron Collider. ","Accelerated Charged Particle Tracking with Graph Neural Networks on
  FPGAs"
117,1334701644509044738,1284197401,John ZuHone,['New paper! “How merger-driven gas motions in galaxy clusters can turn AGN bubbles into radio relics” <LINK> <LINK>'],http://arxiv.org/abs/2012.02001,"Radio relics in galaxy clusters are extended synchrotron sources produced by cosmic-ray electrons in the $\mu$G magnetic field. Many relics are found in the cluster periphery and have a cluster-centric, narrow arc-like shape, which suggests that the electrons are accelerated or re-accelerated by merger shock fronts propagating outward in the intracluster plasma. In the X-ray, some relics do exhibit such shocks at the location of the relic, but many do not. We explore the possibility that radio relics trace not the shock fronts but the shape of the underlying distribution of seed relativistic electrons, lit up by a recent shock passage. We use magnetohydrodynamic simulations of cluster mergers and include bubbles of relativistic electrons injected by jets from the central AGN or from an off-center radio galaxy. We show that the merger-driven gas motions (a) can advect the bubble cosmic rays to very large radii, and (b) spread the relativistic seed electrons preferentially in tangential direction -- along the gravitational equipotential surfaces -- producing extended, filamentary or sheet-like regions of intracluster plasma enriched with aged cosmic rays, which resemble radio relics. Once a shock front passes across such a region, the sharp radio emission edges would trace the sharp boundaries of these enriched regions rather than the front. We also show that these elongated cosmic ray features are naturally associated with magnetic fields stretched tangentially along their long axis, which could help explain the high polarization of relics. ","How merger-driven gas motions in galaxy clusters can turn AGN bubbles
  into radio relics"
118,1334682172142194692,18262687,Rushil,"['New work accepted to #aaai2021:  Making models more robust to unknown ""natural"" perturbations at test-time (SOTA on CIFAR-C). Work led by @trgokhale as part of a very productive summer at LLNL🙂 with @bkailkhu @jjayaram7 @Yezhou_Yang @cbaral \npaper: <LINK> <LINK>', '@bkspears9 @trgokhale @bkailkhu @jjayaram7 @Yezhou_Yang @cbaral Thanks Brian!! 😊']",https://arxiv.org/abs/2012.01806,"While existing work in robust deep learning has focused on small pixel-level norm-based perturbations, this may not account for perturbations encountered in several real-world settings. In many such cases although test data might not be available, broad specifications about the types of perturbations (such as an unknown degree of rotation) may be known. We consider a setup where robustness is expected over an unseen test domain that is not i.i.d. but deviates from the training domain. While this deviation may not be exactly known, its broad characterization is specified a priori, in terms of attributes. We propose an adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to the attributes-space, without having access to the data from the test domain. Our adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial perturbations, and the outer minimization finding model parameters by optimizing the loss on adversarial perturbations generated from the inner maximization. We demonstrate the applicability of our approach on three types of naturally occurring perturbations -- object-related shifts, geometric transformations, and common image corruptions. Our approach enables deep neural networks to be robust against a wide range of naturally occurring perturbations. We demonstrate the usefulness of the proposed approach by showing the robustness gains of deep neural networks trained using our adversarial training on MNIST, CIFAR-10, and a new variant of the CLEVR dataset. ","Attribute-Guided Adversarial Training for Robustness to Natural
  Perturbations"
119,1334593115177451520,1236845758386667520,Trevor McCourt,"['New paper up on arxiv! We use floquet dynamics to estimate the control parameters of multi-qubit quantum algorithms with quantum-limited precision. For example, we can estimate iSWAP angles with a standard deviation on the order of 10^-5. \n\n<LINK>', 'One of the most amazing things to me is that we could accurately model the device using the Lindblad equation with T1 and T2. Below shows data and a fit model for spin chain population oscillations.  This implies a lack of significant drift over practical timescales. https://t.co/b9TLP1uXEE', 'This allows us to perform parameter estimation out to large circuit depths. Left shows a Lindblad equation fit to 4 qubit data, right shows the uncertainly in parameter estimation vs circuit depth. Uncertainty is limited in a fundamental way by interaction with the environment. https://t.co/5bI6ifDieh', ""@dabacon I remember when you first interviewed me at google I said I wanted to work on calibration... well now I've done it! ;)"", '@dabacon @quantumVerd @dabacon if only the same was true for my real cars. Early 2000s GM ownership is fun.', '@BobNiffenegger Thanks Robert! Although when you have guys like Vadim and Lev on the team it’s hard not to work on condensed matter problems.']",https://arxiv.org/abs/2012.00921,"A promising approach to study condensed-matter systems is to simulate them on an engineered quantum platform. However, achieving the accuracy needed to outperform classical methods has been an outstanding challenge. Here, using eighteen superconducting qubits, we provide an experimental blueprint for an accurate condensed-matter simulator and demonstrate how to probe fundamental electronic properties. We benchmark the underlying method by reconstructing the single-particle band-structure of a one-dimensional wire. We demonstrate nearly complete mitigation of decoherence and readout errors and arrive at an accuracy in measuring energy eigenvalues of this wire with an error of ~0.01 rad, whereas typical energy scales are of order 1 rad. Insight into this unprecedented algorithm fidelity is gained by highlighting robust properties of a Fourier transform, including the ability to resolve eigenenergies with a statistical uncertainty of 1e-4 rad. Furthermore, we synthesize magnetic flux and disordered local potentials, two key tenets of a condensed-matter system. When sweeping the magnetic flux, we observe avoided level crossings in the spectrum, a detailed fingerprint of the spatial distribution of local disorder. Combining these methods, we reconstruct electronic properties of the eigenstates where we observe persistent currents and a strong suppression of conductance with added disorder. Our work describes an accurate method for quantum simulation and paves the way to study novel quantum materials with superconducting qubits. ",Accurately computing electronic properties of a quantum ring
120,1334557901508071424,862286842128945152,Sam Mugel,"['New paper out on the arXiv today, showing some really nice results we obtained with @Bankia \n<LINK>\nYou can support our paper by voting here :)\n<LINK>']",https://arxiv.org/abs/2012.01091,"In this paper we propose a hybrid quantum-classical algorithm for dynamic portfolio optimization with minimal holding period. Our algorithm is based on sampling the near-optimal portfolios at each trading step using a quantum processor, and efficiently post-selecting to meet the minimal holding constraint. We found the optimal investment trajectory in a dataset of 50 assets spanning a one year trading period using the D-Wave 2000Q processor. Our method is remarkably efficient, and produces results much closer to the efficient frontier than typical portfolios. Moreover, we also show how our approach can easily produce trajectories adapted to different risk profiles, as typically offered in financial products. Our results are a clear example of how the combination of quantum and classical techniques can offer novel valuable tools to deal with real-life problems, beyond simple toy models, in current NISQ quantum processors. ",Hybrid Quantum Investment Optimization with Minimal Holding Period
121,1334517153270280198,863828060600139776,Dr. Deep Anand,"['paper day! \n\n<LINK>\n\nhere we (myself, @janiceleeastro, and many others) present a catalog of distances for PHANGS, including  new TRGB distances that are derived from PHANGS-HST imaging. <LINK>', 'PHANGS (Physics at High Angular resolution in Nearby GalaxieS) is a large collaboration focused on understanding the connections between young stars, H II regions, and cold molecular gas in nearby star-forming galaxies. \n\nhttps://t.co/aVBYkkMzah https://t.co/SuZ1x61WY0', 'PHANGS-HST (a subset of the larger PHANGS program) is an ultraviolet-optical imaging survey of 38 spiral galaxies within ∼20 Mpc. We use the parallel ACS fields from PHANGS-HST to derive 11 new tip of the red giant branch (TRGB) distances to these galaxies. https://t.co/tqniqMBujw', 'on the near end, we find highly precise distances to galaxies such as NGC 4826. https://t.co/oMAHxtmq7J', 'on the far end, we are able to derive distances to galaxies out into the Virgo Cluster! (like NGC 4321) https://t.co/HbIlgWeTn2', 'we also provide a catalog of best available distances to the full extended PHANGS sample of galaxies (118 targets). https://t.co/TqXqoK6Lve', '@TSpriggs @janiceleeastro thank you!', '@BenneHolwerda thank you!!']",https://arxiv.org/abs/2012.00757,"PHANGS-HST is an ultraviolet-optical imaging survey of 38 spiral galaxies within ~20 Mpc. Combined with the PHANGS-ALMA, PHANGS-MUSE surveys and other multiwavelength data, the dataset will provide an unprecedented look into the connections between young stars, HII regions, and cold molecular gas in these nearby star-forming galaxies. Accurate distances are needed to transform measured observables into physical parameters (e.g., brightness to luminosity, angular to physical sizes of molecular clouds, star clusters and associations). PHANGS-HST has obtained parallel ACS imaging of the galaxy halos in the F606W and F814W bands. Where possible, we use these parallel fields to derive tip of the red giant branch (TRGB) distances to these galaxies. In this paper, we present TRGB distances for 11 galaxies from ~4 to ~15 Mpc, based on the first year of PHANGS-HST observations. Five of these represent the first published TRGB distance measurements (IC 5332, NGC 2835, NGC 4298, NGC 4321, and NGC 4328), and eight of which are the best available distances to these targets. We also provide a compilation of distances for the 118 galaxies in the full PHANGS sample, which have been adopted for the first PHANGS-ALMA public data release. ","Distances to PHANGS Galaxies: New Tip of the Red Giant Branch
  Measurements and Adopted Distances"
122,1334462594858553344,1951616070,Mattia Rigotti,"['Happy that our paper ""Self-correcting Q-learning"" co-authored with Rong Zhu (Columbia U. &amp; Fudan U.) was accepted at #AAAI2021 🎉\n\nWe propose a new value estimator that overcomes the maximization bias of Q-learning and show it can improve Deep RL.\n\nPaper: <LINK> <LINK>', '@LLogiaco @IBMResearch Thanks Laureline! The best part was clearly the excuse to work and keep in touch with a common friend of ours 😄']",https://arxiv.org/abs/2012.01100,"The Q-learning algorithm is known to be affected by the maximization bias, i.e. the systematic overestimation of action values, an important issue that has recently received renewed attention. Double Q-learning has been proposed as an efficient algorithm to mitigate this bias. However, this comes at the price of an underestimation of action values, in addition to increased memory requirements and a slower convergence. In this paper, we introduce a new way to address the maximization bias in the form of a ""self-correcting algorithm"" for approximating the maximum of an expected value. Our method balances the overestimation of the single estimator used in conventional Q-learning and the underestimation of the double estimator used in Double Q-learning. Applying this strategy to Q-learning results in Self-correcting Q-learning. We show theoretically that this new algorithm enjoys the same convergence guarantees as Q-learning while being more accurate. Empirically, it performs better than Double Q-learning in domains with rewards of high variance, and it even attains faster convergence than Q-learning in domains with rewards of zero or low variance. These advantages transfer to a Deep Q Network implementation that we call Self-correcting DQN and which outperforms regular DQN and Double DQN on several tasks in the Atari 2600 domain. ",Self-correcting Q-Learning
123,1334449690943819776,1263870728870469632,Enrico Ronca,['Our new paper on Cavity-induced effects on Intermolecular Interactions is out on ArXiv:\n\n<LINK>'],https://arxiv.org/abs/2012.01080,"Intermolecular bonds are weak compared to covalent bonds, but they are strong enough to influence the properties of large molecular systems. In this work, we investigate how strong light-matter coupling inside an optical cavity can modify these intermolecular forces. We perform a detailed comparison between currently available ab initio electron-photon methodologies. The electromagnetic field inside the cavity can modulate the ground state properties of weakly bound complexes. Controlling the field polarization, the interactions can be stabilized or destabilized, and electron densities, dipole moments, and polarizabilities can be altered. We demonstrate that electron-photon correlation is fundamental to describe intermolecular interactions in strong light-matter coupling. This work proposes optical cavities as a novel tool to manipulate and control ground state properties, solvent effects, and intermolecular interactions for molecules and materials. ",Intermolecular interactions in optical cavities: an ab initio QED study
124,1334432619979792384,772809603046334464,Jonathan Mackey,"['It was fun to work on this new paper led by @LucaGrassitelli from @UniBonn, trying to understand and simulate 10-year cyclic variations in Luminous Blue Variables. Plus North-South collaboration with our friends @ArmaghPlanet, @AstroAndreas and @jorick73.\n<LINK>']",https://arxiv.org/abs/2012.00023,"Luminous blue variables (LBVs) are hot, very luminous massive stars displaying large quasi-periodic variations in brightness, radius,and photospheric temperature, on timescales of years to decades. The physical origin of this variability, called S Doradus cycle after its prototype, has remained elusive. Here, we study the feedback of stellar wind mass-loss on the envelope structure in stars near the Eddington limit. We perform a time-dependent hydrodynamic stellar evolutionary calculation, applying a stellar wind mass-loss prescription with a temperature-dependence inspired by the predicted systematic increase in mass-loss rates below 25 kK. We find that when the wind mass-loss rate crosses a well-defined threshold, a discontinuous change in the wind base conditions leads to a restructuring of the stellar envelope. The induced drastic radius and temperature changes, which occur on the thermal timescale of the inflated envelope, impose in turn mass-loss variations that reverse the initial changes, leading to a cycle that lacks a stationary equilibrium configuration. Our proof-of-concept model broadly reproduces the typical observational phenomenology of the S Doradus variability. We identify three key physical ingredients needed to trigger the instability: inflated envelopes in close proximity to the Eddington limit, a temperature range where decreasing opacities do not lead to an accelerating outflow, and a mass-loss rate that increases with decreasing temperature, crossing a critical threshold value within this temperature range. Our scenario and model provide testable predictions, and open the door for a consistent theoretical treatment of the LBV phase in stellar evolution, with consequences for their further evolution as single stars or in binary systems. ","Wind-envelope interaction as the origin of the slow cyclic brightness
  variations of luminous blue variables"
125,1334409268276105217,4187869443,Elisa Costantini,['today is paper day! A Bayesian approach (+ @SPEX_Xray  new tool) to study narrow absorption lines in X-ray spectra. Check the case study on the ionised gas in our Galaxy! Rogantini et al. (@danieleroga) \n<LINK>\n@SRON_Space'],https://arxiv.org/abs/2012.01381,"High ionisation lines in the soft X-ray band are generally associated to either interstellar hot gas along the line of sight or to photoionised gas intrinsic to the source. In the low-mass X-ray binary 4U 1820-30, the nature of these lines is not well understood. We characterised the ionised gas present along the line of sight towards the source producing the X-ray absorption lines of Mg XI, Ne IX, Fe XVII, O VII, and O VIII. We analysed all the observations available for this source in the XMM-Newton and Chandra archives, taken with the RGS, HETG, and LETG spectrometers. The high-resolution grating spectra have been accurately examined through a standard X-ray analysis based on the $C$-statistic and through the Bayesian parameter inference. We tested two physical models which describe a plasma in either collisional ionisation or photoionisation equilibrium. We adopted the Bayesian model comparison to statistically compare the different combinations of the models used for the analysis.} We found that the lines are consistent with hot gas in the interstellar medium rather than being associated with the intrinsic gas of the X-ray binary. Our best-fit model reveals the presence of a collisionally ionised plasma with a temperature of $T=(1.98\pm0.05)\times10^6 $ K. The photoionisation model fails to fit the Fe XVII line (which is detected with a significance of $6.5\sigma$) due to the low column density predicted by the model. Moreover, the low inclination of the binary system is likely the reason for the non-detection of ionised gas intrinsic to the source.} ",The hot interstellar medium towards 4U 1820-30: a Bayesian analysis
126,1334389947265126401,802543221943439360,Andrea Caputo,"['<LINK>\n\nPaper out, pretty excited with it 🚦🚦🚦\nWith Jose Luis Bernal and  Marc Kamionkowski we propose a new way to detect dark matter decay using Line Intensity Mapping! We essentially suggest to treat the dark matter as a line interloper and look for it 🧐🧐 <LINK>']",https://arxiv.org/abs/2012.00771,"The nature of dark matter is a longstanding mystery in cosmology, which can be studied with laboratory or collider experiments, as well as astrophysical and cosmological observations. In this work, we propose realistic and efficient strategies to detect radiative products from dark-matter decays with line-intensity mapping (LIM) experiments. This radiation will behave as a line interloper for the atomic and molecular spectral lines targeted by LIM surveys. The most distinctive signatures of the contribution from dark-matter radiative decays are an extra anisotropy on the LIM power spectrum due to projection effects, as well as a narrowing and a shift towards higher intensities of the voxel intensity distribution. We forecast the minimum rate of decays into two photons that LIM surveys will be sensitive to as function of the dark-matter mass in the range $\sim 10^{-6}-10$ eV, and discuss how to reinterpret such results for dark matter that decays into a photon and another particle. We find that both the power spectrum and the voxel intensity distribution are expected to be very sensitive to the dark-matter contribution, with the voxel intensity distribution being more promising for most experiments considered. Interpreting our results in terms of the axion, we show that LIM surveys will be extremely competitive to detect its decay products, improving several orders of magnitudes (depending on the mass) the sensitivity of laboratory and astrophysical searches, especially in the mass range $\sim 1-10$ eV. ",Strategies to Detect Dark-Matter Decays with Line-Intensity Mapping
127,1334324629117145088,820077362,Risa Wechsler,"['Really excited about this paper led by my student Elise Darragh-Ford. We used a new method applied to Gaia DR2 to detect possible Milky Way satellites in 4D position-proper motion space. Already super cool, but about to get even better w Gaia EDR3! <LINK>']",https://arxiv.org/abs/2012.00099,"We present a wavelet-based algorithm to identify dwarf galaxies in the Milky Way in ${\it Gaia}$ DR2 data. Our algorithm detects overdensities in 4D position--proper motion space, making it the first search to explicitly use velocity information to search for dwarf galaxy candidates. We optimize our algorithm and quantify its performance by searching for mock dwarfs injected into ${\it Gaia}$ DR2 data and for known Milky Way satellite galaxies. Comparing our results with previous photometric searches, we find that our search is sensitive to undiscovered systems at Galactic latitudes~$\lvert b\rvert>20^{\circ}$ and with half-light radii larger than the 50% detection efficiency threshold for Pan-STARRS1 (PS1) at (${\it i}$) absolute magnitudes of =$-7<M_V<-3$ and distances of $32$ kpc $< D < 64$ kpc, and (${\it ii}$) $M_V< -4$ and $64$ kpc $< D < 128$ kpc. Based on these results, we predict that our search is expected to discover $5 \pm 2$ new satellite galaxies: four in the PS1 footprint and one outside the Dark Energy Survey and PS1 footprints. We apply our algorithm to the ${\it Gaia}$ DR2 dataset and recover $\sim 830$ high-significance candidates, out of which we identify a ""gold standard"" list of $\sim 200$ candidates based on cross-matching with potential candidates identified in a preliminary search using ${\it Gaia}$ EDR3 data. All of our candidate lists are publicly distributed for future follow-up studies. We show that improvements in astrometric measurements provided by ${\it Gaia}$ EDR3 increase the sensitivity of this technique; we plan to continue to refine our candidate list using future data releases. ","Searching for Dwarf Galaxies in ${\it Gaia}$ DR2 Phase-Space Data Using
  Wavelet Transforms"
128,1334166327750221827,17055506,Martin Kleppmann,"['New paper! 😎 In which @heidiann360 and I explore Git-like hash graphs, Bloom filters, and peer-to-peer systems that are immune to Sybil attacks.\n\n📄 Paper: <LINK>\n📎 Blog post: <LINK>']",https://arxiv.org/abs/2012.00472,"Sybil attacks, in which a large number of adversary-controlled nodes join a network, are a concern for many peer-to-peer database systems, necessitating expensive countermeasures such as proof-of-work. However, there is a category of database applications that are, by design, immune to Sybil attacks because they can tolerate arbitrary numbers of Byzantine-faulty nodes. In this paper, we characterize this category of applications using a consistency model we call Byzantine Eventual Consistency (BEC). We introduce an algorithm that guarantees BEC based on Byzantine causal broadcast, prove its correctness, and demonstrate near-optimal performance in a prototype implementation. ","Byzantine Eventual Consistency and the Fundamental Limits of
  Peer-to-Peer Databases"
129,1334130228512354304,314014164,Adrian Price-Whelan,"['Paper day! <LINK>\n\n""Orbital Torus Imaging"" is a new method for dynamical inference (measuring the Galactic mass / dark matter distribution, etc.) that exploits the existence of element-abundance gradients, like these from @APOGEEsurvey: <LINK>', 'Conceptually, the method words because the element-abundance contours ""show"" us the shapes of orbits, demonstrated here with vertical kinematics of stars (in z–vz) https://t.co/gxAGZADoCz', 'You can think of ""Orbital Torus Imaging"" as a replacement for Jeans modeling that will be more precise and requires fewer assumptions in practice: We only require that orbits are phase-mixed locally in action-space', ""We do a simple demonstration in this paper using @APOGEEsurvey data: Using just 8 element abundances, and only modeling the vertical kinematics of stars (but don't assume separability) we get constraints on the disk mass and scale height that are precise to a few percent https://t.co/qde8OQI18F"", 'Plus, now we finally have a (dynamical) use for all of those millions x 30  element abundances that spectroscopic surveys like @APOGEEsurvey, @galahsurvey, LAMOST, etc. are delivering :)', ""As usual in Galactic dynamics, there are many caveats and we make many assumptions, but I'm excited to see what we will learn when we apply this to larger data sets and with more ambitious / flexible models of the Galactic mass!"", 'Thanks to everyone that contributed and helped with this project along the way! @davidwhogg @kvj_astro @melissakness @FritzZwicky @rareflwr41 + many others']",https://arxiv.org/abs/2012.00015,"Many approaches to galaxy dynamics assume that the gravitational potential is simple and the distribution function is time-invariant. Under these assumptions there are traditional tools for inferring potential parameters given observations of stellar kinematics (e.g., Jeans models). However, spectroscopic surveys measure many stellar properties beyond kinematics. Here we present a new approach for dynamical inference, Orbital Torus Imaging, which makes use of kinematic measurements and element abundances (or other invariant labels). We exploit the fact that, in steady state, stellar labels vary systematically with orbit characteristics (actions), yet must be invariant with respect to orbital phases (conjugate angles). The orbital foliation of phase space must therefore coincide with surfaces along which all moments of all stellar label distributions are constant. Both classical-statistics and Bayesian methods can be built on this; these methods will be more robust and require fewer assumptions than traditional tools because they require no knowledge of the (spatial) survey selection function and they do not involve second moments of velocity distributions. We perform a classical-statistics demonstration with red giant branch stars from the APOGEE surveys: We model the vertical orbit structure in the Milky Way disk to constrain the local disk mass, scale height, and the disk--halo mass ratio (at fixed local circular velocity). We find that the disk mass can be constrained (na\""ively) at the few-percent level with Orbital Torus Imaging using only eight element-abundance ratios, demonstrating the promise of combining stellar labels with dynamical invariants. ","Orbital Torus Imaging: Using Element Abundances to Map Orbits and Mass
  in the Milky Way"
130,1334073061654589440,1184553893721726976,Andreas Bluhm,"['New paper out with Martina Gschwendtner and Andreas Winter: <LINK>. In the article, we show how to build a programmable quantum processor for covariant quantum channels with optimal memory requirements.']",https://arxiv.org/abs/2012.00717,"A programmable quantum processor uses the states of a program register to specify one element of a set of quantum channels which is applied to an input register. It is well-known that such a device is impossible with a finite-dimensional program register for any set that contains infinitely many unitary quantum channels (Nielsen and Chuang's No-Programming Theorem), meaning that a universal programmable quantum processor does not exist. The situation changes if the system has symmetries. Indeed, here we consider group-covariant channels. If the group acts irreducibly on the channel input, these channels can be implemented exactly by a programmable quantum processor with finite program dimension (via teleportation simulation, which uses the Choi-Jamiolkowski state of the channel as a program). Moreover, by leveraging the representation theory of the symmetry group action, we show how to remove redundancy in the program and prove that the resulting program register has minimum Hilbert space dimension. Furthermore, we provide upper and lower bounds on the program register dimension of a processor implementing all group-covariant channels approximately. ",Programmability of covariant quantum channels
131,1334044245720764417,2310538183,Ji Won Park,"['Check out my new paper, [2012.00042] Large-Scale Gravitational Lens Modeling with Bayesian Neural Networks for Accurate and Precise Inference of the Hubble Constant <LINK>', 'It demonstrates that lens modeling with Bayesian neural networks (BNN) is accurate and efficient enough so as to enable unbiased recovery of H0 over hundreds (!) of lenses.', 'We recover the true input H0 at 0.7% precision, from our test set of 200 simulated lenses. Our pipeline is implemented in the open-source Python package ""H0rton"": https://t.co/RAxpOreFEl', 'The BNN method of H0 inference only takes ~10 minutes per lens, compared to ~3 hours for the traditional forward modeling method! It promises to become a key tool in exploring ensemble-level systematics in lens modeling for H0 inference ...', '... as we look to the Rubin Observatory LSST era when we will discover thousands of lenses!']",https://arxiv.org/abs/2012.00042,"We investigate the use of approximate Bayesian neural networks (BNNs) in modeling hundreds of time-delay gravitational lenses for Hubble constant ($H_0$) determination. Our BNN was trained on synthetic HST-quality images of strongly lensed active galactic nuclei (AGN) with lens galaxy light included. The BNN can accurately characterize the posterior PDFs of model parameters governing the elliptical power-law mass profile in an external shear field. We then propagate the BNN-inferred posterior PDFs into ensemble $H_0$ inference, using simulated time delay measurements from a plausible dedicated monitoring campaign. Assuming well-measured time delays and a reasonable set of priors on the environment of the lens, we achieve a median precision of $9.3$\% per lens in the inferred $H_0$. A simple combination of 200 test-set lenses results in a precision of 0.5 $\textrm{km s}^{-1} \textrm{ Mpc}^{-1}$ ($0.7\%$), with no detectable bias in this $H_0$ recovery test. The computation time for the entire pipeline -- including the training set generation, BNN training, and $H_0$ inference -- translates to 9 minutes per lens on average for 200 lenses and converges to 6 minutes per lens as the sample size is increased. Being fully automated and efficient, our pipeline is a promising tool for exploring ensemble-level systematics in lens modeling for $H_0$ inference. ","Large-Scale Gravitational Lens Modeling with Bayesian Neural Networks
  for Accurate and Precise Inference of the Hubble Constant"
132,1333967745872916481,1015053310603284480,Stephen Kane,"['My co-authors (@astro_tiff, @ThomasFauchez, @fselsis, Alma Ceja) and I present our new paper ""Phase Modeling of the TRAPPIST-1 Planetary Atmospheres"", in which we use analytical and climate models to simulate phase signatures of the TRAPPIST-1 planets. <LINK>', 'The compact architecture of the TRAPPIST-1 system combined with mean motion resonances ensures frequent syzygy events, where planets line up along the line-of-sight. As such, even though individual phase signatures may be small, combined phase signatures become detectable.', 'We further used ROCKE-3D to simulate Modern Earth and Archean climate models for TRAPPIST-1 e and f, and integrate these to calculate the phase signatures. These show different symmetry properties in the phase curves that will be challenging but rewarding to detect.', '@jcbastro Thanks Juliette! We did take the results into account in our JWST proposals but it\'s a tough detection that will likely need LUVOIR. For JWST, we\'re mostly (ironically) concerned with how planetary alignments may introduce phase ""noise"" into our secondary eclipse measurements.']",https://arxiv.org/abs/2012.00080,"Transiting compact multi-planet systems provide many unique opportunities to characterize the planets, including studies of size distributions, mean densities, orbital dynamics, and atmospheric compositions. The relatively short orbital periods in these systems ensure that events requiring specific orbital locations of the planets (such as primary transit and secondary eclipse points) occur with high frequency. The orbital motion and associated phase variations of the planets provide a means to constrain the atmospheric compositions through measurement of their albedos. Here we describe the expected phase variations of the TRAPPIST-1 system and times of superior conjunction when the summation of phase effects produce maximum amplitudes. We also describe the infrared flux emitted by the TRAPPIST-1 planets and the influence on the overall phase amplitudes. We further present the results from using the global circulation model ROCKE-3D to model the atmospheres of TRAPPIST-1e and TRAPPIST-1f assuming modern Earth and Archean atmospheric compositions. These simulations are used to calculate predicted phase curves for both reflected light and thermal emission components. We discuss the detectability of these signatures and the future prospects for similar studies of phase variations for relatively faint M stars. ",Phase Modeling of the TRAPPIST-1 Planetary Atmospheres
133,1346204816687685633,1638556578,Daniel Kasenberg,"['New paper on arXiv! w/ @vasanthsarathy, Shivam Goel, Jivko Sinapov and Matthias Scheutz @hrilab, in which we venture into the exciting world of combining classical planning with reinforcement learning. More cool things ahead on the SPOTTER project!\n\n<LINK>', 'RIP to the Oxford comma.']",https://arxiv.org/abs/2012.13037,"Symbolic planning models allow decision-making agents to sequence actions in arbitrary ways to achieve a variety of goals in dynamic domains. However, they are typically handcrafted and tend to require precise formulations that are not robust to human error. Reinforcement learning (RL) approaches do not require such models, and instead learn domain dynamics by exploring the environment and collecting rewards. However, RL approaches tend to require millions of episodes of experience and often learn policies that are not easily transferable to other tasks. In this paper, we address one aspect of the open problem of integrating these approaches: how can decision-making agents resolve discrepancies in their symbolic planning models while attempting to accomplish goals? We propose an integrated framework named SPOTTER that uses RL to augment and support (""spot"") a planning agent by discovering new operators needed by the agent to accomplish goals that are initially unreachable for the agent. SPOTTER outperforms pure-RL approaches while also discovering transferable symbolic knowledge and does not require supervision, successful plan traces or any a priori knowledge about the missing planning operator. ","SPOTTER: Extending Symbolic Planning Operators through Targeted
  Reinforcement Learning"
134,1345211709909135360,727242818452897796,Wenhu Chen,"['Happy New Year! Just discovered a very exciting paper released by Facebook: <LINK>, which investigates a unified framework to combine structured/semi-structured/unstructured data to answer open-domain questions.']",https://arxiv.org/abs/2012.14610,"We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More importantly, we demonstrate that our unified knowledge (UniK-QA) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively. The code of UniK-QA is available at: this https URL ","UniK-QA: Unified Representations of Structured and Unstructured
  Knowledge for Open-Domain Question Answering"
135,1345047734215200768,712454533943857152,Luke Benz,"['New paper w/ @StatsbyLopez, ""Estimating the change in soccer’s home advantage during the\nCovid-19 pandemic using bivariate Poisson regression""\n\nWe explore what happened to home advantage (HA) in 17 European ⚽️ leagues in games played w/out fans.\n\n🔗:<LINK>', 'Nearly every paper on the subject thus far has used linear regression. However, soccer outcomes (goals + yellow cards) are non-linear in nature. We use simulations to show that when estimating HA, bivariate Poission can reduce absolute bias by ~85% compared to linear regression. https://t.co/eOyAlLWG0K', 'Unlike most papers, we treat all leagues separately, rather than combining data from different leagues and estimating one HA across all of Europe. This is because leagues:\n- have different pre-Covid HA (🇬🇷 is ~ 2.5x 🇦🇹, for example)\n- are affected differently w/out fans https://t.co/xDHW8VmhTE', 'Simply looking at posterior distributions in the changes in HA, we can see visually several examples where change that happened in one league could not possibly be the change that happened in another league. https://t.co/PuFZBFjorW', 'In contrast to current research that overwhelmingly suggests a drop in the HA, our findings are\nmixed. In some leagues, evidence points to a decrease, while in others, the HA may\nhave risen. Since our model is fit w/ a Bayesian framework, we provide probabilities of HA decline. https://t.co/T7x2hQad2k', 'While traditional soccer research has used yellow cards as a proxy for referee decisions\nrelating to benefits for the home team, we find that it is not always the case that changes in yellow\ncard HA are linked to changes to goal HA.', 'Under this hypothesis, yellow cards aren’t a direct proxy for a referee driven home advantage, and instead imply changes to player behavior without fans. https://t.co/Noyl0Ka7xK', 'Taken wholly, estimates looking at the impact of HA post-Covid are less of a statement about\nthe cause and effect from a lack of fans and as much\nabout changes due to both a lack of fans and changes to training due to Covid-19.', 'All our work is open source, and code + data used in our paper have been made public on GitHub:\nhttps://t.co/bd0V8M3FmZ', ""Finally, a special thanks to @StatsbyLopez. I'm surely preaching to the choir here, but Mike is a great mentor, and I'm very grateful for the chance to have been able to collaborate and learn from him on this project."", '@CausalKathy @StatsbyLopez Observed correlation is much stronger for YC than for goals (Pearson correlation between -0.16 and 0.07 for goals , 0.10-0.22 for YC). Draw rate has also gone down a lot compared to original Karlis paper (3/1/0 for W/D/L vs 2/1/0), perhaps explaining lower correlation in goals.', '@CausalKathy @StatsbyLopez Between this, and other references suggestion lambda 3 = 0 are more suitable for goals models, we decided to run the sims w/out it. Especially since possibility of negative correlation doesn’t make as much sense in this specification.', '@CausalKathy @StatsbyLopez In the paper, we fit models w/ lambda 3 = 0 and lambda 3 &gt; 0, and while we only present goals (lambda 3 = 0) and YC (lambda 3 &gt; 0), we comment that results don’t change much between the 2 variants of goals model, while there are larger differences between the YC variants.', ""@CausalKathy @StatsbyLopez Good question, though, and I think this goes to show that for YC in particular, one's inference can be possibly led astray when not accounting for the correlation.""]",https://arxiv.org/abs/2012.14949,"In wake of the Covid-19 pandemic, 2019-2020 soccer seasons across the world were postponed and eventually made up during the summer months of 2020. Researchers from a variety of disciplines jumped at the opportunity to compare the rescheduled games, played in front of empty stadia, to previous games, played in front of fans. To date, most of this post-Covid soccer research has used linear regression models, or versions thereof, to estimate potential changes to the home advantage. But because soccer outcomes are non-linear, we argue that leveraging the Poisson distribution would be more appropriate. We begin by using simulations to show that bivariate Poisson regression reduces absolute bias when estimating the home advantage benefit in a single season of soccer games, relative to linear regression, by almost 85 percent. Next, with data from 17 professional soccer leagues, we extend bivariate Poisson models estimate the change in home advantage due to games being played without fans. In contrast to current research that overwhelmingly suggests a drop in the home advantage, our findings are mixed; in some leagues, evidence points to a decrease, while in others, the home advantage may have risen. Altogether, this suggests a more complex causal mechanism for the impact of fans on sporting events. ","Estimating the change in soccer's home advantage during the Covid-19
  pandemic using bivariate Poisson regression"
136,1345014876830388225,3253593413,Michael Schlichtkrull,"['Happy new year! 🎉 Why not start the year with a paper, for example our new preprint on open-domain fact checking over tables <LINK>? \n\nWith fantastic coauthors Vladimir Karpukhin, Barlas Oğuz, @ml_perception, @scottyih, and @riedelcastro. <LINK>']",https://arxiv.org/abs/2012.15115,"Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline. ",Joint Verification and Reranking for Open Fact Checking Over Tables
137,1342484643384328193,1222423904955682817,Hanlei Zhang,['Got two papers accepted at AAAI 2021!🤣👏 \npaper 1:\nTitle: Deep Open Intent Classification with Adaptive Decision Boundary \narXiv version: <LINK>\npaper 2:\nTitle: Discovering New Intents with Deep Aligned Clustering\narXiv version: <LINK>'],https://arxiv.org/abs/2012.10209,"Open intent classification is a challenging task in dialogue systems. On the one hand, it should ensure the quality of known intent identification. On the other hand, it needs to detect the open (unknown) intent without prior knowledge. Current models are limited in finding the appropriate decision boundary to balance the performances of both known intents and the open intent. In this paper, we propose a post-processing method to learn the adaptive decision boundary (ADB) for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we automatically learn the adaptive spherical decision boundary for each known class with the aid of well-trained features. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open intent samples and is free from modifying the model architecture. Moreover, our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods. The codes are released at this https URL ",Deep Open Intent Classification with Adaptive Decision Boundary
138,1340865847145934851,1198393614478516224,Vaikuntanathan Lab,"[""Check out Laura's new paper (in collaboration w Etienne)  <LINK> We show how static correlations can contain information about the dissipation in an active liquid. This connection can also be inferred by a simple machine learning setup.""]",https://arxiv.org/abs/2012.10441,"Active systems, which are driven by non-conservative forces acting independently on individual particles, exhibit a variety of unique behaviors and structures, with potential applications in the design of novel materials. Towards designing such materials, an important open challenge is to precisely connect the structure of active systems to the dissipation of energy induced by the local driving. We approach this problem by developing a perturbative mean-field theory which predicts the static structure with good accuracy, even for strongly interacting systems. Significantly, this theory requires only equilibrium measurements of the structure to deduce, systematically, how driving forces shape the structure out-of-equilibrium. Second, based on this theory, we derive an expression for the rate of dissipation and we show that there exists a robust relation between dissipation and structure, which holds even as the system approaches an activity-induced phase transition far from equilibrium. Finally, we construct a neural network which maps static configurations of active particles to their dissipation rate, thus showing that the dissipation of energy can be inferred from purely static structural information, without any prior knowledge of the underlying dynamics. ",Inferring dissipation from static structure in active matter
139,1339937575293886464,1702174146,Janis Keuper,"[""New pre-print on arxiv (3/3):  my wife's team extended our joint #cvpr2020 paper - very nice results (accepted at #aaai2021).  <LINK> <LINK>""]",https://arxiv.org/abs/2012.03110,"Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue. ",Spectral Distribution Aware Image Generation
140,1339855484002693121,464024103,Fabian Schär,"['Decentralized Finance, Centralized Ownership? \n\nRead our new working paper on ownership concentration &amp; wrapping complexity in the #DeFi space. This is joint-work w/ Matthias Nadler @mat_nadler!\n\n<LINK>\n\ncc: @MAMA_global @defiprime @defipulse @DeFi_Dad @CamiRusso <LINK>', '@federiconitidi @mat_nadler @MAMA_global @defiprime @defipulse @DeFi_Dad @CamiRusso So far we have analyzed data from June 2019 to September 2020. $UNI launched after our last snapshot. But we plan to extend the analysis.', '@YannickLegendre @FUTURE_FUND_ @mat_nadler @MAMA_global @defiprime @defipulse @DeFi_Dad @CamiRusso Thank you. So far we have analyzed data from June 2019 to September 2020. $UNI launched after our last snapshot. But we plan to extend the analysis.']",https://arxiv.org/abs/2012.09306,"In this paper, we analyze various Decentralized Finance (DeFi) protocols in terms of their token distributions. We propose an iterative mapping process that allows us to split aggregate token holdings from custodial and escrow contracts and assign them to their economic beneficiaries. This method accounts for liquidity-, lending-, and staking-pools, as well as token wrappers, and can be used to break down token holdings, even for high nesting levels. We compute individual address balances for several snapshots and analyze intertemporal distribution changes. In addition, we study reallocation and protocol usage data, and propose a proxy for measuring token dependencies and ecosystem integration. The paper offers new insights on DeFi interoperability as well as token ownership distribution and may serve as a foundation for further research. ","Decentralized Finance, Centralized Ownership? An Iterative Mapping
  Process to Measure Protocol Token Distribution"
141,1339268844016324610,1238890699111751680,Anirudha Majumdar,"[""New paper on generating maximally adversarial disturbances for a given controller for a robot (assuming only blackbox access to the controller). Collaboration with @HazanPrinceton's group. Work led by Udaya Ghai and David Snyder. \n<LINK> <LINK> <LINK>""]",https://arxiv.org/abs/2012.06695,"We consider the problem of generating maximally adversarial disturbances for a given controller assuming only blackbox access to it. We propose an online learning approach to this problem that \emph{adaptively} generates disturbances based on control inputs chosen by the controller. The goal of the disturbance generator is to minimize \emph{regret} versus a benchmark disturbance-generating policy class, i.e., to maximize the cost incurred by the controller as well as possible compared to the best possible disturbance generator \emph{in hindsight} (chosen from a benchmark policy class). In the setting where the dynamics are linear and the costs are quadratic, we formulate our problem as an online trust region (OTR) problem with memory and present a new online learning algorithm (\emph{MOTR}) for this problem. We prove that this method competes with the best disturbance generator in hindsight (chosen from a rich class of benchmark policies that includes linear-dynamical disturbance generating policies). We demonstrate our approach on two simulated examples: (i) synthetically generated linear systems, and (ii) generating wind disturbances for the popular PX4 controller in the AirSim simulator. On these examples, we demonstrate that our approach outperforms several baseline approaches, including $H_{\infty}$ disturbance generation and gradient-based methods. ",Generating Adversarial Disturbances for Controller Verification
142,1339028608128573440,1166165104808931328,Zahra Tabrizi,"['New paper: ""Gamma Ray Signals from Cosmic Ray Scattering on Axion-Like Particles""\n<LINK>\nI\'m falling in love with ALPs ;-)']",https://arxiv.org/abs/2012.07930,"Dark Matter (DM) may be comprised of axion-like particles (ALPs) with couplings to photons and the standard model fermions. In this paper we study photon signals arising from cosmic ray (CR) electron scattering on background ALPs. For a range of masses we find that these bounds can place competitive new constraints on the ALP-electron coupling, although in many models lifetime constraints may supersede these bounds. In addition to current Fermi constraints, we also consider future e-Astrogram bounds which will have greater sensitivity to ALP-CR induced gamma-rays. ",Gamma Ray Signals from Cosmic Ray Scattering on Axion-Like Particles
143,1338889592548827136,20515971,Erkut Erdem,"['Today, we are releasing MSVD-Turkish, a new benchmark dataset for integrated vision and language research in Turkish. Dataset is available at <LINK>, and the accompanying paper is on arXiv, <LINK> <LINK>', 'This is a joint effort of Hacettepe University, Koç University and Imperial College London by Begum Citamak, @Ozan__Caglayan, Menekse Kuyu, myself, @aykuterdemml, @foobarin and @lspecia.', '@TT20833837 Çok büyük ihtimalle videodaki seslerden. Tabi açıklama üretirken ses bilgisinden yararlanılmıyor oluşu veri kümesinin bu tarz önyargılara (dataset bias) sahip olmasını getiriyor. Bu da kendi başına bir araştırma konusu aslında.']",https://arxiv.org/abs/2012.07098,"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages. ","MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision
  and Language Research in Turkish"
144,1338429580827381761,1249505964392214529,Jae Shin Yoon,['I would like to share our new solution to create a human animation from a still image. This is the collaboration outcome with @UMNComputerSci and MPI for Informatics.\n\nVideo: <LINK>\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2012.03796,"We present a new pose transfer method for synthesizing a human animation from a single image of a person controlled by a sequence of body poses. Existing pose transfer methods exhibit significant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and failures in preserving the identity and textures of the person. To address these limitations, we design a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a unified representation of appearance and its labels in UV coordinates, which remains constant across poses. The unified representation provides an incomplete yet strong guidance to generating the appearance in response to the pose change. We use the trained network to complete the appearance and render it with the background. With these strategies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a temporally coherent way without any fine-tuning of the network on the testing scene. Experiments show that our method outperforms the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability. ",Pose-Guided Human Animation from a Single Image in the Wild
145,1337453577078452231,281711973,Dr. Emily Rickman,"[""New paper day! 🌟🔭\n\nTheres been a lot of discussion recently about white dwarfs (I'm looking at you @WD1856_534b winner of the @exo_cast #ExoCup2020) but how about WDs around MS stars?\n\nWe directly image some of these cool systems with @SPHERE_outreach\n👇\n<LINK> <LINK>"", '@WD1856_534b @exo_cast @SPHERE_outreach WDs to every object in the Universe.. https://t.co/PYzDC4uK7h']",https://arxiv.org/abs/2012.05575,"Sirius-like systems are wide binaries composed of a white dwarf (WD) and a companion of a spectral type earlier than M0. The WD progenitor evolves in isolation, but its wind during the AGB phase pollutes the companion surface and transfers some angular momentum. Within SHINE survey that uses SPHERE at the VLT, we acquired images of HD2133, HD114174, and CD-567708 and combined this data with high resolution spectra of the primaries, TESS, and literature data. We performed accurate abundance analyses for the MS. We found brighter J and K magnitudes for HD114174B than obtained previously and extended the photometry down to 0.95 micron. Our new data indicate a higher temperature and then shorter cooling age (5.57+/-0.02 Gyr) and larger mass (0.75+/-0.03 Mo) for this WD than previously assumed. This solved the discrepancy previously found with the age of the MS star. The two other WDs are less massive, indicating progenitors of ~1.3 Mo and 1.5-1.8 Mo for HD2133B and CD-56 7708B, respectively. We were able to derive constraints on the orbit for HD114174 and CD-56 7708. The composition of the MS stars agrees fairly well with expectations from pollution by the AGB progenitors of the WDs: HD2133A has a small enrichment of n-capture elements, which is as expected for pollution by an AGB star with a mass <1.5 Mo; CD-56 7708A is a previously unrecognized mild Ba-star, which is expected due to pollution by an AGB star with a mass in the range of 1.5-3.0 Mo; and HD114174 has a very moderate excess of n-capture elements, which is in agreement with the expectation for a massive AGB star to have a mass >3.0 Mo. On the other hand, none of these stars show the excesses of C that are expected to go along with those of n-capture elements. This might be related to the fact that these stars are at the edges of the mass range where we expect nucleosynthesis related to thermal pulses. ",Investigating three Sirius-like systems with SPHERE
146,1336972976957370368,718384250270126081,Sven Krippendorf,"['Very excited about this work on learning Calabi-Yau metrics via ML. Improving on SOTA and finding new types metrics inaccessible with current methods. Paper: <LINK> (1/3) <LINK>', 'Why are these metrics important? If you want to know which physics (more precisely which EFT) arises from string theory, the extra-dim. metric is a crucial ingredient. Current knowledge is mostly based on algebraic geometry. This only gives access to some quantities but not all.', 'There is a long way ahead, which does not seem to be possible without ML. Excited to also see this work https://t.co/uAm2ZmE79r today touching on the same issue. Very impressive contributions from my Master student Mathis Gerdes. He is applying for PhD positions this year. (3/3)']",https://arxiv.org/abs/2012.04656,"We use machine learning to approximate Calabi-Yau and SU(3)-structure metrics, including for the first time complex structure moduli dependence. Our new methods furthermore improve existing numerical approximations in terms of accuracy and speed. Knowing these metrics has numerous applications, ranging from computations of crucial aspects of the effective field theory of string compactifications such as the canonical normalizations for Yukawa couplings, and the massive string spectrum which plays a crucial role in swampland conjectures, to mirror symmetry and the SYZ conjecture. In the case of SU(3) structure, our machine learning approach allows us to engineer metrics with certain torsion properties. Our methods are demonstrated for Calabi-Yau and SU(3)-structure manifolds based on a one-parameter family of quintic hypersurfaces in $\mathbb{P}^4.$ ","Moduli-dependent Calabi-Yau and SU(3)-structure metrics from Machine
  Learning"
147,1336898095997661184,230492604,Lisa Randall,"['new paper. for technical.followers. <LINK>', '@newolder doubly holographic as we explain bulk brane brane boundary. bulk and brane both  ads']",https://arxiv.org/abs/2012.04671,"Late-time dominance of entanglement islands plays a critical role in addressing the information paradox for black holes in AdS coupled to an asymptotic non-gravitational bath. A natural question is how this observation can be extended to gravitational systems. To gain insight into this question, we explore how this story is modified within the context of Karch-Randall braneworlds when we allow the asymptotic bath to couple to dynamical gravity. We find that because of the inability to separate degrees of freedom by spatial location when defining the radiation region, the entanglement entropy of radiation emitted into the bath is a time-independent constant, consistent with recent work on black hole information in asymptotically flat space. If we instead consider an entanglement entropy between two sectors of a specific division of the Hilbert space, we then find non-trivial time-dependence, with the Page time a monotonically decreasing function of the brane angle -- provided both branes are below a particular angle. However, the properties of the entropy depend discontinuously on this angle, which is the first example of such discontinuous behavior for an AdS brane in AdS space. ",Information Transfer with a Gravitating Bath
148,1336866272697483265,2337598033,Geraint F. Lewis,"['New paper on @arxiv by PhD student Will Oliver, presenting HALO-Optics, a new clustering algorithm. With @doctorcbpower \n\n<LINK> <LINK>']",https://arxiv.org/abs/2012.04823,"We build upon Ordering Points To Identify Clustering Structure (OPTICS), a hierarchical clustering algorithm well-known to be a robust data-miner, in order to produce Halo-OPTICS, an algorithm designed for the automatic detection and extraction of all meaningful clusters between any two arbitrary sizes. We then apply Halo-OPTICS to the 3D spatial positions of halo particles within four separate synthetic Milky Way type galaxies, classifying the stellar and dark matter structural hierarchies. Through visualisation of the Halo-OPTICS output, we compare its structure identification to the state-of-the-art galaxy/(sub)halo finder VELOCIraptor, finding excellent agreement even though Halo-OPTICS does not consider kinematic information in this current implementation. We conclude that Halo-OPTICS is a robust hierarchical halo finder, although its determination of lower spatial-density features such as the tails of streams could be improved with the inclusion of extra localised information such as particle kinematics and stellar metallicity into its distance metric. ","The Hierarchical Structure of Galactic Haloes: Classification and
  characterisation with Halo-OPTICS"
149,1336601225228410881,2868753520,Earl T Campbell,"[""Today's paper disco dance for our @awscloud fault-tolerant quantum computer design: from cat-code qubits with realistic noise, through biased noise error correction,  two new Toffoli prep protocols, then resource counting the Hubbard model <LINK>"", 'Due to high levels of juicy content, it may take some time for your device to download the pdf.']",https://arxiv.org/abs/2012.04108,"We present a comprehensive architectural analysis for a proposed fault-tolerant quantum computer based on cat codes concatenated with outer quantum error-correcting codes. For the physical hardware, we propose a system of acoustic resonators coupled to superconducting circuits with a two-dimensional layout. Using estimated physical parameters for the hardware, we perform a detailed error analysis of measurements and gates, including CNOT and Toffoli gates. Having built a realistic noise model, we numerically simulate quantum error correction when the outer code is either a repetition code or a thin rectangular surface code. Our next step toward universal fault-tolerant quantum computation is a protocol for fault-tolerant Toffoli magic state preparation that significantly improves upon the fidelity of physical Toffoli gates at very low qubit cost. To achieve even lower overheads, we devise a new magic-state distillation protocol for Toffoli states. Combining these results together, we obtain realistic full-resource estimates of the physical error rates and overheads needed to run useful fault-tolerant quantum algorithms. We find that with around 1,000 superconducting circuit components, one could construct a fault-tolerant quantum computer that can run circuits which are currently intractable for classical computers. Hardware with 18,000 superconducting circuit components, in turn, could simulate the Hubbard model in a regime beyond the reach of classical computing. ",Building a fault-tolerant quantum computer using concatenated cat codes
150,1336477311114174465,220057455,Chen Change Loy,"['Our new video super-resolution method, BasicVSR,  achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms.\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2012.02181,"Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches. ","BasicVSR: The Search for Essential Components in Video Super-Resolution
  and Beyond"
151,1334465652560695298,1541749356,Matt Landreman,"['Paper just out, extending our new approach of optimizing stellarators using expansion around magnetic axis. Now we can rapidly diagnose a bunch of other properties of a stellarator without needing an equilibrium code like VMEC. <LINK> <LINK>']",https://arxiv.org/abs/2012.00865,"A new paradigm for rapid stellarator configuration design has been recently demonstrated, in which the shapes of quasisymmetric or omnigenous flux surfaces are computed directly using an expansion in small distance from the magnetic axis. To further develop this approach, here we derive several other quantities of interest that can be rapidly computed from this near-axis expansion. First, the $\nabla\vec{B}$ and $\nabla\nabla\vec{B}$ tensors are computed, which can be used for direct derivative-based optimization of electromagnetic coil shapes to achieve the desired magnetic configuration. Moreover, if the norm of these tensors is large compared to the field strength for a given magnetic field, the field must have a short length scale, suggesting it may be hard to produce with coils that are suitably far away. Second, we evaluate the minor radius at which the flux surface shapes would become singular, providing a lower bound on the achievable aspect ratio. This bound is also shown to be related to an equilibrium beta limit. Finally, for configurations that are constructed to achieve a desired magnetic field strength to first order in the expansion, we compute the error field that arises due to second order terms. ",Figures of merit for stellarators near the magnetic axis
152,1334032097539833856,92966853,Adeel Razi,"['New from our lab: ""A Generative Model to Synthesize EEG Data for Epileptic Seizure Prediction"". \n\nPaper pre-print is here: <LINK>\n\nLead by @KhansaRasheed (second paper from her MSc thesis) with @junaidq @levink2 Terence O\'Brien @turnerinstitute @MonashNeurosci <LINK>']",https://arxiv.org/abs/2012.00430,"Prediction of seizure before they occur is vital for bringing normalcy to the lives of patients. Researchers employed machine learning methods using hand-crafted features for seizure prediction. However, ML methods are too complicated to select the best ML model or best features. Deep Learning methods are beneficial in the sense of automatic feature extraction. One of the roadblocks for accurate seizure prediction is scarcity of epileptic seizure data. This paper addresses this problem by proposing a deep convolutional generative adversarial network to generate synthetic EEG samples. We use two methods to validate synthesized data namely, one-class SVM and a new proposal which we refer to as convolutional epileptic seizure predictor (CESP). Another objective of our study is to evaluate performance of well-known deep learning models (e.g., VGG16, VGG19, ResNet50, and Inceptionv3) by training models on augmented data using transfer learning with average time of 10 min between true prediction and seizure onset. Our results show that CESP model achieves sensitivity of 78.11% and 88.21%, and FPR of 0.27/h and 0.14/h for training on synthesized and testing on real Epilepsyecosystem and CHB-MIT datasets, respectively. Effective results of CESP trained on synthesized data shows that synthetic data acquired the correlation between features and labels very well. We also show that employment of idea of transfer learning and data augmentation in patient-specific manner provides highest accuracy with sensitivity of 90.03% and 0.03 FPR/h which was achieved using Inceptionv3, and that augmenting data with samples generated from DCGAN increased prediction results of our CESP model and Inceptionv3 by 4-5% as compared to state-of-the-art traditional augmentation techniques. Finally, we note that prediction results of CESP achieved by using augmented data are better than chance level for both datasets. ","A Generative Model to Synthesize EEG Data for Epileptic Seizure
  Prediction"
153,1348522633923477505,38824024,Aman Gupta,"['New paper out on Arxiv! We propose a video embedding technique - deep smoothed gaussian mixture model (DSGMM). We improve on NetVLAD using smoothing for low count clusters. We show classification improvements on YouTube-8M and a LinkedIn dataset. Link - <LINK>', 'Joint work with Sirjan Kafle, Xue Xia, Ananth Sankar, Xi Chen, Di Wen and Liang Zhang']",https://arxiv.org/abs/2012.11673,"Cluster-and-aggregate techniques such as Vector of Locally Aggregated Descriptors (VLAD), and their end-to-end discriminatively trained equivalents like NetVLAD have recently been popular for video classification and action recognition tasks. These techniques operate by assigning video frames to clusters and then representing the video by aggregating residuals of frames with respect to the mean of each cluster. Since some clusters may see very little video-specific data, these features can be noisy. In this paper, we propose a new cluster-and-aggregate method which we call smoothed Gaussian mixture model (SGMM), and its end-to-end discriminatively trained equivalent, which we call deep smoothed Gaussian mixture model (DSGMM). SGMM represents each video by the parameters of a Gaussian mixture model (GMM) trained for that video. Low-count clusters are addressed by smoothing the video-specific estimates with a universal background model (UBM) trained on a large number of videos. The primary benefit of SGMM over VLAD is smoothing which makes it less sensitive to small number of training samples. We show, through extensive experiments on the YouTube-8M classification task, that SGMM/DSGMM is consistently better than VLAD/NetVLAD by a small but statistically significant margin. We also show results using a dataset created at LinkedIn to predict if a member will watch an uploaded video. ","Smoothed Gaussian Mixture Models for Video Classification and
  Recommendation"
154,1345185238276861953,892997634813710336,Adam Fisch,"[""Excited to share a preprint, “Making Pre-trained Language Models Better Few-shot Learners”!\n\nGPT-3's huge LM amazingly works directly for few-shot learning. We take things further, and propose fine-tuning techniques to make smaller LMs work better too.\n\n<LINK> <LINK>"", 'We study a practical scenario in which we only assume access to a moderately-sized LM (e.g., RoBERTa) and a small number of in-domain examples to fine-tune its weights (easy &amp; efficient for smaller LMs!).\n\nThis setting is applicable to most real problems, but very challenging.', 'Our approach includes two simple but effective techniques: (1) a prompt-based fine-tuning method using automatically created prompts, and (2) a refined strategy for using demonstrations within each context.\n\nWe formulate this approach for both classification and regression tasks.', 'We show that we can achieve strong few-shot performance across a range of NLP tasks, greatly outperforming vanilla fine-tuning!\n\nFortunate to have been able to do this work with a great set of collaborators, @gaotianyu1350 and @danqi_chen. https://t.co/seyQ91h0sr']",https://arxiv.org/abs/2012.15723v1,"The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. ",] Making Pre-trained Language Models Better Few-shot Learners
155,1345068435177353216,1138704090106646528,Aitor Ormazabal,"['Check out our new paper ""Beyond offline mapping: Learning Cross Lingual Word Embeddings through Context Anchoring"". We propose a new method to learn word embeddings aligned in a target space without a mapping step, outperforming mapping methods in BLI. <LINK>', 'w/ @artetxem, @Aitor57 , @glabaka and @eagirre']",http://arxiv.org/abs/2012.15715,"Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. ","Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through
  Context Anchoring"
156,1344853003480702977,2348771504,Eric Severson,"[""We barely achieved our goal of getting this paper online in 2020:\n<LINK>\nThis is probably the paper I'm happiest with so far, since we showed the most efficient possible algorithm for one of the most well-studied tasks in the model I've focused my PhD work on (1/"", ""I thought I'd write a thread describing what the problem is we solved.\nThe model is called population protocols. It describes a population of small-state computational agents that interact in random pairwise interactions.\n(2/)"", ""One good motivation for the model (and the reason my advisor studies it) comes from his field of `molecular computation'. This model is a simple description of chemical reactions. We choose what the states of molecules are, and the rules for how each pair will interact (3/)"", 'The problem we studied is majority. All the agents are initially in either the A state or B state. The goal is for them all to learn what the initial majority was. So our agents are trying to learn who wins an election, but all they can do is randomly bump into each other... (4/)', ""And they don't have nearly enough memory to hope to be able to count how many A's and B's there are.\nIt turns out there are some really simple ways to solve the problem. One uses only 4 species A, B, a, b, with rules\nA + B -&gt; a + b\nA + b -&gt; A + a\nB + a -&gt; B + b\n(5/)"", ""The idea is when A meets B, they change into 'weak' states a and b. The 'strong' states A and B can then convince the weak agents to take their opinion. If A has more agents at the start, eventually all the B will get consumed, and then the remaining A convert everyone to a.\n(6/)"", ""So eventually this will exactly solve the problem, since we converge to a population of all A / a or all B / b, and every agent knows the answer. The issue is it's very slow if the initial gap is tiny. That's because we reach a configuration with only a few A and B left.\n(7/)"", ""Say we get down to 2 A, 1 B, and the rest of the n agents are a or b. Picture the size n as really large (like avagadro's constant 6*10^23 of molecules). Then the 1 B has to meet an A to switch it's opinion, which is incredibly unlikely so will take a very long time\n(8/)"", 'When we talk about time in this model, we call each n random pairwise interactions one unit of time. Thus time scales with the average number of interactions per agent. The above B meets A takes about O(n^2) interactions on average, or O(n) time, too slow for huge n\n(9/)', ""Because interactions are random, not every agent will interact in one unit of time. By considering the 'coupon collector problem', you can show it takes expected O(n log n) interactions for every agent to get to interact. So O(log n) time is the fastest to do anything\n(10/)"", 'People have showed lower bounds, that if you want to do any faster than the simple slow 4-state algorithm, you need more states. The number of states actually has to grow with the population size, at least log(n) states.\n(11/)', ""Some older results showed that with log(n) states, you can solve exact majority in time (log n)^2. They first construct a ``phase clock'' where the agents can count through synchronous rounds. A round needs to last log(n) time to be sure everybody has an interaction\n(12/)"", ""Rounds alternate between doing ``cancel reactions'' \nA + B -&gt; 0 + 0, and doing ``split reactions'' \nA + 0 -&gt; A + A,   B + 0 -&gt; B + B.\nEach A / B can only split once per round. The cancels make enough 0s to let everybody split once.\n(13/)"", 'Cancelling preserves the gap (# A - # B), and everybody doing one split reaction will exactly double the gap.\nSo even with initial gap 1, after log n of these rounds, we can amplify the gap up to n. Thus we will fully eliminate the minority opinion, in log n * log n time.\n(14/)', ""Our protocol still uses the minimal O(log n) states, but manages to get the optimal O(log n) time as well.\nWe describe the initial agents as having a ``bias'': A is +1 and B is -1. So we want to learn if the sum of biases was positive or negative.\n(15/)"", 'The goal is to average these biases. If the agents had infinite memory and could store real numbers, they could just take an average in each interaction\n i + j -&gt; (i+j)/2 + (i+j) / 2\nThis would spread the bias around evenly and quickly.\n(16/)', 'For example, if the initial gap (# A - # B) = +1, then all bias values would converge to (+1 / n). Once every agent has positive bias, we have converged to the correct answer.\nSo there is a simple algorithm that takes optimal O(log n) time if we had enough memory.\n(17/)', ""We can think of our algorithm as trying to do averaging, but with less states. The ``cancel reactions'' are\n(+1) + (-1) -&gt; 0 + 0\nThen in ``split reactions'' one agent will give half their bias to the 0 agent, such as \n(+1) + (0) -&gt; (+1/2) + (+1/2)\n(18/)"", 'The allowed values for a bias are fractions of the form +-(1 / 2^k), going down to a smallest possible value k = log(n).\nThese rules ensure the sum of all biases can never change, because only agents at the same exponent can cancel, such as\n(+1 / 8) + (-1/8) -&gt; 0 + 0\n(19/)', ""The tricky part is we aren't careful, the agents will get too spread out over all the possible exponents. This makes the cancel reactions too slow, and then we don't have very many 0 reactions, which then also makes the split reactions too slow.\n(20/)"", 'If we could keep the agents perfectly synchronized, all splitting from (+1, -1) to (+1/2, -1/2) then cancelling and splitting to (+1/4, -1/4), ..., this would be exactly the (log n)^2 time algorithm, which makes sure nobody gets left behind but is too slow.\n(21/)', ""So the key other trick we have is partial synchronization. We don't have enough time to keep everybody fully synchronized, but we run a faster clock that keeps the agents mostly synchronized.\nAfter many pages of proofs, we can show this does most of the averaging we want\n(22/)"", 'Then we also need a bunch of extra backup steps afterward to clean up anybody who gets left behind and make sure every minority gets eliminated. We get something that is fast with high probability, and if it messes up, they always detect the error and do a slow backup\n(23/)', 'So the eventual algorithm is guaranteed to eventually get every agent to stabilize with the correct value.\n(24/24)', '@mechalink Yeah the states are the exponents, so they are just storing +-k.\nWe just interpret this state as describing some sort of fraction bias, to have the guarantee that the net total of all these biases is always preserved.', ""@mechalink We just need the total number of states to be O(log n). And there's actually more states than just these ones, because we have various roles, where some agents will use all their memory to run a clock for example, after they give their initial bias to somebody else."", '@mechalink Yeah even this was just the high level idea of what we are trying to do. The exact states and transition rules are way messier and need multiple pages of pseudocode to describe.']",https://arxiv.org/abs/2012.15800,"We study population protocols, a model of distributed computing appropriate for modeling well-mixed chemical reaction networks and other physical systems where agents exchange information in pairwise interactions, but have no control over their schedule of interaction partners. The well-studied *majority* problem is that of determining in an initial population of $n$ agents, each with one of two opinions $A$ or $B$, whether there are more $A$, more $B$, or a tie. A *stable* protocol solves this problem with probability 1 by eventually entering a configuration in which all agents agree on a correct consensus decision of $A$, $B$, or $T$, from which the consensus cannot change. We describe a protocol that solves this problem using $O(\log n)$ states ($\log \log n + O(1)$ bits of memory) and optimal expected time $O(\log n)$. The number of states $O(\log n)$ is known to be optimal for the class of stable protocols that are ""output dominant"" and ""monotone"". These are two natural constraints satisfied by our protocol, making it state-optimal for that class. We use, and develop novel analysis of, a key technique called a ""fixed resolution clock"" due to Gasieniec, Stachowiak, and Uznanski, who showed a majority protocol using $O(\log n)$ time and states that has a positive probability of error. Our protocol is *nonuniform*: the transition function has the value $\left \lceil {\log n} \right \rceil$ encoded in it. We show that the protocol can be modified to be uniform, while increasing the state complexity to $\Theta(\log n \log \log n)$. ",A stable majority population protocol using logarithmic time and states
157,1344052849991270400,3007703358,Yixin Nie,"['Happy to share our new paper on addressing contradictions in dialogue modeling.\n\n<LINK>\n\nWe introduce DialoguE COntradiction DEtection (DECODE) task and a new dataset with contradictory dialogues to study how well NLU models can capture consistency in dialogues. <LINK>', 'We then compare a structured utterance-based approach with a typical unstructured approach of using pre-trained Transformer models for dialogue contradiction detection. \n\n2/N', 'The result reveals that: \n(1). Our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain (e.g. DNLI); \n\n3/N', '(2). The structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. \n\n4/N', 'The finding argues for the development of more robust and generalizable NLU modeling and challenges a belief amongst some practitioners that they can just use a standard Transformer and it will learn all the structure correctly on its own. \n\n5/N', 'We also show that our best contradiction detector correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. \n\n6/N', 'Going forward, we envision complementary progress on both the modeling of NLU and NLG and the integration of the two. We hope our work could facilitate and provide guidelines for future work on incorporating NLU modeling into dialogue systems. \n\n7/N', 'Check out more details and the data on our project page in @parlai_parley.\n\nhttps://t.co/V0juczVC2o\nhttps://t.co/AcbMz8VKFz\n\n8/N', 'A huge Thank You to my co-authors: Mary Williamson, @mohitban47, @douwekiela, @jaseweston, and the ParlAI team.\n\nAlso, thanks to @wellecks, Arthur Szlam, @kchonyc for the prior work (Dialogue NLI).\n\n9/N']",https://arxiv.org/abs/2012.13391,"To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. ","I like fish, especially dolphins: Addressing Contradictions in Dialogue
  Modeling"
158,1343537233350053889,2800342562,T. Marshall Eubanks,"['PSR J0537-6910 has the largest spin-down luminosity of any pulsar &amp; its 62 Hz rotation puts its gravitational-wave emission in the most sensitive band of LIGO/Virgo. We find no signal, however, &amp; report an upper limit of 14% of the spin-down energy budget. <LINK>']",https://arxiv.org/abs/2012.12926,"We present a search for continuous gravitational-wave signals from the young, energetic X-ray pulsar PSR J0537-6910 using data from the second and third observing runs of LIGO and Virgo. The search is enabled by a contemporaneous timing ephemeris obtained using NICER data. The NICER ephemeris has also been extended through 2020 October and includes three new glitches. PSR J0537-6910 has the largest spin-down luminosity of any pulsar and is highly active with regards to glitches. Analyses of its long-term and inter-glitch braking indices provided intriguing evidence that its spin-down energy budget may include gravitational-wave emission from a time-varying mass quadrupole moment. Its 62 Hz rotation frequency also puts its possible gravitational-wave emission in the most sensitive band of LIGO/Virgo detectors. Motivated by these considerations, we search for gravitational-wave emission at both once and twice the rotation frequency. We find no signal, however, and report our upper limits. Assuming a rigidly rotating triaxial star, our constraints reach below the gravitational-wave spin-down limit for this star for the first time by more than a factor of two and limit gravitational waves from the $l=m=2$ mode to account for less than 14% of the spin-down energy budget. The fiducial equatorial ellipticity is limited to less than about 3e-5, which is the third best constraint for any young pulsar. ","Diving below the spin-down limit: Constraints on gravitational waves
  from the energetic young pulsar PSR J0537-6910"
159,1342545826690850818,1206622352777392146,Daniel Varjas,['Ever thought that amorphous matter is more symmetric than crystals? In our new paper with @helene_spring and @AkhmerovAnton we find amorphous phases where average mirror symmetry protects topological edge states on any edge orientation <LINK> <LINK>'],https://arxiv.org/abs/2012.12909,"Protection of topological surface states by reflection symmetry breaks down when the boundary of the sample is misaligned with one of the high symmetry planes of the crystal. We demonstrate that this limitation is removed in amorphous topological materials, where the Hamiltonian is invariant on average under reflection over any axis due to continuous rotation symmetry. While the local disorder caused by the amorphous structure weakens the topological protection, we demonstrate that the edge remains protected from localization. In order to classify such phases we perform a systematic search over all the possible symmetry classes in two dimensions and construct the example models realizing each of the proposed topological phases. Finally, we compute the topological invariant of these phases as an integral along a meridian of the spherical Brillouin zone of an amorphous Hamiltonian. ",Amorphous topological phases protected by continuous rotation symmetry
160,1342493223072264192,959483581,Slavko Bogdanov,['Up on the ArXiv this Christmas morning: We use the exquisite set of @chandraxray observations to study the millisecond pulsar population in the globular cluster Terzan 5 <LINK>'],https://arxiv.org/abs/2012.12944,"We present an analysis of 745.6 ks of archival Chandra X-ray Observatory Advanced CCD Imaging Spectrometer data accumulated between 2000 and 2016 of the millisecond pulsar (MSP) population in the rich Galactic globular cluster Terzan 5. Eight of the 37 MSPs with precise positions are found to have plausible X-ray source matches. Despite the deep exposure, the remaining MSPs are either marginally detected or have no obvious X-ray counterparts, which can be attributed to the typically soft thermal spectra of rotation-powered MSPs, which are strongly attenuated by the high intervening absorbing column (~$10^{22}$ cm$^{-2}$) towards the cluster, and in some instances severe source crowding/blending. For the ""redback"" MSP binaries, PSRs J1748-2446P and J1748-2446ad, and the ""black widow"" binary PSRs J1748-2446O, we find clear evidence for large-amplitude X-ray variability at the orbital period consistent with an intrabinary shock origin. The third redback MSP in the cluster, PSR J1748-2446A, shows large amplitude variations in flux on time scales of years, possibility due to state transitions or intense flaring episodes from the secondary star. ","A Deep Chandra X-ray Observatory Study of the Millisecond Pulsar
  Population in the Globular Cluster Terzan 5"
161,1342339709587107841,1610691422,Patrick Schwab,"['In what scenarios can generative models be used to overcome barriers to data sharing in biomedical imaging? \n\nWe explore various GANs and multiple imaging datasets to find out (work led by v talented student August)\n\nLink: <LINK>\n\n@ETH @Roche @MPI_IS @uktuebingen <LINK>', '@srchvrs @ETH @Roche @MPI_IS @uktuebingen Great question! Depending on the setting, we found that in certain cases models trained on synthetic data can closely mirror the performance of those trained on real data (Fig. 2), and they focus on similar features as those trained on real data (Fig. 5).']",https://arxiv.org/abs/2012.03769,"Privacy concerns around sharing personally identifiable information are a major practical barrier to data sharing in medical research. However, in many cases, researchers have no interest in a particular individual's information but rather aim to derive insights at the level of cohorts. Here, we utilize Generative Adversarial Networks (GANs) to create derived medical imaging datasets consisting entirely of synthetic patient data. The synthetic images ideally have, in aggregate, similar statistical properties to those of a source dataset but do not contain sensitive personal information. We assess the quality of synthetic data generated by two GAN models for chest radiographs with 14 different radiology findings and brain computed tomography (CT) scans with six types of intracranial hemorrhages. We measure the synthetic image quality by the performance difference of predictive models trained on either the synthetic or the real dataset. We find that synthetic data performance disproportionately benefits from a reduced number of unique label combinations. Our open-source benchmark also indicates that at low number of samples per class, label overfitting effects start to dominate GAN training. We additionally conducted a reader study in which trained radiologists do not perform better than random on discriminating between synthetic and real medical images for intermediate levels of resolutions. In accordance with our benchmark results, the classification accuracy of radiologists increases at higher spatial resolution levels. Our study offers valuable guidelines and outlines practical conditions under which insights derived from synthetic medical images are similar to those that would have been derived from real imaging data. Our results indicate that synthetic data sharing may be an attractive and privacy-preserving alternative to sharing real patient-level data in the right settings. ","Overcoming Barriers to Data Sharing with Medical Image Generation: A
  Comprehensive Evaluation"
162,1342239697012215813,1213772234910654464,Cheng Han Chiang (姜成翰),"['Check out our newest paper ""Pre-Training a Language Model Without Human Language"" on arxiv!\nWe pre-train RoBERTa on different non-human language data and find that in certain cases, the pre-trained models managed to transfer to GLUE.\nabs: <LINK> <LINK>']",https://arxiv.org/abs/2012.11995,"In this paper, we study how the intrinsic nature of pre-training data contributes to the fine-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we fine-tune those language models on GLUE benchmarks. We find that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another non-English language. ",Pre-Training a Language Model Without Human Language
163,1342023173714415616,1125489966585393153,Ugur Tegin,"['Meet with our recent study on optical computing, SOLO <LINK> We demonstrate an optical computing framework based on spatiotemporal effects in multimode fibers for a range of learning tasks from classifying COVID-19 cases and speech recognition to predicting age.', 'In other words, we showed that optical nonlinear mode-coupling can process information nonlinearly. With tuning optical nonlinear propagation strength, the efficiency of learning can be tuned too. We verified this experimental observation with COVID-19 dataset and simulations. https://t.co/vldzG06EXX', 'To reach similar performance levels with our optical computing method, in the literature, heavily pre-trained neural networks (ResNet, GoogleNet or VGG-16) which contain 16-22 layers and 6-130 million parameters to train used. https://t.co/w8JNF9WQAU', 'The computing power of the SOLO is scalable to more than 10^15 Operations per second (Peta Ops) with current technology which is the equivalent of a supercomputer, albeit with only a few Watts of electrical power – 6 orders of magnitude lower than required for a supercomputer.']",https://arxiv.org/abs/2012.12404,"Today's heavy machine learning tasks are fueled by large datasets. Computing is performed with power hungry processors whose performance is ultimately limited by the data transfer to and from memory. Optics is one of the powerful means of communicating and processing information and there is intense current interest in optical information processing for realizing high-speed computations. Here we present and experimentally demonstrate an optical computing framework based on spatiotemporal effects in multimode fibers for a range of learning tasks from classifying COVID-19 X-ray lung images and speech recognition to predicting age from face images. The presented framework overcomes the energy scaling problem of existing systems without compromising speed. We leveraged simultaneous, linear, and nonlinear interaction of spatial modes as a computation engine. We numerically and experimentally showed the ability of the method to execute several different tasks with accuracy comparable to a digital implementation. ",Scalable Optical Learning Operator
164,1341805376166047749,1117093805499355136,Marilena Loverde,"['Do you wonder about CMB+BAO constraints on self-interacting neutrinos if only some of them self-interact? We studied a bunch of examples in this paper led by Thejs Brinckmann. Major congrats to Thejs @ThejsBrinckmann and Jae Hyeok Chang on this nice work! \n<LINK>', 'Some punchlines: we find no strong evidence for self-interactions or larger/smaller values of Neff. On the other hand, there are some additional modes in parameter space with neutrinos decoupling much later than SM ones should, but they are not preferred.', 'We also weigh in on the H0 tension and find (as others have!), that self-interacting neutrinos are not a solution unless you throw out high-ell CMB polarization data, which we have no reason to do. Interestingly, if you did throw it out we found H0 = 74 km/s/Mpc.']",https://arxiv.org/abs/2012.11830,"We perform a comprehensive study of cosmological constraints on non-standard neutrino self-interactions using cosmic microwave background (CMB) and baryon acoustic oscillation data. We consider different scenarios for neutrino self-interactions distinguished by the fraction of neutrino states allowed to participate in self-interactions and how the relativistic energy density, N$_{\textrm{eff}}$, is allowed to vary. Specifically, we study cases in which: all neutrino states self-interact and N$_{\textrm{eff}}$ varies; two species free-stream, which we show alleviates tension with laboratory constraints, while the energy in the additional interacting states varies; and a variable fraction of neutrinos self-interact with either the total N$_{\textrm{eff}}$ fixed to the Standard Model value or allowed to vary. In no case do we find compelling evidence for new neutrino interactions or non-standard values of N$_{\textrm{eff}}$. In several cases we find additional modes with neutrino decoupling occurring at lower redshifts $z_{\textrm{dec}} \sim 10^{3-4}$. We do a careful analysis to examine whether new neutrino self-interactions solve or alleviate the so-called $H_0$ tension and find that, when all Planck 2018 CMB temperature and polarization data is included, none of these examples ease the tension more than allowing a variable N$_{\textrm{eff}}$ comprised of free-streaming particles. Although we focus on neutrino interactions, these constraints are applicable to any light relic particle. ","Self-interacting neutrinos, the Hubble parameter tension, and the Cosmic
  Microwave Background"
165,1341697798690000902,2374216459,Johan Ferret,"['Happy to announce that our paper, ""Self-Imitation Advantage Learning"", was accepted at AAMAS 2021! \n\narxiv link: <LINK>\njoint work w/ O. Pietquin &amp; M. Geist\n\nWe propose SAIL, an extension of Self-Imitation Learning for off-policy RL ⛵️', 'SAIL brings big performance gains in Atari (+244% on average for DQN!), most visibly in hard exploration games (+650% avg!), which we verify for several off-policy RL algorithms. It also synergizes well with exploration bonuses like RND. https://t.co/LkTZYqibnl', 'What I like most about SAIL is its simplicity: it is very easy to add on top of existing RL algorithms (e.g. agents from Dopamine!).\nIn the paper, we make an interesting connection between SAIL and Advantage Learning, an action gap-increasing RL method.']",https://arxiv.org/abs/2012.11989,"Self-imitation learning is a Reinforcement Learning (RL) method that encourages actions whose returns were higher than expected, which helps in hard exploration and sparse reward problems. It was shown to improve the performance of on-policy actor-critic methods in several discrete control tasks. Nevertheless, applying self-imitation to the mostly action-value based off-policy RL methods is not straightforward. We propose SAIL, a novel generalization of self-imitation learning for off-policy RL, based on a modification of the Bellman optimality operator that we connect to Advantage Learning. Crucially, our method mitigates the problem of stale returns by choosing the most optimistic return estimate between the observed return and the current action-value for self-imitation. We demonstrate the empirical effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard exploration games. ",Self-Imitation Advantage Learning
166,1341615441697894400,1185977761032110080,Kazumasa Ohno (大野 和正),"[""A new paper posted! Though the paper is still under review, we have studied haze formation on Triton, an ultra-cold moon of Neptune, using microphysical models. We have discussed what physical processes are going on in Triton's cold and hazy atmosphere.\n<LINK>"", 'We developed a bin-scheme microphysical model that can trace size and porosity distributions of haze particles in a self-consistent manner. We tested several possible nature of Triton hazes, namely Titan-like sphere and aggregate haze, as well as the hazes coated by C2H4 ices.', 'In a nutshell, our main conclusion is that condensation of C2H4 ices likely play crucial role to control physical and optical properties of Triton hazes. It is hard to explain existing observations of Triton hazes by Voyager 2 assuming Titan-like haze without condensation.', 'Both sphere and aggregate icy hazes can reasonably explain observations, so it is currently difficult to conclude which is true. As the spectral behavior and scattering properties are different, future mission on outer solar system will help to unveil the morphological nature.', 'In both sphere and aggregate cases, total haze mass flux (tholin+ice) is about an order of magnitude lower than that on Triton. Given predominant icy composition, Titan-like haze production rate on Triton is likely much lower than that on Titan.', 'Our findings are bloadly consistent with the results and expectation of a Pluto (+Triton) haze paper that appeared in recent Nature Astronomy. I hope our works can complement their paper.\nhttps://t.co/rQEbeARDv2', 'The paper is my first solar system paper. The results highlight a deep connection between haze and cloud formation: vapor condensation considerably alter the haze properties, depending on thermal structure. Hopefully, I would like to explore this phenomena in exoplanet context.']",https://arxiv.org/abs/2012.11932,"The largest moon of Neptune, Triton, possess a cold and hazy atmosphere. Since the discovery of near-surface haze layer during the Voyager fly in 1989, the haze formation mechanism has not been investigated in detail. Here, we provide the first haze microphysical model on Triton. Our model solves the evolution of both size and porosity distributions of haze particles in a self-consistent manner. We simulated the formation of sphere and aggregate hazes with and without condensation of the C$_2$H$_4$ ice. The haze particles can grow into fractal aggregates with mass-equivalent sphere sizes of $\sim0.1$--$1~{\rm {\mu}m}$ and fractal dimension of $D_{\rm f} = 1.8$--$2.2$. The ice-free hazes cannot simultaneously explain both UV and visible observations of Voyager 2, while including the condensation of C$_2$H$_4$ ices provides two better solutions. For ice aggregates, the required total haze mass flux is $\sim2\times{10}^{-15}~{\rm g~{cm}^{-2}~s^{-1}}$. For the icy sphere scenario, the column integrated C$_2$H$_4$ production rate is $\sim8\times{10}^{-15}~{\rm g~{cm}^{-2}~s^{-1}}$, and the ice-free mass flux of $\sim6\times{10}^{-17}~{\rm g~{cm}^{-2}~s^{-1}}$. The UV occultation observations at short wavelength $<0.15~{\rm {\mu}m}$ may slightly favor the icy aggregates. Observations of the haze optical depth and the degree of forward scattering in UV and visible should be able to distinguish whether Triton's hazes are icy spheres or ice aggregates in future Triton missions. ",Haze Formation on Triton
167,1341201038855028738,949703828064129024,Mario Krenn,"[""Human's curiosity drives science.\n\nWe find that artificial curiosity (pioneered by @SchmidhuberAI, @pathak2206) helps deep #ReinforcementLearning agents to efficiently explore the chemical universe: <LINK>\n\nSpearheaded by Luca Thiede w/ @akshat_ai @A_Aspuru_Guzik <LINK>""]",https://arxiv.org/abs/2012.11293,"Computer-aided design of molecules has the potential to disrupt the field of drug and material discovery. Machine learning, and deep learning, in particular, have been topics where the field has been developing at a rapid pace. Reinforcement learning is a particularly promising approach since it allows for molecular design without prior knowledge. However, the search space is vast and efficient exploration is desirable when using reinforcement learning agents. In this study, we propose an algorithm to aid efficient exploration. The algorithm is inspired by a concept known in the literature as curiosity. We show on three benchmarks that a curious agent finds better performing molecules. This indicates an exciting new research direction for reinforcement learning agents that can explore the chemical space out of their own motivation. This has the potential to eventually lead to unexpected new molecules that no human has thought about so far. ","Curiosity in exploring chemical space: Intrinsic rewards for deep
  molecular reinforcement learning"
168,1341045624964300808,976819440288518145,Marc-André Carbonneau,"['Hello friends. At work, we wanted to find the best model to learn a disentangled representation. We found out that measuring disentanglement is challenging. We decided to learn more on metrics and share our conclusions here: <LINK>']",https://arxiv.org/abs/2012.09276,"Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement. ",Measuring Disentanglement: A Review of Metrics
169,1340167309227257856,2324423269,Peyman 𝕄𝕀𝕃𝔸ℕ𝔽𝔸ℝ,"['We propose a loss function based on aggregate 1D Wasserstein distance on projected feature distributions. More stable with far fewer artifacts; improves on state-of-the-art perceptual quality in denoising, super-res, demosaic, deblur &amp; JPG artifact removal\n<LINK> <LINK>', '@EsmalHaj + @2ptmvd  whom I should have tagged in the original post.']",https://arxiv.org/abs/2012.09289,"Features obtained from object recognition CNNs have been widely used for measuring perceptual similarities between images. Such differentiable metrics can be used as perceptual learning losses to train image enhancement models. However, the choice of the distance function between input and target features may have a consequential impact on the performance of the trained model. While using the norm of the difference between extracted features leads to limited hallucination of details, measuring the distance between distributions of features may generate more textures; yet also more unrealistic details and artifacts. In this paper, we demonstrate that aggregating 1D-Wasserstein distances between CNN activations is more reliable than the existing approaches, and it can significantly improve the perceptual performance of enhancement models. More explicitly, we show that in imaging applications such as denoising, super-resolution, demosaicing, deblurring and JPEG artifact removal, the proposed learning loss outperforms the current state-of-the-art on reference-based perceptual losses. This means that the proposed learning loss can be plugged into different imaging frameworks and produce perceptually realistic results. ",Projected Distribution Loss for Image Enhancement
170,1339947842241253377,84822240,Luca Bertinetto 🇮🇹 🇪🇺 🌐,"['New preprint🐣from Steinar Laenen and me.\n\n""On Episodes, Prototypical Networks, and Few-shot Learning"" - <LINK>\n\nTo better understand the usefulness of episodes in few-shot learning, we start with a case study on Prototypical (and Matching) Networks.\n\nMore 👇1/3', 'By functionally dividing batches in the disjoint support set S and query set Q, the episodic strategy of ProtoNets ignores many training pairs (left).\nIf we forego this separation (right), we can use a number of extra pairs that grows as O(ways^2(queries^2+shots^2)).\n\n2/3 https://t.co/33D7XgXK6n', 'With a series of ablations that connect Proto/Matching Nets to the NCA loss [Goldberger et al, 2005], we show that significant performance can be recovered in these methods by simply using standard mini-batches instead of episodes.\n\nCheck out the paper for much more.\n3/3 https://t.co/Xr9ehBbVRl']",https://arxiv.org/abs/2012.09831,"Episodic learning is a popular practice among researchers and practitioners interested in few-shot learning. It consists of organising training in a series of learning problems (or episodes), each divided into a small training and validation subset to mimic the circumstances encountered during evaluation. But is this always necessary? In this paper, we investigate the usefulness of episodic learning in methods which use nonparametric approaches, such as nearest neighbours, at the level of the episode. For these methods, we not only show how the constraints imposed by episodic learning are not necessary, but that they in fact lead to a data-inefficient way of exploiting training batches. We conduct a wide range of ablative experiments with Matching and Prototypical Networks, two of the most popular methods that use nonparametric approaches at the level of the episode. Their ""non-episodic"" counterparts are considerably simpler, have less hyperparameters, and improve their performance in multiple few-shot classification datasets. ","On Episodes, Prototypical Networks, and Few-shot Learning"
171,1339899331533541377,2445322540,Pascal Fua,['Measuring the uncertainty of deep net results is a challenge. Ensembles are one of the most reliable approaches but are computationally demanding. We propose an approach that is much faster while preserving the reliability of ensembles. <LINK> #DeepLearning <LINK>'],https://arxiv.org/abs/2012.08334,"Deep neural networks have amply demonstrated their prowess but estimating the reliability of their predictions remains challenging. Deep Ensembles are widely considered as being one of the best methods for generating uncertainty estimates but are very expensive to train and evaluate. MC-Dropout is another popular alternative, which is less expensive, but also less reliable. Our central intuition is that there is a continuous spectrum of ensemble-like models of which MC-Dropout and Deep Ensembles are extreme examples. The first uses an effectively infinite number of highly correlated models while the second relies on a finite number of independent models. To combine the benefits of both, we introduce Masksembles. Instead of randomly dropping parts of the network as in MC-dropout, Masksemble relies on a fixed number of binary masks, which are parameterized in a way that allows to change correlations between individual models. Namely, by controlling the overlap between the masks and their density one can choose the optimal configuration for the task at hand. This leads to a simple and easy to implement method with performance on par with Ensembles at a fraction of the cost. We experimentally validate Masksembles on two widely used datasets, CIFAR10 and ImageNet. ",Masksembles for Uncertainty Estimation
172,1339606566920540162,816263912630996996,Ivan Evtimov,"['Introducing FoggySight: increasing privacy against facial search services. Similar to Fawkes and Face-off, we propose using adversarial examples. Our focus: a community defense strategy, where ""protectors"" upload “decoys” to poison the lookup database. 1/9 <LINK> <LINK>', 'First, a caveat: our computational resources allowed us to work with only a sample of the VGGFace2 dataset, so we inherit its limitations and sampling errors. Do not use FoggySight until it can be properly vetted for equitable use at-scale. This is an early step, not a done deal.', 'One challenge we focus on: millions of individuals already have photos in those databases. They have not had a chance to modify them. Even if they take them down, links between those photos and their identities remain. 3/9 https://t.co/jtKA1q2kE1', 'Also, people can hardly control the photo used to query the facial search service - that can be taken anywhere, anytime and wearing obscuring accessories is burdensome and error-prone. 4/9', 'We show that protectors can make and inject “decoys” - essentially, adversarial examples that shift the face search model’s output - to crowd out existing clean photos of protected individuals. Decoys protecting A depict someone else in reality but look like A to the model. 5/9', 'When decoys get picked up in scrapes, this happens: when someone searches with a photo of A, they get back an overwhelming majority of photos belonging to the protectors. Anybody using the facial search service cannot be certain that A is in the query photo. 6/9 https://t.co/VIpIbOU7nQ', 'This also works if protectors do not know the internals of the model. But then protectors need to introduce decoys with higher magnitude modifications and more of them. Maybe facial search companies should publish their models so that FoggySight-like protections can be enabled? https://t.co/GyTjNAk3BC', 'Joint work with @PascalSturmfels and @yoshi_kohno, let us know what you think! 7/9', 'Finally, as I mentioned at the top, this idea is not far off from two other works: Fawkes by @shawnshan26 @em_wenger Jiayun Zhang, Huiying Li, Heather Zheng, and @ravenben (https://t.co/Mnt16wrGDk)...', '...and Face-off by @madison_sp @VarunChandrase3, Chuhan Gao, Brian Tang, Kassem Fawaz, @jhasomesh, and Suman Banerjee (https://t.co/cDSBrb5SbJ) Read all 3 for full context :) 9/9', '@VarunChandrase3 It is a very important/interesting area! 😛']",https://arxiv.org/abs/2012.08588,"Advances in deep learning algorithms have enabled better-than-human performance on face recognition tasks. In parallel, private companies have been scraping social media and other public websites that tie photos to identities and have built up large databases of labeled face images. Searches in these databases are now being offered as a service to law enforcement and others and carry a multitude of privacy risks for social media users. In this work, we tackle the problem of providing privacy from such face recognition systems. We propose and evaluate FoggySight, a solution that applies lessons learned from the adversarial examples literature to modify facial photos in a privacy-preserving manner before they are uploaded to social media. FoggySight's core feature is a community protection strategy where users acting as protectors of privacy for others upload decoy photos generated by adversarial machine learning algorithms. We explore different settings for this scheme and find that it does enable protection of facial privacy -- including against a facial recognition service with unknown internals. ",FoggySight: A Scheme for Facial Lookup Privacy
173,1339599440143740929,38897222,Grigory.ai Yaroslavtsev,"['First paper with an undergrad (Stanislav Naumov, who interned a year ago) and a venture into AAAI: <LINK> Want to find structure in a large collection of CV/NLP dep embedding vectors? We have rigorous approaches and practical methods.', 'There is also an insight into how hierarchies persist regardless of the specific embedding used.']",https://arxiv.org/abs/2012.08466,"We initiate a comprehensive experimental study of objective-based hierarchical clustering methods on massive datasets consisting of deep embedding vectors from computer vision and NLP applications. This includes a large variety of image embedding (ImageNet, ImageNetV2, NaBirds), word embedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from several popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our study includes datasets with up to $4.5$ million entries with embedding dimensions up to $2048$. In order to address the challenge of scaling up hierarchical clustering to such large datasets we propose a new practical hierarchical clustering algorithm B++&C. It gives a 5%/20% improvement on average for the popular Moseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared to a wide range of classic methods and recent heuristics. We also introduce a theoretical algorithm B2SAT&C which achieves a $0.74$-approximation for the CKMM objective in polynomial time. This is the first substantial improvement over the trivial $2/3$-approximation achieved by a random binary tree. Prior to this work, the best poly-time approximation of $\approx 2/3 + 0.0004$ was due to Charikar et al. (SODA'19). ",Objective-Based Hierarchical Clustering of Deep Embedding Vectors
174,1339533400596766722,1162662312962121728,Benjamin Wölfl,['In our new preprint (find it here: <LINK>) we expand an inference procedure for estimating the effective reproduction number by considering superspreading.'],https://arxiv.org/abs/2012.08843,"A primary quantity of interest in the study of infectious diseases is the average number of new infections that an infected person produces. This so-called reproduction number has significant implications for the disease progression. There has been increasing literature suggesting that superspreading, the significant variability in number of new infections caused by individuals, plays an important role in the spread of SARS-CoV-2. In this paper, we consider the effect that such superspreading has on the estimation of the reproduction number and subsequent estimates of future cases. Accordingly, we employ a simple extension to models currently used in the literature to estimate the reproduction number and present a case-study of the progression of COVID-19 in Austria. Our models demonstrate that the estimation uncertainty of the reproduction number increases with superspreading and that this improves the performance of prediction intervals. Of independent interest is the derivation of a transparent formula that connects the extent of superspreading to the width of credible intervals for the reproduction number. This serves as a valuable heuristic for understanding the uncertainty surrounding diseases with superspreading. ","Disease Momentum: Estimating the Reproduction Number in the Presence of
  Superspreading"
175,1339329005934104576,1210312444221935616,Cyrus Rashtchian,"['New paper on Approximate Trace Reconstruction with Sami Davies, Miki Racz, Ben Schiffer. You get samples of an unknown n-bit string subject to iid deletions. We study the complexity of finding a string that is close in *edit distance* to the original:\n\n<LINK> <LINK>', 'The motivation comes from practical applications (e.g., DNA data storage), where it would often be fine to get an approximation, as long as you are able to save in the number of samples needed. See our survey for background: https://t.co/HHajyaLORh', 'My favorite result is a black-box lower bound showing that we can take lower bounds for exact reconstruction and turn them into slightly weaker bounds for approximate reconstruction. For example, if you want to get within edit distance n^{1/4}, you need Omega(n^{9/8}) samples. https://t.co/qPxbbz1aiR', 'To complement this, we also provide a handful of new algorithms and upper bounds.  We show that for some large and non-trivial families of strings, there are ""inherently approximate"" algorithms that can get a good approximation using only polylog(n) samples. https://t.co/2ARhFKTgau', 'The main technique is to roughly identify very dense or sparse substrings and then approximate these portions with 1-runs or 0-runs. So we pushed this idea as far as we could, and this led to the families of strings that we can approximately reconstruct.', ""The holy grail would be to show that you can get within edit distance n/100 with poly(n) samples. I don't know how to prove this, but maybe you do! Let me know if you want to chat about this or if you end up solving it!"", 'While our upper bounds leave much room for improvement, we do at least get a separation where many strings need Omega(n) samples for exact reconstruction, but we can approximately reconstruct them with much fewer samples.']",https://arxiv.org/abs/2012.06713,"In the usual trace reconstruction problem, the goal is to exactly reconstruct an unknown string of length $n$ after it passes through a deletion channel many times independently, producing a set of traces (i.e., random subsequences of the string). We consider the relaxed problem of approximate reconstruction. Here, the goal is to output a string that is close to the original one in edit distance while using much fewer traces than is needed for exact reconstruction. We present several algorithms that can approximately reconstruct strings that belong to certain classes, where the estimate is within $n/\mathrm{polylog}(n)$ edit distance, and where we only use $\mathrm{polylog}(n)$ traces (or sometimes just a single trace). These classes contain strings that require a linear number of traces for exact reconstruction and which are quite different from a typical random string. From a technical point of view, our algorithms approximately reconstruct consecutive substrings of the unknown string by aligning dense regions of traces and using a run of a suitable length to approximate each region. To complement our algorithms, we present a general black-box lower bound for approximate reconstruction, building on a lower bound for distinguishing between two candidate input strings in the worst case. In particular, this shows that approximating to within $n^{1/3 - \delta}$ edit distance requires $n^{1 + 3\delta/2}/\mathrm{polylog}(n)$ traces for $0< \delta < 1/3$ in the worst case. ",Approximate Trace Reconstruction
176,1339253278459240449,1117093805499355136,Marilena Loverde,"['First paper by my student Charuhas Shiveshwarkar! (and @sciencedrew ) We study scale-dependent halo bias and  bispectrum induced by the horizon-scale perturbations in radiation. That is, in CMB photons and massless neutrinos. A small but nonzero effect! <LINK> <LINK>']",https://arxiv.org/abs/2012.04691,"We investigate the gravitational effect of large-scale radiation perturbations on small-scale structure formation. In addition to making the growth of matter perturbations scale dependent, the free-streaming of radiation also affects the coupling between structure formation at small and large scales. We study this using Separate Universe N-body simulations to compute the (isotropized) squeezed-limit matter bispectrum and the linear halo bias. Our results show that the scale dependence in the growth of long-wavelength matter perturbations, caused by radiation, translates into these quantities acquiring a non-trivial scale-dependence at $k\lesssim 0.05$ Mpc$^{-1}$. In a universe with radiation composed of cosmic microwave background photons and three species of massless neutrinos, the bias of halos with $b = 2$ at high $k$ will decrease by $0.29\%,\ 0.45\%$ and $0.8\%$ between $k = 0.05$ Mpc$^{-1}$ and $k = 0.0005$ Mpc$^{-1}$ at redshifts $z=0,\ 1$, and $3$ respectively. For objects with $b\gg1$, these differences approach $0.43\%,\ 0.68\%$ and $1.2\%$ respectively. ","Scale-dependent halo bias and the squeezed limit bispectrum in the
  presence of radiation"
177,1339034144194633728,1158733934,Barbara Engelhardt,"['In these hard times, Didong Li, Andy Jones, &amp; I just posted a preprint on probabilistic contrastive PCA. In scientific applications, often we want to find directions of maximal variation in cases relative to controls. <LINK> <LINK>', 'Contrastive PCA (Zou 2013; Abid 2018) does this, but we build a probabilistic model with CPCA and PPCA as special cases. We provide theoretical justification for how to adapt the hyperparameter based on the size of the case/control data. PCPCA allows imputation and missing data. https://t.co/gWT3HFDoaf', 'It has great behavior on case/control expression data: both distinguishing cases and controls precisely and also finding structure in the cases. Feedback welcome! Code available: https://t.co/5NH5HHQFxR https://t.co/qnfGfbkRu9']",http://arxiv.org/abs/2012.07977,"Dimension reduction is useful for exploratory data analysis. In many applications, it is of interest to discover variation that is enriched in a ""foreground"" dataset relative to a ""background"" dataset. Recently, contrastive principal component analysis (CPCA) was proposed for this setting. However, the lack of a formal probabilistic model makes it difficult to reason about CPCA and to tune its hyperparameter. In this work, we propose probabilistic contrastive principal component analysis (PCPCA), a model-based alternative to CPCA. We discuss how to set the hyperparameter in theory and in practice, and we show several of PCPCA's advantages over CPCA, including greater interpretability, uncertainty quantification and principled inference, robustness to noise and missing data, and the ability to generate data from the model. We demonstrate PCPCA's performance through a series of simulations and case-control experiments with datasets of gene expression, protein expression, and images. ",Probabilistic Contrastive Principal Component Analysis
178,1338667309804761090,1169068112177745922,Alexis Plascencia,"['Today @fileviez, @clamurgal and I have a new paper on the arXiv where we study the correlations between dark matter and the decays of a Higgs in a new sector that only couples to quarks  <LINK>\n\nBaryonic Higgs and Dark Matter 1/10 <LINK>', 'We study the theory with local Baryon number that needs the minimal number of fermionic representations to cancel the gauge anomalies (4 new rep’s). One of these representations is a good dark matter candidate since it is neutral and automatically stable 😀 2/10', 'We find three different regions in the parameter space depending on which dark matter annihilation channel gives the dominant contribution to the dark matter relic density. 3/10 https://t.co/tWAYTCe71y', 'We also computed all the tree-level and one-loop decays of the Higgs responsible for the breaking of the U(1)_B.  You can find all the details in Appendix B of the paper 😀 4/10 https://t.co/HjaqPli5xt', 'We find that the most interesting region is the one in which the dominant annihilation channel is DM DM → Z_B h_B since it does not rely on any resonance to obtain the correct relic abundance. 5/10', 'Furthermore, the decay of the new Higgs into a pair of photons can be a few percent! Much larger than the one for the SM Higgs of 0.2%. 6/10 https://t.co/V73vEHA14Z', 'Since constraints from Xenon-1T require the scalar mixing angle to be small, we study the p p → Z_B h_B associated production at the LHC which is not suppressed by the mixing angle.  7/10', 'This is the expected number of events in the g_B vs M_ZB plane for the  High Luminosity LHC. The regions in grey are ruled out from dijet searches by the ATLAS and CMS collaborations.  8/10 https://t.co/o4krnogn9v', 'For the search channel with missing energy in the final state things get more involved since we require the invisible decay of the new Higgs to be the dominant one. 9/10', 'This means the decay h_B → Z_B Z_B needs to be kinematically closed; on the other hand, to satisfy the dark matter relic density we need M_DM = M_ZB/2, so there is only a small window for the mass of the ZB gauge boson for this to take place. 10/10 https://t.co/RCcM8SBhS8']",https://arxiv.org/abs/2012.06599,"We discuss the correlation between dark matter and Higgs decays in gauge theories where the dark matter is predicted from anomaly cancellation. In these theories, the Higgs responsible for the breaking of the gauge symmetry generates the mass for the dark matter candidate. We investigate the Higgs decays in the minimal gauge theory for Baryon number. After imposing the dark matter density and direct detection constraints, we find that the new Higgs can have a large branching ratio into two photons or into dark matter. Furthermore, we discuss the production channels and the unique signatures at the Large Hadron Collider. ",Baryonic Higgs and Dark Matter
179,1338603526511947777,209022852,David Yllanes,"['Our latest preprint is up on the @arxiv.\n\nWe study the effect of thermal fluctuations on buckling. With a mean-field theory and simulations, we quantify the T dependence of the buckling threshold, according to the renormalisation of the elastic constants.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2012.06565,"Understanding thin sheets, ranging from the macro to the nanoscale, can allow control of mechanical properties such as deformability. Out-of-plane buckling due to in-plane compression can be a key feature in designing new materials. While thin-plate theory can predict critical buckling thresholds for thin frames and nanoribbons at very low temperatures, a unifying framework to describe the effects of thermal fluctuations on buckling at more elevated temperatures presents subtle difficulties. We develop and test a theoretical approach that includes both an in-plane compression and an out-of-plane perturbing field to describe the mechanics of thermalised ribbons above and below the buckling transition. We show that, once the elastic constants are renormalised to take into account the ribbon's width (in units of the thermal length scale), we can map the physics onto a mean-field treatment of buckling, provided the length is short compared to a ribbon persistence length. Our theoretical predictions are checked by extensive molecular dynamics simulations of thin thermalised ribbons under axial compression. ",Thermal buckling and symmetry breaking in thin ribbons under compression
180,1338499115697774592,3315576304,Syksy Räsänen,"[""I'm pleased with this paper, where we* simulated a small patch of the primordial universe about 256 million times to find out how often they collapse into a black holes that could be dark matter.\n\n* Meaning my collaborators, really.\n\n<LINK>""]",https://arxiv.org/abs/2012.06551,"We consider quantum diffusion in ultra-slow-roll (USR) inflation. Using the $\Delta N$ formalism, we present the first stochastic calculation of the probability distribution $P(\mathcal{R})$ of the curvature perturbation during USR. We capture the non-linearity of the system, solving the coupled evolution of the coarse-grained background with random kicks from the short wavelength modes, simultaneously with the mode evolution around the stochastic background. This leads to a non-Markovian process from which we determine the highly non-Gaussian tail of $P(\mathcal{R})$. Studying the production of primordial black holes in a viable model, we find that stochastic effects during USR increase their abundance by a factor $\sim 10^5$ compared to the Gaussian approximation. ","Non-Gaussian tail of the curvature perturbation in stochastic
  ultra-slow-roll inflation: implications for primordial black hole production"
181,1337617823942672384,1119400976685965313,Daniel Brown,['Come visit our poster on Value Alignment Verification at the NeurIPS HAMLETS workshop on Sat. We study how a human can efficiently test whether the goals and behavior of other agents are aligned with their values: <LINK>. Joint work with @bayes_irl @scottniekum'],http://arxiv.org/abs/2012.01557,"As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values. The goal is to construct a kind of ""driver's test"" that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents and propose and analyze heuristic and approximate value alignment verification tests in a wide range of gridworlds and a continuous autonomous driving domain. Finally, we prove that there exist sufficient conditions such that we can verify exact and approximate alignment across an infinite set of test environments via a constant-query-complexity alignment test. ",Value Alignment Verification
182,1337424479228669953,1055358835,M.Bülent Sarıyıldız,"['Generalization is at the heart of representation learning; yet the impact of the *semantic relationship* between concepts seen during training and downstream datasets is unclear. In our recent work (<LINK>), we propose a principled way of measuring exactly that. <LINK>', 'To measure ""concept generalization"" in a controlled manner, we use the ImageNet-21K dataset and its ontology. Defining the ImageNet-1K concepts as the set of ""seen"" concepts, we rank all other (""unseen"") ImageNet concepts in increasing semantic similarity to the set of seen ones.', 'Based on this ordering and after discarding unsafe concepts or ones with few images, we define 5 disjoint concept generalization ***levels***, i.e. 5 imageNet-sized (1000-class, approx 1.1M images) datasets whose concepts are semantically less and less similar to ImageNet-1K. https://t.co/C4wvy0v4E3', 'We call the above the ***Concept Generalization (CoG) benchmark***. The basic protocol is simple: We first extract features for all 5 levels, and then train logistic regression classifiers on top for each level.', 'Since ImageNet-1K is the set of seen concepts for CoG, we can evaluate out-of-the-box any publicly available ImageNet-1K-pretrained model! Below are some highlights after analysing the performance of 3 (semi-)supervised and 4 self-supervised (MoCo, SimCLR, SWAV, BYOL) methods. https://t.co/wtAhGNowyt', 'We can see that the performance of all the models monotonically decreases as we move to levels semantically further from the seen ones. Also, despite the superiority of Sup over the self-supervised models when evaluated on ImageNet-1K, it is outperformed by them on most levels. https://t.co/fLiei3NfBa', 'In the paper, we further measure generalization from a few samples per concept, and further study the topology of the feature space across levels via clustering and measuring alignment and uniformity.', 'This is a joint work with my supervisors @dlarlus @inthebrownbag and @skamalas (THANK YOU SO MUCH!) @naverlabseurope']",https://arxiv.org/abs/2012.05649,"Measuring concept generalization, i.e., the extent to which models trained on a set of (seen) visual concepts can be leveraged to recognize a new set of (unseen) concepts, is a popular way of evaluating visual representations, especially in a self-supervised learning framework. Nonetheless, the choice of unseen concepts for such an evaluation is usually made arbitrarily, and independently from the seen concepts used to train representations, thus ignoring any semantic relationships between the two. In this paper, we argue that the semantic relationships between seen and unseen concepts affect generalization performance and propose ImageNet-CoG, a novel benchmark on the ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in a principled way. Our benchmark leverages expert knowledge that comes from WordNet in order to define a sequence of unseen IN-21K concept sets that are semantically more and more distant from the ImageNet-1K (IN-1K) subset, a ubiquitous training set. This allows us to benchmark visual representations learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31 convolution and transformer-based models and show how different architectures, levels of supervision, regularization techniques and use of web data impact the concept generalization performance. ",Concept Generalization in Visual Representation Learning
183,1337300174528520192,1054424174,Roland Herzog,"['Our preprint ""A manifold of planar triangular meshes with complete Riemannian metric"" with @Estefy16Loayza is out! We propose a fresh approach to mesh deformations without the need to re-mesh or postprocess otherwise. <LINK> <LINK>']",https://arxiv.org/abs/2012.05624,"Shape spaces are fundamental in a variety of applications including image registration, morphing, matching, interpolation, and shape optimization. In this work, we consider two-dimensional shapes represented by triangular meshes of a given connectivity. We show that the collection of admissible configurations representable by such meshes form a smooth manifold. For this manifold of planar triangular meshes we propose a geodesically complete Riemannian metric. It is a distinguishing feature of this metric that it preserves the mesh connectivity and prevents the mesh from degrading along geodesic curves. We detail a symplectic numerical integrator for the geodesic equation in its Hamiltonian formulation. Numerical experiments show that the proposed metric keeps the cell aspect ratios bounded away from zero and thus avoids mesh degradation along arbitrarily long geodesic curves. ",A Manifold of Planar Triangular Meshes with Complete Riemannian Metric
184,1337133680670756865,4350016234,Dr Fleur Meddens,"[""Are you the firstborn child in your family? You're likely to have higher educational attainment (EA) than your younger siblings. In our new GxE preprint, we find that you only benefit from being firstborn if you have a higher EA polygenic score! See <LINK>"", 'Congrats to @DilyaMuslimova on the first chapter of her Phd thesis! 🎉🎉', 'In this paper, we are able to extract exogenous information in both G and E by (a) using N = 15,019 full siblings of the @uk_biobank and (b) using birth order, which is unrelated to EA PGS within families. Hence, both G and E can be considered random shocks in this setting.', 'On average, firstborns complete ~4.5 months more schooling than their younger sibs. Firstborns with an EA PGS of +1 SD enjoy an *additional* ~1.9 months of schooling. Firstborns with below average EA PGS do not benefit from being firstborn at all.', ""Why is the case? Firstly, firstborns enjoy their parents' undivided attention until the birth of younger siblings. Other research shows that firstborns receive 20-30 minutes more daily quality time compared to their younger siblings! We are not able to test this, however."", 'Secondly, we think kids with above average EA PGS benefit disproportionally more from this supposed time investment because of ""dynamic complementarity"" in skill formation, where subsequent ""investments"" pay off more for kids with higher endowments.', 'Please note that this a pre-print, so any comments and suggestions are especially welcome!', '@Scientific_Bird We definitely did not consider that, thank you!', '@LEdeRuiter We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out!', '@jessicabakerphd We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out!', '@sihamsikander I think we tried that, @DilyaMuslimova ?', '@MOEENRIAZ Great question for @DilyaMuslimova, more details in section D of the paper. This was a bit tricky, because the age gaps may not be constant within one family (different age gaps between sibs).']",https://arxiv.org/abs/2012.05021,"The birth order literature emphasizes the role of parental investments in explaining why firstborns have higher human capital outcomes than their laterborn siblings. We use birth order as a proxy for investments and interact it with genetic endowments. Exploiting only within-family variation in both ensures they are exogenous as well as orthogonal to each other. As such, our setting is informative about the existence of dynamic complementarity in skill production. Our empirical analysis exploits data from 15,019 full siblings in the UK Biobank. We adopt a family-fixed effects strategy combined with instrumental variables to deal with endogeneity issues arising from omitted variables and measurement error. We find that birth order and genetic endowments interact: those with above-average genetic endowments benefit disproportionally more from being firstborn compared to those with below-average genetic endowments. This finding is a clean example of how genetic endowments (nature) and the environment (nurture) interact in producing educational attainment. Moreover, our results are consistent with the existence of dynamic complementarity in skill formation: additional parental investments associated with being firstborn are more effective for those siblings who randomly inherited higher genetic endowments for educational attainment. ","Dynamic complementarity in skill production: Evidence from genetic
  endowments and birth order"
185,1336544400856543236,99270209,Guillermo Valle,"['I’m super excited to release this! \n\nWhat do we want from a generalization theory of deep learning?\n\nWe propose 7 desiderata (Ds),\n\nreview how existing bounds do at them,\n\nand show that a marginal-likelihood PAC-Bayes bound does better at most Ds\n\n<LINK>', 'The desiderata are:\n\nThe predictions should scale correctly when changing\n1 data complexity\n2 training set size\n3 architecture\n4 optimizer\n\nThe theory should also be\n5 non-vacuous\n6 efficiently computable\n7 rigorous', 'The more of these you can satisfy the better. Which ones matter more depends on the application', 'To help us study the vast literature on generalization bounds, we classify existing types of bounds according to how many assumptions they make on the data or algorithm\n\n(This+Section 4 should also be useful as a tutorial/review the field, we hope) https://t.co/RMoWkh3LOp', 'The main conclusion from the review is that we want bounds which are data and algorithm dependent.\n\nWe prove one such bound, which is a high-probability version of a previous PAC-Bayes bound, and is basically proportional to the marginal likelihood https://t.co/HIswoYy92h', 'The bound does well at predicting what the error does when changing data complexity https://t.co/bM2Tz0KgW6', 'It also predicts learning curves (m is training set size) remarkably well, accross datasets and architectures! https://t.co/14YHZxvxRK', 'Here we see that the exponents predicted from the bound in two different ways correlate well with the empirical learning curve exponents. https://t.co/euEeFfnAFg', 'The bound is also able to capture some of the variation in generalization from changing the architecture (among several SOTA computer vision architectures), across datasets https://t.co/BQkXZroXJV', 'We suggest that one of the main reasons our bound works better than previous ones may be because it works in function space which is able to capture the (statistical) properties of the behavior of neural nets, more easily than parameter space-based bounds.', 'Finally, we discuss potential applications of the bound, including to neural architecture search (NAS). Perhaps combining the use of NNGP for NAS https://t.co/U3Fynzrs3E, with ideas of using learning curves for NAS.', 'btw @roydanroy this is the paper i\'ve been mentioning to you several times, which has been ""cooming soon"" for quite a while \n@_vaishnavh I also talked about this paper with u in an email earlier this year\n@jaschasd @TheGregYang may also be interested as we use NNGPs extensively^^']",https://arxiv.org/abs/2012.04115,"Generalization in deep learning has been the topic of much recent theoretical and empirical research. Here we introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Such predictions should 1) scale correctly with data complexity; 2) scale correctly with training set size; 3) capture differences between architectures; 4) capture differences between optimization algorithms; 5) be quantitatively not too far from the true error (in particular, be non-vacuous); 6) be efficiently computable; and 7) be rigorous. We focus on generalization error upper bounds, and introduce a categorisation of bounds depending on assumptions on the algorithm and data. We review a wide range of existing approaches, from classical VC dimension to recent PAC-Bayesian bounds, commenting on how well they perform against the desiderata. We next use a function-based picture to derive a marginal-likelihood PAC-Bayesian bound. This bound is, by one definition, optimal up to a multiplicative constant in the asymptotic limit of large training sets, as long as the learning curve follows a power law, which is typically found in practice for deep learning problems. Extensive empirical analysis demonstrates that our marginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results for 6 and 7 are promising, but not yet fully conclusive, while only desideratum 4 is currently beyond the scope of our bound. Finally, we comment on why this function-based bound performs significantly better than current parameter-based PAC-Bayes bounds. ",Generalization bounds for deep learning
186,1336515249063800832,2783180568,Max Radin,"['Excited to share our study comparing state-of-the-art VQE to classical quantum-chemistry methods! We found that for small organic molecules, classical methods are much faster. Very grateful to have worked with @jfgonthier_qc, @RomeroFontalvoJ, and @bp_plc <LINK> <LINK>']",https://arxiv.org/abs/2012.04001,"Recent advances in Noisy Intermediate-Scale Quantum (NISQ) devices have brought much attention to the potential of the Variational Quantum Eigensolver (VQE) and related techniques to provide practical quantum advantage in computational chemistry. However, it is not yet clear whether such algorithms, even in the absence of device error, could achieve quantum advantage for systems of practical interest and how large such an advantage might be. To address these questions, we have performed an exhaustive set of benchmarks to estimate number of qubits and number of measurements required to compute the combustion energies of small organic molecules to within chemical accuracy using VQE as well as state-of-the-art classical algorithms. We consider several key modifications to VQE, including the use of Frozen Natural Orbitals, various Hamiltonian decomposition techniques, and the application of fermionic marginal constraints. Our results indicate that although Frozen Natural Orbitals and low-rank factorizations of the Hamiltonian significantly reduce the qubit and measurement requirements, these techniques are not sufficient to achieve practical quantum computational advantage in the calculation of organic molecule combustion energies. This suggests that new approaches to estimation leveraging quantum coherence, such as Bayesian amplitude estimation [arxiv:2006.09350, arxiv:2006.09349], may be required in order to achieve practical quantum advantage with near-term devices. Our work also highlights the crucial role that resource and performance assessments of quantum algorithms play in identifying quantum advantage and guiding quantum algorithm design. ","Identifying challenges towards practical quantum advantage through
  resource estimation: the measurement roadblock in the variational quantum
  eigensolver"
187,1336349078276464641,10236922,Rayid Ghani,"['There is not always a tradeoff between ""accuracy"" and ""fairness"" in ML/AI. Our recent empirical work investigating the accuracy cost of mitigating disparities across policy problems - we find that we can achieve fairness without sacrificing accuracy <LINK>']",https://arxiv.org/abs/2012.02972,"Growing use of machine learning in policy and social impact settings have raised concerns for fairness implications, especially for racial minorities. These concerns have generated considerable interest among machine learning and artificial intelligence researchers, who have developed new methods and established theoretical bounds for improving fairness, focusing on the source data, regularization and model training, or post-hoc adjustments to model scores. However, little work has studied the practical trade-offs between fairness and accuracy in real-world settings to understand how these bounds and methods translate into policy choices and impact on society. Our empirical study fills this gap by investigating the impact of mitigating disparities on accuracy, focusing on the common context of using machine learning to inform benefit allocation in resource-constrained programs across education, mental health, criminal justice, and housing safety. Here we describe applied work in which we find fairness-accuracy trade-offs to be negligible in practice. In each setting studied, explicitly focusing on achieving equity and using our proposed post-hoc disparity mitigation methods, fairness was substantially improved without sacrificing accuracy. This observation was robust across policy contexts studied, scale of resources available for intervention, time, and relative size of the protected groups. These empirical results challenge a commonly held assumption that reducing disparities either requires accepting an appreciable drop in accuracy or the development of novel, complex methods, making reducing disparities in these applications more practical. ","Empirical observation of negligible fairness-accuracy trade-offs in
  machine learning for public policy"
188,1336132443317866497,1167063592941891585,Bonaventure Dossou,"['We propose an AI-based using CNNs to detect Pneumonia from Chest X-rays. Our model achieve high accuracy (91, 04), F1-Score of 97 and AUC 88,04. We also address technical, legal, ethical, and logistical issues, with a blueprint of possible solutions.\n\n<LINK> <LINK>', 'CC: @jacobs_bremen', '@NanaYaaSally Thanks @NanaYaaSally']",https://arxiv.org/abs/2012.03487,"Each year, over 2.5 million people, most of them in developed countries, die from pneumonia [1]. Since many studies have proved pneumonia is successfully treatable when timely and correctly diagnosed, many of diagnosis aids have been developed, with AI-based methods achieving high accuracies [2]. However, currently, the usage of AI in pneumonia detection is limited, in particular, due to challenges in generalizing a locally achieved result. In this report, we propose a roadmap for creating and integrating a system that attempts to solve this challenge. We also address various technical, legal, ethical, and logistical issues, with a blueprint of possible solutions. ",An Approach to Intelligent Pneumonia Detection and Integration
189,1335929920657240064,933091456574808064,Joaquín García de la Cruz ۞,"[""It's been a long way, but I can finally say it's paper day! Check out my first author paper where we use MW-like simulated galaxies to study how the flaring of geometrical thick discs is linked with different aspects of the galaxy, mergers included! 1/6\n<LINK>"", 'Studying the flaring of Mono-Age Populations (MAPs) in the disc, we address three main aspects: 1) thick disc flaring, 2) age radial gradients, 3) are the geometrical thin and thick disc actually two different components? Finally, how does all the above connect with mergers? 2/6 https://t.co/sqYhli2SdJ', 'We find that galaxies with flat thick disc all have age radial gradients, their thin&amp;thick disc are part of the same structure, and have quiescent merger histories (like the MW!) 3/6 https://t.co/U8BSRsBubD', 'Galaxies with flared thick discs are quite more diverse, but those with busier merger histories tend to have flatter age radial gradients &amp; their thin&amp;thick discs tend to form a geometrical bimodal structure. 4/6 https://t.co/aqWXzlr7Jb', 'By looking at the aspects mentioned in 2/6 we are able to find galaxies that resemble the MW. We also find galaxies different from the MW but similar to external galaxies. This helps us to place the MW in the context of the larger population of spiral galaxies. 5/6', 'Please, find all the details in the link and if you have any comments or questions, feel free to reach out!! 6/6']",https://arxiv.org/abs/2012.02741,"Using simulated galaxies in their cosmological context, we analyse how the flaring of mono-age populations (MAPs) influences the flaring and the age structure of geometrically-defined thick discs. We also explore under which circumstances the geometric thin and thick discs are meaningfully distinct components, or are part of a single continuous structure as in the Milky Way. We find that flat thick discs are created when MAPs barely flare or have low surface density at the radius where they start flaring. When looking at the vertical distribution of MAPs, these galaxies show a continuous thin/thick structure. They also have radial age gradients and tend to have quiescent merger histories. Those characteristics are consistent with what is observed in the Milky Way. Flared thick discs, on the other hand, are created when the MAPs that flare have a high surface density at the radius where they start flaring. The thick discs' scale-heights can either be dominated by multiple MAPs or just a few, depending on the mass and scale-height distribution of the MAPs. In a large fraction of these galaxies, thin and thick discs are clearly distinct structures. Finally, flared thick discs have diverse radial age gradients and merger histories, with galaxies that are more massive or that have undergone massive mergers showing flatter age radial gradients in their thick disc. ",On the Flaring of Thick Disc of Galaxies: Insights from Simulations
190,1335685290569072640,1199958835508592640,Michael Dennis,"['Happy to present new work with @natashajaques, @EugeneVinitsky, et. al where we propose PAIRED, a simple training regime using insights from decision theory to generate a curriculum of increasingly complex environments in more settings than self-play <LINK> 1/6 <LINK>', 'In this joint work along with @alexandrebayen, Stuart Russell, Andrew Critch, and @svlevine, we introduce Unsupervised Environment Design (UED) as a framework which encompasses prior approaches such as self-play, domain randomization, and minimax adversarial training. 2/6 https://t.co/j75tmHchzE', 'We show that UED is the dual problem to decisions under ignorance in decision theory, and we show that many of these prior approaches to UED correspond to classical approaches in decision theory. We show that our approach, PAIRED, corresponds to the classic minimax regret. 3/6 https://t.co/jqnNPc5pF5', 'Turns out that minimax regret, unlike the other rules tried, has useful properties which prevent training from stalling. We find that PAIRED motivates the adversary to generate the simplest environment that it knows is solvable that the agent does not yet solve. 4/6', ""There is a lot I am excited about with this work, which I don't have space to get into here. To see more, please check out the paper or see us at #neurips2020 (https://t.co/atWyxexZXF) tomorrow (Monday) at our oral presentation (18:30 PT) or poster session (21:00-23:00 PT) 5/6"", 'You can also see a different side of this project in this video/thread: https://t.co/CLKDmFVgbd  6/6']",https://arxiv.org/abs/2012.02096,"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments. ","Emergent Complexity and Zero-shot Transfer via Unsupervised Environment
  Design"
191,1335555327551561730,878678526865625088,Frank Noe,"['We propose temperature-steerable flows that learn a family of probability distributions parametrized by temperature. Allows to use flows in combination with parallel tempering simulation for sampling. Led by Manuel Dibak and Leon Klein.\n\n<LINK>', ""@_onionesque Check out Hamiltonian Monte Carlo, that's standard there. You sample velocities from a Maxwell Boltzmann distribution, propagate and then accept or reject according to Metropolis.""]",https://arxiv.org/abs/2012.00429,"Boltzmann generators approach the sampling problem in many-body physics by combining a normalizing flow and a statistical reweighting method to generate samples of a physical system's equilibrium density. The equilibrium distribution is usually defined by an energy function and a thermodynamic state, such as a given temperature. Here we propose temperature-steerable flows (TSF) which are able to generate a family of probability densities parametrized by a choosable temperature parameter. TSFs can be embedded in a generalized ensemble sampling framework such as parallel tempering in order to sample a physical system across thermodynamic states, such as multiple temperatures. ",Temperature-steerable flows
192,1334872698133106691,2445322540,Pascal Fua,"['We propose a way to exploit contrastive self-supervised learning to extract rich latent vectors from videos. Given this representation, a mapping to 3D pose can be learned from a very little annotated data.  <LINK> @HelgeRhodin #DeepLearning #computervision <LINK>']",https://arxiv.org/abs/2012.01511,"In this paper we propose an unsupervised learning method to extract temporal information on monocular videos, where we detect and encode subject of interest in each frame and leverage contrastive self-supervised (CSS) learning to extract rich latent vectors. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly disentangle each latent vector into a time-variant component and a time-invariant one. We then show that applying CSS only to the time-variant features and encouraging a gradual transition on them between nearby and away frames while also reconstructing the input, extract rich temporal features into the time-variant component, well-suited for human pose estimation. Our approach reduces error by about 50\% compared to the standard CSS strategies, outperforms other unsupervised single-view methods and matches the performance of multi-view techniques. ","Unsupervised Temporal Learning on Monocular Videos for 3D Human Pose
  Estimation"
193,1334755110505738240,301563239,Isaac Kim,"['Cute project I did with @physicstravels and Patrick Hayden. The problem was practically motivated, but is also mathematically interesting. We had to study properties of linear maps acting on linear maps acting on linear maps acting on a Hilbert space. :)\n<LINK>', '@gregswhitenoise @physicstravels Maybe we should have pushed for that name...']",https://arxiv.org/abs/2012.01676,"Quantum computers are capable of efficiently contracting unitary tensor networks, a task that is likely to remain difficult for classical computers. For instance, networks based on matrix product states or the multi-scale entanglement renormalization ansatz (MERA) can be contracted on a small quantum computer to aid the simulation of a large quantum system. However, without the ability to selectively reset qubits, the associated spatial cost can be exorbitant. In this paper, we propose a protocol that can unitarily reset qubits when the circuit has a common convolutional form, thus dramatically reducing the spatial cost for implementing the contraction algorithm on general near-term quantum computers. This protocol generates fresh qubits from used ones by partially applying the time-reversed quantum circuit over qubits that are no longer in use. In the absence of noise, we prove that the state of a subset of these qubits becomes $|0\ldots 0\rangle$, up to an error exponentially small in the number of gates applied. We also provide a numerical evidence that the protocol works in the presence of noise. We also provide a numerical evidence that the protocol works in the presence of noise, and formulate a condition under which the noise-resilience follows rigorously. ",Recycling qubits in near-term quantum computers
194,1334462594858553344,1951616070,Mattia Rigotti,"['Happy that our paper ""Self-correcting Q-learning"" co-authored with Rong Zhu (Columbia U. &amp; Fudan U.) was accepted at #AAAI2021 🎉\n\nWe propose a new value estimator that overcomes the maximization bias of Q-learning and show it can improve Deep RL.\n\nPaper: <LINK> <LINK>', '@LLogiaco @IBMResearch Thanks Laureline! The best part was clearly the excuse to work and keep in touch with a common friend of ours 😄']",https://arxiv.org/abs/2012.01100,"The Q-learning algorithm is known to be affected by the maximization bias, i.e. the systematic overestimation of action values, an important issue that has recently received renewed attention. Double Q-learning has been proposed as an efficient algorithm to mitigate this bias. However, this comes at the price of an underestimation of action values, in addition to increased memory requirements and a slower convergence. In this paper, we introduce a new way to address the maximization bias in the form of a ""self-correcting algorithm"" for approximating the maximum of an expected value. Our method balances the overestimation of the single estimator used in conventional Q-learning and the underestimation of the double estimator used in Double Q-learning. Applying this strategy to Q-learning results in Self-correcting Q-learning. We show theoretically that this new algorithm enjoys the same convergence guarantees as Q-learning while being more accurate. Empirically, it performs better than Double Q-learning in domains with rewards of high variance, and it even attains faster convergence than Q-learning in domains with rewards of zero or low variance. These advantages transfer to a Deep Q Network implementation that we call Self-correcting DQN and which outperforms regular DQN and Double DQN on several tasks in the Atari 2600 domain. ",Self-correcting Q-Learning
195,1334389947265126401,802543221943439360,Andrea Caputo,"['<LINK>\n\nPaper out, pretty excited with it 🚦🚦🚦\nWith Jose Luis Bernal and  Marc Kamionkowski we propose a new way to detect dark matter decay using Line Intensity Mapping! We essentially suggest to treat the dark matter as a line interloper and look for it 🧐🧐 <LINK>']",https://arxiv.org/abs/2012.00771,"The nature of dark matter is a longstanding mystery in cosmology, which can be studied with laboratory or collider experiments, as well as astrophysical and cosmological observations. In this work, we propose realistic and efficient strategies to detect radiative products from dark-matter decays with line-intensity mapping (LIM) experiments. This radiation will behave as a line interloper for the atomic and molecular spectral lines targeted by LIM surveys. The most distinctive signatures of the contribution from dark-matter radiative decays are an extra anisotropy on the LIM power spectrum due to projection effects, as well as a narrowing and a shift towards higher intensities of the voxel intensity distribution. We forecast the minimum rate of decays into two photons that LIM surveys will be sensitive to as function of the dark-matter mass in the range $\sim 10^{-6}-10$ eV, and discuss how to reinterpret such results for dark matter that decays into a photon and another particle. We find that both the power spectrum and the voxel intensity distribution are expected to be very sensitive to the dark-matter contribution, with the voxel intensity distribution being more promising for most experiments considered. Interpreting our results in terms of the axion, we show that LIM surveys will be extremely competitive to detect its decay products, improving several orders of magnitudes (depending on the mass) the sensitivity of laboratory and astrophysical searches, especially in the mass range $\sim 1-10$ eV. ",Strategies to Detect Dark-Matter Decays with Line-Intensity Mapping
196,1334107030391578626,977906884886827008,Marcos Mariño,"['In certain theories, Stokes rays and their singularities look like peacocks in a pond. In my recent paper with Stavros Garoufalidis and Jie Gu <LINK> we study these peacock patterns in complex Chern-Simons theory <LINK>']",https://arxiv.org/abs/2012.00062,"The partition function of complex Chern-Simons theory on a 3-manifold with torus boundary reduces to a finite dimensional state-integral which is a holomorphic function of a complexified Planck's constant $\tau$ in the complex cut plane and an entire function of a complex parameter $u$. This gives rise to a vector of factorially divergent perturbative formal power series whose Stokes rays form a peacock-like pattern in the complex plane. We conjecture that these perturbative series are resurgent, their trans-series involve two non-perturbative variables, their Stokes automorphism satisfies a unique factorization property and that it is given explicitly in terms of a fundamental matrix solution to a (dual) linear $q$-difference equation. We further conjecture that entries of the Stokes automorphism matrix are the 3D-indices of Dimofte-Gaiotto-Gukov. We provide proofs of our statements regarding the $q$-difference equations and their properties of their fundamental solutions and illustrate our conjectures regarding the Stokes matrices with numerical calculations for the two simplest hyperbolic $4_1$ and $5_2$ knots. ",Peacock patterns and resurgence in complex Chern-Simons theory
197,1334017173849858048,1274360135591309313,Carlos Gonzalez-Ballestero,['New preprint out: <LINK> We propose using solid-state spin baths (e.g. NV centers) to tailor and probe spin waves &amp; discuss exciting prospects for spintronics. In collaboration between us (@Romero_Isart_G @uniinnsbruck @iqoqi) and Toeno van der Sar at @tudelft. <LINK>'],https://arxiv.org/abs/2012.00540,"Spin waves have risen as promising candidate information carriers for the next generation of information technologies. Recent experimental demonstrations of their detection using electron spins in diamond pave the way towards studying the back-action of a controllable paramagnetic spin bath on the spin waves. Here, we present a quantum theory describing the interaction between spin waves and paramagnetic spins. As a case study we consider an ensemble of nitrogen-vacancy spins in diamond in the vicinity of an Yttrium-Iron-Garnet thin film. We show how the back-action of the ensemble results in strong and tuneable modifications of the spin-wave spectrum and propagation properties. These modifications include the full suppression of spin-wave propagation and, in a different parameter regime, the enhancement of their propagation length by $\sim 50\%$. Furthermore, we show how the spin wave thermal fluctuations induce a measurable frequency shift of the paramagnetic spins in the bath. This shift results in a thermal dispersion force that can be measured optically and/or mechanically with a diamond mechanical resonator. In addition, we use our theory to compute the spin wave-mediated interaction between the spins in the bath. We show that all the above effects are measurable by state-of-the-art experiments. Our results provide the theoretical foundation for describing hybrid quantum systems of spin waves and spin baths, and establish the potential of quantum spins as active control, sensing, and interfacing tools for spintronics. ","Towards a quantum interface between spin waves and paramagnetic spin
  baths"
198,1333971104080257026,3166070000,Josepħ Guidry,"['🚨🌟 I Spy Transits &amp; Pulsations (<LINK>)! With @ZachVanderbosch, @jotajotahermes et al., we use excess scatter in @ESAGaia DR2 &amp; @ztfsurvey to find variable white dwarfs, including possibly tripling the number of known WDs hosting transiting debris! Thread 🧵👇🏻', 'As a proof of concept, we inspected the top 1% (121 WDs) of our sample of 12100 WDs centered on the ZZ Ceti instability strip ranked using our metrics. We recovered 39 known variables WDs and confirmed variability in 33/33 observed candidates at @mcdonaldobs!', 'In particular we recover the two WDs known to host transiting planetary debris. Also in the top 1% are two objects demonstrating irregular transits occurring on long timescales (left) and 3 showing more subtle transits in our follow-up McDonald photometry (right). https://t.co/PIaW9byFPm', 'These discoveries are super exciting because WDs are so remarkably simple: they either have pure hydrogen or pure helium atmospheres (mostly). This means any metal pollution from this debris gives us a direct view into the bulk chemical composition of the (former) planet.', 'These 5 new candidates should be closely monitored to confirm their candidacy and to probe for substructures and observe future transits. If all are confirmed, this would more than triple the known number of these systems.', 'Our follow-up @mcdonaldobs and @cerrotololo observations also confirm 29 new pulsating ZZ Cetis, possibly the closest known polar (ZTF J0146+4914, left), and 1 candidate spotted-magnetic variable (ZTF J0534+7707, right)! https://t.co/TsaOA3k8dc', 'Check out the paper for the full scoop! Our study was very restricted and conservative, so broader applications in temperature and space density to future data releases should yield even more fascinating objects!! 🤩🤩', 'Lastly, I am beyond fortunate to have been able to still do this work despite the pandemic and to have do so with such supportive and patient colleagues and advisors.\n\nI hope you all enjoy these results! :)', 'ohh and any grad admissions committee members can y’all take a super close look at this pls 👉🏻👈🏻🥺', '@phy_sicks @ZachVanderbosch @jotajotahermes @ESAGaia @ztfsurvey thanks so much Connor!!!! 😌', '@starstrickenSF @ZachVanderbosch @jotajotahermes @ESAGaia @ztfsurvey Sarafina thank you so, so much this means the world to me!!!!! 😭😭😭😭☺️☺️']",http://arxiv.org/abs/2012.00035,"We present a novel method to detect variable astrophysical objects and transient phenomena using anomalous excess scatter in repeated measurements from public catalogs of Gaia DR2 and Zwicky Transient Facility (ZTF) DR3 photometry. We first provide a generalized, all-sky proxy for variability using only Gaia DR2 photometry, calibrated to white dwarf stars. To ensure more robust candidate detection, we further employ a method combining Gaia with ZTF photometry and alerts. To demonstrate the efficacy, we apply this latter technique to a sample of roughly $12,100$ white dwarfs within 200 pc centered on the ZZ Ceti instability strip, where hydrogen-atmosphere white dwarfs are known to pulsate. Through inspecting the top $1\%$ samples ranked by these methods, we demonstrate that both the Gaia-only and ZTF-informed techniques are highly effective at identifying known and new variable white dwarfs, which we verify using follow-up, high-speed photometry. We confirm variability in all 33 out of 33 ($100\%$) observed white dwarfs within our top $1\%$ highest-ranked candidates, both inside and outside the ZZ Ceti instability strip. In addition to dozens of new pulsating white dwarfs, we also identify five white dwarfs highly likely to show transiting planetary debris; if confirmed, these systems would more than triple the number of white dwarfs known to host transiting debris. ","I Spy Transits and Pulsations: Empirical Variability in White Dwarfs
  Using Gaia and the Zwicky Transient Facility"
199,1346017587495383041,2444302555,Ludovic Denoyer,"['Happy new year !\n\nI am happy to share this exciting work made with @TomVeniat  and @MarcRanzato where we propose new metrics and a benchmark to evaluate multiple dimensions of transfer in continual learning. <LINK>\n\n1/2 <LINK>', 'In addition, we also propose a simple (MNTDP) but very effective NAS-based approach that outperforms most of existing methods on multiple of these dimensions. 2/2']",https://arxiv.org/abs/2012.12631,"Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. ","Efficient Continual Learning with Modular Networks and Task-Driven
  Priors"
200,1344168321327845377,1288954000050552832,Ivan Montero,"['What if we can answer questions from any language reliably, given only an English question answering database?\n\nDetails can be found in a paper I authored as a research intern at Apple! (Huge thanks to @ShayneRedford @lao_ni @drewjohnfrank &amp; @chrisdubois!) <LINK> <LINK>']",https://arxiv.org/abs/2012.14094,"Existing methods for open-retrieval question answering in lower resource languages (LRLs) lag significantly behind English. They not only suffer from the shortcomings of non-English document retrieval, but are reliant on language-specific supervision for either the task or translation. We formulate a task setup more realistic to available resources, that circumvents document retrieval to reliably transfer knowledge from English to lower resource languages. Assuming a strong English question answering model or database, we compare and analyze methods that pivot through English: to map foreign queries to English and then English answers back to target language answers. Within this task setup we propose Reranked Multilingual Maximal Inner Product Search (RM-MIPS), akin to semantic similarity retrieval over the English training set with reranking, which outperforms the strongest baselines by 2.7% on XQuAD and 6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy over state-of-the-art alternatives in challenging settings: low-resource languages, with extensive distractor data and query distribution misalignment. Circumventing retrieval, our analysis shows this approach offers rapid answer generation to almost any language off-the-shelf, without the need for any additional training data in the target language. ","Pivot Through English: Reliably Answering Multilingual Questions without
  Document Retrieval"
201,1343943460420603906,186701821,Aldo Pacchiano,"['Selecting for the right model class in reinforcement learning and bandits is important to find a good policy. We provide a\xa0new algorithmic approach for model selection based on the principle\xa0of regret balancing that guarantees adaptation to the best model:\n<LINK>', '2/3 Our algorithm is based on a simple balancing and elimination procedure that maintains the regret accrued by the base algorithms balanced, while eliminating misspecified ones via a statistical test.', '3/3 We show that our approach works not only in the case of stochastic contexts but also for the setting of stochastic linear bandits with adversarial contexts.', 'w/ C. Dann, C. Gentile, P. Bartlett.']",https://arxiv.org/abs/2012.13045,"We propose a simple model selection approach for algorithms in stochastic bandit and reinforcement learning problems. As opposed to prior work that (implicitly) assumes knowledge of the optimal regret, we only require that each base algorithm comes with a candidate regret bound that may or may not hold during all rounds. In each round, our approach plays a base algorithm to keep the candidate regret bounds of all remaining base algorithms balanced, and eliminates algorithms that violate their candidate bound. We prove that the total regret of this approach is bounded by the best valid candidate regret bound times a multiplicative factor. This factor is reasonably small in several applications, including linear bandits and MDPs with nested function classes, linear bandits with unknown misspecification, and LinUCB applied to linear bandits with different confidence parameters. We further show that, under a suitable gap-assumption, this factor only scales with the number of base algorithms and not their complexity when the number of rounds is large enough. Finally, unlike recent efforts in model selection for linear stochastic bandits, our approach is versatile enough to also cover cases where the context information is generated by an adversarial environment, rather than a stochastic one. ","Regret Bound Balancing and Elimination for Model Selection in Bandits
  and RL"
202,1342123096216645632,81877534,williamxlr,['Imitating Interactive Intelligence\n\nHere we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment\n#MachineLearning  #AI #MultiagentSystems\n\n<LINK>'],https://arxiv.org/abs/2012.05672,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount. ",Imitating Interactive Intelligence
203,1341291782672412673,885070653363298305,Anne Lauscher (she/her),['Visual summaries of scientific papers increase information access for readers. We create a new data set for visual summary identification and propose a new approach based on self-supervised learning: <LINK> Yamamoto @gg42554 @spponzius Morishima @dwsunima <LINK>'],https://arxiv.org/abs/2012.11213,"Providing visual summaries of scientific publications can increase information access for readers and thereby help deal with the exponential growth in the number of scientific publications. Nonetheless, efforts in providing visual publication summaries have been few and far apart, primarily focusing on the biomedical domain. This is primarily because of the limited availability of annotated gold standards, which hampers the application of robust and high-performing supervised learning techniques. To address these problems we create a new benchmark dataset for selecting figures to serve as visual summaries of publications based on their abstracts, covering several domains in computer science. Moreover, we develop a self-supervised learning approach, based on heuristic matching of inline references to figures with figure captions. Experiments in both biomedical and computer science domains show that our model is able to outperform the state of the art despite being self-supervised and therefore not relying on any annotated training data. ","Self-Supervised Learning for Visual Summary Identification in Scientific
  Publications"
204,1339846657429663744,494870213,Thomas Haworth,"['New work looking at old data to find evaporating planet-forming discs (proplyds) in the ""flame nebula"" NGC 2024. A big part of the interest here is that this is a very young region, so even if planets form fast we may have to worry about environment. \n\n<LINK> <LINK>', 'Here again is my #HLTau2020 talk on this paper \n\nhttps://t.co/yvdnm4iYHU']",https://arxiv.org/abs/2012.09166,"A recent survey of the inner $0.35\times0.35$pc of the NGC 2024 star forming region revealed two distinct millimetre continuum disc populations that appear to be spatially segregated by the boundary of a dense cloud. The eastern (and more embedded) population is $\sim0.2-0.5$Myr old, with an ALMA mm continuum disc detection rate of about $45\,$per cent. However this drops to only $\sim15$per cent in the 1Myr western population. When presenting this result, van Terwisga et al. (2020) suggested that the two main UV sources, IRS 1 (a B0.5V star in the western region) and IRS 2b (an O8V star in the eastern region, but embedded) have both been evaporating the discs in the depleted western population. In this paper we report the firm discovery in archival HST data of 4 proplyds and 4 further candidate proplyds in NGC 2024, confirming that external photoevaporation of discs is occurring. However, the locations of these proplyds changes the picture. Only three of them are in the depleted western population and their evaporation is dominated by IRS 1, with no obvious impact from IRS 2b. The other 5 proplyds are in the younger eastern region and being evaporated by IRS 2b. We propose that both populations are subject to significant external photoevaporation, which happens throughout the region wherever discs are not sufficiently shielded by the interstellar medium. The external photoevaporation and severe depletion of mm grains in the 0.2-0.5Myr eastern part of NGC 2024 would be in competition even with very early planet formation. ",Proplyds in the Flame Nebula NGC 2024
205,1339570006191984640,1120577870403911680,Torben Ferber,"['Dark Photons, Dark Higgs, and Dark Matter: Today we (@desy @UBC) published a new phenomenology study using long-lived signatures at Belle II (@belle2collab): <LINK> <LINK>']",https://arxiv.org/abs/2012.08595,"Inelastic dark matter is an interesting scenario for light thermal dark matter which is fully consistent with all cosmological probes as well as direct and indirect dark matter detection. The required mass splitting between dark matter $\chi_1$ and its heavier twin $\chi_2$ is naturally induced by a dark Higgs field which also provides a simple mechanism to give mass to the dark photon $A'$ present in the setup. The corresponding dark Higgs boson $h'$ is naturally the lightest dark sector state and therefore decays into Standard Model particles via Higgs mixing. In this work we study signatures with displaced vertices and missing momentum at Belle II, arising from dark Higgs particles produced in association with dark matter. We find that Belle II can be very sensitive to this scenario, in particular if a displaced vertex trigger is available in the near future. ",Long-lived Dark Higgs and Inelastic Dark Matter at Belle II
206,1339048763864862720,1203139695695269888,Jose Dolz,"['Confident students become masters. Check our latest work on semi-supervised segmentation, where we propose a novel formulation coupling a KL divergence term with an entropy minimization objective\n\nArxiv: <LINK>\nGithub: <LINK> <LINK>']",https://arxiv.org/abs/2012.08051,"Deep segmentation neural networks require large training datasets with pixel-wise segmentations, which are expensive to obtain in practice. Mixed supervision could mitigate this difficulty, with a small fraction of the data containing complete pixel-wise annotations, while the rest being less supervised, e.g., only a handful of pixels are labeled. In this work, we propose a dual-branch architecture, where the upper branch (teacher) receives strong annotations, while the bottom one (student) is driven by limited supervision and guided by the upper branch. In conjunction with a standard cross-entropy over the labeled pixels, our novel formulation integrates two important terms: (i) a Shannon entropy loss defined over the less-supervised images, which encourages confident student predictions at the bottom branch; and (ii) a Kullback-Leibler (KL) divergence, which transfers the knowledge from the predictions generated by the strongly supervised branch to the less-supervised branch, and guides the entropy (student-confidence) term to avoid trivial solutions. Very interestingly, we show that the synergy between the entropy and KL divergence yields substantial improvements in performances. Furthermore, we discuss an interesting link between Shannon-entropy minimization and standard pseudo-mask generation and argue that the former should be preferred over the latter for leveraging information from unlabeled pixels. Through a series of quantitative and qualitative experiments, we show the effectiveness of the proposed formulation in segmenting the left-ventricle endocardium in MRI images. We demonstrate that our method significantly outperforms other strategies to tackle semantic segmentation within a mixed-supervision framework. More interestingly, and in line with recent observations in classification, we show that the branch trained with reduced supervision largely outperforms the teacher. ","Teach me to segment with mixed supervision: Confident students become
  masters"
207,1337417757109968897,1119248934629773313,Javad Shabani,['Not every superconducting transition is a proof that material stays superconducting at milliKelvin. Our new study in Gallium doped Silicon superconductivity shows earlier studies have residual conductivity at mK. But we can fix it. Here is how: <LINK>'],https://arxiv.org/abs/2012.04748,"Hyperdoping with gallium (Ga) has been established as a route to observe superconductivity in silicon (Si). The relatively large critical temperatures (T$_{\rm c}$) and magnetic fields (B$_{\rm c}$) make this phase attractive for cryogenic circuit applications, particularly for scalable hybrid superconductor--semiconductor platforms. However, the robustness of Si:Ga superconductivity at millikelvin temperatures is yet to be evaluated. Here, we report the presence of a reentrant resistive transition below T$_{\rm c}$ for Si:Ga whose strength strongly depends on the distribution of the Ga clusters that precipitate in the implanted Si after annealing. By monitoring the reentrant resistance over a wide parameter space of implantation energies and fluences, we determine conditions that significantly improve the coherent coupling of Ga clusters, therefore, eliminating the reentrant transition even at temperatures as low as 20~mK. ","Tailoring Superconducting Phases Observed in Hyperdoped Si:Ga for
  Cryogenic Circuit Applications"
