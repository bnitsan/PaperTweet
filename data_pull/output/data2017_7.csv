,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,892912467164020737,722842576479322114,Anthony Aguirre,['New paper! Reformulation of quantum entropy using probabilities for coarse-grained observation outcomes. <LINK> @FQXi'],https://arxiv.org/abs/1707.09722,"We extend classical coarse-grained entropy, commonly used in many branches of physics, to the quantum realm. We find two coarse-grainings, one using measurements of local particle numbers and then total energy, and the second using local energy measurements, which lead to an entropy that is defined outside of equilibrium, is in accord with the thermodynamic entropy for equilibrium systems, and reaches the thermodynamic entropy in the long-time limit, even in genuinely isolated quantum systems. This answers the long-standing conceptual problem, as to which entropy is relevant for the formulation of the second thermodynamic law in closed quantum systems. This entropy could be in principle measured, especially now that experiments on such systems are becoming feasible. ",Quantum coarse-grained entropy and thermodynamics
1,892406184921247746,33892163,Ruth Baker,['Our new multilevel Monte Carlo paper now on arXiv: <LINK> - work joint with @Kit_Yates_Maths'],https://arxiv.org/abs/1707.09284,"In this work, we consider the problem of estimating summary statistics to characterise biochemical reaction networks of interest. Such networks are often described using the framework of the Chemical Master Equation (CME). For physically-realistic models, the CME is widely considered to be analytically intractable. A variety of Monte Carlo algorithms have therefore been developed to explore the dynamics of such networks empirically. Amongst them is the multi-level method, which uses estimates from multiple ensembles of sample paths of different accuracies to estimate a summary statistic of interest. {In this work, we develop the multi-level method in two directions: (1) to increase the robustness, reliability and performance of the multi-level method, we implement an improved variance reduction method for generating the sample paths of each ensemble; and (2) to improve computational performance, we demonstrate the successful use of a different mechanism for choosing which ensembles should be included in the multi-level algorithm. ","Robustly simulating biochemical reaction kinetics using multi-level
  Monte Carlo approaches"
2,892386886966284288,87326890,John A. Smolin,"['Just posted a new paper ""Quantifying Coherence and Entanglement via Simple Measurements"" <LINK>', '@dabacon @quantum_aram Good question!  I sick my coauthors on it :)']",https://arxiv.org/abs/1707.09928,"Coherence and entanglement are fundamental properties of quantum systems, promising to power the near future quantum computers, sensors and simulators. Yet, their experimental detection is challenging, usually requiring full reconstruction of the system state. We show that one can extract quantitative bounds to the relative entropy of coherence and the coherent information, coherence and entanglement quantifiers respectively, by a limited number of purity measurements. The scheme is readily implementable with current technology to verify quantum computations in large scale registers, without carrying out expensive state tomography. ",Quantifying Coherence and Entanglement via Simple Measurements
3,892137399370829825,412548718,Kit Yates,['Our new paper on robust multi-level simulation is out on @arxiv <LINK> Check it out.'],https://arxiv.org/abs/1707.09284,"In this work, we consider the problem of estimating summary statistics to characterise biochemical reaction networks of interest. Such networks are often described using the framework of the Chemical Master Equation (CME). For physically-realistic models, the CME is widely considered to be analytically intractable. A variety of Monte Carlo algorithms have therefore been developed to explore the dynamics of such networks empirically. Amongst them is the multi-level method, which uses estimates from multiple ensembles of sample paths of different accuracies to estimate a summary statistic of interest. {In this work, we develop the multi-level method in two directions: (1) to increase the robustness, reliability and performance of the multi-level method, we implement an improved variance reduction method for generating the sample paths of each ensemble; and (2) to improve computational performance, we demonstrate the successful use of a different mechanism for choosing which ensembles should be included in the multi-level algorithm. ","Robustly simulating biochemical reaction kinetics using multi-level
  Monte Carlo approaches"
4,890269597705936896,795877354266456064,KoheiKamadaPhys,['We submitted our new paper on the MF generation from inflation and baryogenesis&amp;GWs. Comments welcome!\n<LINK>'],https://arxiv.org/abs/1707.07943,"In models of inflation driven by an axion-like pseudoscalar field, the inflaton, a, may couple to the standard model hypercharge via a Chern-Simons-type interaction, $L \subset a/(4\Lambda) F\tilde{F}$. This coupling results in explosive gauge field production during inflation, especially at its last stage, which has interesting phenomenological consequences: For one thing, the primordial hypermagnetic field is maximally helical. It is thus capable of sourcing the generation of nonzero baryon number, via the standard model chiral anomaly, around the time of electroweak symmetry breaking. For another thing, the gauge field production during inflation feeds back into the primordial tensor power spectrum, leaving an imprint in the stochastic background of gravitational waves (GWs). In this paper, we focus on the correlation between these two phenomena. Working in the approximation of instant reheating, we (1) update the investigation of baryogenesis via hypermagnetic fields from pseudoscalar inflation and (2) examine the corresponding implications for the GW spectrum. We find that successful baryogenesis requires a suppression scale Lambda of around Lambda ~ 3 x 10^17 GeV, which corresponds to a relatively weakly coupled axion. The gauge field production at the end of inflation is then typically accompanied by a peak in the GW spectrum at frequencies in the MHz range or above. The detection of such a peak is out of reach of present-day technology; but in the future, it may serve as a smoking-gun signal for baryogenesis from pseudoscalar inflation. Conversely, models that do yield an observable GW signal suffer from the overproduction of baryon number, unless the reheating temperature is lower than the electroweak scale. ",Baryon Asymmetry and Gravitational Waves from Pseudoscalar Inflation
5,890210418903445504,114562472,Prof. Emily Levesque ü§ì‚ú®üî≠üìö,['New Massive Stars group paper w/ just-graduated @uwastronomy major @AstroMallory on the host galaxy of GRB 020903! <LINK>'],https://arxiv.org/abs/1707.07697,"GRB 020903 is a long-duration gamma ray burst (LGRB) with a host galaxy close enough and extended enough for spatially-resolved observations, making it one of less than a dozen GRBs where such host studies are possible. GRB 020903 lies in a galaxy host complex that appears to consist of four interacting components. Here we present the results of spatially-resolved spectroscopic observations of the GRB 020903 host. By taking observations at two different position angles we were able to obtain optical spectra (3600-9000{\AA}) of multiple regions in the galaxy. We confirm redshifts for three regions of the host galaxy that match that of GRB 020903. We measure metallicity of these regions, and find that the explosion site and the nearby star-forming regions both have comparable sub-solar metallicities. We conclude that, in agreement with past spatially-resolved studies of GRBs, the GRB explosion site is representative of the host galaxy as a whole rather than localized in a metal-poor region of the galaxy. ",A Spatially Resolved Study of the GRB 020903 Host Galaxy
6,889367680905510912,2301093404,Anirban Santara,['#NEW_PAPER_OUT!\n<LINK>\n\nExtremely informal abstract: The best Artificial Intelligence... <LINK>'],https://arxiv.org/abs/1707.06658,"Imitation learning algorithms learn viable policies by imitating an expert's behavior when reward signals are not available. Generative Adversarial Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies when the expert's behavior is available as a fixed set of trajectories. We evaluate in terms of the expert's cost function and observe that the distribution of trajectory-costs is often more heavy-tailed for GAIL-agents than the expert at a number of benchmark continuous-control tasks. Thus, high-cost trajectories, corresponding to tail-end events of catastrophic failure, are more likely to be encountered by the GAIL-agents than the expert. This makes the reliability of GAIL-agents questionable when it comes to deployment in risk-sensitive applications like robotic surgery and autonomous driving. In this work, we aim to minimize the occurrence of tail-end events by minimizing tail risk within the GAIL framework. We quantify tail risk by the Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse Imitation Learning (RAIL) algorithm. We observe that the policies learned with RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed RAIL algorithm appears as a potent alternative to GAIL for improved reliability in risk-sensitive applications. ",RAIL: Risk-Averse Imitation Learning
7,889309847832973313,2337598033,Geraint F. Lewis,['A new paper on the arxiv! <LINK> <LINK>'],https://arxiv.org/abs/1707.06770,"We describe and test a novel Dark Matter Annihilation Feedback (DMAF) scheme that has been implemented into the well known cosmological simulation code \textsf{GADGET-2}. In the models considered here, dark matter can undergo self-annihilation/decay into radiation and baryons. These products deposit energy into the surrounding gas particles and then the dark matter/baryon fluid is self-consistently evolved under gravity and hydrodynamics. We present tests of this new feedback implementation in the case of idealised dark matter halos with gas components for a range of halo masses, concentrations and annihilation rates. For some dark matter models, DMAF's ability to evacuate gas is enhanced in lower mass, concentrated halos where the injected energy is comparable to its gravitational binding energy. Therefore, we expect the strongest signs of dark matter annihilation to imprint themselves onto the baryonic structure of concentrated dwarf galaxies through their baryonic fraction and star formation history. Finally we present preliminary results of the first self-consistent DMAF cosmological box simulations showing that the small scale substructure is washed out for large annihilation rates. ","Dark Matter Annihilation Feedback in cosmological simulations I: Code
  convergence and idealised halos"
8,888081061565140997,38169381,Nicol√°s Robinson,"[""New paper 'DataCite as a novel bibliometric source' w @RodrigoCostas1 @philippemongeon and W. Jeng now in OA <LINK>""]",https://arxiv.org/abs/1707.06070,"This paper explores the characteristics of DataCite to determine its possibilities and potential as a new bibliometric data source to analyze the scholarly production of open data. Open science and the increasing data sharing requirements from governments, funding bodies, institutions and scientific journals has led to a pressing demand for the development of data metrics. As a very first step towards reliable data metrics, we need to better comprehend the limitations and caveats of the information provided by sources of open data. In this paper, we critically examine records downloaded from the DataCite's OAI API and elaborate a series of recommendations regarding the use of this source for bibliometric analyses of open data. We highlight issues related to metadata incompleteness, lack of standardization, and ambiguous definitions of several fields. Despite these limitations, we emphasize DataCite's value and potential to become one of the main sources for data metrics development. ","DataCite as a novel bibliometric source: Coverage, strengths and
  limitations"
9,887983961514881025,9944362,Luis O. Silva,"['New paper on @arxiv on the kinetic biermann battery, a theoretical ""tour de force"" by kevin schoeffler  <LINK>']",https://arxiv.org/abs/1707.06069,"The dynamical evolution of a fully kinetic, collisionless system with imposed background density and temperature gradients is investigated analytically. The temperature gradient leads to the generation of temperature anisotropy, with the temperature along the gradient becoming larger than that in the direction perpendicular to it. This causes the system to become unstable to pressure anisotropy driven instabilities, dominantly to electron Weibel. When both density and temperature gradients are present and non-parallel to each other, we obtain a Biermann-like linear in time magnetic field growth. Accompanying particle in cell numerical simulations are shown to confirm our analytical results. ","The fully kinetic Biermann battery and associated generation of pressure
  anisotropy"
10,887943778736230401,65010804,Hugh Osborn,"['Apparently I sleep-wrote a new paper on conformal field theory: <LINK>', ""This is one of those papers where reading the title, abstract, and introduction still doesnt give me the foggiest idea of what it's about.""]",https://arxiv.org/abs/1707.06165,"Fixed points for scalar theories in $4-\varepsilon$, $6-\varepsilon$ and $3-\varepsilon$ dimensions are discussed. It is shown how a large range of known fixed points for the four dimensional case can be obtained by using a general framework with two couplings. The original maximal symmetry, $O(N)$, is broken to various subgroups, both discrete and continuous. A similar discussion is applied to the six dimensional case. Perturbative applications of the $a$-theorem are used to help classify potential fixed points. At lowest order in the $\varepsilon$-expansion it is shown that at fixed points there is a lower bound for $a$ which is saturated at bifurcation points. ","Seeking Fixed Points in Multiple Coupling Scalar Theories in the
  $\varepsilon$ Expansion"
11,887320424861892608,223923566,David Van Horn,['New paper: Abstracting Definitional Interpreters w/@daviddarais @pnguyen0112 &amp; N. Labich. To appear @icfp_conference <LINK>'],https://arxiv.org/abs/1707.04755,"In this functional pearl, we examine the use of definitional interpreters as a basis for abstract interpretation of higher-order programming languages. As it turns out, definitional interpreters, especially those written in monadic style, can provide a nice basis for a wide variety of collecting semantics, abstract interpretations, symbolic executions, and their intermixings. But the real insight of this story is a replaying of an insight from Reynold's landmark paper, Definitional Interpreters for Higher-Order Programming Languages, in which he observes definitional interpreters enable the defined-language to inherit properties of the defining-language. We show the same holds true for definitional abstract interpreters. Remarkably, we observe that abstract definitional interpreters can inherit the so-called ""pushdown control flow"" property, wherein function calls and returns are precisely matched in the abstract semantics, simply by virtue of the function call mechanism of the defining-language. The first approaches to achieve this property for higher-order languages appeared within the last ten years, and have since been the subject of many papers. These approaches start from a state-machine semantics and uniformly involve significant technical engineering to recover the precision of pushdown control flow. In contrast, starting from a definitional interpreter, the pushdown control flow property is inherent in the meta-language and requires no further technical mechanism to achieve. ",Abstracting Definitional Interpreters
12,887290364649984001,3301643341,Roger Grosse,['New paper by Aidan Gomez and Mengye Ren on reversible ResNets. O(1) activation storage w/o major loss in accuracy. <LINK>'],https://arxiv.org/abs/1707.04585,"Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth. ","The Reversible Residual Network: Backpropagation Without Storing
  Activations"
13,887248962994024448,292832009,Tomasz Kacprzak,['New paper from our group and ETH Datalab ! We can now do cosmology with Deep Learning.. is that the future? <LINK> <LINK>'],https://arxiv.org/abs/1707.05167,"We demonstrate the potential of Deep Learning methods for measurements of cosmological parameters from density fields, focusing on the extraction of non-Gaussian information. We consider weak lensing mass maps as our dataset. We aim for our method to be able to distinguish between five models, which were chosen to lie along the $\sigma_8$ - $\Omega_m$ degeneracy, and have nearly the same two-point statistics. We design and implement a Deep Convolutional Neural Network (DCNN) which learns the relation between five cosmological models and the mass maps they generate. We develop a new training strategy which ensures the good performance of the network for high levels of noise. We compare the performance of this approach to commonly used non-Gaussian statistics, namely the skewness and kurtosis of the convergence maps. We find that our implementation of DCNN outperforms the skewness and kurtosis statistics, especially for high noise levels. The network maintains the mean discrimination efficiency greater than $85\%$ even for noise levels corresponding to ground based lensing observations, while the other statistics perform worse in this setting, achieving efficiency less than $70\%$. This demonstrates the ability of CNN-based methods to efficiently break the $\sigma_8$ - $\Omega_m$ degeneracy with weak lensing mass maps alone. We discuss the potential of this method to be applied to the analysis of real weak lensing data and other datasets. ",Cosmological model discrimination with Deep Learning
14,887206438896062465,16434310,chrislintott,['New paper up today - combining human &amp; machines outperforms either on their own. Great work from Darryl Wright &amp; co: <LINK>'],https://arxiv.org/abs/1707.05223,"Large modern surveys require efficient review of data in order to find transient sources such as supernovae, and to distinguish such sources from artefacts and noise. Much effort has been put into the development of automatic algorithms, but surveys still rely on human review of targets. This paper presents an integrated system for the identification of supernovae in data from Pan-STARRS1, combining classifications from volunteers participating in a citizen science project with those from a convolutional neural network. The unique aspect of this work is the deployment, in combination, of both human and machine classifications for near real-time discovery in an astronomical project. We show that the combination of the two methods outperforms either one used individually. This result has important implications for the future development of transient searches, especially in the era of LSST and other large-throughput surveys. ",A transient search using combined human and machine classifications
15,887198856383016965,1475769458,Joseph Salmon,"['New paper on Lasso, refitting, Bregman it√©rative...and MCP. \n<LINK>\nCongrats @echzhen']",https://arxiv.org/abs/1707.05232,"A well-know drawback of l_1-penalized estimators is the systematic shrinkage of the large coefficients towards zero. A simple remedy is to treat Lasso as a model-selection procedure and to perform a second refitting step on the selected support. In this work we formalize the notion of refitting and provide oracle bounds for arbitrary refitting procedures of the Lasso solution. One of the most widely used refitting techniques which is based on Least-Squares may bring a problem of interpretability, since the signs of the refitted estimator might be flipped with respect to the original estimator. This problem arises from the fact that the Least-Squares refitting considers only the support of the Lasso solution, avoiding any information about signs or amplitudes. To this end we define a sign consistent refitting as an arbitrary refitting procedure, preserving the signs of the first step Lasso solution and provide Oracle inequalities for such estimators. Finally, we consider special refitting strategies: Bregman Lasso and Boosted Lasso. Bregman Lasso has a fruitful property to converge to the Sign-Least-Squares refitting (Least-Squares with sign constraints), which provides with greater interpretability. We additionally study the Bregman Lasso refitting in the case of orthogonal design, providing with simple intuition behind the proposed method. Boosted Lasso, in contrast, considers information about magnitudes of the first Lasso step and allows to develop better oracle rates for prediction. Finally, we conduct an extensive numerical study to show advantages of one approach over others in different synthetic and semi-real scenarios. ",On Lasso refitting strategies
16,886865482153852928,394580398,Chlo√© Braud,"['New paper on detecting scientific fraud, Workshop on Stylistic Variation @soegaarducph #emnlp2017 <LINK>']",https://arxiv.org/abs/1707.04095,"The problem of detecting scientific fraud using machine learning was recently introduced, with initial, positive results from a model taking into account various general indicators. The results seem to suggest that writing style is predictive of scientific fraud. We revisit these initial experiments, and show that the leave-one-out testing procedure they used likely leads to a slight over-estimate of the predictability, but also that simple models can outperform their proposed model by some margin. We go on to explore more abstract linguistic features, such as linguistic complexity and discourse structure, only to obtain negative results. Upon analyzing our models, we do see some interesting patterns, though: Scientific fraud, for examples, contains less comparison, as well as different types of hedging and ways of presenting logical reasoning. ",Is writing style predictive of scientific fraud?
17,886861417483292673,1707692827,Paul McMillan,"['My new paper with improved distances for RAVE/TGAS stars is online just in time for the #GaiaSprint <LINK> ...', ""The data is online here: https://t.co/CtqDsCwjYI (and on a USB key with me, for anyone at the #GaiaSprint who doesn't want to kill the WiFi)""]",https://arxiv.org/abs/1707.04554,"We combine parallaxes from the first Gaia data release with the spectrophotometric distance estimation framework for stars in the fifth RAVE survey data release. The combined distance estimates are more accurate than either determination in isolation - uncertainties are on average two times smaller than for RAVE-only distances (three times smaller for dwarfs), and 1.4 times smaller than TGAS parallax uncertainties (two times smaller for giants). We are also able to compare the estimates from spectrophotometry to those from Gaia, and use this to assess the reliability of both catalogues and improve our distance estimates. We find that the distances to the lowest log g stars are, on average, overestimated and caution that they may not be reliable. We also find that it is likely that the Gaia random uncertainties are smaller than the reported values. As a byproduct we derive ages for the RAVE stars, many with relative uncertainties less than 20 percent. These results for 219566 RAVE sources have been made publicly available, and we encourage their use for studies that combine the radial velocities provided by RAVE with the proper motions provided by Gaia. A sample that we believe to be reliable can be found by taking only the stars with the flag notification flag_any=0 ",Improved distances and ages for stars common to TGAS and RAVE
18,885860598453604352,882978663825846273,Bethge Lab,['NEW: Our paper on #Foolbox is out! Gen. adversarials with 15+ attacks in most popular DL frameworks. <LINK>  @wielandbr <LINK>'],http://arxiv.org/abs/1707.04131,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at this https URL . The most up-to-date documentation can be found at this http URL . ","Foolbox: A Python toolbox to benchmark the robustness of machine
  learning models"
19,885784558737686528,19510090,Julian Togelius,"['New CIG paper: ""Autoencoder-augmented Neuroevolution for Visual Doom Playing"", by Samuel Alvernaz and me:\n<LINK>', 'Core idea is to train an autoencoder to compress the visual input to a low-dimensional vector and use that as input to an evolved neural net', 'Because evolution seems to work better in small search spaces, and offers a very different solution to the RL problem than e.g. Q-learning', 'Results are not yet state of the art, but this is an approach we will keep exploring.']",https://arxiv.org/abs/1707.03902,"Neuroevolution has proven effective at many reinforcement learning tasks, but does not seem to scale well to high-dimensional controller representations, which are needed for tasks where the input is raw pixel data. We propose a novel method where we train an autoencoder to create a comparatively low-dimensional representation of the environment observation, and then use CMA-ES to train neural network controllers acting on this input data. As the behavior of the agent changes the nature of the input data, the autoencoder training progresses throughout evolution. We test this method in the VizDoom environment built on the classic FPS Doom, where it performs well on a health-pack gathering task. ",Autoencoder-augmented Neuroevolution for Visual Doom Playing
20,885729301290680320,758156197853794304,≈Åukasz Tychoniec,['Magnetic field around baby star changes as you zoom in. We take a close look with @almaobs in this new paper <LINK>'],https://arxiv.org/abs/1707.03827,"We present high angular resolution dust polarization and molecular line observations carried out with the Atacama Large Millimeter/submillimeter Array (ALMA) toward the Class 0 protostar Serpens SMM1. By complementing these observations with new polarization observations from the Submillimeter Array (SMA) and archival data from the Combined Array for Research in Millimeter-wave Astronomy (CARMA) and the James Clerk Maxwell Telescopes (JCMT), we can compare the magnetic field orientations at different spatial scales. We find major changes in the magnetic field orientation between large (~0.1 pc) scales -- where the magnetic field is oriented E-W, perpendicular to the major axis of the dusty filament where SMM1 is embedded -- and the intermediate and small scales probed by CARMA (~1000 AU resolution), the SMA (~350 AU resolution), and ALMA (~140 AU resolution). The ALMA maps reveal that the redshifted lobe of the bipolar outflow is shaping the magnetic field in SMM1 on the southeast side of the source; however, on the northwestern side and elsewhere in the source, low velocity shocks may be causing the observed chaotic magnetic field pattern. High-spatial-resolution continuum and spectral-line observations also reveal a tight (~130 AU) protobinary system in SMM1-b, the eastern component of which is launching an extremely high-velocity, one-sided jet visible in both CO(2-1) and SiO(5-4); however, that jet does not appear to be shaping the magnetic field. These observations show that with the sensitivity and resolution of ALMA, we can now begin to understand the role that feedback (e.g., from protostellar outflows) plays in shaping the magnetic field in very young, star-forming sources like SMM1. ","ALMA observations of dust polarization and molecular line emission from
  the Class 0 protostellar source Serpens SMM1"
21,885235933435527168,2242019113,christian cachin,['Blockchain consensus protocols in the wild - New paper with survey and comparison of #blockchain #consensus  <LINK>'],https://arxiv.org/abs/1707.01873,"A blockchain is a distributed ledger for recording transactions, maintained by many nodes without central authority through a distributed cryptographic protocol. All nodes validate the information to be appended to the blockchain, and a consensus protocol ensures that the nodes agree on a unique order in which entries are appended. Consensus protocols for tolerating Byzantine faults have received renewed attention because they also address blockchain systems. This work discusses the process of assessing and gaining confidence in the resilience of a consensus protocols exposed to faults and adversarial nodes. We advocate to follow the established practice in cryptography and computer security, relying on public reviews, detailed models, and formal proofs; the designers of several practical systems appear to be unaware of this. Moreover, we review the consensus protocols in some prominent permissioned blockchain platforms with respect to their fault models and resilience against attacks. The protocol comparison covers Hyperledger Fabric, Tendermint, Symbiont, R3~Corda, Iroha, Kadena, Chain, Quorum, MultiChain, Sawtooth Lake, Ripple, Stellar, and IOTA. ",Blockchain Consensus Protocols in the Wild
22,885060520772030465,1449978614,Chris Burgess,['Language guided compositional visual concepts. Our paper &amp; blog from @irinavlh @Azhag et al:\n<LINK>\n<LINK>'],https://arxiv.org/abs/1707.03389,"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts. ",SCAN: Learning Hierarchical Compositional Visual Concepts
23,884790572438913025,706175245976248321,Oscar Viyuela,['Our new paper on the arXiv: Chiral Topological Superconductors Enhanced by Long-Range Interactions @frdelpino @MIT \n<LINK>'],https://arxiv.org/abs/1707.02326,"We study the phase diagram and edge states of a two-dimensional p-wave superconductor with long-range hopping and pairing amplitudes. New topological phases and quasiparticles different from the usual short-range model are obtained. When both hopping and pairing terms decay with the same exponent, one of the topological chiral phases with propagating Majorana edge states gets significantly enhanced by long-range couplings. On the other hand, when the long-range pairing amplitude decays more slowly than the hopping, we discover new topological phases where propagating Majorana fermions at each edge pair nonlocally and become gapped even in the thermodynamic limit. Remarkably, these nonlocal edge states are still robust, remain separated from the bulk, and are localized at both edges at the same time. The inclusion of long-range effects is potentially applicable to recent experiments with magnetic impurities and islands in 2D superconductors. ",Chiral Topological Superconductors Enhanced by Long-Range Interactions
24,884293227599323138,372689892,Marton Karsai,['Our new paper on arxiv:\nThreshold driven contagion on weighted networks\nwith @iniguezg @samunicomb\n<LINK>'],https://arxiv.org/abs/1707.02185,"Weighted networks capture the structure of complex systems where interaction strength is meaningful. This information is essential to a large number of processes, such as threshold dynamics, where link weights reflect the amount of influence that neighbours have in determining a node's behaviour. Despite describing numerous cascading phenomena, such as neural firing or social contagion, threshold models have never been explicitly addressed on weighted networks. We fill this gap by studying a dynamical threshold model over synthetic and real weighted networks with numerical and analytical tools. We show that the time of cascade emergence depends non-monotonously on weight heterogeneities, which accelerate or decelerate the dynamics, and lead to non-trivial parameter spaces for various networks and weight distributions. Our methodology applies to arbitrary binary state processes and link properties, and may prove instrumental in understanding the role of edge heterogeneities in various natural and social phenomena. ",Threshold driven contagion on weighted networks
25,884223687519383552,2309105822,Eric Jang,"['Excited to share our new paper from Google Brain Robotics! End-to-end Learning of Semantic Grasping <LINK> <LINK>', '2/ TL;DR: ext. of prior work with parallel robotic learning of grasping, except now we teach them to pick up a user-commanded obj class.', ""3/ Think picking &amp; placing diverse widgets in a warehouse by category (tool, toy, shovel), where widget instances haven't been seen before.""]",https://arxiv.org/abs/1707.01932,"We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A ""ventral stream"" recognizes object class while a ""dorsal stream"" simultaneously interprets the geometric relationships necessary to execute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance. ",End-to-End Learning of Semantic Grasping
26,884098981806186496,3199605543,Afonso S. Bandeira,['New paper on the Sample Complexity of Multi-reference Alignment: <LINK>'],https://arxiv.org/abs/1707.00943,"The growing role of data-driven approaches to scientific discovery has unveiled a large class of models that involve latent transformations with a rigid algebraic constraint. Three-dimensional molecule reconstruction in Cryo-Electron Microscopy (cryo-EM) is a central problem in this class. Despite decades of algorithmic and software development, there is still little theoretical understanding of the sample complexity of this problem, that is, number of images required for 3-D reconstruction. Here we consider multi-reference alignment (MRA), a simple model that captures fundamental aspects of the statistical and algorithmic challenges arising in cryo-EM and related problems. In MRA, an unknown signal is subject to two types of corruption: a latent cyclic shift and the more traditional additive white noise. The goal is to recover the signal at a certain precision from independent samples. While at high signal-to-noise ratio (SNR), the number of observations needed to recover a generic signal is proportional to $1/\mathrm{SNR}$, we prove that it rises to a surprising $1/\mathrm{SNR}^3$ in the low SNR regime. This precise phenomenon was observed empirically more than twenty years ago for cryo-EM but has remained unexplained to date. Furthermore, our techniques can easily be extended to the heterogeneous MRA model where the samples come from a mixture of signals, as is often the case in applications such as cryo-EM, where molecules may have different conformations. This provides a first step towards a statistical theory for heterogeneous cryo-EM. ",The sample complexity of multi-reference alignment
27,883746748207964161,2570689932,Amartya Sanyal,['Our new paper on modelling the evolution of society as an alternate optimization problem. <LINK>'],https://arxiv.org/abs/1707.01546,"Understanding the evolution of human society, as a complex adaptive system, is a task that has been looked upon from various angles. In this paper, we simulate an agent-based model with a high enough population tractably. To do this, we characterize an entity called \textit{society}, which helps us reduce the complexity of each step from $\mathcal{O}(n^2)$ to $\mathcal{O}(n)$. We propose a very realistic setting, where we design a joint alternate maximization step algorithm to maximize a certain \textit{fitness} function, which we believe simulates the way societies develop. Our key contributions include (i) proposing a novel protocol for simulating the evolution of a society with cheap, non-optimal joint alternate maximization steps (ii) providing a framework for carrying out experiments that adhere to this joint-optimization simulation framework (iii) carrying out experiments to show that it makes sense empirically (iv) providing an alternate justification for the use of \textit{society} in the simulations. ","Agent based simulation of the evolution of society as an alternate
  maximization problem"
28,883228517261086720,17055506,Martin Kleppmann,['New paper! üìì ‚ÄúVerifying Strong Eventual Consistency in Distributed Systems‚Äù ‚Äî checking CRDTs with a theorem prover üòé <LINK>'],https://arxiv.org/abs/1707.01747,"Data replication is used in distributed systems to maintain up-to-date copies of shared data across multiple computers in a network. However, despite decades of research, algorithms for achieving consistency in replicated systems are still poorly understood. Indeed, many published algorithms have later been shown to be incorrect, even some that were accompanied by supposed mechanised proofs of correctness. In this work, we focus on the correctness of Conflict-free Replicated Data Types (CRDTs), a class of algorithm that provides strong eventual consistency guarantees for replicated data. We develop a modular and reusable framework in the Isabelle/HOL interactive proof assistant for verifying the correctness of CRDT algorithms. We avoid correctness issues that have dogged previous mechanised proofs in this area by including a network model in our formalisation, and proving that our theorems hold in all possible network behaviours. Our axiomatic network model is a standard abstraction that accurately reflects the behaviour of real-world computer networks. Moreover, we identify an abstract convergence theorem, a property of order relations, which provides a formal definition of strong eventual consistency. We then obtain the first machine-checked correctness theorems for three concrete CRDTs: the Replicated Growable Array, the Observed-Remove Set, and an Increment-Decrement Counter. We find that our framework is highly reusable, developing proofs of correctness for the latter two CRDTs in a few hours and with relatively little CRDT-specific code. ",Verifying Strong Eventual Consistency in Distributed Systems
29,883130829349593088,245262377,Awni Hannun,['New paper: cardiologist-level arrhythmia detection from ECG with @pranavrajpurkar @AndrewYNg and @iRhythmTech - <LINK> <LINK>'],http://arxiv.org/abs/1707.01836,"We develop an algorithm which exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value). ","Cardiologist-Level Arrhythmia Detection with Convolutional Neural
  Networks"
30,882943446880747521,20309837,Michael Veale,"[""New paper‚Äî@RDBinns, me, @emax, @Nigel_Shadbolt on bias/different ideas of offence in algos censoring 'toxic' content <LINK> <LINK>""]",https://arxiv.org/abs/1707.01477,"The internet has become a central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated content moderation aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and are subject to the idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic content moderation systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants. ","Like trainer, like bot? Inheritance of bias in algorithmic content
  moderation"
31,882855240818716673,2886658437,Sean Raymond,"[""Jupiter and Saturn are omelettes. Earth's water and the C-type asteroids are eggshells (just byproducts). New paper: <LINK>""]",https://arxiv.org/abs/1707.01234,"There is a long-standing debate regarding the origin of the terrestrial planets' water as well as the hydrated C-type asteroids. Here we show that the inner Solar System's water is a simple byproduct of the giant planets' formation. Giant planet cores accrete gas slowly until the conditions are met for a rapid phase of runaway growth. As a gas giant's mass rapidly increases, the orbits of nearby planetesimals are destabilized and gravitationally scattered in all directions. Under the action of aerodynamic gas drag, a fraction of scattered planetesimals are deposited onto stable orbits interior to Jupiter's. This process is effective in populating the outer main belt with C-type asteroids that originated from a broad (5-20 AU-wide) region of the disk. As the disk starts to dissipate, scattered planetesimals reach sufficiently eccentric orbits to cross the terrestrial planet region and deliver water to the growing Earth. This mechanism does not depend strongly on the giant planets' orbital migration history and is generic: whenever a giant planet forms it invariably pollutes its inner planetary system with water-rich bodies. ","Origin of water in the inner Solar System: Planetesimals scattered
  inward during Jupiter and Saturn's rapid gas accretion"
32,882647908524412928,40639812,Colin Cotter,"['New paper submitted: <LINK> Incorporating upwinding into compatible FEM schemes for shallow water eqns on rotating sphere.', '@_wence Yes sorry, this just illustrates how long it took us to write this paper! Will fix.']",https://arxiv.org/abs/1707.00855,"We describe a compatible finite element discretisation for the shallow water equations on the rotating sphere, concentrating on integrating consistent upwind stabilisation into the framework. Although the prognostic variables are velocity and layer depth, the discretisation has a diagnostic potential vorticity that satisfies a stable upwinded advection equation through a Taylor-Galerkin scheme; this provides a mechanism for dissipating enstrophy at the gridscale whilst retaining optimal order consistency. We also use upwind discontinuous Galerkin schemes for the transport of layer depth. These transport schemes are incorporated into a semi-implicit formulation that is facilitated by a hybridisation method for solving the resulting mixed Helmholtz equation. We illustrate our discretisation with some standard rotating sphere test problems. ","Higher-order compatible finite element schemes for the nonlinear
  rotating shallow water equations on the sphere"
33,882407992913666048,122780488,Dr. Izabella Laba,"['New paper on lower bounds for the directional Hilbert transform, with Malabika Pramanik and Alessandro Marinelli. <LINK>']",https://arxiv.org/abs/1707.01061,"For any dimension $n \geq 2$, we consider the maximal directional Hilbert transform $\mathscr{H}_U$ on $\mathbb R^n$ associated with a direction set $U \subseteq \mathbb S^{n-1}$: \[ \mathscr{H}_Uf(x) := \frac{1}{\pi} \sup_{v \in U} \Bigl| \text{p.v.} \int f(x - tv) \, \frac{dt}{t}\Bigr|.\] The main result in this article asserts that for any exponent $p \in (1, \infty)$, there exists a positive constant $C_{p,n}$ such that for any finite direction set $U \subseteq \mathbb S^{n-1}$, \[||\mathscr{H}_U||_{p \rightarrow p} \geq C_{p,n} \sqrt{\log \#U}, \] where $\#U$ denotes the cardinality of $U$. As a consequence, the maximal directional Hilbert transform associated with an infinite set of directions cannot be bounded on $L^p(\mathbb{R}^{n})$ for any $n\geq 2$ and any $p \in (1, \infty)$. This completes a result of Karagulyan, who proved a similar statement for $n=2$ and $p=2$. ",On the maximal directional Hilbert transform
34,882111360821329921,824150806396014597,Marco Laudato,['New paper online!\n\n<LINK>'],https://arxiv.org/abs/1707.00293,"In this paper we shall consider the stratified manifold of quantum states and the vector fields which act on it. In particular, we show that the infinitesimal generator of the GKLS evolution is composed of a generator of unitary transformations plus a gradient vector field along with a Kraus vector field transversal to the strata defined by the involutive distribution generated by the former ones. ",Dynamical Vector Fields on the Manifold of Quantum States
35,890388521709588481,2210861,Jacob Andreas,"['New paper: discovering analogs of logical structure in RNN representations <LINK>.', 'Or, Davidson in Vector Space', '@Smerity Oof sorry that was rude. abs indeed &gt; PDF. At least the link was sufficiently interact that you can tell.', '@Smerity other embarrassments: I just noticed that someone named ""Israel Ramat Gan"" is listed as @omerlevy_ \'s coauthor in the bib']",https://arxiv.org/abs/1707.08139,"We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a ""syntax"" with functional analogues to qualitative properties of natural language. ",Analogs of Linguistic Structure in Deep Representations
36,890314301248032769,2157374528,Dan Garisto,"[""New paper (hasn't gone thru peer review yet) takes a second look at reactor neutrino anomaly <LINK> <LINK>"", 'tl;dr: Some error in modeling results from changing levels of isotopes, but even w/ corrections, there should still be an anomaly. https://t.co/7Vib5rXhrz']",https://arxiv.org/abs/1707.07728,"We investigate the recent Daya Bay results on the changes in the antineutrino flux and spectrum with the burnup of the reactor fuel. We find that the discrepancy between current model predictions and the Daya Bay results can be traced to the original measured $^{235}$U/$^{239}$Pu ratio of the fission beta spectra that were used as a base for the expected antineutrino fluxes. An analysis of the antineutrino spectra that is based on a summation over all fission fragment beta-decays, using nuclear database input, explains all of the features seen in the Daya Bay evolution data. However, this summation method still predicts an anomaly. Thus, we conclude that there is currently not enough information to use the antineutrino flux changes to rule out the possible existence of sterile neutrinos. ","Analysis of the Daya Bay Reactor Antineutrino Flux Changes with Fuel
  Burnup"
37,887124977132195840,1950210188,Dr. Ashley T. Rubin,"[""David's new paper (with @drhay53) on rare high-redshift lensed supernova  <LINK>""]",https://arxiv.org/abs/1707.04606,"We present the discovery and measurements of a gravitationally lensed supernova (SN) behind the galaxy cluster MOO J1014+0038. Based on multi-band Hubble Space Telescope and Very Large Telescope (VLT) photometry of the supernova, and VLT spectroscopy of the host galaxy, we find a 97.5% probability that this SN is a SN Ia, and a 2.5% chance of a CC SN. Our typing algorithm combines the shape and color of the light curve with the expected rates of each SN type in the host galaxy. With a redshift of 2.2216, this is the highest redshift SN Ia discovered with a spectroscopic host-galaxy redshift. A further distinguishing feature is that the lensing cluster, at redshift 1.23, is the most distant to date to have an amplified SN. The SN lies in the middle of the color and light-curve shape distributions found at lower redshift, disfavoring strong evolution to z = 2.22. We estimate an amplification due to gravitational lensing of 2.8+0.6-0.5 (1.10 +- 0.23 mag)---compatible with the value estimated from the weak-lensing-derived mass and the mass-concentration relation from LambdaCDM simulations---making it the most amplified SN Ia discovered behind a galaxy cluster. ",The Discovery of a Gravitationally Lensed Supernova Ia at Redshift 2.22
38,882073437950197760,286454227,Melody Guan  ï·µî·¥•·µî î,"['New @emnlp2017 paper just out on arXiv! ""Efficient Attention using a Fixed-Size Memory Representation"" <LINK>']",https://arxiv.org/abs/1707.00110,"The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments. ",Efficient Attention using a Fixed-Size Memory Representation
39,884212636484681728,195143286,Song Huang,"['<LINK> We use the amazing HSC survey to study massive galaxies and their halos; Here is the first of a series, more to come <LINK>', '@730kou_haya Thanks!']",https://arxiv.org/abs/1707.01904,"Massive galaxies display extended light profiles that can reach several hundreds of kilo parsecs. These stellar halos provide a fossil record of galaxy assembly histories. Using data that is both wide (~100 square degree) and deep (i>28.5 mag/arcsec^2 in i-band), we present a systematic study of the stellar halos of a sample of more than 3000 galaxies at 0.3 < z < 0.5 with $\log M_{\star}/M_{\odot} > 11.4$. Our study is based on high-quality (0.6 arcsec seeing) imaging data from the Hyper Suprime-Cam (HSC) Subaru Strategic Program (SSP), which enables us to individually estimate surface mass density profiles to 100 kpc without stacking. As in previous work, we find that more massive galaxies exhibit more extended outer profiles. When this extended light is not properly accounted for as a result of shallow imaging or inadequate profile modeling, the derived stellar mass function can be significantly underestimated at the highest masses. Across our sample, the ellipticity of outer light profiles increases substantially as we probe larger radii. We show for the first time that these ellipticity gradients steepen dramatically as a function of galaxy mass, but we detect no mass-dependence in outer color gradients. Our results support the two-phase formation scenario for massive galaxies in which outer envelopes are built up at late times from a series of merging events. We provide surface mass surface mass density profiles in a convenient tabulated format to facilitate comparisons with predictions from numerical simulations of galaxy formation. ","Individual Stellar Halos of Massive Galaxies Measured to 100 kpc at
  $0.3&lt;z&lt;0.5$ using Hyper Suprime-Cam"
