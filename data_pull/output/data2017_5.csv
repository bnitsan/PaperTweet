,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,871393426553655297,156804540,Francisco Rodrigues,['Our new paper on arxiv: Influence maximization on correlated networks through community identification @didiervega \n<LINK> <LINK>'],https://arxiv.org/abs/1705.00630,"The identification of the minimal set of nodes that maximizes the propagation of information is one of the most relevant problems in network science. In this paper, we introduce a new method to find the set of initial spreaders to maximize the information propagation in complex networks. We evaluate this method in assortative networks and verify that degree-degree correlation plays a fundamental role in the spreading dynamics. Simulation results show that our algorithm is statistically similar, regarding the average size of outbreaks, to the greedy approach in real-world networks. However, our method is much less time consuming than the greedy algorithm. ","Influence maximization by rumor spreading on correlated networks through
  community identification"
1,870774658626453505,2956121356,Russ Salakhutdinov,"['New paper on Good Semi-Supervised Learning that Requires a Bad GAN with Z Dai, Z Yang, F Yang, &amp; @professorwcohen <LINK> <LINK>']",https://arxiv.org/abs/1705.09783,"Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically, we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets. ",Good Semi-supervised Learning that Requires a Bad GAN
2,870686083742183426,162293874,Jeff Clune,['new paper: solving catastrophic forgetting (on small networks/simple problems) with diffusion-based neuromodulation <LINK>'],http://arxiv.org/abs/1705.07241,"A long-term goal of AI is to produce agents that can learn a diversity of skills throughout their lifetimes and continuously improve those skills via experience. A longstanding obstacle towards that goal is catastrophic forgetting, which is when learning new information erases previously learned information. Catastrophic forgetting occurs in artificial neural networks (ANNs), which have fueled most recent advances in AI. A recent paper proposed that catastrophic forgetting in ANNs can be reduced by promoting modularity, which can limit forgetting by isolating task information to specific clusters of nodes and connections (functional modules). While the prior work did show that modular ANNs suffered less from catastrophic forgetting, it was not able to produce ANNs that possessed task-specific functional modules, thereby leaving the main theory regarding modularity and forgetting untested. We introduce diffusion-based neuromodulation, which simulates the release of diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up or down regulate) learning in a spatial region. On the simple diagnostic problem from the prior work, diffusion-based neuromodulation 1) induces task-specific learning in groups of nodes and connections (task-specific localized learning), which 2) produces functional modules for each subtask, and 3) yields higher performance by eliminating catastrophic forgetting. Overall, our results suggest that diffusion-based neuromodulation promotes task-specific localized learning and functional modularity, which can help solve the challenging, but important problem of catastrophic forgetting. ","Diffusion-based neuromodulation can eliminate catastrophic forgetting in
  simple neural networks"
3,870676102842396673,506357248,Tom Sercu,"['Our new paper is out: Fisher GAN. Critic constrained w 2nd order moments: fast and stable training, and good SSL. <LINK>']",https://arxiv.org/abs/1705.09675,"Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN. ",Fisher GAN
4,870270160724275200,228792418,Tim RocktÃ¤schel,"['New paper End-to-end Differentiable Proving w/ @riedelcastro <LINK> ""neuralize"" proving to induce rules w/ gradient descent <LINK>']",https://arxiv.org/abs/1705.11040,"We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules. ",End-to-End Differentiable Proving
5,870181793474924544,1249492212,Karol Hausman ðŸ’™ðŸ’›,['Our new paper on imitation learning from unstructured demonstrations using GANs: <LINK>'],https://arxiv.org/abs/1705.10479,"Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at this http URL ","Multi-Modal Imitation Learning from Unstructured Demonstrations using
  Generative Adversarial Nets"
6,869816454513864704,25068244,Ben Chamberlain,['Our new paper on neural embeddings in hyperbolic space. <LINK> Talking about this at #reworkretail in London tomorrow <LINK>'],http://arxiv.org/abs/1705.10359,"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets. ",Neural Embeddings of Graphs in Hyperbolic Space
7,869813432853385216,65010804,Hugh Osborn,"[""Our new paper is on arXiv this morning about eclipses of the young star PDS 110! <LINK> Here's a quick twitter TLDR:"", ""~3 years ago now I found a ~30%, 3-week long eclipse of this 10Myr-old star in WASP. It was cool but I wasn't sure if it was interesting."", '(Young stars with big dusty discs, as PDS-110 has, have a habit of occasionally kicking dust into our line-of-sight - known as UX Oris)', 'Then, At the Rocks/Rubble/Rings meeting in Leiden, @Astro_JRod searched KELT data &amp; found the same eclipse... except it was 2.2 years later!', '@Astro_JRod Same depth, same duration, same interesting sharp-edge features. It looked like it had to be the same dust structure eclipsing the star... https://t.co/s5Z7hlZkuP', ""@Astro_JRod Dust 0.3AU across can't stay together through a whole orbit. Except with gravity - it turns out a 1.6-70 Mj body can do it!"", '@Astro_JRod So maybe a planet with rings? And @mattkenworthy showed the sharp gradients can be explained by rings &amp; ring gaps (like J1407)! https://t.co/DhEhTq5X2n', ""@Astro_JRod @mattkenworthy Or maybe it's just dusty debris? Despite 10+ yrs of data (ASAS, ASAS-SN, IOMC), other eclipses (if they're there) would happen in data gaps."", '@Astro_JRod @mattkenworthy BUT the next predicted dip is this September, so we can test this theory! We have @LCO_Global time &amp; even amateurs (ie @AAVSO) can help out! https://t.co/IxQ5IWbnst']",https://arxiv.org/abs/1705.10346,"We report the discovery of eclipses by circumstellar disc material associated with the young star PDS 110 in the Ori OB1a association using the SuperWASP and KELT surveys. PDS 110 (HD 290380, IRAS 05209-0107) is a rare Fe/Ge-type star, a ~10 Myr-old accreting intermediate-mass star showing strong infrared excess (L$_{\rm IR}$/L$_{\rm bol}$ ~ 0.25). Two extremely similar eclipses with a depth of ~30\% and duration ~25 days were observed in November 2008 and January 2011. We interpret the eclipses as caused by the same structure with an orbital period of $808\pm2$ days. Shearing over a single orbit rules out diffuse dust clumps as the cause, favouring the hypothesis of a companion at ~2AU. The characteristics of the eclipses are consistent with transits by an unseen low-mass (1.8-70M$_{Jup}$) planet or brown dwarf with a circum-secondary disc of diameter ~0.3 AU. The next eclipse event is predicted to take place in September 2017 and could be monitored by amateur and professional observatories across the world. ","Periodic Eclipses of the Young Star PDS 110 Discovered with WASP and
  KELT Photometry"
8,869573848643104768,301426952,Arttu Rajantie ðŸ‡ªðŸ‡º ðŸ‡«ðŸ‡® #FBPE,['New paper with my student Ed Gillman: Topological Defects in Quantum Field Theory with Matrix Product States <LINK>'],https://arxiv.org/abs/1705.09802,"Topological defects (kinks) in a relativistic $\phi^{4}$ scalar field theory in $D=(1+1)$ are studied using the matrix product state tensor network. The one kink state is approximated as a matrix product state and the kink mass is calculated. The approach used is quite general and can be applied to a variety of theories and tensor networks. Additionally, the contribution of kink-antikink excitations to the ground state is examined and a general method to estimate the scalar mass from equal time ground state observables is provided. The scalar and kink mass are compared at strong coupling and behave as expected from universality arguments. This suggests that the matrix product state can adequately capture the physics of defect-antidefect excitations and thus provides a promising technique to study challenging non-equilibrium physics such as the Kibble-Zurek mechanism of defect formation. ",Topological Defects in Quantum Field Theory with Matrix Product States
9,869530825745936390,17239073,Atabey Kaygun,['Wrote a new math paper :) <LINK>'],https://arxiv.org/abs/1705.10137,"We show that the (even and odd) index cocycles for theta-summable Fredholm modules are in the image of the Connes-Moscovici characteristic map. To show this, we first define a new range of asymptotic cohomologies, and then we extend the Connes-Moscovici characteristic map to our setting. The ordinary periodic cyclic cohomology and the entire cyclic cohomology appear as two instances of this setup. We then construct an asymptotic characteristic class, defined independently from the underlying Fredholm module. Paired with the $K$-theory, the image of this class under the characteristic map yields a non-zero scalar multiple of the index in the even case, and the spectral flow in the odd case. ","The asymptotic Connes-Moscovici characteristic map and the index
  cocycles"
10,869187286864449536,2800204849,Andrew Gordon Wilson,['Bayesian GANs\n<LINK>\nOur new paper about posterior inference with GANs! <LINK>'],https://arxiv.org/abs/1705.09558,"Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles. ",Bayesian GAN
11,868832554002108421,2235119491,Vikash Mansinghka,"['New paper of ours: ""AIDE: an algorithm for measuring the accuracy of approximate probabilistic inference algorithms"" <LINK>']",https://arxiv.org/abs/1705.07224,"Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics. ","AIDE: An algorithm for measuring the accuracy of probabilistic inference
  algorithms"
12,868042107470192640,721931072,Shimon Whiteson,['Our new paper: multi-agent credit assignment in StarCraft with decentralised actors and counterfactual baselines. <LINK>'],https://arxiv.org/abs/1705.08926,"Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. ",Counterfactual Multi-Agent Policy Gradients
13,868035122691485697,334470578,Chris J. Maddison,"['Our new paper, FIVO: tighter bounds on the likelihood with big gains for sequential neural latent variable models <LINK>']",https://arxiv.org/abs/1705.09279,"When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data. ",Filtering Variational Objectives
14,868029228683538432,20058831,Matthew Pitkin,"[""here's a hefty (46 page) new paper with @maxisi @JohnVeitch describing software for #gravitationalwave CW searches <LINK>""]",https://arxiv.org/abs/1705.08978,This document describes a code to perform parameter estimation and model selection in targeted searches for continuous gravitational waves from known pulsars using data from ground-based gravitational wave detectors. We describe the general workings of the code and characterise it on simulated data containing both noise and simulated signals. We also show how it performs compared to a previous MCMC and grid-based approach to signal parameter estimation. Details how to run the code in a variety of cases are provided in Appendix A. ,"A nested sampling code for targeted searches for continuous
  gravitational waves from pulsars"
15,868022926729240577,389027035,Andrea Tassi,"['Find out how to handle #zombie #autonomous #vehicles in our new paper <LINK> to be presented at the IEEE VTC 2017-Fall <LINK>', 'This piece of research has been carried out within @BristolCSN as a part of Innovate UK @Flourish_Cars project']",https://arxiv.org/abs/1705.06903,"The successful deployment of safe and trustworthy Connected and Autonomous Vehicles (CAVs) will highly depend on the ability to devise robust and effective security solutions to resist sophisticated cyber attacks and patch up critical vulnerabilities. Pseudonym Public Key Infrastructure (PPKI) is a promising approach to secure vehicular networks as well as ensure data and location privacy, concealing the vehicles' real identities. Nevertheless, pseudonym distribution and management affect PPKI scalability due to the significant number of digital certificates required by a single vehicle. In this paper, we focus on the certificate revocation process and propose a versatile and low-complexity framework to facilitate the distribution of the Certificate Revocation Lists (CRL) issued by the Certification Authority (CA). CRL compression is achieved through optimized Bloom filters, which guarantee a considerable overhead reduction with a configurable rate of false positives. Our results show that the distribution of compressed CRLs can significantly enhance the system scalability without increasing the complexity of the revocation process. ","Optimized Certificate Revocation List Distribution for Secure V2X
  Communications"
16,867792967703986177,2670767244,Elad Hoffer,"['New paper: ""Train longer, generalize better"" <LINK>\nWith @PyTorch code available at: <LINK>']",https://arxiv.org/abs/1705.08741,"Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the ""generalization gap"" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a ""random walk on random landscape"" statistical model which is known to exhibit similar ""ultra-slow"" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the ""generalization gap"" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named ""Ghost Batch Normalization"" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization. ","Train longer, generalize better: closing the generalization gap in large
  batch training of neural networks"
17,867356622758322178,234416025,Dr Carl Mundy,['New (final?) paper on major #mergers using close-pairs: <LINK> #arxiv #astronomy'],https://arxiv.org/abs/1705.07986,"We use a large sample of $\sim 350,000$ galaxies constructed by combining the UKIDSS UDS, VIDEO/CFHT-LS, UltraVISTA/COSMOS and GAMA survey regions to probe the major merging histories of massive galaxies ($>10^{10}\ \mathrm{M}_\odot$) at $0.005 < z < 3.5$. We use a method adapted from that presented in Lopez-Sanjuan et al. (2014) using the full photometric redshift probability distributions, to measure pair $\textit{fractions}$ of flux-limited, stellar mass selected galaxy samples using close-pair statistics. The pair fraction is found to weakly evolve as $\propto (1+z)^{0.8}$ with no dependence on stellar mass. We subsequently derive major merger $\textit{rates}$ for galaxies at $> 10^{10}\ \mathrm{M}_\odot$ and at a constant number density of $n > 10^{-4}$ Mpc$^{-3}$, and find rates a factor of 2-3 smaller than previous works, although this depends strongly on the assumed merger timescale and likelihood of a close-pair merging. Galaxies undergo approximately 0.5 major mergers at $z < 3.5$, accruing an additional 1-4 $\times 10^{10}\ \mathrm{M}_\odot$ in the process. Major merger accretion rate densities of $\sim 2 \times 10^{-4}$ $\mathrm{M}_\odot$ yr$^{-1}$ Mpc$^{-3}$ are found for number density selected samples, indicating that direct progenitors of local massive ($>10^{11}\mathrm{M}_\odot$) galaxies have experienced a steady supply of stellar mass via major mergers throughout their evolution. While pair fractions are found to agree with those predicted by the Henriques et al. (2014) semi-analytic model, the Illustris hydrodynamical simulation fails to quantitatively reproduce derived merger rates. Furthermore, we find major mergers become a comparable source of stellar mass growth compared to star-formation at $z < 1$, but is 10-100 times smaller than the SFR density at higher redshifts. ","A consistent measure of the merger histories of massive galaxies using
  close-pair statistics I: Major mergers at $z &lt; 3.5$"
18,867352972283568128,2541954109,Victoria Krakovna,['New AI safety paper on helping RL agents overcome reward corruption and avoid reward hacking... <LINK>'],https://arxiv.org/abs/1705.08417,"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions. ",Reinforcement Learning with a Corrupted Reward Channel
19,867309426197143552,737140237948715008,Michael J. Biercuk,['Exciting new paper from our collab on fault-tolerant #quantumcomputing.@Yale_QI @quantumweekly @preskill @dabacon <LINK> <LINK>'],https://arxiv.org/abs/1705.02771,"A quantitative assessment of the progress of small prototype quantum processors towards fault-tolerant quantum computation is a problem of current interest in experimental and theoretical quantum information science. We introduce a necessary and fair criterion for quantum error correction (QEC), which must be achieved in the development of these quantum processors before their sizes are sufficiently big to consider the well-known QEC threshold. We apply this criterion to benchmark the ongoing effort in implementing QEC with topological color codes using trapped-ion quantum processors and, more importantly, to guide the future hardware developments that shall be required in order to demonstrate beneficial QEC with small topological quantum codes. In doing so, we present a thorough description of a realistic trapped-ion toolbox for QEC, and a physically-motivated error model that goes beyond standard simplifications in the QEC literature. Our large-scale numerical analysis shows that two-species trapped-ion crystals in high-optical aperture segmented traps, with the improvements hereby described, are a very promising candidate for fault-tolerant quantum computation. ","Assessing the progress of trapped-ion processors towards fault-tolerant
  quantum computation"
20,867305800250863617,507704346,Isabelle Augenstein,"['New paper on learning what to share in multi-task learning <LINK> @seb_ruder @joabingel @soegaarducph #dlearn #ML #NLProc <LINK>', '@barbara_plank @seb_ruder @joabingel @soegaarducph Thanks, @barbara_plank :-)', ""@barbara_plank @seb_ruder @joabingel @soegaarducph Yes, we're just cleaning it up. We'll let you know once it's online :-)""]",https://arxiv.org/abs/1705.08142,"Multi-task learning (MTL) allows deep neural networks to learn from related tasks by sharing parameters with other networks. In practice, however, MTL involves searching an enormous space of possible parameter sharing architectures to find (a) the layers or subspaces that benefit from sharing, (b) the appropriate amount of sharing, and (c) the appropriate relative weights of the different task losses. Recent work has addressed each of the above problems in isolation. In this work we present an approach that learns a latent multi-task architecture that jointly addresses (a)--(c). We present experiments on synthetic data and data from OntoNotes 5.0, including four different tasks and seven different domains. Our extension consistently outperforms previous approaches to learning latent architectures for multi-task problems and achieves up to 15% average error reductions over common approaches to MTL. ",Latent Multi-task Architecture Learning
21,867255868689141760,2770204252,Michael Gygli,"['Check our new paper ""Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks"" <LINK> <LINK>', '@jponttuset Me Me Me :D. I think it is standard practice: https://t.co/CT92irV3HP', '@jponttuset Oh, I thought you meant in the paper :D. Here it refers to our team @GifsCom ðŸ˜€']",https://arxiv.org/abs/1705.08214,"Shot boundary detection (SBD) is an important component of many video analysis tasks, such as action recognition, video indexing, summarization and editing. Previous work typically used a combination of low-level features like color histograms, in conjunction with simple models such as SVMs. Instead, we propose to learn shot detection end-to-end, from pixels to final shot boundaries. For training such a model, we rely on our insight that all shot boundaries are generated. Thus, we create a dataset with one million frames and automatically generated transitions such as cuts, dissolves and fades. In order to efficiently analyze hours of videos, we propose a Convolutional Neural Network (CNN) which is fully convolutional in time, thus allowing to use a large temporal context without the need to repeatedly processing frames. With this architecture our method obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time. ","Ridiculously Fast Shot Boundary Detection with Fully Convolutional
  Neural Networks"
22,866554386243346433,355616314,Murray Shanahan,['New paper by my PhD student Nat Dilokthanakul on intrinsic motivation and hierarchical reinforcement learning - <LINK>'],https://arxiv.org/abs/1705.06769,"The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using ""auxiliary tasks"") is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games -- Montezuma's revenge -- for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance. UPDATE 22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e. extrinsic reward + feature control intrinsic reward, has comparable performance to our agent in Montezuma Revenge. In light of the new experiments performed, the advantage of our HRL approach can be attributed more to its ability to learn useful features from intrinsic rewards rather than its ability to explore and reuse abstracted skills with hierarchical components. This has led us to a new conclusion about the result. ","Feature Control as Intrinsic Motivation for Hierarchical Reinforcement
  Learning"
23,865369218904150017,280687108,Prof Ferah Munshi,['New paper is out! Have a look: <LINK> <LINK>'],https://arxiv.org/abs/1705.06286,"We predict the stellar mass-halo mass (SMHM) relationship for dwarf galaxies and their satellites residing in halos down to M$_{halo} =$ 10$^7$ M$_{\odot}$ with 10$^4$ M$_{\odot} <$ M$_{star}$($z=0$) $< 10^8$ M$_{\odot}$, and quantify the predicted scatter in the relation at the low mass end, using cosmological simulations. The galaxies were drawn from a cosmological simulation of dwarf galaxies, run with the N-body + SPH code, ChaNGA, at a high resolution of 60 pc. For M$_{halo} > 10^9$ M$_{\odot}$, the simulated SMHM relationship agrees with literature determinations, including exhibiting a small scatter. However, the scatter in the SMHM relation increases dramatically for lower-mass halos. We find that some of this scatter is due to {\em dark dwarfs}, halos devoid of stars. However, even when only considering well-resolved halos that contain a stellar population, the scatter in stellar mass reaches nearly 1 dex for M$_{halo}$($z=0$) 10$^7$ M$_{\odot}$. Much of this scatter is due to including satellites of the dwarf galaxies that have had their halo masses reduced through tidal stripping. The fraction of dark dwarfs (those that contain no stars) increases substantially with decreasing halo mass. When these dark halos are considered, the true scatter in the SMHM at low masses is even larger. At the faintest end of the SMHM relation probed by our simulations, a galaxy cannot be assigned a unique halo mass based solely on its luminosity. We provide a formula to stochastically populate low-mass halos following our results. Our predicted large scatter at low halo masses increases the slope of the resulting stellar mass function on the ultra-faint dwarf galaxy scales currently being probed by such surveys as the Dark Energy Survey or the Hyper-Suprime Cam Subaru Strategic Program, and in the future by the Large Synoptic Survey Telescope. ","Going, going, gone dark: Quantifying the scatter in the faintest dwarf
  galaxies"
24,865273054703869953,3276698761,Albert S. Berahas,['New paper with Bollapragada and @nocedal comparing stochastic second-order methods for ML with surprising results <LINK>'],http://arxiv.org/abs/1705.06211,"Sketching, a dimensionality reduction technique, has received much attention in the statistics community. In this paper, we study sketching in the context of Newton's method for solving finite-sum optimization problems in which the number of variables and data points are both large. We study two forms of sketching that perform dimensionality reduction in data space: Hessian subsampling and randomized Hadamard transformations. Each has its own advantages, and their relative tradeoffs have not been investigated in the optimization literature. Our study focuses on practical versions of the two methods in which the resulting linear systems of equations are solved approximately, at every iteration, using an iterative solver. The advantages of using the conjugate gradient method vs. a stochastic gradient iteration are revealed through a set of numerical experiments, and a complexity analysis of the Hessian subsampling method is presented. ",An Investigation of Newton-Sketch and Subsampled Newton Methods
25,865253221043798017,1478121948,Joe Antognini,['A new paper by Cristobal and Fabio finds substantial compact object merger rates in non-spherical nuclear clusters: <LINK>'],https://arxiv.org/abs/1705.05848,"The Milky Way and a significant fraction of galaxies are observed to host a central Massive Black Hole (MBH) embedded in a non-spherical nuclear star cluster. We study the secular orbital evolution of compact-object binaries in these environments and characterize the excitation of extremely large eccentricities that can lead to mergers by gravitational radiation. We find that the eccentricity excitation occurs most efficiently when the nodal precession timescale of the binary's orbit around the MBH due to the non-spherical cluster becomes comparable (within a factor of ~10) to the timescale on which the binary is torqued by the MBH due to the Lidov-Kozai (LK) mechanism. We show that in this regime the perturbations due to the cluster increase the fraction of systems that reach extreme eccentricities ($1-e\sim10^{-4}-10^{-6}$) by a factor of ~10-100 compared to the idealized case of a spherical cluster, increasing the merger rates of compact objects by a similar factor. We identify two main channels that lead to this extreme eccentricity excitation: (i) chaotic diffusion of the eccentricities due to resonance overlap; (ii) cluster-driven variations of the mutual inclinations between the binary orbit and its center-of-mass orbit around the MBH, which can intensify the LK oscillations. We estimate that our mechanism can produce black hole-black hole and black hole-neutron star binary merger rates of up to $\approx 15$ $\rm{Gpc}^{-3}yr^{-1}$ and $\approx 0.4$ $\rm{Gpc}^{-3}yr^{-1}$, respectively. Thus, we propose the cluster-enhanced Lidov-Kozai mechanism as a new channel for the merger of compact-object binaries, competing with scenarios that invoke isolated binary evolution or dynamical formation in globular clusters. ","Greatly enhanced merger rates of compact-object binaries in
  non-spherical nuclear star clusters"
26,865149423638257665,2872569532,Alejandro S. Borlaff,"['If you want to know how to find 6-Gyr-old anti-truncated stellar discs with HST, check out our new paper in @arxiv  <LINK>']",https://arxiv.org/abs/1705.05850,"The controversy about the origin of the structure of S0--E/S0 galaxies may be due to the difficulty of comparing surface brightness profiles with different depths, photometric corrections and PSF effects (almost always ignored). We aim to quantify the properties of Type-III (anti-truncated) discs in a sample of S0 galaxies at 0.2<z<0.6. In this paper, we present the sample selection and describe in detail the methods to robustly trace the structure in their outskirts and correct for PSF effects. We have selected and classified a sample of 150 quiescent galaxies at 0.2<z<0.6 in the GOODS-N field. We perform a quantitative structural analysis of 44 S0-E/S0 galaxies. We corrected their surface brightness profiles for PSF distortions and analysed the biases in the structural and photometric parameters when the PSF correction is not applied. Additionally, we have developed Elbow, an automatic statistical method to determine whether a possible break is significant - or not - and its type and made it publicly available. We found 14 anti-truncated S0-E/S0 galaxies in the range 0.2<z<0.6 (~30% of the final sample). This fraction is similar to the those reported in the local Universe. In our sample, ~25% of the Type-III breaks observed in PSF-uncorrected profiles are artifacts, and their profiles turn into a Type I after PSF correction. PSF effects also soften Type-II profiles. We found that the profiles of Type-I S0 and E/S0 galaxies of our sample are compatible with the inner profiles of the Type-III, in contrast with the outer profiles. We have obtained the first robust and reliable sample of 14 anti-truncated S0--E/S0 galaxies beyond the local Universe, in the range 0.2<z<0.6. PSF effects significantly affect the shape of the surface brightness profiles in galaxy discs even in the case of the narrow PSF of HST/ACS images, so future studies on the subject should make an effort to correct them. ","Evolution of the anti-truncated stellar profiles of S0 galaxies since
  $z=0.6$ in the SHARDS survey: I - Sample and Methods"
27,865053807805308928,66175375,Jason Wang,['New @PlanetImager paper on how to find exoplanets and find them better courtesy of Stanford grad student JB Ruffio  <LINK>'],https://arxiv.org/abs/1705.05477,"We present a new matched filter algorithm for direct detection of point sources in the immediate vicinity of bright stars. The stellar Point Spread Function (PSF) is first subtracted using a Karhunen-Lo\'eve Image Processing (KLIP) algorithm with Angular and Spectral Differential Imaging (ADI and SDI). The KLIP-induced distortion of the astrophysical signal is included in the matched filter template by computing a forward model of the PSF at every position in the image. To optimize the performance of the algorithm, we conduct extensive planet injection and recovery tests and tune the exoplanet spectra template and KLIP reduction aggressiveness to maximize the Signal-to-Noise Ratio (SNR) of the recovered planets. We show that only two spectral templates are necessary to recover any young Jovian exoplanets with minimal SNR loss. We also developed a complete pipeline for the automated detection of point source candidates, the calculation of Receiver Operating Characteristics (ROC), false positives based contrast curves, and completeness contours. We process in a uniform manner more than 330 datasets from the Gemini Planet Imager Exoplanet Survey (GPIES) and assess GPI typical sensitivity as a function of the star and the hypothetical companion spectral type. This work allows for the first time a comparison of different detection algorithms at a survey scale accounting for both planet completeness and false positive rate. We show that the new forward model matched filter allows the detection of $50\%$ fainter objects than a conventional cross-correlation technique with a Gaussian PSF template for the same false positive rate. ","Improving and Assessing Planet Sensitivity of the GPI Exoplanet Survey
  with a Forward Model Matched Filter"
28,864916024189177857,2789616249,Ivan Martino,"['This week, a new paper on #arxiv on the #face #ring for the #matroids over the #integers. Have a look: <LINK>']",https://arxiv.org/abs/1705.05816,"In this work, we define the face module for a realizable matroid over Z. Its Hilbert series is, indeed, the expected specialization of the Grothendieck - Tutte polynomial defined by Fink and Moci. This work will appear in 'Contributions to Discrete Mathematics' ",Face module for realizable Z-matroids
29,864823902819823616,3716338821,Mikko Tuomi,"['New paper: ""Evidence for at least three planet candidates orbiting #HD20794"". <LINK>', '""We also find a significant signal at a period of about 330 d corresponding to a super-Earth or Neptune in the habitable zone.""', 'There is tentative evidence for as many as 6 planets orbiting #HD20794 but the interpretation of these signals is difficult. https://t.co/NnLzG4Io0O', 'It is now possible to detect signals with amplitudes of only 40cm/s - a factor of 4 in excess that required for detection of Earth analogs.', 'The data reduction and modelling approach applied to HARPS data of #HD20794 explained. https://t.co/435JH9FqjJ', 'The paper also applies the ""moving periodogram"", enabling studying the time-invariance of signals. https://t.co/EaM1D1qlVO https://t.co/h5WQkMvIdM', 'Demonstrating time-invariance is a key when finding Doppler signals of planets - stellar activity will always vary as a function of time.']",https://arxiv.org/abs/1705.05124,"We explore the feasibility of detecting Earth analogs around Sun-like stars using the radial velocity method by investigating one of the largest radial velocities datasets for the one of the most stable radial-velocity stars HD20794. We proceed by disentangling the Keplerian signals from correlated noise and activity-induced variability. We diagnose the noise using the differences between radial velocities measured at different wavelength ranges, so-called ""differential radial velocities"". We apply this method to the radial velocities measured by HARPS, and identify four signals at 18, 89, 147 and 330 d. The two signals at periods of 18 and 89 d are previously reported and are better quantified in this work. The signal at a period of about 147 d is reported for the first time, and corresponds to a super-Earth with a minimum mass of 4.59 Earth mass located 0.51 AU from HD20794. We also find a significant signal at a period of about 330 d corresponding to a super-Earth or Neptune in the habitable zone. Since this signal is close to the annual sampling period and significant periodogram power in some noise proxies are found close to this signal, further observations and analyses are required to confirm it. The analyses of the eccentricity and consistency of signals provide weak evidence for the existence of the previously reported 43 d signal and a new signal at a period of about 11.9 d with a semi amplitude of 0.4 m/s. We find that the detection of a number of signals with radial velocity variations around 0.5\,m/s likely caused by low mass planet candidates demonstrates the important role of noise modeling in searching for Earth analogs. ",Evidence for at least three planet candidates orbiting HD20794
30,864794381232078848,30220394,Albert Puig,"['New paper from @LHCbExperiment, another anomaly in the list! <LINK> <LINK>']",http://arxiv.org/abs/1705.05802,"A test of lepton universality, performed by measuring the ratio of the branching fractions of the $B^{0} \rightarrow K^{*0}\mu^{+}\mu^{-}$ and $B^{0} \rightarrow K^{*0}e^{+}e^{-}$ decays, $R_{K^{*0}}$, is presented. The $K^{*0}$ meson is reconstructed in the final state $K^{+}\pi^{-}$, which is required to have an invariant mass within 100$\mathrm{\,MeV}c^2$ of the known $K^{*}(892)^{0}$ mass. The analysis is performed using proton-proton collision data, corresponding to an integrated luminosity of about 3$\mathrm{\,fb}^{-1}$, collected by the LHCb experiment at centre-of-mass energies of 7 and 8$\mathrm{\,TeV}$. The ratio is measured in two regions of the dilepton invariant mass squared, $q^{2}$, to be \begin{eqnarray*} R_{K^{*0}} = \begin{cases} 0.66~^{+~0.11}_{-~0.07}\mathrm{\,(stat)} \pm 0.03\mathrm{\,(syst)} & \textrm{for } 0.045 < q^{2} < 1.1~\mathrm{\,GeV^2}c^4 \, , \\ 0.69~^{+~0.11}_{-~0.07}\mathrm{\,(stat)} \pm 0.05\mathrm{\,(syst)} & \textrm{for } 1.1\phantom{00} < q^{2} < 6.0~\mathrm{\,GeV^2}c^4 \, . \end{cases} \end{eqnarray*} The corresponding 95.4\% confidence level intervals are $[0.52, 0.89]$ and $[0.53, 0.94]$. The results, which represent the most precise measurements of $R_{K^{*0}}$ to date, are compatible with the Standard Model expectations at the level of 2.1--2.3 and 2.4--2.5 standard deviations in the two $q^{2}$ regions, respectively. ","Test of lepton universality with $B^{0} \rightarrow
  K^{*0}\ell^{+}\ell^{-}$ decays"
31,864692335061237760,837133583558987776,Colin Raffel,"['New paper led by fellow residents Dieterich L. and George T. on seq2seq models with hard attention using RL/VI! <LINK>', 'Joint work wit Dieterich Lawson, George Tucker, Chung-Cheng Chiu, Kevin Swersky, and Navdeep Jaitly.']",https://arxiv.org/abs/1705.05524,"There has recently been significant interest in hard attention models for tasks such as object recognition, visual captioning and speech recognition. Hard attention can offer benefits over soft attention such as decreased computational cost, but training hard attention models can be difficult because of the discrete latent variables they introduce. Previous work used REINFORCE and Q-learning to approach these issues, but those methods can provide high-variance gradient estimates and be slow to train. In this paper, we tackle the problem of learning hard attention for a sequential task using variational inference methods, specifically the recently introduced VIMCO and NVIL. Furthermore, we propose a novel baseline that adapts VIMCO to this setting. We demonstrate our method on a phoneme recognition task in clean and noisy environments and show that our method outperforms REINFORCE, with the difference being greater for a more complicated task. ",Learning Hard Alignments with Variational Inference
32,864020166765576192,153170667,Francesco Nori,['My new paper on using sparse matrix factorisations to reduce the computational complexity of dynamic computations <LINK> <LINK>'],http://arxiv.org/abs/1705.04658,"We propose an algorithm to compute the dynamics of articulated rigid-bodies with different sensor distributions. Prior to the on-line computations, the proposed algorithm performs an off-line optimisation step to simplify the computational complexity of the underlying solution. This optimisation step consists in formulating the dynamic computations as a system of linear equations. The computational complexity of computing the associated solution is reduced by performing a permuted LU-factorisation with off-line optimised permutations. We apply our algorithm to solve classical dynamic problems: inverse and forward dynamics. The computational complexity of the proposed solution is compared to `gold standard' algorithms: recursive Newton-Euler and articulated body algorithm. It is shown that our algorithm reduces the number of floating point operations with respect to previous approaches. We also evaluate the numerical complexity of our algorithm by performing tests on dynamic computations for which no gold standard is available. ","Inverse, forward and other dynamic computations computationally
  optimized with sparse matrix factorizations"
33,863440276739694592,1724973901,Yibo Zhang,['Solving phase recovery problem in holography using Deep Learning! Check out our new paper <LINK>'],https://arxiv.org/abs/1705.04286,"Phase recovery from intensity-only measurements forms the heart of coherent imaging techniques and holography. Here we demonstrate that a neural network can learn to perform phase recovery and holographic image reconstruction after appropriate training. This deep learning-based approach provides an entirely new framework to conduct holographic imaging by rapidly eliminating twin-image and self-interference related spatial artifacts. Compared to existing approaches, this neural network based method is significantly faster to compute, and reconstructs improved phase and amplitude images of the objects using only one hologram, i.e., requires less number of measurements in addition to being computationally faster. We validated this method by reconstructing phase and amplitude images of various samples, including blood and Pap smears, and tissue sections. These results are broadly applicable to any phase recovery problem, and highlight that through machine learning challenging problems in imaging science can be overcome, providing new avenues to design powerful computational imaging systems. ","Phase recovery and holographic image reconstruction using deep learning
  in neural networks"
34,862857280123015168,46316479,Ondrej Pejcha,['A new paper led by Brian Metzger on explaining the light curves of stellar  mergers: a lot of pre-explosion massloss <LINK>'],https://arxiv.org/abs/1705.03895,"Luminous red novae (LRN) are a class of optical transients believed to originate from the mergers of binary stars, or ""common envelope"" events. Their light curves often show secondary maxima, which cannot be explained in the previous models of thermal energy diffusion or hydrogen recombination without invoking multiple independent shell ejections. We propose that double-peaked light curves are a natural consequence of a collision between dynamically-ejected fast shell and pre-existing equatorially-focused material, which was shed from the binary over many orbits preceding the dynamical event. The fast shell expands freely in the polar directions, powering the initial optical peak through cooling envelope emission. Radiative shocks from the collision in the equatorial plane power the secondary light curve peak on the radiative diffusion timescale of the deeper layers, similar to luminous Type IIn supernovae and some classical novae. Using a detailed 1D analytic model, informed by complementary 3D hydrodynamical simulations, we show that shock-powered emission can explain the observed range of peak timescales and luminosities of the secondary peaks in LRN for realistic variations in the binary parameters and fraction of the binary mass ejected. The dense shell created by the radiative shocks in the equatorial plane provides an ideal location for dust nucleation consistent with the the inferred aspherical geometry of dust in LRN. For giant stars, the ejecta forms dust when the shock-powered luminosity is still high, which could explain the infrared transients recently discovered by Spitzer. Our results suggest that pre-dynamical mass loss is common if not ubiquitous in stellar mergers, providing insight into the instabilities responsible for driving the binary merger. ","Shock-powered light curves of luminous red novae as signatures of
  pre-dynamical mass loss in stellar mergers"
35,862555257846542336,75249390,Axel Maas,['I have published a new paper on the intricacies #gaugetheory at <LINK> #np3'],https://arxiv.org/abs/1705.03812,"Beyond perturbation theory the number of gauge copies drastically increases due to the Gribov-Singer ambiguity. Any way of treating them defines, in principle, a new, non-perturbative gauge, and the gauge-dependent correlation functions can vary between them. Herein various such gauges will be constructed as completions of the Landau gauge inside the first Gribov region. The dependence of the propagators and the running coupling on these gauges will be studied for SU(2) Yang-Mills theory in two, three, and four dimensions using lattice gauge theory, and for a wide range of lattice parameters. While the gluon propagator is rather insensitive to the choice, the ghost propagator and the running coupling show a stronger dependence. It is also found that the influence of lattice artifacts is larger than in minimal Landau gauge. ","Dependence of the propagators on the sampling of Gribov copies inside
  the first Gribov region of Landau gauge"
36,862444428329439232,3199605543,Afonso S. Bandeira,"['New paper on Community Detection in Hypergraphs, Spiked Tensor Models, and Sum-of-Squares <LINK>']",https://arxiv.org/abs/1705.02973,"We study the problem of community detection in hypergraphs under a stochastic block model. Similarly to how the stochastic block model in graphs suggests studying spiked random matrices, our model motivates investigating statistical and computational limits of exact recovery in a certain spiked tensor model. In contrast with the matrix case, the spiked model naturally arising from community detection in hypergraphs is different from the one arising in the so-called tensor Principal Component Analysis model. We investigate the effectiveness of algorithms in the Sum-of-Squares hierarchy on these models. Interestingly, our results suggest that these two apparently similar models exhibit significantly different computational to statistical gaps. ","Community Detection in Hypergraphs, Spiked Tensor Models, and
  Sum-of-Squares"
37,862345210226978821,2250923426,Toby Pereira,"['My new paper, arguing against the future existence of superintelligent AI is now up. Let me know what you think! <LINK>']",https://arxiv.org/abs/1705.03078,"This paper uses anthropic reasoning to argue for a reduced likelihood that superintelligent AI will come into existence in the future. To make this argument, a new principle is introduced: the Super-Strong Self-Sampling Assumption (SSSSA), building on the Self-Sampling Assumption (SSA) and the Strong Self-Sampling Assumption (SSSA). SSA uses as its sample the relevant observers, whereas SSSA goes further by using observer-moments. SSSSA goes further still and weights each sample proportionally, according to the size of a mind in cognitive terms. SSSSA is required for human observer-samples to be typical, given by how much non-human animals outnumber humans. Given SSSSA, the assumption that humans experience typical observer-samples relies on a future where superintelligent AI does not dominate, which in turn reduces the likelihood of it being created at all. ","An Anthropic Argument against the Future Existence of Superintelligent
  Artificial Intelligence"
38,862309599361343488,261324356,Ray Jayawardhana,['Coming up dry. Search for #water in a super-Earth #exoplanet atmosphere - our new paper: <LINK> #astronomy #science <LINK>'],https://arxiv.org/abs/1705.03022,"We present the analysis of high-resolution optical spectra of four transits of 55Cnc e, a low-density, super-Earth that orbits a nearby Sun-like star in under 18 hours. The inferred bulk density of the planet implies a substantial envelope, which, according to mass-radius relationships, could be either a low-mass extended or a high-mass compact atmosphere. Our observations investigate the latter scenario, with water as the dominant species. We take advantage of the Doppler cross-correlation technique, high-spectral resolution and the large wavelength coverage of our observations to search for the signature of thousands of optical water absorption lines. Using our observations with HDS on the Subaru telescope and ESPaDOnS on the Canada-France-Hawaii Telescope, we are able to place a 3-sigma lower limit of 10 g/mol on the mean-molecular weight of 55Cnc e's water-rich (volume mixing ratio >10%), optically-thin atmosphere, which corresponds to an atmospheric scale-height of ~80 km. Our study marks the first high-spectral resolution search for water in a super-Earth atmosphere and demonstrates that it is possible to recover known water-vapour absorption signals, in a nearby super-Earth atmosphere, using high-resolution transit spectroscopy with current ground-based instruments. ","Search for water in a super-Earth atmosphere: High-resolution optical
  spectroscopy of 55 Cancri e"
39,862243144808767488,3716338821,Mikko Tuomi,"['New paper of @phillippro: ""Agatha: disentangling periodic signals from correlated noise in a  periodogram framework"" <LINK> <LINK>', '.@phillippro The freely available software ""Agatha"" can calculate useful moving periodograms to study time-invariance of periodic signals. https://t.co/4u7S0Elpqc', '.@phillippro There is also an expression for a Bayesian version of the periodogram, worked out analytically by @phillippro. https://t.co/Y5PXgGHluw', '.@phillippro The Agatha application is accessible online: https://t.co/voDDpeqFyw']",https://arxiv.org/abs/1705.03089,"Periodograms are used as a key significance assessment and visualisation tool to display the significant periodicities in unevenly sampled time series. We introduce a framework of periodograms, called ""Agatha"", to disentangle periodic signals from correlated noise and to solve the 2-dimensional model selection problem: signal dimension and noise model dimension. These periodograms are calculated by applying likelihood maximization and marginalization and combined in a self-consistent way. We compare Agatha with other periodograms for the detection of Keplerian signals in synthetic radial velocity data produced for the Radial Velocity Challenge as well as in radial velocity datasets of several Sun-like stars. In our tests we find Agatha is able to recover signals to the adopted detection limit of the radial velocity challenge. Applied to real radial velocity, we use Agatha to confirm previous analysis of CoRoT-7 and to find two new planet candidates with minimum masses of 15.1 $M_\oplus$ and 7.08 $M_\oplus$ orbiting HD177565 and HD41248, with periods of 44.5 d and 13.4 d, respectively. We find that Agatha outperforms other periodograms in terms of removing correlated noise and assessing the significances of signals with more robust metrics. Moreover, it can be used to select the optimal noise model and to test the consistency of signals in time. Agatha is intended to be flexible enough to be applied to time series analyses in other astronomical and scientific disciplines. Agatha is available at this http URL ","Agatha: disentangling periodic signals from correlated noise in a
  periodogram framework"
40,861868442554597376,858718620,Dr Rebecca,['Our new Optics Letters paper (<LINK>) on speckle-based hyperspectral imaging now also on #arXiv ðŸ¤“\xa0<LINK>'],https://arxiv.org/abs/1705.02991,"Encoding of spectral information onto monochrome imaging cameras is of interest for wavelength multiplexing and hyperspectral imaging applications. Here, the complex spatio-spectral response of a disordered material is used to demonstrate retrieval of a number of discrete wavelengths over a wide spectral range. Strong, diffuse light scattering in a semiconductor nanowire mat is used to achieve a highly compact spectrometer of micrometer thickness, transforming different wavelengths into distinct speckle patterns with nanometer sensitivity. Spatial multiplexing is achieved through the use of a microlens array, allowing simultaneous imaging of many speckles, ultimately limited by the size of the diffuse spot area. The performance of different information retrieval algorithms is compared. A compressive sensing algorithm exhibits efficient reconstruction capability in noisy environments and with only a few measurements. ","Speckle-based hyperspectral imaging combining multiple scattering and
  compressive sensing in nanowire mats"
41,861853655317843968,1576276220,Dr. Emily Petroff,"[""My new paper is up on the arXiv. I'm happy to present FRB 150215! A very special burst that I'll tell you all about! <LINK>"", 'We found this FRB in real-time (*right when it happened*) at Parkes on 15 February, 2015. It looks like this (super zoomed in): https://t.co/6IEBmxnvQ7', 'If you zoom out, it looks like a bright spike in the data: https://t.co/K9rlBNxWqW', 'Because we found it in real time we also were able to save all the raw data, including polarization information, which is really hard to get', 'Only 4 other FRBs have measured polarization. Some have circular (FRB 140514), some have linear (FRB 150807) some have none (FRB 150418)', 'FRB 150215 is only linearly polarized! The pulse is about 43% polarized, meaning 43% of the electric field is all lined up in the same way https://t.co/SKNlio6vSS', ""That's cool. But what's cooler is we can measure the Faraday rotation of polarization due to magnetic fields along the path from host to us"", 'We measure this with a quantity called Rotation Measure (RM) that you can read about here: https://t.co/cNezJKQDjU', 'TL;DR: Big RM = lots of ordered magnetic fields between us and FRB. Small RM = not a lot of magnetic field between us and FRB.', 'For FRB 150215? The RM = 0 ðŸ˜¯\nSurprising, because the FRB went through a dense line through the Milky Way, so there should be *some* RM!', 'It turns out FRB 150215 went through a VERY special sightline in the Galaxy.', 'All the nearby sources have positive RMs (blue,+) but the ONE source (red o) closest to FRB 150215 (the black X) has RM=0 too!! https://t.co/lYYgN4cu1T', 'So this FRB seems to have found a very unusual sightline through the Galaxy, perhaps why we managed to find it given such a dense foreground', 'So TL;DR for this thread: FRB 150215 is new and cool and not quite like any FRB found before. Read the paper! ðŸ˜€', ""@GabeBlessing It's a good question! We still don't know but pulsar is a maybe. FRBs come from far outside our galaxy, so it would have to be a STRONG psr"", '@franco_vazza Yeah we thought so too', ""@Devosv You're right! Coherent emission (like the emission from pulsars) tends to produce linear polarization on these timescales."", ""Final fun fact about this paper: it contains my favorite sentence I've ever written in an academic paper. https://t.co/ZmpHmJ7atX""]",https://arxiv.org/abs/1705.02911,"We report on the discovery of a new fast radio burst, FRB 150215, with the Parkes radio telescope on 2015 February 15. The burst was detected in real time with a dispersion measure (DM) of 1105.6$\pm$0.8 pc cm^{-3}, a pulse duration of 2.8^{+1.2}_{-0.5} ms, and a measured peak flux density assuming the burst was at beam center of 0.7^{+0.2}_{-0.1} Jy. The FRB originated at a Galactic longitude and latitude of 24.66^{\circ}, 5.28^{\circ}, 25 degrees away from the Galactic Center. The burst was found to be 43$\pm$5% linearly polarized with a rotation measure (RM) in the range -9 < RM < 12 rad m^{-2} (95% confidence level), consistent with zero. The burst was followed-up with 11 telescopes to search for radio, optical, X-ray, gamma-ray and neutrino emission. Neither transient nor variable emission was found to be associated with the burst and no repeat pulses have been observed in 17.25 hours of observing. The sightline to the burst is close to the Galactic plane and the observed physical properties of FRB 150215 demonstrate the existence of sight lines of anomalously low RM for a given electron column density. The Galactic RM foreground may approach a null value due to magnetic field reversals along the line of sight, a decreased total electron column density from the Milky Way, or some combination of these effects. A lower Galactic DM contribution might explain why this burst was detectable whereas previous searches at low latitude have had lower detection rates than those out of the plane. ",A polarized fast radio burst at low Galactic latitude
42,861765832765915136,330511669,Simon Ellingsen,['Finding high-mass starless clumps is really hard...  New paper from @jhyuan_naoc gives us hundreds!  <LINK>'],https://arxiv.org/abs/1705.02549,"We report a sample of 463 high-mass starless clump (HMSC) candidates within $-60\deg<l<60\deg$ and $-1\deg<b<1\deg$. This sample has been singled out from 10861 ATLASGAL clumps. All of these sources are not associated with any known star-forming activities collected in SIMBAD and young stellar objects identified using color-based criteria. We also make sure that the HMSC candidates have neither point sources at 24 and 70 \micron~nor strong extended emission at 24 $\mu$m. Most of the identified HMSCs are infrared ($\le24$ $\mu$m) dark and some are even dark at 70 $\mu$m. Their distribution shows crowding in Galactic spiral arms and toward the Galactic center and some well-known star-forming complexes. Many HMSCs are associated with large-scale filaments. Some basic parameters were attained from column density and dust temperature maps constructed via fitting far-infrared and submillimeter continuum data to modified blackbodies. The HMSC candidates have sizes, masses, and densities similar to clumps associated with Class II methanol masers and HII regions, suggesting they will evolve into star-forming clumps. More than 90% of the HMSC candidates have densities above some proposed thresholds for forming high-mass stars. With dust temperatures and luminosity-to-mass ratios significantly lower than that for star-forming sources, the HMSC candidates are externally heated and genuinely at very early stages of high-mass star formation. Twenty sources with equivalent radius $r_\mathrm{eq}<0.15$ pc and mass surface density $\Sigma>0.08$ g cm$^{-2}$ could be possible high-mass starless cores. Further investigations toward these HMSCs would undoubtedly shed light on comprehensively understanding the birth of high-mass stars. ","High-mass Starless Clumps in the inner Galactic Plane: the Sample and
  Dust Properties"
43,860560101924519936,559207602,Juan Carlos Niebles,"['A new task based on our ""Dense-Captioning Events in Videos"" work by @RanjayKrishna! <LINK>\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1705.00754,"Most natural videos contain numerous events. For example, in a video of a ""man playing a piano"", the video might also contain ""another man dancing"" or ""a crowd clapping"". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization. ",Dense-Captioning Events in Videos
44,860508240970469377,570346104,Gokberk Cinbis,['We have posted a new arXiv paper on using attributes in zero-shot learning of classes with unknown attributes: <LINK>'],https://arxiv.org/abs/1705.01734,"We propose a novel approach for unsupervised zero-shot learning (ZSL) of classes based on their names. Most existing unsupervised ZSL methods aim to learn a model for directly comparing image features and class names. However, this proves to be a difficult task due to dominance of non-visual semantics in underlying vector-space embeddings of class names. To address this issue, we discriminatively learn a word representation such that the similarities between class and combination of attribute names fall in line with the visual similarity. Contrary to the traditional zero-shot learning approaches that are built upon attribute presence, our approach bypasses the laborious attribute-class relation annotations for unseen classes. In addition, our proposed approach renders text-only training possible, hence, the training can be augmented without the need to collect additional image data. The experimental results show that our method yields state-of-the-art results for unsupervised ZSL in three benchmark datasets. ","Attributes2Classname: A discriminative model for attribute-based
  unsupervised zero-shot learning"
45,860483687263031296,16079444,Ying-Jer Kao,"['New Paper with Yu-Chin Tzeng, Hiroaki Onishi, Tsuyoshi Okubo\n<LINK>']",https://arxiv.org/abs/1705.01558,"The spin-1 Haldane chain is an example of the symmetry-protected-topological (SPT) phase in one dimension. Experimental realization of the spin chain materials usually involves both the uniaxial-type, $D(S^z)^2$, and the rhombic-type, $E[(S^x)^2-(S^y)^2]$, single-ion anisotropies. Here, we provide a precise ground-state phase diagram for spin-1 Haldane chain with these single-ion anisotropies. Using quantum numbers, we find that the $\mathbb{Z}_2$ symmetry breaking phase can be characterized by double degeneracy in the entanglement spectrum. Topological quantum phase transitions take place on particular paths in the phase diagram, from the Haldane phase to the Large-$E_x$, Large-$E_y$, or Large-$D$ phases. The topological critical points are determined by the level spectroscopy method with a newly developed parity technique in the density matrix renormalization group [Phys. Rev. B 86, 024403 (2012)], and the Haldane-Large-$D$ critical point is obtained with an unprecedentedly precision, $(D/J)_c=0.9684713(1)$. Close to this critical point, a small rhombic single-ion anisotropy $|E|/J\ll1$ can destroy the Haldane phase and bring the system into a $y$-N\'eel phase. We propose that the compound [Ni(HF$_2$)(3-Clpy)$_4$]BF$_4$ is a candidate system to search for the $y$-N\'eel phase. ","Quantum phase transitions driven by rhombic-type single-ion anisotropy
  in the S=1 Haldane chain"
46,860440942645563392,3245949691,Rebecca Leane,['New paper! We show the inclusion of dark initial state radiation can provide the dominant DM annihilation channel: <LINK> <LINK>'],http://arxiv.org/abs/1705.01105,"Many dark matter interaction types lead to annihilation processes which suffer from p-wave suppression or helicity suppression, rendering them subdominant to unsuppressed s-wave processes. We demonstrate that the natural inclusion of dark initial state radiation can open an unsuppressed s-wave annihilation channel, and thus provide the dominant dark matter annihilation process for particular interaction types. We illustrate this effect with the bremsstrahlung of a dark spin-0 or dark spin-1 particle from fermionic dark matter, $\overline{\chi}\chi\rightarrow \overline{f}f\phi$ or $\overline{f}fZ'$. The dark initial state radiation process, despite having a 3-body final state, proceeds at the same order in the new physics scale $\Lambda$ as the annihilation to the 2-body final state $\overline{\chi}\chi\rightarrow \overline{f}f$. This is lower order in $\Lambda$ than the well-studied lifting of helicity suppression via Standard Model final state radiation, or virtual internal bremsstrahlung. This dark bremsstrahlung process should influence LHC and indirect detection searches for dark matter. ",Enhancing Dark Matter Annihilation Rates with Dark Bremsstrahlung
47,860378536888201216,608502805,THOMAS Guillaume,"['What does the Sagittarius stream look like in MOND ? Our new paper with @darth_ben, R.ibata, @splunchy and P. Kroupa\n<LINK> <LINK>']",https://arxiv.org/abs/1705.01552,"Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy offer a unique way to constrain the shape of galactic gravitational potentials. Such streams can be used as leaning tower gravitational experiments on galactic scales. The most well motivated modification of gravity proposed as an alternative to dark matter on galactic scales is Milgromian dynamics (MOND), and we present here the first ever N-body simulations of the dynamical evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a realistic baryonic mass model for the Milky Way, we attempt to reproduce the present-day spatial and kinematic structure of the Sagittarius dwarf and its immense tidal stream that wraps around the Milky Way. With very little freedom on the original structure of the progenitor, constrained by the total luminosity of the Sagittarius structure and by the observed stellar mass-size relation for isolated dwarf galaxies, we find reasonable agreement between our simulations and observations of this system. The observed stellar velocities in the leading arm can be reproduced if we include a massive hot gas corona around the Milky Way that is flattened in the direction of the principal plane of its satellites. This is the first time that tidal dissolution in MOND has been tested rigorously at these mass and acceleration scales. ",Stellar streams as gravitational experiments I. The case of Sagittarius
48,860052316082380801,1601296094,David Bowler,['New paper from my hugely fruitful collaboration with U. Geneva on sub-atomic features in STM of Bi nanolines <LINK>'],https://arxiv.org/abs/1705.01318,"Scanning tunneling microscopy (STM) reveals unusual sharp features in otherwise defect free bismuth nanolines self-assembled on Si(001). They appear as subatomic thin lines perpendicular to the bismuth nanoline at positive biases and as atomic size beads at negative biases. Density functional theory (DFT) simulations show that these features can be attributed to buckled Si dimers substituting for Bi dimers in the nanoline, where the sharp feature is the counterintuitive signature of these dimers flipping during scanning. The perfect correspondence between the STM data and the DFT simulation demonstrated in this study highlights the detailed understanding we have of the complex Bi-Si(001) Haiku system. ","Sub-atomic electronic feature from dynamic motion of Si dimer defects in
  bismuth nanolines on Si(001)"
49,859880863881977865,96779364,Arnab Bhattacharyya,"[""New paper on one-bit compressive sensing (with Acharya and Kamath): <LINK> (ISIT '17 to appear)""]",https://arxiv.org/abs/1705.00763,"Unlike compressive sensing where the measurement outputs are assumed to be real-valued and have infinite precision, in ""one-bit compressive sensing"", measurements are quantized to one bit, their signs. In this work, we show how to recover the support of sparse high-dimensional vectors in the one-bit compressive sensing framework with an asymptotically near-optimal number of measurements. We also improve the bounds on the number of measurements for approximately recovering vectors from one-bit compressive sensing measurements. Our results are universal, namely the same measurement scheme works simultaneously for all sparse vectors. Our proof of optimality for support recovery is obtained by showing an equivalence between the task of support recovery using 1-bit compressive sensing and a well-studied combinatorial object known as Union Free Families. ",Improved Bounds for Universal One-Bit Compressive Sensing
50,859741097509228546,83172841,Phil Rosenfield,"['New paper out! <LINK>, we find correlations between convective core overshooting in stars and other interesting parameters.', 'You can also remake the figures and play with our stellar evolution models, https://t.co/NTDJaioMuz']",https://arxiv.org/abs/1705.00618,"We present a framework to simultaneously constrain the values and uncertainties of the strength of convective core overshooting, metallicity, extinction, distance, and age in stellar populations. We then apply the framework to archival Hubble Space Telescope observations of six stellar clusters in the Large Magellanic Cloud that have reported ages between ~1-2.5 Gyr. Assuming a canonical value of the strength of core convective overshooting, we recover the well-known age-metallicity correlation, and additional correlations between metallicity and extinction and metallicity and distance. If we allow the strength of core overshooting to vary, we find that for intermediate-aged stellar clusters, the measured values of distance and extinction are negligibly effected by uncertainties of core overshooting strength. However, cluster age and metallicity may have disconcertingly large systematic shifts when core overshooting strength is allowed to vary by more than +/- 0.05 Hp. Using the six stellar clusters, we combine their posterior distribution functions to obtain the most probable core overshooting value, 0.500 +0.016 -0.134 Hp, which is in line with canonical values. ","A New Approach to Convective Core Overshooting: Probabilistic
  Constraints from Color-Magnitude Diagrams of LMC Clusters"
51,859680952926208000,1192577568,Daniel Worrall,['Bayesian CNNs for dMRI super-resolution and IQT. Check out our new paper: <LINK> #stateoftheart #deeplearning #bayesian'],https://arxiv.org/abs/1705.00664,"In this work, we investigate the value of uncertainty modeling in 3D super-resolution with convolutional neural networks (CNNs). Deep learning has shown success in a plethora of medical image transformation problems, such as super-resolution (SR) and image synthesis. However, the highly ill-posed nature of such problems results in inevitable ambiguity in the learning of networks. We propose to account for intrinsic uncertainty through a per-patch heteroscedastic noise model and for parameter uncertainty through approximate Bayesian inference in the form of variational dropout. We show that the combined benefits of both lead to the state-of-the-art performance SR of diffusion MR brain images in terms of errors compared to ground truth. We further show that the reduced error scores produce tangible benefits in downstream tractography. In addition, the probabilistic nature of the methods naturally confers a mechanism to quantify uncertainty over the super-resolved output. We demonstrate through experiments on both healthy and pathological brains the potential utility of such an uncertainty measure in the risk assessment of the super-resolved images for subsequent clinical use. ","Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI
  Super-Resolution"
52,859383427447091201,1444716745,Oliver Jensen,['New paper with Feng Xu: Trapping and displacement of liquid collars and plugs in rough-walled tubes <LINK>'],http://arxiv.org/abs/1705.00371,"A liquid film wetting the interior of a long circular cylinder redistributes under the action of surface tension to form annular collars or occlusive plugs. These equilibrium structures are invariant under axial translation within a perfectly smooth uniform tube and therefore can be displaced axially by very weak external forcing. We consider how this degeneracy is disrupted when the tube wall is rough, and determine threshold conditions under which collars or plugs resist displacement under forcing. Wall roughness is modelled as a non-axisymmetric Gaussian random field of prescribed correlation length and small variance, mimicking some of the geometric irregularities inherent in applications such as lung airways. The thin film coating this surface is modelled using lubrication theory. When the roughness is weak, we show how the locations of equilibrium collars and plugs can be identified in terms of the azimuthally averaged tube radius; we derive conditions specifying equilibrium collar locations under an externally imposed shear flow, and plug locations under an imposed pressure gradient. We use these results to determine the probability of external forcing being sufficient to displace a collar or plug from a rough-walled tube, when the tube roughness is defined only in statistical terms. ","Trapping and displacement of liquid collars and plugs in rough-walled
  tubes"
53,859212100002082817,36396172,Diego R. Amancio,"['Our new draft paper has just hit the arXiv: ""Labelled network motifs reveal stylistic subtleties in written texts"" <LINK>']",https://arxiv.org/abs/1705.00545,"The vast amount of data and increase of computational capacity have allowed the analysis of texts from several perspectives, including the representation of texts as complex networks. Nodes of the network represent the words, and edges represent some relationship, usually word co-occurrence. Even though networked representations have been applied to study some tasks, such approaches are not usually combined with traditional models relying upon statistical paradigms. Because networked models are able to grasp textual patterns, we devised a hybrid classifier, called labelled subgraphs, that combines the frequency of common words with small structures found in the topology of the network, known as motifs. Our approach is illustrated in two contexts, authorship attribution and translationese identification. In the former, a set of novels written by different authors is analyzed. To identify translationese, texts from the Canadian Hansard and the European parliament were classified as to original and translated instances. Our results suggest that labelled subgraphs are able to represent texts and it should be further explored in other tasks, such as the analysis of text complexity, language proficiency, and machine translation. ",Labelled network subgraphs reveal stylistic subtleties in written texts
54,866830909764288512,367297219,Melanie Mitchell,"['Unsupervised learning helps! New paper from my research group: ""Sparse coding on stereo video for object detection"": <LINK>']",https://arxiv.org/abs/1705.07144,"Deep Convolutional Neural Networks (DCNN) require millions of labeled training examples for image classification and object detection tasks, which restrict these models to domains where such datasets are available. In this paper, we explore the use of unsupervised sparse coding applied to stereo-video data to help alleviate the need for large amounts of labeled data. We show that replacing a typical supervised convolutional layer with an unsupervised sparse-coding layer within a DCNN allows for better performance on a car detection task when only a limited number of labeled training examples is available. Furthermore, the network that incorporates sparse coding allows for more consistent performance over varying initializations and ordering of training examples when compared to a fully supervised DCNN. Finally, we compare activations between the unsupervised sparse-coding layer and the supervised convolutional layer, and show that the sparse representation exhibits an encoding that is depth selective, whereas encodings from the convolutional layer do not exhibit such selectivity. These result indicates promise for using unsupervised sparse-coding approaches in real-world computer vision tasks in domains with limited labeled training data. ",Sparse Coding on Stereo Video for Object Detection
55,865498451085107200,630560519,Dr Johanna Vos,['New paper on ApJ: Turns out that the viewing angle of brown dwarfs influences their observed colours + variability!\n<LINK>'],https://arxiv.org/abs/1705.06045,"In this paper we study the full sample of known Spitzer [$3.6~\mu$m] and $J$-band variable brown dwarfs. We calculate the rotational velocities, $v\sin i$, of 16 variable brown dwarfs using archival Keck NIRSPEC data and compute the inclination angles of 19 variable brown dwarfs. The results obtained show that all objects in the sample with mid-IR variability detections are inclined at an angle $>20^{\circ}$, while all objects in the sample displaying $J$-band variability have an inclination angle $>35^{\circ}$. $J$-band variability appears to be more affected by inclination than \textit{Spitzer} [$3.6~\mu$m] variability, and is strongly attenuated at lower inclinations. Since $J$-band observations probe deeper into the atmosphere than mid-IR observations, this effect may be due to the increased atmospheric path length of $J$-band flux at lower inclinations. We find a statistically significant correlation between the colour anomaly and inclination of our sample, where field objects viewed equator-on appear redder than objects viewed at lower inclinations. Considering the full sample of known variable L, T and Y spectral type objects in the literature, we find that the variability properties of the two bands display notably different trends, due to both intrinsic differences between bands and the sensitivity of ground-based versus space-based searches. However, in both bands we find that variability amplitude may reach a maximum at $\sim7-9~$hr periods. Finally, we find a strong correlation between colour anomaly and variability amplitude for both the $J$-band and mid-IR variability detections, where redder objects display higher variability amplitudes. ","The Viewing Geometry of Brown Dwarfs Influences Their Observed Colours
  and Variability Properties"
56,864281445824692224,24053629,Balaji Lakshminarayanan,['New paper with @DeepMindAI colleagues on comparison of GAN-training and maximum likelihood training using Real-NVP: <LINK>'],https://arxiv.org/abs/1705.05263,"We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic. ",Comparison of Maximum Likelihood and GAN-based training of Real NVPs
57,861945674228793344,825088493764407298,Noam Brown (in Pittsburgh),"['Added Libratus AI Poker comp results to our paper. New material, and more approachable for a general audience: <LINK>']",https://arxiv.org/abs/1705.02955,"In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker. ",Safe and Nested Subgame Solving for Imperfect-Information Games
58,859377240932536320,338526004,Sam Bowman,"['New paper w/ Yacine Jernite: A fast(!), discriminative version of SkipThought <LINK> <LINK>']",https://arxiv.org/abs/1705.00557,"This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations. ","Discourse-Based Objectives for Fast Unsupervised Sentence Representation
  Learning"
59,867537636218556416,3053460216,Matthias Niessner,['Our material estimation approach by @Nvidia Research! We also propose the SynBRDF dataset :)\n<LINK>\n#deeplearning #3dvision <LINK>'],https://arxiv.org/abs/1705.07162,"Estimating surface reflectance (BRDF) is one key component for complete 3D scene capture, with wide applications in virtual reality, augmented reality, and human computer interaction. Prior work is either limited to controlled environments (\eg gonioreflectometers, light stages, or multi-camera domes), or requires the joint optimization of shape, illumination, and reflectance, which is often computationally too expensive (\eg hours of running time) for real-time applications. Moreover, most prior work requires HDR images as input which further complicates the capture process. In this paper, we propose a lightweight approach for surface reflectance estimation directly from $8$-bit RGB images in real-time, which can be easily plugged into any 3D scanning-and-fusion system with a commodity RGBD sensor. Our method is learning-based, with an inference time of less than 90ms per scene and a model size of less than 340K bytes. We propose two novel network architectures, HemiCNN and Grouplet, to deal with the unstructured input data from multiple viewpoints under unknown illumination. We further design a loss function to resolve the color-constancy and scale ambiguity. In addition, we have created a large synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images rendered with a physically-based ray tracer under a variety of natural illumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first large-scale benchmark dataset for reflectance estimation. Experiments on both synthetic data and real data show that the proposed method effectively recovers surface reflectance, and outperforms prior work for reflectance estimation in uncontrolled environments. ",A Lightweight Approach for On-the-Fly Reflectance Estimation
60,859741097509228546,83172841,Phil Rosenfield,"['New paper out! <LINK>, we find correlations between convective core overshooting in stars and other interesting parameters.', 'You can also remake the figures and play with our stellar evolution models, https://t.co/NTDJaioMuz']",https://arxiv.org/abs/1705.00618,"We present a framework to simultaneously constrain the values and uncertainties of the strength of convective core overshooting, metallicity, extinction, distance, and age in stellar populations. We then apply the framework to archival Hubble Space Telescope observations of six stellar clusters in the Large Magellanic Cloud that have reported ages between ~1-2.5 Gyr. Assuming a canonical value of the strength of core convective overshooting, we recover the well-known age-metallicity correlation, and additional correlations between metallicity and extinction and metallicity and distance. If we allow the strength of core overshooting to vary, we find that for intermediate-aged stellar clusters, the measured values of distance and extinction are negligibly effected by uncertainties of core overshooting strength. However, cluster age and metallicity may have disconcertingly large systematic shifts when core overshooting strength is allowed to vary by more than +/- 0.05 Hp. Using the six stellar clusters, we combine their posterior distribution functions to obtain the most probable core overshooting value, 0.500 +0.016 -0.134 Hp, which is in line with canonical values. ","A New Approach to Convective Core Overshooting: Probabilistic
  Constraints from Color-Magnitude Diagrams of LMC Clusters"
