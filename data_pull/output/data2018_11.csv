,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1073254126581014528,569952316,Anthony Widjaja Lin,['Two papers accepted for presentations at #popl19: \n1. a new string solver based on surprisingly simple ideas (<LINK>)\n2. a TOPLAS paper on CSS minification via constraint solving (<LINK>) - the fruit of 3+ years of labor\n\nSee you in Portugal!'],https://arxiv.org/abs/1811.03167,"The design and implementation of decision procedures for checking path feasibility in string-manipulating programs is an important problem, whose applications include symbolic execution and automated detection of cross-site scripting (XSS) vulnerabilities. A (symbolic) path is a finite sequence of assignments and assertions (i.e. without loops), and checking its feasibility amounts to determining the existence of inputs that yield a successful execution. We give two general semantic conditions which together ensure the decidability of path feasibility: (1) each assertion admits regular monadic decomposition, and (2) each assignment uses a (possibly nondeterministic) function whose inverse relation preserves regularity. We show these conditions are expressive since they are satisfied by a multitude of string operations. They also strictly subsume existing decidable string theories, and most existing benchmarks (e.g. most of Kaluza's, and all of SLOG's, Stranger's, and SLOTH's). We give a simple decision procedure and an extensible architecture of a string solver in that a user may easily incorporate his/her own string functions. We show the general fragment has a tight, but high complexity. To address this, we propose to allow only partial string functions (i.e., prohibit nondeterminism) in condition (2). When nondeterministic functions are needed, we also provide a syntactic fragment that provides a support of nondeterministic functions but can be reduced to an existing solver SLOTH. We provide an efficient implementation of our decision procedure for deterministic partial string functions in a new string solver OSTRICH. It provides built-in support for concatenation, reverse, functional transducers, and replaceall and provides a framework for extensibility to support further string functions. We demonstrate the efficacy of our new solver against other competitive solvers. ","Decision Procedures for Path Feasibility of String-Manipulating Programs
  with Complex Operations"
1,1073250876389761024,75249390,Axel Maas,['I have published a new blog entry on our most recent paper <LINK> on the size of weak vector bosons at <LINK> - another piece of the puzzle we are currently trying to solve #np3'],https://arxiv.org/abs/1811.03395,"Gauge invariance requires even in the weak interactions that physical, observable particles are described by gauge-invariant composite operators. Such operators have the same structure as those describing bound states, and consequently the physical versions of the $W^\pm$, the $Z$, and the Higgs should have some kind of substructure. To test this consequence, we use lattice gauge theory to study the physical weak vector bosons off-shell, especially their form-factor and weak radius, and compare the results to the ones for the elementary particles. We find that the physical particles show substantial deviations from the structure of a point-like particle. At the same time the gauge-dependent elementary particles exhibit unphysical behavior. ",Exploratory study of the off-shell properties of the weak vector bosons
2,1070499250251931649,214639688,Adina Williams,"[""#SCiL2019 paper out w/ @kelina1124 A. Warstadt @sleepinyourhat #LSA2019\nTL;DR Two new CoLA-style arg. structure alternation datasets (LaVA&amp;FAVA)! Our models aren't equally good on all alternations; some info in word embeddings isn't in sentence embeddings\n<LINK>""]",https://arxiv.org/abs/1811.10773,"Verbs occur in different syntactic environments, or frames. We investigate whether artificial neural networks encode grammatical distinctions necessary for inferring the idiosyncratic frame-selectional properties of verbs. We introduce five datasets, collectively called FAVA, containing in aggregate nearly 10k sentences labeled for grammatical acceptability, illustrating different verbal argument structure alternations. We then test whether models can distinguish acceptable English verb-frame combinations from unacceptable ones using a sentence embedding alone. For converging evidence, we further construct LaVA, a corresponding word-level dataset, and investigate whether the same syntactic features can be extracted from word embeddings. Our models perform reliable classifications for some verbal alternations but not others, suggesting that while these representations do encode fine-grained lexical information, it is incomplete or can be hard to extract. Further, differences between the word- and sentence-level models show that some information present in word embeddings is not passed on to the down-stream sentence embeddings. ",Verb Argument Structure Alternations in Word and Sentence Embeddings
3,1070313333650268162,618466970,Damiano Brigo (Prof),"['New paper on #optimal_trade_execution. We compare #optimal_trading_strategies: static vs adaptive.  Joint work with @eyal_neuman , Claudio Bellani and Alex Done. <LINK>']",https://arxiv.org/abs/1811.11265,"We compare optimal static and dynamic solutions in trade execution. An optimal trade execution problem is considered where a trader is looking at a short-term price predictive signal while trading. When the trader creates an instantaneous market impact, it is shown that transaction costs of optimal adaptive strategies are substantially lower than the corresponding costs of the optimal static strategy. In the same spirit, in the case of transient impact it is shown that strategies that observe the signal a finite number of times can dramatically reduce the transaction costs and improve the performance of the optimal static strategy. ",Static vs Adaptive Strategies for Optimal Execution with Signals
4,1070163962405285888,2178510532,Tillman Weyde,['<LINK> our new paper on arXiv: Improved Speech Enhancement with the Wave-U-Net. Work with @craigmacartney'],https://arxiv.org/abs/1811.11307,"We study the use of the Wave-U-Net architecture for speech enhancement, a model introduced by Stoller et al for the separation of music vocals and accompaniment. This end-to-end learning method for audio source separation operates directly in the time domain, permitting the integrated modelling of phase information and being able to take large temporal contexts into account. Our experiments show that the proposed method improves several metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the speech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a reduced number of hidden layers is sufficient for speech enhancement in comparison to the original system designed for singing voice separation in music. We see this initial result as an encouraging signal to further explore speech enhancement in the time-domain, both as an end in itself and as a pre-processing step to speech recognition systems. ",Improved Speech Enhancement with the Wave-U-Net
5,1069649431241342978,206025368,Dr. Amber L. Stuver,['🌌🔭 NEWS: *4 new #GravitationalWaves detections* from binary #BlackHoles documented in the full catalog of detected gravitational waves from the 1st and 2nd observing runs (the complete set of data taken to date)! #O2catalog\nRead the paper here: <LINK>'],https://arxiv.org/abs/1811.12907,"We present the results from three gravitational-wave searches for coalescing compact binaries with component masses above 1$\mathrm{M}_\odot$ during the first and second observing runs of the Advanced gravitational-wave detector network. During the first observing run (O1), from September $12^\mathrm{th}$, 2015 to January $19^\mathrm{th}$, 2016, gravitational waves from three binary black hole mergers were detected. The second observing run (O2), which ran from November $30^\mathrm{th}$, 2016 to August $25^\mathrm{th}$, 2017, saw the first detection of gravitational waves from a binary neutron star inspiral, in addition to the observation of gravitational waves from a total of seven binary black hole mergers, four of which we report here for the first time: GW170729, GW170809, GW170818 and GW170823. For all significant gravitational-wave events, we provide estimates of the source properties. The detected binary black holes have total masses between $18.6_{-0.7}^{+3.2}\mathrm{M}_\odot$, and $84.4_{-11.1}^{+15.8} \mathrm{M}_\odot$, and range in distance between $320_{-110}^{+120}$ Mpc and $2840_{-1360}^{+1400}$ Mpc. No neutron star - black hole mergers were detected. In addition to highly significant gravitational-wave events, we also provide a list of marginal event candidates with an estimated false alarm rate less than 1 per 30 days. From these results over the first two observing runs, which include approximately one gravitational-wave detection per 15 days of data searched, we infer merger rates at the 90% confidence intervals of $110\, -\, 3840$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$ for binary neutron stars and $9.7\, -\, 101$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$ for binary black holes assuming fixed population distributions, and determine a neutron star - black hole merger rate 90% upper limit of $610$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$. ","GWTC-1: A Gravitational-Wave Transient Catalog of Compact Binary Mergers
  Observed by LIGO and Virgo during the First and Second Observing Runs"
6,1069582882715381761,216729597,Marcel S. Pawlowski,"['First day in my new job and I already have a paper on the arXiv 😉\n\nThis one, lead by Tyler Kelley, presents his new Phat ELVIS suite of simulations which investigates how the potential of a MW-like disk affects dark matter substructures in their halos. <LINK> <LINK>']",https://arxiv.org/abs/1811.12413,"We introduce an extension of the ELVIS project to account for the effects of the Milky Way galaxy on its subhalo population. Our simulation suite, Phat ELVIS, consists of twelve high-resolution cosmological dark matter-only (DMO) zoom simulations of Milky Way-size $\Lambda$CDM~ haloes ($M_{\rm v} = 0.7-2 \times 10^{12} \,\mathrm{M}_\odot$) along with twelve re-runs with embedded galaxy potentials grown to match the observed Milky Way disk and bulge today. The central galaxy potential destroys subhalos on orbits with small pericenters in every halo, regardless of the ratio of galaxy mass to halo mass. This has several important implications. 1) Most of the $\mathtt{Disk}$ runs have no subhaloes larger than $V_{\rm max} = 4.5$ km s$^{-1}$ within $20$ kpc and a significant lack of substructure going back $\sim 8$ Gyr, suggesting that local stream-heating signals from dark substructure will be rare. 2) The pericenter distributions of Milky Way satellites derived from $\mathit{Gaia}$ data are remarkably similar to the pericenter distributions of subhaloes in the $\mathtt{Disk}$ runs, while the DMO runs drastically over-predict galaxies with pericenters smaller than 20 kpc. 3) The enhanced destruction produces a tension opposite to that of the classic `missing satellites' problem: in order to account for ultra-faint galaxies known within $30$ kpc of the Galaxy, we must populate haloes with $V_\mathrm{peak} \simeq 7$ km s$^{-1}$ ($M \simeq 3 \times 10^{7} \,\mathrm{M}_\odot$ at infall), well below the atomic cooling limit of $V_\mathrm{peak} \simeq 16$ km s$^{-1}$ ($M \simeq 5 \times 10^{8} \,\mathrm{M}_\odot$ at infall). 4) If such tiny haloes do host ultra-faint dwarfs, this implies the existence of $\sim 1000$ satellite galaxies within 300 kpc of the Milky Way. ","Phat ELVIS: The inevitable effect of the Milky Way's disk on its dark
  matter subhaloes"
7,1069577239996624897,4309903347,MPI GravPhys,"[""The scientific publication about the new @LIGO and @ego_virgo discoveries can be found on the #arXiv at <LINK>. Enjoy reading! In that paper you'll find tables and figures detailing properties of all signals and objects: <LINK>""]",https://arxiv.org/abs/1811.12907,"We present the results from three gravitational-wave searches for coalescing compact binaries with component masses above 1$\mathrm{M}_\odot$ during the first and second observing runs of the Advanced gravitational-wave detector network. During the first observing run (O1), from September $12^\mathrm{th}$, 2015 to January $19^\mathrm{th}$, 2016, gravitational waves from three binary black hole mergers were detected. The second observing run (O2), which ran from November $30^\mathrm{th}$, 2016 to August $25^\mathrm{th}$, 2017, saw the first detection of gravitational waves from a binary neutron star inspiral, in addition to the observation of gravitational waves from a total of seven binary black hole mergers, four of which we report here for the first time: GW170729, GW170809, GW170818 and GW170823. For all significant gravitational-wave events, we provide estimates of the source properties. The detected binary black holes have total masses between $18.6_{-0.7}^{+3.2}\mathrm{M}_\odot$, and $84.4_{-11.1}^{+15.8} \mathrm{M}_\odot$, and range in distance between $320_{-110}^{+120}$ Mpc and $2840_{-1360}^{+1400}$ Mpc. No neutron star - black hole mergers were detected. In addition to highly significant gravitational-wave events, we also provide a list of marginal event candidates with an estimated false alarm rate less than 1 per 30 days. From these results over the first two observing runs, which include approximately one gravitational-wave detection per 15 days of data searched, we infer merger rates at the 90% confidence intervals of $110\, -\, 3840$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$ for binary neutron stars and $9.7\, -\, 101$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$ for binary black holes assuming fixed population distributions, and determine a neutron star - black hole merger rate 90% upper limit of $610$ $\mathrm{Gpc}^{-3}\,\mathrm{y}^{-1}$. ","GWTC-1: A Gravitational-Wave Transient Catalog of Compact Binary Mergers
  Observed by LIGO and Virgo during the First and Second Observing Runs"
8,1069538650638176256,990049407633518592,Philipp Preiss,['New paper on the arxiv on quantum optics with Fermions <LINK>'],https://arxiv.org/abs/1811.12939,"Many-body interference between indistinguishable particles can give rise to strong correlations rooted in quantum statistics. We study such Hanbury Brown-Twiss-type correlations for number states of ultracold massive fermions. Using deterministically prepared $^6$Li atoms in optical tweezers, we measure momentum correlations using a single-atom sensitive time-of-flight imaging scheme. The experiment combines on-demand state preparation of highly indistinguishable particles with high-fidelity detection, giving access to two- and three-body correlations in fields of fixed fermionic particle number. We find that pairs of atoms interfere with a contrast close to 80%. We show that second-order density correlations arise from contributions from all two-particle pairs and detect intrinsic third-order correlations. ",High-Contrast Interference of Ultracold Fermions
9,1069481776173715456,2872132263,Emmanuel Flurin,['New paper (finally:)) on the arxiv! We use deep neural network methods to infer individual quantum trajectories from scratch. It also turns out to be quite effective as a qubit calibration method. Check it out!\n<LINK>'],https://arxiv.org/abs/1811.12420,"At its core, Quantum Mechanics is a theory developed to describe fundamental observations in the spectroscopy of solids and gases. Despite these practical roots, however, quantum theory is infamous for being highly counterintuitive, largely due to its intrinsically probabilistic nature. Neural networks have recently emerged as a powerful tool that can extract non-trivial correlations in vast datasets. They routinely outperform state-of-the-art techniques in language translation, medical diagnosis and image recognition. It remains to be seen if neural networks can be trained to predict stochastic quantum evolution without a priori specifying the rules of quantum theory. Here, we demonstrate that a recurrent neural network can be trained in real time to infer the individual quantum trajectories associated with the evolution of a superconducting qubit under unitary evolution, decoherence and continuous measurement from raw observations only. The network extracts the system Hamiltonian, measurement operators and physical parameters. It is also able to perform tomography of an unknown initial state without any prior calibration. This method has potential to greatly simplify and enhance tasks in quantum systems such as noise characterization, parameter estimation, feedback and optimization of quantum control. ","Using a Recurrent Neural Network to Reconstruct Quantum Dynamics of a
  Superconducting Qubit from Physical Observations"
10,1069257342968086528,17373048,Rodrigo Nemmen,['Paper out by our group: what do globular clusters in Our Galaxy look like in gamma-rays? Analysis of @NASAFermi observations of 157 GCs. New GCs found. Large encounter rates seem necessary for getting ms pulsars. Work led by grad student Raniere Menezes <LINK>'],https://arxiv.org/abs/1811.06957,"Globular clusters (GCs) are evolved stellar systems containing entire populations of millisecond pulsars (MSPs), which are efficient gamma-ray emitters. Observations of this emission can be used as a powerful tool to explore the dynamical processes leading to binary system formation in GCs. In this work, 9 years of Fermi Large Area Telescope data were used to investigate the gamma-ray emission from all GCs in the Milky Way. 23 clusters were found as gamma-ray bright, with 2 of them never having been reported before. It was also found that magnetic braking probably has a smaller impact on the formation rate of binary systems in metal-rich GCs than previously suggested, while a large value for the two-body encounter rate seems to be a necessary condition. The influence of the encounter rate per formed binary was for the first time explored in conjunction with gamma-ray data, giving evidence that if this quantity is very high, binary systems will get destroyed before having time to evolve into MSPs, thus decreasing the total number of MSPs in a GC. No extended emission was found even for clusters whose optical extent is ~0.5 degrees; all of them are point-like sources spatially in agreement with the optical cores of the GCs, supporting previous X-rays results of heavier objects sinking into the clusters' cores via dynamical friction. The possibility of extrapolating these results to ultra-compact dwarf galaxies is discussed, as these systems are believed to be the intermediate case between GCs and dwarf galaxies. ","Milky Way globular clusters in gamma-rays: analyzing the dynamical
  formation of millisecond pulsars"
11,1068569796042645505,60893773,James Bullock,['New paper led by @Alex_Fitts w/ @MBKplus  - Disentangling Baryons and Dark Matter Physics\n\nOne conclusion: The primary way to differentiate between DM models in low-M⋆ galaxies is through rotation curves at r &lt; r1/2 for small dwarfs.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1811.11791,"We present a suite of FIRE-2 cosmological zoom-in simulations of isolated field dwarf galaxies, all with masses of $M_\mathrm{halo} \approx 10^{10}\,$M$_\odot$ at $z=0$, across a range of dark matter models. For the first time, we compare how both self-interacting dark matter (SIDM) and/or warm dark matter (WDM) models affect the assembly histories as well as the central density structure in fully hydrodynamical simulations of dwarfs. Dwarfs with smaller stellar half-mass radii (r$_{1/2}<500$ pc) have lower $\sigma_\star/V_\mathrm{max}$ ratios, reinforcing the idea that smaller dwarfs may reside in halos that are more massive than is naively expected. The majority of dwarfs simulated with self-interactions actually experience contraction of their inner density profiles with the addition of baryons relative to the cores produced in dark-matter-only runs, though the simulated dwarfs are always less centrally dense than in $\Lambda$CDM. The V$_{1/2}-$r$_{1/2}$ relation across all simulations is generally consistent with observations of Local Field dwarfs, though compact objects such as Tucana provide a unique challenge. Spatially-resolved rotation curves in the central regions ($<400$ pc) of small dwarfs could provide a way to distinguish between CDM, WDM, and SIDM, however: at the masses probed in this simulation suite, cored density profiles in dwarfs with small r$_{1/2}$ values can only originate from dark matter self-interactions. ","Dwarf Galaxies in CDM, WDM, and SIDM: Disentangling Baryons and Dark
  Matter Physics"
12,1068355080196304896,1047607005140082688,Daniel Thorngren,"['New paper with @jjfplanet!  Basically, structure modeled bulk metallicity puts upper limits on atmospheric metallicity and comparing the two tells you how well mixed the planet is.  <LINK>', 'Oh, and so we modeled and placed upper limits on 365 giant planets.  The catalog is available for download with the paper on Arxiv.']",https://arxiv.org/abs/1811.11859,"Atmospheric characterization through spectroscopic analysis, an essential tool of modern exoplanet science, can benefit significantly from the context provided by interior structure models. In particular, the planet's bulk metallicity $Z_p$ places an upper limit on potential atmospheric metallicity. Here we construct interior structure models to derive $Z_p$ and atmospheric metallicity upper limits for 403 known transiting giant exoplanets. These limits are low enough that they can usefully inform atmosphere models. Additionally, we argue that comparing $Z_p$ to the observed atmospheric metallicity gives a useful measure of how well-mixed metals are within the planet. This represents a new avenue for learning about planetary interiors. To aid in the future characterization of new planet discoveries we derive analytic prior predictions of atmosphere metallicity as a function of planet mass, and evaluate the effectiveness of our approach on Jupiter and Saturn. We we include log-linear fits for approximating the metallicities of planets not in our catalog. ","Connecting Giant Planet Atmosphere and Interior Modeling: Constraints on
  Atmospheric Metal Enrichment"
13,1068331672062619648,76331020,Caius Selhorst,['Our new paper analysing the polar brightening observed in the  ALMA solar maps has been accepted for publication in ApJ!! <LINK>\n(with @pjsimoes and @guiguesp)'],http://arxiv.org/abs/1811.12158,"Polar brightening of the Sun at radio frequencies has been studied for almost fifty years and yet a disagreement persists between solar atmospheric models and observations. Some observations reported brightening values much smaller than the expected values obtained from the models, with discrepancies being particularly large at millimeter wavelengths. New clues to calibrate the atmospheric models can be obtained with the advent of the Atacama Large Millimeter/submillimeter Array (ALMA) radio interferometer. In this work, we analyzed the lower limit of the polar brightening observed at 100 and 230 GHz by ALMA, during its Science Verification period, 2015 December 16-20. We find that the average polar intensity is higher than the disk intensity at 100 and 230 GHz, with larger brightness intensities at the South pole in eight of the nine maps analyzed. The observational results were compared with calculations of the millimetric limb brightnening emission for two semi-empirical atmospheric models, FAL- C (Fontenla et al. 1993) and SSC (Selhorst et al. 2005a). Both models presented larger limb intensities than the averaged observed values. The intensities obtained with the SSC model were closer to the observations, with polar brightenings of 10.5% and 17.8% at 100 and 230 GHz, respectively. This discrepancy may be due to the presence of chromospheric features (like spicules) at regions close to the limb. ",Solar polar brightening and radius at 100 and 230 GHz observed by ALMA
14,1068320387296100352,626066785,Ken Carpenter,"['The preprint of our new paper on the HST/STIS ASTRAL ultraviolet spectra of the evolved M-stars Alpha Ori and Gamma Cru has gone ""live"" on arXiv.  It has been accepted by the Astrophysical Journal and is expected to be published shortly.  <LINK> <LINK>']",https://arxiv.org/abs/1811.11865,"The HST Treasury Program ""Advanced Spectral Library Project: Cool Stars"" was designed to collect representative, high quality ultraviolet spectra of eight evolved F-M type cool stars. The Space Telescope Imaging Spectrograph (STIS) echelle spectra of these objects enable investigations of a broad range of topics including stellar and interstellar astrophysics. This paper provides a guide to the spectra of the two evolved M-stars, the M2Iab supergiant Alpha Ori and the M3.4 giant Gamma Cru, with comparisons to the prototypical K1.5 giant Alpha Boo. It includes identifications of the significant atomic and molecular emission and absorption features and discusses the character of the photospheric and chromospheric continua and line spectra. The fluorescent processes responsible for a large portion of the emission line spectrum, the characteristics of the stellar winds, and the available diagnostics for hot and cool plasmas are also summarized. This analysis will facilitate the future study of the spectra, outer atmospheres, and winds, not only of these objects, but for numerous other cool, low-gravity stars for years to come. ","The Advanced Spectral Library (ASTRAL): Reference Spectra for Evolved
  M-Stars"
15,1068230164079890432,1002523074,Daniele Pinna,"[""Our new paper on a general theory for rotationally symmetric breathing dynamics in magnetic (anti-) skyrmions is out for review. Read it while it's hot and send your comments over. 😉\n\n<LINK>""]",https://arxiv.org/abs/1811.09949,"We derive an effective Hamiltonian system describing the low energy dynamics of circular magnetic skyrmions and antiskyrmions. Using scaling and symmetry arguments we model (anti-)skyrmion dynamics through a finite set of coupled, canonically conjugated, collective coordinates. The resulting theoretical description is independent of both micromagnetic details as well as any specificity in the ansatz of the skyrmion profile. Based on the Hamiltonian structure we derive a general description for breathing dynamics of (anti-)skyrmions in the limit of radius much larger than the domain wall width. The effective energy landscape reveals two qualitatively different types of breathing behavior. For small energy perturbations we reproduce the well-known small breathing mode excitations, where the magnetic moments of the skyrmion oscillate around their equilibrium solution. At higher energies we find a breathing behavior where the in-plane angle of the skyrmion continuously precesses, transforming N\'eel to Bloch skyrmions and vice versa. For a damped system we observe the transition from the continuously rotating and breathing skyrmion into the oscillatory one. We analyze the characteristic frequencies of both breathing types, as well as their amplitudes and distinct energy dissipation rates. For rotational (oscillatory) breathing modes we predict on average a linear (exponential) decay in energy. We argue that this stark difference in dissipative behavior should be observable in the frequency spectrum of excited (anti-)skyrmions. ","Characterizing breathing dynamics of magnetic (anti-)skyrmions within
  the Hamiltonian formalism"
16,1068190997836963842,2306462901,Alan Nichol,"['Excited to share our new paper ""Few-shot Generalization Across Dialogue Tasks"" <LINK> - will present this at #NeurIPS conversational AI next week! \n\nNew models already available in Rasa Core 0.11+!\nI wrote a blog post about why it matters <LINK>']",https://arxiv.org/abs/1811.11707,"Machine-learning based dialogue managers are able to learn complex behaviors in order to complete a task, but it is not straightforward to extend their capabilities to new domains. We investigate different policies' ability to handle uncooperative user behavior, and how well expertise in completing one task (such as restaurant reservations) can be reapplied when learning a new one (e.g. booking a hotel). We introduce the Recurrent Embedding Dialogue Policy (REDP), which embeds system actions and dialogue states in the same vector space. REDP contains a memory component and attention mechanism based on a modified Neural Turing Machine, and significantly outperforms a baseline LSTM classifier on this task. We also show that both our architecture and baseline solve the bAbI dialogue task, achieving 100% test accuracy. ",Few-Shot Generalization Across Dialogue Tasks
17,1068138508580265984,15098559,Ian Foster,['New paper describes DLHub: Model and Data Serving for Science <LINK>'],https://arxiv.org/abs/1811.11213,"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its selfservice model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications. ",DLHub: Model and Data Serving for Science
18,1068104716805120001,4723994296,Tadhg Garton,"['My favorite scientist just got a new paper on flux tube expansion of coronal hole regions accepted for publication in ApJL. With conclusions so wild, you might just call him the bad boy of solar physics. \n\nFree access to the paper can be found at: <LINK>']",https://arxiv.org/abs/1811.11605,"Coronal holes (CHs) are regions of open magnetic flux which are the source of high speed solar wind (HSSW) streams. To date, it is not clear which aspects of CHs are of most influence on the properties of the solar wind as it expands through the Heliosphere. Here, we study the relationship between CH properties extracted from AIA (Atmospheric Imaging Assembly) images using CHIMERA (Coronal Hole Identification via Multi-thermal Emission Recognition Algorithm) and HSSW measurements from ACE (Advanced Composition Explorer) at L1. For CH longitudinal widths $\Delta\theta_{CH}<$67$^{\circ}$, the peak SW velocity ($v_{max}$) is found to scale as $v_{max}~\approx~330.8~+~5.7~\Delta\theta_{CH}$~km~s$^{-1}$. For larger longitudinal widths ($\Delta\theta_{CH}>$67$^{\circ}$), $v_{max}$ is found to tend to a constant value ($\sim$710~km~s$^{-1}$). Furthermore, we find that the duration of HSSW streams ($\Delta t$) are directly related to the longitudinal width of CHs ($\Delta t_{SW}$~$\approx$~0.09$\Delta\theta_{CH}$) and that their longitudinal expansion factor is $f_{SW}~\approx 1.2~\pm 0.1$. We also derive an expression for the coronal hole flux-tube expansion factor, $f_{FT}$, which varies as $f_{SW} \gtrsim f_{FT} \gtrsim 0.8$. These results enable us to estimate the peak speeds and durations of HSSW streams at L1 using the properties of CHs identified in the solar corona. ","Expansion of High Speed Solar Wind Streams from Coronal Holes through
  the Inner Heliosphere"
19,1067918096046927872,7352832,Theo Weber,"[""New paper on using counterfactual inference for RL! <LINK>. In contrast with ‘classical’ model-based RL questions of type 'what would happen if', counterfactual reasoning's 'what would have happened if' simulates low-bias alternative outcomes of real experience.""]",https://arxiv.org/abs/1811.06272,"Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods. ","Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search"
20,1067599877813735425,2728209696,Xiao Ma,"['We have a new paper on #trust! - Understanding Image Quality and Trust in Peer-to-Peer Marketplaces\nOur models predict image quality in online #marketplaces, and we show that high-quality user-generated images outperform stock imagery for generating trust.\n<LINK> <LINK>', 'In collaboration with Lina Mezghani @Polytechnique, Kimberly Wilber @GoogleAI, Hui Hong and Robinson Piramuthu @eBay, and @informor and @SergeBelongie @cornell_tech @CornellInfoSci. The paper will be presented at #WACV19 https://t.co/vJaVQP8WMk. See you in Hawaii! &lt;3']",https://arxiv.org/abs/1811.10648,"As any savvy online shopper knows, second-hand peer-to-peer marketplaces are filled with images of mixed quality. How does image quality impact marketplace outcomes, and can quality be automatically predicted? In this work, we conducted a large-scale study on the quality of user-generated images in peer-to-peer marketplaces. By gathering a dataset of common second-hand products (~75,000 images) and annotating a subset with human-labeled quality judgments, we were able to model and predict image quality with decent accuracy (~87%). We then conducted two studies focused on understanding the relationship between these image quality scores and two marketplace outcomes: sales and perceived trustworthiness. We show that image quality is associated with higher likelihood that an item will be sold, though other factors such as view count were better predictors of sales. Nonetheless, we show that high quality user-generated images selected by our models outperform stock imagery in eliciting perceptions of trust from users. Our findings can inform the design of future marketplaces and guide potential sellers to take better product images. ",Understanding Image Quality and Trust in Peer-to-Peer Marketplaces
21,1067592690617933825,97707247,Gautam Kamath,"['New paper on the testing simple hypotheses with differential privacy. The sample complexity surprised me! With @ceacy_, A. McMillan, A. Smith, and J. Ullman. #privacy #clickbait <LINK>']",https://arxiv.org/abs/1811.11148,"Hypothesis testing plays a central role in statistical inference, and is used in many settings where privacy concerns are paramount. This work answers a basic question about privately testing simple hypotheses: given two distributions $P$ and $Q$, and a privacy level $\varepsilon$, how many i.i.d. samples are needed to distinguish $P$ from $Q$ subject to $\varepsilon$-differential privacy, and what sort of tests have optimal sample complexity? Specifically, we characterize this sample complexity up to constant factors in terms of the structure of $P$ and $Q$ and the privacy level $\varepsilon$, and show that this sample complexity is achieved by a certain randomized and clamped variant of the log-likelihood ratio test. Our result is an analogue of the classical Neyman-Pearson lemma in the setting of private hypothesis testing. We also give an application of our result to the private change-point detection. Our characterization applies more generally to hypothesis tests satisfying essentially any notion of algorithmic stability, which is known to imply strong generalization bounds in adaptive data analysis, and thus our results have applications even when privacy is not a primary concern. ",The Structure of Optimal Private Tests for Simple Hypotheses
22,1067438141957595136,976155561522794497,Juliano César Silva Neves,"['My new paper ""Proposal for a degree of scientificity in cosmology"" introduces a ""degree of scientificity"" using fuzzy sets. Then the big bang is not conceived of as a scientific issue and bouncing cosmologies are potentially better scientific options.\n<LINK>']",https://arxiv.org/abs/1811.09758?fbclid=IwAR12e4tr_9drQ6gTeFrcre3bINEc7AUtdOcG8ZFY0nZ_Eqw7GDcdlYA6ZVc,"In spite of successful tests, the standard cosmological model, the $\Lambda$CDM model, possesses the most problematic concept: the initial singularity, also known as the big bang. In this paper---by adopting the Kantian difference between to think of an object and to cognize an object---it is proposed a degree of scientificity using fuzzy sets. Thus, the notion of initial singularity will not be conceived of as a scientific issue because it does not belong to the fuzzy set of what is known. Indeed, the problematic concept of singularity is some sort of what Kant called the noumenon, but science, on the other hand, is constructed in the phenomenon. By applying the fuzzy degree of scientificity in cosmological models, one concludes that cosmologies with a contraction phase before the current expansion phase are potentially more scientific than the standard model. At the end of this article, it is shown that Kant's first antinomy of pure reason indicates a limit to our cosmological models. ",Proposal for a degree of scientificity in cosmology
23,1067400953740050432,1005048457,James Stovold,"['Exciting day in the office @CompFoundry -- new Kilobots to play with, and pre-print of my journal paper available on @arxiv <LINK>', '@SwarmDynamics @CompFoundry @arxiv ...like dress them up as sheep with cotton wool and googley eyes so @SHOALgroup can shepherd them?', '@SwarmDynamics @CompFoundry @arxiv @SHOALgroup Once it works!']",https://arxiv.org/abs/1811.10033,"Autonomous robots require the ability to balance conflicting needs, such as whether to charge a battery rather than complete a task. Nature has evolved a mechanism for achieving this in the form of homeostasis. This paper presents CogSis, a cognition-inspired architecture for artificial homeostasis. CogSis provides a robot with the ability to balance conflicting needs so that it can maintain its internal state, while still completing its tasks. Through the use of an associative memory neural network, a robot running CogSis is able to learn about its environment rapidly by making associations between sensors. Results show that a Pi-Swarm robot running CogSis can balance charging its battery with completing a task, and can balance conflicting needs, such as charging its battery without overheating. The lab setup consists of a charging station and high-temperature region, demarcated with coloured lamps. The robot associates the colour of a lamp with the effect it has on the robot's internal environment (for example, charging the battery). The robot can then seek out that colour again when it runs low on charge. This work is the first control architecture that takes inspiration directly from distributed cognition. The result is an architecture that is able to learn and apply environmental knowledge rapidly, implementing homeostatic behaviour and balancing conflicting decisions. ","Cognitively-inspired homeostatic architecture can balance conflicting
  needs in robots"
24,1067335056203673600,822867138,Bradley Kavanagh,"['Digging for Dark Matter!\n\nIs it possible to discover #DarkMatter by the tracks that it leaves behind in ancient minerals?\n\nOur new paper on #PaleoDetectors is now online - <LINK> - with all the code for reproducing the calculations here: <LINK> <LINK>', ""Dark Matter should leave behind tracks in billion year old rocks, but so should neutrinos and radioactive contaminants. \n\nHow well do you need to know your backgrounds to exclude/discover #DarkMatter? \n\nIf you can measure nanometre tracks, you don't need a very precise knowledge! https://t.co/HTcvD4U9vl"", 'And if you did discover a #DarkMatter signal, what could you find out about the mass of this new particle?\n\nThe huge number of events and the wide range of track lengths (i.e. recoil energies) means you can constrain it very precisely, up to masses of 1 TeV! https://t.co/D9e8K1zRe4', 'Our code is here - https://t.co/lJNaNSzoyj (and archived with @ZENODO_ORG) - so the paper should be (almost 100%) #reproducible.\n\nYou can even play with some of it directly in your browser with @mybinderteam: https://t.co/msUWO70Ob4\n\n#OpenScience #Reproducibility', 'A lot of hard work characterising radioactive backgrounds &amp; possible readout methods was done by our collaborators: https://t.co/5dQw1BKcC9, https://t.co/pbHvfdhcys.\n\nNext step: get some mineral samples and start firing neutrons at them to check how the tracks *really* look... https://t.co/AuMorOEYDJ']",https://arxiv.org/abs/1811.10549,"Paleo-detectors are a recently proposed method for the direct detection of Dark Matter (DM). In such detectors, one would search for the persistent damage features left by DM--nucleus interactions in ancient minerals. Initial sensitivity projections have shown that paleo-detectors could probe much of the remaining Weakly Interacting Massive Particle (WIMP) parameter space. In this paper, we improve upon the cut-and-count approach previously used to estimate the sensitivity by performing a full spectral analysis of the background- and DM-induced signal spectra. We consider two scenarios for the systematic errors on the background spectra: i) systematic errors on the normalization only, and ii) systematic errors on the shape of the backgrounds. We find that the projected sensitivity is rather robust to imperfect knowledge of the backgrounds. Finally, we study how well the parameters of the true WIMP model could be reconstructed in the hypothetical case of a WIMP discovery. ","Digging for Dark Matter: Spectral Analysis and Discovery Potential of
  Paleo-Detectors"
25,1067323002935803904,2796003728,Dan Levi,['Check out our new paper on 3D lane detection. Introducing also for the first time an end-to-end lane detection network:\n<LINK> <LINK>'],https://arxiv.org/abs/1811.10203,"We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art. ",3D-LaneNet: End-to-End 3D Multiple Lane Detection
26,1067217786865217537,1065367665278017537,Aaron Walsman,"['We have a new paper showing the performance benefits of early fusion of goal/target information in visual episodic tasks.  Cut down on training time, parameter counts and carbon emissions.  <LINK> <LINK>', 'Joint work with @ybisk, Saadia Gabriel, Dipendra Misra, @yoavartzi, @yejinchoinka and Dieter Fox.', 'Better figure without transparency... https://t.co/BoQD1Yi1rL']",https://arxiv.org/abs/1811.08824,"Building perceptual systems for robotics which perform well under tight computational budgets requires novel architectures which rethink the traditional computer vision pipeline. Modern vision architectures require the agent to build a summary representation of the entire scene, even if most of the input is irrelevant to the agent's current goal. In this work, we flip this paradigm, by introducing EarlyFusion vision models that condition on a goal to build custom representations for downstream tasks. We show that these goal specific representations can be learned more quickly, are substantially more parameter efficient, and more robust than existing attention mechanisms in our domain. We demonstrate the effectiveness of these methods on a simulated robotic item retrieval problem that is trained in a fully end-to-end manner via imitation learning. ",Early Fusion for Goal Directed Robotic Vision
27,1067153892025208842,24608543,Gideon Mann,"['new on arxiv: A new paper on software fuzzing, using a machine learning model to predict likely execution traces and more efficiently pick seeds: <LINK>. A little bit of background: <LINK> joint work with @siddkaramcheti and @drosen']",https://arxiv.org/abs/1811.08973,"Grey-box fuzzers such as American Fuzzy Lop (AFL) are popular tools for finding bugs and potential vulnerabilities in programs. While these fuzzers have been able to find vulnerabilities in many widely used programs, they are not efficient; of the millions of inputs executed by AFL in a typical fuzzing run, only a handful discover unseen behavior or trigger a crash. The remaining inputs are redundant, exhibiting behavior that has already been observed. Here, we present an approach to increase the efficiency of fuzzers like AFL by applying machine learning to directly model how programs behave. We learn a forward prediction model that maps program inputs to execution traces, training on the thousands of inputs collected during standard fuzzing. This learned model guides exploration by focusing on fuzzing inputs on which our model is the most uncertain (measured via the entropy of the predicted execution trace distribution). By focusing on executing inputs our learned model is unsure about, and ignoring any input whose behavior our model is certain about, we show that we can significantly limit wasteful execution. Through testing our approach on a set of binaries released as part of the DARPA Cyber Grand Challenge, we show that our approach is able to find a set of inputs that result in more code coverage and discovered crashes than baseline fuzzers with significantly fewer executions. ",Improving Grey-Box Fuzzing by Modeling Program Behavior
28,1067120096072556544,990433714948661250,Sergey Levine,"['How can agents improve their policies without rewards? Maybe we can tell them how to improve using language.\n\nOur new paper on meta-learning to learn how to acquire new skills with language feedback:\n<LINK>\n<LINK>\n\nw/ JD Co-Reyes, A. Gupta, et al']",https://arxiv.org/abs/1811.07882,"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following. ",Guiding Policies with Language via Meta-Learning
29,1067044285583945728,762420558,Ciaran O'Hare,"['New paper out today on using the new X-ray detectors of #IAXO to measure the properties of Solar axions - not possible in previous helioscopes. <LINK> <LINK>', 'This was a quite a self-contained bit of work so I thought it would be ideal for my first proper foray into #openscience and #reproducibility . See the github page for some @ProjectJupyter notebooks to reproduce the paper. https://t.co/QHQky0aQLl', 'Also, in another peculiar instance of ""arxiv bus-bunching"" a very similar (but complementary!) paper on the same idea also came out today https://t.co/YuHxhO2lUz']",https://arxiv.org/abs/1811.09290,"Axion helioscopes search for solar axions and axion-like particles via inverse Primakoff conversion in strong laboratory magnets pointed at the Sun. While helioscopes can always measure the axion coupling to photons, the conversion signal is independent of the mass for axions lighter than around 0.02 eV. Masses above this value on the other hand have suppressed signals due to axion-photon oscillations which destroy the coherence of the conversion along the magnet. However, the spectral oscillations present in the axion conversion signal between these two regimes are highly dependent on the axion mass. We show that these oscillations are observable given realistic energy resolutions and can be used to determine the axion mass to within percent level accuracies. Using projections for the upcoming helioscope IAXO, we demonstrate that $>3\sigma$ sensitivity to a non-zero axion mass is possible between $3 \times 10^{-3}$ and $10^{-1}$ eV for both the Primakoff and axion-electron solar fluxes. ",Weighing the Solar Axion
30,1067018190604156928,3317230737,Marco_Faggian,"['New Paper! ""Synchronization in time-varying random networks with vanishing connectivity"" \n<LINK>', '@pClusella Thank you Pau ;-)']",https://arxiv.org/abs/1811.09591,"A sufficiently connected topology linking the constituent units of a complex system is usually seen as a prerequisite for the emergence of collective phenomena such as synchronization. We present a random network of heterogeneous phase oscillators in which the links mediating the interactions are constantly rearranged with a characteristic timescale and, possibly, an extremely low instantaneous connectivity. We show that, provided strong coupling and fast enough rewiring are considered, the network is able to reach partial synchronization even in the vanishing connectivity limit. We also provide an intuitive analytical argument, based on the comparison between the different characteristic timescales of our system in the low connectivity regime, which is able to predict the transition to synchronization threshold with satisfactory precision. In the formal fast switching limit, finally, we argue that the onset of collective synchronization is captured by the time-averaged connectivity network. Our results may be relevant to qualitatively describe the emergence of consensus in social communities with time-varying interactions and to study the onset of collective behavior in engineered systems of mobile units with limited wireless capabilities. ","Synchronization in time-varying random networks with vanishing
  connectivity"
31,1066935098442838016,1558538456,Rodrigo Fernández,['Can simulations of HMNS disk outflows match velocities inferred from kilonova fits to GW170817?\n\nNew paper led by #UAlberta graduate student Steven Fahlman \n\n<LINK> <LINK>'],https://arxiv.org/abs/1811.08906,"We study mass ejection from accretion disks around newly-formed hypermassive neutron stars (HMNS). Standard kilonova model fits to GW170817 require at least a lanthanide-poor ('blue') and lanthanide-rich ('red') component. The existence of a blue component has been used as evidence for a HMNS remnant of finite lifetime, but average disk outflow velocities from existing long-term HMNS simulations fall short of the inferred value ($\sim 0.25c$) by a factor of $\sim 2$. Here we use time-dependent, axisymmetric hydrodynamic simulations of HMNS disks to explore the limits of the model and its ability to account for observations. For physically plausible parameter choices compatible with GW170817, we find that hydrodynamic models that use shear viscosity to transport angular momentum cannot eject matter with mass-averaged velocities larger than $\sim 0.15c$. While outflow velocities in our simulations can exceed the asymptotic value for a steady-state neutrino-driven wind, the increase in the average velocity due to viscosity is not sufficient. Therefore, viscous HMNS disk winds cannot reproduce by themselves the ejecta properties inferred from multi-component fits to kilonova light curves from GW170817. Three possible resolutions remain feasible within standard merger ejecta channels: more sophisticated radiative transfer models that allow for photon reprocessing between ejecta components, inclusion of magnetic stresses, or enhancement of the dynamical ejecta. We provide fits to our disk outflow models once they reach homologous expansion. ",Hypermassive Neutron Star Disk Outflows and Blue Kilonovae
32,1065609019887374338,797888987675365377,Tom Rainforth,"['Statistical Verification of Neural Networks: a new approach to verification that provides an informative notion of how robust a network is, rather than just a binary SAT/UNSAT assertion.  New paper from @stefan_webb, @yeewhye, M. Pawan Kumar, and myself <LINK>']",https://arxiv.org/abs/1811.07209,"We present a new approach to assessing the robustness of neural networks based on estimating the proportion of inputs for which a property is violated. Specifically, we estimate the probability of the event that the property is violated under an input model. Our approach critically varies from the formal verification framework in that when the property can be violated, it provides an informative notion of how robust the network is, rather than just the conventional assertion that the network is not verifiable. Furthermore, it provides an ability to scale to larger networks than formal verification approaches. Though the framework still provides a formal guarantee of satisfiability whenever it successfully finds one or more violations, these advantages do come at the cost of only providing a statistical estimate of unsatisfiability whenever no violation is found. Key to the practical success of our approach is an adaptation of multi-level splitting, a Monte Carlo approach for estimating the probability of rare events, to our statistical robustness framework. We demonstrate that our approach is able to emulate formal verification procedures on benchmark problems, while scaling to larger networks and providing reliable additional information in the form of accurate estimates of the violation probability. ",A Statistical Approach to Assessing Neural Network Robustness
33,1065588633292472320,19238958,Daniel S. Katz,"['New paper ""Community Organizations: Changing the Culture in Which Research Software Is Developed and Sustained"" accepted by @cisemag, to appear in 2019, preprint: <LINK>']",https://arxiv.org/abs/1811.08473,"Software is the key crosscutting technology that enables advances in mathematics, computer science, and domain-specific science and engineering to achieve robust simulations and analysis for science, engineering, and other research fields. However, software itself has not traditionally received focused attention from research communities; rather, software has evolved organically and inconsistently, with its development largely as by-products of other initiatives. Moreover, challenges in scientific software are expanding due to disruptive changes in computer hardware, increasing scale and complexity of data, and demands for more complex simulations involving multiphysics, multiscale modeling and outer-loop analysis. In recent years, community members have established a range of grass-roots organizations and projects to address these growing technical and social challenges in software productivity, quality, reproducibility, and sustainability. This article provides an overview of such groups and discusses opportunities to leverage their synergistic activities while nurturing work toward emerging software ecosystems. ","Community Organizations: Changing the Culture in Which Research Software
  Is Developed and Sustained"
34,1065565350731730944,844700194197397506,Alex Kendall,['New paper on 3D object detection from monocular images (with no stereo or lidar)! Using geometry to reason in 3D we obtain state of the art for monocular methods on KITTI. Amazing work lead by Tom Roddick at Cambridge. <LINK> <LINK>'],https://arxiv.org/abs/1811.08188,"3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10\% of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark.\footnote{We will release full source code and pretrained models upon acceptance of this manuscript for publication. ",Orthographic Feature Transform for Monocular 3D Object Detection
35,1065429099487744000,3070996707,Ajay Tanwani,"['Our upcoming work presents various invariant formulations of a hidden semi-Markov model for recognition and synthesis of robot manipulation skills by imitation learning, allowing robots to quickly acquire a new skill in as few as 4 demonstrations\n\nPaper: <LINK>']",https://arxiv.org/abs/1811.07489,"Generalizing manipulation skills to new situations requires extracting invariant patterns from demonstrations. For example, the robot needs to understand the demonstrations at a higher level while being invariant to the appearance of the objects, geometric aspects of objects such as its position, size, orientation and viewpoint of the observer in the demonstrations. In this paper, we propose an algorithm that learns a joint probability density function of the demonstrations with invariant formulations of hidden semi-Markov models to extract invariant segments (also termed as sub-goals or options), and smoothly follow the generated sequence of states with a linear quadratic tracking controller. The algorithm takes as input the demonstrations with respect to different coordinate systems describing virtual landmarks or objects of interest with a task-parameterized formulation, and adapt the segments according to the environmental changes in a systematic manner. We present variants of this algorithm in latent space with low-rank covariance decompositions, semi-tied covariances, and non-parametric online estimation of model parameters under small variance asymptotics; yielding considerably low sample and model complexity for acquiring new manipulation skills. The algorithm allows a Baxter robot to learn a pick-and-place task while avoiding a movable obstacle based on only 4 kinesthetic demonstrations. ","Generalizing Robot Imitation Learning with Invariant Hidden Semi-Markov
  Models"
36,1065146276759654400,929973145,Dr David Sobral 💫🌌,"['New LEGA-C paper! Find out all about how 1D Kinematics from stars and ionized gas compare at z∼0.8 from the LEGA-C VLT spectroscopic survey of massive galaxies [<LINK>] Bezanson, van der Wel, Straatman et al. <LINK>']",https://arxiv.org/abs/1811.07900,"We present a comparison of the observed, spatially integrated stellar and ionized gas velocity dispersions of $\sim1000$ massive ($\log M_{\star}/M_{\odot}\gtrsim\,10.3$) galaxies in the Large Early Galaxy Astrophysics Census (LEGA-C) survey at $0.6\lesssim\,z\lesssim1.0$. The high $S/N\sim20{\rm\AA^{-1}}$ afforded by 20 hour VLT/VIMOS spectra allows for joint modeling of the stellar continuum and emission lines in all galaxies, spanning the full range of galaxy colors and morphologies. These observed integrated velocity dispersions (denoted as $\sigma'_{g, int}$ and $\sigma'_{\star, int}$) are related to the intrinsic velocity dispersions of ionized gas or stars, but also include rotational motions through beam smearing and spectral extraction. We find good average agreement between observed velocity dispersions, with $\langle\log(\sigma'_{g, int}/\sigma'_{\star, int})\rangle=-0.003$. This result does not depend strongly on stellar population, structural properties, or alignment with respect to the slit. However, in all regimes we find significant scatter between $\sigma'_{g, int}$ and $\sigma'_{\star, int}$, with an overall scatter of 0.13 dex of which 0.05 dex is due to observational uncertainties. For an individual galaxy, the scatter between $\sigma'_{g, int}$ and $\sigma'_{\star, int}$ translates to an additional uncertainty of $\sim0.24\rm{dex}$ on dynamical mass derived from $\sigma'_{g, int}$, on top of measurement errors and uncertainties from Virial constant or size estimates. We measure the $z\sim0.8$ stellar mass Faber-Jackson relation and demonstrate that emission line widths can be used to measure scaling relations. However, these relations will exhibit increased scatter and slopes that are artificially steepened by selecting on subsets of galaxies with progressively brighter emission lines. ","1D Kinematics from stars and ionized gas at $z\sim0.8$ from the LEGA-C
  spectroscopic survey of massive galaxies"
37,1065135745344708610,1710697381,Diego F. Torres,"['New paper (ApJ Letters) today (1/n): Theoretically motivated search and detection of non-thermal pulsations from PSRs J1747-2958, J2021+3651, and J1826-1256  <LINK> <LINK>']",https://arxiv.org/abs/1811.08339,"Based on a theoretical selection of pulsars as candidates for detection at X-ray energies, we present an analysis of archival X-ray observations done with Chandra and XMM-Newton of PSR J1747-2958 (the pulsar in the ""Mouse"" nebula), PSR J2021+3651 (the pulsar in the ""Dragonfly"" nebula), and PSR J1826-1256. X-ray pulsations from PSR J1747-2958 and PSR J1826-1256 are detected for the first time, and a previously reported hint of an X-ray pulsation from PSR J2021+3651 is confirmed with a higher significance. We analyze these pulsars' spectra in regards to the theoretically predicted energy distribution, finding a remarkable agreement, and provide here a refined calculation of the model parameters taking into account the newly derived X-ray spectral data. ","Theoretically motivated search and detection of non-thermal pulsations
  from PSRs J1747-2958, J2021+3651, and J1826-1256"
38,1064990372739325954,1708262810,shira mitchell,"['new paper with eric potash and @s010n: <LINK> <LINK>', 'we hope the paper can guide thinking about goals, sampling, measurement, causality, and conditional probability (@stat110 !) for prediction-based decisions', 'especially for high-impact decisions, where carelessness tends to harm already-disadvantaged groups', 'there is no acknowledgment section but the 200+ cited authors deserve a giant *thank you* for putting up with my questions', 'control-F for your name, tell us about mistakes, suggest additions', '@jim_savage_ @s010n I do! he is!']",https://arxiv.org/abs/1811.07867,"A recent flurry of research activity has attempted to quantitatively define ""fairness"" for decisions based on statistical and machine learning (ML) predictions. The rapid growth of this new field has led to wildly inconsistent terminology and notation, presenting a serious challenge for cataloguing and comparing definitions. This paper attempts to bring much-needed order. First, we explicate the various choices and assumptions made---often implicitly---to justify the use of prediction-based decisions. Next, we show how such choices and assumptions can raise concerns about fairness and we present a notationally consistent catalogue of fairness definitions from the ML literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision systems. ","Prediction-Based Decisions and Fairness: A Catalogue of Choices,
  Assumptions, and Definitions"
39,1064934601511092224,2818695390,Sasho Nikolov,"['I am not used to the Twitter game so I am late with this: in a new paper with Jarek Blasiok, Mark Bun, and Thomas Steinke, we show that simple algorithms can answer any set of counting queries with 1% error and sample complexity optimal up to constants. <LINK>', 'Also my talk on this at @BIRS_Math https://t.co/7IOrIduULB.', '@hoonoseme We are! Simple *differentially private* algorithms! Character limit must’ve stressed me out.']",https://arxiv.org/abs/1811.03763,"We study efficient mechanisms for the query release problem in differential privacy: given a workload of $m$ statistical queries, output approximate answers to the queries while satisfying the constraints of differential privacy. In particular, we are interested in mechanisms that optimally adapt to the given workload. Building on the projection mechanism of Nikolov, Talwar, and Zhang, and using the ideas behind Dudley's chaining inequality, we propose new efficient algorithms for the query release problem, and prove that they achieve optimal sample complexity for the given workload (up to constant factors, in certain parameter regimes) with respect to the class of mechanisms that satisfy concentrated differential privacy. We also give variants of our algorithms that satisfy local differential privacy, and prove that they also achieve optimal sample complexity among all local sequentially interactive private mechanisms. ",Towards Instance-Optimal Private Query Release
40,1064862803679432704,5850692,Aaron Roth,"['Excited about a new paper with @datasciwell and @zstevenwu : ""How to Use Heuristics For Differential Privacy"", here: <LINK> We give oracle efficient algorithms for  private learning and synthetic data generation, and leave a bunch of interesting open problems.']",https://arxiv.org/abs/1811.07765,"We develop theory for using heuristics to solve computationally hard problems in differential privacy. Heuristic approaches have enjoyed tremendous success in machine learning, for which performance can be empirically evaluated. However, privacy guarantees cannot be evaluated empirically, and must be proven --- without making heuristic assumptions. We show that learning problems over broad classes of functions can be solved privately and efficiently, assuming the existence of a non-private oracle for solving the same problem. Our first algorithm yields a privacy guarantee that is contingent on the correctness of the oracle. We then give a reduction which applies to a class of heuristics which we call certifiable, which allows us to convert oracle-dependent privacy guarantees to worst-case privacy guarantee that hold even when the heuristic standing in for the oracle might fail in adversarial ways. Finally, we consider a broad class of functions that includes most classes of simple boolean functions studied in the PAC learning literature, including conjunctions, disjunctions, parities, and discrete halfspaces. We show that there is an efficient algorithm for privately constructing synthetic data for any such class, given a non-private learning oracle. This in particular gives the first oracle-efficient algorithm for privately generating synthetic data for contingency tables. The most intriguing question left open by our work is whether or not every problem that can be solved differentially privately can be privately solved with an oracle-efficient algorithm. While we do not resolve this, we give a barrier result that suggests that any generic oracle-efficient reduction must fall outside of a natural class of algorithms (which includes the algorithms given in this paper). ",How to Use Heuristics for Differential Privacy
41,1064844420040806401,611769076,Tim Lichtenberg,"[""Our new paper was released today: Bonati et al., 'Direct imaging of molten protoplanets in nearby young stellar associations', <LINK>. Read a plain-language summary at <LINK>. #magmaocean @ELT_METIS @ELSI_res @PR_ELSI @ETH_ERDW @ETH_en @UZH_en <LINK>""]",https://arxiv.org/abs/1811.07411,"During their formation and early evolution, rocky planets undergo multiple global melting events due to accretionary collisions with other protoplanets. The detection and characterization of their post-collision afterglows (magma oceans) can yield important clues about the origin and evolution of the solar and extrasolar planet population. Here, we quantitatively assess the observational prospects to detect the radiative signature of forming planets covered by such collision-induced magma oceans in nearby young stellar associations with future direct imaging facilities. We have compared performance estimates for near- and mid-infrared instruments to be installed at ESO's Extremely Large Telescope (ELT), and a potential space-based mission called Large Interferometer for Exoplanets (LIFE). We modelled the frequency and timing of energetic collisions using \textit{N}-body models of planet formation for different stellar types, and determine the cooling of the resulting magma oceans with an insulating atmosphere. We find that the probability of detecting at least one magma ocean planet depends on the observing duration and the distribution of atmospheric properties among rocky protoplanets. However, the prospects for detection significantly increase for young and close stellar targets, which show the highest frequencies of giant impacts. For intensive reconnaissance with a K band (2.2 $\mu m$) ELT filter or a 5.6 $\mu m$ LIFE filter, the $\beta$ Pictoris, Columba, TW Hydrae, and Tucana-Horologium associations represent promising candidates for detecting a molten protoplanet. Our results motivate the exploration of magma ocean planets using the ELT and underline the importance of space-based direct imaging facilities to investigate and characterize planet formation and evolution in the solar vicinity. ","Direct imaging of molten protoplanets in nearby young stellar
  associations"
42,1064778320536117248,965383857775300608,Yichuan Zhang,"['Here is a new statistical inference principle after variational inference and Markov chain Monte Carlo. This inference principle is based on ergodic transformations and the convergence in total variation is guaranteed by optimising the loss! See the paper\n<LINK>', 'Like variational inference(VI), ergodic inference (EI) optimise a Monte Carlo estimation of loss function and the loss of EI is equivalent to optimising ELBO. But the extract entropy of proposal in EI is not required. Any distribution with implicit density can be EI proposal!']",https://arxiv.org/abs/1811.07192,"Approximate inference algorithm is one of the fundamental research fields in machine learning. The two dominant theoretical inference frameworks in machine learning are variational inference (VI) and Markov chain Monte Carlo (MCMC). However, because of the fundamental limitation in the theory, it is very challenging to improve existing VI and MCMC methods on both the computational scalability and statistical efficiency. To overcome this obstacle, we propose a new theoretical inference framework called ergodic Inference based on the fundamental property of ergodic transformations. The key contribution of this work is to establish the theoretical foundation of ergodic inference for the development of practical algorithms in future work. ",The Theory and Algorithm of Ergodic Inference
43,1064776285170409472,1014465993463205888,Domenico Barbato,['My second paper is accepted for publication in @AandA_journal and available on #arXiv! Two new long-period giant planets found around metal-poor stars -- what does this mean for #exoplanet populations and formation theories?\n\n<LINK>'],https://arxiv.org/abs/1811.07776,"Statistical studies of exoplanets have shown that giant planets are more commonly hosted by metal-rich dwarf stars than low-metallicity ones, while such a correlation is not evident for lower-mass planets. The search for giant planets around metal-poor stars and the estimate of their occurrence $f_p$ is an important element in providing support to models of planet formation. We present results from the HARPS-N search for giant planets orbiting metal-poor ($-1.0\leq[Fe/H]\leq-0.5$ dex) stars in the northern hemisphere complementing a previous HARPS survey on southern stars in order to update the estimate of $f_p$. High-precision HARPS-N observations of 42 metal-poor stars are used to search for planetary signals to be fitted using differential evolution MCMC single-Keplerian models. We then join our detections to the results of the previous HARPS survey on 88 metal-poor stars to provide a preliminar estimate of the two-hemisphere $f_p$. We report the detection of two new giant planets around HD 220197 and HD 233832. The first companion has M$\sin{i}=0.20_{-0.04}^{+0.07}$ M$_{\rm Jup}$ and orbital period of $1728_{-80}^{+162}$ days, and for the second companion we find two solutions of equal statistical weight having periods $2058_{-40}^{+47}$ and $4047_{-117}^{+91}$ days and minimum masses of $1.78_{-0.06}^{+0.08}$ and $2.72_{-0.23}^{+0.23}$ M$_{\rm Jup}$, respectively. Joining our two detections with the three from the southern survey we obtain a preliminary and conservative estimate of global frequency of $f_p=3.84_{-1.06}^{+2.45}\%$ for giant planets around metal-poor stars. The two new giant planets orbit dwarf stars at the metal-rich end of the HARPS-N metal-poor sample, corroborating previous results suggesting that giant planet frequency still is a rising function of host star [Fe/H]. We also note that all detections in the overall sample are giant long-period planets. ","The GAPS Programme with HARPS-N at TNG XVIII. Two new giant planets
  around the metal-poor stars HD 220197 and HD 233832"
44,1064593651026792448,560473379,nick frosst,"[""1/7 Our new paper on adversarial attack detection and capsule networks with @sabour_sara and Geoff Hinton is out on arxiv today!  <LINK>   it will be presented at the #NeurIPS Workshop on Security. Don't have time to read the paper? Read this thread instead!  :)"", '2/7 The problem with adversarial examples is that they dont look like what they are classified as. Capsule networks output both a classification and a reconstruction of the input conditioned on the classification. A reconstruction of an adversarial looks different from the input https://t.co/xre8arY0nX', '3/7 We can create a detection algorithm by defining a threshold for reconstruction error from a validation set, and flag inputs as adversarial if the reconstruction error exceeds this threshold. https://t.co/WfpoYlJoeN', '4/7 This is an attack agnostic detection algorithm that works quite well for the three datasets we tested - MNIST, fashionMNIST and SVHN and the attacks we tested - black box and white box FGSM and BIM attacks https://t.co/WRk4T2SF3B', ""5/7 The reconstruction error is itself differentiable; we can make a stronger attack by minimize reconstruction error and maximize classification error. This attack can trick our detection, but the results aren't really ‘adversarial’ - they resemble members of the target class https://t.co/JMv5fc0CI0"", '6/7 This is true for MNIT and fashionMNIST https://t.co/lYDpTnpyfJ', '7/7 And SVHN. Read the paper to see to our work in full :)  https://t.co/eiKqc2FcKM https://t.co/h7lRyGEuqP', ""@lavanyashukla Thanks :) I'm really excited about the natural looking adversarial attacks :)"", '@nagaraj_arvind @sabour_sara Thanks for reading :)', '@HoldijkLars @sabour_sara We have black box attacks but nothing more sophisticated than fgsm and BIM. We hope to include those results when we take this from a workshop paper to a full conference paper :) please send me a link to your work when it is out! :)', '@ReubenFeinman 1/2 Yes this is a good point, in fact, it turns out that noisy data has an even higher reconstruction error than adversarial data. To me this is not an issue, as neither noisy data nor adversarially perturbed data lie on the true data manifold. https://t.co/jshvu7AXK8', '@ReubenFeinman 2/2 and if we are attempting to make an attack agnostic defense, then we are really just detecting out of training distribution inputs.', '@ReubenFeinman i mention that this can flag inputs even if the network still gets the classification correct. I think systems should detect when they are being attacked regardless of if the attack is successful. we should flag true data, vs data taken from outside the training data distribution', ""@ReubenFeinman PCA's ability to detect adversarials in MNIST seems to come from border effects in the data set, see https://t.co/UNm5DF5zy6, though tbh i am sure my defense will be put through the wringer if ever Carlini and Wagner look at it the way they have looked at those tin their paper :P"", '@ReubenFeinman I think that is the way system should behave; If ever input comes from a distribution that was not part of the training set then classification behavior cannot be guaranteed, and the input should be flagged.', '@ReubenFeinman tbh i am not particularly worried about adversarial detection as a security issue, see https://t.co/Val9w13NyE. i am more interested in systems that have consistent behavior in distinguishing true data vs perturbed data,  with gradients aligned with the true data manifold', '@ReubenFeinman true true, but as long as the test data is noisy in the way the training data was noisy then the system should be fine.', '@tremblerz @sabour_sara For successful attacks this seems to be the case, but minimizing reconstruction error is an inherently weaker attack and many initial inputs fail to flip the network classification, even with many steps of BIM', '@tremblerz @sabour_sara we mention this result when visualizing the reconstruction BIM attacks on fashionMNIST - no shoes where successfully adversarially perturbed to be classified as pants', '@ReubenFeinman Hey so i ran a quick version of what you said. I trained a model on mnist with some gaussian noise. as expected the reconstructions do ignore the noise, so it has a higher average reconstruction loss, but we are still able to detect adversarials from the data from the train set https://t.co/MmRqn1S2df', '@EarlenceF @sabour_sara thus far i have not! but i am hoping to do so when we take this from a workshop paper to a full  conference paper.']",https://arxiv.org/abs/1811.06969,"We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the $l2$ distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the ""adversarial"" image resemble images of the other class. ","DARCCC: Detecting Adversaries by Reconstruction from Class Conditional
  Capsules"
45,1064556891177545728,1032007830386012160,Paul Dalba,"['Curious about the chances of  @NASA_TESS seeing transits of known RV exoplanets? Check out our new paper that explores this: <LINK>', '@ExoCytherean']",https://arxiv.org/abs/1811.06550,"Radial velocity (RV) surveys have detected hundreds of exoplanets through their gravitational interactions with their host stars. Some will be transiting, but most lack sufficient follow-up observations to confidently detect (or rule out) transits. We use published stellar, orbital, and planetary parameters to estimate the transit probabilities for nearly all exoplanets that have been discovered via the RV method. From these probabilities, we predict that $25.5^{+0.7}_{-0.7}$ of the known RV exoplanets should transit their host stars. This prediction is more than double the amount of RV exoplanets that are currently known to transit. The Transiting Exoplanet Survey Satellite (TESS) presents a valuable opportunity to explore the transiting nature of many of the known RV exoplanet systems. Based on the anticipated pointing of TESS during its two-year primary mission, we identify the known RV exoplanets that it will observe and predict that $11.7^{+0.3}_{-0.3}$ of them will have transits detected by TESS. However, we only expect the discovery of transits for $\sim$3 of these exoplanets to be novel (i.e., not previously known). We predict that the TESS photometry will yield dispositive null results for the transits of $\sim$125 RV exoplanets. This will represent a substantial increase in the effort to refine ephemerides of known RV exoplanets. We demonstrate that these results are robust to changes in the ecliptic longitudes of future TESS observing sectors. Finally, we consider how several potential TESS extended mission scenarios affect the number of transiting RV exoplanets we expect TESS to observe. ","Predicted Yield of Transits of Known Radial Velocity Exoplanets from the
  TESS Primary and Extended Missions"
46,1064550885873905665,1725428047,Jacques Carolan,"['***AIRHORN*** new preprint alert <LINK>😱😱😱  TFW you think an experiment is going to be super quick, then it turns out to be really hard, then you write a paper about why it was so hard and how you solved it @Dirk_Englund #quantum #silicon #photonics #photons', '@RLEatMIT @MIT']",https://arxiv.org/abs/1811.06557,"Large-scale quantum technologies require exquisite control over many individual quantum systems. Typically, such systems are very sensitive to environmental fluctuations, and diagnosing errors via measurements causes unavoidable perturbations. In this work we present an in situ frequency locking technique that monitors and corrects frequency variations in single photon sources based on microring resonators. By using the same classical laser fields required for photon generation as a probe to diagnose variations in the resonator frequency, our protocol applies feedback control to correct photon frequency errors in parallel to the optical quantum computation without disturbing the physical qubit. We implement our technique on a silicon photonic device and demonstrate sub 1 pm frequency stabilization in the presence of applied environmental noise, corresponding to a fractional frequency drift of <1 % of a photon linewidth. Using these methods we demonstrate feedback controlled quantum state engineering. By distributing a single local oscillator across a single chip or network of chips, our approach enables frequency locking of many single photon sources for large-scale photonic quantum technologies. ","Scalable feedback control of single photon sources for photonic quantum
  technologies"
47,1063868168354054145,20810416,Dr. Roman Yampolskiy,['New paper on AI addiction by Vahid Behzadan et al. <LINK>\n\nEmergence of Addictive Behaviors in Reinforcement Learning Agents\n\nThis paper presents a novel approach to the technical analysis of wireheading in... <LINK>'],https://arxiv.org/abs/1811.05590,"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems. ",Emergence of Addictive Behaviors in Reinforcement Learning Agents
48,1063712673173463041,56395761,Reza Abbasi-Asl,"['Our new BCI paper is now on arXiv: Brain-Computer Interface in Virtual Reality <LINK> \nWe build an EEG-controlled VR headset and compare the performance of BCI tasks in VR to 2D regular displays. \nA joint work with @mkeshavarziarch and Dorian Chan. <LINK>', 'Check out a demo of our headset for painting and carving with Brain-Controlled Interface in Virtual Reality. https://t.co/fH07rdvTFH']",https://arxiv.org/abs/1811.06040,"We study the performance of brain computer interface (BCI) system in a virtual reality (VR) environment and compare it to 2D regular displays. First, we design a headset that consists of three components: a wearable electroencephalography (EEG) device, a VR headset and an interface. Recordings of brain and behavior from human subjects, performing a wide variety of tasks using our device are collected. The tasks consist of object rotation or scaling in VR using either mental commands or facial expression (smile and eyebrow movement). Subjects are asked to repeat similar tasks on regular 2D monitor screens. The performance in 3-D virtual reality environment is considerably higher compared to the to the 2D screen. Particularly, the median number of success rate across trials for VR setting is double of that for the 2D setting (8 successful command in VR setting compared to 4 successful command in 2D screen in 1 minute trials). Our results suggest that the design of future BCI systems can remarkably benefit from the VR setting. ",Brain-Computer Interface in Virtual Reality
49,1063484732934160385,710610891058716673,Jan Leike,"['New paper with @OpenAI on learning to play Atari games without rewards: demonstrations help get a better state space coverage and are a more efficient communication channel than preferences. Both together outperform preferences and imitation alone. 1/3\n<LINK>', 'We included more analysis of why reward learning works better in some games than others (Atari games have a lot of diversity!), including evaluating how aligned the learned reward model is. 2/3 https://t.co/lYk4iBfgC5', 'Finally, more evidence that the reward model needs to be trained with humans in the loop; otherwise the agent learns to exploit the reward model, for example by pretending to shoot a spider in Hero. 3/3 https://t.co/AYRBBtIrJh']",https://arxiv.org/abs/1811.06521,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels. ",Reward learning from human preferences and demonstrations in Atari
50,1063329812511682560,1562913787,Nathan Moynihan,['New paper out today: <LINK>'],https://arxiv.org/abs/1811.05985,"We investigate the evolution of complexity and entanglement following a quench in a one-dimensional topological system, namely the Su-Schrieffer-Heeger model. We demonstrate that complexity can detect quantum phase transitions and shows signatures of revivals; this observation provides a practical advantage in information processing. We also show that the complexity saturates much faster than the entanglement entropy in this system, and we provide a physical argument for this. Finally, we demonstrate that complexity is a less sensitive probe of topological order, compared with measures of entanglement. ","Post-Quench Evolution of Complexity and Entanglement in a Topological
  System"
51,1063266302658142209,452384386,Sebastien Bubeck,['New short paper with @ilyaraz2 @ecprice and YT Lee: (trapdoored)pseudorandom generators yield classification tasks admitting *extremely robust* classifier yet it is impossible to obtain efficiently any robustness whatsoever. #AdversarialML \n <LINK>'],https://arxiv.org/abs/1811.06418,"In our recent work (Bubeck, Price, Razenshteyn, arXiv:1805.10204) we argued that adversarial examples in machine learning might be due to an inherent computational hardness of the problem. More precisely, we constructed a binary classification task for which (i) a robust classifier exists; yet no non-trivial accuracy can be obtained with an efficient algorithm in (ii) the statistical query model. In the present paper we significantly strengthen both (i) and (ii): we now construct a task which admits (i') a maximally robust classifier (that is it can tolerate perturbations of size comparable to the size of the examples themselves); and moreover we prove computational hardness of learning this task under (ii') a standard cryptographic assumption. ",Adversarial Examples from Cryptographic Pseudo-Random Generators
52,1063099026621390848,14975979,Abhimat Gautam,"['I just finished a new research paper, studying the variability of stars and looking for binary stars at the center of our galaxy!\n\nThe paper is publicly available here: <LINK>\n(you can download the full paper, with all the details, here: <LINK> )', 'I also wrote a short post on my blog explaining the main results from my paper in a little more accessible way if you’re not an astronomer! https://t.co/OWYeWd20Ww\n\n(I’m really trying to improve my scientific writing for the general public, and would love feedback on this post!)', '@amsingh2 Thank you Baba!']",https://arxiv.org/abs/1811.04898,"We present a $\approx 11.5$ year adaptive optics (AO) study of stellar variability and search for eclipsing binaries in the central $\sim 0.4$ pc ($\sim 10''$) of the Milky Way nuclear star cluster. We measure the photometry of 563 stars using the Keck II NIRC2 imager ($K'$-band, $\lambda_0 = 2.124 \text{ } \mu \text{m}$). We achieve a photometric uncertainty floor of $\Delta m_{K'} \sim 0.03$ ($\approx 3\%$), comparable to the highest precision achieved in other AO studies. Approximately half of our sample ($50 \pm 2 \%$) shows variability. $52 \pm 5\%$ of known early-type young stars and $43 \pm 4 \%$ of known late-type giants are variable. These variability fractions are higher than those of other young, massive star populations or late-type giants in globular clusters, and can be largely explained by two factors. First, our experiment time baseline is sensitive to long-term intrinsic stellar variability. Second, the proper motion of stars behind spatial inhomogeneities in the foreground extinction screen can lead to variability. We recover the two known Galactic center eclipsing binary systems: IRS 16SW and S4-258 (E60). We constrain the Galactic center eclipsing binary fraction of known early-type stars to be at least $2.4 \pm 1.7\%$. We find no evidence of an eclipsing binary among the young S-stars nor among the young stellar disk members. These results are consistent with the local OB eclipsing binary fraction. We identify a new periodic variable, S2-36, with a 39.43 day period. Further observations are necessary to determine the nature of this source. ",An Adaptive Optics Survey of Stellar Variability at the Galactic Center
53,1062988342499987456,15151130,Dr Lynsay Shepherd,['New paper up on @arXiv “Mayall: A Framework for Desktop JavaScript Auditing and Post-Exploitation Analysis” authored with @noktec and @Doctor_Hacker #JavaScript #security <LINK>'],https://arxiv.org/abs/1811.05945,"Writing desktop applications in JavaScript offers developers the opportunity to write cross-platform applications with cutting edge capabilities. However in doing so, they are potentially submitting their code to a number of unsanctioned modifications from malicious actors. Electron is one such JavaScript application framework which facilitates this multi-platform out-the-box paradigm and is based upon the Node.js JavaScript runtime --- an increasingly popular server-side technology. In bringing this technology to the client-side environment, previously unrealized risks are exposed to users due to the powerful system programming interface that Node.js exposes. In a concerted effort to highlight previously unexposed risks in these rapidly expanding frameworks, this paper presents the Mayall Framework, an extensible toolkit aimed at JavaScript security auditing and post-exploitation analysis. The paper also exposes fifteen highly popular Electron applications and demonstrates that two thirds of applications were found to be using known vulnerable elements with high CVSS scores. Moreover, this paper discloses a wide-reaching and overlooked vulnerability within the Electron Framework which is a direct byproduct of shipping the runtime unaltered with each application, allowing malicious actors to modify source code and inject covert malware inside verified and signed applications without restriction. Finally, a number of injection vectors are explored and appropriate remediations are proposed. ","Mayall: A Framework for Desktop JavaScript Auditing and
  Post-Exploitation Analysis"
54,1062886342974009346,2203468841,Dr Jade Powell,['Our new low mass single and binary star supernova simulations paper 💥 <LINK> 💥 more to come soon on GW emission.'],https://arxiv.org/abs/1811.05483,"We present a suite of seven 3D supernova simulations of non-rotating low-mass progenitors using multi-group neutrino transport. Our simulations cover single star progenitors with zero-age main sequence masses between $9.6 M_\odot$ and $12.5 M_\odot$ and (ultra)stripped-envelope progenitors with initial helium core masses between $2.8 M_\odot$ and $3.5 M_\odot$. We find explosion energies between $0.1\,\mathrm{Bethe}$ and $0.4\,\mathrm{Bethe}$, which are still rising by the end of the simulations. Although less energetic than typical events, our models are compatible with observations of less energetic explosions of low-mass progenitors. In six of our models, the mass outflow rate already exceeds the accretion rate onto the proto-neutron star, and the mass and angular momentum of the compact remnant have closely approached their final value, barring the possibility of later fallback. While the proto-neutron star is still accelerated by the gravitational tug of the asymmetric ejecta, the acceleration can be extrapolated to obtain estimates for the final kick velocity. We obtain gravitational neutron star masses between $1.22 M_\odot$ and $1.44 M_\odot$, kick velocities between $11\, \mathrm{km}\, \mathrm{s}^{-1}$ and $695\, \mathrm{km}\, \mathrm{s}^{-1}$, and spin periods from $20\, \mathrm{ms}$ to $2.7\,\mathrm{s}$, which suggests that typical neutron star birth properties can be naturally obtained in the neutrino-driven paradigm. We find a loose correlation between the explosion energy and the kick velocity. There is no indication of spin-kick alignment, but a correlation between the kick velocity and the neutron star angular momentum, which needs to be investigated further as a potential point of tension between models and observations. ","Three-Dimensional Simulations of Neutrino-Driven Core-Collapse
  Supernovae from Low-Mass Single and Binary Star Progenitors"
55,1062682557505069057,131879500,John Ilee,"['Our new paper on G11.92 MM1 - an extreme mass ratio proto-binary star caught in the act of formation, with a fantastic fragmenting disc.  Out on astro-ph today: <LINK>\n\n(with huge thanks to @ToddRHunter, @dh4gan, @TomHaworthAstro, @tharries and others!) <LINK>']",https://arxiv.org/abs/1811.05267,"We present high resolution ($\sim$300 au) Atacama Large Millimeter/submillimeter Array (ALMA) observations of the massive young stellar object G11.92-0.61 MM 1. We resolve the immediate circumstellar environment of MM 1 in 1.3 mm continuum emission and CH$_{3}$CN emission for the first time. The object divides into two main sources - MM 1a, which is the source of a bipolar molecular outflow, and MM 1b, located 0.57'' (1920 au) to the South-East. The main component of MM 1a is an elongated continuum structure, perpendicular to the bipolar outflow, with a size of $0.141'' \times 0.050''$ ($480\times170$ au). The gas kinematics toward MM 1a probed via CH$_{3}$CN trace a variety of scales. The lower energy $J=12-11$ $K=3$ line traces extended, rotating gas within the outflow cavity, while the $v$8=1 line shows a clearly-resolved Keplerian rotation signature. Analysis of the gas kinematics and dust emission shows that the total enclosed mass in MM 1a is $40\pm5$ M$_{\odot}$ (where between 2.2-5.8 M$_{\odot}$ is attributed to the disk), while MM 1b is $<0.6$ M$_{\odot}$. The extreme mass ratio and orbital properties of MM 1a and MM 1b suggest that MM 1b is one of the first observed examples of the formation of a binary star via disk fragmentation around a massive young (proto)star. ",G11.92-0.61 MM 1: A fragmented Keplerian disk surrounding a proto-O star
56,1062638170993500161,356676252,John Regan,"['Check out our new paper (<LINK>) with colleagues from the @astroIAP. Massive black hole seeds can initially have super-Eddington accretion rates but jet outflows quickly regulate the accretion to below Eddington. Conclusion: Black holes are hard to grow quickly!', 'Co-authors: @TurloughDownes, @maximetrebitsch, @RicardaBeckmann, Marta Volonteri, Alessandro Lupi &amp; Yohan Dubois\n@cfar_dcu @DCUMaths @DCUFSH @MSCActions', 'https://t.co/hhAyFeR6Ag']",https://arxiv.org/abs/1811.04953,"Super-Eddington accretion onto massive black hole seeds may be commonplace in the early Universe, where the conditions exist for rapid accretion. Direct collapse black holes are often invoked as a possible solution to the observation of super massive black holes (SMBHs) in the pre-reionisation Universe. We investigate here how feedback, mainly in the form of bipolar jets, from super-Eddington accreting seed black holes will affect their subsequent growth. We find that, nearly independent of the mass loading of the bipolar jets, the violent outflows generated by the jets evacuate a region of approximately 0.1 pc surrounding the black hole seed. However, the jet outflows are unable to break free of the halo and their impact is limited to the immediate vicinity of the black hole. The outflows suppress any accretion for approximately a dynamical time. The gas then cools, recombines and falls back to the centre where high accretion rates are again observed. The overall effect is to create an effective accretion rate with values of between 0.1 and 0.5 times the Eddington rate. If this episodic accretion rate is maintained for order 500 million years then the black hole will increase in mass by a factor of between 3 and 300 but far short of the factor of $10^4$ required for the seeds to become the SMBHs observed at $z>6$. Therefore, direct collapse black holes born into atomic cooling haloes and which experience strong negative mechanical feedback will require external influences (e.g. rapid major mergers with other haloes) to promote efficient accretion and reach SMBH masses within a few hundred million years. ","Super-Eddington Accretion and Feedback from the First Massive Seed Black
  Holes"
57,1062522893303799808,3301643341,Roger Grosse,"['New paper w/ Cem Anil and @james_r_lucas on Lipschitz neural net architectures. Uses sorting as an activation function, with matrix norm constrained weights. Universal Lipschitz function approx. Enforce adversarial robustness (margin) using hinge loss.\n\n<LINK> <LINK>', 'This is anecdotal, but our WGAN worked on the first try.', 'Btw, Cem is an undergrad and is applying to PhD programs in this cycle.', ""@roydanroy @james_r_lucas Yes, the Lipschitz constraint is guaranteed. But that's the easy part. The hard part is enforcing it without limiting the expressiveness too much.""]",https://arxiv.org/abs/1811.05381,"Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy. ",Sorting out Lipschitz function approximation
58,1062306610792546304,2372697584,Chad Giusti,['New paper joint with @yldarrick on the semantics of iterated integrals in time series analysis on the arXiv: <LINK> (And thanks to @viditnanda for reminding me to make an announcement!) <LINK>'],http://arxiv.org/abs/1811.03558,"One of the core advantages topological methods for data analysis provide is that the language of (co)chains can be mapped onto the semantics of the data, providing a natural avenue for human understanding of the results. Here, we describe such a semantic structure on Chen's classical iterated integral cochain model for paths in Euclidean space. Specifically, in the context of population time series data, we observe that iterated integrals provide a model-free measure of pairwise influence that can be used for causality inference. Along the way, we survey recent results and applications, review the current standard methods for causality inference, and briefly provide our outlook on generalizations to go beyond time series data. ",Iterated Integrals and Population Time Series Analysis
59,1062299082818011141,204261944,Matthew Kenworthy,"['Really awesome to see Samuel Mellon\'s paper on the discovery of a new Delta Scuti pulsating star from the bRing project! Mellon et al. ""Discovery of δ Scuti Pulsations in the Young Hybrid Debris Disk Star HD 156623""\n<LINK> <LINK>']",https://arxiv.org/abs/1811.04118,"The bRing robotic observatory network was built to search for circumplanetary material within the transiting Hill sphere of the exoplanet $\beta$ Pic b across its bright host star $\beta$ Pic. During the bRing survey of $\beta$ Pic, it simultaneously monitored the brightnesses of thousands of bright stars in the southern sky ($V$ $\simeq$ 4-8, $\delta$ $\lesssim$ -30$^{\circ}$). In this work, we announce the discovery of $\delta$ Scuti pulsations in the A-type star HD 156623 using bRing data. HD 156623 is notable as it is a well-studied young star with a dusty and gas-rich debris disk, previously detected using ALMA. We present the observational results on the pulsation periods and amplitudes for HD 156623, discuss its evolutionary status, and provide further constraints on its nature and age. We find strong evidence of frequency regularity and grouping. We do not find evidence of frequency, amplitude, or phase modulation for any of the frequencies over the course of the observations. We show that HD 156623 is consistent with other hot and high frequency pre-MS and early ZAMS $\delta$ Scutis as predicted by theoretical models and corresponding evolutionary tracks, although we observe that HD 156623 lies hotter than the theoretical blue edge of the classical instability strip. This, coupled with our characterization and Sco-Cen membership analyses, suggest that the star is most likely an outlying ZAMS member of the $\sim$16 Myr Upper Centaurus-Lupus subgroup of the Sco-Cen association. ","Discovery of {\delta} Scuti Pulsations in the Young Hybrid Debris Disk
  Star HD 156623"
60,1062256813024993280,523241142,Juste Raimbault,['New paper: \nRelating complexities for the reflexive study of complex systems <LINK>\n(or why studying the complex is indeed complex)'],https://arxiv.org/abs/1811.04270,"Several approaches and corresponding definitions of complexity have been developed in different fields. Urban systems are the archetype of complex socio-technical systems concerned with these different viewpoints. We suggest in this chapter some links between three types of complexity, namely emergence, computational complexity and informational complexity. We discuss the implication of these links on the necessity of reflexivity to produce a knowledge of the complex, and how this connects to the interdisciplinary of approaches in particular for socio-technical systems. We finally synthesize this positioning as a proposal of an epistemological framework called applied perspectivism, and discuss the implications for the study of urban systems. ",Relating complexities for the reflexive study of complex systems
61,1062255081272291328,280083723,Yoh Tanimoto,"['new paper~ <LINK> we show that any soliton in a conformal net has positive energy, construct continuously many solitons, and extend conformal covariance of the U(1)-current to Sobolev class D^s, s&gt;2~']",https://arxiv.org/abs/1811.04501,"We show that any solitonic representation of a conformal (diffeomorphism covariant) net on S^1 has positive energy and construct an uncountable family of mutually inequivalent solitonic representations of any conformal net, using nonsmooth diffeomorphisms. On the loop group nets, we show that these representations induce representations of the subgroup of loops compactly supported in S^1 \ {-1} which do not extend to the whole loop group. In the case of the U(1)-current net, we extend the diffeomorphism covariance to the Sobolev diffeomorphisms D^s(S^1), s > 2, and show that the positive-energy vacuum representations of Diff_+(S^1) with integer central charges extend to D^s(S^1). The solitonic representations constructed above for the U(1)-current net and for Virasoro nets with integral central charge are continuously covariant with respect to the stabilizer subgroup of Diff_+(S^1) of -1 of the circle. ",Solitons and nonsmooth diffeomorphisms in conformal nets
62,1061976636906786816,341126513,Francesca Fragkoudi,"['Read something cool today: Our new paper on how inner bars also buckle (or if you want to get poetic about it: about galaxies within galaxies, bars within bars and peanuts within peanuts 😎) as uncovered using #VLT-MUSE data \n<LINK>', 'And check out this awesome animation and press release related to the paper:\nhttps://t.co/CGR05yB4Pf\nhttps://t.co/4e5R37ouBg']",https://arxiv.org/abs/1811.03855v1,"Double bars are thought to be important features for secular evolution in the central regions of galaxies. However, observational evidence about their origin and evolution is still scarce. We report on the discovery of the first Box-Peanut (B/P) structure in an inner bar detected in the face-on galaxy NGC 1291. We use the integral field data obtained from the MUSE spectrograph within the TIMER project. The B/P structure is detected as bi-symmetric minima of the $h_4$ moment of the line-of-sight velocity distribution along the major axis of the inner bar, as expected from numerical simulations. Our observations demonstrate that inner bars can follow a similar evolutionary path as outer bars, undergoing buckling instabilities. They also suggest that inner bars are long-lived structures, thus imposing tight constraints to their possible formation mechanisms ","] Inner bars also buckle. The MUSE TIMER view of the double-barred galaxy
  NGC 1291"
63,1061964806532534273,2654165034,Jason M Pittman,['The danger of stovepiping in AGI containment - our new paper is out:\n<LINK>\n#ResearchInstituteForSyntheticIntelligence #AI #AGI #Containment'],https://arxiv.org/abs/1811.03653,"Awareness of the possible impacts associated with artificial intelligence has risen in proportion to progress in the field. While there are tremendous benefits to society, many argue that there are just as many, if not more, concerns related to advanced forms of artificial intelligence. Accordingly, research into methods to develop artificial intelligence safely is increasingly important. In this paper, we provide an overview of one such safety paradigm: containment with a critical lens aimed toward generative adversarial networks and potentially malicious artificial intelligence. Additionally, we illuminate the potential for a developmental blindspot in the stovepiping of containment mechanisms. ",Stovepiping and Malicious Software: A Critical Review of AGI Containment
64,1061957787041910785,3306943245,Konstantin Klemmer,"['New paper, to be presented at @NipsConference Spatiotemporal Workshop: Using #MachineLearning to understand and predict rape reporting delays (time between crime occurrence and reporting). Joint work with @NYU_CUSP! <LINK> #NIPS2018 <LINK>']",https://arxiv.org/abs/1811.03939,"We present a novel approach to estimate the delay observed between the occurrence and reporting of rape crimes. We explore spatial, temporal and social effects in sparse aggregated (area-level) and high-dimensional disaggregated (event-level) data for New York and Los Angeles. Focusing on inference, we apply Gradient Boosting and Random Forests to assess predictor importance, as well as Gaussian Processes to model spatial disparities in reporting times. Our results highlight differences and similarities between the two cities. We identify at-risk populations and communities which may be targeted with focused policies and interventions to support rape victims, apprehend perpetrators, and prevent future crimes. ","Modeling Rape Reporting Delays Using Spatial, Temporal and Social
  Features"
65,1061907026165534720,721931072,Shimon Whiteson,"['Are you interested in variational reinforcement learning but find existing formalisms confusing or just plain wrong?  Check out our new paper on VIREL, a new theoretically grounded variational framework for RL. <LINK> @mattfellowsoxcs', 'Applying EM to our framework induces actor-critic methods in which the E-step = policy improvement and the M-step = policy evaluation.  In high-dimensional tasks, this approach outperforms soft actor-critic.']",https://arxiv.org/abs/1811.01132,"Applying probabilistic models to reinforcement learning (RL) enables the application of powerful optimisation tools such as variational inference to RL. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, e.g., the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties learning deterministic policies in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises a parametrised action-value function to summarise future dynamics of the underlying MDP. This gives VIREL a mode-seeking form of KL divergence, the ability to learn deterministic optimal polices naturally from inference and the ability to optimise value functions and policies in separate, iterative steps. In applying variational expectation-maximisation to VIREL we thus show that the actor-critic algorithm can be reduced to expectation-maximisation, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We then derive a family of actor-critic methods from VIREL, including a scheme for adaptive exploration. Finally, we demonstrate that actor-critic algorithms from this family outperform state-of-the-art methods based on soft value functions in several domains. ",VIREL: A Variational Inference Framework for Reinforcement Learning
66,1061688696674680832,39640065,Dan Scolnic,"['New paper out I got to help with on using LSST to find kilonovae, the EM counterparts to gravitational wave events: <LINK>  This will be big business some day - and next run starts in February!']",https://arxiv.org/abs/1811.03098,"We present simulated observations to assess the ability of LSST and the WFD survey to detect and characterize kilonovae - the optical emission associated with binary neutron star (and possibly black hole - neutron star) mergers. We expand on previous studies in several critical ways by exploring a range of kilonova models and several choices of cadence, as well as by evaluating the information content of the resulting light curves. We find that, depending on the precise choice of cadence, the WFD survey will achieve an average kilonova detection efficiency of $\approx 1.6-2.5\%$ and detect only $\approx 3-6$ kilonovae per year. The detected kilonovae will be within the detection volume of Advanced LIGO/Virgo (ALV). By refitting the best resulting LSST light curves with the same model used to generate them we find the model parameters are generally weakly constrained, and are accurate to at best a factor of $2-3$. Motivated by the finding that the WFD will yield a small number of kilonova detections, with poor light curves and marginal information content, and that the detections are in any case inside the ALV volume, we argue that target-of-opportunity follow-up of gravitational wave triggers is a much more effective approach for kilonova studies. We outline the qualitative foundation for such a program with the goal of minimizing the impact on LSST operations. We argue that observations in the $gz$-bands with a total time investment per event of $\approx 1.5$ hour per 10 deg$^2$ of search area is sufficient to rapidly detect and identify kilonovae with $\gtrsim 90\%$ efficiency. For an estimated event rate of $\sim20$ per year visible to LSST, this accounts for $\sim1.5\%$ of the total survey time. In this regime, LSST has the potential to be a powerful tool for kilonovae discovery, with detected events handed off to other narrow-field facilities for further monitoring. ","LSST Target-of-Opportunity Observations of Gravitational Wave Events:
  Essential and Efficient"
67,1061348205634314240,250846437,Nouha Dziri,"['Our paper is now on Arxiv!  <LINK> with @korymath, @ehsk0 and @ozaiane. We introduce THRED model which can generate diverse and contextual responses. We also introduce two new, easy to implement evaluation metrics and provide a large clean conversational dataset.', '@bose_joey @korymath @ehsk0 @ozaiane Thank you so much Joey :)) Congrats for your 2 papers in Nips 💪 See you in NIPS very soon!']",https://arxiv.org/abs/1811.01063,"Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines. ","Augmenting Neural Response Generation with Context-Aware Topical
  Attention"
68,1060966083551813632,45144224,Ben Green,"['🚨 Just posted a new paper! 🚨\n\n""Data Science as Political Action: Grounding Data Science in a Politics of Justice""\n\nI argue that data scientists must recognize themselves as political actors and adopt an explicit politics of social justice.\n\nRead it here: <LINK> <LINK>', 'I start by highlighting the limits of current moves toward ethics codes: they misunderstand the relationship between technology and society, they rarely come with mechanisms for accountability, and they lack a clear normative underpinning.', 'In part 1 of the essay, I explain why must data scientists recognize themselves as political actors by responding to three common arguments:\n1) “I’m just an engineer.”\n2) “Our job isn’t to take political stances.”\n3) “We should not let the perfect be the enemy of the good.”', 'In particular, I explain why attempting to remain apolitical is itself a political stance—a fundamentally conservative one—and why the field’s current attempts to promote “social good” dangerously rely on vague and unarticulated political assumptions.', 'In part 2, I describe four stages of developing new methods and structures that orient data science around a politics of social justice. We must strive for a non-reformist and anti-oppressive data science that focuses on downstream impacts rather than good intentions.', 'In sum: data scientists must abandon their self-conception of being neutral engineers. The path ahead does not require data scientists to abandon our technical expertise, but it does entail expanding our notions of what problems to work on and how to engage with society.', '@karen_ec_levy Would love to hear your reactions! It’s very much a work in progress.']",https://arxiv.org/abs/1811.03435,"In response to public scrutiny of data-driven algorithms, the field of data science has adopted ethics training and principles. Although ethics can help data scientists reflect on certain normative aspects of their work, such efforts are ill-equipped to generate a data science that avoids social harms and promotes social justice. In this article, I argue that data science must embrace a political orientation. Data scientists must recognize themselves as political actors engaged in normative constructions of society and evaluate their work according to its downstream impacts on people's lives. I first articulate why data scientists must recognize themselves as political actors. In this section, I respond to three arguments that data scientists commonly invoke when challenged to take political positions regarding their work. In confronting these arguments, I describe why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why data science's attempts to promote ""social good"" dangerously rely on unarticulated and incrementalist political assumptions. I then propose a framework for how data science can evolve toward a deliberative and rigorous politics of social justice. I conceptualize the process of developing a politically engaged data science as a sequence of four stages. Pursuing these new approaches will empower data scientists with new methods for thoughtfully and rigorously contributing to social justice. ","Data Science as Political Action: Grounding Data Science in a Politics
  of Justice"
69,1060938543546187778,712975316587819008,Pieter Roelfsema,['A new paper on how deep learning could be implemented in the brain. We trained a network with a biologically plausible learning rule to recognize hand-written digits by trial-and-error: 2x slower than the non-biological error back propagation rule: <LINK>'],https://arxiv.org/abs/1811.01768,"Researchers have proposed that deep learning, which is providing important progress in a wide range of high complexity tasks, might inspire new insights into learning in the brain. However, the methods used for deep learning by artificial neural networks are biologically unrealistic and would need to be replaced by biologically realistic counterparts. Previous biologically plausible reinforcement learning rules, like AGREL and AuGMEnT, showed promising results but focused on shallow networks with three layers. Will these learning rules also generalize to networks with more layers and can they handle tasks of higher complexity? We demonstrate the learning scheme on classical and hard image-classification benchmarks, namely MNIST, CIFAR10 and CIFAR100, cast as direct reward tasks, both for fully connected, convolutional and locally connected architectures. We show that our learning rule - Q-AGREL - performs comparably to supervised learning via error-backpropagation, with this type of trial-and-error reinforcement learning requiring only 1.5-2.5 times more epochs, even when classifying 100 different classes as in CIFAR100. Our results provide new insights into how deep learning may be implemented in the brain. ",A Biologically Plausible Learning Rule for Deep Learning in the Brain
70,1060919453066510337,406316583,Giancarlo Pellegrino,"['Perceptual adblockers are great but operate in the worst threat model possible for computer vision/ML with attacks going beyond mere evasion/detection. We present all that and much more in a new paper with F. Tramèr, P. Dupré, G. Rusak and D. Boneh <LINK> <LINK>']",https://arxiv.org/abs/1811.03194,"Perceptual ad-blocking is a novel approach that detects online advertisements based on their visual content. Compared to traditional filter lists, the use of perceptual signals is believed to be less prone to an arms race with web publishers and ad networks. We demonstrate that this may not be the case. We describe attacks on multiple perceptual ad-blocking techniques, and unveil a new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual ad-blocking can also introduce new vulnerabilities that let an attacker bypass web security boundaries and mount DDoS attacks. We first analyze the design space of perceptual ad-blockers and present a unified architecture that incorporates prior academic and commercial work. We then explore a variety of attacks on the ad-blocker's detection pipeline, that enable publishers or ad networks to evade or detect ad-blocking, and at times even abuse its high privilege level to bypass web security boundaries. On one hand, we show that perceptual ad-blocking must visually classify rendered web content to escape an arms race centered on obfuscation of page markup. On the other, we present a concrete set of attacks on visual ad-blockers by constructing adversarial examples in a real web page context. For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and native web content that misleads perceptual ad-blocking with 100% success rates. In one of our attacks, we demonstrate how a malicious user can upload adversarial content, such as a perturbed image in a Facebook post, that fools the ad-blocker into removing another users' non-ad content. Moving beyond the Web and visual domain, we also build adversarial examples for AdblockRadio, an open source radio client that uses machine learning to detects ads in raw audio streams. ",AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning
71,1060822585188016128,1030693296,Nicolas Martin,"['Exciting new paper led by PhD students @Astro_Sestitof and @Nico_Longeard this morning! We use #GaiaDR2 astrometry and photometry to get the distances to all known ultra-metal poor ([Fe/H]&lt;-4.0) stars known to date. 1/19 <LINK>', 'Since these stars contain so few heavy elements, they are thought to be among the oldest stars known to date and can be used to trace how the Milky Way assembled. And boy does the combination of UMP stars and #GaiaDR2 deliver! 2/19', 'What @Astro_Sestitof and @Nico_Longeard did is use isochrone/luminosity function models, combined with the #GaiaDR2 photometry and parallax (and associated uncertainties, of course!) to infer the PDF on the distance to every known UMP star. 3/19', ""For a star of a given color, #GaiaDR2 is essential to break the dwarf/giant degeneracy that comes from the comparison of the star's photometry with the isochrone models. Even a small and uncertain parallax from Gaia is very informative as it confirms the star isn't close-by 4/19"", 'For instance, with this star, the comparison with the isochrone models gives the black likelihood but Gaia and a MW density prior (red line) clearly reject the nearby dwarf solution and allows us to infer the star is a more distance giant (blue posterior PDF). 5/19 https://t.co/WfTZrfSEWs', 'So we get distance inferences for all 41 known UMP stars in the literature, found over the last 3 decades. Some are well constrained, others less so but we track our uncertainties all the way through in all cases. 6/19', ""The distance is a crucial piece of information that, when combined with the #GaiaDR2 proper motions and literature radial velocities, allow us to derive the orbit of all those UMP stars. And that's where we got a mighty surprise! 7/19"", 'Common wisdom is that UMP stars, since they are supposed to be very old, have spheroid orbits (bulge, inner halo, outer halo; they have no preferred plane) as they were brought in by proto-Milky Way bits merging together and, later, through the accretion of dwarf galaxies. 8/19', 'We do find such expected orbits but we also find that 12 stars (out of 41, i.e. ~29%) of all known UMP stars appear to known where the plane of the MW is and stay confined to is! And most rotate in the same direction as the disk! 9/19', 'That can be seen in this plot showing the vertical action (how much stars journey away from the MW plane) vs. z component of the angular momentum (how much and which way they turn around the Galactic center). 10/19 https://t.co/cqIHrEWzOe', ""In this plot, the Sun would be at ~(1900,0) and there's even two star (salmon &amp; orange star symbols) with thin &amp; thick disk-like orbit, one of which is the recent and v. nice Schlaufman, Thompson &amp; @astrowizicist result from Monday (which we confirm) https://t.co/gspfCMQkPx 11/19"", ""Another way to look at this if you don't like action variables is to look at how far these stars journey above the plane of the MW (Z_max) vs. how far away they travel (d_apo). Large d_apo are most likely accreted stars from dwarf galaxies… 12/19 https://t.co/8AJmRhJPxr"", '… but we still clearly see this group of stars that stick to the MW plane and disk, even though their orbits are usually quite a bit more eccentric than disk stars. 13/19', ""So what does it all mean?\n1. Were these stars formed in the disk?\n2. Was there a massive accretion in the MW plane that brought those?\n3. Are we seeing how the MW disk put itself in place?\n4. Something else we haven't thought about?\n14/19"", 'We think 1. is unrealistic because the MW gas was very quickly enriched and it seems unlikely they were still pockets of pristine gas to form these UMPs when the HI disk was finally in place. 15/19', 'Scenario 2. is possible but we known that massive accretion isn\'t the recently discovered Gaia-Enceladus because the latter is retrograde and most of the ""MW planar"" UMP stars are prograde. And a massive accretion is likely to tilt the MW plane. 16/19', 'Scenario 3 has our preference but means that the MW disk hasn\'t been put together from many ""building blocks"" or else there would be no connection between the oldest stars brought by those and their HI gas assembling into a disk. 17/19', 'It would also mean that the MW has had a very quiet accretion life to avoid the disk plane tilting and these UMP stars being misaligned with the MW plane. 18/19', ""Whatever the valid scenario, it's fascinating and we're doing pure Galactic Archaeology, probing the MW formation via the oldest stars we can find in our surroundings. And none of this would have been possible without #GaiaDR2 and the tremendous work of the @ESAGaia team! 19/19"", '@KnudJahnke @ESAGaia It’s hard to know in detail because these stars were found in many different complex surveys. But my expectation in that they strongly *disfavor* the disk region by observing towards the halo. Makes it even more fascinating!', ""@KnudJahnke @ESAGaia We're planning to do exactly this with our Pristine survey (at slightly higher metallicities). It'll be far less of a headache since we know exactly what we select for our followup. :)"", '@KnudJahnke @ESAGaia It’s the student’s next paper. ;)']",https://arxiv.org/abs/1811.03099,"We use Gaia DR2 astrometric and photometric data, published radial velocities and MESA models to infer distances, orbits, surface gravities, and effective temperatures for all ultra metal-poor stars ($\FeH<-4.0$ dex) available in the literature. Assuming that these stars are old ($>11\Gyr$) and that they are expected to belong to the Milky Way halo, we find that these 42 stars (18 dwarf stars and 24 giants or sub-giants) are currently within $\sim20\kpc$ of the Sun and that they map a wide variety of orbits. A large fraction of those stars remains confined to the inner parts of the halo and was likely formed or accreted early on in the history of the Milky Way, while others have larger apocentres ($>30\kpc$), hinting at later accretion from dwarf galaxies. Of particular interest, we find evidence that a significant fraction of all known UMP stars ($\sim26$\%) are on prograde orbits confined within $3\kpc$ of the Milky Way plane ($J_z < 100 \kms \kpc$). One intriguing interpretation is that these stars belonged to the massive building block(s) of the proto-Milky Way that formed the backbone of the Milky Way disc. Alternatively, they might have formed in the early disc and have been dynamically heated, or have been brought into the Milky Way by one or more accretion events whose orbit was dragged into the plane by dynamical friction before disruption. The combination of the exquisite Gaia DR2 data and surveys of the very metal-poor sky opens an exciting era in which we can trace the very early formation of the Milky Way. ",Tracing the formation of the Milky Way through ultra metal-poor stars
72,1060818165423398912,623579783,Dr Jacinta Delhaize,"['We have a new paper out! Lead by Lana Ceraj (Uni. Zagreb), it examines what causes the radio light emitted by powerful Active Galactic Nuclei. Is it stars forming in these galaxies? Or is it the feeding supermassive black hole?\n<LINK>']",https://arxiv.org/abs/1811.02966,"We study a sample of 1,604 moderate-to-high radiative luminosity active galactic nuclei (HLAGN) selected at 3 GHz within the VLA-COSMOS 3 GHz Large Project. These were classified by combining multiple AGN diagnostics: X-ray data, mid-infrared data and broad-band spectral energy distribution fitting. We decompose the total radio 1.4 GHz luminosity ($\mathrm{L_{1.4\ GHz,TOT}}$) into the emission originating from star formation and AGN activity by measuring the excess in $\mathrm{L_{1.4\ GHz,TOT}}$ relative to the infrared-radio correlation of star-forming galaxies. To quantify the excess, for each source we calculate the AGN fraction ($\mathrm{f_{AGN}}$), the fractional contribution of AGN activity to $\mathrm{L_{1.4\ GHz,TOT}}$. The majority of the HLAGN, $(68.0\pm1.5)\%$, are dominated by star-forming processes ($f_{AGN}\leq0.5$), while $(32.0\pm1.5)\%$ are dominated by AGN-related radio emission ($0.5<f_{AGN}\leq1$). We use the AGN-related 1.4 GHz emission to derive the 1.4 GHz AGN luminosity functions of HLAGN. By assuming pure density and pure luminosity evolution models we constrain their cosmic evolution out to $z\sim6$, finding $\mathrm{\Phi^* (z) \propto (1+z)^{(2.64\pm0.10)+(-0.61\pm0.04) z}}$ and $\mathrm{L^* (z) \propto (1+z)^{(3.97\pm0.15) + (-0.92\pm0.06)z}}$. These evolutionary laws show that the number and luminosity density of HLAGN increased from higher redshifts ($z\sim6$) up to a maximum in the redshift range $ 1<z<2.5$, followed by a decline towards local values. By scaling the 1.4 GHz AGN luminosity to kinetic luminosity using the standard conversion, we estimate the kinetic luminosity density as a function of redshift. We compare our result to the semi-analytic models of radio mode feedback finding that this feedback could have played an important role in the context of AGN-host coevolution in HLAGN which show evidence of AGN-related radio emission ($f_{AGN}>0$). ","The VLA-COSMOS 3 GHz Large Project: Star formation properties and radio
  luminosity functions of AGN with moderate-to-high radiative luminosities out
  to $z\sim6$"
73,1060797051108253697,75249390,Axel Maas,"['We have published a new paper on subtle field-theoretical effects affecting the properties of the W and Z bosons, which are in principle testable in experiment, see <LINK> #np3']",https://arxiv.org/abs/1811.03395,"Gauge invariance requires even in the weak interactions that physical, observable particles are described by gauge-invariant composite operators. Such operators have the same structure as those describing bound states, and consequently the physical versions of the $W^\pm$, the $Z$, and the Higgs should have some kind of substructure. To test this consequence, we use lattice gauge theory to study the physical weak vector bosons off-shell, especially their form-factor and weak radius, and compare the results to the ones for the elementary particles. We find that the physical particles show substantial deviations from the structure of a point-like particle. At the same time the gauge-dependent elementary particles exhibit unphysical behavior. ",Exploratory study of the off-shell properties of the weak vector bosons
74,1060795208974168064,3098870333,Michael J. Baker,"['New paper out today, where the dark matter relic abundance is set by finite temperature effects <LINK>']",https://arxiv.org/abs/1811.03101,"In this work we consider a simple model for dark matter and identify regions of parameter space where the relic abundance is set via kinematic thresholds, which open and close due to thermal effects. We discuss instantaneous freeze-out, where dark matter suddenly freezes-out when the channel connecting dark matter to the thermal bath closes, and decaying dark matter, where dark matter freezes-out while relativistic and later decays when a kinematic threshold temporarily opens. These mechanisms can occur in the vicinity of a one-step or a two-step phase transition. In all cases thermal effects provide this dynamic behaviour, while ensuring that dark matter remains stable until the present day. ","Variations on the Vev Flip-Flop: Instantaneous Freeze-out and Decaying
  Dark Matter"
75,1060665251111170048,503452360,William Wang,"[""Here's our new arxiv version of #AAAI2019 paper: <LINK> A new task on zero-shot video captioning and see if your algorithm generalizes to unseen scenarios. #nlproc <LINK>""]",https://arxiv.org/abs/1811.02765,"Although promising results have been achieved in video captioning, existing models are limited to the fixed inventory of activities in the training corpus, and do not generalize to open vocabulary scenarios. Here we introduce a novel task, zero-shot video captioning, that aims at describing out-of-domain videos of unseen activities. Videos of different activities usually require different captioning strategies in many aspects, i.e. word selection, semantic construction, and style expression etc, which poses a great challenge to depict novel activities without paired training data. But meanwhile, similar activities share some of those aspects in common. Therefore, We propose a principled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video captioning, which learns to compose different experts based on different topic embeddings, implicitly transferring the knowledge learned from seen activities to unseen ones. Besides, we leverage external topic-related text corpus to construct the topic embedding for each activity, which embodies the most relevant semantic vectors within the topic. Empirical results not only validate the effectiveness of our method in utilizing semantic knowledge for video captioning, but also show its strong generalization ability when describing novel activities. ","Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video
  Captioning"
76,1060478239431372800,301426952,Arttu Rajantie 🇪🇺 🇫🇮 #FBPE,"['In this new paper we argue that a minimally coupled scalar field with no coupling to the Standard Model can account for the observed dark matter abundance in the Universe - T. Markkanen, A. Rajantie and T. Tenkanen, ""Spectator Dark Matter"" <LINK>']",https://arxiv.org/abs/1811.02586,"The observed dark matter abundance in the Universe can be fully accounted for by a minimally coupled spectator scalar field that was light during inflation and has sufficiently strong self-coupling. In this scenario, dark matter was produced during inflation by amplification of quantum fluctuations of the spectator field. The self-interaction of the field suppresses its fluctuations on large scales, and therefore avoids isocurvature constraints. The scenario does not require any fine-tuning of parameters. In the simplest case of a single real scalar field, the mass of the dark matter particle would be in the range $1~{\rm GeV}\lesssim m\lesssim 10^8~{\rm GeV}$, depending on the scale of inflation, and the lower bound for the quartic self-coupling is $\lambda\gtrsim 0.45$. ",Spectator Dark Matter
77,1060281478347509760,840339066340167680,Shany Danieli,"['I’m happy to share with you our new paper!\nIn this paper we revisit the size-luminosity relation of galaxies in the Coma cluster while taking into account the large population of Ultra Diffuse Galaxies.\n\n<LINK> <LINK>', 'We studied a size-limited sample of galaxies in the Coma cluster, using carefully-filtered CFHT images covering an area of 9 deg^2. The sample is complete to a surface brightness of µg,0≈25.0 mag arcsec−2 and includes all galaxies with an effective radius larger than 2 kpc. https://t.co/ccHwBhVgJ3', 'Unexpectedly, we find that red, large galaxies have a fairly uniform distribution in the size-luminosity plane: there is no peak at the absolute magnitude implied by the canonical size-luminosity relation. https://t.co/gI8u3WVKYo', 'Large, faint galaxies such as UDGs are far more common than large galaxies that are on the size-luminosity relation. An implication is that, for large galaxies, size is not an indicator of halo mass.', 'Finally, we show that the structure of faint large galaxies is different from that of bright large galaxies: at fixed large size, the Sersic index decreases with magnitude following the relation log10 n ≈ −0.067Mg − 0.989. https://t.co/uBeJF4dtfv']",https://arxiv.org/abs/1811.01962v1,"Galaxies are generally found to follow a relation between their size and luminosity, such that luminous galaxies typically have large sizes. The recent identification of a significant population of galaxies with large sizes but low luminosities (""ultra diffuse galaxies"", or UDGs) raises the question whether the inverse is also true, that is, whether large galaxies typically have high luminosities. Here we address this question by studying a size-limited sample of galaxies in the Coma cluster. We select red cluster galaxies with sizes $r_{\mathrm{eff}} > 2 \ \mathrm{kpc}$ down to $M_{g} \sim -13 \ \mathrm{mag}$ in an area of $9 \ \mathrm{deg}^2$, using carefully-filtered CFHT images. The sample is complete to a central surface brightness of $\mu_{g,0}\approx 25.0 \ \mathrm{mag\,arcsec}^{-2}$ and includes 90% of Dragonfly-discovered UDGs brighter than this limit. Unexpectedly, we find that red, large galaxies have a fairly uniform distribution in the size-luminosity plane: there is no peak at the absolute magnitude implied by the canonical size-luminosity relation. The number of galaxies within $\pm 0.5$ magnitudes of the canonical peak ($M_g = -19.69$ for $2<r_{\mathrm{eff}}<3$ kpc) is a factor of $\sim 9$ smaller than the number of fainter galaxies with $-19<M_g<-13$. Large, faint galaxies such as UDGs are far more common than large galaxies that are on the size-luminosity relation. An implication is that, for large galaxies, size is not an indicator of halo mass. Finally, we show that the structure of faint large galaxies is different from that of bright large galaxies: at fixed large size, the S\'ersic index decreases with magnitude following the relation $\log_{10} n \approx -0.067M_g-0.989$. ","] Revisiting the Size-Luminosity Relation in the Era of Ultra Diffuse
  Galaxies"
78,1060188480070926337,1705940262,Kent Bonsma-Fisher,['Check out our new paper on quantum fully homomorphic encryption!\n\n<LINK>'],https://arxiv.org/abs/1811.02149,"A fully homomorphic encryption system hides data from unauthorized parties, while still allowing them to perform computations on the encrypted data. Aside from the straightforward benefit of allowing users to delegate computations to a more powerful server without revealing their inputs, a fully homomorphic cryptosystem can be used as a building block in the construction of a number of cryptographic functionalities. Designing such a scheme remained an open problem until 2009, decades after the idea was first conceived, and the past few years have seen the generalization of this functionality to the world of quantum machines. Quantum schemes prior to the one implemented here were able to replicate some features in particular use-cases often associated with homomorphic encryption but lacked other crucial properties, for example, relying on continual interaction to perform a computation or leaking information about the encrypted data. We present the first experimental realisation of a quantum fully homomorphic encryption scheme. We further present a toy two-party secure computation task enabled by our scheme. Finally, as part of our implementation, we also demonstrate a post-selective two-qubit linear optical controlled-phase gate with a much higher post-selection success probability (1/2) when compared to alternate implementations, e.g. with post-selective controlled-$Z$ or controlled-$X$ gates (1/9). ","Experimental Demonstration of Quantum Fully Homomorphic Encryption with
  Application in a Two-Party Secure Protocol"
79,1060124541589090305,991380306,James Jackman,"['I have a new paper out on arXiv today, on the detection of a giant flare from a young (~2 Myr) M star with NGTS. The flare exhibited relatively rare quasi-periodic pulsations which we used to probe its behaviour: <LINK>', ""@WardHoward4Him @ProfAbelMendez Cheers! We were observing in the NGTS bandpass, which is between about 500 to 900 nm (https://t.co/pF1Ucby3A3) - so observations in g' would complement it really well, particularly for comparisons of flare amplitude.""]",https://arxiv.org/abs/1811.02008,"We present the detection of an energetic flare on the pre-main sequence M3 star NGTS J121939.5-355557, which we estimate as only 2 Myr old. The flare had an energy of $3.2\pm^{0.4}_{0.3}\times 10^{36}$erg and a fractional amplitude of $7.2\pm0.8$, making it one of the most energetic flares seen on an M star. The star is also X-ray active, in the saturated regime with $log L_{X}/L_{Bol} = -3.1$. In the flare peak we have identified multi-mode quasi-periodic pulsations formed of two statistically significant periods of approximately 320 and 660 seconds. This flare is one of the largest amplitude events to exhibit such pulsations. The shorter period mode is observed to start after a short-lived spike in flux lasting around 30 seconds, which would not have been resolved in Kepler or TESS short cadence modes. Our data shows how the high cadence of NGTS can be used to apply solar techniques to stellar flares and identify potential causes of the observed oscillations. We also discuss the implications of this flare for the habitability of planets around M star hosts and how NGTS can aid in our understanding of this. ","Detection of a giant flare displaying quasi-periodic pulsations from a
  pre-main sequence M star with NGTS"
80,1060116932836409344,1230746966,Adam Płoszaj,"['More flight connections and proximity of airport increase the number of coauthored scientific papers. More in my &amp; @katycns &amp; @everyxs new paper ""The impact of air transport availability on research collaboration: A case study of four universities"": <LINK> <LINK>']",https://arxiv.org/abs/1811.02106,This paper analyzes the impact of air transport connectivity and accessibility on scientific collaboration. ,"The impact of air transport availability on research collaboration: A
  case study of four universities"
81,1059823098516647937,700704725826572290,"Joe Guinness, valued customer","['New paper on multivariate spatial and spatial-temporal data. Computationally feasible methods for estimating flexible models from very large incomplete gridded data. Applications to #GOES16 data.\n<LINK> <LINK>', '@jbayers The people need to know']",https://arxiv.org/abs/1811.01280,"We propose computationally efficient methods for estimating stationary multivariate spatial and spatial-temporal spectra from incomplete gridded data. The methods are iterative and rely on successive imputation of data and updating of model estimates. Imputations are done according to a periodic model on an expanded domain. The periodicity of the imputations is a key feature that reduces edge effects in the periodogram and is facilitated by efficient circulant embedding techniques. In addition, we describe efficient methods for decomposing the estimated cross spectral density function into a linear model of coregionalization plus a residual process. The methods are applied to two storm datasets, one of which is from Hurricane Florence, which struck the souteastern United States in September 2018. The application demonstrates how fitted models from different datasets can be compared, and how the methods are computationally feasible on datasets with more than 200,000 total observations. ","Nonparametric Spectral Methods for Multivariate Spatial and
  Spatial-Temporal Data"
82,1059730423071367169,892059194240532480,Mikel Artetxe,"['Check out our new paper ""Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings"". Joint work with Holger Schwenk during my @facebookai internship.\n<LINK> <LINK>', '@mzeid4real @facebookai Not yet.']",https://arxiv.org/abs/1811.01136,"Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version. ","Margin-based Parallel Corpus Mining with Multilingual Sentence
  Embeddings"
83,1059708407777767424,1071548796,Cătălina Cangea,"['New paper with @PetarV_93, @thomaskipf to be presented at the #NIPS2018 Relational Representation Learning workshop: Towards Sparse Hierarchical Graph Classifiers (<LINK>)!', 'https://t.co/IvmUPh24dP']",https://arxiv.org/abs/1811.01287,"Recent advances in representation learning on graphs, mainly leveraging graph convolutional networks, have brought a substantial improvement on many graph-based benchmark tasks. While novel approaches to learning node embeddings are highly suitable for node classification and link prediction, their application to graph classification (predicting a single label for the entire graph) remains mostly rudimentary, typically using a single global pooling step to aggregate node features or a hand-designed, fixed heuristic for hierarchical coarsening of the graph structure. An important step towards ameliorating this is differentiable graph coarsening---the ability to reduce the size of the graph in an adaptive, data-dependent manner within a graph neural network pipeline, analogous to image downsampling within CNNs. However, the previous prominent approach to pooling has quadratic memory requirements during training and is therefore not scalable to large graphs. Here we combine several recent advances in graph neural network design to demonstrate that competitive hierarchical graph classification results are possible without sacrificing sparsity. Our results are verified on several established graph classification benchmarks, and highlight an important direction for future research in graph-based neural networks. ",Towards Sparse Hierarchical Graph Classifiers
84,1059699725648232449,1000951360404312065,Alexei Moiseev,"['The galactic wind in NGC 6286 was discovered at the 6-m telescope in 2004. Now it is only a member of a large team of wind outflows in our new paper: \n""Systematic study of outflows in the Local Universe using CALIFA: I. Sample selection...""  Lopez Coba +\n<LINK> <LINK>']",https://arxiv.org/abs/1811.01253,"We present a sample of 17 objects from the CALIFA survey where we find initial evidence of galactic winds based on their off-axis ionization properties. We identify the presence of outflows using various optical diagnostic diagrams (e.g., EW(H$\alpha$), [Nii]/H$\alpha$, [Sii]/H$\alpha$, [Oi]/H$\alpha$ line-ratio maps). We find that all 17 candidate outflow galaxies lie along the sequence of active star formation in the M$_\star$ vs. star-formation rate diagram, without a clear excess in the integrated SFR. The location of galaxies along the star-formation main sequence (SFMS) does not influence strongly the presence or not of outflows. The analysis of the star-formation rate density ($\Sigma_{\rm SFR}$) reveals that the CALIFA sources present higher values when compared with normal star-forming galaxies. The strength of this relation depends on the calibrator used to estimate the SFR. This excess in $\Sigma_{\rm SFR}$ is significant within the first effective radius supporting the idea that most outflows are driven by processes in the inner regions of a galaxy. We find that the molecular gas mass density ($\Sigma_\mathrm{gas}$) is a key parameter that plays an important role in the generation of outflows through its association with the local SFR. The canonical threshold reported for the generation of outflows -- $\Sigma_{\rm SFR}>0.1$ $\mathrm{M}_\odot \mathrm{yr}^{-1} \mathrm{kpc}^{-2}$ -- is only marginally exceeded in our sample. Within the Kennicutt-Schmidt diagram we propose a domain for galaxies hosting starburst-driven outflows defined by $\Sigma_{\rm SFR}>10^{-2} \,\mathrm{M}_\odot \mathrm{yr}^{-1} \mathrm{kpc}^{-2}$ and $\Sigma_\mathrm{gas}>10^{1.2} \, \mathrm{M}_\odot \mathrm{pc}^{-2}$ within a central kiloparcec region. ","Systematic study of outflows in the Local Universe using CALIFA: I.
  Sample selection and main properties"
85,1059503172270673921,157014702,Jessie Shelton,"['New paper today about footprints of dark matter self-interactions in our solar system: <LINK>', ""Let's put the fine print up front: this mechanism for discovering dark matter self-interactions only works if dark matter ALSO talks to nuclear matter at rates that would lead to detectable signals soon-ish.  2/N"", '#darkmatter that does interact with nuclei can scatter inside the sun or earth, and if it loses enough energy in the collision, it will then be gravitationally bound. In this way the sun and earth can build up dark matter balls in their cores 3/N', 'If #darkmatter can scatter off itself too, then in the sun that is just one more way that the bound population grow. But the Earth is tiny and its gravitational potential well is shallow.  In the earth turning on self-interactions EJECTS previously-captured dark matter 4/N', ""What Cristian Gaidau and I showed in today's paper is that comparing the size of the populations of #darkmatter bound to the sun and the earth is a sensitive diagnostic of dark matter self-interactions 5/N"", 'By sensitive, I mean: the impact of self-interactions can be very large (multiple orders of magnitude), in particular for astrophysically interesting cross-sections\n 6/6', '@davemckeen Thanks, Dave! Cristian did some really great work on this project.']",https://arxiv.org/abs/1811.00557,"Dark matter (DM) self-interactions affect the gravitational capture of DM in the Sun and Earth differently as a simple consequence of the differing kinematics of collisions within the two potential wells: the dominant effect of self-interactions in the Sun is to provide an additional channel for capture, while the dominant effect in the Earth is to eject previously captured DM. We point out that this simple observation can be used to deduce the existence of DM self-interactions by comparing the annihilation rates of DM gravitationally bound within the Sun and Earth. We compute the Sun and Earth annihilation fluxes for DM with spin-independent nuclear cross-sections and thermal annihilation cross-sections and demonstrate that, for cross-sections allowed by direct detection, self-interactions can easily suppress the expected Earth flux by multiple orders of magnitude. This suppression is potentially significant even for self-interaction cross-sections orders of magnitude below the Bullet Cluster bounds, making this solar system comparison a leading test of dark matter self-interactions. Additionally, we consider thermalization of the captured DM population with the nuclei of the capturing body in some detail, accounting for both nuclear and self-interactions, and point out some consequential and broadly applicable considerations. ",A Solar System Test of Self-Interacting Dark Matter
86,1059378565102387200,149096802,Oliver Lemon,"['Our new paper ""Neural Response Ranking for Social Conversation: A Data-Efficient Approach"" shows how conversation length can be used as an effective training signal for open-domain dialogue. Much easier to collect than user ratings! <LINK>']",https://arxiv.org/abs/1811.00967,"The overall objective of 'social' dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics, including social chit-chat. Apart from raw dialogue data, user-provided ratings are the most common signal used to train such systems to produce engaging responses. In this paper we show that social dialogue systems can be trained effectively from raw unannotated data. Using a dataset of real conversations collected in the 2017 Alexa Prize challenge, we developed a neural ranker for selecting 'good' system responses to user utterances, i.e. responses which are likely to lead to long and engaging conversations. We show that (1) our neural ranker consistently outperforms several strong baselines when trained to optimise for user ratings; (2) when trained on larger amounts of data and only using conversation length as the objective, the ranker performs better than the one trained using ratings -- ultimately reaching a Precision@1 of 0.87. This advance will make data collection for social conversational agents simpler and less expensive in the future. ","Neural Response Ranking for Social Conversation: A Data-Efficient
  Approach"
87,1059366993118330880,4861119593,Davide Maiorca,"['Our new paper ""Towards Robust Detection of Adversarial Infection Vectors: Lessons Learned in PDF Malware"" has just been released! Check it out! <LINK> #PDF #Malware #Security #MachineLearning #Adversarial @biggiobattista @PRA_Lab @GiorgioGiacinto']",https://arxiv.org/abs/1811.00830,"Malware still constitutes a major threat in the cybersecurity landscape, also due to the widespread use of infection vectors such as documents. These infection vectors hide embedded malicious code to the victim users, facilitating the use of social engineering techniques to infect their machines. Research showed that machine-learning algorithms provide effective detection mechanisms against such threats, but the existence of an arms race in adversarial settings has recently challenged such systems. In this work, we focus on malware embedded in PDF files as a representative case of such an arms race. We start by providing a comprehensive taxonomy of the different approaches used to generate PDF malware, and of the corresponding learning-based detection systems. We then categorize threats specifically targeted against learning-based PDF malware detectors, using a well-established framework in the field of adversarial machine learning. This framework allows us to categorize known vulnerabilities of learning-based PDF malware detectors and to identify novel attacks that may threaten such systems, along with the potential defense mechanisms that can mitigate the impact of such threats. We conclude the paper by discussing how such findings highlight promising research directions towards tackling the more general challenge of designing robust malware detectors in adversarial settings. ","Towards Adversarial Malware Detection: Lessons Learned from PDF-based
  Attacks"
88,1058457460128669696,548718054,Marc Khoury,"['Our new paper ""On the Geometry of Adversarial Examples"" gives a geometric framework for proving learning algorithms are resistant to adversarial attacks and highlights codimension as a key contributor to adversarial examples. With @dhadfieldmenell. <LINK>', ""@scheidegger @dhadfieldmenell Thanks Carlos! I put a lot of time into those figures and I'm particularly fond of Figure 7 because it makes the difference between the settings so clear."", ""@scheidegger @dhadfieldmenell Yes, absolutely. In the case of spheres you can lift them to the hyperbola in one dimension higher, in which case they're linearly separable for any amount of robustness desired. I discuss that in the beginning of 7.1."", ""@scheidegger @dhadfieldmenell More generally though, as you said, you don't know the right features to use. ReLU networks learn features that compute a decision boundary using piecewise-linear approximations, in which case the model size bounds in section 7 apply."", '@scheidegger @dhadfieldmenell Feel free to send me an email if you have any more questions.']",https://arxiv.org/abs/1811.00525,"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust. ",On the Geometry of Adversarial Examples
89,1058155262714859520,1954858226,Lloyd Knox,"['1/8 \nThe following is a haiku tweet thread about a new paper (<LINK>) with K. Aylor, M. Joy,  @cosmic_mar, S. Raghunathan, and K. Wu, called ""Sounds Discordant: Classical Distance Ladder and LCDM-based Determinations of the Cosmological Sound Horizon.""', '2/8 \nThe Sound Horizon.\nIt is how far sound travels\nIn Big Bang plasma.', ""3/8 \nEmpirically small.\nStandard model says it's big.\nIs the model wrong?"", '4/8 \nIf model is wrong,\nJust before end of plasma\nIs probably when.', '5/8\nBut how is it wrong?\nIs this the sound of darkness?\nWe do not yet know.', ""6/8\nIs the dark sector \nMore than 'matter', 'energy'?\nIs there a third piece?"", '7/8\nOr are we confused\nBy distance ladder foibles?\nThis seems unlikely.', ""8/8\nMicrowave mapping\nwill test viable models\nwithin five years' time.""]",https://arxiv.org/abs/1811.00537,"Type Ia Supernovae, calibrated by classical distance ladder methods, can be used, in conjunction with galaxy survey two-point correlation functions, to empirically determine the size of the sound horizon $r_{\rm s}$. Assumption of the $\Lambda$CDM model, together with data to constrain its parameters, can also be used to determine the size of the sound horizon. Using a variety of cosmic microwave background (CMB) datasets to constrain $\Lambda$CDM parameters, we find the model-based sound horizon to be larger than the empirically-determined one with a statistical significance of between 2 and 3$\sigma$, depending on the dataset. If reconciliation requires a change to the cosmological model, we argue that change is likely to be important in the two decades of scale factor evolution prior to recombination. Future CMB observations will therefore likely be able to test any such adjustments; e.g., a third generation CMB survey like SPT-3G can achieve a three-fold improvement in the constraints on $r_{\rm s}$ in the $\Lambda$CDM model extended to allow additional light degrees of freedom. ","Sounds Discordant: Classical Distance Ladder & $\Lambda$CDM -based
  Determinations of the Cosmological Sound Horizon"
90,1073648277562761216,2439364105,Fei Wang,['Check out our new paper accepted to #AAAI19 on compressing RNN with tensor ring decomposition <LINK>'],https://arxiv.org/abs/1811.07503,"Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term Memory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved promising performance in sequential data modeling. The hidden layers in RNNs can be regarded as the memory units, which are helpful in storing information in sequential contexts. However, when dealing with high dimensional input data, such as video and text, the input-to-hidden linear transformation in RNNs brings high memory usage and huge computational cost. This makes the training of RNNs unscalable and difficult. To address this challenge, we propose a novel compact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring decomposition (TRD) to reformulate the input-to-hidden transformation. Compared with other tensor decomposition methods, TR-LSTM is more stable. In addition, TR-LSTM can complete an end-to-end training and also provide a fundamental building block for RNNs in handling large input data. Experiments on real-world action recognition datasets have demonstrated the promising performance of the proposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art competitors. ","Compressing Recurrent Neural Networks with Tensor Ring for Action
  Recognition"
91,1068320456300736512,1047899041311412224,Francois Grondin,"['I propose a new sound source localization method, that I called SVD-PHAT, in this recently submitted paper: <LINK>']",https://arxiv.org/abs/1811.11785,"This paper introduces a new localization method called SVD-PHAT. The SVD-PHAT method relies on Singular Value Decomposition of the SRP-PHAT projection matrix. A k-d tree is also proposed to speed up the search for the most likely direction of arrival of sound. We show that this method performs as accurately as SRP-PHAT, while reducing significantly the amount of computation required. ",SVD-PHAT: A Fast Sound Source Localization Method
92,1068107730601340929,62044012,Michael Bronstein,"['Can one hear the shape of the drum? The answer to this classical question in spectral geometry is known to be negative. In our new paper we show that this can be done in practice - a powerful method for many hard problems in computer graphics and vision! <LINK> <LINK>', '@docmilanfar Thanks for the references Peyman - we will have a look']",https://arxiv.org/abs/1811.11465,"The question whether one can recover the shape of a geometric object from its Laplacian spectrum ('hear the shape of the drum') is a classical problem in spectral geometry with a broad range of implications and applications. While theoretically the answer to this question is negative (there exist examples of iso-spectral but non-isometric manifolds), little is known about the practical possibility of using the spectrum for shape reconstruction and optimization. In this paper, we introduce a numerical procedure called isospectralization, consisting of deforming one shape to make its Laplacian spectrum match that of another. We implement the isospectralization procedure using modern differentiable programming techniques and exemplify its applications in some of the classical and notoriously hard problems in geometry processing, computer vision, and graphics such as shape reconstruction, pose and style transfer, and dense deformable correspondence. ","Isospectralization, or how to hear shape, style, and correspondence"
93,1063712263696236544,312448486,Dr. Karan Jani,"['New paper in collaboration with @CNRS: a CubeSat gravitational wave space-mission to find the elusive intermediate-sized black holes in our universe. \n\nA cost effective mission that can be launched within next few years! \n\nSubmitted to @CQGplus: <LINK> <LINK>', 'The mission will hunt for black holes 100 to Million times the mass of our Sun to cosmological distances. \n\nA sweet spot that is currently being missed by other proposed experiments. https://t.co/n2DjAPI974', 'This new kid in the block is called #SAGE: SagnAc interferometer Gravitational wavE space observatory.\n\nIt will have peak sensitivity in the gravitational wave spectrum between @LISACommunity and @LIGO-@ego_virgo. https://t.co/5ZsfmGuoMn']",https://arxiv.org/abs/1811.04743,"SAGE (SagnAc interferometer for Gravitational wavE) is a project for a space observatory based on multiple 12-U CubeSats in geosynchronous orbit. The objective is a fast track mission which would fill the observational gap between LISA and ground based observatories. With albeit a lower sensitivity, it would allow early investigation of the nature and event rate of intermediate-mass black hole (IMBH) mergers, constraining our understanding of the universe formation by probing the building up of IMBH up to supermassive black holes. Technically, the CubeSats would create a triangular Sagnac interferometer with 140.000km roundtrip arm length, optimized to be sensitive to gravitational waves at frequencies between 10mHz and 2Hz. The nature of the Sagnac measurement makes it almost insensitive to position error, enabling the use of spacecrafts in ballistic trajectories. The light source and recombination units of the interferometer are based on compact fibered technologies without bulk optics. A peak sensitivity of 23 pm/sqrt(Hz) is expected at 1Hz assuming a 200mW internal laser source and 10-centimeter diameter apertures. Because of the absence of a test mass, the main limitation would come from the non-gravitational forces applied on the spacecrafts. However, conditionally upon our ability to partially post-process the effect of solar wind and solar pressure, SAGE would allow detection of gravitational waves with strains as low as a few 1e-19 within the 0.1 to 1Hz range. Averaged over the entire sky, and including the antenna gain of the Sagnac interferometer, the SAGE observatory would sense equal mass black hole mergers in the 1e4 to 1e6 solar masses range up to a luminosity distance of 800Mpc. Additionally, coalescence of stellar black holes (10Msun) around SMBH (IMBH) forming extreme (intermediate) mass ratio inspirals could be detected within a sphere of radius 200Mpc. ",SAGE: finding IMBH in the black hole desert
94,1063100286686781441,14975979,Abhimat Gautam,"['Here’s a really brief summary thread of my work and results. Check out the paper for full details behind these results and graphs: <LINK>\n\nWe developed some new techniques to get precise stellar photometry from adaptive optics images of the Galactic center!\n(1/4) <LINK>', 'With the precise photometry, we found that roughly half of all stars in our sample are variable over the 11.5 years of our experiment!\n(2/4) https://t.co/ZVCU7S4NnI', 'We recovered the two known young eclipsing binary star systems at the Galactic center, and put a tight lower limit on the eclipsing binary fraction at the Galactic center.\n(3/4) https://t.co/kzHKOuO81e', 'We discovered a new periodic variable at the Galactic center, with a roughly 39 day period! Figuring out what is causing this variability still requires a little more work!\n(4/4) https://t.co/DaoCW3yV3G']",https://arxiv.org/abs/1811.04898,"We present a $\approx 11.5$ year adaptive optics (AO) study of stellar variability and search for eclipsing binaries in the central $\sim 0.4$ pc ($\sim 10''$) of the Milky Way nuclear star cluster. We measure the photometry of 563 stars using the Keck II NIRC2 imager ($K'$-band, $\lambda_0 = 2.124 \text{ } \mu \text{m}$). We achieve a photometric uncertainty floor of $\Delta m_{K'} \sim 0.03$ ($\approx 3\%$), comparable to the highest precision achieved in other AO studies. Approximately half of our sample ($50 \pm 2 \%$) shows variability. $52 \pm 5\%$ of known early-type young stars and $43 \pm 4 \%$ of known late-type giants are variable. These variability fractions are higher than those of other young, massive star populations or late-type giants in globular clusters, and can be largely explained by two factors. First, our experiment time baseline is sensitive to long-term intrinsic stellar variability. Second, the proper motion of stars behind spatial inhomogeneities in the foreground extinction screen can lead to variability. We recover the two known Galactic center eclipsing binary systems: IRS 16SW and S4-258 (E60). We constrain the Galactic center eclipsing binary fraction of known early-type stars to be at least $2.4 \pm 1.7\%$. We find no evidence of an eclipsing binary among the young S-stars nor among the young stellar disk members. These results are consistent with the local OB eclipsing binary fraction. We identify a new periodic variable, S2-36, with a 39.43 day period. Further observations are necessary to determine the nature of this source. ",An Adaptive Optics Survey of Stellar Variability at the Galactic Center
95,1063099026621390848,14975979,Abhimat Gautam,"['I just finished a new research paper, studying the variability of stars and looking for binary stars at the center of our galaxy!\n\nThe paper is publicly available here: <LINK>\n(you can download the full paper, with all the details, here: <LINK> )', 'I also wrote a short post on my blog explaining the main results from my paper in a little more accessible way if you’re not an astronomer! https://t.co/OWYeWd20Ww\n\n(I’m really trying to improve my scientific writing for the general public, and would love feedback on this post!)', '@amsingh2 Thank you Baba!']",https://arxiv.org/abs/1811.04898,"We present a $\approx 11.5$ year adaptive optics (AO) study of stellar variability and search for eclipsing binaries in the central $\sim 0.4$ pc ($\sim 10''$) of the Milky Way nuclear star cluster. We measure the photometry of 563 stars using the Keck II NIRC2 imager ($K'$-band, $\lambda_0 = 2.124 \text{ } \mu \text{m}$). We achieve a photometric uncertainty floor of $\Delta m_{K'} \sim 0.03$ ($\approx 3\%$), comparable to the highest precision achieved in other AO studies. Approximately half of our sample ($50 \pm 2 \%$) shows variability. $52 \pm 5\%$ of known early-type young stars and $43 \pm 4 \%$ of known late-type giants are variable. These variability fractions are higher than those of other young, massive star populations or late-type giants in globular clusters, and can be largely explained by two factors. First, our experiment time baseline is sensitive to long-term intrinsic stellar variability. Second, the proper motion of stars behind spatial inhomogeneities in the foreground extinction screen can lead to variability. We recover the two known Galactic center eclipsing binary systems: IRS 16SW and S4-258 (E60). We constrain the Galactic center eclipsing binary fraction of known early-type stars to be at least $2.4 \pm 1.7\%$. We find no evidence of an eclipsing binary among the young S-stars nor among the young stellar disk members. These results are consistent with the local OB eclipsing binary fraction. We identify a new periodic variable, S2-36, with a 39.43 day period. Further observations are necessary to determine the nature of this source. ",An Adaptive Optics Survey of Stellar Variability at the Galactic Center
96,1063090044586475520,736190574328467457,Aline Vidotto,['Is ZDI a reliable method to recover the magnetic field of stars? PhD student Lisa Lehmann answers this in her new (nearly female-only!) paper <LINK> @onlygaitee @SMTG_StAndrews <LINK>'],https://arxiv.org/abs/1811.03703,"The large-scale magnetic fields of stars can be obtained with the Zeeman-Doppler-Imaging (ZDI) technique, but their interpretation is still challenging as the contribution of the small-scale field or the reliability of the reconstructed field properties is still not fully understood. To quantify this, we use 3D non-potential magnetic field simulations for slowly rotating solar-like stars as inputs to test the capabilities of ZDI. These simulations are based on a flux transport model connected to a non-potential coronal evolution model using the observed solar flux emergence pattern. We first compare four field prescriptions regarding their reconstruction capabilities and investigate the influence of the spatial resolution of the input maps on the corresponding circularly polarised profiles. We then generate circularly polarised spectra based on our high resolution simulations of three stellar models with different activity levels, and reconstruct their large-scale magnetic fields using a non-potential ZDI code assuming two different stellar inclination angles. Our results show that the ZDI technique reconstructs the main features of slowly rotating solar-like stars but with $\sim\,$one order of magnitude less magnetic energy. The large-scale field morphologies are recovered up to harmonic modes $\ell \sim 5$, especially after averaging over several maps for each stellar model. While ZDI is not able to reproduce the input magnetic energy distributions across individual harmonic modes, the fractional energies across the modes are generally within $20\,\%$ agreement. The fraction of axisymmetric and toroidal field tends to be overestimated for stars with solar flux emergence patterns for more pole-on inclination angles. ","Observing the simulations: Applying ZDI to 3D non-potential magnetic
  field simulations"
97,1062688893890387968,430217063,Dúalta Ó Fionnagáin,"['Want a very engaging and interesting read for the afternoon??! Read our  new paper: <LINK> ! We simulate the winds  of solar-type stars and predict their thermal radio emission! @tcdastro  @TCD_physics @AlineVidotto @pascalou_petit @ichec <LINK>', 'Example of our wind simulation for HD190771, wind velocity is in yellow/blue and Alfven surface is shown in orange! https://t.co/oNtPGColuq', 'Predicted stellar wind thermal radio emission with some current and future telescope sensitivities:\n@SKA_telescope #VLA https://t.co/egx7bVGDpv', '@jadwiszj @SKA_telescope Thanks, I used python with seaborn! Messed around with some default settings. Seaborn is great!']",https://arxiv.org/abs/1811.05356,"In this work, we simulate the evolution of the solar wind along its main sequence lifetime and compute its thermal radio emission. To study the evolution of the solar wind, we use a sample of solar mass stars at different ages. All these stars have observationally-reconstructed magnetic maps, which are incorporated in our 3D magnetohydrodynamic simulations of their winds. We show that angular-momentum loss and mass-loss rates decrease steadily on evolutionary timescales, although they can vary in a magnetic cycle timescale. Stellar winds are known to emit radiation in the form of thermal bremsstrahlung in the radio spectrum. To calculate the expected radio fluxes from these winds, we solve the radiative transfer equation numerically from first principles. We compute continuum spectra across the frequency range 100 MHz - 100 GHz and find maximum radio flux densities ranging from 0.05 - 8.3 $\mu$Jy. At a frequency of 1 GHz and a normalised distance of d = 10 pc, the radio flux density follows 0.24 $(\Omega/\Omega_{\odot})^{0.9}$ (d/[10pc])$^2$ $\mu$Jy, where $\Omega$ is the rotation rate. This means that the best candidates for stellar wind observations in the radio regime are faster rotators within distances of 10 pc, such as $\kappa^1$ Ceti (2.83 $\mu$Jy) and $\chi^1$ Ori (8.3 $\mu$Jy). These flux predictions provide a guide to observing solar-type stars across the frequency range 0.1 - 100 GHz in the future using the next generation of radio telescopes, such as ngVLA and SKA. ",The Solar Wind in Time II: 3D stellar wind structure and radio emission
98,1062440331973705728,4526863835,Nikos Deligiannis,['Our new paper on #deeplearning for hyperlocal #AirPollution estimation: <LINK> @imec_int @VUBrussel'],https://arxiv.org/abs/1811.01662,"Inferring air quality from a limited number of observations is an essential task for monitoring and controlling air pollution. Existing inference methods typically use low spatial resolution data collected by fixed monitoring stations and infer the concentration of air pollutants using additional types of data, e.g., meteorological and traffic information. In this work, we focus on street-level air quality inference by utilizing data collected by mobile stations. We formulate air quality inference in this setting as a graph-based matrix completion problem and propose a novel variational model based on graph convolutional autoencoders. Our model captures effectively the spatio-temporal correlation of the measurements and does not depend on the availability of additional information apart from the street-network topology. Experiments on a real air quality dataset, collected with mobile stations, shows that the proposed model outperforms state-of-the-art approaches. ","Matrix Completion With Variational Graph Autoencoders: Application in
  Hyperlocal Air Quality Inference"
99,1060596260908855296,112333725,Nesreen K. Ahmed,"['New paper with my summer student Guixiang on ""Similarity Learning with Higher-Order Proximity for Brain Network Analysis"" <LINK> - a short abstract will be presented at WiML 2018 #MachineLearning #graphs # networkscience #DeepLearning']",https://arxiv.org/abs/1811.02662,"Learning a similarity metric has gained much attention recently, where the goal is to learn a function that maps input patterns to a target space while preserving the semantic distance in the input space. While most related work focused on images, we focus instead on learning a similarity metric for neuroimages, such as fMRI and DTI images. We propose an end-to-end similarity learning framework called Higher-order Siamese GCN for multi-subject fMRI data analysis. The proposed framework learns the brain network representations via a supervised metric-based approach with siamese neural networks using two graph convolutional networks as the twin networks. Our proposed framework performs higher-order convolutions by incorporating higher-order proximity in graph convolutional networks to characterize and learn the community structure in brain connectivity networks. To the best of our knowledge, this is the first community-preserving similarity learning framework for multi-subject brain network analysis. Experimental results on four real fMRI datasets demonstrate the potential use cases of the proposed framework for multi-subject brain analysis in health and neuropsychiatric disorders. Our proposed approach achieves an average AUC gain of 75% compared to PCA, an average AUC gain of 65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared to S-GCN across the four datasets, indicating promising application in clinical investigation and brain disease diagnosis. ","Similarity Learning with Higher-Order Graph Convolutions for Brain
  Network Analysis"
100,1059722316354072577,14750065,Ittai Abraham,"['New paper on Validated Asynchronous Byzantine Agreement:\n<LINK>\n\nIt has optimal resilience of n=3f+1, reaches agreement in expected O(1) time and uses only O(n^2) expected word communication']",https://arxiv.org/abs/1811.01332,We provide a new protocol for Validated Asynchronous Byzantine Agreement. Validated (multi-valued) Asynchronous Byzantine Agreement is a key building block in constructing Atomic Broadcast and fault-tolerant state machine replication in the asynchronous setting. Our protocol can withstand the optimal number $f<n/3$ of Byzantine failures and reaches agreement in the asymptotically optimal expected $O(1)$ running time. Honest parties in our protocol send only an expected $O(n^2)$ messages where each message contains a value and a constant number of signatures. Hence our total expected communication is $O(n^2)$ words. The best previous result of Cachin et al. from 2001 solves Validated Byzantine Agreement with optimal resilience and $O(1)$ expected time but with $O(n^3)$ expected word communication. Our work addresses an open question of Cachin et al. from 2001 and improves the expected word communication from $O(n^3)$ to the asymptotically optimal $O(n^2)$. ,"Validated Asynchronous Byzantine Agreement with Optimal Resilience and
  Asymptotically Optimal Time and Word Communication"
101,1068352434051772416,501353692,Travis Metcalfe,['Ever wonder what happens beyond the Skumanich relations? We’re starting to find out... <LINK> <LINK>'],https://arxiv.org/abs/1811.11905,"Nearly half a century has passed since the initial indications that stellar rotation slows while chromospheric activity weakens with a power-law dependence on age, the so-called Skumanich relations. Subsequent characterization of the mass-dependence of this behavior up to the age of the Sun led to the advent of gyrochronology, which uses the rotation rate of a star to infer its age from an empirical calibration. The efficacy of the method relies on predictable angular momentum loss from a stellar wind entrained in the large-scale magnetic field produced by global dynamo action. Recent observational evidence suggests that the global dynamo begins to shut down near the middle of a star's main-sequence lifetime, leading to a disruption in the production of large-scale magnetic field, a dramatic reduction in angular momentum loss, and a breakdown of gyrochronology relations. For solar-type stars this transition appears to occur near the age of the Sun, when rotation becomes too slow to imprint Coriolis forces on the global convective patterns, reducing the shear induced by differential rotation, and disrupting the large-scale dynamo. We use data from Barnes (2007) to reveal the signature of this transition in the observations that were originally used to validate gyrochronology. We propose that chromospheric activity may ultimately provide a more reliable age indicator for older stars, and we suggest that asteroseismology can be used to help calibrate activity-age relations for field stars beyond the middle of their main-sequence lifetimes. ",Understanding the Limitations of Gyrochronology for Old Field Stars
102,1068260003784679424,901427649360392198,QiLi,"[""Excited to work with awesome guys and produce an interesting research which is available on arXiv (<LINK> ) now. We used robust 3D objects to simulate the real objects and it's not difficult to find the poses that fool the DNN. <LINK>""]",https://arxiv.org/abs/1811.11553,"Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO. ","Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses
  of Familiar Objects"
103,1068151984023310336,1019760963569049601,Almog Yalinewich,"['Our paper is on the arxiv. We study ripples in the stellar wind of a supernova progenitor in a binary and use them to infer the properties of the binary before the explosion. <LINK> <LINK>', 'from the cutting room floor https://t.co/acK1CTgUv1']",https://arxiv.org/abs/1811.11193,"Type II supernova progenitors are expected to emit copious amounts of mass in a dense stellar wind prior to the explosion. When the progenitor is a member of a binary, the orbital motion modulates the density of this wind. When the progenitor explodes, the high-velocity ejecta collides with the modulated wind, which in turn produces a modulated radio signal. In this work we derive general analytic relations between the parameters of the radio signal modulations and binary parameter in the limit of large member mass ratio. We use these relations to infer the semi major axis of SN1979c and a lower bound for the mass of the companion. We further constrain the analytic estimates by numerical simulations using the AMUSE framework. In these calculations we simulate the progenitor binary system including the wind and the gravitational effect of a companion star. The simulation output is compared to the observed radio signal in supernova SN1979C. We find that it must have been a binary with an orbital period of about 2000 year. If the exploding star evolved from a $\sim 18 M_{\odot}$ zero-age main-sequence at solar metalicity, we derive a companion mass of $5$ to $12 M_{\odot}$ in an orbit with an eccentricity lower than about 0.8. ",The Signature of a Windy Radio Supernova Progenitor in a Binary System
104,1067563659864330240,39525395,Aditya Grover,"['In our #NeurIPS2018 paper, we propose a streamlined variational inference approach for branching during search and demonstrate its utility on some of the hardest constraint satisfaction problems. \n<LINK> \nCode: <LINK>\nw/ Tudor Achim &amp; @ermonste <LINK>']",http://arxiv.org/abs/1811.09813,"Several algorithms for solving constraint satisfaction problems are based on survey propagation, a variational inference scheme used to obtain approximate marginal probability estimates for variable assignments. These marginals correspond to how frequently each variable is set to true among satisfying assignments, and are used to inform branching decisions during search; however, marginal estimates obtained via survey propagation are approximate and can be self-contradictory. We introduce a more general branching strategy based on streamlining constraints, which sidestep hard assignments to variables. We show that streamlined solvers consistently outperform decimation-based solvers on random k-SAT instances for several problem sizes, shrinking the gap between empirical performance and theoretical limits of satisfiability by 16.3% on average for k=3,4,5,6. ",Streamlining Variational Inference for Constraint Satisfaction Problems
105,1065271383624699904,105817235,Jose Sabater,['Our latest study on the local radio AGN population with LOFAR is in arxiv: <LINK> We find that the most massive galaxies are always switched on even if it is at relatively low radio levels #LOFAR #LoTSS'],https://arxiv.org/abs/1811.05528,"This paper presents a study of the local radio source population, by cross-comparing the data from the first data release (DR1) of the LOFAR Two-Metre Sky Survey (LoTSS) with the Sloan Digital Sky Survey (SDSS) DR7 main galaxy spectroscopic sample. The LoTSS DR1 provides deep data (median rms noise of 71 $\mathrm{\mu}$Jy at 150 MHz) over 424 square degrees of sky, which is sufficient to detect 10615 (32 per cent) of the SDSS galaxies over this sky area. An improved method to separate active galactic nuclei (AGN) accurately from sources with radio emission powered by star formation (SF) is developed and applied, leading to a sample of 2121 local ($z < 0.3$) radio AGN. The local 150 MHz luminosity function is derived for radio AGN and SF galaxies separately, and the good agreement with previous studies at 1.4 GHz suggests that the separation method presented is robust. The prevalence of radio AGN activity is confirmed to show a strong dependence on both stellar and black hole masses, remarkably reaching a fraction of 100 per cent of the most massive galaxies ($> 10^{11} \mathrm{M_{\odot}}$) displaying radio-AGN activity with $L_{\rm 150 MHz} \geq 10^{21}$W Hz$^{-1}$; thus, the most massive galaxies are always switched on at some level. The results allow the full Eddington-scaled accretion rate distribution (a proxy for the duty cycle) to be probed for massive galaxies. More than 50 per cent of the energy is released during the $\le 2$ per cent of the time spent at the highest accretion rates, $L_{\mathrm{mech}}/L_{\mathrm{Edd}} > 10^{-2.5}$. Stellar mass is shown to be a more important driver of radio-AGN activity than black hole mass, suggesting a possible connection between the fuelling gas and the surrounding halo. This result is in line with models in which these radio AGN are essential for maintaining the quenched state of galaxies at the centres of hot gas haloes. ","The LoTSS view of radio AGN in the local Universe. The most massive
  galaxies are always switched on"
106,1065195965198540800,957689165902118912,Alexandre Dauphin,"['Fresh from the arXivs: In this work, we study an interaction induced topological insulator on a dynamical lattice. We characterize the topology of this system with both bulk and edge observables. \n<LINK>']",https://arxiv.org/abs/1811.08392,"In this work, we study a one-dimensional model of interacting bosons coupled to a dynamical $\mathbb{Z}_2$ field, the $\mathbb{Z}_2$ Bose-Hubbard model, and analyze the interplay between spontaneous symmetry breaking and topological symmetry protection. In a previous work, we showed how this model exhibits a spontaneous breaking of the translational symmetry through a bosonic Peierls transition. Here we find how, at half filling, the resulting phase also displays topological features that coexist with the presence of long-range order and yields a topological bond order wave. Using both analytical and numerical methods, we describe the properties of this phase, showing that it cannot be adiabatically connected to a bosonic topological phase with vanishing Hubbard interactions, and thus constitutes an instance of an interaction-induced symmetry-breaking topological insulator. ","Symmetry-Breaking Topological Insulators in the $\mathbb{Z}_2$
  Bose-Hubbard Model"
107,1062799885458501632,36653441,Johan Ugander,"['New on arxiv, ""Choosing to grow a graph"" with @janovergoor &amp; @austinbenson. We show that many widely-studied network formation models (PA, fitness, triadic closure, etc.) can be formulated as discrete choice models --&gt; model selection and inference. <LINK> <LINK>']",https://arxiv.org/abs/1811.05008,"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a ""choice"" made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age. ",Choosing to Grow a Graph: Modeling Network Formation as Discrete Choice
108,1062716984805244933,378228706,Hannah Wakeford,"['Our #paper ""Disentangling the planet from the star in late type M dwarfs: A case study of TRAPPIST-1g"" is out  <LINK> We use the star to work out various possible contrast effects, and the geometry of the transit to rule them out. Then let the planet decide! <LINK>', 'In this study we find that #TRAPPIST1 is best described as a 0.08Msun, 0.117Rsun, M8V star with a photospheric Teff=2400K, with ~35% @ 3000K &amp; &lt;3% @ ~5800K\nBottom panel - Left: data black, model green. Right: full res models used broken into components https://t.co/8NX7wkqsW7', 'The reconstructed stellar flux results in 11 possible scenarios for the area occulted by the planet as it transits the star. We are able to rule out 8/11 based on the geometry and the planet #TRAPPIST1 #paper https://t.co/nm3ZhPF07F', 'We are further able to use planetary models to rule out the 2Tc+m scenario. Because, while models fit in the wavelengths we measured with @NASAHubble if you include @NASAspitzer this one breaks down #TRAPPIST1 https://t.co/Qv4tNbXB2j', 'This leaves the final scenarios 2T and 3T which result in the same transmission spectrum for the planet, where no stellar flux is contaminating it, as the remaining most probable result where 3T is the favored scenario for the star #TRAPPIST1 https://t.co/USiN6eob5R', 'The series of steps we take in the case study on #TRAPPIST1g can be applied to any star planet combination, but is most effective for late type M stars where stellar molecular features will mimic the planet signals.', 'In each of the steps we detail the important procedures to use to make sure that units are correctly calculated and that each assumption is explored in relation to the data available.', 'and finally I will say this work would not have been possible without my coauthors especially @NikoleKLewis, @Onoastrmer @Of_FallingStars, @NatashaBatalha, Giovanni Bruno, Jules Fowler, and Jeff Valenti who joined me in the #TRAPPISTbunker', 'It also makes for a very pretty title slide #TRAPPIST1g #scicomm https://t.co/q6n2TNAFTI']",https://arxiv.org/abs/1811.04877,"The atmospheres of late M stars represent a significant challenge in the characterization of any transiting exoplanets due to the presence of strong molecular features in the stellar atmosphere. TRAPPIST-1 is an ultra-cool dwarf, host to seven transiting planets, and contains its own molecular signatures which can potentially be imprinted on planetary transit lightcurves due to inhomogeneities in the occulted stellar photosphere. We present a case study on TRAPPIST-1g, the largest planet in the system, using a new observation together with previous data, to disentangle the atmospheric transmission of the planet from that of the star. We use the out-of-transit stellar spectra to reconstruct the stellar flux based on one-, two-, and three-temperature components. We find that TRAPPIST-1 is a 0.08 M$_*$, 0.117 R$_*$, M8V star with a photospheric effective temperature of 2400 K, with ~35% 3000 K spot coverage and a very small fraction, <3%, of ~5800 K hot spot. We calculate a planetary radius for TRAPPIST-1g to be Rp = 1.124 R$_\oplus$ with a planetary density of $\rho_p$ = 0.8214 $\rho_\oplus$. Based on the stellar reconstruction there are eleven plausible scenarios for the combined stellar photosphere and planet transit geometry; in our analysis we are able to rule out 8 of the 11 scenarios. Using planetary models we evaluate the remaining scenarios with respect to the transmission spectrum of TRAPPIST-1g. We conclude that the planetary transmission spectrum is likely not contaminated by any stellar spectral features, and are able to rule out a clear solar H2/He-dominated atmosphere at greater than 3-sigma. ","Disentangling the planet from the star in late type M dwarfs: A case
  study of TRAPPIST-1g"
109,1061797951880740866,130881465,Alex Nitz,"[""In our paper just on the arxiv, we've investigated possible excess correlations at the time of #GW150914 in @LIGO data. We find that when you subtract a best fit waveform based on GR,  there are no statistically significant correlations in the residual. \n\n<LINK>"", 'To help people reproduce our results, we have supplementary materials on github. These include notebooks to generate all the figures from our paper.\n\nhttps://t.co/hn9MZNfCjR']",https://arxiv.org/abs/1811.04071,"We use the Pearson cross-correlation statistic proposed by Liu and Jackson, and employed by Creswell et al., to look for statistically significant correlations between the LIGO Hanford and Livingston detectors at the time of the binary black hole merger GW150914. We compute this statistic for the calibrated strain data released by LIGO, using both the residuals provided by LIGO and using our own subtraction of a maximum-likelihood waveform that is constructed to model binary black hole mergers in general relativity. To assign a significance to the values obtained, we calculate the cross-correlation of both simulated Gaussian noise and data from the LIGO detectors at times during which no detection of gravitational waves has been claimed. We find that after subtracting the maximum likelihood waveform there are no statistically significant correlations between the residuals of the two detectors at the time of GW150914. ","Investigating the noise residuals around the gravitational wave event
  GW150914"
110,1060809374757806080,232169311,Mathias Payer,"[""Want to protect the backward edge against ROP? CFI is too weak and parallel shadow stacks have security implications. We did a large security/performance study of shadow stacks (and implemented all of them). See the preprint at: <LINK> We'd love to hear feedback!"", ""A note on the paper: we started 18 months ago; it was narrowly rejected at OSDI. We fixed reviewer's issues and it was just (very) narrowly rejected at ASPLOS. We want to get the word out (and know others have started working on the same topic). So enjoy the read and comment!"", ""Shadow stacks are an efficient against ROP. They can be implemented at low overhead and, with a little help from the hardware, give absolute protection against ROP. The only accepted (cited) paper is the AsiaCCS parallel shadow stack one. It's time for a reevaluation."", ""@ovahldy IMIX was published at CCS, we're aware of it but the submission deadline for ASPLOS was before CCS; we'll cite IMIX if we submit again. Unfortunately, there's no HW of IMIX available yet :)"", ""@pramod_bhatotia @ovahldy Nice, wasn't aware of your sigmetrics paper. We'll add it and discuss it as RW! Nice study!"", ""@pramod_bhatotia @ovahldy We thought we had the nice trifecta: it was systems related as a generic strong defense against ROP, it was compiler related as we implement it on LLVM and it was architecture related as a small HW tweak can really improve performance. Plus it's a neat security angle 😊"", '@pramod_bhatotia @ovahldy The lateral exposure from sigmetrics is a little limited. I missed your work BTW which is quite related 😪', ""@pramod_bhatotia @ovahldy There's Oakland's SoK but it's super narrow. IMO measurement papers (or papers that reimplement systems w/o open-source prototypes) should be OK and would provide an interesting angle."", '@nikitab Right but ensuring security (integrity) and making it low overhead goes beyond a class project', ""@gianluca_string @pramod_bhatotia @ovahldy People are very opinionated, that's why we evaluated the full design space and optimized for very low overhead and high security guarantees. This was not easy"", ""@michioh @pramod_bhatotia @ovahldy That's why we evaluated the full design space. It also shows that parallel stacks are not necessarily better. Interestingly one of the reviewers complained that photonic is not real enough 🙄"", '@nikitab What architecture did the class project use? This could be a fun task for an upcoming OS class 😊', ""@michioh @pramod_bhatotia @ovahldy Hahahahaha, yeah, I wished. Note that we received very thorough reviews from ASPLOS and that they were mostly of high quality! We'll definitively apply that feedback and see what we do with the paper. But other works are popping up that started after ours and that's frustrating.""]",https://arxiv.org/abs/1811.03165,"Control-Flow Hijacking attacks are the dominant attack vector against C/C++ programs. Control-Flow Integrity (CFI) solutions mitigate these attacks on the forward edge,i.e., indirect calls through function pointers and virtual calls. Protecting the backward edge is left to stack canaries, which are easily bypassed through information leaks. Shadow Stacks are a fully precise mechanism for protecting backwards edges, and should be deployed with CFI mitigations. We present a comprehensive analysis of all possible shadow stack mechanisms along three axes: performance, compatibility, and security. For performance comparisons we use SPEC CPU2006, while security and compatibility are qualitatively analyzed. Based on our study, we renew calls for a shadow stack design that leverages a dedicated register, resulting in low performance overhead, and minimal memory overhead, but sacrifices compatibility. We present case studies of our implementation of such a design, Shadesmar, on Phoronix and Apache to demonstrate the feasibility of dedicating a general purpose register to a security monitor on modern architectures, and the deployability of Shadesmar. Our comprehensive analysis, including detailed case studies for our novel design, allows compiler designers and practitioners to select the correct shadow stack design for different usage scenarios. ",Shining Light On Shadow Stacks
111,1060348622087340032,50901426,Rafael Alves Batista,['In this conference proceedings (<LINK>) we use cosmological simulations of the magnetised cosmic web to study how high-energy cosmic rays propagate inside clusters of galaxies. We investigate the transition between ballistic and diffusive propagation.'],http://arxiv.org/abs/1811.03062,"Cosmic rays (CRs) may be used to infer properties of intervening cosmic magnetic fields. Conversely, understanding the effects of magnetic fields on the propagation of high-energy CRs is crucial to elucidate their origin. In the present work we investigate the role of intracluster magnetic fields on the propagation of CRs with energies between $10^{16}$ and $10^{18.5}$ eV. We look for possible signatures of a transition in the CR propagation regime, from diffusive to ballistic. Finally, we discuss the consequences of the confinement of high-energy CRs in clusters and superclusters for the production of gamma rays and neutrinos. ",Cosmic-ray propagation in the turbulent intergalactic medium
112,1060012580507435008,882511862524465152,Aleksander Madry,"['Does the RL framework accurately capture the behavior of deep policy gradient algorithms? We take a fine-grained look at key primitives (gradient estimation, variance reduction, opt. landscape, trust region) to find out! <LINK> (mirror: <LINK>) <LINK>', 'Joint work with @andrew_ilyas @logan_engstrom @ShibaniSan @tsiprasd Firdaus_Janoos and @CuriousLarry', '@roydanroy Oh, right. Should have added ""The answer will surprise you!"" :P']",https://arxiv.org/abs/1811.02553,"We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the ""true"" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods. ",A Closer Look at Deep Policy Gradients
113,1058212889843171329,2972152960,Renato Negrinho,"['We use imitation learning insights to learn beam search policies for structured prediction. We propose a DAgger-like algorithm (with similar regret guarantees) and a meta-algorithm that puts much of the existing work in perspective. To appear at NIPS 2018. <LINK>', ""@Shuai93Tang Just pick problems that you are excited about, otherwise it's not fun :).""]",https://arxiv.org/abs/1811.00512,"Beam search is widely used for approximate decoding in structured prediction problems. Models often use a beam at test time but ignore its existence at train time, and therefore do not explicitly learn how to use the beam. We develop an unifying meta-algorithm for learning beam search policies using imitation learning. In our setting, the beam is part of the model, and not just an artifact of approximate decoding. Our meta-algorithm captures existing learning algorithms and suggests new ones. It also lets us show novel no-regret guarantees for learning beam search policies. ",Learning Beam Search Policies via Imitation Learning
114,1067498854726156295,3053460216,Matthias Niessner,"['Check out Scan2Mesh: <LINK>\n\nWe propose a graph-neural network that generates actual 3D meshes (i.e., indexed face sets). As an example, we use unstructured range scans as input where outperform existing shape completion approaches in terms of object quality. <LINK>']",https://arxiv.org/abs/1811.10464,"We introduce Scan2Mesh, a novel data-driven generative approach which transforms an unstructured and potentially incomplete range scan into a structured 3D mesh representation. The main contribution of this work is a generative neural network architecture whose input is a range scan of a 3D object and whose output is an indexed face set conditioned on the input scan. In order to generate a 3D mesh as a set of vertices and face indices, the generative model builds on a series of proxy losses for vertices, edges, and faces. At each stage, we realize a one-to-one discrete mapping between the predicted and ground truth data points with a combination of convolutional- and graph neural network architectures. This enables our algorithm to predict a compact mesh representation similar to those created through manual artist effort using 3D modeling software. Our generated mesh results thus produce sharper, cleaner meshes with a fundamentally different structure from those generated through implicit functions, a first step in bridging the gap towards artist-created CAD models. ",Scan2Mesh: From Unstructured Range Scans to 3D Meshes
115,1067370794911756289,426509606,Yamir Moreno,"['In our last work, out today (<LINK>), we dissect the dynamics of collective social behavior in a crowd-controlled game (Twitch Plays Pokémon). We found both crowd and swarm like behaviors. Work with A. Aleta. <LINK>', ""@dgarcia_eu @ciro Yes, that's the data we used!""]",https://arxiv.org/abs/1811.09730,"Despite many efforts, the behavior of a crowd is not fully understood. The advent of modern communication media has made it an even more challenging problem, as crowd dynamics could be driven by both human-to-human and human-technology interactions. Here, we study the dynamics of a crowd controlled game (Twitch Plays Pok\'emon), in which nearly a million players participated during more than two weeks. We dissect the temporal evolution of the system dynamics along the two distinct phases that characterized the game. We find that players who do not follow the crowd average behavior are key to succeed in the game. The latter finding can be well explained by an n-$th$ order Markov model that reproduces the observed behavior. Secondly, we analyze a phase of the game in which players were able to decide between two different modes of playing, mimicking a voting system. Our results suggest that under some conditions, the collective dynamics can be better regarded as a swarm-like behavior instead of a crowd. Finally, we discuss our findings in the light of the social identity theory, which appears to describe well the observed dynamics. ",Collective social behavior in a crowd controlled game
116,1060211883112841216,885528008,William Fedus,"[""With the careful investigative work of @masscaccia and @LucasPCaccia, we find that NLP GAN models still aren't improving over a simple maximum-likelihood baseline with reduced softmax temperature as assessed on (local/global) quality-diversity spectrum! \n\n<LINK>"", 'This was a funny paper for me as a past author of an NLP GAN paper (MaskGAN:  https://t.co/YhuUvXui5i).  In MaskGAN, we demonstrated that mode-collapse and loss of diversity was occurring Sec 5.4, appendix C.4).', 'However, the recent *global* diversity advances by the Zurich Brain Group, S. Semeniuta, A. Severyn, S. Gelly https://t.co/8kTzSDTJ4E helped us make this comparison more rigorous.  @sylvain_gelly']",https://arxiv.org/abs/1811.02549,"Generating high-quality text with sufficient diversity is essential for a wide range of Natural Language Generation (NLG) tasks. Maximum-Likelihood (MLE) models trained with teacher forcing have consistently been reported as weak baselines, where poor performance is attributed to exposure bias (Bengio et al., 2015; Ranzato et al., 2015); at inference time, the model is fed its own prediction instead of a ground-truth token, which can lead to accumulating errors and poor samples. This line of reasoning has led to an outbreak of adversarial based approaches for NLG, on the account that GANs do not suffer from exposure bias. In this work, we make several surprising observations which contradict common beliefs. First, we revisit the canonical evaluation framework for NLG, and point out fundamental flaws with quality-only evaluation: we show that one can outperform such metrics using a simple, well-known temperature parameter to artificially reduce the entropy of the model's conditional distributions. Second, we leverage the control over the quality / diversity trade-off given by this parameter to evaluate models over the whole quality-diversity spectrum and find MLE models constantly outperform the proposed GAN variants over the whole quality-diversity space. Our results have several implications: 1) The impact of exposure bias on sample quality is less severe than previously thought, 2) temperature tuning provides a better quality / diversity trade-off than adversarial training while being easier to train, easier to cross-validate, and less computationally expensive. Code to reproduce the experiments is available at github.com/pclucas14/GansFallingShort ",Language GANs Falling Short
