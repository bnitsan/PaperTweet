,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1369040438699626500,265414308,Atƒ±lƒ±m G√ºne≈ü Baydin,"['New paper <LINK> where we propose a theoretically grounded method for domain-invariant representation learning by enforcing invariance under density transformations, with @a_tuan_nguyen Toan Tran @yaringal @OATML_Oxford <LINK>']",https://arxiv.org/abs/2102.05082,"Domain generalization refers to the problem where we aim to train a model on data from a set of source domains so that the model can generalize to unseen target domains. Naively training a model on the aggregate set of data (pooled from all source domains) has been shown to perform suboptimally, since the information learned by that model might be domain-specific and generalize imperfectly to target domains. To tackle this problem, a predominant approach is to find and learn some domain-invariant information in order to use it for the prediction task. In this paper, we propose a theoretically grounded method to learn a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions among domains. We also show how to use generative adversarial networks to learn such domain transformations to implement our method in practice. We demonstrate the effectiveness of our method on several widely used datasets for the domain generalization problem, on all of which we achieve competitive results with state-of-the-art models. ","Domain Invariant Representation Learning with Domain Density
  Transformations"
1,1367648813364649989,2492016278,Adrian Raftery,"['New paper on arXiv: ""Estimating SARS-CoV-2 Infections from Deaths, Confirmed Cases, Tests, and Random Surveys"" w Nick Irons: <LINK> 1/4 <LINK>', ""Most data sources for estimating Covid incidence &amp; prevalence are biased or delayed: cases underestimate, positivity rate overestimates, deaths data are delayed, hospitalizations aren't comparable between states. Random testing surveys are the least biased, but rare &amp; delayed 2/4"", 'We propose a Bayesian estimation method for all states that bias-corrects and combines number of cases, test positivity rates &amp; deaths, and anchors them with the few random testing surveys that have been done, in Indiana &amp; Ohio. 3/4', 'Results for USA to Feb 25: \n- Cumulative undercount factor: 2.2. \n- Initial undercount (to April 15, 2020): 11.1.\n- Cumulative incidence 18.4% of population (61M)\n- current reproductive rate R: 0.87\n- decline of infections since peak of cases on Jan 8: 70%\n4/4']",https://arxiv.org/abs/2102.10741,"There are many sources of data giving information about the number of SARS-CoV-2 infections in the population, but all have major drawbacks, including biases and delayed reporting. For example, the number of confirmed cases largely underestimates the number of infections, deaths lag infections substantially, while test positivity rates tend to greatly overestimate prevalence. Representative random prevalence surveys, the only putatively unbiased source, are sparse in time and space, and the results come with a big delay. Reliable estimates of population prevalence are necessary for understanding the spread of the virus and the effects of mitigation strategies. We develop a simple Bayesian framework to estimate viral prevalence by combining the main available data sources. It is based on a discrete-time SIR model with time-varying reproductive parameter. Our model includes likelihood components that incorporate data of deaths due to the virus, confirmed cases, and the number of tests administered on each day. We anchor our inference with data from random sample testing surveys in Indiana and Ohio. We use the results from these two states to calibrate the model on positive test counts and proceed to estimate the infection fatality rate and the number of new infections on each day in each state in the USA. We estimate the extent to which reported COVID cases have underestimated true infection counts, which was large, especially in the first months of the pandemic. We explore the implications of our results for progress towards herd immunity. ","Estimating SARS-CoV-2 Infections from Deaths, Confirmed Cases, Tests,
  and Random Surveys"
2,1367173820734439433,1283426602349744136,Apostolos Modas,"['Apparently adversarial robustness improves transfer learning even in non-trivial classification tasks!üßê\n\nCheck our new paper ""Improving filling level classification with adversarial training"" üç∑ü•É\n\nüìÑarXiv: <LINK>\n\n@pafrossard @smartcameras @aXompi @RicSanMat <LINK>']",https://arxiv.org/abs/2102.04057,"We investigate the problem of classifying - from a single image - the level of content in a cup or a drinking glass. This problem is made challenging by several ambiguities caused by transparencies, shape variations and partial occlusions, and by the availability of only small training datasets. In this paper, we tackle this problem with an appropriate strategy for transfer learning. Specifically, we use adversarial training in a generic source dataset and then refine the training with a task-specific dataset. We also discuss and experimentally evaluate several training strategies and their combination on a range of container types of the CORSMAL Containers Manipulation dataset. We show that transfer learning with adversarial training in the source domain consistently improves the classification accuracy on the test set and limits the overfitting of the classifier to specific features of the training data. ",Improving filling level classification with adversarial training
3,1367084260520300544,157973000,Michael Pfarrhofer,"['New working paper (with M. M. Fischer, @NHauzenberger and @FlorianHuber8): ""General Bayesian time-varying parameter VARs for predicting government bond yields"" #econometrics #research #EconTwitter \n<LINK>', 'We introduce a flexible framework for time-varying parameter models that allows for tracing the sources of parameter change without sacrificing predictive accuracy due to overfitting. The framework is applied to modeling US government bond yields with different maturities.']",https://arxiv.org/abs/2102.13393,"Time-varying parameter (TVP) regressions commonly assume that time-variation in the coefficients is determined by a simple stochastic process such as a random walk. While such models are capable of capturing a wide range of dynamic patterns, the true nature of time variation might stem from other sources, or arise from different laws of motion. In this paper, we propose a flexible TVP VAR that assumes the TVPs to depend on a panel of partially latent covariates. The latent part of these covariates differ in their state dynamics and thus capture smoothly evolving or abruptly changing coefficients. To determine which of these covariates are important, and thus to decide on the appropriate state evolution, we introduce Bayesian shrinkage priors to perform model selection. As an empirical application, we forecast the US term structure of interest rates and show that our approach performs well relative to a set of competing models. We then show how the model can be used to explain structural breaks in coefficients related to the US yield curve. ","General Bayesian time-varying parameter VARs for predicting government
  bond yields"
4,1366647976974639106,3094610676,Pranav Rajpurkar,"['Common practice for training medical imaging AI models is to use labels extracted from radiology reports.\n\nThis assumes that report labels are good proxies for image labels. BUT are they?\n\nNo! üò≤\n\nNew paperüîé <LINK>\n\n@saahil9jain @AkshaySmit @mattlungrenMD \n\n1/9 <LINK>', 'Overall, we investigate this discrepancy between radiology report labels and image labels.\n\nWe develop a radiology report labeler, VisualCheXbert, that better agrees with radiologists labeling images than do radiologists labeling reports. \n\n4 primary questions &amp; findings &gt;\n\n2/9', 'Q1: Do radiologists labeling reports agree with radiologists labeling X-ray images?\n\nA: We find significant disagreement between radiologists labeling reports and radiologists labeling images.\n\n3/9 https://t.co/3Ufzpr549K', 'Q2: Why is there so much disagreement?\n\nA: We report multiple reasons, one of which is that radiologists labeling reports have access to clinical report history, which biases their diagnoses compared to radiologists labeling images who do not have access to this information.\n\n4/9 https://t.co/OeXcObOeav', 'Q3: Are there significant relationships between report labels and image labels for different conditions?\n\nA: Yes! We report how the presence of a condition like Atelectasis in a report is related to the odds of a condition like Support Devices in an X-ray image.\n\n5/9 https://t.co/jJXoAtGi5A', 'Q4: Importantly, can we learn to map reports directly to X-ray image labels?\n\nA: Yes! VisualCheXbert, uses a biomedically pretrained BERT model that is supervised by a computer vision model trained to detect diseases from chest X-rays.\n\nInfo on VisualCheXbert‚Äôs performance &gt;\n\n6/9 https://t.co/Lq6DS9j2RU', 'When evaluated with radiologist image labels on the CheXpert test set, VisualCheXbert obtains a statistically significant overall improvement over a commonly used radiology report labeler (the CheXpert labeler) as well as radiologists labeling reports.\n\n7/9 https://t.co/8z1Zr3o6Yc', 'Our approach of supervising a report labeler with a vision model can be applied across domains.\n \nWhile previous labelers replicate radiologists labeling reports, VisualCheXbert improves over radiologists labeling reports on the more relevant task of producing image labels!\n \n8/9', 'Had a great time working with stars and first authors @saahil9jain &amp; @AkshaySmit.\n\nGreat team of @steventruongq, Chanh DT Nguyen, Minh-Thanh Huynh, Mudit Jain, @VinBrainAI, and Victoria Young, @AndrewYNg @mattlungrenMD.\n\nRead more about our work here: https://t.co/Ug17cCCt8C\n\n9/9', '@StanfordAILab @stanfordnlp']",https://arxiv.org/abs/2102.11467,"Automatic extraction of medical conditions from free-text radiology reports is critical for supervising computer vision models to interpret medical images. In this work, we show that radiologists labeling reports significantly disagree with radiologists labeling corresponding chest X-ray images, which reduces the quality of report labels as proxies for image labels. We develop and evaluate methods to produce labels from radiology reports that have better agreement with radiologists labeling images. Our best performing method, called VisualCheXbert, uses a biomedically-pretrained BERT model to directly map from a radiology report to the image labels, with a supervisory signal determined by a computer vision model trained to detect medical conditions from chest X-ray images. We find that VisualCheXbert outperforms an approach using an existing radiology report labeler by an average F1 score of 0.14 (95% CI 0.12, 0.17). We also find that VisualCheXbert better agrees with radiologists labeling chest X-ray images than do radiologists labeling the corresponding radiology reports by an average F1 score across several medical conditions of between 0.12 (95% CI 0.09, 0.15) and 0.21 (95% CI 0.18, 0.24). ","VisualCheXbert: Addressing the Discrepancy Between Radiology Report
  Labels and Image Labels"
5,1366361984040529920,20245047,Yael Grushka-Cockayne,['New paper alert: Fast and frugal time series forecasting\n\n@fotpetr and I question the need to consider large families of forecasting models. We argue that parsimoniously identifying suitable subsets of models will not decrease the‚Ä¶<LINK> <LINK>'],https://arxiv.org/abs/2102.13209,"Over the years, families of forecasting models, such as the exponential smoothing family and Autoregressive Integrated Moving Average, have expanded to contain multiple possible forms and forecasting profiles. In this paper, we question the need to consider such large families of models. We argue that parsimoniously identifying suitable subsets of models will not decrease the forecasting accuracy nor will it reduce the ability to estimate the forecast uncertainty. We propose a framework that balances forecasting performance versus computational cost, resulting in a set of reduced families of models and empirically demonstrate this trade-offs. We translate computational benefits to monetary cost savings and discuss the implications of our results in the context of large retailers. ",Fast and frugal time series forecasting
6,1366319555434455043,964624420630401024,borjan geshkovski,['Check out my new paper on sparsity and approximation in neural ODEs: <LINK>\nWith @carlosEsteveYag'],https://arxiv.org/abs/2102.13566,"We consider the neural ODE and optimal control perspective of supervised learning with $L^1(0,T;\mathbb{R}^{d_u})$ control penalties, where rather than only minimizing a final cost for the state, we integrate this cost over the entire time horizon. Under natural homogeneity assumptions on the nonlinear dynamics, we prove that any optimal control (for this cost) is sparse, in the sense that it vanishes beyond some positive stopping time. We also provide a polynomial stability estimate for the running cost of the state with respect to the time horizon. This can be seen as a \emph{turnpike property} result, for nonsmooth functionals and dynamics, and without any smallness assumptions on the data, both of which are new in the literature. In practical terms, the temporal sparsity and stability results could then be used to discard unnecessary layers in the corresponding residual neural network (ResNet), without removing relevant information. ",Sparse approximation in learning via neural ODEs
7,1366223874686291970,1075649842955866114,Luca Cortese,"['New paper by @ICRAR @UWAresearch @ARC_ASTRO3D @amelia_fmc looking at how the spin of galaxies varies across the star forming main sequence, and intriguing hints on the origin of its bending. The power of combining @SAMI_survey and @MaNGASurvey  <LINK> <LINK> <LINK>']",https://arxiv.org/abs/2102.13342,"Galaxy internal structure growth has long been accused of inhibiting star formation in disc galaxies. We investigate the potential physical connection between the growth of dispersion-supported stellar structures (e.g. classical bulges) and the position of galaxies on the star-forming main sequence at $z\sim0$. Combining the might of the SAMI and MaNGA galaxy surveys, we measure the $\lambda_{Re}$ spin parameter for 3781 galaxies over $9.5 < \log M_{\star} [\rm{M}_{\odot}] < 12$. At all stellar masses, galaxies at the locus of the main sequence possess $\lambda_{Re}$ values indicative of intrinsically flattened discs. However, above $\log M_{\star}[\rm{M}_{\odot}]\sim10.5$ where the main sequence starts bending, we find tantalising evidence for an increase in the number of galaxies with dispersion-supported structures, perhaps suggesting a connection between bulges and the bending of the main sequence. Moving above the main sequence, we see no evidence of any change in the typical spin parameter in galaxies once gravitationally-interacting systems are excluded from the sample. Similarly, up to 1 dex below the main sequence, $\lambda_{Re}$ remains roughly constant and only at very high stellar masses ($\log M_{\star}[\rm{M}_{\odot}]>11$), do we see a rapid decrease in $\lambda_{Re}$ once galaxies decline in star formation activity. If this trend is confirmed, it would be indicative of different quenching mechanisms acting on high- and low-mass galaxies. The results suggest that while a population of galaxies possessing some dispersion-supported structure is already present on the star-forming main sequence, further growth would be required after the galaxy has quenched to match the kinematic properties observed in passive galaxies at $z\sim0$. ","A SAMI and MaNGA view on the stellar kinematics of galaxies on the
  star-forming main sequence"
8,1366218659379613700,97939183,Yuandong Tian,"['Our AIStats\'21 paper ""Understanding Robustness in Teacher-Student Setting: A New Perspective"" is on arXiv now: <LINK>. <LINK>', '1/ Assuming that the ground truth labels are the outputs of a hidden fixed teacher network, we study what vulnerability a student network could have. Extending our previous work (https://t.co/Pm9pbXPvGD), we build our empirical model of learned student weight as the following: https://t.co/FIiQ2bswxl', '2/ Here, w_j is the ""ground-truth"" teacher weight, plus two additional residual terms (in-plane eps_in and out-plane eps_out) that trigger adversarial samples. Here ""plane"" refers to input data subspace X. Our theorems relate these residual terms to input data distribution.', '3/ Empirically, the robustness of a student model is indeed highly correlated with the two residual terms in standard/adversarial training, etc. Furthermore, adversarial training and data augmentation leads to smaller residual terms by increasing the rank of the data subspace.']",https://arxiv.org/abs/2102.13170,"Adversarial examples have appeared as a ubiquitous property of machine learning models where bounded adversarial perturbation could mislead the models to make arbitrarily incorrect predictions. Such examples provide a way to assess the robustness of machine learning models as well as a proxy for understanding the model training process. Extensive studies try to explain the existence of adversarial examples and provide ways to improve model robustness (e.g. adversarial training). While they mostly focus on models trained on datasets with predefined labels, we leverage the teacher-student framework and assume a teacher model, or oracle, to provide the labels for given instances. We extend Tian (2019) in the case of low-rank input data and show that student specialization (trained student neuron is highly correlated with certain teacher neuron at the same layer) still happens within the input subspace, but the teacher and student nodes could differ wildly out of the data subspace, which we conjecture leads to adversarial examples. Extensive experiments show that student specialization correlates strongly with model robustness in different scenarios, including student trained via standard training, adversarial training, confidence-calibrated adversarial training, and training with robust feature dataset. Our studies could shed light on the future exploration about adversarial examples, and enhancing model robustness via principled data augmentation. ",Understanding Robustness in Teacher-Student Setting: A New Perspective
9,1366101265793622023,420323549,Erez Hatna,['Our new arXiv paper: Coupled Contagion: A Two-Fears Epidemic Model. <LINK>'],https://arxiv.org/abs/2102.11045,"We present a differential equations model in which contagious disease transmission is affected by contagious fear of the disease and contagious fear of the control, in this case vaccine. The three contagions are coupled. The two fears evolve and interact in ways that shape distancing behavior, vaccine uptake, and their relaxation. These behavioral dynamics in turn can amplify or suppress disease transmission, which feeds back to affect behavior. The model reveals several coupled contagion mechanisms for multiple epidemic waves. Methodologically, the paper advances infectious disease modeling by including human behavioral adaptation, drawing on the neuroscience of fear learning, extinction, and transmission. ",Coupled Contagion: A Two-Fears Epidemic Model
10,1365692722217156612,618128128,Shuiwang Ji,"['New paper alert:\nSelf-Supervised Learning of Graph Neural Networks: A Unified Review\n<LINK>\nA comprehensive code library is coming in a few weeks. Stay tuned...', '@PetarV_93 Thank you. Will look into it.']",https://arxiv.org/abs/2102.10757,"Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics. ",Self-Supervised Learning of Graph Neural Networks: A Unified Review
11,1365413415976591364,1300413792862580740,Fernando,"[""I'm happy to share our new paper on arxiv\n\n<LINK>\n\nwhere we solve the two-star model with degree-degree correlations. The model has a novel phase characterized by a bimodal degree distribution.\n@networkscience @CompSysSoc #statisticalphysics  @LdnMathLab <LINK>""]",https://arxiv.org/abs/2102.09629,"Exponential random graphs are important to model the structure of real-world complex networks. Here we solve the two-star model with degree-degree correlations in the sparse regime. The model constraints the average correlation between the degrees of adjacent nodes (nearest neighbors) and between the degrees at the end-points of two-stars (next nearest neighbors). We compute exactly the network free energy and show that this model undergoes a first-order transition to a condensed phase. For non-negative degree correlations between next nearest neighbors, the degree distribution inside the condensed phase has a single peak at the largest degree, while for negative degree correlations between next nearest neighbors the condensed phase is characterized by a bimodal degree distribution. We calculate the degree assortativities and show they are non-monotonic functions of the model parameters, with a discontinuous behavior at the first-order transition. The first-order critical line terminates at a second-order critical point, whose location in the phase diagram can be accurately determined. Our results can help to develop more detailed models of complex networks with correlated degrees. ",Analytic solution of the two-star model with correlated degrees
12,1365311399287808002,1084212657761148928,Geoffrey Hinton,['I have a new paper on how to represent part-whole hierarchies in neural networks. \n\n<LINK>'],http://arxiv.org/abs/2102.12627,"This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language ",How to represent part-whole hierarchies in a neural network
13,1365299334741331968,872188363,Sandro Tacchella,"['new paper (<LINK>): z~0.8 galaxies quench fast, slow, early &amp; late, pointing toward a large diversity of quenching pathways']",http://arxiv.org/abs/2102.12494,"We investigate the stellar populations for a sample of 161 massive, mainly quiescent galaxies at $\langle z_{\rm obs} \rangle=0.8$ with deep Keck/DEIMOS rest-frame optical spectroscopy (HALO7D survey). With the fully Bayesian framework Prospector, we simultaneously fit the spectroscopic and photometric data with an advanced physical model (including non-parametric star-formation histories, emission lines, variable dust attenuation law, and dust and AGN emission) together with an uncertainty and outlier model. We show that both spectroscopy and photometry are needed to break the dust-age-metallicity degeneracy. We find a large diversity of star-formation histories: although the most massive ($M_{\star}>2\times10^{11}~M_{\odot}$) galaxies formed the earliest (formation redshift of $z_{\rm f}\approx5-10$ with a short star-formation timescale of $\tau_{\rm SF}\lesssim1~\mathrm{Gyr}$), lower-mass galaxies have a wide range of formation redshifts, leading to only a weak trend of $z_{\rm f}$ with $M_{\star}$. Interestingly, several low-mass galaxies with have formation redshifts of $z_{\rm f}\approx5-8$. Star-forming galaxies evolve about the star-forming main sequence, crossing the ridgeline several times in their past. Quiescent galaxies show a wide range and continuous distribution of quenching timescales ($\tau_{\rm quench}\approx0-5~\mathrm{Gyr}$) with a median of $\langle\tau_{\rm quench}\rangle=1.0_{-0.9}^{+0.8}~\mathrm{Gyr}$ and of quenching epochs of $z_{\rm quench}\approx0.8-5.0$ ($\langle z_{\rm quench}\rangle=1.3_{-0.4}^{+0.7}$). This large diversity of quenching timescales and epochs points toward a combination of internal and external quenching mechanisms. In our sample, rejuvenation and ""late bloomers"" are uncommon. In summary, our analysis supports the ""grow & quench"" framework and is consistent with a wide and continuously-populated diversity of quenching timescales. ","Fast, Slow, Early, Late: Quenching Massive Galaxies at z~0.8"
14,1365252469907939328,795343576590848000,David Raposo,['Excited to share our new paper on credit assignment in RL with Sam Ritter and colleagues at DeepMind. <LINK>\n\nWe propose to learn a model that links past states to current reward and use it to predict the contribution of a new state to the far future.'],http://arxiv.org/abs/2102.12425,"Since the earliest days of reinforcement learning, the workhorse method for assigning credit to actions over time has been temporal-difference (TD) learning, which propagates credit backward timestep-by-timestep. This approach suffers when delays between actions and rewards are long and when intervening unrelated events contribute variance to long-term returns. We propose state-associative (SA) learning, where the agent learns associations between states and arbitrarily distant future rewards, then propagates credit directly between the two. In this work, we use SA-learning to model the contribution of past states to the current reward. With this model we can predict each state's contribution to the far future, a quantity we call ""synthetic returns"". TD-learning can then be applied to select actions that maximize these synthetic returns (SRs). We demonstrate the effectiveness of augmenting agents with SRs across a range of tasks on which TD-learning alone fails. We show that the learned SRs are interpretable: they spike for states that occur after critical actions are taken. Finally, we show that our IMPALA-based SR agent solves Atari Skiing -- a game with a lengthy reward delay that posed a major hurdle to deep-RL agents -- 25 times faster than the published state-of-the-art. ",Synthetic Returns for Long-Term Credit Assignment
15,1365144954197012482,345254938,Snehasish Bhattacharjee (Bil),"['A new paper in arXiv: BBN Constraints on f(Q,T) Gravity\n<LINK>']",https://arxiv.org/abs/2102.12921,"$f(Q,T)$ gravity is a novel extension of the symmetric teleparallel gravity where the Lagrangian $L$ is represented through an arbitrary function of the nonmetricity $Q$ and the trace of the energy-momentum tensor $T$ \cite{fqt}. In this work, we have constrained a widely used $f(Q,T)$ gravity model of the form $f(Q,T) = Q^{n+1} + m T$ from the primordial abundances of the light elements to understand its viability in Cosmology. We report that the $f(Q,T)$ gravity model can elegantly explain the observed abundances of Helium and Deuterium while the Lithium problem persists. From the constraint on the expansion factor in the range $0.9425 \lesssim Z \lesssim1.1525$, we report strict constraints on the parameters $m$ and $n$ in the range $-1.13 \lesssim n \lesssim -1.08$ and $-5.86 \lesssim m \lesssim12.52$ respectively. ","BBN Constraints on $f(Q,T)$ Gravity"
16,1365138687298707460,1146266767880155136,Sina Baharlouei,"['Check out our new paper ""FERMI: Fair Empirical Risk Minimization via Exponential R√©nyi Mutual Information"": A new notion of fairness, upper-bounding many well-known fairness measures and the first unbiased stochastic algorithm with*convergence guarantee*. \n<LINK>']",https://arxiv.org/abs/2102.12586,"Despite the success of large-scale empirical risk minimization (ERM) at achieving high accuracy across a variety of machine learning tasks, fair ERM is hindered by the incompatibility of fairness constraints with stochastic optimization. In this paper, we propose the fair empirical risk minimization via exponential R\'enyi mutual information (FERMI) framework. FERMI is built on a stochastic estimator for exponential R\'enyi mutual information (ERMI), an information divergence measuring the degree of the dependence of predictions on sensitive attributes. Theoretically, we show that ERMI upper bounds existing popular fairness violation metrics, thus controlling ERMI provides guarantees on other commonly used violations, such as $L_\infty$. We derive an unbiased estimator for ERMI, which we use to derive the FERMI algorithm. We prove that FERMI converges for demographic parity, equalized odds, and equal opportunity notions of fairness in stochastic optimization. Empirically, we show that FERMI is amenable to large-scale problems with multiple (non-binary) sensitive attributes and non-binary targets. Extensive experiments show that FERMI achieves the most favorable tradeoffs between fairness violation and test accuracy across all tested setups compared with state-of-the-art baselines for demographic parity, equalized odds, equal opportunity. These benefits are especially significant for non-binary classification with large sensitive sets and small batch sizes, showcasing the effectiveness of the FERMI objective and the developed stochastic algorithm for solving it. ","FERMI: Fair Empirical Risk Minimization via Exponential R\'enyi Mutual
  Information"
17,1365057786250489858,1882939814,Sarah Wiegreffe,"['Happy to share our new preprint (with @anmarasovic) ‚ÄúTeach Me to Explain: A Review of Datasets for Explainable NLP‚Äù\nPaper: <LINK>\nWebsite: <LINK>\n\nIt‚Äôs half survey, half reflections for more standardized ExNLP dataset collection. Highlights:\n\n1/6', 'We focus on datasets of the form: (inputs, labels, explanations). We describe these instance-wise explanations as ‚Äúexplaining human decisions‚Äù (the labels). Other types of explanations may explain something about the world. We focus on the shaded area.\n\n2/6 https://t.co/EUxVH09c8U', 'We identify 3 major classes of explanation datasets: highlights, free-text, and structured explanations. Each one is surveyed in a table like this (structured explanations here).\n\n&gt;95% of the datasets are collected using human annotation, and &gt;70% use crowdsourcing.\n\n3/6 https://t.co/jduqxNq174', 'We discuss how constraints placed on data annotation can influence modeling and evaluation, and suggest the use of datasheets to make collection decisions more transparent.\n\nAlso discuss structure that sometimes emerges free-text rationales and what to do about it.\n\n4/6 https://t.co/G6u3nvQrrC', 'Finally, we synthesize crowdsourcing methods from NLP/ HCI literature on improving quality+diversity of ExNLP datasets, such as using a crowd editing stage, collecting large set of explanations per instance, and using a diverse set of crowdworkers to avoid annotator bias.\n\n5/6', 'Please submit an issue or PR to our Github repository (linked through website) to add or edit characteristics of datasets! We are open to feedback.\n\n6/6']",https://arxiv.org/abs/2102.12060,"Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future. ","Teach Me to Explain: A Review of Datasets for Explainable Natural
  Language Processing"
18,1365040453763014660,719057596540461057,Steven Brunton,"['Very excited to announce a new review paper on ""Modern Koopman Theory for Dynamical Systems""\n\nwhich was a great collaboration with Marko Budi≈°iƒá (@dynamicalmarko), Eurika Kaiser, and Nathan Kutz.\n\nCheck it out here: <LINK>\n\n1/n <LINK>', 'In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. \n\n2/n https://t.co/3YlyhUtRYL', 'Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system.  \n\n3/n https://t.co/eZHjRbJeNw', 'This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems.\n\n4/n https://t.co/ZjkQEN1flO', 'However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge.\n\n5/n https://t.co/6ziZSpHKnl']",https://arxiv.org/abs/2102.12086,"The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems. ",Modern Koopman Theory for Dynamical Systems
19,1364955339544797195,100319693,Matthew Scroggs,['Happy new-paper-on-ArŒßiv-day to me! <LINK>'],https://arxiv.org/abs/2102.11901,"We develop a method for generating degree-of-freedom maps for arbitrary order finite element spaces for any cell shape. The approach is based on the composition of permutations and transformations by cell sub-entity. Current approaches to generating degree-of-freedom maps for arbitrary order problems typically rely on a consistent orientation of cell entities that permits the definition of a common local coordinate system on shared edges and faces. However, while orientation of a mesh is straightforward for simplex cells and is a local operation, it is not a strictly local operation for quadrilateral cells and in the case of hexahedral cells not all meshes are orientable. The permutation and transformation approach is developed for a range of element types, including Lagrange, and divergence- and curl-conforming elements, and for a range of cell shapes. The approach is local and can be applied to cells of any shape, including general polytopes and meshes with mixed cell types. A number of examples are presented and the developed approach has been implemented in an open-source finite element library. ","Construction of arbitrary order finite element degree-of-freedom maps on
  polygonal and polyhedral cell meshes"
20,1364916245502586888,1002513128,Dr. Phil Metzger,"['Relevant to #PerseveranceRover: the new decadal survey white paper led by @Ryan_N_Watkins on the rocket exhaust effects for Moon and Mars landings. Downloadable here: <LINK>', '@rjward1775 @Ryan_N_Watkins I don‚Äôt have funding to work on that and I don‚Äôt know who does. Probably someone at JPL. Would be interesting to see, though.', '@LyndaSuzanneYa1 @Ryan_N_Watkins Well it would be a lot more for a bigger lander. This was ~2 tons of mass suspended on those thrusters, and the thrusters were 21 feet above the surface. A 20 ton lander will make huge holes in the ground. Scary to think about! We can only extrapolate at this point. Need data üôè']",https://arxiv.org/abs/2102.12312,"This 2020 Decadal Survey White Paper reviews what is known about lunar and martian lander Plume Surface Interactions (PSI) during powered descent. This includes an overview of the phenomenology and a description of the induced hardware and environmental impacts. Then it provides an overview of mitigation techniques and a summary of the outstanding questions and strategic knowledge gaps. It finishes with five recommendations: to include dedicated descent imagers on every surface mission so that PSI can be directly recorded and reviewed by ground teams; as far as possible, to make all data related to PSI effects publicly accessible; to develop methods and instruments for making key measurements of PSI; to assess and record key flight data; and to invest funding into studies of long-term infrastructure architectures and mitigation techniques. ","Understanding and Mitigating Plume Effects During Powered Descents on
  the Moon and Mars"
21,1364889643695677441,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New arXiv paper with @meetlilychenv2 and Tobias Stollenwerk showing that the new domain-wall encoding developed in my group can lead to large increases in performance for quantum annealers <LINK> work performed @DurhamQlm and @jqcDurNew', 'The original encoding is described in https://t.co/32gSPIQycb but this new work is the first experimental test I am aware of, and shows that the right encoding can make a bigger difference than a more advanced QPU']",https://arxiv.org/abs/2102.12224,"In this paper we experimentally test the performance of the recently proposed domain-wall encoding of discrete variables from [Chancellor Quantum Sci. Technol. 4 045004] on Ising model flux qubit quantum annealers. We compare this encoding with the traditional one-hot methods and find that they outperform the one-hot encoding for three different problems at different sizes both of the problem and of the variables. From these results we conclude that the domain-wall encoding yields superior performance against a variety of metrics furthermore, we do not find a single metric by which one hot performs better. We even find that a 2000Q quantum annealer with a drastically less connected hardware graph but using the domain-wall encoding can outperform the next generation Advantage processor if that processor uses one-hot encoding. ",Performance of Domain-Wall Encoding for Quantum Annealing
22,1364860968333795334,1473984524,Keisuke Okumura,"['My new paper ""Iterative Refinement for Real-Time Multi-Robot Path Planning‚Äù, w/@y7amura and Prof. Xavier D√©fago, is out on arXivüëÄ. An anytime path planner with effective use of existing solvers is presented.\n<LINK>\n\n1-min introductionüé•\n<LINK>', 'I also make the entire code public (with a nice visualizer).\nhttps://t.co/4z0sTIUvxB\n\nEnjoy planningü§ú']",https://arxiv.org/abs/2102.12331,"We study the iterative refinement of path planning for multiple robots, known as multi-agent pathfinding (MAPF). Given a graph, agents, their initial locations, and destinations, a solution of MAPF is a set of paths without collisions. Iterative refinement for MAPF is desirable for three reasons: 1)~optimization is intractable, 2)~sub-optimal solutions can be obtained instantly, and 3)~it is anytime planning, desired in online scenarios where time for deliberation is limited. Despite the high demand, this is under-explored in MAPF because finding good neighborhoods has been unclear so far. Our proposal uses a sub-optimal MAPF solver to obtain an initial solution quickly, then iterates the two procedures: 1)~select a subset of agents, 2)~use an optimal MAPF solver to refine paths of selected agents while keeping other paths unchanged. Since the optimal solvers are used on small instances of the problem, this scheme yields efficient-enough solutions rapidly while providing high scalability. We also present reasonable candidates on how to select a subset of agents. Evaluations in various scenarios show that the proposal is promising; the convergence is fast, scalable, and with reasonable quality. ",Iterative Refinement for Real-Time Multi-Robot Path Planning
23,1364837491736535043,2816968963,Miguel A.F. Sanju√°n,['Our new paper ùêÖùê®ùê´ùêúùê¢ùêßùê† ùê≠ùê°ùêû ùêûùê¨ùêúùêöùê©ùêû: ùêèùêöùê´ùê≠ùê¢ùêöùê• ùêúùê®ùêßùê≠ùê´ùê®ùê• ùê®ùêü ùêûùê¨ùêúùêöùê©ùê¢ùêßùê† ùê®ùê´ùêõùê¢ùê≠ùê¨ ùêüùê´ùê®ùê¶ ùêö ùê≠ùê´ùêöùêßùê¨ùê¢ùêûùêßùê≠ ùêúùê°ùêöùê®ùê≠ùê¢ùêú ùê´ùêûùê†ùê¢ùê®ùêß has been accepted in Nonlinear Dynamics @SpringerPhysics @URJCcientifica @urjc\n\n<LINK> <LINK>'],https://arxiv.org/abs/2102.11927,"A new control algorithm based on the partial control method has been developed. The general situation we are considering is an orbit starting in a certain phase space region Q having a chaotic transient behavior affected by noise, so that the orbit will definitely escape from Q in an unpredictable number of iterations. Thus, the goal of the algorithm is to control in a predictable manner when to escape. While partial control has been used as a way to avoid escapes, here we want to adapt it to force the escape in a controlled manner. We have introduced new tools such as escape functions and escape sets that once computed makes the control of the orbit straightforward. We have applied the new idea to three different cases in order to illustrate the various application possibilities of this new algorithm. ","Forcing the escape: Partial control of escaping orbits from a transient
  chaotic region"
24,1364816124743876608,1075649842955866114,Luca Cortese,['New paper by @astro_katharina looking at the link between cold gas and metals in galaxies. Using xCOLDGASS and @ICRAR @UWAresearch xGASS survey to quantify the role of atomic hydrogen in regulating the shape of metallicity gradients. @ARC_ASTRO3D <LINK> <LINK>'],https://arxiv.org/abs/2102.09909,"Context. The xGASS and xCOLD GASS surveys have measured the atomic (HI) and molecular gas (H2) content of a large and representative sample of nearby galaxies (redshift range of 0.01 $\lt$ z $\lt$ 0.05). Aims. We present optical longslit spectra for a subset of the xGASS and xCOLD GASS galaxies to investigate the correlation between radial metallicity profiles and cold gas content. In addition to data from Moran et al. (2012), this paper presents new optical spectra for 27 galaxies in the stellar mass range of 9.0 $\leq$ log Mstar/Msun $\leq$ 10.0. Methods. The longslit spectra were taken along the major axis of the galaxies, allowing us to obtain radial profiles of the gas-phase oxygen abundance (12 + log(O/H)). The slope of a linear fit to these radial profiles is defined as the metallicity gradient. We investigated correlations between these gradients and global galaxy properties, such as star formation activity and gas content. In addition, we examined the correlation of local metallicity measurements and the global HI mass fraction. Results. We obtained two main results: (i) the local metallicity is correlated with the global HI mass fraction, which is in good agreement with previous results. A simple toy model suggests that this correlation points towards a 'local gas regulator model'; (ii) the primary driver of metallicity gradients appears to be stellar mass surface density (as a proxy for morphology). Conclusions. This work comprises one of the few systematic observational studies of the influence of the cold gas on the chemical evolution of star-forming galaxies, as considered via metallicity gradients and local measurements of the gas-phase oxygen abundance. Our results suggest that local density and local HI mass fraction are drivers of chemical evolution and the gas-phase metallicity. ","xCOLDGASS and xGASS: Radial metallicity gradients and global properties
  on the star-forming main sequence"
25,1364758667225137163,1012125662117851136,Edward Kennedy,"[""Siva &amp; Larry &amp; I have a new paper on counterfactual density estimation:\n\n<LINK>\n\nMeans are everywhere in causal inference, but *densities* can be much more informative &amp; useful\n\nWe give efficiency bounds, optimal estimators, &amp; easy-to-use code - hope it's useful! <LINK>"", '@c3K Thanks Chris!', ""@ccall727 @StatsbyLopez The first place I could find them being used was in this paper:\n\nhttps://t.co/1khPpf2MXN\n\nBut there's been surprisingly little work in the area, unless I'm missing some of the literature!"", '@nshejazi Thanks Nima! I definitely agree about it being a fundamental problem - &amp; was a lot of fun to work on!', '@StatsbyLopez Pretty wild!', '@syrgkanis @ccall727 @StatsbyLopez Thanks will check it out! At first glance they seem to be considering the distribution function version of the problem, which we note is related but different below - I‚Äôll read more closely https://t.co/KWfqjPWfU7', '@syrgkanis @ccall727 @StatsbyLopez Thanks, sounds cool!']",https://arxiv.org/abs/2102.12034,"Causal effects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters. The first is a density approximation, defined by a projection onto a finite-dimensional model using a generalized distance metric, which includes f-divergences as well as $L_p$ norms. The second is the distance between counterfactual densities, which can be used as a more nuanced effect measure than the mean difference, and as a tool for model selection. We study nonparametric efficiency bounds for these targets, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular non-trivial functions of counterfactuals, linking the problems of density and mean estimation. We go on to propose doubly robust-style estimators for the density approximations and distances, and study their rates of convergence, showing they can be optimally efficient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but throughout we highlight what happens for particular choices, such as $L_2$ projections on linear models, and KL projections on exponential families. Finally we illustrate by estimating the density of CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density effect. Our results suggest combination therapy may have increased CD4 count most for high-risk patients. Our methods are implemented in the freely available R package npcausal on GitHub. ",Semiparametric counterfactual density estimation
26,1364714604241186816,947289175509819393,Nikhil Medhekar,"['Two dimensional Bismuth is a very exciting material. It comes in three different forms and each of them has very different electronic and topological features. We ask the question why in our new paper:\n\n<LINK>\n\n@FLEETCentre @monashengineers @MonashMSE', 'The most rewarding aspect of this work was certainly the close collaboration with @quantschmant group at RMIT, enabled by @FLEETCentre.  This was so far out of our comfort zone, that it was daunting and yet fun to learn new things and slowly figure things out.', 'With our fundamental models, we now look forward to apply them to study new transport phenomena in these materials.', 'All credit goes to young members of both groups, the honours student who led this study, plus the PhD students and postdocs who helped. \n\nAcknowledging support from @FLEETCentre as well as our fantastic computing facilities @NCInews and @PawseyCentre.']",https://arxiv.org/abs/2102.11486,"With its monoelemental composition, various crystalline forms and an inherently strong spin-orbit coupling, bismuth has been regarded as an ideal prototype material to expand our understanding of topological electronic structures. In particular, two-dimensional bismuth thin films have attracted a growing interest due to potential applications in topological transistors and spintronics. This calls for an effective physical model to give an accurate interpretation of the novel topological phenomena shown by two-dimensional bismuth. However, the conventional semi-empirical approach of adapting bulk bismuth hoppings fails to capture the topological features of two-dimensional bismuth allotropes because the electronic band topology is heavily influenced by crystalline symmetries as well as atom spacings. Here we provide a new parameterization using localized Wannier functions derived from the Bloch states in first-principles calculations. We construct new tight-binding models for three types of two-dimensional bismuth allotropes: a Bi (111) bilayer, bismuthene and a Bi(110) bilayer. We demonstrate that our tight-binding models can successfully reproduce the band structures, symmetries and topological features of these two-dimensional allotropes. We anticipate that these models can be extended to other similar two-dimensional topological structures such as antimonene and arsenene. Moreover, these models can serve as a starting point for investigating the electron/spin transport and electromagnetic response in low-dimensional topological devices. ","Localized Wannier function based tight-binding models for
  two-dimensional allotropes of bismuth"
27,1364690743735025668,1012689495420833792,Simon Powers,"['Our new paper reviews agent-based traffic simulators, with the aim of helping you choose the right one for your research. Led by @ComputingNapier PhD student Johannes Nguyen, and co-authored by @NeilUrquhart1, Thomas Farrenkopf, Michael Guckert and myself.<LINK>']",https://arxiv.org/abs/2102.07505,"Individual traffic significantly contributes to climate change and environmental degradation. Therefore, innovation in sustainable mobility is gaining importance as it helps to reduce environmental pollution. However, effects of new ideas in mobility are difficult to estimate in advance and strongly depend on the individual traffic participants. The application of agent technology is particularly promising as it focuses on modelling heterogeneous individual preferences and behaviours. In this paper, we show how agent-based models are particularly suitable to address three pressing research topics in mobility: 1. Social dilemmas in resource utilisation; 2. Digital connectivity; and 3. New forms of mobility. We then explain how the features of several agent-based simulators are suitable for addressing these topics. We assess the capability of simulators to model individual travel behaviour, discussing implemented features and identifying gaps in functionality that we consider important. ",An Overview of Agent-based Traffic Simulators
28,1364649133110398978,1271460562980016129,Mohamed El Banani,"['New paper (w/ Luya Gao and @jcjohnss) proposes an unsupervised approach to point cloud registration using RGB-D video. \n\nproject: <LINK>\npaper: <LINK>\ncode: <LINK> <LINK>', 'RGB-D cameras are increasingly more prevalent. Large scale RGB-D video is next! Can we leverage this data stream for point cloud registration? We use differentiable alignment and rendering to learn accurate registration without any pose supervision. (2/3) https://t.co/SenRqQEEaK', ""Interestingly, we find that we can use the Lowe's ratio test weights to rank and filter our correspondances while maintaining end-to-end differentiability. This allows us to learn good features for registration from scratch. (3/3) https://t.co/vEunMAbbHp""]",https://arxiv.org/abs/2102.11870,"Aligning partial views of a scene into a single whole is essential to understanding one's environment and is a key component of numerous robotics tasks such as SLAM and SfM. Recent approaches have proposed end-to-end systems that can outperform traditional methods by leveraging pose supervision. However, with the rising prevalence of cameras with depth sensors, we can expect a new stream of raw RGB-D data without the annotations needed for supervision. We propose UnsupervisedR&R: an end-to-end unsupervised approach to learning point cloud registration from raw RGB-D video. The key idea is to leverage differentiable alignment and rendering to enforce photometric and geometric consistency between frames. We evaluate our approach on indoor scene datasets and find that we outperform existing traditional approaches with classic and learned descriptors while being competitive with supervised geometric point cloud registration approaches. ","UnsupervisedR&R: Unsupervised Point Cloud Registration via
  Differentiable Rendering"
29,1364587701882806273,187339880,Xavier I Gonzalez,['My new paper is out now: <LINK>\n\nMeet The FaCells. An Exploratory Study about \nLSTM Layers on Face Sketches Classifiers. \n\n#plottertwitter #axidraw #creativecoding #generativeart #lineart #magenta #lstm <LINK>'],http://arxiv.org/abs/2102.11361,"Lines are human mental abstractions. A bunch of lines may form a drawing. A set of drawings can feed an LSTM network input layer, considering each draw as a list of lines and a line a list of points. This paper proposes the pointless motive to classify the gender of celebrities' portraits as an excuse for exploration in a broad, more artistic sense. Investigation results drove compelling ideas here discussed. The experiments compared different ways to represent draws to be input in a network and showed that an absolute format of coordinates (x, y) was a better performer than a relative one (Dx, Dy) with respect to prior points, most frequent in the reviewed literature. Experiments also showed that, due to the recurrent nature of LSTMs, the order of lines forming a drawing is a relevant factor for input in an LSTM classifier not studied before. A minimum 'pencil' traveled length criteria for line ordering proved suitable, possible by reducing it to a TSP particular instance. The best configuration for gender classification appears with an LSTM layer that returns the hidden state value for each input point step, followed by a global average layer along the sequence, before the output dense layer. That result guided the idea of removing the average in the network pipeline and return a per-point attribute score just by adjusting tensors dimensions. With this trick, the model detects an attribute in a drawing and also recognizes the points linked to it. Moreover, by overlapping filtered lines of portraits, an attribute's visual essence is depicted. Meet the FaCells. ","The FaCells. An Exploratory Study about LSTM Layers on Face Sketches
  Classifiers"
30,1364583109132496897,2766925212,Andrew Childs,"['Fun new paper with @hungshihhan and @tongyang93 on quantum query complexity with matrix-vector products. How fast can you learn properties of a matrix A with queries giving Ax for any vector x? Some problems have exponential speedup; others have none. <LINK>', '@newplatonism @hungshihhan @tongyang93 I found it fun; YMMV. But not sure what you mean about building oracles. Certainly you can instantiate some (but of course not all) Deutsch-Jozsa oracles efficiently.']",http://arxiv.org/abs/2102.11349,"We study quantum algorithms that learn properties of a matrix using queries that return its action on an input vector. We show that for various problems, including computing the trace, determinant, or rank of a matrix or solving a linear system that it specifies, quantum computers do not provide an asymptotic speedup over classical computation. On the other hand, we show that for some problems, such as computing the parities of rows or columns or deciding if there are two identical rows or columns, quantum computers provide exponential speedup. We demonstrate this by showing equivalence between models that provide matrix-vector products, vector-matrix products, and vector-matrix-vector products, whereas the power of these models can vary significantly for classical computation. ",Quantum query complexity with matrix-vector products
31,1364519048940314625,39688015,Chris Usher,"['New paper out on the central kinematics of the globular cluster M15 with Sebastian Kamann, @vincenthb, @balbinotdd et al. <LINK>', 'We used the integral field spectrograph MUSE on the @VLT in its new narrow field adaptive optics mode to get spectra of 864 stars in the inner 8 arcsec of the core collapse globular cluster M15', 'Together with literature data at larger radii we use this to show that the kinematics of the stars in M15 in its core are disconnected from the rest of the cluster. The central stars are rotating much faster and in a different direction to the rest of the cluster', 'We speculate that post core collapse oscillations and angular momentum transfers from black hole binaries could produce these decoupled kinematics but future modelling will be required to see if this is a valid explanation', 'And yes I know there is a typo in the title in the arXiv metadata - an update has been submitted.', 'By using adaptive optics we are able to get HST like spatial resolution, allowing us to measure radial velocities for 864 stars within 8 arcseconds of the centre of M15']",https://arxiv.org/abs/2102.11721,"We present observations of the stellar kinematics of the centre of the core collapsed globular cluster M15 obtained with the MUSE integral field spectrograph on the VLT operating in narrow field mode. Thanks to the use of adaptive optics, we obtain a spatial resolution of 0.1arcsec and are able to reliably measure the radial velocities of 864 stars within 8 arcsec of the centre of M15 thus providing the largest sample of radial velocities ever obtained for the innermost regions of this system. Combined with previous observations of M15 using MUSE in wide field mode and literature data, we find that the central kinematics of M15 are complex with the rotation axis of the core of M15 offset from the rotation axis of the bulk of the cluster. While this complexity has been suggested by previous work, we confirm it at higher significance and in more detail. ",MUSE narrow field mode observations of the central kinematics of M15
32,1364485567468208128,734484164070772736,Guy Tennenholtz,"['GELATO: How do we leverage proximity and uncertainty to improve offline reinforcement learning (RL) algorithms? In our new paper we answer this question through variational pullback metrics of proximity and uncertainty. <LINK>', 'GELATO: We construct Riemannian metrics on submanifolds induced by a variational forward model. These metrics, capturing both proximity and uncertainty w.r.t the data, are leveraged in a model based offline RL framework (MOPO) https://t.co/PkbJ69F4oa', 'GELATO is capable of capturing intrinsic characteristics of the data manifold, trading off proximity and uncertainty in order to enjoy the benefits of both worlds. Read more in our paper! https://t.co/PkbJ69F4oa']",https://arxiv.org/abs/2102.11327,"Offline reinforcement learning approaches can generally be divided to proximal and uncertainty-aware methods. In this work, we demonstrate the benefit of combining the two in a latent variational model. We impose a latent representation of states and actions and leverage its intrinsic Riemannian geometry to measure distance of latent samples to the data. Our proposed metrics measure both the quality of out of distribution samples as well as the discrepancy of examples in the data. We integrate our metrics in a model-based offline optimization framework, in which proximity and uncertainty can be carefully controlled. We illustrate the geodesics on a simple grid-like environment, depicting its natural inherent topology. Finally, we analyze our approach and improve upon contemporary offline RL benchmarks. ","GELATO: Geometrically Enriched Latent Model for Offline Reinforcement
  Learning"
33,1364480362458583042,802543221943439360,Andrea Caputo,['New paper out! <LINK>\nWe study the impact of the plasma around BHs on the superradiance for dark photons. We notice that it is possible -- in the presence of kinetic mixing -- that superradiance is shut down before extracting a sizable spin energy from the BH. <LINK>'],https://arxiv.org/abs/2102.11280,"Black hole superradiance is a powerful tool in the search for ultra-light bosons. Constraints on the existence of such particles have been derived from the observation of highly spinning black holes, absence of continuous gravitational-wave signals, and of the associated stochastic background. However, these constraints are only strictly speaking valid in the limit where the boson's interactions can be neglected. In this work we investigate the extent to which the superradiant growth of an ultra-light dark photon can be quenched via scattering processes with ambient electrons. For dark photon masses $m_{\gamma^\prime} \gtrsim 10^{-17}\,{\rm eV}$, and for reasonable values of the ambient electron number density, we find superradiance can be quenched prior to extracting a significant fraction of the black-hole spin. For sufficiently large $m_{\gamma^\prime}$ and small electron number densities, the in-medium suppression of the kinetic mixing can be efficiently removed, and quenching occurs for mixings $\chi_0 \gtrsim \mathcal{O}(10^{-8})$; at low masses, however, in-medium effects strongly inhibit otherwise efficient scattering processes from dissipating energy. Intriguingly, this quenching leads to a time- and energy-oscillating electromagnetic signature, with luminosities potentially extending up to $\sim 10^{57}\,{\rm erg / s}$, suggesting that such events should be detectable with existing telescopes. As a byproduct we also show that superradiance cannot be used to constrain a small mass for the Standard Model photon. ",Electromagnetic Signatures of Dark Photon Superradiance
34,1364465902134165505,1968365508,Samaya Nissanke (she/her) üíô,"['Group news: new paper by PhD student @GRaaymakers on ‚Äúthe challenges ahead for multi messenger analyses of gravitational waves and kilonova: a case study on GW190425.‚ÄùV proud of Geert &amp; paper after 2 + yrs of developing the framework &amp; analyses. <LINK>', '22 pages long, quite a lot of physics in it, &amp; an end to end analysis developed by Geert and pause for reflection by the group &amp; hopefully useful for the community!', 'Well done @GRaaymakers for this comprehensive &amp; challenging paper, and thank you to our excellent collaborators, @FrancoisFoucart @BullaMattia @astro_rafernan @ameliahenkel @tedwards2412 @AntierSarah &amp; many others not on twitter who all contributed wonderfully!', '&amp; thanks to @NWO_Science for supporting this work through a VIDI. Written originally in 2014, then reapplied in 2015, awarded in 2016 but the actual discovery of gravitational waves and multi messenger members, plus becoming new mum, had us occupied. Mission completed!', 'And code will be made open source shortly!']",https://arxiv.org/abs/2102.11569,"In recent years, there have been significant advances in multi-messenger astronomy due to the discovery of the first, and so far only confirmed, gravitational wave event with a simultaneous electromagnetic (EM) counterpart, as well as improvements in numerical simulations, gravitational wave (GW) detectors, and transient astronomy. This has led to the exciting possibility of performing joint analyses of the GW and EM data, providing additional constraints on fundamental properties of the binary progenitor and merger remnant. Here, we present a new Bayesian framework that allows inference of these properties, while taking into account the systematic modeling uncertainties that arise when mapping from GW binary progenitor properties to photometric light curves. We extend the relative binning method presented in Zackay et al. (2018) to include extrinsic GW parameters for fast analysis of the GW signal. The focus of our EM framework is on light curves arising from r-process nucleosynthesis in the ejected material during and after merger, the so called kilonova, and particularly on black hole - neutron star systems. As a case study, we examine the recent detection of GW190425, where the primary object is consistent with being either a black hole (BH) or a neutron star (NS). We show quantitatively how improved mapping between binary progenitor and outflow properties, and/or an increase in EM data quantity and quality are required in order to break degeneracies in the fundamental source parameters. ","The Challenges Ahead for Multimessenger Analyses of Gravitational Waves
  and Kilonova: a Case Study on GW190425"
35,1364387994493280257,80080794,Daniel Levy,"['New paper out on learning under *user*-level differential privacy constraints! <LINK>\n\nIn the standard DP setting, we implicitly assume that each user contributes a single sample but it turns out we often contribute many many samples (like all of our texts). 1/', 'Protecting against information leaks for users is harder but more meaningful. We show that for different learning tasks, increasing the number of sample per user m decreases the privacy cost at a slower 1/sqrt{m} rate (in contrast to 1/n when increasing the number of users n). 2/', 'Note that naively using the group-property of DP does not even decrease the privacy cost when users provide more samples. 3/', 'We provide algorithms and lower bounds for these tasks (and sometimes they even match!) based on a novel generic mean estimator with error scaling as the concentration radius---think sub-Gaussian parameter---rather than the whole range of the random variable. 4/', 'Great collaboration with @SZiteng, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri and @th33rtha this summer @GoogleAI. 5/5']",https://arxiv.org/abs/2102.11845,"We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user's entire contribution ($m \ge 1$ samples), providing more stringent but more realistic protection against information leaks. We show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy cost decreases at a faster $O(1/n)$ rate. We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius $\tau$ of the distribution rather than the entire range. ",Learning with User-Level Privacy
36,1364295810717011974,373525906,Weijie Su,"['New paper: In *Federated f-Differential Privacy* (<LINK>), we proposed a new privacy notion tailored to the setting where the clients ally in the attack. This privacy concept is adapted from f-differential privacy.  w/ Qinqing Zheng, @ShuxiaoC, and Qi Long.']",https://arxiv.org/abs/2102.11158,"Federated learning (FL) is a training paradigm where the clients collaboratively learn models by repeatedly sharing information without compromising much on the privacy of their local sensitive data. In this paper, we introduce federated $f$-differential privacy, a new notion specifically tailored to the federated setting, based on the framework of Gaussian differential privacy. Federated $f$-differential privacy operates on record level: it provides the privacy guarantee on each individual record of one client's data against adversaries. We then propose a generic private federated learning framework {PriFedSync} that accommodates a large family of state-of-the-art FL algorithms, which provably achieves federated $f$-differential privacy. Finally, we empirically demonstrate the trade-off between privacy guarantee and prediction performance for models trained by {PriFedSync} in computer vision tasks. ",Federated $f$-Differential Privacy
37,1364234820050640899,1269670536,Lukasz Olejnik,"['Our new research paper. We find that the CNAME tracking scheme is gaining popularity and is leading to security and privacy risks on the web. Data is leaking. Joint work with Yana Dimova, Gunes Acar @tomvangoethem. Paper accepted to @PET_Symposium 2021. <LINK> <LINK>']",https://arxiv.org/abs/2102.09301,"Online tracking is a whack-a-mole game between trackers who build and monetize behavioral user profiles through intrusive data collection, and anti-tracking mechanisms, deployed as a browser extension, built-in to the browser, or as a DNS resolver. As a response to pervasive and opaque online tracking, more and more users adopt anti-tracking tools to preserve their privacy. Consequently, as the information that trackers can gather on users is being curbed, some trackers are looking for ways to evade these tracking countermeasures. In this paper we report on a large-scale longitudinal evaluation of an anti-tracking evasion scheme that leverages CNAME records to include tracker resources in a same-site context, effectively bypassing anti-tracking measures that use fixed hostname-based block lists. Using historical HTTP Archive data we find that this tracking scheme is rapidly gaining traction, especially among high-traffic websites. Furthermore, we report on several privacy and security issues inherent to the technical setup of CNAME-based tracking that we detected through a combination of automated and manual analyses. We find that some trackers are using the technique against the Safari browser, which is known to include strict anti-tracking configurations. Our findings show that websites using CNAME trackers must take extra precautions to avoid leaking sensitive information to third parties. ","The CNAME of the Game: Large-scale Analysis of DNS-based Tracking
  Evasion"
38,1364228937753133065,367297219,Melanie Mitchell,"['New paper from me: ""Abstraction and Analogy-Making in Artificial Intelligence"": <LINK>\n\nüßµ  (1/4)', 'This paper is part review, part opinion.  I argue that conceptual abstraction is driven by analogy, and that analogy is an understudied area of AI that will be essential to overcoming the brittleness and narrowness of AI systems. (2/4)', 'I review both older and very recent approaches to analogy in AI, including symbolic systems, deep learning, and probabilistic program induction.  I then propose some ideas for how to best make progress in this area. (3/4)', 'I would be very happy to receive any feedback on these ideas!  (4/4)', ""@markcannon5 @hardmaru @fchollet Indeed, this was @fchollet's point."", '@summerstay1 Of course GPT-3 has been pre-trained on a lot of text, including text that mentions bridges across the gender gap, etc.  It would be good to find a way to test its analogy-making abilities on analogies that are definitely outside of its training data.']",http://arxiv.org/abs/2102.10717,"Conceptual abstraction and analogy-making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite of a long history of research on constructing AI systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area. ",Abstraction and Analogy-Making in Artificial Intelligence
39,1364225144810573824,741547129,Britt Lundgren,"['Excited to share this new paper on the arXiv today! ""The Geometry of Cold, Metal-Enriched Gas Around Galaxies at z‚àº1.2"" <LINK>', 'If you have 4 minutes, check out the excellent video summary of this paper made for the KITP Halo21 workshop by my co-author and former student, Samantha Creech! https://t.co/jyMmE9evLh', 'Thanks also to Co-I @gbrammer and amazing former @UncAvl students Matthew Peek &amp; Nathan Kirse!']",https://arxiv.org/abs/2102.10117,"We present the first results from a Hubble Space Telescope WFC3/IR program, which obtained direct imaging and grism observations of galaxies near quasar sightlines with a high frequency of uncorrelated foreground Mg II absorption. These highly efficient observations targeted 54 Mg II absorbers along the line of sight to nine quasars at $z_{qso}\sim2$. We find that 89% of the absorbers in the range $0.64< z < 1.6$ can be spectroscopically matched to at least one galaxy with an impact parameter less than 200 kpc and $|\Delta z|/(1+z)<0.006$. We have estimated the star formation rates and measured structural parameters for all detected galaxies with impact parameters in the range 7-200 kpc and star formation rates greater than 1.3 M$_{\odot}$ yr$^{-1}$. We find that galaxies associated with Mg II absorption have significantly higher mean star formation rates and marginally higher mean star formation rate surface densities compared to galaxies with no detected Mg II. Nearly half of the Mg II absorbers match to more than one galaxy, and the mean equivalent width of the Mg II absorption is found to be greater for groups, compared to isolated galaxies. Additionally, we observe a significant redshift evolution in the physical extent of Mg II-absorbing gas around galaxies and evidence of an enhancement of Mg II within 50 degrees of the minor axis, characteristic of outflows, which persists to 80 kpc around the galaxies, in agreement with recent predictions from simulations. ","The Geometry of Cold, Metal-Enriched Gas Around Galaxies at $z\sim1.2$"
40,1364217858641829888,1140025148004810752,Pierre Arthuis,"['üóû New paper alert! üóû\n\nWe propose a new many-body expansion formalism for open-shell mid-mass nuclei. Additional perk: it comes with contributions derived at all orders!\n\n<LINK> <LINK>', 'In-Medium Similarity Renormalization Group has been a theory of choice for ab initio many-body practitioners, and with its Multi-Reference and Valence-Space counterparts have been instrumental in recent progress.\n\nFigure from H.Hergert, Front. Phys. 8:379, https://t.co/w21k8sXZ3d https://t.co/phNvwqC5CV', 'Though MR-IMSRG and VS-IMSRG are already able to tackle open-shell nuclei, they are pretty costly methods. Here we propose a single-reference, symmetry-breaking alternative, similar to the recently successful Bogoliubov MBPT.\n\nFigure from Tichai et al., https://t.co/4aNMvq0kCa https://t.co/R2tYsYlwQV', 'Because Bogoliubov IMSRG inherently relies on a simple commutator, the structure of its contributions is pretty well constrained. This makes for an easy automated generation of diagrams and expressions from the get go.\n\nFigure taken from our new paper, https://t.co/JjraJoxNKN https://t.co/iQtNKd46DQ', 'So with this new paper, we have updated the Automated Diagram Generator ADG to v3.0.0. It is now able to generate BIMSRG expressions at arbitrary orders and for traditional or exotic truncations.\n\nhttps://t.co/JDRNyc89Er']",https://arxiv.org/abs/2102.10889,"The goal of the present paper is twofold. First, a novel expansion many-body method applicable to superfluid open-shell nuclei, the so-called Bogoliubov in-medium similarity renormalization group (BIMSRG) theory, is formulated. This generalization of standard single-reference IMSRG theory for closed-shell systems parallels the recent extensions of coupled cluster, self-consistent Green's function or many-body perturbation theory. Within the realm of IMSRG theories, BIMSRG provides an interesting alternative to the already existing multi-reference IMSRG (MR-IMSRG) method applicable to open-shell nuclei. The algebraic equations for low-order approximations, i.e., BIMSRG(1) and BIMSRG(2), can be derived manually without much difficulty. However, such a methodology becomes already impractical and error prone for the derivation of the BIMSRG(3) equations, which are eventually needed to reach high accuracy. Based on a diagrammatic formulation of BIMSRG theory, the second objective of the present paper is thus to describe the third version (v3.0.0) of the ADG code that automatically (1) generates all valid BIMSRG(n) diagrams and (2) evaluates their algebraic expressions in a matter of seconds. This is achieved in such a way that equations can easily be retrieved for both the flow equation and the Magnus expansion formulations of BIMSRG. Expanding on this work, the first future objective is to numerically implement BIMSRG(2) (eventually BIMSRG(3)) equations and perform ab initio calculations of mid-mass open-shell nuclei. ","ADG: Automated generation and evaluation of many-body diagrams III.
  Bogoliubov in-medium similarity renormalization group formalism"
41,1364208954000220162,775002133041152001,Panagiotis Tsiotras,['New paper on multiagent consensus protocol that guards against adversarial eavesdropping now available on ArXiv\n<LINK>'],http://arxiv.org/abs/2102.10642,"We consider a multi-agent consensus problem in the presence of adversarial agents. The adversaries are able to listen to the inter-agent communications and try to estimate the state of the agents. The agents have a limited bit-rate for communication and are required to quantize the transmitted signal in order to meet the bit-rate constraint of the communication channel. We propose a consensus protocol that is protected against the adversaries, i.e., the expected mean-square error of the adversary state estimate is lower bounded. In order to deal with the bit-rate constraint, we propose a dynamic quantization scheme that guarantees protected consensus. ",Multi-Agent Consensus Subject to Communication and Privacy Constraints
42,1364193108079112193,963916467531067393,Nick Hawker,"[""Check out this new paper from the numerical team @FLFusion. It talks about modelling of thermonuclear fusion in a conical system. We're working to understand an old literature result which tests some important physics in unexpected ways. #fusion <LINK> <LINK>""]",https://arxiv.org/abs/2102.09466,"The role of flux-limited thermal conduction on the fusion performance of the uniaxially-driven targets studied by Derentowicz et al.; Jour. Tech. Phys. 18, 465 (1977) and Jour. Tech. Phys. 25, 135 (1977), is explored as part of a wider effort to understand and quantify uncertainties in ICF systems sharing similarities with First Light Fusion's projectile-driven concept. We examine the role of uncertainties in plasma microphysics and different choices for the numerical implementation of the conduction operator on simple metrics encapsulating the target performance. The results indicate that choices which affect the description of ionic heat flow between the heated fusion fuel and the gold anvil used to contain it are the most important. The electronic contribution is found to be robustly described by local diffusion. The sensitivities found suggest a prevalent role for quasi-nonlocal ionic transport, especially in the treatment of conduction across material interfaces with strong gradients in temperature and conductivity. We note that none of the simulations produce neutron yields which substantiate those reported by Derentowicz et al. Jour. Tech. Phys. 25, 135 (1977), leaving open future studies aimed at more fully understanding this class of ICF systems. ","A preliminary assessment of the sensitivity of uniaxially-driven fusion
  targets to flux-limited thermal conduction modeling"
43,1364114742160351232,1392935011,Ole-Chr. Granmo,"[""New paper from @cairuia's talented Rupsa Saha, with co-authors @mortengoodwin  and @vizadorozhny! She has designed the first Relational #TsetlinMachine, which reasons with relations, variables, and constants.  @uiagder  <LINK> #ML #MachineLearning #AI #NLP <LINK>""]",https://arxiv.org/abs/2102.10952,"TMs are a pattern recognition approach that uses finite state machines for learning and propositional logic to represent patterns. In addition to being natively interpretable, they have provided competitive accuracy for various tasks. In this paper, we increase the computing power of TMs by proposing a first-order logic-based framework with Herbrand semantics. The resulting TM is relational and can take advantage of logical structures appearing in natural language, to learn rules that represent how actions and consequences are related in the real world. The outcome is a logic program of Horn clauses, bringing in a structured view of unstructured data. In closed-domain question-answering, the first-order representation produces 10x more compact KBs, along with an increase in answering accuracy from 94.83% to 99.48%. The approach is further robust towards erroneous, missing, and superfluous information, distilling the aspects of a text that are important for real-world understanding. ","A Relational Tsetlin Machine with Applications to Natural Language
  Understanding"
44,1364105189544894465,16079444,Ying-Jer Kao,['New paper with Yu-Hseuh and Ching-Yu\n \n<LINK>'],https://arxiv.org/abs/2102.10980,"We propose a unified scheme to identify phase transitions out of the $\mathbb{Z}_2$ Abelian topological order, including the transition to a non-Abelian chiral spin liquid. Using loop gas and and string gas states [H.-Y. Lee, R. Kaneko, T. Okubo, N. Kawashima, Phys. Rev. Lett. 123, 087203 (2019)] on the star lattice Kitaev model as an example, we compute the overlap of minimally entangled states through transfer matrices. We demonstrate that, similar to the anyon condensation, continuous deformation of a $\mathbb{Z}_2$-injective projected entangled-pair state (PEPS) also allows us to study the transition between Abelian and non-Abelian topological orders. We show that the charge and flux anyons defined in the Abelian phase transmute into the $\sigma$ anyon in the non-Abelian topological order. Furthermore, we show that contrary to the claim in [Phys. Rev. B 101, 035140 (2020)], both the LG and SG states have infinite correlation length in the non-Abelian regime, consistent with the no-go theorem that a chiral PEPS has a gapless parent Hamiltonian. ","Detecting transition between Abelian and non-Abelian topological orders
  through symmetric tensor networks"
45,1364096212517855236,897971748,barnabe.eth,"['We have a new paper on #eip1559, written with @StefLeonardos, Dani√´l Reijsbergen, Stratis Skoulakis and Georgios Piliouras!\n\n""Dynamical Analysis of the EIP-1559 Ethereum Fee Market"" is available on arXiv: <LINK> <LINK>', ""It follows a shorter, workshop version of the paper that was accepted in WINE's blockchain workshop https://t.co/vtxm31Qqdr"", 'tl;dr\n- We show stability results of eip1559, as well as instability and chaotic behaviour\n- We perform simulations to interpret them in a more ""real"" context\n\nThis analysis will help design a more robust update rule, likely after 1559 is deployed and early data is available too', '1/ ""Stationary behaviour of eip1559"" showed that basefee eventually stabilises around some value when the environment is stationary (https://t.co/TS0MzjKgUV)\n\nIn our paper, we give a formal treatment to convergence results, based on the demand process and the basefee update rate https://t.co/isnUtGKbnW', '2/ In my simulations, there was some noisy behaviour, which at the time I ascribed to ""discontinuous behaviour"" (https://t.co/usuLQocgRp).\n\nThe oscillations were also noted in @algo_class \'s report on eip1559. https://t.co/49dp1IRzpL', 'Turns out, a much more general theory helps explain why these arise. When demand concentrates and the update rate is too high, the basefee cannot settle at its correct ""equilibrium"" point. Hence oscillations between full blocks and empty blocks. https://t.co/9wc5iNGiLr', 'The theory is based on Li-Yorke\'s ""Period 3 implies chaos"" paper (some notes: https://t.co/lw7PEgZbIa). Briefly, we have an update function that maps the current basefee to its update. If this function has points of period 3, the inputs give rise to chaotic behaviour!', 'In this plot, the blue curve crosses the diagonal line in multiple instances, meaning that the update function composed three times with itself (three successive updates) has a fixed point, meaning that the update function has points of period 3. https://t.co/VwP2AeB7HH', 'The analysis is done for user values which are uniformly distributed. In a sense it is the ""simplest"" case, which means that chaos could still exist in more general conditions.', 'The takeaway is that a higher update rate, a higher demand or a more concentrated demand all induce more chaotic conditions. This is also seen in the bifurcation diagrams: chaos happens whenever the line ""bifurcates"" (branches) https://t.co/e4FF8ky4Or', '(Interestingly these bifurcation diagrams have essential differences with other diagrams obtained for different games, e.g. routing games https://t.co/VWUuM2ip3I)', '3/ This is the theory, which is obtained for a stationary demand process (Poisson) and no transaction pool (i.e., no queuing).\n\nWe also checked empirically with simulations in which conditions chaos appears in practice, using the abm1559 library https://t.co/dxMsI8MSy7', 'Our demand is generated to reproduce a noisy arrival process with sudden spikes that decay over time, changing the variance of the process in treatment https://t.co/6JhNMbfwPU', 'In particular, we instantiated transaction pools which behave differently than they currently do. Our pools have an eviction policy which kicks out transactions with a fee cap too far below the current basefee. The least tolerant pool evicts any tx with fee cap &lt; basefee', 'We find that the more tolerant the pool is, the more high variance (~chaos) conditions we obtain. With this inventory of available transactions which are not currently includable, when basefee drops enough, suddenly the gates open and variance obtains. https://t.co/ZUSYpc2nV3', 'The variance of the demand process as well as its general level also correlate positively with greater incidence of high variance conditions. https://t.co/cSTIGPZW86', 'These results should give us a better handle on both the guarantees of the eip1559 design, as well as its boundary conditions!']",https://arxiv.org/abs/2102.10567,"Participation in permissionless blockchains results in competition over system resources, which needs to be controlled with fees. Ethereum's current fee mechanism is implemented via a first-price auction that results in unpredictable fees as well as other inefficiencies. EIP-1559 is a recent, improved proposal that introduces a number of innovative features such as a dynamically adaptive base fee that is burned, instead of being paid to the miners. Despite intense interest in understanding its properties, several basic questions such as whether and under what conditions does this protocol self-stabilize have remained elusive thus far. We perform a thorough analysis of the resulting fee market dynamic mechanism via a combination of tools from game theory and dynamical systems. We start by providing bounds on the step-size of the base fee update rule that suffice for global convergence to equilibrium via Lyapunov arguments. In the negative direction, we show that for larger step-sizes instability and even formally chaotic behavior are possible under a wide range of settings. We complement these qualitative results with quantitative bounds on the resulting range of base fees. We conclude our analysis with a thorough experimental case study that corroborates our theoretical findings. ",Dynamical Analysis of the EIP-1559 Ethereum Fee Market
46,1364064956086280200,1091818701819498496,Hong-Ye HU,"['Our new paper ‚ÄúHamiltonian-driven shadow tomography of quantum states‚Äù<LINK> I find it is very interesting, so I also wrote a blog and created some cartoons during the weekend. <LINK>  üçæÔ∏è <LINK>']",https://arxiv.org/abs/2102.10132,"Classical shadow tomography provides an efficient method for predicting functions of an unknown quantum state from a few measurements of the state. It relies on a unitary channel that efficiently scrambles the quantum information of the state to the measurement basis. Facing the challenge of realizing deep unitary circuits on near-term quantum devices, we explore the scenario in which the unitary channel can be shallow and is generated by a quantum chaotic Hamiltonian via time evolution. We provide an unbiased estimator of the density matrix for all ranges of the evolution time. We analyze the sample complexity of the Hamiltonian-driven shadow tomography. For Pauli observables, we find that it can be more efficient than the unitary-2-design-based shadow tomography in a sequence of intermediate time windows that range from an order-1 scrambling time to a time scale of $D^{1/6}$, given the Hilbert space dimension $D$. In particular, the efficiency of predicting diagonal Pauli observables is improved by a factor of $D$ without sacrificing the efficiency of predicting off-diagonal Pauli observables. ",Hamiltonian-Driven Shadow Tomography of Quantum States
47,1364023824606494722,760143004275503108,Sameer Deshpande,['This is among my favorite parts of our new paper! It turns out an estimator with lower loss can have larger risk on &gt; 50% of datasets (keeping parameter fixed). <LINK> <LINK>'],https://arxiv.org/abs/2102.09705,"Modern statistics provides an ever-expanding toolkit for estimating unknown parameters. Consequently, applied statisticians frequently face a difficult decision: retain a parameter estimate from a familiar method or replace it with an estimate from a newer or complex one. While it is traditional to compare estimators using risk, such comparisons are rarely conclusive in realistic settings. In response, we propose the ""c-value"" as a measure of confidence that a new estimate achieves smaller loss than an old estimate on a given dataset. We show that it is unlikely that a computed c-value is large and that the new estimate has larger loss than the old. Therefore, just as a small p-value provides evidence to reject a null hypothesis, a large c-value provides evidence to use a new estimate in place of the old. For a wide class of problems and estimators, we show how to compute a c-value by first constructing a data-dependent high-probability lower bound on the difference in loss. The c-value is frequentist in nature, but we show that it can provide a validation of Bayesian estimates in real data applications involving hierarchical models and Gaussian processes. ",Confidently Comparing Estimators with the c-value
48,1363911435525398529,92002686,Xiang 'Anthony' Chen,"['üì¢New #CHI2021 paper ""Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision (BLV) Users"" extracts appearance info of online products for BLV users\n\nüîó: <LINK>\nüìÑ: <LINK>\nüìΩÔ∏è: <LINK> <LINK>']",https://arxiv.org/abs/2102.00576,"Online shopping has become a valuable modern convenience, but blind or low vision (BLV) users still face significant challenges using it, because of: 1) inadequate image descriptions and 2) the inability to filter large amounts of information using screen readers. To address those challenges, we propose Revamp, a system that leverages customer reviews for interactive information retrieval. Revamp is a browser integration that supports review-based question-answering interactions on a reconstructed product page. From our interview, we identified four main aspects (color, logo, shape, and size) that are vital for BLV users to understand the visual appearance of a product. Based on the findings, we formulated syntactic rules to extract review snippets, which were used to generate image descriptions and responses to users' queries. Evaluations with eight BLV users showed that Revamp 1) provided useful descriptive information for understanding product appearance and 2) helped the participants locate key information efficiently. ","Revamp: Enhancing Accessible Information Seeking Experience of Online
  Shopping for Blind or Low Vision Users"
49,1363885588328239108,14211199,Gustavo Lacerda üá∫üá¶ in Berkeley,"[""This Tuesday Feb/23 at 6pm Pacific @HonglerClement will present our new protocol for mathematical debate #SPRIG. Paper: <LINK>\n\nAt @gauntletnetwork's ClubHouse room:\n<LINK>""]",https://arxiv.org/abs/2102.03044,"Modern mathematics is built on the idea that proofs should be translatable into formal proofs, whose validity is an objective question, decidable by a computer. Yet, in practice, proofs are informal and may omit many details. An agent considers a proof valid if they trust that it could be expanded into a machine-verifiable proof. A proof's validity can thus become a subjective matter and lead to a debate, which may be difficult to settle. Hence, while the concept of valid proof is well-defined, the process to establish validity is itself a complex multi-agent problem. We introduce the SPRIG protocol. SPRIG allows agents to propose and verify succinct and informative proofs in a decentralized fashion; the trust is established by agents being able to request more details in the proof steps; debates, if they arise, must isolate details of proofs and, if they persist, go down to machine-level details, where they are automatically settled. A structure of bounties and stakes is set to incentivize agents to act in good faith. We propose a game-theoretic discussion of SPRIG, showing how agents with various types of information interact, leading to a proof tree with an appropriate level of detail and to the invalidation of wrong proofs, and we discuss resilience against various attacks. We then analyze a simplified model, characterize its equilibria and compute the agents' level of trust. SPRIG is designed to run as a smart contract on a blockchain platform. This allows anonymous agents to participate in the verification debate, and to contribute with their information. The smart contract mediates the interactions, settles debates, and guarantees that bounties and stakes are paid as specified. SPRIG enables new applications, such as the issuance of bounties for open problems, and the creation of derivatives markets, allowing agents to inject more information pertaining to proofs. ","Smart Proofs via Smart Contracts: Succinct and Informative Mathematical
  Derivations via Decentralized Markets"
50,1363847490190135297,114485232,Jimmy Lin,"[""We've released a new version (v0.11.0.0) of our Pyserini Python toolkit to support replicable IR research, now providing first-stage retrieval for sparse, dense, and hybrid representations. <LINK> Our new arXiv paper provides an overview: <LINK>"", ""Also of interest in the paper is the culture of replicability we've been trying to cultivate in the group: both social processes (dog-fooding, replicability as a shared norm, properly-aligned incentive structures) as well as technical infrastructure (regression testing).""]",https://arxiv.org/abs/2102.10073,"Pyserini is an easy-to-use Python toolkit that supports replicable IR research by providing effective first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. We also describe how our group has built a culture of replicability through shared norms and tools that enable rigorous automated testing. ","Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR
  Research with Sparse and Dense Representations"
51,1363720587500269575,64958481,Birhanu Eshete,"[""New paper üì¢:\nWe empirically explore the combination of additive secret sharing and differential privacy for privacy-preserving collaborative prediction with minimal loss in model accuracy.\nPaper: <LINK>\nTo appear at IWSPA'21 co-located with @acmcodaspy '21 <LINK>""]",https://arxiv.org/abs/2102.09751,"When multiple parties that deal with private data aim for a collaborative prediction task such as medical image classification, they are often constrained by data protection regulations and lack of trust among collaborating parties. If done in a privacy-preserving manner, predictive analytics can benefit from the collective prediction capability of multiple parties holding complementary datasets on the same machine learning task. This paper presents PRICURE, a system that combines complementary strengths of secure multi-party computation (SMPC) and differential privacy (DP) to enable privacy-preserving collaborative prediction among multiple model owners. SMPC enables secret-sharing of private models and client inputs with non-colluding secure servers to compute predictions without leaking model parameters and inputs. DP masks true prediction results via noisy aggregation so as to deter a semi-honest client who may mount membership inference attacks. We evaluate PRICURE on neural networks across four datasets including benchmark medical image classification datasets. Our results suggest PRICURE guarantees privacy for tens of model owners and clients with acceptable accuracy loss. We also show that DP reduces membership inference attack exposure without hurting accuracy. ","PRICURE: Privacy-Preserving Collaborative Inference in a Multi-Party
  Setting"
52,1363693025596116992,245956666,Lina Necib,['New paper led by Jacob Shen (Caltech) on dissipative Dark Matter in dwarf galaxies!!! <LINK>'],https://arxiv.org/abs/2102.09580,"We present the first set of cosmological baryonic zoom-in simulations of galaxies including dissipative self-interacting dark matter (dSIDM). These simulations utilize the Feedback In Realistic Environments (FIRE-2) galaxy formation physics, but allow the dark matter to have dissipative self-interactions analogous to Standard Model forces, parameterized by the self-interaction cross-section per unit mass, $(\sigma/m)$, and the dimensionless degree of dissipation, $0<f_{\rm diss}<1$. We survey this parameter space, including constant and velocity-dependent cross-sections, and focus on structural and kinematic properties of dwarf galaxies with $M_{\rm halo} \simeq 10^{10-11} {\rm M}_{\odot}$. Central density profiles of simulated dwarfs become cuspy when $(\sigma/m)_{\rm eff} \gtrsim 0.1\,{\rm cm^{2}\,g^{-1}}$ (and $f_{\rm diss}=0.5$ as fiducial). The power-law slopes asymptote to $\alpha \approx -1.5$ in low-mass dwarfs independent of cross-section, which arises from a dark matter ""cooling flow"". Through comparisons with dark matter only simulations, we find the profile in this regime is insensitive to the inclusion of baryons. However, when $(\sigma/m)_{\rm eff} \ll 0.1\,{\rm cm^{2}\,g^{-1}}$, baryonic effects can produce cored density profiles comparable to non-dissipative cold dark matter (CDM) runs but at smaller radii. Simulated galaxies with $(\sigma/m) \gtrsim 10\,{\rm cm^{2}\,g^{-1}}$ develop significant coherent rotation of dark matter, accompanied by halo deformation, but this is unlike the well-defined thin ""dark disks"" often attributed to baryon-like dSIDM. The density profiles in this high cross-section model exhibit lower normalizations given the onset of halo deformation. For our surveyed dSIDM parameters, halo masses and galaxy stellar masses do not show appreciable difference from CDM, but dark matter kinematics and halo concentrations/shapes can differ. ","Dissipative Dark Matter on FIRE: I. Structural and kinematic properties
  of dwarf galaxies"
53,1362805401608220674,963916467531067393,Nick Hawker,['Check out a new paper from the numerical team @FLFusion  on ArXiv. We trying to model an old experiment from the literature which turns out to have much richer plasma physics than we ever thought it would. More coming later in the year. #fusion #plasma <LINK>'],https://arxiv.org/abs/2102.09466,"The role of flux-limited thermal conduction on the fusion performance of the uniaxially-driven targets studied by Derentowicz et al.; Jour. Tech. Phys. 18, 465 (1977) and Jour. Tech. Phys. 25, 135 (1977), is explored as part of a wider effort to understand and quantify uncertainties in ICF systems sharing similarities with First Light Fusion's projectile-driven concept. We examine the role of uncertainties in plasma microphysics and different choices for the numerical implementation of the conduction operator on simple metrics encapsulating the target performance. The results indicate that choices which affect the description of ionic heat flow between the heated fusion fuel and the gold anvil used to contain it are the most important. The electronic contribution is found to be robustly described by local diffusion. The sensitivities found suggest a prevalent role for quasi-nonlocal ionic transport, especially in the treatment of conduction across material interfaces with strong gradients in temperature and conductivity. We note that none of the simulations produce neutron yields which substantiate those reported by Derentowicz et al. Jour. Tech. Phys. 25, 135 (1977), leaving open future studies aimed at more fully understanding this class of ICF systems. ","A preliminary assessment of the sensitivity of uniaxially-driven fusion
  targets to flux-limited thermal conduction modeling"
54,1362767329680109591,1868847132,Emily Deibert,"['New paper on the arXiv today (and coming soon to AJ)! We (@DrRayJay, @AstroAndrew123, and others) used near-IR observations from @CARMENES_exopl and SPIRou @CFHTelescope to study the atmosphere of super-hot super-Earth 55 Cancri e üî≠\n<LINK>', '@RonDeibert Thank you! It was 2 years in the making.', ""@di_goldene_pave Thanks so much Aaron! It took a while so I'm glad to get it out there üòÖ""]",https://arxiv.org/abs/2102.08965,"We present high-resolution near-infrared spectra taken during eight transits of 55 Cancri e, a nearby low-density super-Earth with a short orbital period (< 18 hours). While this exoplanet's bulk density indicates a possible atmosphere, one has not been detected definitively. Our analysis relies on the Doppler cross-correlation technique, which takes advantage of the high spectral resolution and broad wavelength coverage of our data, to search for the thousands of absorption features from hydrogen-, carbon-, and nitrogen-rich molecular species in the planetary atmosphere. Although we are unable to detect an atmosphere around 55 Cancri e, we do place strong constraints on the levels of HCN, NH${}_3$, and C${}_2$H${}_2$ that may be present. In particular, at a mean molecular weight of 5 amu we can rule out the presence of HCN in the atmosphere down to a volume mixing ratio (VMR) of 0.02%, NH${}_3$ down to a VMR of 0.08%, and C${}_2$H${}_2$ down to a VMR of 1.0%. If the mean molecular weight is relaxed to 2 amu, we can rule out the presence of HCN, NH${}_3$, and C${}_2$H${}_2$ down to VMRs of 0.001%, 0.0025%, and 0.08% respectively. Our results reduce the parameter space of possible atmospheres consistent with the analysis of HST/WFC3 observations by Tsiaras et al. (2016), and indicate that if 55 Cancri e harbors an atmosphere, it must have a high mean molecular weight and/or clouds. ",A Near-Infrared Chemical Inventory of the Atmosphere of 55 Cancri e
55,1362414525698306053,1072763353674608641,Jingyi Wang,"['Preprint is now available for our paper ""RobOT: Robustness-Oriented Testing for Deep Learning Systems"" @ICSEconf Check it out: <LINK>\nWe proposed a new testing metric and corresponding fuzzing algorithm for robustness improvement of DL models.']",https://arxiv.org/abs/2102.05913,"Recently, there has been a significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is deep learning testing, where adversarial examples (a.k.a.~bugs) of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the commonly used neuron coverage metrics by existing DL testing approaches are not correlated to model robustness. It is also not an effective measurement on the confidence of the model robustness after testing. In this work, we address this gap by proposing a novel testing framework called Robustness-Oriented Testing (RobOT). A key part of RobOT is a quantitative measurement on 1) the value of each test case in improving model robustness (often via retraining), and 2) the convergence quality of the model robustness improvement. RobOT utilizes the proposed metric to automatically generate test cases valuable for improving model robustness. The proposed metric is also a strong indicator on how well robustness improvement has converged through testing. Experiments on multiple benchmark datasets confirm the effectiveness and efficiency of RobOT in improving DL model robustness, with 67.02% increase on the adversarial robustness that is 50.65% higher than the state-of-the-art work DeepGini. ",RobOT: Robustness-Oriented Testing for Deep Learning Systems
56,1362358042482851844,27686902,Dr Becky Smethurst,"['Take a look at some of the galaxy classifications done by volunteers on @galaxyzoo on the DECaLS imaging! Mike has put together a fun interface for you to explore them yourself \n\nAll from our new research paper published today: <LINK> <LINK>', '@makizdat In astronomy it‚Äôs usually who contributed the most. The first author is usually the main writer of the text. And then it‚Äôs alphabetical at a certain point when there‚Äôs bigger collaborations. In chemistry though, it‚Äôs usually the most senior person comes last']",https://arxiv.org/abs/2102.08414,"We present Galaxy Zoo DECaLS: detailed visual morphological classifications for Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8 footprint. Deeper DECaLS images (r=23.6 vs. r=22.2 from SDSS) reveal spiral arms, weak bars, and tidal features not previously visible in SDSS imaging. To best exploit the greater depth of DECaLS images, volunteers select from a new set of answers designed to improve our sensitivity to mergers and bars. Galaxy Zoo volunteers provide 7.5 million individual classifications over 314,000 galaxies. 140,000 galaxies receive at least 30 classifications, sufficient to accurately measure detailed morphology like bars, and the remainder receive approximately 5. All classifications are used to train an ensemble of Bayesian convolutional neural networks (a state-of-the-art deep learning method) to predict posteriors for the detailed morphology of all 314,000 galaxies. When measured against confident volunteer classifications, the networks are approximately 99% accurate on every question. Morphology is a fundamental feature of every galaxy; our human and machine classifications are an accurate and detailed resource for understanding how galaxies evolve. ","Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from
  Volunteers and Deep Learning for 314,000 Galaxies"
57,1362251312285372417,962876421268914177,Hanlin Ren,"['<LINK>\nNew paper with Yong Gu! We further improve the preprocessing time of Distance Sensitivity Oracles (DSOs) to O(n^{2.5794}) (also with constant query time). This is the first DSO below the ""n^{8/3}-preprocessing barrier"" (footnote 2 of the paper).\n(1/5)', ""Again, in case you're not familiar with the problem: You are given a directed graph, and you want to preprocess it and answer the following queries: Given vertices u, v, x, what is the length of the shortest path from u to v ùíèùíêùíï ùíàùíêùíäùíèùíà ùíïùíâùíìùíêùíñùíàùíâ ùíô?\n(2/5)"", 'Two techniques. The first one is an algebraic approach inspired by Brand &amp; Saranurak @eig [FOCS\'19]: the ùíÇùíÖùíãùíêùíäùíèùíï of the ""symbolic adjacency matrix"" encodes shortest path information. A vertex failure corresponds to a ùíçùíêùíò-ùíìùíÇùíèùíå\nupdate which is easy to maintain.\n(3/5)', 'The second one is an algorithm to compute ùíñùíèùíäùííùíñùíÜ shortest paths in current APSP time. (If we could handle very large weights, we could add random perturbation to the edge weights and simply compute APSP. But here we are dealing with small integer weights.)\n(4/5)', 'Finally, some drawback: our DSO only tells you the ùíçùíÜùíèùíàùíïùíâ of the shortest path, but does not provide the path quickly. This seems a common drawback of algebraic graph algorithms. :(\n(5/5)']",https://arxiv.org/abs/2102.08569,"We continue the study of distance sensitivity oracles (DSOs). Given a directed graph $G$ with $n$ vertices and edge weights in $\{1, 2, \dots, M\}$, we want to build a data structure such that given any source vertex $u$, any target vertex $v$, and any failure $f$ (which is either a vertex or an edge), it outputs the length of the shortest path from $u$ to $v$ not going through $f$. Our main result is a DSO with preprocessing time $O(n^{2.5794}M)$ and constant query time. Previously, the best preprocessing time of DSOs for directed graphs is $O(n^{2.7233}M)$, and even in the easier case of undirected graphs, the best preprocessing time is $O(n^{2.6865}M)$ [Ren, ESA 2020]. One drawback of our DSOs, though, is that it only supports distance queries but not path queries. Our main technical ingredient is an algorithm that computes the inverse of a degree-$d$ polynomial matrix (i.e. a matrix whose entries are degree-$d$ univariate polynomials) modulo $x^r$. The algorithm is adapted from [Zhou, Labahn, and Storjohann, Journal of Complexity, 2015], and we replace some of its intermediate steps with faster rectangular matrix multiplication algorithms. We also show how to compute unique shortest paths in a directed graph with edge weights in $\{1, 2, \dots, M\}$, in $O(n^{2.5286}M)$ time. This algorithm is crucial in the preprocessing algorithm of our DSO. Our solution improves the $O(n^{2.6865}M)$ time bound in [Ren, ESA 2020], and matches the current best time bound for computing all-pairs shortest paths. ",Constructing a Distance Sensitivity Oracle in $O(n^{2.5794}M)$ Time
58,1362039463329767425,79918104,Chris Amato,['Really excited about our new AAMAS paper exploring centralized vs. decentralized critics in multi-agent reinforcement learning. We managed to correct some common misconceptions and open up some interesting questions: <LINK>'],https://arxiv.org/abs/2102.04402,"Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers. ","Contrasting Centralized and Decentralized Critics in Multi-Agent
  Reinforcement Learning"
59,1362016738041356288,16674284,Mirco Musolesi,"['New paper to be presented at @AAMAS2021: ""Cooperation and Reputation Dynamics with Reinforcement Learning"" \\w @nicanastassacos @garcia_juliang and Steve Hailes <LINK> #AAMAS2021 <LINK>']",https://arxiv.org/abs/2102.07523,"Creating incentives for cooperation is a challenge in natural and artificial systems. One potential answer is reputation, whereby agents trade the immediate cost of cooperation for the future benefits of having a good reputation. Game theoretical models have shown that specific social norms can make cooperation stable, but how agents can independently learn to establish effective reputation mechanisms on their own is less understood. We use a simple model of reinforcement learning to show that reputation mechanisms generate two coordination problems: agents need to learn how to coordinate on the meaning of existing reputations and collectively agree on a social norm to assign reputations to others based on their behavior. These coordination problems exhibit multiple equilibria, some of which effectively establish cooperation. When we train agents with a standard Q-learning algorithm in an environment with the presence of reputation mechanisms, convergence to undesirable equilibria is widespread. We propose two mechanisms to alleviate this: (i) seeding a proportion of the system with fixed agents that steer others towards good equilibria; and (ii), intrinsic rewards based on the idea of introspection, i.e., augmenting agents' rewards by an amount proportionate to the performance of their own strategy against themselves. A combination of these simple mechanisms is successful in stabilizing cooperation, even in a fully decentralized version of the problem where agents learn to use and assign reputations simultaneously. We show how our results relate to the literature in Evolutionary Game Theory, and discuss implications for artificial, human and hybrid systems, where reputations can be used as a way to establish trust and cooperation. ",Cooperation and Reputation Dynamics with Reinforcement Learning
60,1361883693074292736,29997510,Neel Dey,"['üì£ Our new paper (jointly led w/ the amazing @MengweiR) on injecting structural priors into ill-posed unpaired image translation tasks was accepted into IEEE Transactions on Medical Imaging!\n\nPre-print: <LINK>\nCode: <LINK>\n\nüßµbelow (1/x) <LINK>', 'Translation GANs are ubiquitous in medical imaging research, yet are not used in practice due to their tendency to hallucinate pathologies, break on ill-posed domain pairs, and corrupt translations w/ high-frequency noise.\n\nSemantic renormalization reduces all of the above. (2/x) https://t.co/lzhVt1gzbm', 'For example, when translating images with Multiple Sclerosis lesions on 3 different MRI scanners: standard GANs forget lesions, add new ones, invert their contrast, and are generally quite irritable.\n\nUsing segmentation-driven featurewise modulation, translations stabilize (3/x) https://t.co/HScOnYl4v8', 'Our model is also less sensitive to self-adversarial attacks wherein generators ""cheat"" discriminators by adding imperceptible noise to the translations. As medical image translation aims for downstream usage, imperceptible corruptions may introduce unexpected failure modes (4/x) https://t.co/jWvbOzYBf9', 'The proposed method has benefits even when the image domains are not too dissimilar and only very rough segmentation masks are available/can be generated. Here, we translate between scanning sites on the IXI T1w MRI dataset using rough segmentations generated using FSL FAST (5/x) https://t.co/P1D2asZgAd', ""Lastly, this anatomically-modulated approach typically outperforms standard baselines when using the usual methods of evaluating downstream medical image translations (i.e., post-hoc segmentation accuracy, inception distances) as well. We're happy to hear any feedback! (6/6) https://t.co/6ehCR2Fpcf""]",https://arxiv.org/abs/2102.06315,"Deep networks are now ubiquitous in large-scale multi-center imaging studies. However, the direct aggregation of images across sites is contraindicated for downstream statistical and deep learning-based image analysis due to inconsistent contrast, resolution, and noise. To this end, in the absence of paired data, variations of Cycle-consistent Generative Adversarial Networks have been used to harmonize image sets between a source and target domain. Importantly, these methods are prone to instability, contrast inversion, intractable manipulation of pathology, and steganographic mappings which limit their reliable adoption in real-world medical imaging. In this work, based on an underlying assumption that morphological shape is consistent across imaging sites, we propose a segmentation-renormalized image translation framework to reduce inter-scanner heterogeneity while preserving anatomical layout. We replace the affine transformations used in the normalization layers within generative networks with trainable scale and shift parameters conditioned on jointly learned anatomical segmentation embeddings to modulate features at every level of translation. We evaluate our methodologies against recent baselines across several imaging modalities (T1w MRI, FLAIR MRI, and OCT) on datasets with and without lesions. Segmentation-renormalization for translation GANs yields superior image harmonization as quantified by Inception distances, demonstrates improved downstream utility via post-hoc segmentation accuracy, and improved robustness to translation perturbation and self-adversarial attacks. ","Segmentation-Renormalized Deep Feature Modulation for Unpaired Image
  Harmonization"
61,1361813504114630658,1230579863444054018,Ziang Yan,"['Check out our new paper: <LINK>. We measure the tomographic cross-correlation between KiDS-1000 galaxy catalogue and Planck tSZ y map to study the thermodynamic property of intergalactic gas. (1/n)', ""This is my first project in the KiDS collaboration. I'm very happy to work in such a wonderful team! I've learned a lot about every detail in a cross-correlation-related project from my collaborators. Looking forward to my next one! (2/2)""]",https://arxiv.org/abs/2102.07701,"We constrain the redshift dependence of gas pressure bias $\left\langle b_{y} P_{\mathrm{e}}\right\rangle$ (bias-weighted average electron pressure), which characterises the thermodynamics of intergalactic gas, through a combination of cross-correlations between galaxy positions and the thermal Sunyaev-Zeldovich (tSZ) effect, as well as galaxy positions and the gravitational lensing of the cosmic microwave background (CMB). The galaxy sample is from the fourth data release of the Kilo-Degree Survey (KiDS). The tSZ $y$ map and the CMB lensing map are from the {\textit{Planck}} 2015 and 2018 data releases, respectively. The measurements are performed in five redshift bins with $z\lesssim1$. With these measurements, combining galaxy-tSZ and galaxy-CMB lensing cross-correlations allows us to break the degeneracy between galaxy bias and gas pressure bias, and hence constrain them simultaneously. In all redshift bins, the best-fit values of $\bpe$ are at a level of $\sim 0.3\, \mathrm{meV/cm^3}$ and increase slightly with redshift. The galaxy bias is consistent with unity in all the redshift bins. Our results are not sensitive to the non-linear details of the cross-correlation, which are smoothed out by the {\textit{Planck}} beam. Our measurements are in agreement with previous measurements as well as with theoretical predictions. We also show that our conclusions are not changed when CMB lensing is replaced by galaxy lensing, which shows the consistency of the two lensing signals despite their radically different redshift ranges. This study demonstrates the feasibility of using CMB lensing to calibrate the galaxy distribution such that the galaxy distribution can be used as a mass proxy without relying on the precise knowledge of the matter distribution. ","Probing galaxy bias and intergalactic gas pressure with KiDS
  Galaxies-tSZ-CMB lensing cross-correlations"
62,1361787544107552769,1513989272,Dr. Breanna Binder,"[""New paper on arXiv today (accepted to ApJ)! <LINK>\n\nThe Wolf-Rayet + Black Hole Binary NGC 300 X-1: What is the Mass of the Black Hole?\n\nTLDR; it's 17 +/- 4 Msun."", 'We used a new @chandraxray observation + 20 years worth of archival data from Chandra and @ESA_XMM to measure the orbital period: 32.7921 hours, with an uncertainty of 1.2 *seconds*.\n\nWe were able to measure (to really high precision) exactly where the X-ray eclipse occurs.', 'We also obtained four epochs of FUV spectroscopy with @NASAHubble (COS). Excellent ephemeris + timing of the X-ray eclipse means we could determine the orbital phases of our spectra (nearly perfect 0.00, 0.25, 0.50, and 0.75).', 'From this we were able to search for radial velocity variations in the UV spectral lines, and see which ones tracked the expected motion of the Wolf-Rayet donor star. We found one: the C IV 1550 emission line (notably, NOT the He II 1640 line!).', 'RV variations from C IV gave us the mass.\n\nBut there are some other nifty results in there too! We found evidence for a hotspot, where an accretion stream of Wolf-Rayet wind material slams into the black hole accretion disk (likely source of He II 1640 and 4686 emission lines!)', ""The Wolf-Rayet star also doesn't fill its Roche lobe - mass transfer isn't happening via standard Roche lobe overflow. Instead, material from the Wolf-Rayet winds are being focused within the Roche lobe of the *black hole*; the accretion disk is *wind-fed*."", ""I'll end by saying a LOT of this work (esp. the UV spectral modeling) was aided by my undergrad Janelle Sy.\n\nShe's applied to *lots* of grad schools. So if you see her name among your lists of applicants, you DEFINITELY want her in your program!""]",https://arxiv.org/abs/2102.07065,"We present new X-ray and UV observations of the Wolf-Rayet + black hole binary system NGC 300 X-1 with the Chandra X-ray Observatory and the Hubble Space Telescope Cosmic Origins Spectrograph. When combined with archival X-ray observations, our X-ray and UV observations sample the entire binary orbit, providing clues to the system geometry and interaction between the black hole accretion disk and the donor star wind. We measure a binary orbital period of 32.7921$\pm$0.0003 hr, in agreement with previous studies, and perform phase-resolved spectroscopy using the X-ray data. The X-ray light curve reveals a deep eclipse, consistent with inclination angles of $i=60-75^{\circ}$, and a pre-eclipse excess consistent with an accretion stream impacting the disk edge. We further measure radial velocity variations for several prominent FUV spectral lines, most notably He II $\lambda$1640 and C IV $\lambda$1550. We find that the He II emission lines systematically lag the expected Wolf-Rayet star orbital motion by a phase difference $\Delta \phi\sim0.3$, while C IV $\lambda$1550 matches the phase of the anticipated radial velocity curve of the Wolf-Rayet donor. We assume the C IV $\lambda$1550 emission line follows a sinusoidal radial velocity curve (semi-amplitude = 250 km s$^{-1}$) and infer a BH mass of 17$\pm$4 M$_{\odot}$. Our observations are consistent with the presence of a wind-Roche lobe overflow accretion disk, where an accretion stream forms from gravitationally focused wind material and impacts the edge of the black hole accretion disk. ","The Wolf-Rayet + Black Hole Binary NGC 300 X-1: What is the Mass of the
  Black Hole?"
63,1361767641732546570,2344176362,Dr. Caitlin Casey,"['So while Texas froze over yesterday, we published a new paper I‚Äôm eager to share with y‚Äôall!\n\nBurnham et al. (2021), an observational study about the physical driver of dust temperatures in galaxies! üßµ Link ‚û°Ô∏è\n<LINK>', 'This paper shows two things:\n\n1) SFR surface density correlates most closely with ISM dust temperature\n\n2) the Stefan-Boltzmann law, in all its simplicity, works pretty well to describe the integrated luminosity, size, and temperature of galaxies‚Äô ISM', 'I had a real pleasure working with Anne Burnham, the lead author (formerly a post-bac student @UTAstronomy), on this project. She joined @YaleAstronomy as a grad student this year! Here‚Äôs a summary of Anne‚Äôs findings...', 'We gathered some @ALMAobs high-resolution imaging of a set of high-z galaxies that spanned a wide range of dust temperatures from hot to cold with the goal of seeing if the spatial distribution of the dust had something to do with their temperatures. https://t.co/movRwlVz3x', 'In theory, they should! Intro astronomy classes all teach us about the Stefan-Boltzmann Law (SB) that work well for stars: if two stars have the same luminosity, but one is much larger, it should also be cooler.  Size ‚¨ÜÔ∏è Temperature ‚¨áÔ∏è. https://t.co/zHS5QDi3gt', 'So would we expect L=4 pi R^2 sigma T^4 to hold for galaxies? Well, for better or worse,  stars != galaxies.  The former are spherical &amp; optically thick, simple*, and the latter are‚Ä¶ a mess.\n\n*stars people please don‚Äôt kill me for saying stars are simple', 'Our hypothesis was that the measured sizes of the dust emitting region in these galaxies would indeed relate closely to the emergent infrared luminosity and (luminosity-weighted) dust temperatures. I‚Äôll get back to this shortly, but first...', 'The literature has said that very close correlations exist btw dust temperature and total star-formation rate (SFR), specific SFR (sSFR), and distance to the galaxy ‚Äòmain sequence‚Äô (MS).  In this paper we test the correlation btw temperature and these four quantities. https://t.co/zZlSqDhe16', 'Our first conclusion. is that we find that SFR and SFR surface density (Sigma_IR) correlate very closely with luminosity-weighted dust temperature.  sSFR and D_MS are much weaker correlations. https://t.co/7PLTA1ayBF', 'Our result for high-z galaxies is well aligned with results in the local Universe (Lutz et al. 2016) who analyze different scaling relations, including dust temp, SFR, Sigma_IR, and sSFR.', 'This suggests that SFR surface density is more fundamentally linked to the galaxy-wide ISM temperature than quantities related to the stellar mass of the galaxy ‚Äî and this makes sense thinking back to the SB law, where SFR surface density (L/R^2) would track roughly as T^4.', 'Note that we use the observable quantity rest-frame peak wavelength (lambda_peak) instead of dust temperature, because it‚Äôs independent of model opacity.\n(More on that in the paper, and almost every other paper I‚Äôve co-written since 2017 üòÇ..)', 'Now back to the inconvenient fact that galaxies != stars. One big problem with the tidy correlation btw temperature and SFR surface density is that the ISM is typically optically thin, and are cartoon SB law correlation exists for an optically thick medium.', 'This issue came up in Simpson et al. (2017), who found that galaxies‚Äô temperatures ‚Äî assuming optically thin modified blackbodies ‚Äî align with predictions from the SB law.  Optically thick MBBs don‚Äôt. That doesn‚Äôt make sense physically. https://t.co/CM410j675n', 'We suggest is that a modification of the SB law for galaxies, assuming a pancake like geometry with embedded SF regions, can fix this dilemma.  The optically thick temperatures then make sense. https://t.co/K4noRz7yHN', 'But is the ISM optically thick? Our textbooks suggest not, right? What‚Äôs the deal?\n\nThe temp we measure here is *luminosity-weighted*.  Galaxies are made up of ISM of many temperatures, and the L-weighted temp is naturally higher than the mass-weighted temp.', 'Our 2nd conclusion: because the L-weighted temperature is hotter, it‚Äôs also more likely probing more dense, optically thick portions of the ISM. So the assumption of an optically thick ISM works in this case, at least for somewhat dusty star-forming galaxies at high-redshift!']",https://arxiv.org/abs/2102.06250,"The underlying distribution of galaxies' dust SEDs (i.e., their spectra re-radiated by dust from rest-frame $\sim$3$\mu$m-3mm) remains relatively unconstrained due to a dearth of FIR/(sub)mm data for large samples of galaxies. It has been claimed in the literature that a galaxy's dust temperature -- observed as the wavelength where the dust SED peaks ($\lambda_{peak}$) -- is traced most closely by its specific star-formation rate (sSFR) or parameterized 'distance' to the SFR-M$_\star$ relation (the galaxy 'main sequence'). We present 0.24"" resolved 870$\mu$m ALMA dust continuum observations of seven $z=1.4-4.6$ dusty star-forming galaxies (DSFGs) chosen to have a large range of well-constrained luminosity-weighted dust temperatures. We also draw on similar resolution dust continuum maps from a sample of ALESS submillimeter galaxies from Hodge et al. (2016). We constrain the physical scales over which the dust radiates and compare those measurements to characteristics of the integrated SED. We confirm significant correlations of $\lambda_{peak}$ with both L$_{IR}$ (or SFR) and $\Sigma_{\rm IR}$ ($\propto$SFR surface density). We investigate the correlation between $\log_{10}$($\lambda_{peak}$) and $\log_{10}$($\Sigma_{\rm IR}$) and find the relation to hold as would be expected from the Stefan-Boltzmann Law, or the effective size of an equivalent blackbody. The correlations of $\lambda_{peak}$ with sSFR and distance from the SFR-M$_\star$ relation are less significant than those for $\Sigma_{\rm IR}$ or L$_{IR}$; therefore, we conclude that the more fundamental tracer of galaxies' luminosity-weighted integrated dust temperatures are indeed their star-formation surface densities in line with local Universe results, which relate closely to the underlying geometry of dust in the ISM. ","The Physical Drivers of the Luminosity-Weighted Dust Temperatures in
  High-Redshift Galaxies"
64,1361682746641436677,1030500158159695872,Patrick Lewis,"['üö® New work üö® ‚ÄúPAQ: 65 Million Probably-Asked Questions and What You Can Do With Them‚Äù. \nRead the paper here: <LINK>, and check out the thread below \nw/ @mindjimmy,@likicode, @PMinervini, @HeinrichKuttler,@olapiktus, Pontus Stenetorp, @riedelcastro. 1/N <LINK>', 'Retrieve-and-read Open-Domain QA models achieve high accuracy, but they have a number of downsides - They are generally big, slow and expensive, because they have to keep around a whole background corpus, and read lots of text for every question. 2/N', 'Alternatives like closed-book QA and QA-pair retrievers have faster inference, smaller sizes &amp; (for QA-pair retrievers) other useful properties (see later!). But they rely on memorizing (only ~100K) training QA-pairs. This lack of coverage limits accuracy WRT retrieve&amp;read 3/N', 'PAQ is a huge collection of QA-pairs, generated automatically from Wikipedia. They are generated to be likely to be asked by humans, making them great for ODQA. We use a novel ‚Äúglobal consistency filter‚Äù to reject poor/ambiguous Qs, which is critical to downstream QA results 4/N', 'We build a QA-pair retriever, RePAQ which, fir given a test question, retrieves the closest QA-pair from PAQ and returns its answer. RePAQ consists of a dense neural MIPS retriever, optionally followed by a neural reranker.  5/N', 'RePAQ is super flexible. If speed matters, RePAQ‚Äôs dense retriever can answer over 1000 questions/sec at 41% accuracy on NQ. If accuracy matters more, applying the reranker boosts scores to 48%, outperforming retrieve-and-read models like RAG, and it‚Äôs still fast - 6 Qs/sec 6/N https://t.co/T7q5kXJQSn', ""RePAQ ‚Äúknows when it doesn‚Äôt know‚Äù, allowing it to refuse to answer when it's likely to be wrong.  We can use this to first ask the fast &amp; precise RePAQ, then if confidence is low, back off to a SoTA model. This is BOTH more accurate and 2x faster than the SoTA model alone 7/N https://t.co/hyZoCvMRso"", 'You can update RePAQ‚Äôs QA-pair KB at test time, giving fine control over system size. RePAQ can be shrunk to very small sizes before accuracy degrades. We used variants of RePAQ to win 2 tracks at the 2020 EfficientQA competition (see https://t.co/xaOvyTfWqX for more details) 8/N', 'We also train a Closed-Book QA BART-large on PAQ. Results improve over models trained on standard training data by 5%, but trails RePAQ by &gt;15%. This suggests the BART-large struggles to memorize all the knowledge in PAQ, &amp; demonstrates the effectiveness of explicit retrieval 9/N', 'Check the paper out for full details! https://t.co/MA8BTtWBi6  10/10']",https://arxiv.org/abs/2102.07033,"Open-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to ``back-off"" to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone. ",PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them
65,1361663915072118784,806058672619212800,Guillaume Lample,"['New paper on code de-obfuscation: <LINK>\nWe show that if you obfuscate the name of identifiers in source code, a model can retrieve the original names with very high accuracy. It even works when you remove the name of each variable / function! 1/3 <LINK>', 'Retrieving masked variable names is much more difficult than retrieving randomly masked tokens, and turns out to be an excellent pretraining objective, that significantly outperforms Masked LM in unsupervised code translation and language code search. 2/3 https://t.co/7l8VWDP9OW', ""Also very useful if you try to understand other people's code when it is written with uninformative variable names!\nWith @b_roziere @MaLachaux @MarcSzafraniec 3/3 https://t.co/NF0qG68REm""]",https://arxiv.org/abs/2102.07492,"Recent advances in self-supervised learning have dramatically improved the state of the art on a wide variety of tasks. However, research in language model pre-training has mostly focused on natural languages, and it is unclear whether models like BERT and its variants provide the best pre-training when applied to other modalities, such as source code. In this paper, we introduce a new pre-training objective, DOBF, that leverages the structural aspect of programming languages and pre-trains a model to recover the original version of obfuscated source code. We show that models pre-trained with DOBF significantly outperform existing approaches on multiple downstream tasks, providing relative improvements of up to 13% in unsupervised code translation, and 24% in natural language code search. Incidentally, we found that our pre-trained model is able to de-obfuscate fully obfuscated source files, and to suggest descriptive variable names. ",DOBF: A Deobfuscation Pre-Training Objective for Programming Languages
66,1361643489797234692,120814510,Haroldo V. Ribeiro,['Our new preprint introduces ordpy - a simple Python module for data analysis with methods derived from permutation entropy and ordinal networks. We also present a review of these methods. (w/ @pessarthur) \n\nCode: <LINK>\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2102.06786,"Since Bandt and Pompe's seminal work, permutation entropy has been used in several applications and is now an essential tool for time series analysis. Beyond becoming a popular and successful technique, permutation entropy inspired a framework for mapping time series into symbolic sequences that triggered the development of many other tools, including an approach for creating networks from time series known as ordinal networks. Despite the increasing popularity, the computational development of these methods is fragmented, and there were still no efforts focusing on creating a unified software package. Here we present ordpy, a simple and open-source Python module that implements permutation entropy and several of the principal methods related to Bandt and Pompe's framework to analyze time series and two-dimensional data. In particular, ordpy implements permutation entropy, Tsallis and R\'enyi permutation entropies, complexity-entropy plane, complexity-entropy curves, missing ordinal patterns, ordinal networks, and missing ordinal transitions for one-dimensional (time series) and two-dimensional (images) data as well as their multiscale generalizations. We review some theoretical aspects of these tools and illustrate the use of ordpy by replicating several literature results. ","ordpy: A Python package for data analysis with permutation entropy and
  ordinal network methods"
67,1361621180269154304,926551285788233728,Vincent Fortuin,"['Have you ever wondered whether isotropic Gaussian priors are good enough for your Bayesian neural network weights? They are often used in practice, but we find in our new paper (<LINK>) that they are indeed suboptimal! Details in thread. 1/13 <LINK>', 'To get an intuition for what kind of weight distributions work well in neural networks, we looked at the distributions of SGD-trained NNs. As we see in the figure above, in fully-connected networks, these distributions are much more heavy-tailed than Gaussians. 2/13', 'If we now use a more heavy-tailed prior for a BNN on the same task (in this case MNIST), we do not only improve the performance significantly, but also remove the cold posterior effect, as hypothesized by Wenzel et al. (https://t.co/tWjCtlhv3f). 3/13', 'These performance improvements also transfer to a different task than the original one from the SGD-training, in this case FashionMNIST. 4/13 https://t.co/ICbrcAnZlP', 'When looking at CNNs, we also find these heavy-tailed weight distributions in SGD-trained networks. 5/13 https://t.co/YH9vHCMkN1', 'Moreover, we find that CNNs often have strong empirical weight correlations, especially between weights within the same filter. 6/13 https://t.co/uIsDtJn2Oe', 'When introducing these observations into convolutional BNN priors, we find that the heavy-tailed priors again reduce the cold posterior effect, but do not improve performance. Conversely, the correlated priors improve performance, but do not reduce the cold posterior effect. 7/13 https://t.co/TPETCTX00F', 'Note also that we used data augmentation for CIFAR-10, which seems to strengthen the cold posterior effect. The cold posterior effect is thus less clearly connected to the prior in CNNs than in the fully-connected networks above. 8/13', 'All in all, we thus recommend to ditch the isotropic Gaussian priors and instead use heavy-tailed priors for fully-connected BNNs and correlated priors for convolutional BNNs. 9/13', 'For all these experiments in the paper, we tried to make assertions that are as accurate as possible with respect to the true posterior, so we used the GGMC inference from https://t.co/DkOYQbi7zK. 10/13', 'Moreover, we combined it with the cyclical learning rates from https://t.co/OwKHo4bYjg, and the preconditioner from https://t.co/tWjCtlhv3f. 11/13', 'We open-sourced our whole library for accurate SG-MCMC inference in BNNs with different priors at https://t.co/DaHzpFwvM1. We hope that it will foster future research into different BNN priors. 12/13', 'Finally, this work would not have been possible without our great team of collaborators, @AdriGarriga, @flwenz, @gxr, Richard Turner, @markvanderwilk, and @laurence_ai and the support from our institutions @ETH_en, @Cambridge_Uni, @imperialcollege, and @BristolUni. 13/13', '@EmtiyazKhan @_joaogui1 @sindero @JulyanArbel Thanks for the pointers!']",https://arxiv.org/abs/2102.06571,"Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets. ",Bayesian Neural Network Priors Revisited
68,1361577810553356292,270481144,Antoine Tilloy,"['I put on arxiv a new paper where I apply the variational method in relativistic quantum field theory in 1+1 dimensions. The novelty is that there is no cutoff, infrared or ultraviolet, as is usually the case, and so the results are truly variational\n\n<LINK>', 'The idea is to modify the continuous matrix product states (CMPS) introduced by @fverstraete and Ignacio Cirac in 2010. The CMPS already do the job in the non-relativistic case, but still require a UV cutoff for relativistic QFT.', ""The modification consists in changing of operator basis, to work in one that solves the short distance behavior exactly, and fits the singular UV behavior. This allows to bypass the last of Feynman's objections of 1987 against the variational method in relativistic QFT."", 'The idea is simple, and had been in my head for a while, but I only got the courage to do the (semi-tedious) computations recently. In my opinion, they are not super enlightening except for experts, so I dropped them in a second paper\n\nhttps://t.co/K0AMyG5tVp', ""@fverstraete Thanks, means a lot! For the entanglement entropy, since I softly break locality, I wouldn't know how to compute the usual one. But it does define a new analogous quantity that should be finite (but not sure if physically meaningful though)"", ""@fverstraete Thanks for the reference, I'll have a look"", ""@mmanuF In principle, CMPS have a generalization, continuous tensor networks states but it's much more difficult to use and do computations in practice. The renormalization of the Hamiltonian is also trickier in higher dimensions (need more than normal ordering)"", '@mmanuF So to be transparent, right now, I know only how to deal with 1+1 dimensions. There are a number of hurdles to lift before doing higher dimensions. I discuss it a bit at the end of the papers (short and long)']",https://arxiv.org/abs/2102.07733,"The variational method is a powerful approach to solve many-body quantum problems non perturbatively. However, in the context of relativistic quantum field theory (QFT), it needs to meet 3 seemingly incompatible requirements outlined by Feynman: extensivity, computability, and lack of UV sensitivity. In practice, variational methods break one of the 3, which translates into the need to have an IR or UV cutoff. In this letter, I introduce a relativistic modification of continuous matrix product states that satisfies the 3 requirements jointly in 1+1 dimensions. I apply it to the self-interacting scalar field, without UV cutoff and directly in the thermodynamic limit. Numerical evidence suggests the error decreases faster than any power law in the number of parameters, while the cost remains only polynomial. ",Variational method in relativistic quantum field theory without cutoff
69,1361520865888755717,222777271,Tan Van Vu,"['Happy to share another new paper by @tanvvu and @unlimitcycle entitled ""Toward Conjecture: Warming is Faster than Cooling"" that appeared in arXiv today.\n<LINK>']",https://arxiv.org/abs/2102.07429,"An asymmetry in thermal relaxation toward equilibrium has been uncovered for Langevin systems near stable minima [Phys. Rev. Lett. 125, 110602 (2020)]. It has been shown that, given the same degree of nonequilibrium of the initial distributions, relaxation from a lower temperature state (heating) is faster than that from a higher temperature state (cooling). In this study, we elucidate this relaxation asymmetry for discrete-state Markovian systems described by the master equation. We rigorously prove that heating is faster than cooling for arbitrary two-state systems, whereas for systems with more than two distinct energy levels, the relaxation asymmetry is no longer universal. Furthermore, for systems whose energy levels degenerate into two energy states, we find that there exist critical thresholds of the energy gap. Depending on the magnitude of the energy gap, heating can be faster or slower than cooling, irrespective of the transition rates between states. Our results clarify the relaxation asymmetry for discrete-state systems and reveal several hidden features inherent in thermal relaxation. ",Toward relaxation asymmetry: Heating is faster than cooling
70,1361518998655881220,222777271,Tan Van Vu,"['Our new paper by @tanvvu and @unlimitcycle entitled ""Lower Bound on Irreversibility in Thermal Relaxation of Open Quantum Systems"" appeared in arXiv.\n<LINK>']",https://arxiv.org/abs/2102.07348,"We consider thermal relaxation process of a quantum system attached to a single or multiple reservoirs. Quantifying the degree of irreversibility by entropy production, we prove that the irreversibility of the thermal relaxation is lower-bounded by a relative entropy between the unitarily-evolved state and the final state. The bound characterizes the state discrepancy induced by the non-unitary dynamics, thus reflecting the dissipative nature of irreversibility. Intriguingly, the bound can be evaluated solely in terms of the initial and final states and the system Hamiltonian; hence, providing a feasible way to estimate entropy production without prior knowledge of the underlying coupling structure. Our finding refines the second law of thermodynamics and reveals a universal feature of thermal relaxation processes. ","Lower Bound on Irreversibility in Thermal Relaxation of Open Quantum
  Systems"
71,1361494696271302660,11778512,Mason Porter,"['Here is one of my new papers out on arXiv tonight: ""Learning Low-Rank Latent Mesoscale Structures in Networks"": <LINK>\n\nby Hanbaek Lyu, Yacoub H. Kureh, Joshua Vendrow, and MAP\n\nThis paper is about ""Network Dictionary Learning"" <LINK>']",https://arxiv.org/abs/2102.06984,"It is common to use networks to encode the architecture of interactions between entities in complex systems in the physical, biological, social, and information sciences. Moreover, to study the large-scale behavior of complex systems, it is important to study mesoscale structures in networks as building blocks that influence such behavior. In this paper, we present a new approach for describing low-rank mesoscale structure in networks, and we illustrate our approach using several synthetic network models and empirical friendship, collaboration, and protein--protein interaction (PPI) networks. We find that these networks possess a relatively small number of `latent motifs' that together can successfully approximate most subnetworks at a fixed mesoscale. We use an algorithm that we call ""network dictionary learning"" (NDL), which combines a network sampling method and nonnegative matrix factorization, to learn the latent motifs of a given network. The ability to encode a network using a set of latent motifs has a wide range of applications to network-analysis tasks, such as comparison, denoising, and edge inference. Additionally, using our new network denoising and reconstruction (NDR) algorithm, we demonstrate how to denoise a corrupted network by using only the latent motifs that one learns directly from the corrupted networks. ",Learning low-rank latent mesoscale structures in networks
72,1361494657746804737,1047227002388959232,Giannis Daras,"['New paper: ""Intermediate Layer Optimization for Inverse Problems using Deep Generative Models"".\n\nPaper: <LINK>\n\nCode: <LINK>\n\nColab: <LINK>\n\nBelow a video of the Mona Lisa with inpainted eyes and a threadüßµ <LINK>', 'We propose Intermediate Layer Optimization (ILO), a novel optimization algorithm for solving inverse problems with deep generative models. Instead of optimizing only over the initial latent code, we progressively change the input layer obtaining more expressive generators.', 'We search for codes that lie within an l1 ball around the manifold of the previous layer. Our theoretical analysis shows that by keeping the radius of the ball small, we can improve the established error bound for compressed sensing with generative models.', 'We show that our framework is very general; it can be used for inpainting, denoising, super-resolution, compressed-sensing and even to generate human-like objects/animals from ImageNet!\n\nResults for inpainting  (the last column is our method): https://t.co/539RjBw5X0', 'Super-resolution: https://t.co/wJcSha4Lad', 'Denoising: https://t.co/zyFd6KfEI3', 'Morphing a human face (@StefanoErmon,  with his kind permission) to a frog using a robust ImageNet classifier (@aleks_madry). Due to ethical concerns,  we do not release our code for that experiment. Interested artists please contact us for code. https://t.co/cVqkmM1mnN', 'There is nothing specific to human-faces in our results. For example, we can use our technique to inpaint cats. https://t.co/XKymzmt3ol', 'Joint work with: Joseph Dean (equal contribution), Ajil Jalal, Alexandros Dimakis  (@AlexGDimakis).', '@aaron_j_chavez @AlexGDimakis Thanks a lot for the kind words, we very much appreciate it! We were also very very surprised with the super-resolution results. Interestingly, expanding the range of the generator also seems to lead to less biased reconstructions, compared to PULSE.']",https://arxiv.org/abs/2102.07364,"We propose Intermediate Layer Optimization (ILO), a novel optimization algorithm for solving inverse problems with deep generative models. Instead of optimizing only over the initial latent code, we progressively change the input layer obtaining successively more expressive generators. To explore the higher dimensional spaces, our method searches for latent codes that lie within a small $l_1$ ball around the manifold induced by the previous layer. Our theoretical analysis shows that by keeping the radius of the ball relatively small, we can improve the established error bound for compressed sensing with deep generative models. We empirically show that our approach outperforms state-of-the-art methods introduced in StyleGAN-2 and PULSE for a wide range of inverse problems including inpainting, denoising, super-resolution and compressed sensing. ","Intermediate Layer Optimization for Inverse Problems using Deep
  Generative Models"
73,1361309386610728961,19011452,Peter Brown,"['#OnThisDay in 2013 the shock wave from the Chelyabinsk airburst caused widespread damage. Immediately after the event, the big question was how much energy did it release? \nOur new paper <LINK> \nprovides a method for finding energies form big fireballs. 1/3']",http://arxiv.org/abs/2102.06574,"Near field acoustical signals from fireballs (ranges<200 km), when detected by dense ground networks, may be used to estimate the orientation of the trajectory of a fireball (Pujol et al., 2005) as well as fragmentation locations (Kalenda et al., 2014; Edwards and Hildebrand, 2004). Distinguishing ballistic arrivals (from the cylindrical shock of the fireball)from fragmentation generated signals (quasi-spherical sources) remains a challenge, but are obtainable through analysis of the acoustic path and the timing observed at ground instruments. Here we describe an integrated computer code, termed the Bolide Acoustic Modelling program or BAM, to estimate fireball trajectories and energetics. We develop a new methodology for measuring energy release from bolide fragmentation episodes solely from acoustic measurements and incorporate this into BAM. We also explore the sensitivity of seismo-acoustic fireball solutions and energy estimates to uncertainty in the underlying atmospheric model. Applying BAM to the Stubenberg meteorite producing fireball, we find the total fireball energy from ballistic arrivals to be approximately $5 \times 10^{10}$J which compares favorably to the optical estimate of $4.36 \times 10^{10}$J. The combined fragmentation energy of the Stubenberg event from acoustic data was found to be $1.47^{+0.28}_{-0.12} \times 10^{10}$J, roughly one third of the ballistic or optical total energy. We also show that measuring fireball velocities from acoustic data alone is very challenging but may be possible for slow, deeply penetrating fireballs with shallow entry angles occurring over dense seismic/infrasound networks. ",Fireball characteristics derivable from acoustic data
74,1361305266805964800,1263810665476653057,Denis Vida,"['In our new paper, we develop a method to measure energies and trajectories of big fireballs using only nearby seismic and infrasound stations!\n<LINK>\nThe approach is simple, we just assume fireballs are seismic sources moving at Mach ~50.\n@westernuScience <LINK>', 'We also provide open-source software for energy and trajectory inversion: https://t.co/9JDrU9XFVu https://t.co/iUfz7V7zE9', ""Congrats @LukeFMcFadden on your first publication! More are to come, and I'm sure they will be as amazing as this one!""]",https://arxiv.org/abs/2102.06574,"Near field acoustical signals from fireballs (ranges<200 km), when detected by dense ground networks, may be used to estimate the orientation of the trajectory of a fireball (Pujol et al., 2005) as well as fragmentation locations (Kalenda et al., 2014; Edwards and Hildebrand, 2004). Distinguishing ballistic arrivals (from the cylindrical shock of the fireball)from fragmentation generated signals (quasi-spherical sources) remains a challenge, but are obtainable through analysis of the acoustic path and the timing observed at ground instruments. Here we describe an integrated computer code, termed the Bolide Acoustic Modelling program or BAM, to estimate fireball trajectories and energetics. We develop a new methodology for measuring energy release from bolide fragmentation episodes solely from acoustic measurements and incorporate this into BAM. We also explore the sensitivity of seismo-acoustic fireball solutions and energy estimates to uncertainty in the underlying atmospheric model. Applying BAM to the Stubenberg meteorite producing fireball, we find the total fireball energy from ballistic arrivals to be approximately $5 \times 10^{10}$J which compares favorably to the optical estimate of $4.36 \times 10^{10}$J. The combined fragmentation energy of the Stubenberg event from acoustic data was found to be $1.47^{+0.28}_{-0.12} \times 10^{10}$J, roughly one third of the ballistic or optical total energy. We also show that measuring fireball velocities from acoustic data alone is very challenging but may be possible for slow, deeply penetrating fireballs with shallow entry angles occurring over dense seismic/infrasound networks. ",Fireball characteristics derivable from acoustic data
75,1361268832195796992,17373048,Rodrigo Nemmen,"['Today we have a new paper out led by Roberta Duarte. She trained machines to imagine the future of an accreting black hole, based on turbulent numerical simulations. Very soon there will be a thread in @BlackHolesUSP about this work. Stay tuned! <LINK> <LINK>', 'In this paper, we‚Äôve collaborated with Joao P. Navarro (@NVIDIABrasil ). This scientific collaboration was crucial to advance the work', 'The GPU used for this research was donated by @nvidia to my group @BlackHolesUSP . We are very grateful for the donation. üôè Without it, the work would have taken much longer to be completed.\n\nPaper: https://t.co/O20Xri57oE']",https://arxiv.org/abs/2102.06242,"In this pilot study, we investigate the use of a deep learning (DL) model to temporally evolve the dynamics of gas accreting onto a black hole in the form of a radiatively inefficient accretion flow (RIAF). We have trained a machine to forecast such a spatiotemporally chaotic system -- i.e. black hole weather forecasting -- using a convolutional neural network (CNN) and a training dataset which consists of numerical solutions of the hydrodynamical equations, for a range of initial conditions. We find that deep neural networks seem to learn well black hole accretion physics and evolve the accretion flow orders of magnitude faster than traditional numerical solvers, while maintaining a reasonable accuracy for a long time. For instance, CNNs predict well the temporal evolution of a RIAF over a long duration of $8\times 10^4 GM/c^3$, which corresponds to 80 dynamical times at $r=100 GM/c^2$. The DL model is able to evolve flows from initial conditions not present in the training dataset with good accuracy. Our approach thus seems to generalize well. Once trained, the DL model evolves a turbulent RIAF on a single GPU four orders of magnitude faster than usual fluid dynamics integrators running in parallel on 200 CPU cores. We speculate that a data-driven machine learning approach should be very promising for accelerating not only fluid dynamics simulations, but also general relativistic magnetohydrodynamic ones. ",Black Hole Weather Forecasting with Deep Learning: A Pilot Study
76,1360687800690085888,841497890,Jonathan Sobel,"['Our new perspective paper on #medicine during #sleep is now available on #arXiv. Thanks to @lab_aim @BeharJoachim Nitai Bar, Thomas Penzel and Yosi Shamay for the great collaboration! <LINK>', '@TechnionLive @ESRS_Sleep']",https://arxiv.org/abs/2102.05452,"Sleep has a profound influence on the physiology of body systems and biological processes. Molecular studies have shown circadian-regulated shifts in protein expression patterns across human tissues, further emphasizing the unique functional, behavioral and pharmacokinetic landscape of sleep. Thus, many pathological processes are also expected to exhibit sleep-specific manifestations. Nevertheless, sleep is seldom utilized for the study, detection and treatment of non-sleep-specific pathologies. Modern advances in biosensor technologies have enabled remote, non-invasive recording of a growing number of physiologic parameters and biomarkers. Sleep is an ideal time frame for the collection of long and clean physiological time series data which can then be analyzed using data-driven algorithms such as deep learning. In this perspective paper, we aim to highlight the potential of sleep as an auspicious time for diagnosis, management and therapy of nonsleep-specific pathologies. We introduce key clinical studies in selected medical fields, which leveraged novel technologies and the advantageous period of sleep to diagnose, monitor and treat pathologies. We then discuss possible opportunities to further harness this new paradigm and modern technologies to explore human health and disease during sleep and to advance the development of novel clinical applications: From sleep medicine to medicine during sleep. ",From sleep medicine to medicine during sleep: A clinical perspective
77,1360685486776328194,898869630,Daniel Khashabi üïäÔ∏è,"['ü•≥ New dataset release! ü•≥\n\nARC-DA dataset, a direct-answer (‚Äúopen response‚Äù, ‚Äúfreeform‚Äù) QA dataset for elementary-school science domain. \n\nPaper: <LINK>\nDataset: <LINK>\n\nJoint work w/ Aristo team at @allen_ai.']",https://arxiv.org/abs/2102.03315,"We present the ARC-DA dataset, a direct-answer (""open response"", ""freeform"") version of the ARC (AI2 Reasoning Challenge) multiple-choice dataset. While ARC has been influential in the community, its multiple-choice format is unrepresentative of real-world questions, and multiple choice formats can be particularly susceptible to artifacts. The ARC-DA dataset addresses these concerns by converting questions to direct-answer format using a combination of crowdsourcing and expert review. The resulting dataset contains 2985 questions with a total of 8436 valid answers (questions typically have more than one valid answer). ARC-DA is one of the first DA datasets of natural questions that often require reasoning, and where appropriate question decompositions are not evident from the questions themselves. We describe the conversion approach taken, appropriate evaluation metrics, and several strong models. Although high, the best scores (81% GENIE, 61.4% F1, 63.2% ROUGE-L) still leave considerable room for improvement. In addition, the dataset provides a natural setting for new research on explanation, as many questions require reasoning to construct answers. We hope the dataset spurs further advances in complex question-answering by the community. ARC-DA is available at this https URL ","Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the
  Direct-Answer AI2 Reasoning Challenge"
78,1360509664354516992,1288593824,Michael Felderer,"['Our new paper clarifying terminology and challenges of quality assurance for AI-Enabled Systems is available online at <LINK> - it also characterizes the dimensions artifact type, quality assurance, and process for quality assurance for AI-Enabled Systems üëá <LINK>', '@prof_wagnerst @WhiteAero Yes, Safety is a Quality-in-Use Property!']",https://arxiv.org/abs/2102.05351,"The number and importance of AI-based systems in all domains is growing. With the pervasive use and the dependence on AI-based systems, the quality of these systems becomes essential for their practical usage. However, quality assurance for AI-based systems is an emerging area that has not been well explored and requires collaboration between the SE and AI research communities. This paper discusses terminology and challenges on quality assurance for AI-based systems to set a baseline for that purpose. Therefore, we define basic concepts and characterize AI-based systems along the three dimensions of artifact type, process, and quality characteristics. Furthermore, we elaborate on the key challenges of (1) understandability and interpretability of AI models, (2) lack of specifications and defined requirements, (3) need for validation data and test input generation, (4) defining expected outcomes as test oracles, (5) accuracy and correctness measures, (6) non-functional properties of AI-based systems, (7) self-adaptive and self-learning characteristics, and (8) dynamic and frequently changing environments. ",Quality Assurance for AI-based Systems: Overview and Challenges
79,1360251013765214214,1078236938669379584,Daniel Arteaga,"['We just released a new paper, together with @jordiponsdotme:\n\n""Multichannel-based learning for audio object extraction""\n\nAccepted for presentation in #ICASSP2021.\n\n<LINK> <LINK>', 'We show how to train a system to extract objects (audio + spatial location) out a multichannel mix without ever comparing with the reference objects.', 'After having worked part-time in the field for 2-3 years behind closed doors, this is my first paper in deep learning.']",https://arxiv.org/abs/2102.06142,"The current paradigm for creating and deploying immersive audio content is based on audio objects, which are composed of an audio track and position metadata. While rendering an object-based production into a multichannel mix is straightforward, the reverse process involves sound source separation and estimating the spatial trajectories of the extracted sources. Besides, cinematic object-based productions are often composed by dozens of simultaneous audio objects, which poses a scalability challenge for audio object extraction. Here, we propose a novel deep learning approach to object extraction that learns from the multichannel renders of object-based productions, instead of directly learning from the audio objects themselves. This approach allows tackling the object scalability challenge and also offers the possibility to formulate the problem in a supervised or an unsupervised fashion. Since, to our knowledge, no other works have previously addressed this topic, we first define the task and propose an evaluation methodology, and then discuss under what circumstances our methods outperform the proposed baselines. ",Multichannel-based learning for audio object extraction
80,1360145614735048705,860184781,Rineke Verbrugge,"['In this new paper, I prove 2 zero-one laws for provability logic: with respect to model and frame validity. I axiomatize validity in almost all relevant finite models and almost all relevant finite frames, both by infinite sets of axioms. Comments welcome! <LINK> <LINK>']",https://arxiv.org/abs/2102.05947,"It has been shown in the late 1960s that each formula of first-order logic without constants and function symbols obeys a zero-one law: As the number of elements of finite models increases, every formula holds either in almost all or in almost no models of that size. Therefore, many properties of models, such as having an even number of elements, cannot be expressed in the language of first-order logic. For modal logics, limit behavior for models and frames may differ. Halpern and Kapron proved zero-one laws for classes of models corresponding to the modal logics K, T, S4, and S5. In this paper, we prove zero-one laws for provability logic with respect to both model and frame validity. Moreover, we axiomatize validity in almost all relevant finite models and in almost all relevant finite frames, leading to two different axiom systems. In the proofs, we use a combinatorial result by Kleitman and Rothschild about the structure of almost all finite partial orders. On the way, we also show that a previous result by Halpern and Kapron about the axiomatization of almost sure frame validity for S4 is not correct. Finally, we consider the complexity of deciding whether a given formula is almost surely valid in the relevant finite models and frames. ","Zero-one laws for provability logic: Axiomatizing validity in almost all
  models and almost all frames"
81,1360119371092398082,1138762581164855298,Christoph Ternes,"['New paper today, <LINK> Using COHERENT and APV data we calculate the neutron rms radii and neutron skins of cesium and iodide nuclei, disentangling the contributions from both nuclei. The same data is used to calculate the weak mixing angle.']",https://arxiv.org/abs/2102.06153,"Using the new results on coherent elastic neutrino-nucleus scattering data in cesium-iodide provided by the COHERENT experiment, we determine a new measurement of the average neutron rms radius of $^{133}\text{Cs}$ and $^{127}\text{I}$. In combination with the atomic parity violation (APV) experimental result, we derive the most precise measurement of the neutron rms radii of $^{133}\text{Cs}$ and $^{127}\text{I}$, disentangling for the first time the contributions of the two nuclei. By exploiting these measurements we determine the corresponding neutron skin values for $^{133}\text{Cs}$ and $^{127}\text{I}$. These results suggest a preference for models which predict large neutron skin values, as corroborated by the only other electroweak measurements of the neutron skin of $^{208}\text{Pb}$ performed by PREX experiments. Moreover, for the first time, we obtain a data-driven APV+COHERENT measurement of the low-energy weak mixing angle with a percent uncertainty, independent of the value of the average neutron rms radius of $^{133}\text{Cs}$ and $^{127}\text{I}$, that is allowed to vary freely in the fit. The value of the low-energy weak mixing angle that we found is slightly larger than the standard model prediction. ","New insights into nuclear physics and weak mixing angle using
  electroweak probes"
82,1360058440496906244,901266828655284225,Brian Metzger,"['Paper led by @navinsridhar on a new model for fast radio bursts powered by accreting neutron star or black holes at ~super-Eddington rates.  Mostly an ""ideas"" paper, making the case for how such engines could in principle account for many FRB properties. <LINK> <LINK>', 'Don\'t get me wrong: I still think magnetars are the most promising FRB central engines.  However, we should keep our minds open and let ""different ideas compete like horses in a race"" (to quote \n@LordMartinRees).', 'This paper also serves to emphasize that the emission mechanism we find most promising (relativistic shocks from transient relativistic energy injected into a magnetized environment) is actually independent of the specific engine generating the relativistic flares.']",https://arxiv.org/abs/2102.06138,"The discovery of periodicity in the arrival times of the fast radio bursts (FRBs) poses a challenge to the oft-studied magnetar scenarios. However, models that postulate that FRBs result from magnetized shocks or magnetic reconnection in a relativistic outflow are not specific to magnetar engines; instead, they require only the impulsive injection of relativistic energy into a dense magnetized medium. Motivated thus, we outline a new scenario in which FRBs are powered by short-lived relativistic outflows (``flares'') from accreting black holes or neutron stars, which propagate into the cavity of the pre-existing (``quiescent'') jet. In order to reproduce FRB luminosities and rates, we are driven to consider binaries of stellar-mass compact objects undergoing super-Eddington mass-transfer, similar to ultraluminous X-ray (ULX) sources. Indeed, the host galaxies of FRBs, and their spatial offsets within their hosts, show broad similarities with ULXs. Periodicity on timescales of days to years could be attributed to precession (e.g., Lens-Thirring) of the polar accretion funnel, along which the FRB emission is geometrically and relativistically beamed, which sweeps across the observer line of sight. Accounting for the most luminous FRBs via accretion power may require a population of binaries undergoing brief-lived phases of unstable (dynamical-timescale) mass-transfer. This will lead to secular evolution in the properties of some repeating FRBs on timescales of months to years, followed by a transient optical/IR counterpart akin to a luminous red nova, or a more luminous accretion-powered optical/X-ray transient. We encourage targeted FRB searches of known ULX sources. ",Periodic Fast Radio Bursts from Luminous X-ray Binaries
83,1360047081465937923,3022633752,Tianle Cai,"['Thrilled to share our new work on certifying L_inf adversarial robustness! New architecture with inherent robustness and a tailored training pipeline. Achieving SOTA performance on several benchmarks!\npaper: <LINK>\ncode: <LINK> <LINK>', 'Joint work with amazing collaborators Bohang Zhang, Zhou Lu, Di He and Prof. Liwei Wang']",https://arxiv.org/abs/2102.05363,"It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded adversarial perturbations. Although many attempts have been made, most previous works either can only provide empirical verification of the defense to a particular attack method, or can only develop a certified guarantee of the model robustness in limited scenarios. In this paper, we seek for a new approach to develop a theoretically principled neural network that inherently resists $\ell_\infty$ perturbations. In particular, we design a novel neuron that uses $\ell_\infty$-distance as its basic operation (which we call $\ell_\infty$-dist neuron), and show that any neural network constructed with $\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a 1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides a rigorous guarantee of the certified robustness based on the margin of prediction outputs. We then prove that such networks have enough expressive power to approximate any 1-Lipschitz function with robust generalization guarantee. We further provide a holistic training strategy that can greatly alleviate optimization difficulties. Experimental results show that using $\ell_{\infty}$-dist nets as basic building blocks, we consistently achieve state-of-the-art performance on commonly used datasets: 93.09% certified accuracy on MNIST ($\epsilon=0.3$), 35.42% on CIFAR-10 ($\epsilon=8/255$) and 16.31% on TinyImageNet ($\epsilon=1/255$). ","Towards Certifying L-infinity Robustness using Neural Networks with
  L-inf-dist Neurons"
84,1360043026027020297,872330935584403456,Mehran Kazemi,['No graph structure? No worries!\nOur new paper with @BahareFatemi and @elasri_layla  proposes a way to learn both GNN parameters and a graph structure simultaneously.\nLink: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2102.05034,"Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks. ","SLAPS: Self-Supervision Improves Structure Learning for Graph Neural
  Networks"
85,1359945259946565636,807684708,Emma Louden,"['Ahhhh I am so excited to share that my new paper is out on @arxiv! This project was my first junior paper at @Princeton and I learned so much, not just about the work, but about how to do research! Check out the science summary in the thread below ‚¨áÔ∏è <LINK>', 'Soooo one of the coolest things to happen in astro in the past two decades imo is the discovery of *thousands* of exoplanets and hypotheses of many more to be discovered. These planets give us comparison points to learn about how common different types of planets are üåè', 'BUT to make meaningful comparisons we need awesomely precise and accurate measurements of things like the mass and radius of these planets. Since the first exoplanet discoveries, the observation technology has improved dramatically üî≠', 'Thanks to newer missions like @NASA_TESS and @ESAGaia we can tighten up the precision on measurements (less uncertainty) AND have more accurate measurements. This is where this paper comes in:', 'We re-analyzed two planets that had been found prior to the new telescopes and incorporated the new observations of those planets into the fitting process.', 'We showed that precision increases by as much as a factor of ten ü§© (and some other interesting facts about where in the process the data should be put in) AND', 'ultimately argue that to really be able to do bulk analysis of all these awesome planets, a full re-analysis of previously discovered transiting planets is warranted.', 'Phew! If you got all the way here, thank you! Happy #InternationalDayOfWomenInScience\n\nAlso, I have to give huge thanks to my advisor for this project, Joel Hartman, for teaching me so much about the research project and being an awesome mentor.']",https://arxiv.org/abs/2102.05420,"We present a revised characterisation of the previously discovered transiting planet systems HATS-34 and HATS-46. We make use of the newly available space-based light curves from the NASA TESS mission and high-precision parallax and absolute photometry measurements from the ESA Gaia mission to determine the mass and radius of the planets and host stars with dramatically increased precision and accuracy compared to published values, with the uncertainties in some parameters reduced by as much as a factor of seven. Using an isochrone based fit, for HATS-34 we measure a revised host star mass and radius of $0.952_{-0.02}^{+0.04}M_S$ and of $0.9381\pm0.0080R_S$, respectively, and a revised mass and radius for the transiting planet of $0.951\pm0.050 M_J$ and $1.282 \pm0.064 R_J$ respectively. Similarly, for HATS-46 we measure a revised mass and radius for the host star of $0.869\pm0.023M_S$, and $0.894\pm0.010 R_S$, respectively, and a revised mass and radius for the planet of $0.158 \pm0.042 M_J$, and $0.951 \pm 0.029 R_J$, respectively. The uncertainties that we determine on the stellar and planetary masses and radii are also substantially lower than re-determinations that incorporate the Gaia results without performing a full re-analysis of the light curves and other observational data. We argue that, in light of Gaia and TESS, a full re-analysis of previously discovered transiting planets is warranted. ",HATS-34b and HATS-46b: Re-characterisation Using TESS and Gaia
86,1359792433786212361,1231566825588232192,DipakMunshi5,['New paper:  <LINK>'],https://arxiv.org/abs/2102.05521,"We investigate three-point statistics in weak lensing convergence, through the integrated bispectrum. This statistic involves measuring power spectra in patches, and is thus easy to measure, and avoids the complexity of estimating the very large number of possible bispectrum configurations. The integrated bispectrum principally probes the squeezed limit of the bispectrum. To be useful as a set of summary statistics, accurate theoretical predictions of the signal are required, and, assuming Gaussian sampling distributions, the covariance matrix. In this paper, we investigate through simulations how accurate are theoretical formulae for both the integrated bispectrum and its covariance, finding that there a small inaccuracies in the theoretical signal, and more serious deviations in the covariance matrix, which may need to be estimated using simulations. ",The integrated angular bispectrum of weak lensing
87,1359785433660555264,1219950128640995328,Jan Dirk Wegner,"['New benchmark: ""H3D: BENCHMARK ON SEMANTIC SEGMENTATION OF HIGH-RESOLUTION 3D POINT CLOUDS AND TEXTURED MESHES FROM UAV LIDAR AND MULTI-VIEW-STEREO""  #mesh #3dmodeling #texture  @isprs @ISPRS_SC @Uni_Stuttgart \n\nwebsite: <LINK>\n\npaper: <LINK> <LINK>']",https://arxiv.org/abs/2102.05346,"Automated semantic segmentation and object detection are of great importance in geospatial data analysis. However, supervised machine learning systems such as convolutional neural networks require large corpora of annotated training data. Especially in the geospatial domain, such datasets are quite scarce. Within this paper, we aim to alleviate this issue by introducing a new annotated 3D dataset that is unique in three ways: i) The dataset consists of both an Unmanned Aerial Vehicle (UAV) laser scanning point cloud and a 3D textured mesh. ii) The point cloud features a mean point density of about 800 pts/sqm and the oblique imagery used for 3D mesh texturing realizes a ground sampling distance of about 2-3 cm. This enables the identification of fine-grained structures and represents the state of the art in UAV-based mapping. iii) Both data modalities will be published for a total of three epochs allowing applications such as change detection. The dataset depicts the village of Hessigheim (Germany), henceforth referred to as H3D. It is designed to promote research in the field of 3D data analysis on one hand and to evaluate and rank existing and emerging approaches for semantic segmentation of both data modalities on the other hand. Ultimately, we hope that H3D will become a widely used benchmark dataset in company with the well-established ISPRS Vaihingen 3D Semantic Labeling Challenge benchmark (V3D). The dataset can be downloaded from this https URL ","The Hessigheim 3D (H3D) Benchmark on Semantic Segmentation of
  High-Resolution 3D Point Clouds and Textured Meshes from UAV LiDAR and
  Multi-View-Stereo"
88,1359626223840882690,45807937,Geoffrey Mo,"['Want to look at a kilonova while it\'s happening? In this new paper we demonstrate that LIGO/Virgo is capable of distributing ""early warning"" gravitational-wave alerts and that some might actually go out in O4! <LINK>', ""A lot of work has gone into reducing the latency between when a GW wobbles the test masses and when we let the world know about it. This neat figure shows just how much it's improved in the last few years. https://t.co/7wMlALiNNL""]",https://arxiv.org/abs/2102.04555,"Gravitational-wave observations became commonplace in Advanced LIGO-Virgo's recently concluded third observing run. 56 non-retracted candidates were identified and publicly announced in near real time. Gravitational waves from binary neutron star mergers, however, remain of special interest since they can be precursors to high-energy astrophysical phenomena like $\gamma$-ray bursts and kilonovae. While late-time electromagnetic emissions provide important information about the astrophysical processes within, the prompt emission along with gravitational waves uniquely reveals the extreme matter and gravity during - and in the seconds following - merger. Rapid communication of source location and properties from the gravitational-wave data is crucial to facilitate multi-messenger follow-up of such sources. This is especially enabled if the partner facilities are forewarned via an early-warning (pre-merger) alert. Here we describe the commissioning and performance of such a low-latency infrastructure within LIGO-Virgo. We present results from an end-to-end mock data challenge that detects binary neutron star mergers and alerts partner facilities before merger. We set expectations for these alerts in future observing runs. ",First demonstration of early warning gravitational wave alerts
89,1359558443443568642,1359421250431516683,Dr. Eva Laplace,"['1/6 New paper thread! <LINK> \n\nSummary: Not only the surface properties but also the pre-supernova core structures of massive single and binary-stripped stars are systematically different, even when considering the same core mass! <LINK>', '2/6 We show the pre-supernova core composition with new diagrams that bring into focus three distinct regions I) a He-rich layer II) an O/Ne-rich layer III) an iron-rich core. Binary-stripped stars contain a gradient of C/O/Ne around II that is not present in single stars. https://t.co/1wIEJqfTun', '3/6 We find that due to this layer, massive binary-stripped stars contain systematically higher masses of carbon at the end of their lives than single stars with the same helium core mass. This is very exciting because the nucleosynthesis from these stars may be different!', '4/6 This layer originates from a distinct behavior during core helium burning. The convective He-burning cores of single stars grow in mass while they recede for binaries due to wind mass loss, leaving behind a C/O/Ne layer. https://t.co/VaGDPy0XIo', '5/6 We find that binary-stripped stars have systematically different density structures from single stars. These are tied to how differently they burn. These differences in the core structures also have implications for how explodable these stars are!', '6/6 For more of these cool diagrams, stay tuned for my next paper on TULIPS: the Tool for Understanding the Lives, Interiors, and Physics of Stars üå∑']",https://arxiv.org/abs/2102.05036,"The majority of massive stars live in binary or multiple systems and will interact during their lifetimes, which helps to explain the observed diversity of core-collapse supernovae. Donor stars in binary systems can lose most of their hydrogen-rich envelopes through mass transfer, which not only affects the surface properties, but also the core structure. However, most calculations of the core-collapse properties of massive stars rely on single-star models. We present a systematic study of the difference between the pre-supernova structures of single stars and stars of the same initial mass (11 - 21\Msun) that have been stripped due to stable post-main sequence mass transfer at solar metallicity. We present the pre-supernova core composition with novel diagrams that give an intuitive representation of the isotope distribution. As shown in previous studies, at the edge of the carbon-oxygen core, the binary-stripped star models contain an extended gradient of carbon, oxygen, and neon. This layer originates from the receding of the convective helium core during core helium burning in binary-stripped stars, which does not occur in single-star models. We find that this same evolutionary phase leads to systematic differences in the final density and nuclear energy generation profiles. Binary-stripped star models have systematically higher total masses of carbon at the moment of core collapse compared to single star models, which likely results in systematically different supernova yields. In about half of our models, the silicon-burning and oxygen-rich layers merge after core silicon burning. We discuss the implications of our findings for the explodability, supernova observations, and nucleosynthesis from these stars. Our models will be publicly available and can be readily used as input for supernova simulations. [Abridged] ","Different to the core: the pre-supernova structures of massive single
  and binary-stripped stars"
90,1359546579204468743,505759891,Xin Eric Wang,"['Can models describe visual differences in natural language? Yes! It requires visual comparison and SEMANTIC understanding of individuals.\n\nCheck out our new #EACL2021 paper ""L2C: Describing Visual Differences Needs Semantic Understanding of Individuals"": <LINK> <LINK>']",https://arxiv.org/abs/2102.01860,"Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I_1 and I_2, and the task is to generate a description W_{1,2} comparing them, existing methods directly model { I_1, I_2 } -> W_{1,2} mapping without the semantic understanding of individuals. In this paper, we introduce a Learning-to-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and single-image captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset. ","L2C: Describing Visual Differences Needs Semantic Understanding of
  Individuals"
91,1359534248445710340,28840722,Phil Long,"['New paper with @niladrichat and Peter Bartlett called ""When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?"": <LINK>.']",https://arxiv.org/abs/2102.04998,"We establish conditions under which gradient descent applied to fixed-width deep networks drives the logistic loss to zero, and prove bounds on the rate of convergence. Our analysis applies for smoothed approximations to the ReLU, such as Swish and the Huberized ReLU, proposed in previous applied work. We provide two sufficient conditions for convergence. The first is simply a bound on the loss at initialization. The second is a data separation condition used in prior analyses. ","When does gradient descent with logistic loss interpolate using deep
  networks with smoothed ReLU activations?"
92,1359453128802852867,761626039,Stefan Feuerriegel,"['New paper on real-time risk scoring in critical care\n\nüéØWe predict in-hospital mortality with AUC = 0.88; 2.2% better than state-of-the-art\nüö®Better care: high risk alerts 12 hours earlier than in current practice\nüí° Novel attentive deep Markov model\n\n<LINK>', 'Our new model captures (1) long-term dependencies and (2) latent disease states simultaneously. This is done by a combining (1) attention layer and (2) deep Markov. \n\nJoint work with Yilmazcan √ñzyurt, Mathias Kraus, @tobias_hatt @d_mtec @ETH_en']",http://arxiv.org/abs/2102.04702,"Clinical practice in intensive care units (ICUs) requires early warnings when a patient's condition is about to deteriorate so that preventive measures can be undertaken. To this end, prediction algorithms have been developed that estimate the risk of mortality in ICUs. In this work, we propose a novel generative deep probabilistic model for real-time risk scoring in ICUs. Specifically, we develop an attentive deep Markov model called AttDMM. To the best of our knowledge, AttDMM is the first ICU prediction model that jointly learns both long-term disease dynamics (via attention) and different disease states in health trajectory (via a latent variable model). Our evaluations were based on an established baseline dataset (MIMIC-III) with 53,423 ICU stays. The results confirm that compared to state-of-the-art baselines, our AttDMM was superior: AttDMM achieved an area under the receiver operating characteristic curve (AUROC) of 0.876, which yielded an improvement over the state-of-the-art method by 2.2%. In addition, the risk score from the AttDMM provided warnings several hours earlier. Thereby, our model shows a path towards identifying patients at risk so that health practitioners can intervene early and save patient lives. ","AttDMM: An Attentive Deep Markov Model for Risk Scoring in Intensive
  Care Units"
93,1359452589105033216,1465200073,Yannis Flet-Berliac,"['Check out our new #ICLR2021 paper AGAC!\n\n<LINK>\n\nA great colab w/ @johanferret, O. Pietquin, P. Preux and M. Geist.\n\nWe introduce an adversary to the AC framework that drives the agent to explore more efficiently in the hard-exploration tasks MiniGrid &amp; Vizdoom. <LINK> <LINK>']",http://arxiv.org/abs/2102.04376,"Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks. ",Adversarially Guided Actor-Critic
94,1359436094098583553,739505640326987777,Guido Roberts-Borsani,"['New paper out today wooo! ü•≥ <LINK>\nHere we look at the level of accuracy that JWST wide-band imaging will afford for z=7-11 galaxy properties from simulated photometry, and how these can be significantly improved with medium-band imaging! üôå', 'Using NIRCam WB photometry adopted by upcoming ERS and GTO programs, we simulate additional photometry from single MB filters and compare the accuracies. The accuracies of e.g. Z, stellar age and SF-related props can be improved by almost ~0.5-1 dex in some cases.', 'We find the F430M filt affords the highest gain in accuracy for z~8 galaxies and significantly improves the photo-zs as well, since it straddles the [OIII]+Hb lines which result in especially red colours, even for moderately strong lines.', 'These comparisons will be particularly useful for upcoming GO proposals to maximise science with the short lifespan of JWST: the addition of unique WB imaging can yield large samples of z&gt;7 galaxies with robust photo-zs and unprecedented accuracy on their inferred properties.']",https://arxiv.org/abs/2102.04469,"The past decade has seen impressive progress in the detection of $z>7$ galaxies with the Hubble Space Telescope, however little is known about their properties. The James Webb Space Telescope will revolutionise the high-$z$ field by providing NIR (i.e., rest-frame optical) data of unprecedented depth and spatial resolution. Measuring galaxy quantities such as resolved stellar ages or gas metallicity gradients traditionally requires spectroscopy, as broad-band imaging filters are generally too coarse to fully isolate diagnostics such as the 4000 \r{A} (rest-frame) break, continuum emission from aged stars, and key emission lines (e.g., [OII], [OIII], H$\beta$). However, in this paper, we show that adding NIRCam images through a strategically chosen medium-band filter to common wide-band filters sets adopted by ERS and GTO programs delivers tighter constraints on these galactic properties. To constrain the choice of filter, we perform a systematic investigation of which combinations of wide-band filters from ERS and GTO programs and single medium-band filters offer the tightest constraints on several galaxy properties at redshifts $z\sim7-11$. We employ the JAGUAR extragalactic catalogs to construct statistical samples of physically-motivated mock photometry and conduct SED-fitting procedures to evaluate the accuracy of galaxy property (and photo-$z$) recovery with a simple star-formation history model. We find that adding $>4.1 \mu$m medium filters at comparable depth to the broad-band filters can significantly improve photo-$z$s and yield close to order-of-magnitude improvements in the determination of quantities such as stellar ages, metallicities, SF-related quantities and emission line fluxes at $z\sim8$. For resolved sources, the proposed approach enables spatially-resolved determination of these quantities that would be prohibitive with slit spectroscopy. ","Improving $z\sim7-11$ Galaxy Property Estimates with JWST/NIRCam
  Medium-Band Photometry"
95,1359422545087389697,1207610988993896449,Dr Laura Wolz,"['üö® NEW PAPER ALERTTTT üö® <LINK>\n\nHI constraints from the cross-correlation of eBOSS galaxies and Green Bank Telescope intensity maps!\n\nIt has been in the making for a reaaallly long time, and really should have a shared first-author with my fave @AlkistisPou üëØ\u200d‚ôÄÔ∏è <LINK>']",https://arxiv.org/abs/2102.04946,"We present the joint analysis of Neutral Hydrogen (HI) Intensity Mapping observations with three galaxy samples: the Luminous Red Galaxy (LRG) and Emission Line Galaxy (ELG) samples from the eBOSS survey, and the WiggleZ Dark Energy Survey sample. The HI intensity maps are Green Bank Telescope observations of the redshifted 21cm emission on 100deg2 covering the redshift range $0.6<z<1.0$. We process the data by separating and removing the foregrounds with FastICA, and construct a transfer function to correct for the effects of foreground removal on the HI signal. We cross-correlate the cleaned HI data with the galaxy samples and study the overall amplitude as well as the scale-dependence of the power spectrum. We also qualitatively compare our findings with the predictions by a semi-analytic galaxy evolution simulation. The cross-correlations constrain the quantity $\Omega_{{HI}} b_{{HI}} r_{{HI},{opt}}$ at an effective scale $k_{eff}$, where $\Omega_{HI}$ is the HI density fraction, $b_{HI}$ is the HI bias, and $r_{{HI},{opt}}$ the galaxy-hydrogen correlation coefficient, which is dependent on the HI content of the optical galaxy sample. At $k_{eff}=0.31 \, h/{Mpc}$ we find $\Omega_{{HI}} b_{{HI}} r_{{HI},{Wig}} = [0.58 \pm 0.09 \, {(stat) \pm 0.05 \, {(sys)}}] \times 10^{-3}$ for GBT-WiggleZ, $\Omega_{{HI}} b_{{HI}} r_{{HI,{ELG}}} = [0.40 \pm 0.09 \, {(stat) \pm 0.04 \, {(sys)}}] \times 10^{-3}$ for GBT-ELG, and $\Omega_{{HI}} b_{{HI}} r_{{HI},{LRG}} = [0.35 \pm 0.08 \, {(stat) \pm 0.03 \, {(sys)}}] \times 10^{-3}$ for GBT-LRG, at $z\simeq 0.8$. We also report results at $k_{eff}=0.24 \, h/{Mpc}$ and $k_{eff}=0.48 \, h/{Mpc}$. With little information on HI parameters beyond our local Universe, these are amongst the most precise constraints on neutral hydrogen density fluctuations in an underexplored redshift range. ","HI constraints from the cross-correlation of eBOSS galaxies and Green
  Bank Telescope intensity maps"
96,1359156610283757574,20865039,Tristan Deleu,"['Our new paper ""Structured Sparsity Inducing Adaptive Optimizers for Deep Learning"" is out. We explore the applications of adaptive methods &amp; proximal gradient descent for structured sparsity inducing penalties.\nPaper: <LINK> / Code: <LINK>', 'I am kind of a late adopter of PGD, I have been happily surprised by how effective it is, especially when it comes to sparsity. However applying it with adaptive methods (e.g. RMSprop, Adam) on structured sparsity turned out to be a bit more challenging than I anticipated.', 'This is an optimization paper which is a bit outside of my comfort zone, with simple applications to pruning neural networks (which is way outside of my comfort zone!), but hopefully this will all make sense soon! https://t.co/fcl12muo4c']",https://arxiv.org/abs/2102.03869,"The parameters of a neural network are naturally organized in groups, some of which might not contribute to its overall performance. To prune out unimportant groups of parameters, we can include some non-differentiable penalty to the objective function, and minimize it using proximal gradient methods. In this paper, we derive the weighted proximal operator, which is a necessary component of these proximal methods, of two structured sparsity inducing penalties. Moreover, they can be approximated efficiently with a numerical solver, and despite this approximation, we prove that existing convergence guarantees are preserved when these operators are integrated as part of a generic adaptive proximal method. Finally, we show that this adaptive method, together with the weighted proximal operators derived here, is indeed capable of finding solutions with structure in their sparsity patterns, on representative examples from computer vision and natural language processing. ",Structured Sparsity Inducing Adaptive Optimizers for Deep Learning
97,1359155090767163402,34606205,Muhammad Firmansyah Kasim,['Fully-differentiable Density Functional Theory written in @PyTorch. Pre-print paper here: <LINK>. I wonder what new applications this could bring.'],https://arxiv.org/abs/2102.04229,"Improving the predictive capability of molecular properties in ab initio simulations is essential for advanced material discovery. Despite recent progress making use of machine learning, utilizing deep neural networks to improve quantum chemistry modelling remains severely limited by the scarcity and heterogeneity of appropriate experimental data. Here we show how training a neural network to replace the exchange-correlation functional within a fully-differentiable three-dimensional Kohn-Sham density functional theory (DFT) framework can greatly improve simulation accuracy. Using only eight experimental data points on diatomic molecules, our trained exchange-correlation networks enable improved prediction accuracy of atomization energies across a collection of 104 molecules containing new bonds and atoms that are not present in the training dataset. ","Learning the exchange-correlation functional from nature with fully
  differentiable density functional theory"
98,1359004813942525953,206818334,Ivan Oseledets,"['1/n Analyzing GAN convergence is hard. We derived exact rates and oscillations providing complete analysis from Poincare constant and weighted Laplacian. Check our new paper <LINK> w @vforvalya1 and Artem Babenko.', 'This is the first time the convergence is analyzed in the functional space (not for a fixed NN approx). I really enjoyed working on this paper: the concepts match each other very well! Check how it helps to understand data augs for GAN.', 'We link local GAN training convergence to the fundamental properties of the density.']",https://arxiv.org/abs/2102.04448,"Recent work demonstrated the benefits of studying continuous-time dynamics governing the GAN training. However, this dynamics is analyzed in the model parameter space, which results in finite-dimensional dynamical systems. We propose a novel perspective where we study the local dynamics of adversarial training in the general functional space and show how it can be represented as a system of partial differential equations. Thus, the convergence properties can be inferred from the eigenvalues of the resulting differential operator. We show that these eigenvalues can be efficiently estimated from the target dataset before training. Our perspective reveals several insights on the practical tricks commonly used to stabilize GANs, such as gradient penalty, data augmentation, and advanced integration schemes. As an immediate practical benefit, we demonstrate how one can a priori select an optimal data augmentation strategy for a particular generation task. ",Functional Space Analysis of Local GAN Convergence
99,1358976863754801153,2377407248,Daniel Whiteson,"['New paper!\n\n""Feasibility of Correlated Extensive Air Shower Detection with a Distributed Cosmic Ray Network""\n<LINK>\n\nWith Eric Albin.\n\nThis paper asks: is it possible that there are cosmic ray signal SO BIG that they cover the Earth?', 'We found a mechanism, the GZ effect, which would shatter cosmic rays in half before they reach the Earth, creating two impact sites that are correlated in time.', 'Current cosmic ray observatories are awesome, but not big enough to see Earth-sized effects like this.', 'So we wondered if one could see them using a cosmic-ray telescope made out of smartphones, based on this idea:  https://t.co/7LDvu0qNNL', ""Turns out you can! It's a bit tricky, and you need something more common than vanilla GZ effect, but there's a laundry list of exotic theories that would generate such effects."", ""Right now, we're just not looking for it.""]",https://arxiv.org/abs/2102.03466,"We explore the sensitivity offered by a global network of cosmic ray detectors to a novel, unobserved phenomena: widely separated simultaneous extended air showers. Existing localized observatories work independently to observe individual showers, offering insight into the source and nature of ultra-high energy cosmic rays. However no current observatory is large enough to provide sensitivity to anticipated processes such as the GZ effect or potential new physics that generate simultaneous air showers separated by hundreds to thousands of kilometers. A global network of consumer electronics (the CRAYFIS experiment), may provide a novel opportunity for observation of such phenomena. Two user scenarios are explored. In the first, with maximal user adoption, we find that statistically significant discoveries of spatially-separated but coincident showers are possible within a couple years. In the second, more practical adoption model with $10^6$ active devices, we find a worldwide CRAYFIS to be sensitive to novel ""burst"" phenomena where many simultaneous EASs occur at once. ","Feasibility of Correlated Extensive Air Shower Detection with a
  Distributed Cosmic Ray Network"
100,1358973091213266945,2416760538,Peter Gao,"['Live from arXiv, it\'s our new review paper, ""Aerosols in Exoplanet Atmospheres"" by yours truly, @StellarPlanet, @Of_FallingStars, and @V_Parmentier, to be published in the special exoplanets edition of @jgrplanets! \n\n<LINK>', 'As we scoured the literature of the last 10+ years, a few themes stood out. For those who study exoplanet atmospheres, these may sound familiar.', '1. Aerosols are everywhere. \n\nThey show up in transmission, emission, and reflection, sculpting exoplanet transmission spectra, the emitted flux of brown dwarfs, directly imaged exoplanets, and transiting exoplanets, and exoplanet albedos https://t.co/nxujl6AS1H', '2. Exoplanet aerosols are spatially heterogeneous \n\nCombined reflected light and emission observations have revealed that hot Jupiters are mostly clear on their daysides but cloudy on their western (morning) limbs and nightsides.', 'While global circulation model predictions are mostly in agreement with these observations, there are still some important questions, such as how to keep the dayside clear when refractory clouds like metal oxides should blanket it \n\n(image credit: @V_Parmentier) https://t.co/CkV6mkkvyt', ""At the same time, brown dwarfs and wide orbit planet-mass companions show distinct variability in their light curves, an important sign of heterogeneity in their cloud coverage \n\n(in hindsight we probably should've added a figure for this, but the review was long enough as is)"", ""3. Laboratory work is essential for learning more about exoplanet aerosols SO PLEASE FUND IT\n\nHow do exoplanet photochemical hazes form? What are their compositions? What do they look like? What clouds can actually condense?\n\nWe can't answer these (and more) without lab work!"", 'Take for instance this image of exoplanet haze analogues from @horstlab: Over a range of gas compositions and temperatures, the haze material look completely different - what diversity awaits us out there? \n\n(Original paper: https://t.co/FAryg5KPux) https://t.co/hRBdYi4EZs', 'So what lies in store in the next 10+ years? First, observations from next generation telescopes, including @NASAWebb, @NASARoman, @GMTelescope will go a long way towards constraining aerosol compositions', 'This includes measuring any aerosol spectral features in transmission and emission spectra and exoplanet albedos in reflected light, as well as atomic gas features at high spectral resolution in the optical and near UV to look for signs of condensation https://t.co/GzKCIVManV', 'At the same time, more sophisticated models that couple global dynamics with aerosol microphysics are needed to understand how aerosols form, evolve, and are transported in exoplanet atmospheres.', 'Lessons learned from these complex models can inform the development of more physical cloud models in retrieval codes, so model comparisons are vital \n\nIt also bears repeating that WE NEED TO FUND MORE LAB WORK', ""Finally, as we dive towards smaller planets, we'll have to worry about aerosols in their atmospheres too, and more compositionally diverse atmospheres beget more diverse aerosols as well"", ""We'll be wise to learn from studies of aerosols in the atmospheres of the terrestrial and icy words of our own solar system, both past and present. \n\nHope you all enjoy the paper!""]",https://arxiv.org/abs/2102.03480,"Observations of exoplanet atmospheres have shown that aerosols, like in the Solar System, are common across a variety of temperatures and planet types. The formation and distribution of these aerosols are inextricably intertwined with the composition and thermal structure of the atmosphere. At the same time, these aerosols also interfere with our probes of atmospheric composition and thermal structure, and thus a better understanding of aerosols lead to a better understanding of exoplanet atmospheres as a whole. Here we review the current state of knowledge of exoplanet aerosols as determined from observations, modeling, and laboratory experiments. Measurements of the transmission spectra, dayside emission, and phase curves of transiting exoplanets, as well as the emission spectrum and light curves of directly imaged exoplanets and brown dwarfs have shown that aerosols are distributed inhomogeneously in exoplanet atmospheres, with aerosol distributions varying significantly with planet equilibrium temperature and gravity. Parameterized and microphysical models predict that these aerosols are likely composed of oxidized minerals like silicates for the hottest exoplanets, while at lower temperatures the dominant aerosols may be composed of alkali salts and sulfides. Particles originating from photochemical processes are also likely at low temperatures, though their formation process is highly complex, as revealed by laboratory work. In the years to come, new ground- and space-based observatories will have the capability to assess the composition of exoplanet aerosols, while new modeling and laboratory efforts will improve upon our picture of aerosol formation and dynamics. ",Aerosols in Exoplanet Atmospheres
101,1358970018231320578,383142451,Shinnosuke Takamichi (È´òÈÅì ÊÖé‰πã‰ªã),['Our new paper is out! Human is acting as discriminator and classifier functions in GAN!\n\nHumanACGAN: conditional generative adversarial network with human-based auxiliary classifier and its evaluation in phoneme perception\n<LINK> <LINK>'],https://arxiv.org/abs/2102.04051,"We propose a conditional generative adversarial network (GAN) incorporating humans' perceptual evaluations. A deep neural network (DNN)-based generator of a GAN can represent a real-data distribution accurately but can never represent a human-acceptable distribution, which are ranges of data in which humans accept the naturalness regardless of whether the data are real or not. A HumanGAN was proposed to model the human-acceptable distribution. A DNN-based generator is trained using a human-based discriminator, i.e., humans' perceptual evaluations, instead of the GAN's DNN-based discriminator. However, the HumanGAN cannot represent conditional distributions. This paper proposes the HumanACGAN, a theoretical extension of the HumanGAN, to deal with conditional human-acceptable distributions. Our HumanACGAN trains a DNN-based conditional generator by regarding humans as not only a discriminator but also an auxiliary classifier. The generator is trained by deceiving the human-based discriminator that scores the unconditioned naturalness and the human-based classifier that scores the class-conditioned perceptual acceptability. The training can be executed using the backpropagation algorithm involving humans' perceptual evaluations. Our experimental results in phoneme perception demonstrate that our HumanACGAN can successfully train this conditional generator. ","HumanACGAN: conditional generative adversarial network with human-based
  auxiliary classifier and its evaluation in phoneme perception"
102,1358813708550606852,1358812012223037441,Sylvain Carr√©,"['New paper: <LINK>; we suggest a decentralized proof protocol with various potential applications. Fantastic team @FranckGabriel6 , Cl√©ment Hongler, Gustavo Lacerda and Gloria Capano!']",https://arxiv.org/abs/2102.03044,"Modern mathematics is built on the idea that proofs should be translatable into formal proofs, whose validity is an objective question, decidable by a computer. Yet, in practice, proofs are informal and may omit many details. An agent considers a proof valid if they trust that it could be expanded into a machine-verifiable proof. A proof's validity can thus become a subjective matter and lead to a debate, which may be difficult to settle. Hence, while the concept of valid proof is well-defined, the process to establish validity is itself a complex multi-agent problem. We introduce the SPRIG protocol. SPRIG allows agents to propose and verify succinct and informative proofs in a decentralized fashion; the trust is established by agents being able to request more details in the proof steps; debates, if they arise, must isolate details of proofs and, if they persist, go down to machine-level details, where they are automatically settled. A structure of bounties and stakes is set to incentivize agents to act in good faith. We propose a game-theoretic discussion of SPRIG, showing how agents with various types of information interact, leading to a proof tree with an appropriate level of detail and to the invalidation of wrong proofs, and we discuss resilience against various attacks. We then analyze a simplified model, characterize its equilibria and compute the agents' level of trust. SPRIG is designed to run as a smart contract on a blockchain platform. This allows anonymous agents to participate in the verification debate, and to contribute with their information. The smart contract mediates the interactions, settles debates, and guarantees that bounties and stakes are paid as specified. SPRIG enables new applications, such as the issuance of bounties for open problems, and the creation of derivatives markets, allowing agents to inject more information pertaining to proofs. ","Smart Proofs via Smart Contracts: Succinct and Informative Mathematical
  Derivations via Decentralized Markets"
103,1358740305118388227,2252040079,Auriol Degbelo,['New paper out: User interface factors of mobile UX: A study with an incident reporting application | @VISIGRAPPConf\n\nPreprint: <LINK>\n\nTalk@HUCAPP2021: <LINK>\n\n#geoux #uxdesign #mobileux #mobilemaps #incidentreporting'],http://arxiv.org/abs/2102.02510,"Smartphones are now ubiquitous, yet our understanding of user interface factors that maximize mobile user experience (UX), is still limited. This work presents a controlled experiment, which investigated factors that affect the usability and UX of a mobile incident reporting app. The results indicate that sequence of user interface elements matters while striving to increase UX, and that there is no difference between tab and scrolling as navigation modalities in short forms. These findings can serve as building blocks for empirically-derived guidelines for mobile incident reporting. ","User Interface Factors of Mobile UX: A Study with an Incident Reporting
  Application"
104,1358699269310607362,738769492122214400,Johannes Lischner,"['In our new paper, we calculate the band structures of all twisted homo- and heterobilayers composed of MoS2, WS2, MoSe2 and WSe2: <LINK>. So many interesting findings - pls see thread below.  #compchem #2dmaterials <LINK>', 'Relaxations matter...A LOT! In homobilayers relaxations change the symmetry of the highest valence states from hexagonal to triangular. In heterobilayers, relaxations can change the ordering of states suggesting that these materials are highly sensitive to pressure.', ""In many twisted bilayer materials flat bands are found...but not in all: in some heterobilayers, the highest valence band remains dispersive even at small twist angles. Such valence bands originate from K/K'-valleys of the monolayer."", ""Such K/K'-derived dispersive bands are spin-polarized, but the flat bands are not. In some heterobilayers, we observe a weird phenomenon: spin-split bands (almost) without spin polarization."", 'Finally, we present a new and efficient tight-binding model to describe twisted bilayer transition-metal dichalcogenides and give a complete set of parameters for all possible combinations.']",https://arxiv.org/abs/2102.03259,"Twisted bilayers of two-dimensional materials, such as twisted bilayer graphene, often feature flat electronic bands that enable the observation of electron correlation effects. In this work, we study the electronic structure of twisted transition metal dichalcogenide (TMD) homo- and heterobilayers that are obtained by combining MoS$_2$, WS$_2$, MoSe$_2$ and WSe$_2$ monolayers, and show how flat band properties depend on the chemical composition of the bilayer as well as its twist angle. We determine the relaxed atomic structure of the twisted bilayers using classical force fields and calculate the electronic band structure using a tight-binding model parametrized from first-principles density-functional theory. We find that the highest valence bands in these systems can derive either from $\Gamma$-point or $K$/$K'$-point states of the constituent monolayers. For homobilayers, the two highest valence bands are composed of monolayer $\Gamma$-point states, exhibit a graphene-like dispersion and become flat as the twist angle is reduced. The situation is more complicated for heterobilayers where the ordering of $\Gamma$-derived and $K$/$K'$-derived states depends both on the material composition and also the twist angle. In all systems, qualitatively different band structures are obtained when atomic relaxations are neglected. ","Flat band properties of twisted transition metal dichalcogenide homo-
  and heterobilayers of MoS$_2$, MoSe$_2$, WS$_2$ and WSe$_2$"
105,1358603807966470146,1050793558096191488,Lindsey Bignell,['My new paper is on the arxiv: Quenching factor measurements of sodium nuclear recoils in NaI:Tl determined by spectrum fitting\n<LINK>'],https://arxiv.org/abs/2102.02833,"We have performed measurements of sodium nuclear recoils in NaI:Tl crystals, following scattering by neutrons produced in a $^{7}$Li(p,n)$^{7}$Be reaction. Understanding the light output from such recoils, which is reduced relative to electrons of equivalent energy by the quenching factor, is critical to interpret dark matter experiments that search for nuclear scattering interactions. We have developed a spectrum-fitting methodology to extract the quenching factor from our measurements, and report quenching factors for nuclear recoil energies between 36 and 401 keV. Our results agree with other recent quenching factor measurements that use quasi-monoenergetic neutron sources. The new method will be applied in the future to the NaI:Tl crystals used in the SABRE experiment. ","Quenching factor measurements of sodium nuclear recoils in NaI:Tl
  determined by spectrum fitting"
106,1357988380978515971,2358873296,Shao-Jiang Wang,['Very excited for our new paper <LINK> .Thanks Sunny for the nice introduction. Our model rules out self-acceleration since $\\Delta\\Omega/\\Omega\\ll 1$. The acceleration is driven by quintessence-like effect of chameleon trapped at Vmin acting as an effective C.C. <LINK>'],https://arxiv.org/abs/2102.02020,"Values of the Hubble constant between the direct measurements from various independent local observations and that inferred from the cosmic microwave background with the $\Lambda$-cold-dark-matter model are in tension with persistent significance. We propose a late-time inhomogeneous resolution suggesting that a chameleon field coupled to a local overdensity of matter could be trapped at a higher potential energy density as an effective cosmological constant driving the local expansion rate faster than that of the background with lower matter density. We illustrate this mechanism in a toy model in which a region with only $20\%$ overdensity of matter is sufficient to resolve the Hubble tension, and the Hubble constant measured by the local distance ladders could be accommodated by the chameleon coupled to the observed overdensities from the large-scale structure surveys. ",Chameleon dark energy can resolve the Hubble tension
107,1357800351575990272,17239073,Atabey Kaygun,"['New paper with my Ph.D. student A. Karan: ""Time Series Classification via Topological Data Analysis"" <LINK>', ""@cagdastopcu The data wasn't ours. It came with that freq.""]",https://arxiv.org/abs/2102.01956,"In this paper, we develop topological data analysis methods for classification tasks on univariate time series. As an application, we perform binary and ternary classification tasks on two public datasets that consist of physiological signals collected under stress and non-stress conditions. We accomplish our goal by using persistent homology to engineer stable topological features after we use a time delay embedding of the signals and perform a subwindowing instead of using windows of fixed length. The combination of methods we use can be applied to any univariate time series and in this application allows us to reduce noise and use long window sizes without incurring an extra computational cost. We then use machine learning models on the features we algorithmically engineered to obtain higher accuracies with fewer features. ",Time Series Classification via Topological Data Analysis
108,1357797386576400385,913475709867315201,Priyank Jaini,"['Excited to present our new work ‚ÄúSampling in Combinatorial Spaces with SurVAE Flow augmented MCMC‚Äù with @nielsen_didrik  and \n@wellingmax to appear @aistats_conf. #AISTATS2021\n\nPaper: <LINK>\nCode: [coming soon] \n\n1/6', 'We combine ideas from neural transport based samplers for continuous data (eg NeuTra) and flows for discrete data (eg Flow++/SurVAE) to build samplers for combinatorial spaces.\n2/6 https://t.co/w97WfUhQoP', 'We first learn a series of transport maps that transform the discrete space onto a continuous latent space with a distribution that is easy to sample from.\n3/6 https://t.co/rY0uZDkvgZ', 'Our learning stage differs from the continuous neural transport analogs. We parameterize an approximate distribution  in the z-space by transporting the discrete distribution. We learn the transport mapping by minimising the KL between a gaussian and this distribution \n4/6 https://t.co/BkgQzXvlSC', 'Thus, the learned transformations (T*) define a transport map between this approximately gaussian latent distribution and the target discrete distribution. \n5/6 https://t.co/5N2tb67RVu', 'Sampling phase: we then run any sampler of choice (eg. MCMC, HMC, etc..) to generate samples from this approximate distribution in the latent space. Subsequently, using the learned transport maps on these samples we obtain samples from the discrete space.  \n6/6 https://t.co/oy8mkmOW8n']",https://arxiv.org/abs/2102.02374,"Hybrid Monte Carlo is a powerful Markov Chain Monte Carlo method for sampling from complex continuous distributions. However, a major limitation of HMC is its inability to be applied to discrete domains due to the lack of gradient signal. In this work, we introduce a new approach based on augmenting Monte Carlo methods with SurVAE Flows to sample from discrete distributions using a combination of neural transport methods like normalizing flows and variational dequantization, and the Metropolis-Hastings rule. Our method first learns a continuous embedding of the discrete space using a surjective map and subsequently learns a bijective transformation from the continuous space to an approximately Gaussian distributed latent variable. Sampling proceeds by simulating MCMC chains in the latent space and mapping these samples to the target discrete space via the learned transformations. We demonstrate the efficacy of our algorithm on a range of examples from statistics, computational physics and machine learning, and observe improvements compared to alternative algorithms. ",Sampling in Combinatorial Spaces with SurVAE Flow Augmented MCMC
109,1357756290412523532,1204291434842595328,Cl√©ment Rebuffel,"['Our new paper is out! \n\nControlling Hallucinations at Word Level in Data-to-Text Generation\n\nwith @MarcoRoberti, @LaureSoulier, G. Scoutheeten, R. Cancelliere and P. Gallinari\n\nPre-print: <LINK>\ncode: <LINK>\n\n1/3', 'We argue that hallucinations are best dealt with at the word level rather than instance level: since an hallucinated statement could be anywhere in a text, simply giving a global score (e.g. 80%  factual) results to loss of signal. Instead, we guide our model word by word.\n\n2/3 https://t.co/Tuo6EqVSpj', 'Results on WikiBio show that this approach is more efficient than previous SOTA + experiments on a noisy version of ToTTo are very promising for one day real life applications. Our approach makes no assumption regarding enc/dec and can be seamlessly integrated in any model!\n\n3/3 https://t.co/Gp2UPjmZAP']",https://arxiv.org/abs/2102.02810,"Data-to-Text Generation (DTG) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statements - usually called hallucinations - in their outputs. The control of this phenomenon is today a major challenge for DTG, and is the problem addressed in the paper. Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard WikiBio benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder. Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of ToTTo show that our model could be successfully used on very noisy settings. ",Controlling Hallucinations at Word Level in Data-to-Text Generation
110,1357525242902577152,2377407248,Daniel Whiteson,"['New paper!\n\n‚ÄúLearning to Isolate Muons‚Äù\n<LINK>\n\nA short study that tells us that there is still a LOT to learn about muons!', 'Telling prompt muons (from heavy bosons) apart from those embedded in jets should be easy. Jets are big and messy and leave a lot of energy everywhere. https://t.co/bdz7FIOKMr', 'Forever, people have just calculated ‚Äúisolation‚Äù which measures how much energy there is around the muon.\n\nWe had two Q:  can deep learning do better, and if so, can we interpret what it‚Äôs done?  \n\nThread on interpretable deep learning: https://t.co/nWmR3jPopH', 'It turns out that deep learning can do a LOT better, even if you use multiple isolation cones!  \n\nI was pretty surprised. Muons are simple, and the information is all radial, right? Wrong. https://t.co/hOUN7OXeK7', 'So what is the network doing? Is there non-radially symmetric information? \n\nWe found some observables that partially close the gap, but not all of it. https://t.co/Qlf0FmcHLC', ""So what ELSE is the network doing? \n\nWe still don't know!\n\nSomething that needs a new class of observables, perhaps ones that include the location of the muon to capture relative angular information."", ""@dangaristo Yeah, it's the same problem actually, but e+e- colliders don't produce jets as often, so it's not as important.""]",https://arxiv.org/abs/2102.02278,"Distinguishing between prompt muons produced in heavy boson decay and muons produced in association with heavy-flavor jet production is an important task in analysis of collider physics data. We explore whether there is information available in calorimeter deposits that is not captured by the standard approach of isolation cones. We find that convolutional networks and particle-flow networks accessing the calorimeter cells surpass the performance of isolation cones, suggesting that the radial energy distribution and the angular structure of the calorimeter deposits surrounding the muon contain unused discrimination power. We assemble a small set of high-level observables which summarize the calorimeter information and close the performance gap with networks which analyze the calorimeter cells directly. These observables are theoretically well-defined and can be studied with collider data. ",Learning to Isolate Muons
111,1357513543080431619,1278881046839398401,Mma Ikwut-Ukwa,"['New paper on the arXiv today! We confirm and characterize two new massive, short-period Jupiters, TOI-558 and TOI-559, found in the @TESSatMIT Full Frame Images. This work is the final form of my senior thesis! üôå\n\n<LINK> <LINK>', 'These planets were originally identified as candidates by two high school students, Asma Ali and Katya Bunten, working with George Zhou at @CenterForAstro to search through some of the early sectors of the TESS data. Then TESS recently reobserved them at higher cadence https://t.co/29SPzJMxls', 'We globally modeled these systems with plentiful follow-up transits from @LCO_Global and PEST, and RVs from PFS and CHIRON\nTOI-558 b: Mp = 3.61 Mj, P = 14.57 days, e = 0.298\nTOI-559 b: Mp = 6.01 Mj, P = 6.98 days, e = 0.151 https://t.co/nECcHVcR2p', 'We also examine the current sample of known transiting hot Jupiters--could there be multiple distinct mass-period distributions within this population? We‚Äôll know more as TESS eventually delivers a nearly complete sample of hot Jupiters transiting nearby, bright stars https://t.co/BhgvhvSFk3', 'Huge thank you to the best advisor @Astro_JRod + coauthors @samuelnquinn @amvanderburg @exofastupdates @bsgaudi @Therbaer @abieryla @lkreidberg @Jonmjenkins @ProfSaraSeager @twitspek @astrojennb @johannateske @JoshuaSchlieder and all the others who I have yet to find on twitter!', '@astroshrey @TESSatMIT thank you Shreyas!!', '@exoplamets @TESSatMIT thanks Samantha!!!']",https://arxiv.org/abs/2102.02222,"We report the discovery of two short-period massive giant planets from NASA's Transiting Exoplanet Survey Satellite (TESS). Both systems, TOI-558 (TIC 207110080) and TOI-559 (TIC 209459275), were identified from the 30-minute cadence Full Frame Images and confirmed using ground-based photometric and spectroscopic follow-up observations from TESS's Follow-up Observing Program Working Group. We find that TOI-558 b, which transits an F-dwarf ($M_{*}=1.349^{+0.064}_{-0.065}\ M_{\odot}$, $R_{*}=1.496^{+0.042}_{-0.040}\ R_{\odot}$, $T_{eff}=6466^{+95}_{-93}\ K$, age $1.79^{+0.91}_{-0.73}\ Gyr$) with an orbital period of 14.574 days, has a mass of $3.61\pm0.15\ M_{\rm J}$, a radius of $1.086^{+0.041}_{-0.038}\ R_{\rm J}$, and an eccentric (e=$0.300^{+0.022}_{-0.020}$) orbit. TOI-559 b transits a G-dwarf ($M_{*}=1.026\pm0.057\ M_{\odot}$, $R_{*}=1.233^{+0.028}_{-0.026}\ R_{\odot}$, $T_{eff}=5925^{+85}_{-76}\ K$, age $6.8^{+2.5}_{-2.0}\ Gyr$) in an eccentric (e=$0.151\pm0.011$) 6.984-day orbit with a mass of $6.01^{+0.24}_{-0.23}\ M_{\rm J}$ and a radius of $1.091^{+0.028}_{-0.025}\ R_{\rm J}$. Our spectroscopic follow-up also reveals a long-term radial velocity trend for TOI-559, indicating a long-period companion. The statistically significant orbital eccentricity measured for each system suggests that these planets migrated to their current location through dynamical interactions. Interestingly, both planets are also massive ($>3\ M_{\rm J}$), adding to the population of massive giant planets identified by TESS. Prompted by these new detections of high-mass planets, we analyzed the known mass distribution of hot and warm Jupiters but find no significant evidence for multiple populations. TESS should provide a near magnitude-limited sample of transiting hot Jupiters, allowing for future detailed population studies. ",Two Massive Jupiters in Eccentric Orbits from the TESS Full Frame Images
112,1357509481639403520,1556664198,Kyle Cranmer,"['New paper:\nA deep search for decaying dark matter with XMM-Newton blank-sky observations\nwith Joshua W. Foster, Marius Kongsore, Christopher Dessert, Yujin Park, Nicholas L. Rodd, Benjamin R. Safdi\n<LINK> <LINK>', 'In this work, we perform the most sensitive search to date for tmsteril neutrinos and other decaying DM scenarios across the mass range from 5 to 16 keV using archival XMM-Newton data.\n\nhttps://t.co/iXbpV1Wy1z https://t.co/CW5Ch3ztTB', 'We reduce 547 Ms of data from both the MOS and PN instruments using observations taken across the full sky and then use this data to search for evidence of DM decay in the ambient halo of the Milky Way.', 'We use a data-driven background subtraction strategy that removes most astrophysical and instrumental lines. We model the remaining continuum with a Gaussian process ‚Äî a non parametric approach that is a great fit (no pun intended) for this purpose. https://t.co/6Ce5iGMhJr', 'This is one of my few forays into astrophysics. The rest of the team did the lion‚Äôs share of the work, but I had a hand in the statistical approaches that were used. It‚Äôs an interesting example of cross-over in techniques from the LHC to astrophysics.', '@nausheenrshah @NYUPhysics @NYUDataScience https://t.co/BPiuGvDSaT']",https://arxiv.org/abs/2102.02207,"Sterile neutrinos with masses in the keV range are well-motivated extensions to the Standard Model that could explain the observed neutrino masses while also making up the dark matter (DM) of the Universe. If sterile neutrinos are DM then they may slowly decay into active neutrinos and photons, giving rise to the possibility of their detection through narrow spectral features in astrophysical X-ray data sets. In this work, we perform the most sensitive search to date for this and other decaying DM scenarios across the mass range from 5 to 16 keV using archival XMM-Newton data. We reduce 547 Ms of data from both the MOS and PN instruments using observations taken across the full sky and then use this data to search for evidence of DM decay in the ambient halo of the Milky Way. We determine the instrumental and astrophysical baselines with data taken far away from the Galactic Center, and use Gaussian Process modeling to capture additional continuum background contributions. No evidence is found for unassociated X-ray lines, leading us to produce the strongest constraints to date on decaying DM in this mass range. ","A deep search for decaying dark matter with XMM-Newton blank-sky
  observations"
113,1357446044657848320,39640065,Dan Scolnic,"[""Proud of Duke grad student Brodie Popovic for new paper 'Improved Treatment of Host-Galaxy Correlations in Cosmological Analyses With Type Ia Supernovae'.  Really important technical tool for next-generation cosmological analyses with SNIa.  <LINK>""]",https://arxiv.org/abs/2102.01776,"Improving the use of Type Ia supernovae (SNIa) as standard candles requires a better approach to incorporate the relationship between SNIa and the properties of their host galaxies. Using a spectroscopically-confirmed sample of $\sim$1600 SNIa, we develop the first empirical model of underlying populations for SNIa light-curve properties that includes their dependence on host-galaxy stellar mass. These populations are important inputs to simulations that are used to model selection effects and correct distance biases within the BEAMS with Bias Correction (BBC) framework. Here we improve BBC to also account for SNIa-host correlations, and we validate this technique on simulated data samples. We recover the input relationship between SNIa luminosity and host-galaxy stellar mass (the mass step, $\gamma$) to within 0.004 mags, which is a factor of 5 improvement over the previous method that results in a $\gamma$-bias of ${\sim}0.02$. We adapt BBC for a novel dust-based model of intrinsic brightness variations, which results in a greatly reduced mass step for data ($\gamma = 0.017 \pm 0.008$), and for simulations ($\gamma =0.006 \pm 0.007$). Analysing simulated SNIa, the biases on the dark energy equation-of-state, $w$, vary from $\Delta w = 0.006(5)$ to $0.010(5)$ with our new BBC method; these biases are significantly smaller than the $0.02(5)$ $w$-bias using previous BBC methods that ignore SNIa-host correlations. ","Improved Treatment of Host-Galaxy Correlations in Cosmological Analyses
  With Type Ia Supernovae"
114,1357435757946888192,795096464,Ariah Klages-Mundt,"['üö®New paperüö®: Optimal Intervention in Economic Networks using Influence Maximization Methods\n\nüëâ paper: <LINK>\nüëâ NetSci talk last Sep: <LINK>', 'We explore targeted financial system interventions. Here the goal is to aid targeted firms in the network to prevent cascading losses and so prevent other firms from needing aid (vs. aiding all firms directly). Network effects play a large role in such targeting. 2/6', 'We focus on the computational aspect of optimizing a targeted intervention (as opposed to, e.g., the also very important moral hazard problems around future intervention expectations). We derive analytical results + demonstrate methods on World Input Output Data. 3/6 https://t.co/BccaQ0aZiJ', ""We show that targeted intervention is computationally hard to optimize or even to approximate in general. This is especially a challenge for large, complex networks, like realistic financial systems. But under a smoothed analysis-like idea, the problem isn't unmanageable! 4/6"", ""In a sort of 'relaxed' version of the problem with randomized thresholds, the intervention problem in expectation can be well approximated. This builds on submodularity results from influence maximization models. 5/6"", ""Some other interesting results pop out. Parallel results apply to identifying large failure cascades and can contribute toward efficient scenario generation for stress testing. It's also computationally hard to calculate expected firm values in the face of potential cascades. 6/6""]",https://arxiv.org/abs/2102.01800,"We consider optimal intervention in the Elliott-Golub-Jackson network model and show that it can be transformed into an influence maximization problem, interpreted as the reverse of a default cascade. Our analysis of the optimal intervention problem extends well-established targeting results to the economic network setting, which requires additional theoretical steps. We prove several results about optimal intervention: it is NP-hard and additionally hard to approximate to a constant factor in polynomial time. In turn, we show that randomizing failure thresholds leads to a version of the problem which is monotone submodular, for which existing powerful approximations in polynomial time can be applied. In addition to optimal intervention, we also show practical consequences of our analysis to other economic network problems: (1) it is computationally hard to calculate expected values in the economic network, and (2) influence maximization algorithms can enable efficient importance sampling and stress testing of large failure scenarios. We illustrate our results on a network of firms connected through input-output linkages inferred from the World Input Output Database. ","Optimal Intervention in Economic Networks using Influence Maximization
  Methods"
115,1357409170622930949,165162168,Roman Jurowetzki,['New paper: üëâ<LINK>\n\nDo industry AI-labs cherry-pick üçí some of the best researchers? Yup! \nIs that potentially a problem for the alignment of AI tech and societal goals? What do you think? New working paper with @Daniel_S_Hain  @JMateosGarcia @kstathou <LINK>'],https://arxiv.org/abs/2102.01648,"The private sector is playing an increasingly important role in basic Artificial Intelligence (AI) R&D. This phenomenon, which is reflected in the perception of a brain drain of researchers from academia to industry, is raising concerns about a privatisation of AI research which could constrain its societal benefits. We contribute to the evidence base by quantifying transition flows between industry and academia and studying its drivers and potential consequences. We find a growing net flow of researchers from academia to industry, particularly from elite institutions into technology companies such as Google, Microsoft and Facebook. Our survival regression analysis reveals that researchers working in the field of deep learning as well as those with higher average impact are more likely to transition into industry. A difference-in-differences analysis of the effect of switching into industry on a researcher's influence proxied by citations indicates that an initial increase in impact declines as researchers spend more time in industry. This points at a privatisation of AI knowledge compared to a counterfactual where those high-impact researchers had remained in academia. Our findings highlight the importance of strengthening the public AI research sphere in order to ensure that the future of this powerful technology is not dominated by private interests. ","The Privatization of AI Research(-ers): Causes and Potential
  Consequences -- From university-industry interaction to public research
  brain-drain?"
116,1357363110697443328,2949874961,Ramis_Movassagh,['Improved robustness bounds for quantum supremacy. Enjoyed working with Ryuhei Mori and Yasuhiro Kondo on our joint new paper posted today:\n<LINK> <LINK>'],https://arxiv.org/abs/2102.01960,"Motivated by the recent experimental demonstrations of quantum supremacy, proving the hardness of the output of random quantum circuits is an imperative near term goal. We prove under the complexity theoretical assumption of the non-collapse of the polynomial hierarchy that approximating the output probabilities of random quantum circuits to within $\exp(-\Omega(m\log m))$ additive error is hard for any classical computer, where $m$ is the number of gates in the quantum computation. More precisely, we show that the above problem is $\#\mathsf{P}$-hard under $\mathsf{BPP}^{\mathsf{NP}}$ reduction. In the recent experiments, the quantum circuit has $n$-qubits and the architecture is a two-dimensional grid of size $\sqrt{n}\times\sqrt{n}$. Indeed for constant depth circuits approximating the output probabilities to within $2^{-\Omega(n\log{n})}$ is hard. For circuits of depth $\log{n}$ or $\sqrt{n}$ for which the anti-concentration property holds, approximating the output probabilities to within $2^{-\Omega(n\log^2{n})}$ and $2^{-\Omega(n^{3/2}\log n)}$ is hard respectively. We then show that the hardness results extend to any open neighborhood of an arbitrary (fixed) circuit including the trivial circuit with identity gates. We made an effort to find the best proofs and proved these results from first principles, which do not use the standard techniques such as the Berlekamp--Welch algorithm, the usual Paturi's lemma, and Rakhmanov's result. ","Quantum supremacy and hardness of estimating output probabilities of
  quantum circuits"
117,1357228966269112320,96930393,Fernando P√©rez Pe√±a,"['Our new paper is available on @arxiv : ""Real-time detection of uncalibrated sensors using Neural Networks"" <LINK>']",https://arxiv.org/abs/2102.01565,"Nowadays, sensors play a major role in several contexts like science, industry and daily life which benefit of their use. However, the retrieved information must be reliable. Anomalies in the behavior of sensors can give rise to critical consequences such as ruining a scientific project or jeopardizing the quality of the production in industrial production lines. One of the more subtle kind of anomalies are uncalibrations. An uncalibration is said to take place when the sensor is not adjusted or standardized by calibration according to a ground truth value. In this work, an online machine-learning based uncalibration detector for temperature, humidity and pressure sensors was developed. This solution integrates an Artificial Neural Network as main component which learns from the behavior of the sensors under calibrated conditions. Then, after trained and deployed, it detects uncalibrations once they take place. The obtained results show that the proposed solution is able to detect uncalibrations for deviation values of 0.25 degrees, 1% RH and 1.5 Pa, respectively. This solution can be adapted to different contexts by means of transfer learning, whose application allows for the addition of new sensors, the deployment into new environments and the retraining of the model with minimum amounts of data. ",Real-time detection of uncalibrated sensors using Neural Networks
118,1357169891724455936,1119252439050354688,Peter Hase,"['This project has been a nice and long effort, but I‚Äôm excited to share a new paper: **When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data**\n\nWork done with @mohitban47\n\nArxiv: <LINK>\nThread below 1/n <LINK>', 'There are datasets where people explain why data point x gets label y, and the explanations look very helpful for solving the task. But what if models already know the relevant facts or can infer what they need to from a task input alone?', 'To test this question, we first design a synthetic task where we vary the num. of distinct hidden tasks in the data (we also test with existing datasets later). Our ‚Äúexplanations‚Äù of each point reveal what hidden task it belongs to &amp; provide helpful info for predicting its label', 'We find that when there are a small number of tasks, models do well (98% acc), but performance quickly falls off as the number of tasks increases. If we could condition on explanations here, performance would jump back to 98%. https://t.co/8YALCUFA9O', 'But we shouldn‚Äôt expect to have explanations at test time -- that‚Äôs new unexplained data we need to predict. So we learn to retrieve past explanations from training data, &amp; we can solve the task again (98% acc). Explanation retrieval even beats a baseline with 10x training data. https://t.co/XMO2PS6HNo', 'We argue that this approach is suitable for explanation data since large LMs can extract relevant facts from inputs or infer task representations from them. We contrast with other graphical models in the paper. https://t.co/T1boc0ghtC', 'We investigate several other questions, including whether models can combine info across explanations (yes!), as well as whether explanations can help point models toward using strong/causal features rather than weak/spurious ones (depends on their exact correlation, actually!) https://t.co/fBhL7VhHZ2', 'After investigating 7 total research questions with our synthetic task, we try this model on 3 existing datasets. Accuracy usually goes up a little bit across conditions, but we do not find any stat-sig improvements. This applies for several training set sizes.', 'Ultimately, we claim that for explanations to be helpful, the model must be able to better infer relevant latent info given the explanation and input together, relative to using the input alone. We also identify 5 conditions for learning retrieval to be feasible in practice. n/n', '@kerstingAIML @PMinervini @heghbalz @mohitban47 @oanacamb @looselycorrect Thanks for the interest in our work and the pointers! These papers are good examples of explanations as priors, and we will make sure to add them to appropriate related works section in the next version of the paper', '@looselycorrect @mohitban47 Thanks for the appreciation, looking forward to more discussion and work on the topic :)']",https://arxiv.org/abs/2102.02201,"Many methods now exist for conditioning model outputs on task instructions, retrieved documents, and user-provided explanations and feedback. Rather than relying solely on examples of task inputs and outputs, these approaches use valuable additional data for improving model correctness and aligning learned models with human priors. Meanwhile, a growing body of evidence suggests that some language models can (1) store a large amount of knowledge in their parameters, and (2) perform inference over tasks in textual inputs at test time. These results raise the possibility that, for some tasks, humans cannot explain to a model any more about the task than it already knows or could infer on its own. In this paper, we study the circumstances under which explanations of individual data points can (or cannot) improve modeling performance. In order to carefully control important properties of the data and explanations, we introduce a synthetic dataset for experiments, and we also make use of three existing datasets with explanations: e-SNLI, TACRED, and SemEval. We first give a formal framework for the available modeling approaches, in which explanation data can be used as model inputs, as targets, or as a prior. After arguing that the most promising role for explanation data is as model inputs, we propose to use a retrieval-based method and show that it solves our synthetic task with accuracies upwards of 95%, while baselines without explanation data achieve below 65% accuracy. We then identify properties of datasets for which retrieval-based modeling fails. With the three existing datasets, we find no improvements from explanation retrieval. Drawing on findings from our synthetic task, we suggest that at least one of six preconditions for successful modeling fails to hold with these datasets. Our code is publicly available at this https URL ","When Can Models Learn From Explanations? A Formal Framework for
  Understanding the Roles of Explanation Data"
119,1357006401952972808,970803673155751936,Charline Le Lan,"['Excited to present our #AAAI2021 paper ‚ÄúMetrics and continuity in Reinforcement learning‚Äù with @marcgbellemare and @pcastr tomorrow ! We propose a new perspective on representation learning and generalization in RL üåü\nüìúPaper: <LINK>\n\nIntro Videoüëá 1/8 <LINK>', 'In RL, we often deal with systems with large state spaces. We can‚Äôt exactly represent the value of each of these states and need some type of generalization. One way to do that is to look at structured representations in which similar states are assigned similar predictions.\n2/8 https://t.co/xwJgRi552q', 'We consider the case where these representations are induced by state metrics.\nHow shall we choose a metric to get good generalization properties?ü§î\nAn interesting metric should give us continuity properties for the RL function of interest in our problem.\n3/8 https://t.co/jV5QwffeYP', 'A good metric should also give us a representation as coarse as possible so that we can cheaply generalize to new states.\n4/8 https://t.co/nJl7bGrqlq', 'Our paper unifies representations of states spaces and the notion of continuity via a taxonomy of metrics. We also provide a hierarchy of metrics to compare the topologies induced by all these metrics. \n5/8 https://t.co/8mvjIrG1Fr', 'Using our taxonomy, we find that most commonly discussed metrics are actually poorly suited for algorithms that convert representations into values, so we introduce new metrics to overcome this shortcoming.\n6/8 https://t.co/vETKDBaeMw', 'What kind of generalization do these metrics produce when used for value function approximation? We present an empirical evaluation comparing our metrics and showing the importance of the choice of a neighborhood in RL algorithms.\n7/8 https://t.co/WTIGGoc0SA', 'Poster: https://t.co/cV21MM2gLs\nSlides:https://t.co/BH9r6tmgv5\nüíªCode: https://t.co/xvxSA8oWLL\n8/8', '@tw_killian @marcgbellemare @pcastr Thank you Taylor! hope you enjoy it‚ò∫Ô∏è‚ò∫Ô∏è', '@khimya @marcgbellemare @pcastr Thanks a lot Khimya üòÑ!']",https://arxiv.org/abs/2102.01514,"In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of ""well-behaved"" neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered. ",Metrics and continuity in reinforcement learning
120,1356995005135601664,352507474,Gorka Azkune,"['Check out our new paper entitled ""Inferring spatial relations from textual descriptions of images"" with @Aitzole @oierldl @IgnacioArganda @Aitor57 and @eagirre where we show that NNs learn prototypical spatial relations btw entities <LINK>', 'The paper has been published by the Pattern Recognition journal https://t.co/gJb9Gvq1EH We share our code and the used dataset publicly (specially created for this task and called REC-COCO) https://t.co/j4s7SfzCwt']",https://arxiv.org/abs/2102.00997,"Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image. ",Inferring spatial relations from textual descriptions of images
121,1356937293534269440,1109745957346963456,Rub√©n Moreno-Bote,"['New @arxiv paper with @joramirezruiz on multi-alternative choices and accumulator models under finite sampling capacity. Take-home message: sampling exactly 5 options is optimal when capacity is low\n\n<LINK>', 'We assume that the environment produces a large number of options, each characterized by a drift, unknown to the agent. The agent has a finite resource T, that they divide and allocate across options in order to sample them. What is the best one-shot allocation policy? https://t.co/2uOBvjZQ6d', 'We found a duality in our definition of sampling capacity: sampling capacity can be understood as finite sampling time or as finite sampling precision https://t.co/a9EP9Qi7hg', 'And here the main result: at low capacity is it best to sample exactly 5 options, and there is a fast and sudden transition to a power-low sampling regime that happens at low capacity. Results are general for a large family of priors. So far studied for Gaussian likelihoods https://t.co/FXwywD87gk', 'We provide an analytical characterization of the limiting behaviors at low and high capacity, predicting nicely the simulation results. Our results have implications in multi-alternative decision making under cognitive limits.', 'Open source codes can be found here: \n\nhttps://t.co/Cy6STwFk5B']",https://arxiv.org/abs/2102.01597,"When facing many options, we narrow down our focus to very few of them. Although behaviors like this can be a sign of heuristics, they can actually be optimal under limited cognitive resources. Here we study the problem of how to optimally allocate limited sampling time to multiple options, modelled as accumulators of noisy evidence, to determine the most profitable one. We show that the effective sampling capacity of an agent increases with both available time and the discriminability of the options, and optimal policies undergo a sharp transition as a function of it. For small capacity, it is best to allocate time evenly to exactly five options and to ignore all the others, regardless of the prior distribution of rewards. For large capacities, the optimal number of sampled accumulators grows sub-linearly, closely following a power law for a wide variety of priors. We find that allocating equal times to the sampled accumulators is better than using uneven time allocations. Our work highlights that multi-alternative decisions are endowed with breadth-depth tradeoffs, demonstrates how their optimal solutions depend on the amount of limited resources and the variability of the environment, and shows that narrowing down to a handful of options is always optimal for small capacities. ","Optimal allocation of finite sampling capacity in accumulator models of
  multi-alternative decision making"
122,1356927302819545089,405103790,Matteo Angelinelli,"['A new paper today <LINK> by myself, S. Ettori, @franco_vazza  and T.W. Jones!  We study the outskirts of simulated galaxy clusters to investigate the physical properties of matter clumps and filaments #magcow <LINK>', 'We developed two different algorithms, which detect matter clumps, starting from overdensity in the simulated density field, and filaments, using a new proxy based on the gas radial velocity and gas entropy. https://t.co/l47aFkS8gc', ""We find that density and temperature for our clumps population are independent by the central cluster's mass, while for filaments we note a slight increase of temperature with the cluster's mass. https://t.co/GxJL6FDkzk"", 'We investigate possible relations between clumps and filaments proprieties, and we find a high level of correlation, both for density and temperature. https://t.co/GAuuVIZOgr', 'Moreover, we study the mass and volume contribution of clumps and filaments over the total amount of gas in our simulations. We find that combing the different contributions account for ~17% of the total gas mass and only ~1% of the volume. https://t.co/8PS5LG1qG0', ""We divide our simulated volume into two different radial shells. We note that closer to the central cluster, both clumps' density and temperature are higher than ones in the periphery regions, making the clumps' X-rays detection easier. https://t.co/tv9gfrctq6"", ""Furthermore, analysing clumps' and filaments' mass and volume contributions in the different shells, we conclude that the inner one is a suitable candidate to detect and analyse matter clumps, while the outer one is better to filaments' studies. https://t.co/GqarDD9BwE"", ""Finally, we study three different scale relations M-L, L-T and M-T. Up to 3*R500, the interactions between clumps and ICM change the physical proprieties of the infalling structures. Otherwise, over 3*R500, clumps are described by scaling relations similar to the clusters' ones. https://t.co/sud4AhOYt1"", 'We are working on possible extensions of this work, using a different kind of simulations. Moreover, using SIXTE simulator, we are simulating what the WFI and @AthenaXIFU instruments (which will be on aboard on the @AthenaXobs) would observe in the outskirts of galaxy clusters https://t.co/gnL6nOfmDi']",https://arxiv.org/abs/2102.01096,"We report on the possibility of studying the proprieties of cosmic diffuse baryons by studying self-gravitating clumps and filaments connected to galaxy clusters. While filaments are challenging to detect with X-ray observations, the higher density of clumps makes them visible and a viable tracer to study the thermodynamical proprieties of baryons undergoing accretion along cosmic web filaments onto galaxy clusters. We developed new algorithms to identify these structures and applied them to a set of non-radiative cosmological simulations of galaxy clusters at high resolution. We find that in those simulated clusters, the density and temperature of clumps are independent of the mass of the cluster where they reside. We detected a positive correlation between the filament temperature and the host cluster mass. The density and temperature of clumps and filaments also tended to correlate. Both the temperature and density decrease moving outward. We observed that clumps are hotter, more massive, and more luminous if identified closer to the cluster center. Especially in the outermost cluster regions (~3*R500,c or beyond), X-ray observations might already have the potential to locate cosmic filaments based on the distribution of clumps and to allow one to study the thermodynamics of diffuse baryons before they are processed by the intracluster medium. ",Properties of clumps and filaments around galaxy clusters
123,1356911899317788673,1110110589202956289,Alessandro Strumia,"[""New paper freely appeared on arXiv. Of course it'about physics, not gender.  \n<LINK>""]",https://arxiv.org/abs/2102.01084,"The Coleman-Weinberg mechanism can realise different phases of dynamical symmetry breaking. In each phase a combination of scalars, corresponding to the pseudo-Goldstone boson of scale invariance, has a loop-suppressed mass. We show that additional scalars, beyond the pseudo-Goldstone bosons, can become light at critical points in the parameter space where two different phases co-exist. We present a minimal implementation of the mechanism in multi-scalar models, detailing how loop-suppressed masses and mixings can be computed. We discuss realisations of the resulting multi-phase criticality principle and its relevance to the case of the Higgs boson. ","Light Higgs boson from multi-phase criticality in dynamical symmetry
  breaking"
124,1356909056049508353,743028716457070592,Franco Vazza,"['Kudos to @MatteoAnge for his new subm. paper, which can be paraphrased as \n""Fantastic clumps/filaments and where to find them""  showing that X-ray detectable clumps are the tip of the iceberg allowing us study cosmic filaments around clusters  <LINK>  #magcow <LINK>', ""I'll leave the stage for him to have a more proper thread on his nice results! Here is one of Matteo's movies, showing the richness of Enzo-AMR simulations he's using to study gas clumps and filaments. https://t.co/3mU6ekJyZ1""]",https://arxiv.org/abs/2102.01096,"We report on the possibility of studying the proprieties of cosmic diffuse baryons by studying self-gravitating clumps and filaments connected to galaxy clusters. While filaments are challenging to detect with X-ray observations, the higher density of clumps makes them visible and a viable tracer to study the thermodynamical proprieties of baryons undergoing accretion along cosmic web filaments onto galaxy clusters. We developed new algorithms to identify these structures and applied them to a set of non-radiative cosmological simulations of galaxy clusters at high resolution. We find that in those simulated clusters, the density and temperature of clumps are independent of the mass of the cluster where they reside. We detected a positive correlation between the filament temperature and the host cluster mass. The density and temperature of clumps and filaments also tended to correlate. Both the temperature and density decrease moving outward. We observed that clumps are hotter, more massive, and more luminous if identified closer to the cluster center. Especially in the outermost cluster regions (~3*R500,c or beyond), X-ray observations might already have the potential to locate cosmic filaments based on the distribution of clumps and to allow one to study the thermodynamics of diffuse baryons before they are processed by the intracluster medium. ",Properties of clumps and filaments around galaxy clusters
125,1356888188170760196,1169196060130123782,Gergely Neu,"['MYSTERY: why does SGD generalize well? My new paper may provide some answers: a perturbation analysis identifies three factors that contribute to good generalization. ""Flatness"" is one of them.\n\n(And yes, the analysis is information-theoretic.)\n<LINK>\n1/ <LINK>', 'SGD: the algorithm that we all know and love. We consider it for minimizing a general (non-convex) differentiable loss, by doing multiple passes over the data set S = (Z_1,...,Z_n). The stepsize schedule and sampling rule are fixed but arbitrary.\n2/ https://t.co/maqMd9MTWn', 'The main result is a generalization-error bound that depends on 3 key quantities:\n- The variance of the gradients along the SGD path,\n- the perturbation-sensitivity of the gradients along the SGD path, and\n- the perturbation-sensitivity of the loss at the final output.\n3/ https://t.co/A5cAvpatHx', 'The bound involves some tradeoff parameters œÉ that are independent of the algorithm. This means that the bound holds true for every possible choice of œÉ, and in particular the one that minimizes the bound---no need for hyperparameter-tuning!\n4/', 'The gradient variance V measures the variability of the gradients with respect to the randomness of the data. This term is small if the gradients are consistent between data points, as measured by the L2 norm.\n5/ https://t.co/QeycQE0GdU', 'The gradient sensitivity Œì measures the variability of the gradients with respect to small changes to the parameter vector. This term is small if the gradients are ""spatially consistent"", as measured by the L2 norm---essentially a local measure of smoothness.\n6/ https://t.co/UAy9ZRJm2U', 'The value sensitivity Œî measures the variability of the loss function to small changes to the parameter vector. This term is small if the training and test loss functions are flat around the final solution produced by SGD.\n7/ https://t.co/n77xr2emHd', 'These three quantities depend on the parametrization and are sometimes at conflict: one cannot be made arbitrarily small without making another one huge. This fig sketches some possible tradeoffs for 3 parametrizations.\n8/ https://t.co/rV1svMufv9', 'Is it possible then to measure V, Œì and Œî under the most favorable parametrization? Turns out that this is indeed possible: our bound holds simultaneously for *all* parametrizations!\n9/ https://t.co/PVHnc1jqrz', 'The quantities appearing in the bound are defined in terms of perturbations of general covariance matrices, which allows measuring the gradients and the flatness of the objective in various geometries.\n10/ https://t.co/0cEhw5nsN7', 'One way to read this result is that ""SGD generalizes well as long as there\'s a geometry where the optimum it converges to is flat and the stochastic gradients are well-behaved."" Note though that the result does not explain why SGD would satisfy these properties.\n11/', 'One possible takeaway is that one should run SGD with a parametrization that makes these quantities small. A caveat is that the key quantities are not directly observable as they depend on test data.\n12/', 'The analysis is based on the information-theoretic techniques of Russo &amp; Zhou (2016) and Xu &amp; Raginsky (2017) that bounds the generalization error of any algorithm in terms of the mutual information between its input and output.\n13/ https://t.co/BaFY9ttXQp', 'The twist is to apply it to a randomly perturbed version of the output of SGD and explicitly account for the mismatch between the perturbed and the original version.\n14/ https://t.co/dWB4Iz2t9B', 'The mutual information between the input and the perturbed output can then be bounded using a technique pioneered by Pensia, Jog &amp; Loh (2017) for analyzing stochastic gradient Langevin dynamics.\n15/ https://t.co/mLVYphBxCy', 'The main innovation is that our perturbations *only exist in the analysis*, which gives us the flexibility of choosing them arbitrarily without affecting the actual performance of SGD. This enables adaptivity to parametrization, etc.\n16/', 'While the generalization bounds for SGLD are generally better, tuning its hyperparameters to trade off the training and generalization errors is much harder (especially when also allowing non-isotropic noise).\n17/', ""If you're interested to learn more, check out the paper on arxiv:\nhttps://t.co/NOpqEy1BDp\n\nI'm very curious what you all think, especially since I don't consider myself an expert in the area. I'd particularly appreciate pointers to relevant literature that I have missed.\n18/"", ""Special thanks to @lugosi_gabor for his help while preparing this paper!\n\nAlso, apologies for the bad drawings and handwriting in this thread. My PR budget couldn't afford HD images of purebred nonconvex loss surfaces this time.\n19/FIN"", '@vitalyFM you mean an extra factor of T, right? that is indeed there, due to the accumulation of perturbations between iterations. this is a key difference with SGLD where these accumulated perturbations end up influencing the training error instead of the generalization error.', '@vitalyFM also, d can be improved to the trace of the covariance matrix of the perturbations.', '@vitalyFM thanks so much for the references!!', '@vitalyFM do you think that such improvement is possible for plain SGD as well? (that is, the non-DP version considered here)', '@pfau @lugosi_gabor i do believe that adaptive stepsizes and preconditioners could be included in the framework, but extra care needs to be taken since the iterates are no longer Markovian in this case. i expect that the bounds should hold after replacing g with the preconditioned update.', '@pfau @lugosi_gabor this gradient sensitivity is a scalar: the expected squared L2 distance between the gradient evaluated at w and at w + Œæ for Œæ ~ N(0,œÉ¬≤I). i believe that for small œÉ¬≤, it should converge to the Frobenius norm of the Hessian.', ""@pfau @lugosi_gabor you're probably right: it might actually be the nuclear norm rather than the Frobenius norm. i didn't think about this enough.."", '@vitalyFM basically the idea is adding a single perturbation to the final output, then slice it up and pretend in the analysis that we added it incrementally at each iteration. this could work for other analysis styles as well..']",https://arxiv.org/abs/2102.00931,"We study the generalization properties of the popular stochastic optimization method known as stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates. ","Information-Theoretic Generalization Bounds for Stochastic Gradient
  Descent"
126,1356682139127877634,1152296594,Swabha Swayamdipta,"['Can we rid language representations of pernicious social (racial) biases, in a hate speech detection setting? Not so easily ‚òπÔ∏è\n\nInvesting in better data collection is probably a better route. Check out our new work at EACL to learn more üëá\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2102.00086,"Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases. ",Challenges in Automated Debiasing for Toxic Language Detection
127,1356668112238706692,852846184557416448,Anirban Roy,['Our new paper on ArXiv today!  \n<LINK> <LINK>'],https://arxiv.org/abs/2102.00975,"We propose a new reionization probe that uses cosmic microwave background (CMB) observations; the cross-correlation between fluctuations in the CMB optical depth which probes the integrated electron density, $\delta\tau$, and the Compton $y$-map which probes the integrated electron pressure. This cross-correlation is much less contaminated than the $y$-map power spectrum by late-time cluster contributions. In addition, this cross-correlation can constrain the temperature of ionized bubbles while the optical-depth fluctuations and kinetic SZ effect can not. We measure this new observable using a Planck $y$-map as well as a map of optical-depth fluctuations that we reconstruct from Planck CMB temperature data. We use our measurements to derive a first CMB-only upper limit on the temperature inside ionized bubbles, $T_{\rm b}\lesssim 7.0\times10^5\,$K ($2\,\sigma$). We also present future forecasts, assuming a fiducial model with characteristic reionization bubble size $R_{\rm b}=5\,$Mpc and $T_{\rm b}=5\times10^4\,$K. The signal-to-noise ratio of the fiducial cross-correlation using a signal dominated PICO-like $y$-map becomes $\simeq7$ with CMB-S4 $\delta\tau$ and $\simeq13$ with CMB-HD $\delta\tau$. For the fiducial model, we predict that the CMB-HD $-$ PICO cross-correlation should achieve an accurate measurement of the reionization parameters; $T_{\rm b}\simeq 49800^{+4500}_{-5100}\,$K and $R_{\rm b}\simeq 5.09^{+0.66}_{-0.79}\,$Mpc. Since the power spectrum of the electron density fluctuations is constrained by the $\delta\tau$ auto spectrum, the temperature constraints should be only weakly model-dependent on the details of the electron distributions and should be statistically representative of the temperature in ionized bubbles during reionization. This cross-correlation could, therefore, become an important observable for future CMB experiments. ","Constraining reionization with the first measurement of the
  cross-correlation between the CMB optical-depth fluctuations and the Compton
  y-map"
128,1356603664610246659,1143517184611799040,Jamie Smith,"['In this new paper from my team @GoogleAI we use ML + a differentiable CFD simulator + TPUs to achieve ~86x speedup over direct numerical simulation.\n\nCoauthored with @shoyer, @dkochkov1, Ayya Alieva, Qing Wang and Michael Brenner.\n\n<LINK> <LINK> <LINK>']",https://arxiv.org/abs/2102.01010,"Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics and plasma physics. Fluids are well described by the Navier-Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large eddy simulation, our results are as accurate as baseline solvers with 8-10x finer resolution in each spatial dimension, resulting in 40-80x fold computational speedups. Our method remains stable during long simulations, and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black box machine learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization. ",Machine learning accelerated computational fluid dynamics
129,1356552795055349760,1146409804610584577,Alexander Jahn,"['Our new paper explores the universal behavior of entanglement of purification in CFTs at long distances, generalizing well-known results for mutual information: <LINK>']",https://arxiv.org/abs/2102.00013,"Quantifying entanglement properties of mixed states in quantum field theory via entanglement of purification and reflected entropy is a new and challenging subject. In this work, we study both quantities for two spherical subregions far away from each other in the vacuum of a conformal field theory in any number of dimensions. Using lattice techniques, we find an elementary proof that the decay of both, the entanglement of purification and reflected entropy, is enhanced with respect to the mutual information behaviour by a logarithm of the distance between the subregions. In the case of the Ising spin chain at criticality and the related free fermion conformal field theory, we compute also the overall coefficients numerically for the both quantities of interest. ","Long-distance entanglement of purification and reflected entropy in
  conformal field theory"
130,1356541380412211215,899968956253044737,Yanai Elazar,"['Are our Language Models consistent? Apparently not!\nOur new paper quantifies that: <LINK>\n\nw/ @KassnerNora, @ravfogel, @Lasha1608, Ed Hovy, @HinrichSchuetze, and @yoavgo <LINK>', 'Consistency is important for most NLP tasks, and multiple papers observed the lack of this property in downstream tasks (including cool works by @AkariAsai , @bhavana_dalvi, @jasonbaldridge, @AllysonEttinger  --- some of the authors of these papers which are on twitter)', ""We test if this property exists in PLMs. If not, it's unlikely to acquire it during standard FT, and thus important for PLMs to obtain.\nWe focus on consistency with respect to factual knowledge in the LAMA setup (@Fabio_Petroni et al.) and test for invariance to paraphrasing."", 'For testing consistency, we manually create a new resource: ParaRelü§ò, that contains 328 paraphrases for 38 relations. This resource was curated manually and was a huge effort to create properly, with high agreement between the team (and others!)', 'The models we test are generally not consistent, although it really depends on the type of relations.\n\nFinally, we also propose a method to improve this property in the pretraining, which improves also the ability to extract the factual knowledge from these models.', 'And yes, @aclmeeting deadline is in a couple of hours, but you can read it afterwards, or, if you wish to procrastinate üòâ', ""@LChoshen @aclmeeting It's more nuanced than that, but basically yes (although not ending, as we're dealing with MLM, and cloze-patterns). Checkout section 3.1"", ""@boknilev @LChoshen ah, we wondered if that was clear.\nIt depends on the schema/relations, but if it's an N-1 relation, answering a question on a born-in relation, two different cities would be contradictory."", ""@boknilev @LChoshen On the other hand, in the 'shares-border-with' relation (N-M), there's more than one possible answer (usually).\nIt is possible that this would result in an inconsistent predictions, but it may be hard to evaluate, since you need to have all of the correct answers"", ""@boknilev @LChoshen And that would be fine (not contradictory). The data we used defines it (if I recall correctly) as the most fine-grained info, which is not ideal.\nWe currently don't control for that, however, from what we noticed it's not a big issue in practice."", ""@oanacamb @AkariAsai @bhavana_dalvi @jasonbaldridge @AllysonEttinger @BrendanShilling @PMinervini Cool!\nThanks for pointing this out, we'll make sure to refer to this work."", ""@oanacamb @AkariAsai @bhavana_dalvi @jasonbaldridge @AllysonEttinger @BrendanShilling @PMinervini I need to read the paper first üòÄ\nI'll reach out after reading and then we can discuss!"", ""@mariusmosbach @aclmeeting Thanks!\nThe idea is that similarly to KNN, similar representations should have similar predictions (supposedly). However, it's not the case.\nHow is it possible? The model (e.g. the Embedding matrix) is identical, and the representation are similar.\n1/"", '@mariusmosbach @aclmeeting How is it possible? The model (e.g. the Embedding matrix) is identical, and the representation are similar.\nBut similar by what measure? -&gt; the full embedding space.\nDistances between vectors do not correspond to the difference in the behavior of the model over those vectors 2/', '@mariusmosbach @aclmeeting I hope this is clear.\nI can also give a concrete demonstrating example, but maybe in a non-character limiting platform. 3/3']",https://arxiv.org/abs/2102.01017,"Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness. ",Measuring and Improving Consistency in Pretrained Language Models
131,1356522163520569345,21902101,Jim Geach,"['New paper on arXiv this morning - my student Kushatha\'s first journal article ""Rapid sorting of radio galaxy morphology using Haralick features"" Ntwaetsile &amp; Geach: <LINK> accepted in MNRAS']",https://arxiv.org/abs/2102.00843,"We demonstrate the use of Haralick features for the automated classification of radio galaxies. The set of thirteen Haralick features represent an extremely compact non-parametric representation of image texture, and are calculated directly from imagery using the Grey Level Co-occurrence Matrix (GLCM). The GLCM is an encoding of the relationship between the intensity of neighbouring pixels in an image. Using 10,000 sources detected in the first data release of the LOFAR Two-metre Sky Survey (LoTSS), we demonstrate that Haralick features are highly efficient, rotationally invariant descriptors of radio galaxy morphology. After calculating Haralick features for LoTSS sources, we employ the fast density-based hierarchical clustering algorithm HDBSCAN to group radio sources into a sequence of morphological classes, illustrating a simple methodology to classify and label new, unseen galaxies in large samples. By adopting a 'soft' clustering approach, we can assign each galaxy a probability of belonging to a given cluster, allowing for more flexibility in the selection of galaxies according to combinations of morphological characteristics and for easily identifying outliers: those objects with a low probability of belonging to any cluster in the Haralick space. Although our demonstration focuses on radio galaxies, Haralick features can be calculated for any image, making this approach also relevant to large optical imaging galaxy surveys. ",Rapid sorting of radio galaxy morphology using Haralick features
132,1356432367586533378,549460404,ÂêâÁî∞ Á¥Ö (Beni Yoshida),"['A new paper is out. \n""Many-body quantum teleportation""\n<LINK>', 'One of our (many!) results is that the traversable wormhole protocol is actually a special case of the decoding protocol I came up with Kitaev.', ""(Let me add that I didn't make substantial contribution to this project except providing some rigorous bounds etc... )"", ""And don't forget to check out the paper from google group as well. (We coordinated the submission date.)\nhttps://t.co/78OdrgsFSx""]",https://arxiv.org/abs/2102.00010,"By leveraging shared entanglement between a pair of qubits, one can teleport a quantum state from one particle to another. Recent advances have uncovered an intrinsically many-body generalization of quantum teleportation, with an elegant and surprising connection to gravity. In particular, the teleportation of quantum information relies on many-body dynamics, which originate from strongly-interacting systems that are holographically dual to gravity; from the gravitational perspective, such quantum teleportation can be understood as the transmission of information through a traversable wormhole. Here, we propose and analyze a new mechanism for many-body quantum teleportation -- dubbed peaked-size teleportation. Intriguingly, peaked-size teleportation utilizes precisely the same type of quantum circuit as traversable wormhole teleportation, yet has a completely distinct microscopic origin: it relies upon the spreading of local operators under generic thermalizing dynamics and not gravitational physics. We demonstrate the ubiquity of peaked-size teleportation, both analytically and numerically, across a diverse landscape of physical systems, including random unitary circuits, the Sachdev-Ye-Kitaev model (at high temperatures), one-dimensional spin chains and a bulk theory of gravity with stringy corrections. Our results pave the way towards using many-body quantum teleportation as a powerful experimental tool for: (i) characterizing the size distributions of operators in strongly-correlated systems and (ii) distinguishing between generic and intrinsically gravitational scrambling dynamics. To this end, we provide a detailed experimental blueprint for realizing many-body quantum teleportation in both trapped ions and Rydberg atom arrays; effects of decoherence and experimental imperfections are analyzed. ","Many-body quantum teleportation via operator spreading in the
  traversable wormhole protocol"
133,1356420877114572802,1092114266952531968,Graeme Addison,"['My new paper looking at Hubble constant constraints from CMB E-mode data sets: <LINK>\n\nRecently, the @SPTelescope paper Dutcher+ showed that Planck, ACTPol, SPTpol &amp; SPT-3G all get higher H0 in EE than the Planck TT LCDM constraint (see their Fig 13). [1/6] <LINK>', 'What happens if we *combine* the different EE spectra? Does this reinforce the preference for a higher H0? If yes, perhaps it could be some clue that to resolve the Hubble tension we want a model that departs from LCDM more strongly in temperature than polarization. [2/6]', 'I ran the fit‚Ä¶ and found that combining Planck EE + ACTPol EE + SPTpol EE actually gives 68.7 +/- 1.3 km/s/Mpc, 2.4 sigma lower than the latest SH0ES distance ladder (73.2 +/- 1.3).\n\nSo how can you combine three values that are all &gt;=70 and get 68.7? [3/6]', 'The answer lies in the different degeneracy directions across the full LCDM param space (look at n_s vs Obh2), related to sensitivity to different multipole ranges. To reach a consensus on Obh2 between Planck and ACTPol/SPTpol you end up shifting lower in H0, n_s. [4/6] https://t.co/m620SQwkiN', 'Are the constraints from different EE data sets consistent with one another? Yes - difference at most 1.4 sigma across the LCDM space (see Table 2). Also consistent with Planck TT LCDM at 0.8 sigma. [5/6] https://t.co/fOeSK26c9b', 'NB - The SPT-3G likelihood isn‚Äôt public yet, but since the degeneracy directions are going to be pretty similar between SPTpol and SPT-3G EE I expect that combining Planck EE + SPT-3G EE will similarly lead to a lower H0 / lower n_s. [6/6]']",https://arxiv.org/abs/2102.00028,"The E-mode (EE) CMB power spectra measured by Planck, ACTPol, and SPTpol constrain the Hubble constant to be $70.0\pm2.7$, $72.4^{+3.9}_{-4.8}$, and $73.1^{+3.3}_{-3.9}$ km s$^{-1}$ Mpc$^{-1}$ within the standard $\Lambda$CDM model (posterior mean and central 68% interval bounds). These values are higher than the constraints from the Planck temperature (TT) power spectrum, and consistent with the Cepheid-supernova distance ladder measurement $H_0=73.2\pm1.3$ km s$^{-1}$ Mpc$^{-1}$. If this preference for a higher value was strengthened in a joint analysis it could provide an intriguing hint at the resolution of the Hubble disagreement. We show, however, that combining the Planck, ACTPol, and SPTpol EE likelihoods yields $H_0=68.7\pm1.3$ km s$^{-1}$ Mpc$^{-1}$, $2.4\sigma$ lower than the distance ladder measurement. This is due to different degeneracy directions across the full parameter space, particularly involving the baryon density, $\Omega_bh^2$, and scalar tilt, $n_s$, arising from sensitivity to different multipole ranges. We show that the E-mode $\Lambda$CDM constraints are consistent across the different experiments within $1.4\sigma$, and with the Planck TT results at $0.8\sigma$. Combining the Planck, ACTPol, and SPTpol EE data constrains the phenomenological lensing amplitude, $A_L=0.89\pm0.10$, consistent with the expected value of unity. ","High $H_0$ Values from CMB E-mode Data: A Clue for Resolving the Hubble
  Tension?"
134,1370703967437557767,1339309323839700992,Shuaiwen Leon Song,['Our paper on system-level design for future virtual reality systems will appear at @ASPLOSConf. This work offers a new class of intelligent mobile/cloud collaborative systems that enable future high-quality low-latency planet-scale VR experiences. @drmbt \n\n<LINK>'],https://arxiv.org/abs/2102.13191,"High Quality Mobile Virtual Reality (VR) is what the incoming graphics technology era demands: users around the world, regardless of their hardware and network conditions, can all enjoy the immersive virtual experience. However, the state-of-the-art software-based mobile VR designs cannot fully satisfy the realtime performance requirements due to the highly interactive nature of user's actions and complex environmental constraints during VR execution. Inspired by the unique human visual system effects and the strong correlation between VR motion features and realtime hardware-level information, we propose Q-VR, a novel dynamic collaborative rendering solution via software-hardware co-design for enabling future low-latency high-quality mobile VR. At software-level, Q-VR provides flexible high-level tuning interface to reduce network latency while maintaining user perception. At hardware-level, Q-VR accommodates a wide spectrum of hardware and network conditions across users by effectively leveraging the computing capability of the increasingly powerful VR hardware. Extensive evaluation on real-world games demonstrates that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to 6.7x) over the traditional local rendering design in commercial VR devices, and a 4.1x frame rate improvement over the state-of-the-art static collaborative rendering. ","Q-VR: System-Level Design for Future Mobile Collaborative Virtual
  Reality"
135,1366699271144284160,2849656667,Granqvist,"['I am excited to announce that our new paper ""Federated Evaluation and tuning for on-device\npersonalization: system design &amp; applications""  is now available on arXiv <LINK> ü§†']",https://arxiv.org/abs/2102.08503,"We describe the design of our federated task processing system. Originally, the system was created to support two specific federated tasks: evaluation and tuning of on-device ML systems, primarily for the purpose of personalizing these systems. In recent years, support for an additional federated task has been added: federated learning (FL) of deep neural networks. To our knowledge, only one other system has been described in literature that supports FL at scale. We include comparisons to that system to help discuss design decisions and attached trade-offs. Finally, we describe two specific large scale personalization use cases in detail to showcase the applicability of federated tuning to on-device personalization and to highlight application specific solutions. ","Federated Evaluation and Tuning for On-Device Personalization: System
  Design & Applications"
136,1365259949585145859,22604662,Florian Tschorsch,"['Our paper ‚ÄúIPFS and Friends‚Äù is now available as preprint. We believe that due to rather recent advancements a new generation of P2P data networks emerges: <LINK> <LINK>', 'In the paper, we extract the building blocks of this new generation of data networks and discuss their similarities and challenges. In particular, we cover @IPFS, @ethswarm, @HypercoreProto, @safenetworktech, @storjproject, and @ArweaveTeam https://t.co/SU2UOC5mq0', 'If you have any remarks, please feel to get in touch. The paper is currently under review and we still have room for minor revisions.', '@paddypisa We briefly cover it in the ""honorable mentions""']",https://arxiv.org/abs/2102.12737,"Decentralized, distributed storage offers a way to reduce the impact of data silos as often fostered by centralized cloud storage. While the intentions of this trend are not new, the topic gained traction due to technological advancements, most notably blockchain networks. As a consequence, we observe that a new generation of peer-to-peer data networks emerges. In this survey paper, we therefore provide a technical overview of the next generation data networks. We use select data networks to introduce general concepts and to emphasize new developments. Specifically, we provide a deeper outline of the Interplanetary File System and a general overview of Swarm, the Hypercore Protocol, SAFE, Storj, and Arweave. We identify common building blocks and provide a qualitative comparison. From the overview, we derive future challenges and research goals concerning data networks. ","IPFS and Friends: A Qualitative Comparison of Next Generation
  Peer-to-Peer Data Networks"
137,1364988920249815045,247800333,Ahmad ÿ∑Ÿá,"['New paper on model order reduction (MOR) for water quality dynamics with @ShenWang9, @ahmedabokifa, Lina, and Ankush. \n\n<LINK>\n\nEver wondered whether MOR algorithms (that reduce state-dimension by orders of magnitude) work for large-scale networks?', 'Probably not but hey you can always learn something new. :p\n\nAnyway, we study how large-scale water quality models (with ~ 30,000+ states) can be reduced to hundreds of states via MOR algorithms.', ""Do mainstream MOR algorithms work? Some do, but they're intractable or intolerant to some initial conditions. We fix that through simple and extremely efficient algorithms."", ""Which makes me think:\n\nYou should never run MPC (model predictive control) for a large-scale system with fast sampling and time-constant. You're wasting your time and energy with applying MPC considering the full order model. Use MOR-based MPC instead."", ""The paper also acts as a good summary for the mainstream MOR algorithms, in addition to some new theoretical developments that are specific for water quality control modeling.\n\nAs always, the codes are Github.\n\nAnd the paper is under review as of yesterday (if you're curious ;p).""]",https://arxiv.org/abs/2102.10737,"A state-space representation of water quality dynamics describing disinfectant (e.g., chlorine) transport dynamics in drinking water distribution networks has been recently proposed. Such representation is a byproduct of space- and time-discretization of the PDE modeling transport dynamics. This results in a large state-space dimension even for small networks with tens of nodes. Although such a state-space model provides a model-driven approach to predict water quality dynamics, incorporating it into model-based control algorithms or state estimators for large networks is challenging and at times intractable. To that end, this paper investigates model order reduction (MOR) methods for water quality dynamics with the objective of performing post-reduction feedback control. The presented investigation focuses on reducing state-dimension by orders of magnitude, the stability of the MOR methods, and the application of these methods to model predictive control. ",Model Order Reduction for Water Quality Dynamics
138,1364655410523738112,3279117936,Denis Yarats üá∫üá¶,"['Happy to share my new work -- Proto-RL, a task-agnostic pre-training scheme that reconciles exploration and representation learning in image-based RL!\n\nwith:  @rob_fergus, Alessandro Lazaric, and @LerrelPinto.\npaper: <LINK>\ncode:  <LINK>\n\n[1/N] <LINK>', 'In practice, learning useful representations requires a diverse dataset, but on the other hand, collecting a diverse dataset requires an efficient exploration policy, which needs to distinguish new states from already visited.  \n\nThis leads to an üêî and ü•ö problem!\n\n[2/N]', 'To address this problem, we propose a self-supervised framework that both learns latent embeddings of visual observations and a set of prototypes that define a basis in the embedding space. Then, we use these prototypes to devise an intrinsic max-ent reward.\n\n[3/N] https://t.co/2ukfNYXi0e', 'During the task-agnostic pre-training stage Proto-RL unsupervisely explores the state space in pixels RL and learn task-agnostic representations and prototypes. Empirically, Proto-RL is able to learn a exploration policy with uniform state-visitation distribution:\n\n[4/N] https://t.co/1GTQpxpUBb', 'During the downstream phase, Proto-RL freezes the encoder and prototypes, which provide generalizable representations and faster downstream exploration.\nWe then train a standard state-based RL method (e.g. SAC) to solve an image-based downstream task.\n\n[5/N] https://t.co/pqb9Rn3Iww', 'We scale Proto-RL on hard tasks, both with sparse and dense rewards, from dm_control and set SOTA results on task-agnostic pre-training and fast downstream learning:\n\n[6/N] https://t.co/b5BkpMZh7L', 'Proto-RL also allows efficient multi-task generalization after initial task-agnostic pre-training:\n\n[7/N] https://t.co/UudIcnfZX6', 'We ablate Proto-RL and observe that our method can efficiently explore the environment with around 500k steps:\n\n[8/N] https://t.co/3yIv1ybb0y', 'Finally, we demonstrate the importance of having prototypes to speed up downstream exploration:\n\n[9/N] https://t.co/zPnPcBnGD9']",https://arxiv.org/abs/2102.11271,"Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks. ",Reinforcement Learning with Prototypical Representations
139,1364480243482861571,1576235694,Michael Brown,"['Madhooshi Senarath has a paper on tracking down low redshift changing look AGNs, using both colours and flux variability to ID candidates. A huge body of work to pull together archival data, classifications and new WiFeS spectra.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2102.07351,"We have conducted a systematic survey for z $<$ 0.04 active Galactic nuclei (AGNs) that may have changed spectral class over the past decade. We use SkyMapper, Pan-STARRS and the V\'eron-Cetty & V\'eron (2010) catalogue to search the entire sky for these ``changing-look'' AGNs using a variety of selection methods, where Pan-STARRS has a coverage of 3$\pi$ steradians (sky north of Declination $-30^\circ$) and SkyMapper has coverage of $\sim$ 21,000$~\rm{deg^2}$ (sky south of Declination $0^\circ$). We use small aperture photometry to measure how colour and flux have changed over time, where a change may indicate a change in spectral type. Optical colour and flux are used as a proxy for changing H$\alpha$ equivalent width, while WISE 3.4 $\mu$m flux is used to look for changes in the hot dust component. We have identified four AGNs with varying spectra selected using our optical colour selection method. Three AGNs were confirmed from recent observations with WiFeS on the 2.3 m telescope at Siding Spring and the other was identified from archival spectra alone. From this, we identify two new changing look AGNs; NGC 1346 and 2MASX J20075129-1108346. We also recover Mrk 915 and Mrk 609, which are known to have varying spectra in the literature, but they do not meet our specific criteria for changing look AGNs. ",A Systematic Survey for z &lt; 0.04 Changing-Look AGNs
140,1364047104931287040,1045482385943359488,Ruby Wright,"['We have a new paper pre-print on the arXiv today! The work focuses on the properties of inter-galactic gas accreting to haloes in the EAGLE simulations. \n\n<LINK>', 'with @CDPLagos, @doctorcbpower and @_astrocamila', '@ARC_ASTRO3D @ICRAR']",https://arxiv.org/abs/2102.10913,"The inflow of cosmological gas onto haloes, while challenging to directly observe and quantify, plays a fundamental role in the baryon cycle of galaxies. Using the EAGLE suite of hydrodynamical simulations, we present a thorough exploration of the physical properties of gas accreting onto haloes -- namely, its spatial characteristics, density, temperature, and metallicity. Classifying accretion as ``hot'' or `` cold'' based on a temperature cut of $10^{5.5}{\rm K}$, we find that the covering fraction ($f_{\rm cov}$) of cold-mode accreting gas is significantly lower than the hot-mode, with $z=0$ $f_{\rm cov}$ values of $\approx 50\%$ and $\approx 80\%$ respectively. Active Galactic Nuclei (AGN) feedback in EAGLE reduces inflow $f_{\rm cov}$ values by $\approx 10\%$, with outflows decreasing the solid angle available for accretion flows. Classifying inflow by particle history, we find that gas on first-infall onto a halo is metal-depleted by $\approx 2$~dex compared to pre-processed gas, which we find to mimic the circum-galactic medium (CGM) in terms of metal content. We also show that high (low) halo-scale gas accretion rates are associated with metal-poor (rich) CGM in haloes below $10^{12}M_{\odot}$, and that variation in halo-scale gas accretion rates may offer a physical explanation for the enhanced scatter in the star-forming main sequence at low ($\lesssim10^{9}M_{\odot}$) and high ($\gtrsim10^{10}M_{\odot}$) stellar masses. Our results highlight how gas inflow influences several halo- and galaxy-scale properties, and the need to combine kinematic and chemical data in order to confidently break the degeneracy between accreting and outgoing gas in CGM observations. ","Revealing the physical properties of gas accreting to haloes in the
  EAGLE simulations"
141,1363171994565509121,1004816650628227073,C√©sar A. Uribe,"['üö®üö®üö® New paper alert! The first ever official paper from my Lab is out!ü¶æü¶æü•≥  @RiceECE @RiceEngineering Amazing work of my student @mttoghani. We show non-asymptotic concentration bounds for social learning with very efficient communication of beliefs. <LINK>', '@AnaMaPorras @RiceECE @RiceEngineering @mttoghani Gracias! Siempre tenemos esa conversaci√≥n con @SandraAriassu sobre lo diferentes que son las escalas de tiempo en nuestras √°reas.']",https://arxiv.org/abs/2102.07767,"We study the problem of distributed cooperative learning, where a group of agents seeks to agree on a set of hypotheses that best describes a sequence of private observations. In the scenario where the set of hypotheses is large, we propose a belief update rule where agents share compressed (either sparse or quantized) beliefs with an arbitrary positive compression rate. Our algorithm leverages a unified communication rule that enables agents to access wide-ranging compression operators as black-box modules. We prove the almost sure asymptotic exponential convergence of beliefs around the set of optimal hypotheses. Additionally, we show a non-asymptotic, explicit, and linear concentration rate in probability of the beliefs on the optimal hypothesis set. We provide numerical experiments to illustrate the communication benefits of our method. The simulation results show that the number of transmitted bits can be reduced to 5-10% of the non-compressed method in the studied scenarios. ","Communication-efficient Distributed Cooperative Learning with Compressed
  Beliefs"
142,1362013880818499585,460489687,Juan Mateos Garcia,['Nice to see our new working paper about AI researcher career transitions from academia to industry in @stateofaireport. \n\n<LINK> \n\nAbstract: <LINK>'],https://arxiv.org/abs/2102.01648,"The private sector is playing an increasingly important role in basic Artificial Intelligence (AI) R&D. This phenomenon, which is reflected in the perception of a brain drain of researchers from academia to industry, is raising concerns about a privatisation of AI research which could constrain its societal benefits. We contribute to the evidence base by quantifying transition flows between industry and academia and studying its drivers and potential consequences. We find a growing net flow of researchers from academia to industry, particularly from elite institutions into technology companies such as Google, Microsoft and Facebook. Our survival regression analysis reveals that researchers working in the field of deep learning as well as those with higher average impact are more likely to transition into industry. A difference-in-differences analysis of the effect of switching into industry on a researcher's influence proxied by citations indicates that an initial increase in impact declines as researchers spend more time in industry. This points at a privatisation of AI knowledge compared to a counterfactual where those high-impact researchers had remained in academia. Our findings highlight the importance of strengthening the public AI research sphere in order to ensure that the future of this powerful technology is not dominated by private interests. ","The Privatization of AI Research(-ers): Causes and Potential
  Consequences -- From university-industry interaction to public research
  brain-drain?"
143,1360309433910235136,2492016278,Adrian Raftery,"['New paper ""The vote Package: Single Transferable Vote and Other Electoral Systems in R"" w @hanasbc &amp; @silverman_b : <LINK> New in STV: equal preferences, seats for subgroups. Also plurality, runoff, score, approval, Condorcet methods.  <LINK>']",https://arxiv.org/abs/2102.05801,"We describe the vote package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval and single transferable vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, the STV is also called instant runoff voting (IRV), ranked choice voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples. ","The vote Package: Single Transferable Vote and Other Electoral Systems
  in R"
144,1360197339953045510,4018882938,Marcus Lower,"[""New paper alert! The census paper describing the relativistic binary program on the #MeerKAT @SKA_telescope precursor: <LINK> was (finally) accepted! \n\nIncludes a sneak preview of one of the most exciting experiments I've been involved in to date. <LINK>""]",https://arxiv.org/abs/2102.05160,"We describe the ongoing Relativistic Binary programme (RelBin), a part of the MeerTime large survey project with the MeerKAT radio telescope. RelBin is primarily focused on observations of relativistic effects in binary pulsars to enable measurements of neutron star masses and tests of theories of gravity. We selected 25 pulsars as an initial high priority list of targets based on their characteristics and observational history with other telescopes. In this paper, we provide an outline of the programme, present polarisation calibrated pulse profiles for all selected pulsars as a reference catalogue along with updated dispersion measures. We report Faraday rotation measures for 24 pulsars, twelve of which have been measured for the first time. More than a third of our selected pulsars show a flat position angle swing confirming earlier observations. We demonstrate the ability of the Rotating Vector Model (RVM), fitted here to seven binary pulsars, including the Double Pulsar (PSR J0737$-$3039A), to obtain information about the orbital inclination angle. We present a high time resolution light curve of the eclipse of PSR J0737$-$3039A by the companion's magnetosphere, a high-phase resolution position angle swing for PSR J1141$-$6545, an improved detection of the Shapiro delay of PSR J1811$-$2405, and pulse scattering measurements for PSRs J1227$-$6208, J1757$-$1854, and J1811$-$1736. Finally, we demonstrate that timing observations with MeerKAT improve on existing data sets by a factor of, typically, 2-3, sometimes by an order of magnitude. ","The Relativistic Binary Programme on MeerKAT: Science objectives and
  first results"
145,1359890227381366787,45872583,Fran Chadha-Day,['New paper up today - using condensed matter axions to detect fundamental axions: \n<LINK>'],https://arxiv.org/abs/2102.05366,"It has been suggested that certain antiferromagnetic topological insulators contain axion quasiparticles (AQs), and that such materials could be used to detect axion dark matter (DM). The AQ is a longitudinal antiferromagnetic spin fluctuation coupled to the electromagnetic Chern-Simons term, which, in the presence of an applied magnetic field, leads to mass mixing between the AQ and the electric field. The electromagnetic boundary conditions and transmission and reflection coefficients are computed. A model for including losses into this system is presented, and the resulting linewidth is computed. It is shown how transmission spectroscopy can be used to measure the resonant frequencies and damping coefficients of the material, and demonstrate conclusively the existence of the AQ. The dispersion relation and boundary conditions permit resonant conversion of axion DM into THz photons in a material volume that is independent of the resonant frequency, which is tuneable via an applied magnetic field. A parameter study for axion DM detection is performed, computing boost amplitudes and bandwidths using realistic material properties including loss. The proposal could allow for detection of axion DM in the mass range between 1 and 10 meV using current and near future technology. ",Axion Quasiparticles for Axion Dark Matter Detection
146,1359446823035949058,1541749356,Matt Landreman,"['New paper shows the analytic derivative of magnetic island width with respect to electromagnetic coil shapes. Useful for optimization &amp; coil tolerances for #fusion. By Alessandro Geraldini @epfl, me @UofMaryland &amp; Elizabeth Paul @Princeton <LINK> <LINK>']",https://arxiv.org/abs/2102.04497,"An adjoint method to calculate the gradient of island width in stellarators is presented and applied to a set of magnetic field configurations. The underlying method of calculation of the island width is that of Cary & Hanson (1991) (with a minor modification), and requires that the residue of the island centre be small. Therefore, the gradient of the residue is calculated in addition. Both the island width and the gradient calculations are verified using an analytical magnetic field configuration introduced in Reiman & Greenside (1986). The method is also applied to the calculation of the shape gradient of the width of a magnetic island in an NCSX vacuum configuration with respect to positions on a coil. A gradient-based optimization is applied to a magnetic field configuration studied in Hanson & Cary (1984) to minimize stochasticity by adding perturbations to a pair of helical coils. Although only vacuum magnetic fields and an analytical magnetic field model are considered in this work, the adjoint calculation of the island width gradient could also be applied to a magnetohydrodynamic (MHD) equilibrium if the derivative of the magnetic field with respect to the equilibrium parameters was known. Using the island width gradient calculation presented here, more general gradient-based optimization methods can be applied to design stellarators with small magnetic islands. Moreoever, the sensitivity of the island size may itself be optimized to ensure that coil tolerances with respect to island size are kept as high as possible. ","An adjoint method for determining the sensitivity of island size to
  magnetic field variations"
147,1359334053061615616,70614241,Dr Fiona H. Panther,"[""Early warning of neutron star mergers for multimessenger observatories? Its more likely than you think!\n\nNew paper on SPIIR and GstLAL's performance test for sending early warning alerts on arXiv today:\n\n<LINK>""]",https://arxiv.org/abs/2102.04555,"Gravitational-wave observations became commonplace in Advanced LIGO-Virgo's recently concluded third observing run. 56 non-retracted candidates were identified and publicly announced in near real time. Gravitational waves from binary neutron star mergers, however, remain of special interest since they can be precursors to high-energy astrophysical phenomena like $\gamma$-ray bursts and kilonovae. While late-time electromagnetic emissions provide important information about the astrophysical processes within, the prompt emission along with gravitational waves uniquely reveals the extreme matter and gravity during - and in the seconds following - merger. Rapid communication of source location and properties from the gravitational-wave data is crucial to facilitate multi-messenger follow-up of such sources. This is especially enabled if the partner facilities are forewarned via an early-warning (pre-merger) alert. Here we describe the commissioning and performance of such a low-latency infrastructure within LIGO-Virgo. We present results from an end-to-end mock data challenge that detects binary neutron star mergers and alerts partner facilities before merger. We set expectations for these alerts in future observing runs. ",First demonstration of early warning gravitational wave alerts
148,1359301186734620675,2162872302,Mingxing Tan,"['Nystromformer: a new linear self-attention.\n\nIt turns out a simple Nystr√∂m method is quite effective in approximating the full attention, outperforming reformer/linformer/performer by +3% accuracy on LRA. @YoungXiong1\n\nPaper: <LINK>\nCode: <LINK> <LINK> <LINK>']",https://arxiv.org/abs/2102.03902,"Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr\""{o}mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\""{o}m method to approximate standard self-attention with $O(n)$ complexity. The scalability of Nystr\""{o}mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\""{o}mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\""{o}mformer performs favorably relative to other efficient self-attention methods. Our code is available at this https URL ","Nystr\""omformer: A Nystr\""om-Based Algorithm for Approximating
  Self-Attention"
149,1359200194529755138,861775999,Laura A. Hayes,['New co-authored paper out today on @arxiv led by @Brendan_PClarke at @DIASAstronomy identifying signatures of quasi-periodic particle acceleration in a solar flare - from low-frequency radio to hard X-ray <LINK> üåû <LINK>'],https://arxiv.org/abs/2102.04267,"A common feature of electromagnetic emission from solar flares is the presence of intensity pulsations that vary as a function of time. Known as quasi-periodic pulsations (QPPs), these variations in flux appear to include periodic components and characteristic time-scales. Here, we analyse a GOES M3.7 class flare exhibiting pronounced QPPs across a broad band of wavelengths using imaging and time-series analysis. We identify QPPs in the timeseries of X-ray, low frequency radio and EUV wavelengths using wavelet analysis, and localise the region of the flare site from which the QPPs originate via X-ray and EUV imaging. It was found that the pulsations within the 171 \.A, 1600 \.A, soft X-ray (SXR), and hard X-ray (HXR) light curves yielded similar periods of $\sim$122 s, $\sim$131s, $\sim$123 s, and $\sim$137 s, respectively, indicating a common progenitor. The low frequency radio emission at 2.5 MHz contained a longer period of $\sim$231 s. Imaging analysis indicates that the location of the X-ray and EUV pulsations originates from a HXR footpoint linked to a system of nearby open magnetic field lines. Our results suggest that intermittent particle acceleration, likely due to 'bursty' magnetic reconnection, is responsible for the QPPs. The precipitating electrons accelerated towards the chromosphere produce the X-ray and EUV pulsations, while the escaping electrons result in low frequency radio pulses in the form of type III radio bursts. The modulation of the reconnection process, resulting in episodic particle acceleration, explains the presence of these QPPs across the entire spatial range of flaring emission. ",Quasi-Periodic Particle Acceleration in a Solar Flare
150,1359139033553776646,1002014071,Frank Wilczek,['New paper using color erasure on a grand scale to achieve remarkable resolving power: <LINK> . <LINK>'],https://arxiv.org/abs/2102.02060,"Interferometers are widely used in imaging technologies to achieve enhanced spatial resolution, but require that the incoming photons be indistinguishable. In previous work, we built and analyzed color erasure detectors which expand the scope of intensity interferometry to accommodate sources of different colors. Here we experimentally demonstrate how color erasure detectors can achieve improved spatial resolution in an imaging task, well beyond the diffraction limit. Utilizing two 10.9 mm-aperture telescopes and a 0.8 m baseline, we measure the distance between a 1063.6 nm source and a 1064.4 nm source separated by 4.2 mm at a distance of 1.43 km, which surpasses the diffraction limit of a single telescope by about 40 times. Moreover, chromatic intensity interferometry allows us to recover the phase of the Fourier transform of the imaged objects - a quantity that is, in the presence of modest noise, inaccessible to conventional intensity interferometry. ","Improved Spatial Resolution Achieved by Chromatic Intensity
  Interferometry"
151,1359133014526205952,597726901,Hannah Rose Kirk,"['Is #AI  generated text biased towards protected groups? In a new  @OxfordAI paper, we look at occupational biases for gender intersectionality with religion, sexuality, political affiliation...Why does #GPT-2 think women are maids not computer programmers?\n<LINK> <LINK>', 'Shout out to my co-authors @YennieJun, @y_m_asano, @EliasBenussi, @shaideriqbal, @frdreyer, Aleksander Shtedritski and Filippo Volpin! üåü']",https://arxiv.org/abs/2102.04130,"The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities. ","Bias Out-of-the-Box: An Empirical Analysis of Intersectional
  Occupational Biases in Popular Generative Language Models"
152,1358763306895486985,1277975431723900936,Laura Rogers,"['New co-authored paper on arXiv today! \nWe find evidence that exoplanetary bodies have the same refractory composition as their host star\n<LINK>', 'Some white dwarfs show evidence that they have accreted planetary material. Analysing their spectra reveals the bulk composition of the exoplanetary material that they have accreted.', 'We analyse the abundances of a wide binary system consisting of a K-dwarf and a white dwarf which has accreted planetary material. As binary pairs are chemically homogeneous, the K-dwarf is used as a proxy for the progenitor composition of the white dwarf.', 'The abundances of the K-dwarf and the planetary material polluting the atmosphere of the white dwarf are consistent with the hypothesis that exoplanetary bodies have the same refractory composition as their host stars.', 'Check out the paper for more detail, discussions and caveats', '@MarcoMarco1822 Potentially! Although planet formation is very complicated. It may help to target certain stars when looking for potential exoplanets where life as we know it could exist!', '@MarcoMarco1822 We can use observations of polluted white dwarfs to infer if there was a core/mantle or even water sometimes, however, these planets have been destroyed so this makes it hard to find life there!']",https://arxiv.org/abs/2102.02843,"Planets and stars ultimately form out of the collapse of the same cloud of gas. Whilst planets, and planetary bodies, readily loose volatiles, a common hypothesis is that they retain the same refractory composition as their host star. This is true within the Solar System. The refractory composition of chondritic meteorites, Earth and other rocky planetary bodies are consistent with solar, within the observational errors. This work aims to investigate whether this hypothesis holds for exoplanetary systems. If true, the internal structure of observed rocky exoplanets can be better constrained using their host star abundances. In this paper, we analyse the abundances of the K-dwarf, G200-40, and compare them to its polluted white dwarf companion, WD 1425+540. The white dwarf has accreted planetary material, most probably a Kuiper belt-like object, from an outer planetary system surviving the star's evolution to the white dwarf phase. Given that binary pairs are chemically homogeneous, we use the binary companion, G200-40, as a proxy for the composition of the progenitor to WD 1425+540. We show that the elemental abundances of the companion star and the planetary material accreted by WD 1425+540 are consistent with the hypothesis that planet and host-stars have the same true abundances, taking into account the observational errors. ","Host-star and exoplanet compositions: a pilot study usinga wide binary
  with a polluted white dwarf"
153,1357611002548736001,4666231375,Konstantin Batygin,"['In a new paper led by Walker Melton @wmelton12 (formerly at @Caltech, now at @Harvard), we explore tantalizing analogies between waves in astrophysical disks and quantum mechanics. Paper up on #arxiv tonight (<LINK>) and brief thread below. <LINK>', 'Disks of particles that encircle big objects (debris around planets, planetesimals around stars, stars around black holes, etc) are common in the universe. One of the basic things such disks can do is warp. What does that look like? In other words, what are the ""normal modes""?', 'One way to answer this question is to imagine that the disk is composed of an infinite number of wires that interact gravitationally. Fundamentally, this approach dates back to the work of Lagrange (1778) and is the basis for secular perturbation theory of celestial mechanics. https://t.co/LypEJ111CP', ""If we limit ourselves to only near-neighbor interactions among the wires, this purely classical calculation boils down to Schrodinger's equation, and the solution gives a clear prediction for the disk's normal modes: they go like cosines of the logarithm of the orbital radius."", 'Now, this picture has a simple drawback: gravity extends well beyond nearest neighbors. Still, through a clever set of calculations, Walker showed that if the disk is relatively narrow, this local coupling model gives a great approximation for the normal modes of the system. https://t.co/UkeXCYeMjV', 'The paper outlines a bunch of other cool analogies with quantum mechanics including the existence of a conserved inner product (which corresponds to the total angular momentum), Wick rotations, etc -- check out the draft for more details.']",https://arxiv.org/abs/2102.02312,"Although quasi-Keplerian discs are among the most common astrophysical structures, computation of secular angular momentum transport within them routinely presents a considerable practical challenge. In this work, we investigate the secular small-inclination dynamics of a razor-thin particle disc as the continuum limit of a discrete Lagrange-Laplace secular perturbative theory and explore the analogy between the ensuing secular evolution -- including non-local couplings of self-gravitating discs -- and quantum mechanics. We find the 'quantum' Hamiltonian that describes the time evolution of the system and demonstrate the existence of a conserved inner product. The lowest-frequency normal modes are numerically approximated by performing a Wick rotation on the equations of motion. These modes are used to quantify the accuracy of a much simpler local-coupling model, revealing that it predicts the shape of the normal modes to a high degree of accuracy, especially in narrow annuli, even though it fails to predict their eigenfrequencies. ",Eigenstates of Quasi-Keplerian Self-Gravitating Particle Discs
154,1356915866475069444,460489687,Juan Mateos Garcia,"['The privatisation of AI research(ers)\n\nIn a new working paper, we map career AI researcher career transitions between academia and industry, providing evidence about the scale, drivers and potential consequences of the AI brain drain. Deeper thread soon\n\n<LINK> <LINK>', '[Co authored w/ @Daniel_S_Hain, @kstathou &amp; @RJurowetzki ]', '@jermainkaminski That sounds like a great idea - our analysis of drivers and outcomes is quite simple right now. It would be very interesting to enhance it from a directional angle as you say. Maybe a good opportunity for collaboration?', '@gjrietveld @jermainkaminski Definitely - Thank you for flagging this up!', '@gjrietveld @jermainkaminski $30 for 3-day access it better be! üòÑ']",https://arxiv.org/abs/2102.01648,"The private sector is playing an increasingly important role in basic Artificial Intelligence (AI) R&D. This phenomenon, which is reflected in the perception of a brain drain of researchers from academia to industry, is raising concerns about a privatisation of AI research which could constrain its societal benefits. We contribute to the evidence base by quantifying transition flows between industry and academia and studying its drivers and potential consequences. We find a growing net flow of researchers from academia to industry, particularly from elite institutions into technology companies such as Google, Microsoft and Facebook. Our survival regression analysis reveals that researchers working in the field of deep learning as well as those with higher average impact are more likely to transition into industry. A difference-in-differences analysis of the effect of switching into industry on a researcher's influence proxied by citations indicates that an initial increase in impact declines as researchers spend more time in industry. This points at a privatisation of AI knowledge compared to a counterfactual where those high-impact researchers had remained in academia. Our findings highlight the importance of strengthening the public AI research sphere in order to ensure that the future of this powerful technology is not dominated by private interests. ","The Privatization of AI Research(-ers): Causes and Potential
  Consequences -- From university-industry interaction to public research
  brain-drain?"
155,1356789323241152513,2337598033,Geraint F. Lewis,"['New paper on @arxiv by my PhD students, @Zen_W_ and Will Oliver. We look at the globular cluster, NGC3201 and measure its stellar velocities, looking at how it has been influenced by the Galaxy.\n\nThis is Zhen‚Äôs final paper from his PhD.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2102.01472,"As part of a chemo-dynamical survey of five nearby globular clusters with 2dF/AAOmega on the AAT, we have obtained kinematic information for the globular cluster NGC3201. Our new observations confirm the presence of a significant velocity gradient across the cluster which can almost entirely be explained by the high proper motion of the cluster. After subtracting the contribution of this perspective rotation, we found a remaining rotation signal with an amplitude of $\sim1\ km/s$ around a different axis to what we expect from the tidal tails and the potential escapers, suggesting that this rotation is internal and can be a remnant of its formation process. At the outer part, we found a rotational signal that is likely a result from potential escapers. The proper motion dispersion at large radii reported by Bianchini et al. has previously been attributed to dark matter. Here we show that the LOS dispersion between 0.5-1 Jacobi radius is lower, yet above the predictions from an N-body model of NGC3201 that we ran for this study. Based on the simulation, we find that potential escapers cannot fully explain the observed velocity dispersion. We also estimate the effect on the velocity dispersion of different amounts of stellar-mass black holes and unbound stars from the tidal tails with varying escape rates and find that these effects cannot explain the difference between the LOS dispersion and the N-body model. Given the recent discovery of tidal tail stars at large distances from the cluster, a dark matter halo is an unlikely explanation. We show that the effect of binary stars, which is not included in the N-body model, is important and can explain part of the difference in dispersion. We speculate that the remaining difference must be the result of effects not included in the N-body model, such as initial cluster rotation, velocity anisotropy and Galactic substructure. ",The dynamics of the globular cluster NGC3201 out to the Jacobi radius
156,1356669509680066560,1177300366620184576,Xuhui,"['Our new work on debiasing toxic language detection systems @eacl2021 with @MaartenSap @swabhz @nlpnoah @YejinChoinka gives you a better understanding of challenges in automated debiasing those systems 1/4\n\nPaper: <LINK>', 'With many discovered (and potentially undiscovered) unwanted biases presenting in current toxic language detection systemsüòü, we apply bias-aware and bias-agnostic methods on debiasing those systems. 2/4\n\n‚ö†Ô∏è: offensive contents üëá', 'Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English).\nWe find that methods show debiasing effects on the in-distribution test sets but fail to debias on the out-of-distribution test sets.  3/4 https://t.co/2u8EvxUED3', 'We then propose an automatic, dialect-aware data correction method, as a proof-of-concept study. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. 4/4']",https://arxiv.org/abs/2102.00086,"Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases. ",Challenges in Automated Debiasing for Toxic Language Detection
157,1369040438699626500,265414308,Atƒ±lƒ±m G√ºne≈ü Baydin,"['New paper <LINK> where we propose a theoretically grounded method for domain-invariant representation learning by enforcing invariance under density transformations, with @a_tuan_nguyen Toan Tran @yaringal @OATML_Oxford <LINK>']",https://arxiv.org/abs/2102.05082,"Domain generalization refers to the problem where we aim to train a model on data from a set of source domains so that the model can generalize to unseen target domains. Naively training a model on the aggregate set of data (pooled from all source domains) has been shown to perform suboptimally, since the information learned by that model might be domain-specific and generalize imperfectly to target domains. To tackle this problem, a predominant approach is to find and learn some domain-invariant information in order to use it for the prediction task. In this paper, we propose a theoretically grounded method to learn a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions among domains. We also show how to use generative adversarial networks to learn such domain transformations to implement our method in practice. We demonstrate the effectiveness of our method on several widely used datasets for the domain generalization problem, on all of which we achieve competitive results with state-of-the-art models. ","Domain Invariant Representation Learning with Domain Density
  Transformations"
158,1367816153830199297,936197799766618112,Michael P. Sheehan,"['New preprint out: ""A Sketching Framework for Single Photon Counting Lidar"" with Juli√°n Tachella and Mike Davies - <LINK>. We propose a sketching method to compress the time-of-flight data without any significant loss of information. 1/8', 'In single photon counting lidar, the time-of-arrival of each detected photon is recorded in a histogram for every pixel in the scene. Peaks in the histogram indicate the presence of an object or surface.  See illustration below. 2/8 https://t.co/XNLLLVh6ZS', 'For every pixel in the scene, a histogram must be computed, stored in memory and transferred off-chip so that the location and intensity of the peaks can be estimated. All of which scales with the number of detected photons or the number of histogram bins ( depth resolution). 3/8', 'As technology of lidar arrays improves, the no. of photons and the depth resolution increases which generates a significant data transfer bottleneck. So far, most attempts to alleviate this bottleneck consist in building coarser histograms, which sacrifices depth resolution. 4/8', 'In our paper, we propose a sketching method that achieves massive compression of the time-of-flight data whilst retaining full depth resolution. The technique builds on recent advances in compressive learning by constructing a compressed statistic, or a so-called sketch, ... 5/8', 'of the time-of-arrival information. Significantly, the sketch has size that is both independent of the number of photons and the depth resolution. It can also be computed on-the-fly, i.e. updated with each photon arrival, bypassing the need to form the histogram.  6/8', 'Below you can see the large difference between compressing the time-of-arrival data using our proposed sketched lidar framework and the technique of coarsely binning the original histogram as the number of measurements increases. 7/8 https://t.co/ceyG7Tz8Ba', 'For more details check out our paper or visit https://t.co/L4IFKsxEtw for a more detailed post and links to the code. 8/8']",https://arxiv.org/abs/2102.08732,"Single-photon lidar has become a prominent tool for depth imaging in recent years. At the core of the technique, the depth of a target is measured by constructing a histogram of time delays between emitted light pulses and detected photon arrivals. A major data processing bottleneck arises on the device when either the number of photons per pixel is large or the resolution of the time stamp is fine, as both the space requirement and the complexity of the image reconstruction algorithms scale with these parameters. We solve this limiting bottleneck of existing lidar techniques by sampling the characteristic function of the time of flight (ToF) model to build a compressive statistic, a so-called sketch of the time delay distribution, which is sufficient to infer the spatial distance and intensity of the object. The size of the sketch scales with the degrees of freedom of the ToF model (number of objects) and not, fundamentally, with the number of photons or the time stamp resolution. Moreover, the sketch is highly amenable for on-chip online processing. We show theoretically that the loss of information for compression is controlled and the mean squared error of the inference quickly converges towards the optimal Cram\'er-Rao bound (i.e. no loss of information) for modest sketch sizes. The proposed compressed single-photon lidar framework is tested and evaluated on real life datasets of complex scenes where it is shown that a compression rate of up-to 150 is achievable in practice without sacrificing the overall resolution of the reconstructed image. ",A Sketching Framework for Reduced Data Transfer in Photon Counting Lidar
159,1366723676155092994,1366169431496343557,Matthew Whelan,"['How do Hippocampal Reverse Replays support Biological Reinforcement Learning? Can they be used in Robotic RL? With @EGVasilaki and @tonyjprescott, we implement a computational model of reverse replays in the biomimetic robot @CqRMiRo to find out. Preprint: <LINK> <LINK>']",https://arxiv.org/abs/2102.11914,"Hippocampal reverse replay is thought to contribute to learning, and particularly reinforcement learning, in animals. We present a computational model of learning in the hippocampus that builds on a previous model of the hippocampal-striatal network viewed as implementing a three-factor reinforcement learning rule. To augment this model with hippocampal reverse replay, a novel policy gradient learning rule is derived that associates place cell activity with responses in cells representing actions. This new model is evaluated using a simulated robot spatial navigation task inspired by the Morris water maze. Results show that reverse replay can accelerate learning from reinforcement, whilst improving stability and robustness over multiple trials. As implied by the neurobiological data, our study implies that reverse replay can make a significant positive contribution to reinforcement learning, although learning that is less efficient and less stable is possible in its absence. We conclude that reverse replay may enhance reinforcement learning in the mammalian hippocampal-striatal system rather than provide its core mechanism. ",A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning
160,1366346652798312448,2829399740,Venkat Kapil,"['Wondering how the interplay of physical effects influences polymorphic stabilities of molecular crystals ? In this study with Edgar Engel, we account for major physical effects using rigorous first principles free energies enabled by  #MachineLearning. <LINK>', 'We develop a machine learning potential (MLP) by sampling with DFTB, and calculating energetics at DFT level. Free energies at MLP level are estimated including quantum nuclear effects, thermal expansion, cell flexibility and anharmonicity, followed by a correction to DFT level. https://t.co/bpAsy3zC2f', 'We get correct stability rankings for polymorphs of benzene, glycine and succinic acid at DFT level, while machine learning predictions highlight the importance of including corrections to DFT level. https://t.co/9nx7ZO7aRq', 'And finally we are also able to assess the accuracy of commonly used ranking methods in crystal structure prediction studies. A big thank you to @lab_COSMO for support and feedback, and to @snsf_ch, @nccr_marvel and @cscsch for funding and computational resources. https://t.co/8qWzYI0Zco', 'Happy to get feedback from the community :)']",https://arxiv.org/abs/2102.13598,"Predictions of relative stabilities of (competing) molecular crystals are of great technological relevance, most notably for the pharmaceutical industry. However, they present a long-standing challenge for modeling, as often minuscule free energy differences are sensitively affected by the description of electronic structure, the statistical mechanics of the nuclei and the cell, and thermal expansion. The importance of these effects has been individually established, but rigorous free energy calculations for general molecular compounds, which simultaneously account for all effects,have hitherto not been computationally viable. Here we present an efficient ""end to end"" frame-work that seamlessly combines state-of-the art electronic structure calculations, machine-learning potentials, and advanced free energy methods to calculate ab initio Gibbs free energies for general organic molecular materials. The facile generation of machine-learning potentials for a diverse set of polymorphic compounds, benzene, glycine, and succinic acid, and predictions of thermodynamic stabilities in qualitative and quantitative agreement with experiments highlights that predictive thermodynamic studies of industrially-relevant molecular materials are no longer a daunting task. ","A complete description of thermodynamic stabilities of molecular
  crystals"
161,1366150839291826182,947891260555169792,Ant√¥nio Horta Ribeiro,"['Aliasing! The theme of my most recent paper :)\n\nHere the rotation speed is a multiple of the frame rate, generating this weird effect :) There (<LINK>) we study it inside a deep neural network... <LINK>']",http://arxiv.org/abs/2102.07757,"The convolutional neural network (CNN) remains an essential tool in solving computer vision problems. Standard convolutional architectures consist of stacked layers of operations that progressively downscale the image. Aliasing is a well-known side-effect of downsampling that may take place: it causes high-frequency components of the original signal to become indistinguishable from its low-frequency components. While downsampling takes place in the max-pooling layers or in the strided-convolutions in these models, there is no explicit mechanism that prevents aliasing from taking place in these layers. Due to the impressive performance of these models, it is natural to suspect that they, somehow, implicitly deal with this distortion. The question we aim to answer in this paper is simply: ""how and to what extent do CNNs counteract aliasing?"" We explore the question by means of two examples: In the first, we assess the CNNs capability of distinguishing oscillations at the input, showing that the redundancies in the intermediate channels play an important role in succeeding at the task; In the second, we show that an image classifier CNN while, in principle, capable of implementing anti-aliasing filters, does not prevent aliasing from taking place in the intermediate layers. ",How Convolutional Neural Networks Deal with Aliasing
162,1365854515161890821,794340080727064576,Chaochao Lu,"['We (with Yuhuai, Miguel, and Bernhard) propose a learning paradigm that enables out-of-distribution (OOD) generalization in the nonlinear setting, through Causal Representation Learning under the Agnostic Hypothesis.\n<LINK> <LINK>']",https://arxiv.org/abs/2102.12353,"Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation (i.e., a set of latent variables encoding the data) given the target and the environment belongs to general exponential family distributions. Based on this, we show that it is possible to identify the data representation up to simple transformations. We also prove that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach outperforms a variety of baseline methods. Finally, in the discussion, we further explore the aforementioned assumption and propose a more general hypothesis, called the Agnostic Hypothesis: there exist a set of hidden causal factors affecting both inputs and outcomes. The Agnostic Hypothesis can provide a unifying view of machine learning. More importantly, it can inspire a new direction to explore a general theory for identifying hidden causal factors, which is key to enabling the OOD generalization guarantees. ",Nonlinear Invariant Risk Minimization: A Causal Approach
163,1365445258914496517,52036434,Fabio Petrillo,"['Sharing our **rejected** ICSE/RoSE 2021 paper  ""Software Engineering for Robotic Systems: a systematic mapping study."" at <LINK>. I hope you enjoy it, and let we know your thoughts. Thanks @marcelapesquisa for hard work and reviewers for insightful suggestions.', 'https://t.co/wO3Xtga0Au']",https://arxiv.org/abs/2102.12520,"Robots are being applied in a vast range of fields, leading researchers and practitioners to write tasks more complex than in the past. The robot software complexity increases the difficulty of engineering the robot's software components with quality requirements. Researchers and practitioners have applied software engineering (SE) approaches and robotic domains to address this issue in the last two decades. This study aims to identify, classify and evaluate the current state-of-the-art Software Engineering for Robotic Systems (SERS). We systematically selected and analyzed 50 primary studies extracted from an automated search on Scopus digital library and manual search on the two editions of the RoSE workshop. We present three main contributions. Firstly, we provide an analysis from three following perspectives: demographics of publication, SE areas applied in robotics domains, and RSE findings. Secondly, we show a catalogue of research studies that apply software engineering techniques in the robotic domain, classified with the SWEBOK guide. We have identified 5 of 15 software engineering areas from the SWEBOK guide applied explicitly in robotic domains. The majority of the studies focused on the development phase (design, models and methods and construction). Testing and quality software areas have little coverage in SERS. Finally, we identify research opportunities and gaps in software engineering for robotic systems for future studies. ",Software Engineering for Robotic Systems:a systematic mapping study
164,1365443033001525252,2892508670,Guillermo Lorenzo,"['How can we combine medical imaging, mathematical models, and computer simulations to render personalized organ-scale tumor growth forecasts? \n\nFind out in our most recent preprint from @UTCompOnco and @OdenInstitute !\n\nLink üëâ <LINK>']",https://arxiv.org/abs/2102.12602,"Current clinical decision-making in oncology relies on averages of large patient populations to both assess tumor status and treatment outcomes. However, cancers exhibit an inherent evolving heterogeneity that requires an individual approach based on rigorous and precise predictions of cancer growth and treatment response. To this end, we advocate the use of quantitative in vivo imaging data to calibrate mathematical models for the personalized forecasting of tumor development. In this chapter, we summarize the main data types available from both common and emerging in vivo medical imaging technologies, and how these data can be used to obtain patient-specific parameters for common mathematical models of cancer. We then outline computational methods designed to solve these models, thereby enabling their use for producing personalized tumor forecasts in silico, which, ultimately, can be used to not only predict response, but also optimize treatment. Finally, we discuss the main barriers to making the above paradigm a clinical reality. ","Quantitative in vivo imaging to enable tumor forecasting and treatment
  optimization"
165,1365252469907939328,795343576590848000,David Raposo,['Excited to share our new paper on credit assignment in RL with Sam Ritter and colleagues at DeepMind. <LINK>\n\nWe propose to learn a model that links past states to current reward and use it to predict the contribution of a new state to the far future.'],http://arxiv.org/abs/2102.12425,"Since the earliest days of reinforcement learning, the workhorse method for assigning credit to actions over time has been temporal-difference (TD) learning, which propagates credit backward timestep-by-timestep. This approach suffers when delays between actions and rewards are long and when intervening unrelated events contribute variance to long-term returns. We propose state-associative (SA) learning, where the agent learns associations between states and arbitrarily distant future rewards, then propagates credit directly between the two. In this work, we use SA-learning to model the contribution of past states to the current reward. With this model we can predict each state's contribution to the far future, a quantity we call ""synthetic returns"". TD-learning can then be applied to select actions that maximize these synthetic returns (SRs). We demonstrate the effectiveness of augmenting agents with SRs across a range of tasks on which TD-learning alone fails. We show that the learned SRs are interpretable: they spike for states that occur after critical actions are taken. Finally, we show that our IMPALA-based SR agent solves Atari Skiing -- a game with a lengthy reward delay that posed a major hurdle to deep-RL agents -- 25 times faster than the published state-of-the-art. ",Synthetic Returns for Long-Term Credit Assignment
166,1365227598658347008,2794056066,Matthias Grundmann,['We estimated the number of unreachable peers in the #Bitcoin #P2P network using ADDR messages collected by our long-term monitoring of the Bitcoin P2P network. Find our preprint at <LINK>.'],https://arxiv.org/abs/2102.12774,"Bitcoin is based on a P2P network that is used to propagate transactions and blocks. While the P2P network design intends to hide the topology of the P2P network, information about the topology is required to understand the network from a scientific point of view. Thus, there is a natural tension between the 'desire' for unobservability on the one hand, and for observability on the other hand. On a middle ground, one would at least be interested on some statistical features of the Bitcoin network like the number of peers that participate in the propagation of transactions and blocks. This number is composed of the number of reachable peers that accept incoming connections and unreachable peers that do not accept incoming connections. While the number of reachable peers can be measured, it is inherently difficult to determine the number of unreachable peers. Thus, the number of unreachable peers can only be estimated based on some indicators. In this paper, we first define our understanding of unreachable peers and then propose the PAL (Passive Announcement Listening) method which gives an estimate of the number of unreachable peers by observing ADDR messages that announce active IP addresses in the network. The PAL method allows for detecting unreachable peers that indicate that they provide services useful to the P2P network. In conjunction with previous methods, the PAL method can help to get a better estimate of the number of unreachable peers. We use the PAL method to analyze data from a long-term measurement of the Bitcoin P2P network that gives insights into the development of the number of unreachable peers over five years from 2015 to 2020. Results show that about 31,000 unreachable peers providing useful services were active per day at the end of the year 2020. An empirical validation indicates that the approach finds about 50 % of unreachable peers that provide useful services. ","On the Estimation of the Number of Unreachable Peers in the Bitcoin P2P
  Network by Observation of Peer Announcements"
167,1365210166019444736,2984713195,Behnam Javanmardi,"['(1/5)\nIn our recent study (led by myself and including @PierreKervella &amp; @LBreuval) we inspected the Cepheid distance ladder and the Hubble Constant (H0) by an independent and complete re-analysis of the (raw) HST data of the SNIa host galaxy NGC 5584: <LINK> <LINK>', '(2/5)\nWhy we chose NGC 5584? because the periods and amplitude ratios of its Cepheids in different HST bands played a key role in the local measurement of H0 by SH0ES (see our abstract). https://t.co/2hiX0OAPYt', '(3/5)\nWe intentionally chose different analysis methods and tools, in particular, different tools for photometry and a completely different method for light curve analysis. https://t.co/yUtlcBrg4T', '(4/5)\nResult: we do not find a systematic difference between our results compared to those reported by SH0ES. The significant Hubble Constant tension remains a problem for the LCDM standard model of cosmology. https://t.co/TTVGYe9ozM', '(5 /5)\nOur paper is accepted for publication in the ApJ and my talk on the results of this study at the ""Cosmology at the Crossroads"" conference is available here: https://t.co/NbxWrx0OrE\nSee also the Tweet from @jrdmb:\nhttps://t.co/auAHsTU7IZ']",https://arxiv.org/abs/2102.12489,"The current tension between the direct and the early Universe measurements of the Hubble Constant, $H_0$, requires detailed scrutiny of all the data and methods used in the studies on both sides of the debate. The Cepheids in the type Ia supernova (SNIa) host galaxy NGC 5584 played a key role in the local measurement of $H_0$. The SH0ES project used the observations of this galaxy to derive a relation between Cepheids' periods and ratios of their amplitudes in different optical bands of the Hubble Space Telescope (HST), and used these relations to analyse the light curves of the Cepheids in around half of the current sample of local SNIa host galaxies. In this work, we present an independent detailed analysis of the Cepheids in NGC 5584. We employ different tools for our photometric analysis and a completely different method for our light curve analysis, and we do not find a systematic difference between our period and mean magnitude measurements compared to those reported by SH0ES. By adopting a period-luminosity relation calibrated by the Cepheids in the Milky Way, we measure a distance modulus $\mu=31.810\pm0.047$ (mag) which is in agreement with $\mu=31.786\pm0.046$ (mag) measured by SH0ES. In addition, the relations we find between periods and amplitude ratios of the Cepheids in NGC 5584 are significantly tighter than those of SH0ES and their potential impact on the direct $H_0$ measurement will be investigated in future studies. ","Inspecting the Cepheid distance ladder: The Hubble Space Telescope
  distance to the SNIa host galaxy NGC 5584"
168,1364505783187214336,1057966638748876800,Joey O'Brien,"[""New on arXiv - latest work with @gleesonj where we study Herbert Simon's classical neutral model using branching processes to further understand the orig. model and also generalize to incorporate arbitrary memory effects. Delighted to have this one out!\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2102.11705,"Simon's classical random-copying model, introduced in 1955, has garnered much attention for its ability, in spite of an apparent simplicity, to produce characteristics similar to those observed across the spectrum of complex systems. Through a discrete-time mechanism in which items are added to a sequence based upon rich-gets-richer dynamics, Simon demonstrated that the resulting size distributions of such sequences exhibit power-law tails. The simplicity of this model arises from the approach by which copying occurs uniformly over all previous elements in the sequence. Here we propose a generalization of this model which moves away from this uniform assumption, instead incorporating memory effects that allow the copying event to occur via an arbitrary age-dependent kernel. Through this approach we first demonstrate the potential to determine further information regarding the structure of sequences from the classical model before illustrating, via analytical study and numeric simulation, the flexibility offered by the arbitrary choice of memory. Furthermore we demonstrate how previously proposed memory-dependent models can be further studied as specific cases of the proposed framework. ",Memory-cognizant generalization to Simon's random-copying neutral model
169,1364480362458583042,802543221943439360,Andrea Caputo,['New paper out! <LINK>\nWe study the impact of the plasma around BHs on the superradiance for dark photons. We notice that it is possible -- in the presence of kinetic mixing -- that superradiance is shut down before extracting a sizable spin energy from the BH. <LINK>'],https://arxiv.org/abs/2102.11280,"Black hole superradiance is a powerful tool in the search for ultra-light bosons. Constraints on the existence of such particles have been derived from the observation of highly spinning black holes, absence of continuous gravitational-wave signals, and of the associated stochastic background. However, these constraints are only strictly speaking valid in the limit where the boson's interactions can be neglected. In this work we investigate the extent to which the superradiant growth of an ultra-light dark photon can be quenched via scattering processes with ambient electrons. For dark photon masses $m_{\gamma^\prime} \gtrsim 10^{-17}\,{\rm eV}$, and for reasonable values of the ambient electron number density, we find superradiance can be quenched prior to extracting a significant fraction of the black-hole spin. For sufficiently large $m_{\gamma^\prime}$ and small electron number densities, the in-medium suppression of the kinetic mixing can be efficiently removed, and quenching occurs for mixings $\chi_0 \gtrsim \mathcal{O}(10^{-8})$; at low masses, however, in-medium effects strongly inhibit otherwise efficient scattering processes from dissipating energy. Intriguingly, this quenching leads to a time- and energy-oscillating electromagnetic signature, with luminosities potentially extending up to $\sim 10^{57}\,{\rm erg / s}$, suggesting that such events should be detectable with existing telescopes. As a byproduct we also show that superradiance cannot be used to constrain a small mass for the Standard Model photon. ",Electromagnetic Signatures of Dark Photon Superradiance
170,1364403286212677640,961993666545029120,Andr√©s R. Vindas Mel√©ndez,"['Happy to share this paper written with some of my amazing Kentucky peeps! We study triangulations of order polytopes arising from the class of generalized snake posets and their posets of meet-irreducible elements. Check it out! üòä<LINK> @BraunMath @JointMathLife <LINK>', '@SamKolhatkar @BraunMath @JointMathLife Thanks Sampada! Will let you know :)', '@aidadoesmath @BraunMath @JointMathLife üíõ']",https://arxiv.org/abs/2102.11306,"This work regards the order polytopes arising from the class of generalized snake posets and their posets of meet-irreducible elements. Among generalized snake posets of the same rank, we characterize those whose order polytopes have minimal and maximal volume. We give a combinatorial characterization of the circuits in these order polytopes and then conclude that every triangulation is unimodular. For a generalized snake word, we count the number of flips for the canonical triangulation of these order polytopes. We determine that the flip graph of the order polytope of the poset whose lattice of filters comes from a ladder is the Cayley graph of a symmetric group. Lastly, we introduce an operation on triangulations called twists and prove that twists preserve regular triangulations. ","Triangulations, order polytopes, and generalized snake posets"
171,1364292898351681540,3045030351,Wesley Cota,"['Our new preprint is out! We propose a theoretical framework that allows accommodating the heterogeneity of human contacts and the complexity of mobility patterns in infectious disease dynamics.\n\n<LINK>\n\nw/ @sorianopanos @silviojrufv @_AlexArenas @gomezgardenes <LINK>', 'Desenvolvemos um ferramental te√≥rico que permite incorporar a heterogeneidade de contatos humanos e a complexidade dos padr√µes de mobilidade em din√¢micas de propaga√ß√£o de epidemias.\n\nO preprint √© fruto do Programa de Doutorado Sandu√≠che no Exterior da @CAPES_Oficial de 2019/2020. https://t.co/GElqCzk8Ni']",https://arxiv.org/abs/2102.10614,"Human mobility, contact patterns, and their interplay are key aspects of our social behavior that shape the spread of infectious diseases across different regions. In the light of new evidence and data sets about these two elements, epidemic models should be refined to incorporate both the heterogeneity of human contacts and the complexity of mobility patterns. Here, we propose a theoretical framework that allows accommodating these two aspects in the form of a set of Markovian equations. We validate these equations with extensive mechanistic simulations and derive analytically the epidemic threshold. The expression of this critical value allows us to evaluate its dependence on the specific demographic distribution, the structure of mobility flows, and the heterogeneity of contact patterns, thus shedding light on the microscopic mechanisms responsible for the epidemic detriment driven by recurrent mobility patterns reported in the literature. ","Infectious disease dynamics in metapopulations with heterogeneous
  transmission and recurrent mobility"
172,1364234820050640899,1269670536,Lukasz Olejnik,"['Our new research paper. We find that the CNAME tracking scheme is gaining popularity and is leading to security and privacy risks on the web. Data is leaking. Joint work with Yana Dimova, Gunes Acar @tomvangoethem. Paper accepted to @PET_Symposium 2021. <LINK> <LINK>']",https://arxiv.org/abs/2102.09301,"Online tracking is a whack-a-mole game between trackers who build and monetize behavioral user profiles through intrusive data collection, and anti-tracking mechanisms, deployed as a browser extension, built-in to the browser, or as a DNS resolver. As a response to pervasive and opaque online tracking, more and more users adopt anti-tracking tools to preserve their privacy. Consequently, as the information that trackers can gather on users is being curbed, some trackers are looking for ways to evade these tracking countermeasures. In this paper we report on a large-scale longitudinal evaluation of an anti-tracking evasion scheme that leverages CNAME records to include tracker resources in a same-site context, effectively bypassing anti-tracking measures that use fixed hostname-based block lists. Using historical HTTP Archive data we find that this tracking scheme is rapidly gaining traction, especially among high-traffic websites. Furthermore, we report on several privacy and security issues inherent to the technical setup of CNAME-based tracking that we detected through a combination of automated and manual analyses. We find that some trackers are using the technique against the Safari browser, which is known to include strict anti-tracking configurations. Our findings show that websites using CNAME trackers must take extra precautions to avoid leaking sensitive information to third parties. ","The CNAME of the Game: Large-scale Analysis of DNS-based Tracking
  Evasion"
173,1364217858641829888,1140025148004810752,Pierre Arthuis,"['üóû New paper alert! üóû\n\nWe propose a new many-body expansion formalism for open-shell mid-mass nuclei. Additional perk: it comes with contributions derived at all orders!\n\n<LINK> <LINK>', 'In-Medium Similarity Renormalization Group has been a theory of choice for ab initio many-body practitioners, and with its Multi-Reference and Valence-Space counterparts have been instrumental in recent progress.\n\nFigure from H.Hergert, Front. Phys. 8:379, https://t.co/w21k8sXZ3d https://t.co/phNvwqC5CV', 'Though MR-IMSRG and VS-IMSRG are already able to tackle open-shell nuclei, they are pretty costly methods. Here we propose a single-reference, symmetry-breaking alternative, similar to the recently successful Bogoliubov MBPT.\n\nFigure from Tichai et al., https://t.co/4aNMvq0kCa https://t.co/R2tYsYlwQV', 'Because Bogoliubov IMSRG inherently relies on a simple commutator, the structure of its contributions is pretty well constrained. This makes for an easy automated generation of diagrams and expressions from the get go.\n\nFigure taken from our new paper, https://t.co/JjraJoxNKN https://t.co/iQtNKd46DQ', 'So with this new paper, we have updated the Automated Diagram Generator ADG to v3.0.0. It is now able to generate BIMSRG expressions at arbitrary orders and for traditional or exotic truncations.\n\nhttps://t.co/JDRNyc89Er']",https://arxiv.org/abs/2102.10889,"The goal of the present paper is twofold. First, a novel expansion many-body method applicable to superfluid open-shell nuclei, the so-called Bogoliubov in-medium similarity renormalization group (BIMSRG) theory, is formulated. This generalization of standard single-reference IMSRG theory for closed-shell systems parallels the recent extensions of coupled cluster, self-consistent Green's function or many-body perturbation theory. Within the realm of IMSRG theories, BIMSRG provides an interesting alternative to the already existing multi-reference IMSRG (MR-IMSRG) method applicable to open-shell nuclei. The algebraic equations for low-order approximations, i.e., BIMSRG(1) and BIMSRG(2), can be derived manually without much difficulty. However, such a methodology becomes already impractical and error prone for the derivation of the BIMSRG(3) equations, which are eventually needed to reach high accuracy. Based on a diagrammatic formulation of BIMSRG theory, the second objective of the present paper is thus to describe the third version (v3.0.0) of the ADG code that automatically (1) generates all valid BIMSRG(n) diagrams and (2) evaluates their algebraic expressions in a matter of seconds. This is achieved in such a way that equations can easily be retrieved for both the flow equation and the Magnus expansion formulations of BIMSRG. Expanding on this work, the first future objective is to numerically implement BIMSRG(2) (eventually BIMSRG(3)) equations and perform ab initio calculations of mid-mass open-shell nuclei. ","ADG: Automated generation and evaluation of many-body diagrams III.
  Bogoliubov in-medium similarity renormalization group formalism"
174,1364170235021037569,937194665991987200,Enrique Solano,"['Today in arXiv we propose ""Light-matter quantum Otto engine in finite time"" <LINK> in great collaboration @usach @QUTIS3 @artist_qu @meetIQM @upvehu @Ikerbasque We deal with ultrastrong coupling regimes in quantum Rabi model referred to Curzon-Ahlborn efficiency.']",https://arxiv.org/abs/2102.10559,"We study a quantum Otto engine at finite time, where the working substance is composed of a two-level system interacting with a harmonic oscillator, described by the quantum Rabi model. We obtain the limit cycle and calculate the total work extracted, efficiency, and power of the engine by numerically solving the master equation describing the open system dynamics. We relate the total work extracted and the efficiency at maximum power with the quantum correlations embedded in the working substance, which we consider through entanglement of formation and quantum discord. Interestingly, we find that the engine can overcome the Curzon-Ahlborn efficiency when the working substance is in the ultrastrong coupling regime. This high-efficiency regime roughly coincides with the cases where the entanglement in the working substance experiences the greatest reduction in the hot isochoric stage. Our results highlight the efficiency performance of correlated working substances for quantum heat engines. ",Light-matter quantum Otto engine in finite time
175,1364127006435213314,786855300322172928,Alkistis Pourtsidou,"['Paper alert! In <LINK>, led by @CunningtonSD and @CatAstro_Phy, we present a simulations and modelling study of the HI intensity mapping bispectrum, including nasty observational effects from foregrounds with polarisation leakage + a beam with sidelobes. <LINK>', ""And here's how the covariance looks including these effects https://t.co/Jk8SyDUOTT""]",https://arxiv.org/abs/2102.11153,"The bispectrum is a 3-point statistic with the potential to provide additional information beyond power spectra analyses of survey datasets. Radio telescopes which broadly survey the 21cm emission from neutral hydrogen (HI) are a promising way to probe LSS and in this work we present an investigation into the HI intensity mapping (IM) bispectrum using simulations. We present a model of the redshift space HI IM bispectrum including observational effects from the radio telescope beam and 21cm foreground contamination. We validate our modelling prescriptions with measurements from robust IM simulations, inclusive of these observational effects. Our foreground simulations include polarisation leakage, on which we use a Principal Component Analysis cleaning method. We also investigate the effects from a non-Gaussian beam including side-lobes. For a MeerKAT-like single-dish IM survey at $z=0.39$, we find that foreground removal causes a 8% reduction in the equilateral bispectrum's signal-to-noise ratio $S/N$, whereas the beam reduces it by 62%. We find our models perform well, generally providing $\chi^2_\text{dof}\sim 1$, indicating a good fit to the data. Whilst our focus is on post-reionisation, single-dish IM, our modelling of observational effects, especially foreground removal, can also be relevant to interferometers and reionisation studies. ",The HI intensity mapping bispectrum including observational effects
176,1364050833495650309,3022633752,Tianle Cai,"['Subpopulation shift is a ubiquitous component of natural distribution shift. We propose a general theoretical framework of learning under subpopulation shift based on label propagation. And our insights can help to improve domain adaptation algorithms. <LINK> <LINK>', 'Joint work with incredible coauthors @SparkyTruck @jasondeanlee @QiLei45724485 ü•∞']",https://arxiv.org/abs/2102.11203,"One of the central problems in machine learning is domain adaptation. Unlike past theoretical work, we consider a new model for subpopulation shift in the input or representation space. In this work, we propose a provably effective framework for domain adaptation based on label propagation. In our analysis, we use a simple but realistic expansion assumption, proposed in \citet{wei2021theoretical}. Using a teacher classifier trained on the source domain, our algorithm not only propagates to the target domain but also improves upon the teacher. By leveraging existing generalization bounds, we also obtain end-to-end finite-sample guarantees on the entire algorithm. In addition, we extend our theoretical framework to a more general setting of source-to-target transfer based on a third unlabeled dataset, which can be easily applied in various learning scenarios. Inspired by our theory, we adapt consistency-based semi-supervised learning methods to domain adaptation settings and gain significant improvements. ",A Theory of Label Propagation for Subpopulation Shift
177,1362767329680109591,1868847132,Emily Deibert,"['New paper on the arXiv today (and coming soon to AJ)! We (@DrRayJay, @AstroAndrew123, and others) used near-IR observations from @CARMENES_exopl and SPIRou @CFHTelescope to study the atmosphere of super-hot super-Earth 55 Cancri e üî≠\n<LINK>', '@RonDeibert Thank you! It was 2 years in the making.', ""@di_goldene_pave Thanks so much Aaron! It took a while so I'm glad to get it out there üòÖ""]",https://arxiv.org/abs/2102.08965,"We present high-resolution near-infrared spectra taken during eight transits of 55 Cancri e, a nearby low-density super-Earth with a short orbital period (< 18 hours). While this exoplanet's bulk density indicates a possible atmosphere, one has not been detected definitively. Our analysis relies on the Doppler cross-correlation technique, which takes advantage of the high spectral resolution and broad wavelength coverage of our data, to search for the thousands of absorption features from hydrogen-, carbon-, and nitrogen-rich molecular species in the planetary atmosphere. Although we are unable to detect an atmosphere around 55 Cancri e, we do place strong constraints on the levels of HCN, NH${}_3$, and C${}_2$H${}_2$ that may be present. In particular, at a mean molecular weight of 5 amu we can rule out the presence of HCN in the atmosphere down to a volume mixing ratio (VMR) of 0.02%, NH${}_3$ down to a VMR of 0.08%, and C${}_2$H${}_2$ down to a VMR of 1.0%. If the mean molecular weight is relaxed to 2 amu, we can rule out the presence of HCN, NH${}_3$, and C${}_2$H${}_2$ down to VMRs of 0.001%, 0.0025%, and 0.08% respectively. Our results reduce the parameter space of possible atmospheres consistent with the analysis of HST/WFC3 observations by Tsiaras et al. (2016), and indicate that if 55 Cancri e harbors an atmosphere, it must have a high mean molecular weight and/or clouds. ",A Near-Infrared Chemical Inventory of the Atmosphere of 55 Cancri e
178,1362752838477176837,399331417,Max Falkenberg,"['New preprint!ü•≥\n""Correlated Copying in a Hidden Network Model"" \n\nWhat happens if we break the assumptions of uniform copying in simple models?\nHow do we use hidden multilayer networks to systematically introduce copying asymmetry?\n\nFind out üëá\n<LINK> <LINK>']",https://arxiv.org/abs/2102.09489,"Node copying is an important mechanism for network formation, yet most models assume uniform copying rules. Motivated by observations of heterogeneous triadic closure in real networks, we introduce the concept of a hidden network model - a generative two-layer model in which an observed network evolves according to the structure of an underlying hidden layer - and apply the framework to a model of heterogeneous copying. Framed in a social context, these two layers represent a node's inner social circle, and wider social circle, such that the model can bias copying probabilities towards, or against, a node's inner circle of friends. Comparing the case of extreme inner circle bias to an equivalent model with uniform copying, we find that heterogeneous copying suppresses the power-law degree distributions commonly seen in copying models, and results in networks with much higher clustering than even the most optimum scenario for uniform copying. Similarly large clustering values are found in real collaboration networks, lending empirical support to the mechanism. ",Heterogeneous node copying from hidden network structure
179,1362479759255605255,1888564675,Chivukula Sai Shruthi,['Ethics-focused methods! \nHow many are there? What are they? How can you describe them for design action? Where can I find them?\n\nWe tried to address these questions in our paper and collection ‚ÄúSurveying the Landscape of Ethics-Focused Design Methods.‚Äù <LINK> <LINK>'],https://arxiv.org/abs/2102.08909,"Over the past decade, HCI researchers, design researchers, and practitioners have increasingly addressed ethics-focused issues through a range of theoretical, methodological and pragmatic contributions to the field. While many forms of design knowledge have been proposed and described, we focus explicitly on knowledge that has been codified as ""methods,"" which we define as any supports for everyday work practices of designers. In this paper, we identify, analyze, and map a collection of 63 existing ethics-focused methods intentionally designed for ethical impact. We present a content analysis, providing a descriptive record of how they operationalize ethics, their intended audience or context of use, their ""core"" or ""script,"" and the means by which these methods are formulated, articulated, and languaged. Building on these results, we provide an initial definition of ethics-focused methods, identifying potential opportunities for the development of future methods to support design practice and research. ",Surveying the Landscape of Ethics-Focused Design Methods
180,1362453490866348032,1226265894495539200,Anirudh Joshi,"['A new look into clinical generalization of deep learning. We find that even if performance drops on distribution shifts, the resulting performance is still equivalent to radiologists on that distribution. Read more in our publication @CHILconference (<LINK>) <LINK>']",https://arxiv.org/abs/2102.08660,"Recent advances in training deep learning models have demonstrated the potential to provide accurate chest X-ray interpretation and increase access to radiology expertise. However, poor generalization due to data distribution shifts in clinical settings is a key barrier to implementation. In this study, we measured the diagnostic performance for 8 different chest X-ray models when applied to (1) smartphone photos of chest X-rays and (2) external datasets without any finetuning. All models were developed by different groups and submitted to the CheXpert challenge, and re-applied to test datasets without further tuning. We found that (1) on photos of chest X-rays, all 8 models experienced a statistically significant drop in task performance, but only 3 performed significantly worse than radiologists on average, and (2) on the external set, none of the models performed statistically significantly worse than radiologists, and five models performed statistically significantly better than radiologists. Our results demonstrate that some chest X-ray models, under clinically relevant distribution shifts, were comparable to radiologists while other models were not. Future work should investigate aspects of model training procedures and dataset collection that influence generalization in the presence of data distribution shifts. ","CheXternal: Generalization of Deep Learning Models for Chest X-ray
  Interpretation to Photos of Chest X-rays and External Clinical Settings"
181,1362074954762579970,3269585132,limmerlab,"[""Big day: Yoonjae's first group paper has posted to the arXiv! In a study with @yanggroupucb and @quan_lina, we detail how coherent vibrations generated during the relaxation of optical excitations dephase in layered perovskites.  <LINK>""]",https://arxiv.org/abs/2102.07957,"Organic-inorganic layered perovskites are two-dimensional quantum wells with layers of lead-halide octahedra stacked between organic ligand barriers. The combination of their dielectric confinement and ionic sublattice results in excitonic excitations with substantial binding energies that are strongly coupled to the surrounding soft, polar lattice. However, the ligand environment in layered perovskites can significantly alter their optical properties due to the complex dynamic disorder of soft perovskite lattice. Here, we observe the dynamic disorder through phonon dephasing lifetimes initiated by ultrafast photoexcitation employing high-resolution resonant impulsive stimulated Raman spectroscopy of a variety of ligand substitutions. We demonstrate that vibrational relaxation in layered perovskite formed from flexible alkyl-amines as organic barriers is fast and relatively independent of the lattice temperature. Relaxation in aromatic amine based layered perovskite is slower, though still fast relative to pure inorganic lead bromide lattices, with a rate that is temperature dependent. Using molecular dynamics simulations, we explain the fast rates of relaxation by quantifying the large anharmonic coupling of the optical modes with the ligand layers and rationalize the temperature independence due to their amorphous packing. This work provides a molecular and time-domain depiction of the relaxation of nascent optical excitations and opens opportunities to understand how they couple to the complex layered perovskite lattice, elucidating design principles for optoelectronic devices. ",Vibrational relaxation dynamics in layered perovskite quantum wells
182,1361813504114630658,1230579863444054018,Ziang Yan,"['Check out our new paper: <LINK>. We measure the tomographic cross-correlation between KiDS-1000 galaxy catalogue and Planck tSZ y map to study the thermodynamic property of intergalactic gas. (1/n)', ""This is my first project in the KiDS collaboration. I'm very happy to work in such a wonderful team! I've learned a lot about every detail in a cross-correlation-related project from my collaborators. Looking forward to my next one! (2/2)""]",https://arxiv.org/abs/2102.07701,"We constrain the redshift dependence of gas pressure bias $\left\langle b_{y} P_{\mathrm{e}}\right\rangle$ (bias-weighted average electron pressure), which characterises the thermodynamics of intergalactic gas, through a combination of cross-correlations between galaxy positions and the thermal Sunyaev-Zeldovich (tSZ) effect, as well as galaxy positions and the gravitational lensing of the cosmic microwave background (CMB). The galaxy sample is from the fourth data release of the Kilo-Degree Survey (KiDS). The tSZ $y$ map and the CMB lensing map are from the {\textit{Planck}} 2015 and 2018 data releases, respectively. The measurements are performed in five redshift bins with $z\lesssim1$. With these measurements, combining galaxy-tSZ and galaxy-CMB lensing cross-correlations allows us to break the degeneracy between galaxy bias and gas pressure bias, and hence constrain them simultaneously. In all redshift bins, the best-fit values of $\bpe$ are at a level of $\sim 0.3\, \mathrm{meV/cm^3}$ and increase slightly with redshift. The galaxy bias is consistent with unity in all the redshift bins. Our results are not sensitive to the non-linear details of the cross-correlation, which are smoothed out by the {\textit{Planck}} beam. Our measurements are in agreement with previous measurements as well as with theoretical predictions. We also show that our conclusions are not changed when CMB lensing is replaced by galaxy lensing, which shows the consistency of the two lensing signals despite their radically different redshift ranges. This study demonstrates the feasibility of using CMB lensing to calibrate the galaxy distribution such that the galaxy distribution can be used as a mass proxy without relying on the precise knowledge of the matter distribution. ","Probing galaxy bias and intergalactic gas pressure with KiDS
  Galaxies-tSZ-CMB lensing cross-correlations"
183,1361767641732546570,2344176362,Dr. Caitlin Casey,"['So while Texas froze over yesterday, we published a new paper I‚Äôm eager to share with y‚Äôall!\n\nBurnham et al. (2021), an observational study about the physical driver of dust temperatures in galaxies! üßµ Link ‚û°Ô∏è\n<LINK>', 'This paper shows two things:\n\n1) SFR surface density correlates most closely with ISM dust temperature\n\n2) the Stefan-Boltzmann law, in all its simplicity, works pretty well to describe the integrated luminosity, size, and temperature of galaxies‚Äô ISM', 'I had a real pleasure working with Anne Burnham, the lead author (formerly a post-bac student @UTAstronomy), on this project. She joined @YaleAstronomy as a grad student this year! Here‚Äôs a summary of Anne‚Äôs findings...', 'We gathered some @ALMAobs high-resolution imaging of a set of high-z galaxies that spanned a wide range of dust temperatures from hot to cold with the goal of seeing if the spatial distribution of the dust had something to do with their temperatures. https://t.co/movRwlVz3x', 'In theory, they should! Intro astronomy classes all teach us about the Stefan-Boltzmann Law (SB) that work well for stars: if two stars have the same luminosity, but one is much larger, it should also be cooler.  Size ‚¨ÜÔ∏è Temperature ‚¨áÔ∏è. https://t.co/zHS5QDi3gt', 'So would we expect L=4 pi R^2 sigma T^4 to hold for galaxies? Well, for better or worse,  stars != galaxies.  The former are spherical &amp; optically thick, simple*, and the latter are‚Ä¶ a mess.\n\n*stars people please don‚Äôt kill me for saying stars are simple', 'Our hypothesis was that the measured sizes of the dust emitting region in these galaxies would indeed relate closely to the emergent infrared luminosity and (luminosity-weighted) dust temperatures. I‚Äôll get back to this shortly, but first...', 'The literature has said that very close correlations exist btw dust temperature and total star-formation rate (SFR), specific SFR (sSFR), and distance to the galaxy ‚Äòmain sequence‚Äô (MS).  In this paper we test the correlation btw temperature and these four quantities. https://t.co/zZlSqDhe16', 'Our first conclusion. is that we find that SFR and SFR surface density (Sigma_IR) correlate very closely with luminosity-weighted dust temperature.  sSFR and D_MS are much weaker correlations. https://t.co/7PLTA1ayBF', 'Our result for high-z galaxies is well aligned with results in the local Universe (Lutz et al. 2016) who analyze different scaling relations, including dust temp, SFR, Sigma_IR, and sSFR.', 'This suggests that SFR surface density is more fundamentally linked to the galaxy-wide ISM temperature than quantities related to the stellar mass of the galaxy ‚Äî and this makes sense thinking back to the SB law, where SFR surface density (L/R^2) would track roughly as T^4.', 'Note that we use the observable quantity rest-frame peak wavelength (lambda_peak) instead of dust temperature, because it‚Äôs independent of model opacity.\n(More on that in the paper, and almost every other paper I‚Äôve co-written since 2017 üòÇ..)', 'Now back to the inconvenient fact that galaxies != stars. One big problem with the tidy correlation btw temperature and SFR surface density is that the ISM is typically optically thin, and are cartoon SB law correlation exists for an optically thick medium.', 'This issue came up in Simpson et al. (2017), who found that galaxies‚Äô temperatures ‚Äî assuming optically thin modified blackbodies ‚Äî align with predictions from the SB law.  Optically thick MBBs don‚Äôt. That doesn‚Äôt make sense physically. https://t.co/CM410j675n', 'We suggest is that a modification of the SB law for galaxies, assuming a pancake like geometry with embedded SF regions, can fix this dilemma.  The optically thick temperatures then make sense. https://t.co/K4noRz7yHN', 'But is the ISM optically thick? Our textbooks suggest not, right? What‚Äôs the deal?\n\nThe temp we measure here is *luminosity-weighted*.  Galaxies are made up of ISM of many temperatures, and the L-weighted temp is naturally higher than the mass-weighted temp.', 'Our 2nd conclusion: because the L-weighted temperature is hotter, it‚Äôs also more likely probing more dense, optically thick portions of the ISM. So the assumption of an optically thick ISM works in this case, at least for somewhat dusty star-forming galaxies at high-redshift!']",https://arxiv.org/abs/2102.06250,"The underlying distribution of galaxies' dust SEDs (i.e., their spectra re-radiated by dust from rest-frame $\sim$3$\mu$m-3mm) remains relatively unconstrained due to a dearth of FIR/(sub)mm data for large samples of galaxies. It has been claimed in the literature that a galaxy's dust temperature -- observed as the wavelength where the dust SED peaks ($\lambda_{peak}$) -- is traced most closely by its specific star-formation rate (sSFR) or parameterized 'distance' to the SFR-M$_\star$ relation (the galaxy 'main sequence'). We present 0.24"" resolved 870$\mu$m ALMA dust continuum observations of seven $z=1.4-4.6$ dusty star-forming galaxies (DSFGs) chosen to have a large range of well-constrained luminosity-weighted dust temperatures. We also draw on similar resolution dust continuum maps from a sample of ALESS submillimeter galaxies from Hodge et al. (2016). We constrain the physical scales over which the dust radiates and compare those measurements to characteristics of the integrated SED. We confirm significant correlations of $\lambda_{peak}$ with both L$_{IR}$ (or SFR) and $\Sigma_{\rm IR}$ ($\propto$SFR surface density). We investigate the correlation between $\log_{10}$($\lambda_{peak}$) and $\log_{10}$($\Sigma_{\rm IR}$) and find the relation to hold as would be expected from the Stefan-Boltzmann Law, or the effective size of an equivalent blackbody. The correlations of $\lambda_{peak}$ with sSFR and distance from the SFR-M$_\star$ relation are less significant than those for $\Sigma_{\rm IR}$ or L$_{IR}$; therefore, we conclude that the more fundamental tracer of galaxies' luminosity-weighted integrated dust temperatures are indeed their star-formation surface densities in line with local Universe results, which relate closely to the underlying geometry of dust in the ISM. ","The Physical Drivers of the Luminosity-Weighted Dust Temperatures in
  High-Redshift Galaxies"
184,1361621180269154304,926551285788233728,Vincent Fortuin,"['Have you ever wondered whether isotropic Gaussian priors are good enough for your Bayesian neural network weights? They are often used in practice, but we find in our new paper (<LINK>) that they are indeed suboptimal! Details in thread. 1/13 <LINK>', 'To get an intuition for what kind of weight distributions work well in neural networks, we looked at the distributions of SGD-trained NNs. As we see in the figure above, in fully-connected networks, these distributions are much more heavy-tailed than Gaussians. 2/13', 'If we now use a more heavy-tailed prior for a BNN on the same task (in this case MNIST), we do not only improve the performance significantly, but also remove the cold posterior effect, as hypothesized by Wenzel et al. (https://t.co/tWjCtlhv3f). 3/13', 'These performance improvements also transfer to a different task than the original one from the SGD-training, in this case FashionMNIST. 4/13 https://t.co/ICbrcAnZlP', 'When looking at CNNs, we also find these heavy-tailed weight distributions in SGD-trained networks. 5/13 https://t.co/YH9vHCMkN1', 'Moreover, we find that CNNs often have strong empirical weight correlations, especially between weights within the same filter. 6/13 https://t.co/uIsDtJn2Oe', 'When introducing these observations into convolutional BNN priors, we find that the heavy-tailed priors again reduce the cold posterior effect, but do not improve performance. Conversely, the correlated priors improve performance, but do not reduce the cold posterior effect. 7/13 https://t.co/TPETCTX00F', 'Note also that we used data augmentation for CIFAR-10, which seems to strengthen the cold posterior effect. The cold posterior effect is thus less clearly connected to the prior in CNNs than in the fully-connected networks above. 8/13', 'All in all, we thus recommend to ditch the isotropic Gaussian priors and instead use heavy-tailed priors for fully-connected BNNs and correlated priors for convolutional BNNs. 9/13', 'For all these experiments in the paper, we tried to make assertions that are as accurate as possible with respect to the true posterior, so we used the GGMC inference from https://t.co/DkOYQbi7zK. 10/13', 'Moreover, we combined it with the cyclical learning rates from https://t.co/OwKHo4bYjg, and the preconditioner from https://t.co/tWjCtlhv3f. 11/13', 'We open-sourced our whole library for accurate SG-MCMC inference in BNNs with different priors at https://t.co/DaHzpFwvM1. We hope that it will foster future research into different BNN priors. 12/13', 'Finally, this work would not have been possible without our great team of collaborators, @AdriGarriga, @flwenz, @gxr, Richard Turner, @markvanderwilk, and @laurence_ai and the support from our institutions @ETH_en, @Cambridge_Uni, @imperialcollege, and @BristolUni. 13/13', '@EmtiyazKhan @_joaogui1 @sindero @JulyanArbel Thanks for the pointers!']",https://arxiv.org/abs/2102.06571,"Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets. ",Bayesian Neural Network Priors Revisited
185,1361560280828846081,1114972720826068992,Danny Horta Darrington üá∫üá¶,"[""What's that, stars in the inner Milky Way that come from GCs and from an accreted origin? Check out some awesome results in a paper led by my PhD sibling Shobhit Kisku where we study the nature of chemically tagged dissolved GC stars in the inner Galaxy <LINK>"", '@kaosteorin Thanks @kaosteorin !! I will make sure to pass the message on to Shobhit. Let us know if you wanna chat about it!']",https://arxiv.org/abs/2102.06720,"Recent evidence based on APOGEE data for stars within a few kpc of the Galactic centre suggests that dissolved globular clusters (GCs) contribute significantly to the stellar mass budget of the inner halo. In this paper we enquire into the origins of tracers of GC dissolution, N-rich stars, that are located in the inner 4 kpc of the Milky Way. From an analysis of the chemical compositions of these stars we establish that about 30% of the N-rich stars previously identified in the inner Galaxy may have an accreted origin. This result is confirmed by an analysis of the kinematic properties of our sample. The specific frequency of N-rich stars is quite large in the accreted population, exceeding that of its in situ counterparts by near an order of magnitude, in disagreement with predictions from numerical simulations. We hope that our numbers provide a useful test to models of GC formation and destruction. ","An enquiry on the origins of N-rich stars in the inner Galaxy basedon
  APOGEE chemical compositions"
186,1361414327031369729,1155072721325383680,Adolfo Carvalho,"[""Hi all! Super excited to emerge from the void to share my first first-author paper, which I've been working on for over 3 years now. \n\nWe used 14 years of RV measurements and one set of Hubble Space Telescope images to study the Hubble 4 young star system\n\n<LINK>"", '@IveyEDavis Thanks Ivey!']",https://arxiv.org/abs/2102.06257,"We studied the weak-lined T Tauri star Hubble 4, a known long-period binary, and its starspot phenomena. We used optical radial velocity (RV) data taken over a span of 14 years (2004-2010, 2017-2019) at the McDonald Observatory 2.7m Harlan J. Smith telescope and single epoch imaging from the HST/WFC3 instrument. The observed and apparent RV variations show contributions, respectively, from the binary motion as well as from a large spot group on one of the stars, presumed to be the primary. Fitting and removing the orbital signal from the RVs, we found the lower bound on the lifetime of a previously identified large spot group on the surface of the star to be at least 5.1 years. A $\sim5$ year lower limit is a long, but not unprecedented, duration for a single spot group. The later epoch data indicate significant spot evolution has occurred, placing an upper bound on the spot group lifetime at 12 years. We find that pre-main sequence evolutionary models for the age of Taurus ($\sim2$ Myr), combined with component mass estimates from the literature, permit us to reproduce the HST relative photometry and the binary-induced contribution to the apparent RV variations. The long-lived star spot we find on Hubble 4 has significant implications for dynamo models in young stars, as it adds evidence for long lifetimes of magnetic field topologies. There are also significant implications for young star exoplanet searches as long-lived coherent RV signals may be spot-induced and not the result of planetary motion. (This paper includes data taken at The McDonald Observatory of The University of Texas at Austin.) ","Radial Velocity Monitoring of the Young Star Hubble 4: Disentangling
  Starspot Lifetimes from Orbital Motion"
187,1361198098588962817,352885549,Hideaki Hata,"[""In our @ICSEconf 2021 paper we (@Augaiko, Takashi Ishio, @ctreude) propose 'meta-maintenance', a concept for maintaining the entire software ecosystem.\n #icsePromo\n\nPreprint: <LINK>\nData: <LINK>""]",https://arxiv.org/abs/2102.06355,"Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance -- i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential. ","Same File, Different Changes: The Potential of Meta-Maintenance on
  GitHub"
188,1361188427547496457,1367912887,St√©phane Deny,"['With @D_Bouchacourt and @marksibrahim at @facebookai, we try to Address the Topological Defects of Disentanglement via Distributed Operators: <LINK>\nWait, what? üßê Find below a short introduction to the topic and the work!', '1/ In machine learning, disentanglement is the art of learning the factors of variation that compose a dataset. For example, in a dataset of dog pictures, some relevant factors of variation are pose, color and breed. https://t.co/siqiWIusme', '2/ In its traditional implementation, disentanglement attempts to isolate each of these factors of variation into a distinct subspace of a latent representation, as shown in this cartoon. https://t.co/prJ4VbNcyH', '3/ In this work, using considerations from topology (the mathematical field), we show that this notion of disentanglement is impossible to achieve for many transformations, including basic ones such as simple rotations of shapes. https://t.co/s9tML4QgoN', '4/ We then study an alternative approach to disentanglement, which relies on distributed latent operators, potentially acting on the entire latent space. With group theory we show that this approach does not suffer from the same shortcomings as disentanglement via subspaces. https://t.co/6lZZa5mnOa', '5/ We back up these mathematical observations with simple experiments on toy datasets. We show for example that the rotation of simple shapes can be successfully learned via distributed operators, but not via subspaces. https://t.co/TxtmCoRh5C', '6/ We hope that our work will be a starting point to more discussions on how to choose operators in latent space for disentanglement, and to understand the success of distributed operator strategies in recent work such as https://t.co/sIkpZOymUR and https://t.co/yTQ5w0W8dz', '7/ All comments and feedback are welcome on this attempt to improve our understanding of disentanglement! Tagging some relevant folks below: @airalcorn2 @mnorko @crozSciTech @emidup @TacoCohen @pfau @StefanoFusi2 @andrewgwils @ylecun @yanndubs @Jack_W_Lindsey @casellesdupre', '@wellingmax @maurice_weiler @mario1geiger @Yubei_Chen @DylanPaiton @tylerraye @yash_j_sharma @klindt_david @Hidenori8Tanaka @lcfalors @thomaskipf @LakeBrenden @ReubenFeinman @FrancescoLocat8 @poolio @DaniloJRezende @OlivierBachem @chelseabfinn @armandjoulin', '@Kamesh_Kris @StefanoErmon @jaschasd @davidwromero @erikjbekkers @JeffreySeely @SuryaGanguli @leventsagun @arimorcos @tydsh @davidjschwab @kaishengtai @simple_cell_ @jaakkolehtinen']",https://arxiv.org/abs/2102.05623,"A core challenge in Machine Learning is to learn to disentangle natural factors of variation in data (e.g. object shape vs. pose). A popular approach to disentanglement consists in learning to map each of these factors to distinct subspaces of a model's latent representation. However, this approach has shown limited empirical success to date. Here, we show that, for a broad family of transformations acting on images--encompassing simple affine transformations such as rotations and translations--this approach to disentanglement introduces topological defects (i.e. discontinuities in the encoder). Motivated by classical results from group representation theory, we study an alternative, more flexible approach to disentanglement which relies on distributed latent operators, potentially acting on the entire latent space. We theoretically and empirically demonstrate the effectiveness of this approach to disentangle affine transformations. Our work lays a theoretical foundation for the recent success of a new generation of models using distributed operators for disentanglement. ","Addressing the Topological Defects of Disentanglement via Distributed
  Operators"
189,1359989526001815552,1539866125,Jacy Reese Anthis,"['New preprint by Jamie Harris &amp; me reviews the 294 articles on the moral inclusion of AI. We find ""widespread agreement among scholars that some artificial entities could warrant moral consideration."" This motivates our work at @SentienceInst on #AIEthics. <LINK> <LINK>']",https://arxiv.org/abs/2102.04215,"Ethicists, policy-makers, and the general public have questioned whether artificial entities such as robots warrant rights or other forms of moral consideration. There is little synthesis of the research on this topic so far. We identify 294 relevant research or discussion items in our literature review of this topic. There is widespread agreement among scholars that some artificial entities could warrant moral consideration in the future, if not also the present. The reasoning varies, such as concern for the effects on artificial entities and concern for the effects on human society. Beyond the conventional consequentialist, deontological, and virtue ethicist ethical frameworks, some scholars encourage ""information ethics"" and ""social-relational"" approaches, though there are opportunities for more in-depth ethical research on the nuances of moral consideration of artificial entities. There is limited relevant empirical data collection, primarily in a few psychological studies on current moral and social attitudes of humans towards robots and other artificial entities. This suggests an important gap for social science research on how artificial entities will be integrated into society and the factors that will determine how the interests of sentient artificial entities are considered. ",The Moral Consideration of Artificial Entities: A Literature Review
190,1359517751212138501,1117927926152990720,Ramon Astudillo,"['Are implicit word node-alignments useful for AMR parsing? They do help in multi-lingual settings!. We propose a multi-lingual AMR alignment method which applied to the stack-Transformer yields best results till date for multi-lingual AMR (EACL2021) <LINK>', 'Work led by @JanakiSheth while interning at IBM!']",https://arxiv.org/abs/2102.02189,"We develop high performance multilingualAbstract Meaning Representation (AMR) sys-tems by projecting English AMR annotationsto other languages with weak supervision. Weachieve this goal by bootstrapping transformer-based multilingual word embeddings, in partic-ular those from cross-lingual RoBERTa (XLM-R large). We develop a novel technique forforeign-text-to-English AMR alignment, usingthe contextual word alignment between En-glish and foreign language tokens. This wordalignment is weakly supervised and relies onthe contextualized XLM-R word embeddings.We achieve a highly competitive performancethat surpasses the best published results forGerman, Italian, Spanish and Chinese. ",Bootstrapping Multilingual AMR with Contextual Word Alignments
191,1359084489557880833,1314104323,The Anh Han,"['New pre-print: we study how to interfere in a spatial Ultimatum Game to promote fairness at a minimal  cost   <LINK>  @Cedric_Perret13', 'We show that, to minimize the cost of incentive/intervention, it is important to distinguish the role (i.e. provider vs receiver): Promoting Fair Proposers, Fair Responders or Both? Cost-Efficient Interference in the Spatial Ultimatum Game']",https://arxiv.org/abs/2102.03461,"Institutions and investors face the constant challenge of making accurate decisions and predictions regarding how best they should distribute their endowments. The problem of achieving an optimal outcome at minimal cost has been extensively studied and resolved using several heuristics. However, these works usually fail to address how an external party can target different types of fair behaviour or do not take into account how limited information can shape this complex interplay. Here, we consider the well-known Ultimatum game in a spatial setting and propose a hierarchy of interference mechanisms based on the amount of information available to an external decision-maker and desired standards of fairness. Our analysis reveals that monitoring the population at a macroscopic level requires more strict information gathering in order to obtain an optimal outcome and that local observations can mediate this requirement. Moreover, we identify the conditions which must be met for an individual to be eligible for investment in order to avoid unnecessary spending. We further explore the effects of varying mutation or behavioural exploration rates on the choice of investment strategy and total accumulated costs to the investor. Overall, our analysis provides new insights about efficient heuristics for cost-efficient promotion of fairness in societies. Finally, we discuss the differences between our findings and previous work done on the PD and present our suggestions for promoting fairness as an external decision-maker. ","Promoting Fair Proposers, Fair Responders or Both? Cost-Efficient
  Interference in the Spatial Ultimatum Game"
192,1358599302944149505,920110446581092352,Rikiya Yamashita,"['Happy to share our recent preprint. We propose STRAP, a form of data augmentation based on random style transfer from non-medical images, for learning domain-agnostic visual representations in digital pathology.\n\nArxiv: <LINK>\nGitHub: <LINK> <LINK>', 'Grateful to my co-authors, @JinLong19844963, Snikitha Banda, @jeanne_shen, @rubinqilab, for their hard work and support.\n@BdsStanford @StanfordAIMI']",https://arxiv.org/abs/2102.01678,"Suboptimal generalization of machine learning models on unseen data is a key challenge which hampers the clinical applicability of such models to medical imaging. Although various methods such as domain adaptation and domain generalization have evolved to combat this challenge, learning robust and generalizable representations is core to medical image understanding, and continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation for histoPathology), a form of data augmentation based on random style transfer from non-medical style source such as artistic paintings, for learning domain-agnostic visual representations in computational pathology. Style transfer replaces the low-level texture content of an image with the uninformative style of randomly selected style source image, while preserving the original high-level semantic content. This improves robustness to domain shift and can be used as a simple yet powerful tool for learning domain-agnostic representations. We demonstrate that STRAP leads to state-of-the-art performance, particularly in the presence of domain shifts, on two particular classification tasks in computational pathology. ","Learning domain-agnostic visual representation for computational
  pathology using medically-irrelevant style transfer augmentation"
193,1357717456475705344,830120476282408960,Nienke van der Marel,"['Really proud of @BrodieNorfolk, whose paper on transition disks with @almaobs  and ATCA just got accepted! In this multi-wavelength study we compare the dust cavities in mm vs cm emission for 15 transition disks.\n<LINK>\n#proudsupervisor #prettydiskimages']",https://arxiv.org/abs/2102.02316,"The origin of the inner dust cavities observed in transition discs remains unknown. The segregation of dust and size of the cavity is expected to vary depending on which clearing mechanism dominates grain evolution. We present the results from the Discs Down Under program, an 8.8 mm continuum Australia Telescope Compact Array (ATCA) survey targeting 15 transition discs with large (> 20 au) cavities, and compare the resulting dust emission to Atacama Large millimetre/sub-millimetre Array (ALMA) observations. Our ATCA observations resolve the inner cavity for 8 of the 14 detected discs. We fit the visibilities and reconstruct 1D radial brightness models for 10 sources with a S/N > 5sigma. We find that, for sources with a resolved cavity in both wavebands, the 8.8 mm and sub-mm brightness distributions peak at the same radius from the star. We suggest that a similar cavity size for 8.8 mm and sub-mm dust grains is due to a dust trap induced by the presence of a companion. ","Dust Traps and the Formation of Cavities in Transition Discs: A
  millimetre to sub-millimetre comparison survey"
194,1357364064154968066,1062061528931819523,Kale-ab Tessera,"['Excited to present my first preprint ü•≥üéâ - ""Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization"" , with @sarahookr and @BenjaminRosman .\n\nüìú <LINK> .\n\nWe use Gradient Flow (GF) to study sparse network optimization. <LINK>', 'That is to say, we use GF to study how sparse networks are affected by different optimizers, activation functions, architectures, learning rates and regularizers. \n\nThis follows on promising GF work - https://t.co/OI4skNZc20 (by @utkuevci) and https://t.co/9BqCmwendV .', '1. Firstly, to study sparse networks, we propose a simple, empirical framework - Same Capacity Sparse vs Dense Comparison (SC-SDC). The key idea is to compare sparse networks to their equivalent dense counterparts (same number of connections and same initial weight init). https://t.co/mahGNc0gxs', '2. Secondly, we also propose a new, normalized, layerwise measure of gradient flow, Effective Gradient Flow (EGF). EGF is normalized by the number of active weights and distributed evenly across all the layers. We show EGF correlates better, than other GF measures, to  ... https://t.co/pWBxLTVDhs', 'performance in sparse networks and hence it is a good formulation for studying the training dynamics of sparse networks. https://t.co/VY7cEESZ8V', '3.1 Using EGF and SC-SDC, we show that BatchNorm is more important for sparse networks than it is for dense networks (the result is statistically significant), which suggests that gradient instability is a key obstacle to starting sparse.', '3.2 We show that optimizers that use an exponentially weighted moving average (EWMA) to obtain an estimate of the variance of the gradient, such as Adam and RMSProp, are sensitive to higher gradient flow. This could explain why these methods are more sensitive to L2 and data aug.', '3.3 Finally, we show that Swish and PReLU (when using SGD) are promising activation functions, especially for sparse networks. For the Swish result, we suggest this could be due to Swish‚Äôs non-monotonic formulation, that allows for negative gradient flow, which helps with 3.2. https://t.co/rUj6onLF4B', 'We also extend some of these results from MLPs -&gt; CNNs and from random, fixed sparse networks -&gt; magnitude pruned networks. https://t.co/13t4TTct5Q', 'In conclusion, our work agrees with and contributes to the literature that emphasizes that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results.', 'Thanks for reading. Also, please let us know if we missed any related work/results and if you have any feedback üôè:)']",https://arxiv.org/abs/2102.01670,"Training sparse networks to converge to the same performance as dense neural architectures has proven to be elusive. Recent work suggests that initialization is the key. However, while this direction of research has had some success, focusing on initialization alone appears to be inadequate. In this paper, we take a broader view of training sparse networks and consider the role of regularization, optimization, and architecture choices on sparse models. We propose a simple experimental framework, Same Capacity Sparse vs Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and dense networks. Furthermore, we propose a new measure of gradient flow, Effective Gradient Flow (EGF), that better correlates to performance in sparse networks. Using top-line metrics, SC-SDC and EGF, we show that default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, we show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. Our work suggests that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results. ","Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network
  Optimization"
195,1357164692733652994,742149925,David Meyer,"['May &amp; Wakeham, Quantum tasks require islands on the brane <LINK> ""By considering quantum tasks which exploit information localized to the brane, we find a new connected wedge theorem."" <LINK>']",https://arxiv.org/abs/2102.01810,"In recent work, it was argued that quantum computations with inputs and outputs distributed in spacetime, or quantum tasks, impose constraints on entanglement in holographic theories. The resulting constraint was named the connected wedge theorem and can verified by a direct bulk proof using focusing arguments in general relativity. In this article we extend this work to the context of AdS/BCFT, where an end-of-the-world brane is present in the bulk. By considering quantum tasks which exploit information localized to the brane, we find a new connected wedge theorem. We apply this theorem to brane models of black holes, where it relates the formation of islands in the Ryu-Takayanagi formula to causal features of the ambient spacetime. In particular, we find that if the black hole interior is causally connected to the radiation system through the ambient spacetime, then an island forms. For constant tension branes in pure AdS the converse also holds. ",Quantum tasks require islands on the brane
196,1357006401952972808,970803673155751936,Charline Le Lan,"['Excited to present our #AAAI2021 paper ‚ÄúMetrics and continuity in Reinforcement learning‚Äù with @marcgbellemare and @pcastr tomorrow ! We propose a new perspective on representation learning and generalization in RL üåü\nüìúPaper: <LINK>\n\nIntro Videoüëá 1/8 <LINK>', 'In RL, we often deal with systems with large state spaces. We can‚Äôt exactly represent the value of each of these states and need some type of generalization. One way to do that is to look at structured representations in which similar states are assigned similar predictions.\n2/8 https://t.co/xwJgRi552q', 'We consider the case where these representations are induced by state metrics.\nHow shall we choose a metric to get good generalization properties?ü§î\nAn interesting metric should give us continuity properties for the RL function of interest in our problem.\n3/8 https://t.co/jV5QwffeYP', 'A good metric should also give us a representation as coarse as possible so that we can cheaply generalize to new states.\n4/8 https://t.co/nJl7bGrqlq', 'Our paper unifies representations of states spaces and the notion of continuity via a taxonomy of metrics. We also provide a hierarchy of metrics to compare the topologies induced by all these metrics. \n5/8 https://t.co/8mvjIrG1Fr', 'Using our taxonomy, we find that most commonly discussed metrics are actually poorly suited for algorithms that convert representations into values, so we introduce new metrics to overcome this shortcoming.\n6/8 https://t.co/vETKDBaeMw', 'What kind of generalization do these metrics produce when used for value function approximation? We present an empirical evaluation comparing our metrics and showing the importance of the choice of a neighborhood in RL algorithms.\n7/8 https://t.co/WTIGGoc0SA', 'Poster: https://t.co/cV21MM2gLs\nSlides:https://t.co/BH9r6tmgv5\nüíªCode: https://t.co/xvxSA8oWLL\n8/8', '@tw_killian @marcgbellemare @pcastr Thank you Taylor! hope you enjoy it‚ò∫Ô∏è‚ò∫Ô∏è', '@khimya @marcgbellemare @pcastr Thanks a lot Khimya üòÑ!']",https://arxiv.org/abs/2102.01514,"In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of ""well-behaved"" neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered. ",Metrics and continuity in reinforcement learning
197,1356927302819545089,405103790,Matteo Angelinelli,"['A new paper today <LINK> by myself, S. Ettori, @franco_vazza  and T.W. Jones!  We study the outskirts of simulated galaxy clusters to investigate the physical properties of matter clumps and filaments #magcow <LINK>', 'We developed two different algorithms, which detect matter clumps, starting from overdensity in the simulated density field, and filaments, using a new proxy based on the gas radial velocity and gas entropy. https://t.co/l47aFkS8gc', ""We find that density and temperature for our clumps population are independent by the central cluster's mass, while for filaments we note a slight increase of temperature with the cluster's mass. https://t.co/GxJL6FDkzk"", 'We investigate possible relations between clumps and filaments proprieties, and we find a high level of correlation, both for density and temperature. https://t.co/GAuuVIZOgr', 'Moreover, we study the mass and volume contribution of clumps and filaments over the total amount of gas in our simulations. We find that combing the different contributions account for ~17% of the total gas mass and only ~1% of the volume. https://t.co/8PS5LG1qG0', ""We divide our simulated volume into two different radial shells. We note that closer to the central cluster, both clumps' density and temperature are higher than ones in the periphery regions, making the clumps' X-rays detection easier. https://t.co/tv9gfrctq6"", ""Furthermore, analysing clumps' and filaments' mass and volume contributions in the different shells, we conclude that the inner one is a suitable candidate to detect and analyse matter clumps, while the outer one is better to filaments' studies. https://t.co/GqarDD9BwE"", ""Finally, we study three different scale relations M-L, L-T and M-T. Up to 3*R500, the interactions between clumps and ICM change the physical proprieties of the infalling structures. Otherwise, over 3*R500, clumps are described by scaling relations similar to the clusters' ones. https://t.co/sud4AhOYt1"", 'We are working on possible extensions of this work, using a different kind of simulations. Moreover, using SIXTE simulator, we are simulating what the WFI and @AthenaXIFU instruments (which will be on aboard on the @AthenaXobs) would observe in the outskirts of galaxy clusters https://t.co/gnL6nOfmDi']",https://arxiv.org/abs/2102.01096,"We report on the possibility of studying the proprieties of cosmic diffuse baryons by studying self-gravitating clumps and filaments connected to galaxy clusters. While filaments are challenging to detect with X-ray observations, the higher density of clumps makes them visible and a viable tracer to study the thermodynamical proprieties of baryons undergoing accretion along cosmic web filaments onto galaxy clusters. We developed new algorithms to identify these structures and applied them to a set of non-radiative cosmological simulations of galaxy clusters at high resolution. We find that in those simulated clusters, the density and temperature of clumps are independent of the mass of the cluster where they reside. We detected a positive correlation between the filament temperature and the host cluster mass. The density and temperature of clumps and filaments also tended to correlate. Both the temperature and density decrease moving outward. We observed that clumps are hotter, more massive, and more luminous if identified closer to the cluster center. Especially in the outermost cluster regions (~3*R500,c or beyond), X-ray observations might already have the potential to locate cosmic filaments based on the distribution of clumps and to allow one to study the thermodynamics of diffuse baryons before they are processed by the intracluster medium. ",Properties of clumps and filaments around galaxy clusters
198,1356563822216110084,1023681782363901952,Michal P. Heller,"[""We're very happy to share our group's latest collaborative @_arXiv_hep_th preprint, where we study the long-distance behaviour of #Entanglement of purification in CFTs and find interesting universal results. Check it out here: [<LINK>] #GQFI #QuantumInformation <LINK>""]",https://arxiv.org/abs/2102.00013,"Quantifying entanglement properties of mixed states in quantum field theory via entanglement of purification and reflected entropy is a new and challenging subject. In this work, we study both quantities for two spherical subregions far away from each other in the vacuum of a conformal field theory in any number of dimensions. Using lattice techniques, we find an elementary proof that the decay of both, the entanglement of purification and reflected entropy, is enhanced with respect to the mutual information behaviour by a logarithm of the distance between the subregions. In the case of the Ising spin chain at criticality and the related free fermion conformal field theory, we compute also the overall coefficients numerically for the both quantities of interest. ","Long-distance entanglement of purification and reflected entropy in
  conformal field theory"
199,1356517919853748224,956810201251770368,Soravitt Sangnark,"['Music preference was regularly mentioned as an influential factor in music (emotion) study. We investigated it in our latest work ""Revealing Preference in Popular Music Through Familiarity and Brain Response"".\n\nPreprint : <LINK> <LINK>']",https://arxiv.org/abs/2102.00159,"Music preference was reported as a factor, which could elicit innermost music emotion, entailing accurate ground-truth data and music therapy efficiency. This study executes statistical analysis to investigate the distinction of music preference through familiarity scores, response times (response rates), and brain response (EEG). Twenty participants did self-assessment after listening to two types of popular music's chorus section: music without lyrics (Melody) and music with lyrics (Song). \textcolor{red}{We then conduct a music preference classification using a support vector machine, random forest, and k-nearest neighbors with the familiarity scores, the response rates, and EEG as the feature vectors. The statistical analysis and F1-score of EEG are congruent, which is the brain's right side outperformed its left side in classification performance.} Finally, these behavioral and brain studies support that preference, familiarity, and response rates can contribute to the music emotion experiment's design to understand music, emotion, and listener. Not only to the music industry, the biomedical and healthcare industry can also exploit this experiment to collect data from patients to improve the efficiency of healing by music. ","Revealing Preference in Popular Music Through Familiarity and Brain
  Response"
200,1364135825424871425,3094610676,Pranav Rajpurkar,"['Can we leverage patient metadata for contrastive learning with medical images?\n\nYes! We propose to treat images that share common properties (e.g. patient, study, laterality) as positive pairs.\n\nPaperüéâ <LINK>\n\n@nhi_truongvu, @richcmwang, @Nir_Bala @StanfordAILab <LINK>', 'Introducing MedAug!\n\nWe develop a method, MedAug, to use patient metadata to select positive pairs in contrastive learning, and apply this method to chest X-rays for the downstream task of pleural effusion.\n\nMain findings next!\n\n2/n', 'Finding 1: Beats previous SOTA üåü.\n\nOur best pretrained representation achieves a performance increase of 3.4% and 14.4% in mean AUC compared to MoCo-CXR (https://t.co/HzfmL3ROdE) and the ImageNet pretrained baseline respectively.\n\n3/n https://t.co/qXSETf98jJ', 'Finding 2: Shared disease information matters!\n\nWe perform comparative empirical analysis to show that using positive pairs that share underlying pathologies improves pretrained representations.\n\n4/n https://t.co/LWSnl1nI9o', 'Finding 3: Number of positive pairs matters.\n\nWe also show that increasing the number of distinct images selected to form positive pairs per image query improves the quality of pretrained representations.\n\n5/n https://t.co/eWrO6Ymr8Y', ""Finding 4: Smart negative pair selection doesn't help.\n\nWe perform an exploratory analysis on strategies to select negative pairs using patient metadata, and do not find improvement over the default strategy that does not use metadata.\n\n6/n https://t.co/bpbZABIi17"", ""See https://t.co/O6F78eoLeR for more details, and for comparison to strategies including @facebookai/@nyugrossman's https://t.co/718UqbqyUT, @GoogleHealth's  https://t.co/R3AXZ01QXS, and @StanfordAILab's https://t.co/HzfmL3ROdE\n\n7/n"", 'Thank you to a wonderful team of Nhi Truong Vu (@nhi_truongvu), Richard Wang (@richcmwang), Niranjan Balachandar (@Nir_Bala), &amp; Can Liu.\n\nThank you to @AndrewYNg @StanfordAILab &amp; @StanfordAIMI for support!\n\n8/8']",https://arxiv.org/abs/2102.10663,"Self-supervised contrastive learning between pairs of multiple views of the same image has been shown to successfully leverage unlabeled data to produce meaningful visual representations for both natural and medical images. However, there has been limited work on determining how to select pairs for medical images, where availability of patient metadata can be leveraged to improve representations. In this work, we develop a method to select positive pairs coming from views of possibly different images through the use of patient metadata. We compare strategies for selecting positive pairs for chest X-ray interpretation including requiring them to be from the same patient, imaging study or laterality. We evaluate downstream task performance by fine-tuning the linear layer on 1% of the labeled dataset for pleural effusion classification. Our best performing positive pair selection strategy, which involves using images from the same patient from the same study across all lateralities, achieves a performance increase of 14.4% in mean AUC from the ImageNet pretrained baseline. Our controlled experiments show that the keys to improving downstream performance on disease classification are (1) using patient metadata to appropriately create positive pairs from different images with the same underlying pathologies, and (2) maximizing the number of different images used in query pairing. In addition, we explore leveraging patient metadata to select hard negative pairs for contrastive learning, but do not find improvement over baselines that do not use metadata. Our method is broadly applicable to medical image interpretation and allows flexibility for incorporating medical insights in choosing pairs for contrastive learning. ","MedAug: Contrastive learning leveraging patient metadata improves
  representations for chest X-ray interpretation"
201,1363097289024299008,168475031,Burak Hasƒ±rcƒ±oƒülu,['The preprint of our recent work is available on arxiv: <LINK> We propose using bivariate polynomial codes to multiply two massive matrices privately and efficiently in a distributed manner.'],https://arxiv.org/abs/2102.08304,"We consider the problem of private distributed matrix multiplication under limited resources. Coded computation has been shown to be an effective solution in distributed matrix multiplication, both providing privacy against the workers and boosting the computation speed by efficiently mitigating stragglers. In this work, we propose the use of recently-introduced bivariate polynomial codes to further speed up private distributed matrix multiplication by exploiting the partial work done by the stragglers rather than completely ignoring them. We show that the proposed approach reduces the average computation time of private distributed matrix multiplication compared to its competitors in the literature while improving the upload communication cost and the workers' storage efficiency. ","Speeding Up Private Distributed Matrix Multiplication via Bivariate
  Polynomial Codes"
202,1362312710608719874,744256054465302532,Anurag Kumar,"['Multi-channel audio and graph neural networks.   In our latest work ‚ÄúMulti-Channel Speech Enhancement Using Graph Neural Networks‚Äù (<LINK> ) accepted at @ieeeICASSP #icassp2021 #icassp we propose that multi-channel audio processing can be formulated through GNNs.', 'We  view each audio channel (or representations of audio from each channel) as a node of the graph. This formulation allows one to apply graph neural networks (GNN) to find spatial correlations among the different channels (nodes),', 'giving a flexible framework applicable to different microphones geometries as well as to even distributed microphones.', 'Our current paper (with Panagiotis Tzirakis  and Jacob Donley) applies this idea to multi-channel speech enhancement and shows encouraging results on using GNNs for learning from multi-channel audio. https://t.co/ABAHSbLKoY']",https://arxiv.org/abs/2102.06934,"Multi-channel speech enhancement aims to extract clean speech from a noisy mixture using signals captured from multiple microphones. Recently proposed methods tackle this problem by incorporating deep neural network models with spatial filtering techniques such as the minimum variance distortionless response (MVDR) beamformer. In this paper, we introduce a different research direction by viewing each audio channel as a node lying in a non-Euclidean space and, specifically, a graph. This formulation allows us to apply graph neural networks (GNN) to find spatial correlations among the different channels (nodes). We utilize graph convolution networks (GCN) by incorporating them in the embedding space of a U-Net architecture. We use LibriSpeech dataset and simulate room acoustics data to extensively experiment with our approach using different array types, and number of microphones. Results indicate the superiority of our approach when compared to prior state-of-the-art method. ",Multi-Channel Speech Enhancement using Graph Neural Networks
203,1361745710941671426,20499721,Bhaskar Mitra,"['üö®ANNOUNCEMENTüö®\nOverview of TREC 2020 Deep Learning Track now on arXiv: <LINK>\n\nTl;dr: we find further evidence that pretrained Transformers outperform others in large data regime.\n\nJoint work w/ @nick_craswell @xxEmineYilmazxx @spacemanidol #TRECDL #TRECDL2020 <LINK>', 'We also participated in the track this year (group name: MSAI) with Conformer-Kernel models w/ Query Term Independence (CK-QTI). Our best model outperforms all trad &amp; remaining nn runs, and 2/3rd of nnlm runs.\n\nPaper: https://t.co/sbMxoR6lRE\nCode: https://t.co/7F5Yn9Wy7I https://t.co/OFVMK2FvGJ', 'CK-QTI models are cheaper to train (no pretraining, shallower, better scale to long text) and infer (offline precompute, single-stage w/o cascading) but perform competitively. We hope CK-QTI can serve as an easy-to-repro baseline for the community.\n\nCode: https://t.co/7F5Yn9Wy7I', 'Finally, please watch this space for more news and announcements about TREC 2021 Deep Learning Track: https://t.co/pyRUKYs87k \n\nAlso, if you want to participate in #TRECDL2021 then please REGISTER BY FEBRUARY 23: https://t.co/iFf8zkLv9z']",https://arxiv.org/abs/2102.07662,"This is the second year of the TREC Deep Learning Track, with the goal of studying ad hoc ranking in the large training data regime. We again have a document retrieval task and a passage retrieval task, each with hundreds of thousands of human-labeled training queries. We evaluate using single-shot TREC-style evaluation, to give us a picture of which ranking methods work best when large data is available, with much more comprehensive relevance labeling on the small number of test queries. This year we have further evidence that rankers with BERT-style pretraining outperform other rankers in the large data regime. ",Overview of the TREC 2020 deep learning track
204,1359994265808601088,390275711,Christof Torres,['We found 15 smart contract lotteries that were victims of insertion frontrunning attacks (also known as block stuffing) over the past 5 yearsüö®An attacker made with a single attack a profit of over 700K USD üíµ <LINK> #DeFi #Ethereum <LINK>'],https://arxiv.org/abs/2102.03347,"Ethereum prospered the inception of a plethora of smart contract applications, ranging from gambling games to decentralized finance. However, Ethereum is also considered a highly adversarial environment, where vulnerable smart contracts will eventually be exploited. Recently, Ethereum's pool of pending transaction has become a far more aggressive environment. In the hope of making some profit, attackers continuously monitor the transaction pool and try to frontrun their victims' transactions by either displacing or suppressing them, or strategically inserting their transactions. This paper aims to shed some light into what is known as a dark forest and uncover these predators' actions. We present a methodology to efficiently measure the three types of frontrunning: displacement, insertion, and suppression. We perform a large-scale analysis on more than 11M blocks and identify almost 200K attacks with an accumulated profit of 18.41M USD for the attackers, providing evidence that frontrunning is both, lucrative and a prevalent issue. ","Frontrunner Jones and the Raiders of the Dark Forest: An Empirical Study
  of Frontrunning on the Ethereum Blockchain"
205,1359824344264962052,77712285,Andrea Baronchelli,"['New report out: ""Dark Web Marketplaces and COVID-19:\nThe vaccines"". We found 16 vendors offering unspecified, AstraZeneca and Pfizer vaccines.\n\n<LINK>\n \nSlow and unequal of vaccine distribution increase the appeal of illicit trade. Risks are huge. <LINK>', 'Research with @albe_bracci @matt55nado @maxzzze @iw_gray @t8el @DrAngelaGallo and supported by a @ESRC COVID-19 research grant.']",https://arxiv.org/abs/2102.05470,"Early analyses revealed that dark web marketplaces (DWMs) started offering COVID-19 related products (e.g., masks and COVID-19 tests) as soon as the COVID-19 pandemic started, when these goods were in shortage in the traditional economy. Here, we broaden the scope and depth of previous investigations by analysing 194 DWMs until July 2021, including the crucial period in which vaccines became available, and by considering the wider impact of the pandemic on DWMs. First, we focus on vaccines. We find 250 listings offering approved vaccines, like Pfizer/BioNTech and AstraZeneca, as well as vendors offering fabricated proofs of vaccination and COVID-19 passports. Second, we consider COVID-19 related products. We reveal that, as the regular economy has become able to satisfy the demand of these goods, DWMs have decreased their offer. Third, we analyse the profile of vendors of COVID-19 related products and vaccines. We find that most of them are specialized in a single type of listings and are willing to ship worldwide. Finally, we consider a broader set of listings mentioning COVID-19 as proxy for the general impact of the pandemic on these DWMs . Among 10,330 such listings, we show that recreational drugs are the most affected among traditional DWMs product, with COVID-19 mentions steadily increasing since March 2020. We anticipate that our effort is of interest to researchers, practitioners, and law enforcement agencies focused on the study and safeguard of public health. ",The illicit trade of COVID-19 vaccines on the dark web
206,1359770387572281345,973232482660290565,Oscar Macias,"['Paper I: In this paper, spearheaded by awesome PhD student @songdeheng, we have found evidence for inverse Compton emission from millisecond pulsar populations in globular clusters <LINK> <LINK>', '1. Pulsars have long been predicted to be sources of electron/positron pairs. When such particles are injected in the wind regions, or inner magnetospheres of pulsars, and also into the interstellar medium, they can inverse-Compton scatter ambient photons to very high energies.', '2. However, predicting and observing high energy gamma-ray emission from rotation-powered millisecond pulsars (MSPs) presents a number of challenges compared to that of the non-recycled pulsar population (normal pulsars).', '3. We have first looked for potential correlations between the gamma-ray luminosity of globular clusters and the interstellar radiation field energy density. https://t.co/k5SXyV7pKX', '4. Though, intriguingly, the statistical significance of the correlation above is 3.8 sigma, that correlation might also be explained  by hidden correlations between the gamma-ray luminosity and the globular cluster stellar mass.', '5. So, we decided to look for evidence of inverse Compton gamma-rays in the measured photon spectra. The typical spectrum of a gamma-ray globular cluster looks like this. https://t.co/nqswSc1dr4', '6. We stacked the spectra from the 30 globular clusters detected in gamma rays, and found a ~8 sigma significance detection for a inverse Compton component (here modeled with a simple power law [blue line])']",https://arxiv.org/abs/2102.00061,"Millisecond pulsars are very likely the main source of gamma-ray emission from globular clusters. However, the relative contributions of two separate emission processes--curvature radiation from millisecond pulsar magnetospheres vs. inverse Compton emission from relativistic pairs launched into the globular cluster environment by millisecond pulsars--have long been unclear. To address this, we search for evidence of inverse Compton emission in 8-year $\textit{Fermi}$-LAT data from the directions of 157 Milky Way globular clusters. We find a mildly statistically significant (3.8$\sigma$) correlation between the measured globular cluster gamma-ray luminosities and their photon field energy densities. However, this may also be explained by a hidden correlation between the photon field densities and the stellar encounter rates of globular clusters. Analysed $\textit{in toto}$, we demonstrate that the gamma-ray emission of globular clusters can be resolved spectrally into two components: i) an exponentially cut-off power law and ii) a pure power law. The latter component--which we uncover at a significance of 8.2$\sigma$--has a power index of 2.79 $\pm$ 0.25. It is most naturally interpreted as inverse Compton emission by cosmic-ray electrons and positrons injected by millisecond pulsars. We find the luminosity of this power-law component is comparable to, or slightly smaller than, the luminosity of the curved component, suggesting the fraction of millisecond pulsar spin-down luminosity into relativistic leptons is similar to the fraction of the spin-down luminosity into prompt magnetospheric radiation. ","Evidence for a high-energy tail in the gamma-ray spectra of globular
  clusters"
207,1359520140438691841,268337552,Nicolas Kourtellis,"['<LINK> We find great discrepancies in cookie syncing, pixel tracking &amp; device fingerprinting within 100+ Left, Center&amp;Right-leaning Indian news sites &amp; third-parties.\n@vibhor711,@vekariayash,@pk_plus_plus,@sangeetamptra,@shounakset,@sakthibalanm,@nishanthsastry <LINK>']",https://arxiv.org/abs/2102.03656,"India is experiencing intense political partisanship and sectarian divisions. The paper performs, to the best of our knowledge, the first comprehensive analysis on the Indian online news media with respect to tracking and partisanship. We build a dataset of 103 online, mostly mainstream news websites. With the help of two experts, alongside data from the Media Ownership Monitor of the Reporters without Borders, we label these websites according to their partisanship (Left, Right, or Centre). We study and compare user tracking on these sites with different metrics: numbers of cookies, cookie synchronizations, device fingerprinting, and invisible pixel-based tracking. We find that Left and Centre websites serve more cookies than Right-leaning websites. However, through cookie synchronization, more user IDs are synchronized in Left websites than Right or Centre. Canvas fingerprinting is used similarly by Left and Right, and less by Centre. Invisible pixel-based tracking is 50% more intense in Centre-leaning websites than Right, and 25% more than Left. Desktop versions of news websites deliver more cookies than their mobile counterparts. A handful of third-parties are tracking users in most websites in this study. This paper, by demonstrating intense web tracking, has implications for research on overall privacy of users visiting partisan news websites in India. ",Under the Spotlight: Web Tracking in Indian Partisan News Websites
208,1358763306895486985,1277975431723900936,Laura Rogers,"['New co-authored paper on arXiv today! \nWe find evidence that exoplanetary bodies have the same refractory composition as their host star\n<LINK>', 'Some white dwarfs show evidence that they have accreted planetary material. Analysing their spectra reveals the bulk composition of the exoplanetary material that they have accreted.', 'We analyse the abundances of a wide binary system consisting of a K-dwarf and a white dwarf which has accreted planetary material. As binary pairs are chemically homogeneous, the K-dwarf is used as a proxy for the progenitor composition of the white dwarf.', 'The abundances of the K-dwarf and the planetary material polluting the atmosphere of the white dwarf are consistent with the hypothesis that exoplanetary bodies have the same refractory composition as their host stars.', 'Check out the paper for more detail, discussions and caveats', '@MarcoMarco1822 Potentially! Although planet formation is very complicated. It may help to target certain stars when looking for potential exoplanets where life as we know it could exist!', '@MarcoMarco1822 We can use observations of polluted white dwarfs to infer if there was a core/mantle or even water sometimes, however, these planets have been destroyed so this makes it hard to find life there!']",https://arxiv.org/abs/2102.02843,"Planets and stars ultimately form out of the collapse of the same cloud of gas. Whilst planets, and planetary bodies, readily loose volatiles, a common hypothesis is that they retain the same refractory composition as their host star. This is true within the Solar System. The refractory composition of chondritic meteorites, Earth and other rocky planetary bodies are consistent with solar, within the observational errors. This work aims to investigate whether this hypothesis holds for exoplanetary systems. If true, the internal structure of observed rocky exoplanets can be better constrained using their host star abundances. In this paper, we analyse the abundances of the K-dwarf, G200-40, and compare them to its polluted white dwarf companion, WD 1425+540. The white dwarf has accreted planetary material, most probably a Kuiper belt-like object, from an outer planetary system surviving the star's evolution to the white dwarf phase. Given that binary pairs are chemically homogeneous, we use the binary companion, G200-40, as a proxy for the composition of the progenitor to WD 1425+540. We show that the elemental abundances of the companion star and the planetary material accreted by WD 1425+540 are consistent with the hypothesis that planet and host-stars have the same true abundances, taking into account the observational errors. ","Host-star and exoplanet compositions: a pilot study usinga wide binary
  with a polluted white dwarf"
209,1357350848792301575,460489687,Juan Mateos Garcia,"['""The privatisation of AI researchers""\n\nIn our latest working paper we study AI researcher career transitions between academic &amp; industry.\n<LINK>\n\nOpening q: How do we preserve a public interest AI research sphere in the face of strong industry demand for talent? <LINK>', '1. We study career transitions since 2020 using data from Microsoft Academic Graph. \n\nWe find a growing flow of researchers from academia to industry [1], particularly from elite universities [2], and particularly to tech companies [3] https://t.co/JT8I2dUCub', '2. Our survival analysis shows that industry tends to recruit highly cited researchers with a specialism in deep learning. We also find evidence that female researchers are less likely to transition into industry after we account for other variables. https://t.co/VO1ietz8HB', '3. We also compare the ""productivity""* of researchers who transition into industry with peers who stay in academia. ""Switchers"" see a bump in productivity which is offset over time. Are they burning out, over-specialising, being captured...? ü§∑\n\n____\n* Proxied w/ citation rank https://t.co/FownbBZ4oe', 'Conclusion: Our results raise concerns about a potential hollowing out AI public research &amp; knowledge as talented researchers transition into industry and become less productive over time.\n\nThis provides a rationale to invest in public AI research &amp; preserve its independence. https://t.co/3nh0YqG25h', 'PS. Lots of potential avenues to explore in further work eg:\n\nIt has bee fun to work with @kstathou, @Daniel_S_Hain &amp; @RJurowetzki on this one :-) https://t.co/xdMnvLhQIY', '* typo in # 2: since 2000']",https://arxiv.org/abs/2102.01648,"The private sector is playing an increasingly important role in basic Artificial Intelligence (AI) R&D. This phenomenon, which is reflected in the perception of a brain drain of researchers from academia to industry, is raising concerns about a privatisation of AI research which could constrain its societal benefits. We contribute to the evidence base by quantifying transition flows between industry and academia and studying its drivers and potential consequences. We find a growing net flow of researchers from academia to industry, particularly from elite institutions into technology companies such as Google, Microsoft and Facebook. Our survival regression analysis reveals that researchers working in the field of deep learning as well as those with higher average impact are more likely to transition into industry. A difference-in-differences analysis of the effect of switching into industry on a researcher's influence proxied by citations indicates that an initial increase in impact declines as researchers spend more time in industry. This points at a privatisation of AI knowledge compared to a counterfactual where those high-impact researchers had remained in academia. Our findings highlight the importance of strengthening the public AI research sphere in order to ensure that the future of this powerful technology is not dominated by private interests. ","The Privatization of AI Research(-ers): Causes and Potential
  Consequences -- From university-industry interaction to public research
  brain-drain?"
