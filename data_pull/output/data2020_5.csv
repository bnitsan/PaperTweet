,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1271501734045761537,864540407383937024,Dr. Estelle Smith 🌟 (she/her),"['The new @CaringBridge Planner is a great way to coordinate support during health crises &amp; our new @acmtochi paper explores design implications for features like this. Pre-print here: <LINK> Results spoilers below! Thread 1/6 <LINK>', ""Results Spoiler #1: Friends and family of patients don't always align with patients/caregivers on what types of instrumental support they are interested in providing. However, tools like the Planner can possibly help to re-align these user groups. 2/6"", 'Results Spoiler #2: Emotional and instrumental support are important to CaringBridge users. However, we found that *prayer support* is the most important support category to these users. This suggests a significantly understudied design opportunity in online communities. 3/6', 'Results Spoiler #3: People generally have more trust in their closest social connections to provide instrumental support than more distant acquaintances or businesses. (No surprise there!) 4/6', 'Results Spoiler #4: People generally trust traditional businesses more than sharing economy apps to help out during health crises. But in some cases, their trust in app-based businesses like @lyft @uber or @gofundme is greater than their trust in taxi services or banks. 5/6', ""I'm looking forward to tweeting out a full blog post once the paper has been officially published, but here's a first peek at the pre-print! :) https://t.co/9Um0Z838ee 👀  Thanks for reading! 6/6"", ""@andrewmiller @CaringBridge @acmtochi That's great to hear @andrewmiller. Will keep an eye out for your upcoming work! Support coordination is challenging, but there are important opportunities for tech to make it a little easier. It's certainly a big focus at CaringBridge right now, as the new Planner shows.""]",https://arxiv.org/abs/2005.11884,"Online Health Communities (OHCs) are known to provide substantial emotional and informational support to patients and family caregivers facing life-threatening diagnoses like cancer and other illnesses, injuries, or chronic conditions. Yet little work explores how OHCs facilitate other vital forms of social support, especially instrumental support. We partner with CaringBridge.org---a prominent OHC for journaling about health crises---to complete a two-phase study focused on instrumental support. Phase one involves a content analysis of 641 CaringBridge updates. Phase two is a survey of 991 CaringBridge users. Results show that patients and family caregivers diverge from their support networks in their preferences for specific instrumental support types. Furthermore, ``prayer support'' emerged as the most prominent support category across both phases. We discuss design implications to accommodate divergent preferences and to expand the instrumental support network. We also discuss the need for future work to empower family caregivers and to support spirituality. ","""I Cannot Do All of This Alone"": Exploring Instrumental and Prayer
  Support in Online Health Communities"
1,1268197452563259395,151982094,Tom Richardson Ph. D.,['My new math paper at the arxiv.\n\n<LINK>'],https://arxiv.org/abs/2005.08939,We prove that the inverse of the Hankel matrix of the reciprocals of the Catalan numbers has integer entries. We generalize the result to an infinite family of generalized Catalan numbers. The Hankel matrices that we consider are associated with orthogonal polynomials that are variants of Jacobi polynomials. Our proofs use these polynomials and computer algebra based on Wilf-Zeilberger theory. ,Catalan Numbers and Jacobi Polynomials
2,1267437876905738240,2283510367,Eamonn Kerins,"[""New paper on arXiv (<LINK>) led by my PhD student, David Specht, describing our shiny new MaBulS-2 microlensing simulator. Try for yourself at <LINK> \n\nWe've tested it against the massive 8,000 event dataset from OGLE-IV. Spot the difference? <LINK>"", 'The really amazing thing about MaBulS-2 is that it allows anyone to request a bespoke calculation in a few seconds that would take days of high-performance parallel computing to calculate directly!', ""With MaBulS-2 we have a theoretical tool that is fit the the era of large-scale microlensing datasets. We're now much better placed to optimize how future billion-dollar surveys like @NASARoman and @ESA_Euclid can best look for cool exoplanets. #microlensing #exoplanets""]",https://arxiv.org/abs/2005.14668,"Galactic microlensing datasets now comprise in excess of $10^4$ events, and with the advent of next generation microlensing surveys that may be undertaken with facilities such as the Rubin Observatory (formerly LSST) and Roman Space Telescope (formerly WFIRST), this number will increase significantly. So too will the fraction of events with measurable higher order information such as finite source effects and lens-source relative proper motion. Analysing such data requires a more sophisticated Galactic microlens modeling approach. We present a new second-generation Manchester-Besan\c{c}on Microlensing Simulator (MaB$\mu$lS-2), which uses a version of the Besan\c{c}on population synthesis Galactic model that provides good agreement with stellar kinematics observed by HST towards the bulge. MaB$\mu$lS-2 provides high-fidelity signal-to-noise limited maps of the microlensing optical depth, rate and average timescale towards a 400 sq. degree region of the Galactic bulge in several optical to near-infrared pass-bands. The maps take full account of the unresolved stellar background as well as limb-darkened source profiles. Comparing MaB$\mu$lS-2 to the efficiency-corrected OGLE-IV 8,000 event sample shows a much improved agreement over the previous version of MaB$\mu$lS, and succeeds in matching even small-scale structural features in the OGLE-IV event rate map. However, there remains evidence for a small under-prediction in the event rate per source and over-prediction in timescale. MaB$\mu$lS-2 is available online (<www.mabuls.net>) to provide on-the-fly maps for user supplied cuts in survey magnitude, event timescale and relative proper motion. ","MaB$\mu$lS-2: high-precision microlensing modelling for the large-scale
  survey era"
3,1267386848843894786,1000951360404312065,Alexei Moiseev,"['It was not so quick, but finally the MaNGaL ""instrumental"" paper was submitted! Mapper of Narrow Galaxy Lines (MaNGaL): new tunable filter imager for Caucasian telescopes\n<LINK> #SAIMSU25m #BTA6m <LINK>']",https://arxiv.org/abs/2005.14598,"We described the design and operation principles of a new tunable-filter photometer developed for the 1-m telescope of the Special Astrophysical Observatory of the Russian Academy of Sciences and the 2.5-m telescope of the Sternberg Astronomical Institute of the Moscow State University. The instrument is mounted on the scanning Fabry-Perot interferometer operating in the tunable-filter mode in the spectral range of 460-800 nm with a typical spectral resolution of about 1.3 nm. It allows one to create images of galactic and extragalactic nebulae in the emission lines having different excitation conditions and to carry out diagnostics of the gas ionization state. The main steps of observations, data calibration, and reduction are illustrated by examples of different emission-line objects: galactic HII regions, planetary nebulae, active galaxies with extended filaments, starburst galaxies, and Perseus galaxy cluster. ","Mapper of Narrow Galaxy Lines (MaNGaL): new tunable filter imager for
  Caucasian telescopes"
4,1267272735127941120,618128128,Shuiwang Ji,['New paper alert: Non-Local Graph Neural Networks\n<LINK>'],https://arxiv.org/abs/2005.14612,"Modern graph neural networks (GNNs) learn node embeddings through multilayer local aggregation and achieve great success in applications on assortative graphs. However, tasks on disassortative graphs usually require non-local aggregation. In addition, we find that local aggregation is even harmful for some disassortative graphs. In this work, we propose a simple yet effective non-local aggregation framework with an efficient attention-guided sorting for GNNs. Based on it, we develop various non-local GNNs. We perform thorough experiments to analyze disassortative graph datasets and evaluate our non-local GNNs. Experimental results demonstrate that our non-local GNNs significantly outperform previous state-of-the-art methods on seven benchmark datasets of disassortative graphs, in terms of both model performance and efficiency. ",Non-Local Graph Neural Networks
5,1266808664054206466,49347058,Albert Zeyer,"['I\'m quite happy with our paper ""A New Training Pipeline for an Improved Neural Transducer"" (<LINK>) about variations of transducer models, label topology RNN-T, RNA / monotonic RNN-T, CTC, max approx for training. configs: <LINK>']",https://arxiv.org/abs/2005.09319,"The RNN transducer is a promising end-to-end model candidate. We compare the original training criterion with the full marginalization over all alignments, to the commonly used maximum approximation, which simplifies, improves and speeds up our training. We also generalize from the original neural network model and study more powerful models, made possible due to the maximum approximation. We further generalize the output label topology to cover RNN-T, RNA and CTC. We perform several studies among all these aspects, including a study on the effect of external alignments. We find that the transducer model generalizes much better on longer sequences than the attention model. Our final transducer model outperforms our attention model on Switchboard 300h by over 6% relative WER. ",A New Training Pipeline for an Improved Neural Transducer
6,1266527998448005122,374233623,Shane Barratt,"[""New paper, and it's pertinent to COVID studies!!!\n\nOptimal Representative Sample Weighting\n\nw/ \n@GuilleAngeris\n and Stephen Boyd (sadly not on twitter).\n\nPaper: <LINK>"", 'We consider the problem of assigning weights to data records or samples, with the goal of generating a representative weighting. We it as a convex problem, which trades off l(f,fdes) the inconsistency between sample and desired expected values, and a regularizer r(w). https://t.co/8wJiNik7yL', 'A very important special cases is raking, or iterative proportional fitting (https://t.co/57ZhCCbXke), which, according to Pew is the weighting method of choice for public pollsters. Iterative proportional fitting can be recovered with the loss and regularizer https://t.co/CzCAeMBb4s', '(In the appendix we give a thorough proof of how raking is simply block coordinate descent applied to the dual of this convex problem.) Another very cool special case is representative selection, where we allow exactly a small number of the entries in w to be nonzero https://t.co/7dySLSkHTu', 'Representative selection can be useful if you have a lot of samples, and want to select a much smaller number of them for further analysis or collect more data on them. This could be useful for Covid seroprevalence studies, by selecting a representative sample to test for antibd.', 'We propose a distributed method for solving these problems that is based on ADMM. It handily scales to very large problems (millions of data points and hundreds of features), and runs on a single core of a standard Intel/AMD CPU. https://t.co/KF8gS7nyyF', ""We apply our methodology to the CDC BRFSS dataset (https://t.co/vjWdyZbA9Q), which is the world's largest telephone survey. 437436 responses for over 275 questions in 2018! We consider state/age, sex, education/income, and general health as features."", 'We generate a skewed sample of 10000 samples, and consider as desired expected values the true expected values in the whole dataset. We solve the maximum entropy weighting problem, which results in 10000 weights with the histogram https://t.co/CzbF2DKtpx', 'We took height and weight, two unobserved features, and compared the CDF of each for our weighted and unweighted samples. The weighted distribution matches the true distribution much better! https://t.co/JkK97vff9h', 'Here it is for weight!\nWe also calculated the Kolmogorov-Smirnov (K-S) test statistic, finding the K-S for the weighted distribution for height to be 0.008 (0.078 for unweighted), and for height 0.009 (0.064 for unweighted). Lower is better. https://t.co/LkWPF1M9sP', 'Next we performed representative selection, selecting 500 of the 10000 samples that are representative. The loss for the subset we found was much lower than randomly sampling from the maximum entropy weights! https://t.co/QEiAw83wTq', ""That's it for now. If you're curious, go check out the paper/software, and let us know if you use it in your research! The software is very customizable, allowing you to mix and match various losses and regularizers.""]",https://arxiv.org/abs/2005.09065,"We consider the problem of assigning weights to a set of samples or data records, with the goal of achieving a representative weighting, which happens when certain sample averages of the data are close to prescribed values. We frame the problem of finding representative sample weights as an optimization problem, which in many cases is convex and can be efficiently solved. Our formulation includes as a special case the selection of a fixed number of the samples, with equal weights, i.e., the problem of selecting a smaller representative subset of the samples. While this problem is combinatorial and not convex, heuristic methods based on convex optimization seem to perform very well. We describe rsw, an open-source implementation of the ideas described in this paper, and apply it to a skewed sample of the CDC BRFSS dataset. ",Optimal Representative Sample Weighting
7,1266378580218589184,823957466,Hanna Wallach,"['Alright, people! New paper alert!!! (w/ @sulin_blodgett, @haldaume3, and @s010n) Prepare for a thread! \n\n<LINK>\n\nFirst, the incredible @sulin_blodgett surveyed 146 papers (yes, really!!!) analyzing ""bias"" in NLP systems. (1/8)', 'We found that motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ""bias"" is an inherently normative process. (2/8)', 'We also found that the papers\' proposed quantitative techniques for measuring or mitigating ""bias"" are often poorly matched to their motivations and do not engage with the relevant literature outside of NLP. (3/8)', 'Based on these findings, we describe a path forward by proposing three recommendations that should guide work analyzing ""bias"" in NLP systems. (4/8)', 'First, we call for a greater recognition of the relationships between language and social hierarchies. (5/8)', 'Second, we encourage researchers and practitioners to clearly articulate their conceptualizations of ""bias""---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, including the normative reasoning underlying these statements. (6/8)', 'Finally, we suggest centering work around the lived experiences of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities. (7/8)', 'You can find the paper here (also to appear at @aclmeeting):  https://t.co/eAUkMOjBYG \n\nLet us know what you think!!! (8/8)', ""@schock @aclmeeting They're listed in appendix B!"", '@krob @aclmeeting @sulin_blodgett @haldaume3 @s010n Thanks, Kevin!!! 😁❤️']",https://arxiv.org/abs/2005.14050,"We survey 146 papers analyzing ""bias"" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ""bias"" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating ""bias"" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ""bias"" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ""bias""---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities. ","Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP"
8,1266372249428176896,973860795246198784,Z.Wei,"['Our new paper is on arXiv! Really thank my collaborators Y. Nakata, T. Takayanagi, Y. Taki and K. Tamaoka (@harbour_gomahu) for bringing a fantastic collaboration experience to me! \n<LINK> <LINK>']",http://arxiv.org/abs/2005.13801,"We introduce a quantity, called pseudo entropy, as a generalization of entanglement entropy via post-selection. In the AdS/CFT correspondence, this quantity is dual to areas of minimal area surfaces in time-dependent Euclidean spaces which are asymptotically AdS. We study its basic properties and classifications in qubit systems. In specific examples, we provide a quantum information theoretic meaning of this new quantity as an averaged number of Bell pairs when the post-selection is performed. We also present properties of the pseudo entropy for random states. We then calculate the pseudo entropy in the presence of local operator excitations for both the two dimensional free massless scalar CFT and two dimensional holographic CFTs. We find a general property in CFTs that the pseudo entropy is highly reduced when the local operators get closer to the boundary of the subsystem. We also compute the holographic pseudo entropy for a Janus solution, dual to an exactly marginal perturbation of a two dimensional CFT and find its agreement with a perturbative calculation in the dual CFT. We show the linearity property holds for holographic states, where the holographic pseudo entropy coincides with a weak value of the area operator. Finally, we propose a mixed state generalization of pseudo entropy and give its gravity dual. ",Holographic Pseudo Entropy
9,1266327644397645824,3232627976,Khyati Malhan,"['New mechanism for the formation of gaps in ""accreted stellar streams"" (Section 4.2 of  <LINK>) -- presented as a video-graphic tour <LINK>. \nThere is a lot more detailed in the paper.']",https://arxiv.org/abs/2005.12919,"The steepness of the central density profiles of dark matter (DM) in low-mass galaxy halos (e.g. dwarf galaxies) is a powerful probe of the nature of DM. We propose a novel scheme to probe the inner profiles of galaxy subhalos using stellar streams. We show that the present day morphological and dynamical properties of accreted globular cluster (GC) streams - those produced from tidal stripping of GCs that initially evolved within satellite galaxies and later merged with the Milky Way (MW) - are sensitive to the central DM density profile and mass of their parent satellites. GCs that accrete within cuspy CDM subhalos produce streams that are physically wider and dynamically hotter than streams that accrete inside cored subhalos. A first comparison of MW streams ""GD-1"" and ""Jhelum"" (likely of accreted GC origin) with our simulations indicates a preference for cored subhalos. If these results hold up in future data, the implication is that either the DM cusps were erased by baryonic feedback, or their subhalos naturally possessed cored density profiles implying DM models beyond CDM. Moreover, accreted GC streams are highly structured and exhibit complex morphological features (e.g., parallel structures and ""spurs""). This implies that the accretion scenario can naturally explain the recently observed peculiarities in some of the MW streams. We also propose a novel mechanism for forming ""gaps"" in streams when the remnant of the parent subhalo later passes through the stream. This encounter can last a longer time (and have more of an impact) than the random encounters with DM subhalos previously considered, because the GC stream and its parent subhalo are on similar orbits with small relative velocities. Current and future surveys of the MW halo will uncover numerous faint stellar streams and provide the data needed to substantiate our preliminary tests with this new probe of DM. ",Probing the nature of dark matter with accreted globular cluster streams
10,1266263305003216904,77990334,Jari Saramäki,"['And now for something completely different: speciation. Our new review paper, accepted to Phil Trans R Soc B, with @InaSatok @simonhmartin @HeikkiHelantera @jonna_kulmuni [TL;DR for network scientists: networks are involved &amp; there is a lot to discover]. <LINK>']",http://arxiv.org/abs/2005.13790,"All genes interact with other genes, and their additive effects and epistatic interactions affect an organism's phenotype and fitness. Recent theoretical and empirical work has advanced our understanding of the role of multi-locus interactions in speciation. However, relating different models to one another and to empirical observations is challenging. This review focuses on multi-locus interactions that lead to reproductive isolation (RI) through reduced hybrid fitness. We first review theoretical approaches and show how recent work incorporating a mechanistic understanding of multi-locus interactions recapitulates earlier models, but also makes novel predictions concerning the build-up of RI. These include high variance in the build-up rate of RI among taxa, the emergence of strong incompatibilities producing localised barriers to introgression, and an effect of population size on the build-up of RI. We then review recent experimental approaches to detect multi-locus interactions underlying RI using genomic data. We argue that future studies would benefit from overlapping methods like Ancestry Disequilibrium scans, genome scans of differentiation and analyses of hybrid gene expression. Finally, we highlight a need for further overlap between theoretical and empirical work, and approaches that predict what kind of patterns multi-locus interactions resulting in incompatibilities will leave in genome-wide polymorphism data. ",Multi-locus interactions and the build-up of reproductive isolation
11,1266224248260984832,19619556,Nathan Schneider,"['New draft of the #ModularPolitics paper from the Metagovernance Project, now up: <LINK>']",https://arxiv.org/abs/2005.13701,"Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies--juries, political parties, term limits, and formal debates, to name a few--are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance. ",Modular Politics: Toward a Governance Layer for Online Communities
12,1266159178747449355,1227677558055063555,Maria Veiga,"['New paper just dropped: ""An arbitrary high-order Spectral Difference method for the induction equation"" <LINK> ;)\n#numerics #induction #highorder']",https://arxiv.org/abs/2005.13563,"We study in this paper three variants of the high-order Discontinuous Galerkin (DG) method with Runge-Kutta (RK) time integration for the induction equation, analysing their ability to preserve the divergence free constraint of the magnetic field. To quantify divergence errors, we use a norm based on both a surface term, measuring global divergence errors, and a volume term, measuring local divergence errors. This leads us to design a new, arbitrary high-order numerical scheme for the induction equation in multiple space dimensions, based on a modification of the Spectral Difference (SD) method [1] with ADER time integration [2]. It appears as a natural extension of the Constrained Transport (CT) method. We show that it preserves $\nabla\cdot\vec{B}=0$ exactly by construction, both in a local and a global sense. We compare our new method to the 3 RKDG variants and show that the magnetic energy evolution and the solution maps of our new SD-ADER scheme are qualitatively similar to the RKDG variant with divergence cleaning, but without the need for an additional equation and an extra variable to control the divergence errors. [1] Liu Y., Vinokur M., Wang Z.J. (2006) Discontinuous Spectral Difference Method for Conservation Laws on Unstructured Grids. In: Groth C., Zingg D.W. (eds) Computational Fluid Dynamics 2004. Springer, Berlin, Heidelberg [2] Dumbser M., Castro M., Par\'es C., Toro E.F (2009) ADER schemes on unstructured meshes for nonconservative hyperbolic systems: Applications to geophysical flows. In: Computers & Fluids, Volume 38, Issue 9 ","An arbitrary high-order Spectral Difference method for the induction
  equation"
13,1266081705779097600,1968563462,anhad mohananey,"['New Paper: (w @kelina1124 and @sleepinyourhat) -- IWPT, ACL 2020. <LINK>\n\nGiven the low-self agreement across runs for standard Unsupervised Parsing (UP) models, we wanted to answer whether self-training could be used to improve performance of UP models.', 'Our model, SS-PRPN, a self-trained version of PRPN improves upon the baseline PRPN (tuned) model by 8.1%, setting a new SoTA.', 'We also experiment with using the self-training architecture for parsing with limited amount of gold data (ultra-low-resource), and find that we generally beat both supervised and UP baselines.']",https://arxiv.org/abs/2005.13455,"Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our model's predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a) such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1% F1 and the previous state of the art by 1.6% F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings. ",Self-Training for Unsupervised Parsing with PRPN
14,1266046361990037505,785284514947932160,Thomas Davidson,"[""I'm excited to share a new paper with @fuchsiacheer. We use structural topic modeling to uncover potential racial biases in an abusive language detection dataset. \n\n<LINK>\n\n1/n <LINK>"", 'We train an STM on the corpus, including covariates for the predicted probability each tweet is written in African-American English and whether it is annotated as hateful or abusive.\n\n2/n https://t.co/nHvnIf8p72', 'We find that two topics appear to account for most of the abusive language. The prevalence of one topic is positively and strongly associated with AAE, while the other is weakly associated.\n\n3/n https://t.co/EqQAz8Iocz', 'This suggests that unsupervised methods can be useful for identifying biases in annotated text corpora. But further work is needed to explore how such methods can be used to mitigate bias.\n\nWe look forward to presenting this at the @icwsm Data Challenge workshop.\n\n4/4.', '@JaeJaeykim2 I have also been planning some experimental work in this area but have to finish my dissertation first! It would be great to chat sometime.', '@ryanwesslen Thanks, Ryan!']",http://arxiv.org/abs/2005.13041,We use structural topic modeling to examine racial bias in data collected to train models to detect hate speech and abusive language in social media posts. We augment the abusive language dataset by adding an additional feature indicating the predicted probability of the tweet being written in African-American English. We then use structural topic modeling to examine the content of the tweets and how the prevalence of different topics is related to both abusiveness annotation and dialect prediction. We find that certain topics are disproportionately racialized and considered abusive. We discuss how topic modeling may be a useful approach for identifying bias in annotated data. ,"Examining Racial Bias in an Online Abuse Corpus with Structural Topic
  Modeling"
15,1266035801604923395,2530947115,Max Tegmark,"['We just posted a new #AI paper on how to auto-discover laws of physics from raw warped video with machine learning. It took Silviu &amp; me a year to get this working, using ideas inspired by #generalrelativity &amp; high-dimensional #knottheory - phew! <LINK> <LINK>']",https://arxiv.org/abs/2005.11212,"We present a method for unsupervised learning of equations of motion for objects in raw and optionally distorted unlabeled video. We first train an autoencoder that maps each video frame into a low-dimensional latent space where the laws of motion are as simple as possible, by minimizing a combination of non-linearity, acceleration and prediction error. Differential equations describing the motion are then discovered using Pareto-optimal symbolic regression. We find that our pre-regression (""pregression"") step is able to rediscover Cartesian coordinates of unlabeled moving objects even when the video is distorted by a generalized lens. Using intuition from multidimensional knot-theory, we find that the pregression step is facilitated by first adding extra latent space dimensions to avoid topological problems during training and then removing these extra dimensions via principal component analysis. ",Symbolic Pregression: Discovering Physical Laws from Distorted Video
16,1266020849758302208,21611239,Sean Carroll,"['Quantum Mereology: how do we decide, given an arbitrary quantum-mechanical system, the best way to divide it into subsystems, or into ""system"" and ""environment""? New paper by @ashmeetastro and me.\n<LINK>', 'The rough answer is that there is a best way of factorizing Hilbert space so that the system looks as classical as possible. You can quantify it by minimizing the growth of entropy.', 'But mostly we are just happy to get to use ""Mereology"" (the study of the relations between wholes and parts) in the title of a physics paper.\nhttps://t.co/Azg8EPlKSE', '@PramodhYapa @ashmeetastro Looks like a typo to me!']",https://arxiv.org/abs/2005.12938,"We study the question of how to decompose Hilbert space into a preferred tensor-product factorization without any pre-existing structure other than a Hamiltonian operator, in particular the case of a bipartite decomposition into ""system"" and ""environment."" Such a decomposition can be defined by looking for subsystems that exhibit quasi-classical behavior. The correct decomposition is one in which pointer states of the system are relatively robust against environmental monitoring (their entanglement with the environment does not continually and dramatically increase) and remain localized around approximately-classical trajectories. We present an in-principle algorithm for finding such a decomposition by minimizing a combination of entanglement growth and internal spreading of the system. Both of these properties are related to locality in different ways. This formalism could be relevant to the emergence of spacetime from quantum entanglement. ","Quantum Mereology: Factorizing Hilbert Space into Subsystems with
  Quasi-Classical Dynamics"
17,1265970271904768001,733933863282585600,Cameron,"[""After months of work, our new computer vision dataset, the Northumberland Dolphin Dataset, is now available! Our work is presented in an @fgvcworkshop paper at this year's @CVPR. Paper incl dataset link: <LINK>"", 'Thanks to everyone involved incl. @ccbdcdt @profnickwright @KN_Richardson @Sealdiver @PerBerggren1 @MSharpe_ @GeorgiaWA95 @stevemcgough']",http://arxiv.org/abs/2005.13359,"We introduce the Northumberland Dolphin Dataset 2020 (NDD20), a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation. All data contained in NDD20 was obtained via manual collection in the North Sea around the Northumberland coastline, UK. We present experimentation using standard deep learning network architecture trained using NDD20 and report baselines results. ","NDD20: A large-scale few-shot dolphin dataset for coarse and
  fine-grained categorisation"
18,1265943687147683840,2742282828,Andreea Font,"['New paper by Shaun Brown et al connecting the structure of dark matter haloes with the primoridal power spectrum <LINK>', '""the previously identified universality of the structure of dark matter haloes is mostly a consequence of adopting a narrow range of (CMB-normalised) initial conditions for the simulations.""']",https://arxiv.org/abs/2005.12933,"A large body of work based on collisionless cosmological N-body simulations going back over two decades has advanced the idea that collapsed dark matter haloes have simple and approximately universal forms for their mass density and pseudo-phase space density (PPSD) distributions. However, a general consensus on the physical origin of these results has not yet been reached. In the present study, we explore to what extent the apparent universality of these forms holds when we vary the initial conditions (i.e., the primordial power spectrum of density fluctuations) away from the standard CMB-normalised case, but still within the context of LCDM with a fixed expansion history. Using simulations that vary the initial amplitude and shape, we show that the structure of dark matter haloes retains a clear memory of the initial conditions. Specifically, increasing (lowering) the amplitude of fluctuations increases (decreases) the concentration of haloes and, if pushed far enough, the density profiles deviate strongly from the NFW form that is a good approximation for the CMB-normalised case. Although, an Einasto form works well. Rather than being universal, the slope of the PPSD (or pseudo-entropy) profile steepens (flattens) with increasing (decreasing) power spectrum amplitude and can exhibit a strong halo mass dependence. Our results therefore indicate that the previously identified universality of the structure of dark matter haloes is mostly a consequence of adopting a narrow range of (CMB-normalised) initial conditions for the simulations. Our new suite provides a useful test-bench against which physical models for the origin of halo structure can be validated. ","Connecting the structure of dark matter haloes to the primordial power
  spectrum"
19,1265644032291717123,2797254210,Michael A. Perlin,"['New paper from a summer project at Argonne National Lab (@argonne)!\n\nWe present an improved method to ""cut"" quantum circuits into smaller sub-circuits. We also show that circuit cutting can do *better* than running the full circuit to estimate its output.\n\n<LINK>']",https://arxiv.org/abs/2005.12702,"We introduce maximum likelihood fragment tomography (MLFT) as an improved circuit cutting technique for running clustered quantum circuits on quantum devices with a limited number of qubits. In addition to minimizing the classical computing overhead of circuit cutting methods, MLFT finds the most likely probability distribution for the output of a quantum circuit, given the measurement data obtained from the circuit's fragments. We demonstrate the benefits of MLFT for accurately estimating the output of a fragmented quantum circuit with numerical experiments on random unitary circuits. Finally, we show that circuit cutting can estimate the output of a clustered circuit with higher fidelity than full circuit execution, thereby motivating the use of circuit cutting as a standard tool for running clustered circuits on quantum hardware. ",Quantum Circuit Cutting with Maximum Likelihood Tomography
20,1265635419699515396,1403815458,Amir Siraj,"['In a new paper on the arXiv today, we propose a way to figure out whether or not Planet Nine, if it exists, could be a black hole: <LINK>']",https://arxiv.org/abs/2005.12280,"Planet Nine has been proposed to potentially be a black hole in the outer solar system. We investigate the accretion flares that would result from impacts of small Oort cloud objects, and find that the upcoming LSST observing program will be able to either rule out or confirm Planet Nine as a black hole within a year. We also find that LSST could rule out or confirm the existence of trapped planet-mass black holes out to the edge of the Oort cloud, indirectly probing the dark matter fraction in subsolar mass black holes and potentially improving upon current limits by orders of magnitude. ",Searching for Black Holes in the Outer Solar System with LSST
21,1265630473885188096,3433220662,Anthony Bonato,"['New paper up on @arxiv, joint with my post-docs Melissa Huggan and Trent Marbach: The localization number of designs  <LINK>']",https://arxiv.org/abs/2005.12780,"We study the localization number of incidence graphs of designs. In the localization game played on a graph, the cops attempt to determine the location of an invisible robber via distance probes. The localization number of a graph $G$, written $\zeta(G)$, is the minimum number of cops needed to ensure the robber's capture. We present bounds on the localization number of incidence graphs of balanced incomplete block designs. Exact values of the localization number are given for the incidence graphs of projective and affine planes. Bounds are given for Steiner systems and for transversal designs. ",The localization number of designs
22,1265563243378085888,2352149191,Mimmo Nardiello,"['My 10th paper as 1st author and maybe one of the hardest works I have done so far. Many new candidate worlds orbiting stars in open clusters!🪐\nPaper #10 come primo autore, forse uno dei piu` impegnativi. Tanti nuovi candidati pianeti in ammassi aperti!😀\n<LINK>', 'This is also my first work as 1st author since when I work at @LAM_Marseille  as @CNES fellow! 😀', '@AdrienCoffinet Yes! Pathos 1 was discovered in paper I (Nardiello et al. 2019). \nAn updated list of PATHOS candidates is now available here:\nhttps://t.co/tGwboKyTfg']",https://arxiv.org/abs/2005.12281,"The scope of the project ""A PSF-based Approach to TESS High Quality data Of Stellar clusters"" (PATHOS) is the extraction and analysis of high-precision light curves of stars in stellar clusters and young associations for the identification of candidate exoplanets and variable stars. The cutting-edge tools used in this project allow us to measure the real flux of stars in dense fields, minimising the effects due to contamination by neighbour sources. We extracted about 200 000 light curves of stars in 645 open clusters located in the southern ecliptic hemisphere and observed by TESS during the first year of its mission. We searched for transiting signals and we found 33 objects of interest, 11 of them are strong candidate exoplanets. Because of the limited S/N, we did not find any Earth or super-Earth. We identified two Neptune-size planets orbiting stars with $R_{\star}<1.5\,R_{\odot}$, implying a frequency $f_{\star}=1.34 \pm 0.95\,\%$, consistent with the frequency around field stars. The 7 Jupiter candidates around stars with $R_{\star}<\,1.5R_{\odot}$ imply a frequency $f_{\star}=0.19\pm 0.07\,\%$, smaller than in the field. A more complete estimate of the survey completeness and false positive rate is needed to confirm these results. Light curves used in this work will be made available to the astronomical community on the Mikulski Archive for Space Telescope under the project PATHOS. ","A PSF-based Approach to TESS High quality data Of Stellar clusters
  (PATHOS) -- II. Search for exoplanets in open clusters of the southern
  ecliptic hemisphere and their frequency"
23,1265481552860811264,795877354266456064,KoheiKamadaPhys,"['Our new paper has appeared! \n<LINK>\nWe studied the false vacuum decay (Minkowski to AdS) catalyzed by a black hole, which found to be enhanced compared to the usual CdL.', 'We took into account the thermal mass to the potential originated by the Hawking radiation to construct the bounce solution. We expected that it would reduce the decay rate.', 'But it turned out its effect is negligibly small unless the coupling between the tunneling scalar and the Hawking radiation is extremely large.', 'This result suggests together with the electroweak vacuum metastability, small primordial BHs should not be formed in the entire history of the Universe otherwise our Universe should have been collapsed into the unwanted AdS vacuum. \nOr the electroweak vacuum is completely stable']",https://arxiv.org/abs/2005.12808,"False vacuum decay is a key feature in quantum field theories and exhibits a distinct signature in the early Universe cosmology. It has recently been suggested that the false vacuum decay is catalyzed by a black hole (BH), which might cause the catastrophe of the Standard Model Higgs vacuum if primordial BHs are formed in the early Universe. We investigate vacuum phase transition of a scalar field around a radiating BH with taking into account the effect of Hawking radiation. We find that the vacuum decay rate slightly decreases in the presence of the thermal effect since the scalar potential is stabilized near the horizon. However, the stabilization effect becomes weak at the points sufficiently far from the horizon. Consequently, we find that the decay rate is not significantly changed unless the effective coupling constant of the scalar field to the radiation is extremely large. This implies that the change of the potential from the Hawking radiation does not help prevent the Standard Model Higgs vacuum decay catalyzed by a BH. ","On catalyzed vacuum decay around a radiating black hole and the crisis
  of the electroweak vacuum"
24,1265463725089308678,29149902,Isaac Tamblyn,['Our new reinforcement learning paper is up on the arXiv. Sometimes measurements are expensive :)\n\n<LINK>'],https://arxiv.org/abs/2005.12697,"Standard reinforcement learning (RL) algorithms assume that the observation of the next state comes instantaneously and at no cost. In a wide variety of sequential decision making tasks ranging from medical treatment to scientific discovery, however, multiple classes of state observations are possible, each of which has an associated cost. We propose the active measure RL framework (Amrl) as an initial solution to this problem where the agent learns to maximize the costed return, which we define as the discounted sum of rewards minus the sum of observation costs. Our empirical evaluation demonstrates that Amrl-Q agents are able to learn a policy and state estimator in parallel during online training. During training the agent naturally shifts from its reliance on costly measurements of the environment to its state estimator in order to increase its reward. It does this without harm to the learned policy. Our results show that the Amrl-Q agent learns at a rate similar to standard Q-learning and Dyna-Q. Critically, by utilizing an active strategy, Amrl-Q achieves a higher costed return. ",Active Measure Reinforcement Learning for Observation Cost Minimization
25,1265458238469713920,549460404,吉田 紅 (Beni Yoshida),"['A new paper. \n新しい論文です。\n<LINK>', 'AdS/CFTでの計算複雑性に関連するパラドックスについてです。\n「量子効果を使って動く量子コンピューターですが、もし、重力の効果も使うことでさらに計算速度を上げることができるかどうか？」\nという問いがあります。量子チャーチ＝チューリングのテーゼと言われています。', 'AdS/CFT対応に当てはめると、重力側で起こっていることは、全てCFT側で（効率的に）シミュレートできてしまうはずだという主張になります。', 'これまで重力効果を用いることで計算速度を上げられるという例は存在しなかったのですが、もしかしたら出来るのでは？という提案が最近なされました。', '重力側ではウァームホールの長さは簡単に測定できそうですが、CFT側ではその対応物（複雑性）は簡単に測定できない。なので、複雑性を推定するという計算タスクは、重力側ではすごく簡単だけど、量子側ではすごく難しいということになります。', 'これだと、量子チャーチ＝チューリングのテーゼが間違っていることになって、困ってしまう、というパラドックス（パズル？）ですね。', 'で、解決方法なんですが、そもそもの誤りは、重力側とCFT側の対応ルール（辞書とか言われてます）が不変だと仮定していることです。', '実際に観測者がブラックホールの内部に入ろうとすると、強いbackreactionが起こります。そしてその度に、ブラックホール内部の自由度の記述が動的に変わっていきます。なので、内部にあるホーキング輻射のペアの記述も観測者に応じて変化します。', 'ちなみに、これは、ブラックホールがscrambleすることを仮定すると厳密に証明できる、量子情報理論からの主張です。', 'ウァームホールの長さを効率的に測ろうとすれば、何人かの観測者がブラックホール内部に入る必要があるのですが、各人がbackreactionを起こすので内部で会うことはできません。そもそも内部の構造が変わってしまっているので。', 'なので、量子チャーチ＝チューリングのテーゼに対する反例にはなっていません。', 'ちなみに、同じ議論でファイヤーウォール問題も回避できます。', 'OTOCとdisentanglingの関係の定理と、内部自由度の記述方法。\nこの２つがシンプルだけど強力すぎるので、こればっかり使って論文を書いている気がしますが・・・。\nもし何か他に面白いパラドックスがあるならば教えていただけると嬉しいです・・・。', 'ちなみに、このパラドックス自体は、Bouland-Fefferman-Vaziraniが最初で、Susskindが最近拡張したものを提唱しました。\nそして、BFVの論文の存在を最初に僕に教えてくれた森前さんに感謝です。']",https://arxiv.org/abs/2005.12491,"Recently a certain conceptual puzzle in the AdS/CFT correspondence, concerning the growth of quantum circuit complexity and the wormhole volume, has been identified by Bouland-Fefferman-Vazirani and Susskind. In this note, we propose a resolution of the puzzle and save the quantum Extended Church-Turing thesis by arguing that there is no computational shortcut in measuring the volume due to gravitational backreaction from bulk observers. A certain strengthening of the firewall puzzle from the computational complexity perspective, as well as its potential resolution, is also presented. ",Remarks on Black Hole Complexity Puzzle
26,1265451863006216202,16015499,ProfGhristMath,"['1/ announcing a new paper w/@_jakobhansen \n<LINK>\nread on to learn about:\n&gt; social networks \n&gt; opinion dynamics (e.g. political discourse)\n&gt; sheaf cohomology\n&gt; harmonic opinions <LINK>', '2/ in classical opinion dynamics, everyone in a social network (graph G) has a real-valued opinion on topic X (+/-).  using the graph laplacian to define diffusion, any initial opinion distribution flows to consensus. \ncool. but that’s not how political discourse goes, right?', '3/ we develop a new model using sheaves.  a sheaf over a graph G valued in inner-product spaces attaches vector spaces to vertices &amp; edges of G.  these are “connected” with linear transformations.  sheaves are bundle-like data structures that can model lots of systems. https://t.co/SprNHAM9lC', '4/ we introduce a *discourse sheaf*\n&gt; each person has an opinion space with basis a set of topics important to this person\n&gt; each pairwise discussion (edge) has a discourse space with basis topics under discussion\n&gt; the linear transformations are expressions of opinions https://t.co/9SpXyzKWet', '5/ what’s great about this model is that everyone keeps their opinions private.  you can formulate opinions based on your core [basis] beliefs without sharing what you think.  you can exaggerate or even lie (negative coefficients) depending on who you are talking to.', '6/ ex: 4 people, each with a single opinion about a *certain politician*. \nthe two people on the left share their opinions plainly.\nthe two people on the right lie to their friends on the left, but are straight with each other.\nthe global discourse sheaf has a twist https://t.co/LvtBsL4k8i', '7/ an *opinion distribution* is a choice of opinions on each person [vertex].  this is what topologists call a 0-cochain for the sheaf.  if there is expressed consensus – the expression of opinions “match” over each edge, then you have a *section* of the discourse sheaf.', '8/ a section of the discourse sheaf is a choice of opinions that leads to a “harmonious” social network: there is no expressed disagreement (though everyone maintains private opinions).', '9/ keep going: the *hodge laplacian* of a cellular sheaf generalizes the graph laplacian &amp; measures the “average discordance” on a per-person basis.  an opinion distribution is *harmonic* if its laplacian vanishes – everyone is in expressed agreement.', '10/ sidequest: this is the first hint of the hodge theorem: the kernel of the laplacian is isomorphic to the cohomology of the sheaf. (of course, this requires a higher-dimensional base complex, etc., but that all works &amp; is very cool).', '11/ so, what can you do with this? https://t.co/bv65r3TNbI', '12/ theorem: using the hodge laplacian to define a heat equation gives network dynamics that sends any initial opinion distribution to the nearest section – you always flow to the closest available consensus opinion distribution.', '13/ how quickly does this converge? it depends on the spectral properties of the sheaf laplacian: see the paper https://t.co/GJtBLvhayZ for how spectral sheaf theory works in the abstract setting.', '14/ the sheaf laplacian gives an inner product on 0-cochains which tells you how far away from expressed consensus your social network is, in the same way that the graph laplacian tells you how far away from being a connected graph you are.', '15/ next, relative sheaf cohomology classifies harmonic extension.  if you have a subgraph A of people in accord and they are stubbornly inflexible, will the dynamics flow to a harmonic opinion distribution?  the obstruction is H^0(G,A) for the discourse sheaf.', '16/ what if, instead of inflexibly stubborn people, you just have mildly (or not so mildly) stubborn agents? replicate the vertex set, giving every vertex a “stubborn parent” and extend the discourse sheaf with weights; the hodge theorem does the rest. https://t.co/bcBqCPa8uU', '17/ i wonder if one can control opinions? if you can *plant* certain (stubborn?) opinions at key locations, can you drive the consensus opinions of others to be what you want?  in this framework, it’s a simple problem of harmonic extension &amp; relative sheaf cohomology.', ""18/ controllability &amp; observability are all simple consequences of sheaf cohomology.  \nthat's... interesting..."", '19/ this is all very cool, but relatively simple.  the base space is 1-d (a network) and the dynamics are linear.  can we do this with higher dimensions? higher cohomology? nonlinear laplacians? \nyes, all yes: see the preprint.', '20/ but wait, you say… in real political discourse, people don’t really change their opinions, do they? \ni mean, just look at twitter, fergahdsake.\nwell, let’s think about that…', '21/ what if, instead of using sheaf diffusion to modulate everyone’s opinions, keeping their expressions fixed, we use the heat equation to diffuse the expression maps, keeping everyone’s opinions fixed.  everyone “learns to communicate better” unto apparent consensus.', '22/ this works, and initial conditions in which one person has an extreme opinion evolves to a consensus state where the extremist has “learned to lie” in order to maintain harmony. https://t.co/1OFxI8ye7E', '23/ of course, you can evolve both opinions and expressions simultaneously, and this leads to more moderate expressions; but lying is still a learned behavior. https://t.co/B2QoWk87qb', '24/ this is a mathematics paper, and not a sociological work; we’re not claiming that this is exactly how political discourse or any other real opinion dynamics operates.  but we do claim that the sheaf-theoretic approach is more flexible than any model out there.', '25/ the full paper contains lots of details and additional analyses and examples: you can find it at https://t.co/WqakBmXOFN\nthis is just some of the great work being done by @_jakobhansen', '@JSEllenberg @_jakobhansen i like the way you think...', ""@JSEllenberg @_jakobhansen on a more serious note, we do have results about observability based on relative cohomology...  so, it's not too far a jump to what you propose.  one can do persistent sheaf cohomology (in field coeffs), so..."", '@JSEllenberg @_jakobhansen i think of it this way: i have a lot of basis opinions (on, say, free speech, respect for law, the 2nd amendment, valuing life, civil disobedience, ...). when you &amp; i discuss a new law, i will formulate an initial opinion based on my [literal] basis opinions.', ""@JSEllenberg @_jakobhansen after discussing things with you, i might be willing to change some of my basis opinions.  or, i might be willing to change the way i express my opinions, to be less forceful and, perhaps, to obfuscate.  that's diffusion."", ""@JSEllenberg @_jakobhansen in the extreme case of 1 opinion, it does seem odd to decide that i'm going to lie to you about my opinion no matter what.  but i can certainly think of things about which (say, in more unenlightened times) the default position would be to lie about X for some X."", '@JSEllenberg @_jakobhansen remember, this is not encoding that you lie about everything to everyone by default.  the stalk over a vertex is a vector space with a dedicated basis.  so, i might decide to a priori lie about my opinion on one basis topic XXX until i know you are ""safe"".', ""@curiasdetours yes, each pairwise interaction can be different. it's a bit more subtle, though, since what you are discussing (basis of discussion space) might not directly overlap with your core opinions (basis of opinion space).  lying makes sense for discussing the same things."", ""@curiasdetours example: my basis opinion is on *clarity in writing*. if the topic up for discussion is *how good is foucault*, then my expression map will have a negative coefficient.  the more i care about clarity, the less i like foucault.  i'm not lying about anything."", '@BatteryHorse @_jakobhansen great questions.  we do not address that directly in the paper, but there are a few ways to get the sheaf-theoretic methods to do this.']",http://arxiv.org/abs/2005.12798,"We introduce a novel class of Laplacians and diffusion dynamics on discourse sheaves as a model for network dynamics, with application to opinion dynamics on social networks. These sheaves are algebraic data structures tethered to a network (or more general space) that can represent various modes of communication, including selective opinion modulation and lying. After introducing the sheaf model, we develop a sheaf Laplacian in this context and show how to evolve both opinions and communications with diffusion dynamics over the network. Issues of controllability, reachability, bounded confidence, and harmonic extension are addressed using this framework. ",Opinion Dynamics on Discourse Sheaves
27,1265413436403412993,3105498192,Julie Clutterbuck,"['New paper: the first two eigenvalues of a convex domain in hyperbolic space fail to socially distance <LINK>', 'Wait, that title was vetoed. Real title: the vanishing of the fundamental gap of convex domains in hyperbolic space', 'With these fun people https://t.co/ODlta2S2e2', '.. Guofang Wei, Alina Stancu, Hien Nguyen, Theodora Bourni, and Valentina Wheeler (pictured working while also scoffing pastries)']",https://arxiv.org/abs/2005.11784,"For the Laplace operator with Dirichlet boundary conditions on convex domains in $\mathbb H^n$, $n\geq 2$, we prove that the product of the fundamental gap with the square of the diameter can be arbitrarily small for domains of any diameter. ",The vanishing of the fundamental gap of convex domains in $\mathbb H^n$
28,1265306705887453184,2340917156,Daniel Kohlsdorf,['New Paper on deep learning for dolphin communication. <LINK>'],https://arxiv.org/abs/2005.07623,"Research in dolphin communication and cognition requires detailed inspection of audible dolphin signals. The manual analysis of these signals is cumbersome and time-consuming. We seek to automate parts of the analysis using modern deep learning methods. We propose to learn an autoencoder constructed from convolutional and recurrent layers trained in an unsupervised fashion. The resulting model embeds patterns in audible dolphin communication. In several experiments, we show that the embeddings can be used for clustering as well as signal detection and signal type classification. ",An Auto Encoder For Audio Dolphin Communication
29,1265125334434766848,1699320672,Dr Sabine Bellstedt,"['Paper out on arXiv this week presenting the new GAMA photometry catalogue - featuring KiDS data, using the source-detection code ProFound over 20 bands in the far-UV to far-IR! <LINK>', 'Catalogue is the result of years of effort by the team, and will be available through the GAMA website via a collaboration request. https://t.co/NOTjK5b1Vm @astrowelshluke @AstroRobbo @HoseinHashemi9 @SoheilKoushan @_jessthorne @ryan_jturner @AstroAngus']",https://arxiv.org/abs/2005.11215,"The Galaxy And Mass Assembly Survey (GAMA) covers five fields with highly complete spectroscopic coverage ($>95$ per cent) to intermediate depths ($r<19.8$ or $i < 19.0$ mag), and collectively spans 250 square degrees of Equatorial or Southern sky. Four of the GAMA fields (G09, G12, G15 and G23) reside in the ESO VST KiDS and ESO VISTA VIKING survey footprints, which combined with our GALEX, WISE and Herschel data provide deep uniform imaging in the $FUV\,NUV\,ugriZYJHK_s\,W1\,W2\,W3\,W4\,P100\,P160\,S250\,S350\,S500$ bands. Following the release of KiDS DR4, we describe the process by which we ingest the KiDS data into GAMA (replacing the SDSS data previously used for G09, G12 and G15), and redefine our core optical and near-IR catalogues to provide a complete and homogeneous dataset. The source extraction and analysis is based on the new ProFound image analysis package, providing matched-segment photometry across all bands. The data are classified into stars, galaxies, artefacts, and ambiguous objects, and objects are linked to the GAMA spectroscopic target catalogue. Additionally, a new technique is employed utilising ProFound to extract photometry in the unresolved MIR-FIR regime. The catalogues including the full FUV-FIR photometry are described and will be fully available as part of GAMA DR4. They are intended for both standalone science, selection for targeted follow-up with 4MOST, as well as an accompaniment to the upcoming and ongoing radio arrays now studying the GAMA $23^h$ field. ","Galaxy And Mass Assembly (GAMA): Assimilation of KiDS into the GAMA
  database"
30,1265081568529448969,15327263,Carl-Johan Haster,"[""New paper led by Yiwen Huang (grad student @MITKavli &amp; @MIT_Physics) where we've looked at how well we should trust future analyses of gravitational waves from Neutron Star-Black Hole binaries. <LINK>"", 'In addition to Yiwen and myself, @sasomao, @vijay_x_varma (from @Caltech, and soon @Cornell) , @FrancoisFoucart (from @UofNH ) and @sylvia_bisco have contributed.', ""We find, not too surprisingly, that one has to be quite careful with these NSBH-signals. But as long as you've chosen a reasonable model to do your analysis with (ie. one that's constructed to describe NSBH systems), the potential model systematics shouldn't be too bad.""]",https://arxiv.org/abs/2005.11850,"Gravitational waves emitted by neutron star black hole mergers encode key properties of neutron stars - such as their size, maximum mass and spins - and black holes. However, the presence of matter and the high mass ratio makes generating long and accurate waveforms from these systems hard to do with numerical relativity, and not much is known about systematic uncertainties due to waveform modeling. We simulate gravitational waves from neutron star black hole mergers by hybridizing numerical relativity waveforms produced with the SpEC code with a recent numerical relativity surrogate NRHybSur3dq8Tidal. These signals are analyzed using a range of available waveform families, and statistical and systematic errors are reported. We find that at a network signal-to-noise ratio (SNR) of 30, statistical uncertainties are usually larger than systematic offsets, while at an SNR of 70 the two become comparable. The individual black hole and neutron star masses, as well as the mass ratios, are typically measured very precisely, though not always accurately at high SNR. At a SNR of 30 the neutron star tidal deformability can only be bound from above, while for louder sources it can be measured and constrained away from zero. All neutron stars in our simulations are non-spinning, but in no case we can constrain the neutron star spin to be smaller than $\sim0.4$ (90% credible interval). Waveform families whose late inspiral has been tuned specifically for neutron star black hole signals typically yield the most accurate characterization of the source parameters. Their measurements are in tension with those obtained using waveform families tuned against binary neutron stars, even for mass ratios that could be relevant for both binary neutron stars and neutron star black holes mergers. ","Statistical and systematic uncertainties in extracting the source
  properties of neutron star - black hole binaries with gravitational waves"
31,1264964629647593472,1240603790,Victor Fragoso 💻,"['Overwhelmed by finding optimal hyper-parameters of a CNN for a task? \n\nCheck out our HyperSTAR CVPR paper, a new method that:\n☑️ “Understands” a task\n☑️ Recommends hyper-parameters for it\n☑️ Accelerates a hyper-parameter optimization algorithm\n👉 <LINK> #CVPR2020 <LINK>']",https://arxiv.org/abs/2005.10524,"While deep neural networks excel in solving visual recognition tasks, they require significant effort to find hyperparameters that make them work optimally. Hyperparameter Optimization (HPO) approaches have automated the process of finding good hyperparameters but they do not adapt to a given task (task-agnostic), making them computationally inefficient. To reduce HPO time, we present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a task-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks and recommends hyperparameters by predicting their performance conditioned on a joint dataset-hyperparameter space. It learns a dataset (task) representation along with the performance predictor directly from raw images in an end-to-end fashion. The recommendations, when integrated with an existing HPO method, make it task-aware and significantly reduce the time to achieve optimal performance. We conduct extensive experiments on 10 publicly available large-scale image classification datasets over two different network architectures, validating that HyperSTAR evaluates 50% less configurations to achieve the best performance compared to existing methods. We further demonstrate that HyperSTAR makes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of the budget required by both vanilla HB and Bayesian Optimized HB~(BOHB). ",HyperSTAR: Task-Aware Hyperparameters for Deep Networks
32,1264942753097682944,38715455,Saad Bhamla,"['Congrats to @Kate__Burgener and graduating @GTChBE undergrad on her 1st 1st author paper out: <LINK> ($) and free pdf (arXiv: <LINK>)\nIn this paper, we describe a new way to clean pollutants from contact lenses... <LINK>', 'If you wear contact lenses, typically you use a rinse-and-rub method -- which involves rinsing with a multipurpose solution and rubbing between your fingers. We wondered if this would remove pollutants such as pollen and nanoparticles? https://t.co/dS6ZntrXd3', ""If you imagine tiny pollutant particles stuck on a contact lens (wet, soft, fragile and squishy) material, we envisioned rubbing it between fingers would lead to abrasion and couldn't provide sufficient shear forces to remove say nanoparticles (from air pollution)..."", '@Kate__Burgener and I developed a new method we refer to as PoPPR (popper) that stands for Polymer on Polymer Pollutant Removal. Essentially we use another softer polymer (PDMS) to remove pollutants... https://t.co/caol7bn3Yo', 'We found that although our PoPPR method was as good as the rinse-and-rub technique for Pollen (25-40um) sized particles, our method really shines for microbeads (1-5um) and nanoparticles (5-10) nanometers.. https://t.co/2wVnuqeTA0', 'Interestingly we found that that of different ratios of PDMS (setting agent to polymer) which tunes how soft PDMS is, an optimal ratio (1:40) exists that offers the best performance for lens cleaning across all particles - pollen to nanoparticles. https://t.co/O2k0lAWkTE', 'Ultimately, we think that this method would be useful for contact lens wearers in areas with high air pollution -- if you wear contact lens and live in any area with high pollution - contact us if you would like to try our method and we can ship you some cleaning polymers.', 'Last, I am proud of @Kate__Burgener who is started as a research in my lab in 2017 (when lab was founded) and is now off for her PhD at UW Madison in molecular and environmental toxicology. Good luck to her in her next adventure! https://t.co/yQscx7BZ1u', ""Here's one last image from her work of pollutants glowing on a contact lens under a microscope -- if you have never looked at a contact lens under a microscope -- you should! https://t.co/Cxt6nVKFbk"", '@nateorndorf @Kate__Burgener @GordonConf Nice - thanks for sharing - these are fantastic!']",https://arxiv.org/abs/2005.08732,"Purpose: To demonstrate an alternative to the rinse and rub (RR) method for cleaning pollutants from the exterior surface of soft contact lenses. This proposed technique is termed Polymer on Polymer Pollutant Removal (PoPPR), which utilizes the elastic properties of polydimethylsiloxane (PDMS) to physically remove contaminants from contact lens surfaces through non-adhesive unpeeling. Methods: Three different ratios of setting agent to polymer PDMS (1:30, 1:40, and 1:50) were evaluated using the PoPPR method against the control method of RR with a commercial multi-purpose lens cleaning solution. Three simulated pollutants of different sizes: pollen (25-40 {\mu}m), microbeads (1-5 {\mu}m), and nanoparticles (5-10 nm), were used to test the effectiveness of both cleaning methods. The fraction of pollutants removed from each contact lens was recorded and evaluated for significance. Results: PDMS 1:40 was found to be the optimal ratio for lens cleaning using the PoPPR method. For larger particles (>10 {\mu}m), no difference was observed between conventional RR and proposed PoPPR method (p > 0.05). However, the new PoPPR technique was significantly better at removing small PM2.5 particles (<2.5 {\mu}m) compared to the RR method, specifically for microbeads (p = 0.006) and nanoparticles (p < 0.001). Conclusion: This proof-of-concept work demonstrates that the PoPPR method of cleaning contact lenses is as effective as the conventional cleaning method for larger particles such as pollen. The PoPPR method is more effective at removing extremely fine particulate pollutants, including microplastics and nanoparticles. This method offers a potentially more efficient cleaning protocol that could enhance the safety, health, and comfort of contact lens users, especially those living in regions with significant air pollution. ",A polymer-based technique to remove pollutants from soft contact lenses
33,1264905358293774337,1547487810,basaktaraktas,['our new COVID paper <LINK>'],http://arxiv.org/abs/2005.09751,"We investigate phase transitions associated with three control methods for epidemics on small world networks. Motivated by the behavior of SARS-CoV-2, we construct a theoretical SIR model of a virus that exhibits presymptomatic, asymptomatic, and symptomatic stages in two possible pathways. Using agent-based simulations on small world networks, we observe phase transitions for epidemic spread related to: 1) Global social distancing with a fixed probability of adherence. 2) Individually initiated social isolation when a threshold number of contacts are infected. 3) Viral shedding rate. The primary driver of total number of infections is the viral shedding rate, with probability of social distancing being the next critical factor. Individually initiated social isolation was effective when initiated in response to a single infected contact. For each of these control measures, the total number of infections exhibits a sharp phase transition as the strength of the measure is varied. ","Phase transitions and control measures for network epidemics caused by
  infections with presymptomatic, asymptomatic,and symptomatic stages"
34,1264843208854568963,738769492122214400,Johannes Lischner,"['In our new paper, we study how plasmon decay in Ag nanoparticles can be controlled by dielectric embedding in different host materials. Last paper of my fantastic student Lara who now works for @DeepMind. 🥲  <LINK> #plasmonics #compchem <LINK>']",https://arxiv.org/abs/2005.10932,"Understanding and controlling properties of plasmon-induced hot carriers is a key step towards next-generation photovoltaic and photocatalytic devices. Here, we uncover a route to engineering hot-carrier generation rates of silver nanoparticles by designed embedding in dielectric host materials. Extending our recently established quantum-mechanical approach to describe the decay of quantized plasmons into hot carriers we capture both external screening by the nanoparticle environment and internal screening by silver d-electrons through an effective electron-electron interaction. We find that hot-carrier generation can be maximized by engineering the dielectric host material such that the energy of the localized surface plasmon coincides with the highest value of the nanoparticle joint density of states. This allows us to uncover a path to control the energy of the carriers and the amount produced, for example a large number of relatively low-energy carriers are obtained by embedding in strongly screening environments. ","Dielectric engineering of hot carrier generation by quantized plasmons
  in embedded silver nanoparticles"
35,1264753233072816130,408932801,Florian Richoux,"['New #GameAI paper about decision-making under uncertainty in the RTS game #microRTS, with probability distributions changing on-the-fly according to the opponent strategy.\n<LINK>', 'As always, source code is available on GitHub.\nhttps://t.co/fGckqbin9Y', ""In the paper's conclusion, I propose a new track for the #microRTS AI competition: the chaotic track, where rules change at each game, and eventually even during a game! That could interest @santiontanon :)"", 'My bot microPhantom will participate in the #microRTS AI competition partial observability track this year.\n\nOur previous bot POAdaptive won the 2018 and 2019 PO track and microPhantom is clearly stronger than POAdaptive. Looking forward to seeing the competition results!', '@santiontanon I mostly aim hardcoded bots. :p (I wrote about that in the introduction)']",https://arxiv.org/abs/2005.11019,"This competition paper presents microPhantom, a bot playing microRTS and participating in the 2020 microRTS AI competition. microPhantom is based on our previous bot POAdaptive which won the partially observable track of the 2018 and 2019 microRTS AI competitions. In this paper, we focus on decision-making under uncertainty, by tackling the Unit Production Problem with a method based on a combination of Constraint Programming and decision theory. We show that using our method to decide which units to train improves significantly the win rate against the second-best microRTS bot from the partially observable track. We also show that our method is resilient in chaotic environments, with a very small loss of efficiency only. To allow replicability and to facilitate further research, the source code of microPhantom is available, as well as the Constraint Programming toolkit it uses. ",microPhantom: Playing microRTS under uncertainty and chaos
36,1263891945400807424,2999702157,Anton Ilderton,['New paper on the #arxiv by #me! With many thanks to @GiesJena for inspiring and useful discussions!\n\n<LINK>\n\n#physics #renormalisation #JaynesCummings #QuantumOptics <LINK> <LINK>'],https://arxiv.org/abs/2005.06485,"The Jaynes-Cummings model is a cornerstone of light-matter interactions. While finite, the model provides an illustrative example of renormalisation in perturbation theory. We show, however, that exact renormalisation reveals a rich non-perturbative structure, and that the model provides a physical example of a theory with a chaotic coupling trajectory and multi-valued beta-function. We also construct an exact Wilsonian-like renormalisation group flow for the effective scattering matrix, and show how multi-valued features arise in the flow. Our results shed light on non-perturbative aspects of renormalisation and on the structure of the Jaynes-Cummings model itself. ",Renormalisation group flow of the Jaynes-Cummings model
37,1263888683385552898,722842576479322114,Anthony Aguirre,"['Very excited about a new paper with J. Schindler and D. Safranek in which we propose a preferred generalization of entanglement entropy to multipartite and/or mixed systems, based on our previous work on coarse-grained entropy. <LINK>']",https://arxiv.org/abs/2005.05408,"We study quantum coarse-grained entropy and demonstrate that the gap in entropy between local and global coarse-grainings is a natural generalization of entanglement entropy to mixed states and multipartite systems. This ""quantum correlation entropy"" $S^{\rm QC}$ is additive over independent systems, is invariant under local unitary operations, measures total nonclassical correlations (vanishing on states with strictly classical correlation), and reduces to the entanglement entropy for bipartite pure states. It quantifies how well a quantum system can be understood via local measurements, and ties directly to non-equilibrium thermodynamics, including representing a lower bound on the quantum part of thermodynamic entropy production. We discuss two other measures of nonclassical correlation to which this entropy is equivalent, and argue that together they provide a unique thermodynamically distinguished measure. ",Quantum correlation entropy
38,1263874095751532545,1263870728870469632,Enrico Ronca,['A preprint of our new paper on Quantum Electrodynamics Coupled Cluster Theory (QED-CC) is out on ArXiv:\n<LINK> <LINK>'],https://arxiv.org/abs/2005.04477,"We present an ab initio correlated approach to study molecules that interact strongly with quantum fields in an optical cavity. Quantum electrodynamics coupled cluster theory provides a non-perturbative description of cavity-induced effects in ground and excited states. Using this theory, we show how quantum fields can be used to manipulate charge transfer and photochemical properties of molecules. We propose a strategy to lift electronic degeneracies and induce modifications in the ground state potential energy surface close to a conical intersection. ","Coupled Cluster Theory for Molecular Polaritons: Changing Ground and
  Excited States"
39,1263839754996629504,40299444,Alexey Petrov,"['New paper! If you wanted to know about muonium-antimuonium oscillations (but were afraid to ask), see <LINK>! It was a pleasure working with a WSU grad student Renae Conlin.']",https://arxiv.org/abs/2005.10276,"Flavor violating processes in the lepton sector have highly suppressed branching ratios in the standard model mainly due to the tiny neutrino mass. This means that observing lepton flavor violation (LFV) in the next round of experiments would constitute a clear indication of physics beyond the standard model (BSM). We revisit a discussion of one possible way to search for LFV, muonium-antimuonium oscillations. This process violates muon lepton number by two units and could be sensitive to the types of BSM physics that are not probed by other types of LFV processes. Using techniques of effective field theory, we calculate the mass and width differences of the mass eigenstates of muonium. We argue that its invisible decays give the parametrically leading contribution to the lifetime difference and put constraints on the scales of new physics probed by effective operators in muonium oscillations. ",Muonium-antimuonium oscillations in effective field theory
40,1263811203895345152,1558231266,Ogan Ozsoy,"['My new paper on primordial ""synthetic"" gravitational waves from inflation: <LINK>.   \nIt turns out that the motion of string theory inspired axion-like states can lead to efficient emission of #GravitationalWaves that does not have quantum vacuum origin. <LINK>']",https://arxiv.org/abs/2005.10280,"In string theory inspired models of axion-like fields, sub-leading non-perturbative effects, if sufficiently large, can introduce steep cliffs and gentle plateaus onto the underlying scalar potential. During inflation, the motion of a spectator axion $\sigma$ on this potential becomes temporarily fast, leading to localized amplification of one helicity state of gauge fields. In this model, the tensor and scalar correlators sourced by the vector fields exhibit localized peak(s) in momentum space corresponding to the modes that exit the horizon while the roll of $\sigma$ is fast. Thanks to the gravitational coupling of gauge fields with the visible sector and the localized nature of particle production, this model can generate observable gravitational waves (GWs) at CMB scales while satisfying the current limits on scalar perturbations. The resulting GW signal breaks parity and exhibit sizeable non-Gaussianity that can be probed by future CMB B-mode missions. Depending on the initial conditions and model parameters, the roll of the spectator axion can also generate an observably large GW signature at interferometer scales while respecting the bounds on the scalar fluctuations from primordial black hole limits. In our analysis, we carefully investigate bounds on the model parameters that arise through back-reaction and perturbativity considerations to show that these limits are satisfied by the implementations of the model that generate GW signals at CMB and sub-CMB scales. ",Synthetic Gravitational Waves from a Rolling Axion Monodromy
41,1263756233800126464,1134375290581524480,Kai Schmitz,"['New paper on the arXiv: ""LISA Sensitivity to Gravitational Waves from Sound Waves"" 🎉 <LINK> <LINK>']",https://arxiv.org/abs/2005.10789,"Gravitational waves (GWs) produced by sound waves in the primordial plasma during a strong first-order phase transition in the early Universe are going to be a main target of the upcoming Laser Interferometer Space Antenna (LISA) experiment. In this short note, I draw a global picture of LISA's expected sensitivity to this type of GW signal, based on the concept of peak-integrated sensitivity curves (PISCs) recently introduced in [1909.11356, 2002.04615]. In particular, I use LISA's PISC to perform a systematic comparison of several thousands of benchmark points in ten different particle physics models in a compact fashion. The presented analysis (i) retains the complete information on the optimal signal-to-noise ratio, (ii) allows for different power-law indices describing the spectral shape of the signal, (iii) accounts for galactic confusion noise from compact binaries, and (iv) exhibits the dependence of the expected sensitivity on the collected amount of data. An important outcome of this analysis is that, for the considered set of models, galactic confusion noise typically reduces the number of observable scenarios by roughly a factor two, more or less independent of the observing time. The numerical results presented in this paper are also available on Zenodo [this http URL]. ",LISA Sensitivity to Gravitational Waves from Sound Waves
42,1263523528411758599,1002688565946601472,Connor W. Coley,"['Our new paper (<LINK>) led by @liorhir evaluates methods for uncertainty quantification in QSPR modeling. Results are densely summarized below, where +z means that the primary estimator tended to outrank the secondary across datasets and splits #compchem <LINK>']",https://arxiv.org/abs/2005.10036,"Uncertainty quantification (UQ) is an important component of molecular property prediction, particularly for drug discovery applications where model predictions direct experimental design and where unanticipated imprecision wastes valuable time and resources. The need for UQ is especially acute for neural models, which are becoming increasingly standard yet are challenging to interpret. While several approaches to UQ have been proposed in the literature, there is no clear consensus on the comparative performance of these models. In this paper, we study this question in the context of regression tasks. We systematically evaluate several methods on five benchmark datasets using multiple complementary performance metrics. Our experiments show that none of the methods we tested is unequivocally superior to all others, and none produces a particularly reliable ranking of errors across multiple datasets. While we believe these results show that existing UQ methods are not sufficient for all common use-cases and demonstrate the benefits of further research, we conclude with a practical recommendation as to which existing techniques seem to perform well relative to others. ","Uncertainty Quantification Using Neural Networks for Molecular Property
  Prediction"
43,1263499423683883009,160687843,Alexey Melnikov,"['Our new paper on designing better Bell test experiments, where we propose new unintuitive experiments with Bell inequality violations higher than the best known setups. Here we go towards device-independent quantum information processing <LINK> @UniBasel <LINK>']",https://arxiv.org/abs/2005.01697,"Finding optical setups producing measurement results with a targeted probability distribution is hard as a priori the number of possible experimental implementations grows exponentially with the number of modes and the number of devices. To tackle this complexity, we introduce a method combining reinforcement learning and simulated annealing enabling the automated design of optical experiments producing results with the desired probability distributions. We illustrate the relevance of our method by applying it to a probability distribution favouring high violations of the Bell-CHSH inequality. As a result, we propose new unintuitive experiments leading to higher Bell-CHSH inequality violations than the best currently known setups. Our method might positively impact the usefulness of photonic experiments for device-independent quantum information processing. ",Setting up experimental Bell test with reinforcement learning
44,1263452551682371584,931290667,Prof. Dr. Christopher Shingledecker,"['New paper on arXiv just accepted in @AAS_Publishing with @liton_majumdar! (<LINK>) I had one of the central ideas described in the paper while visiting Arcetri Observatory in Florence last year. Somehow, I managed to get some work done - despite the view! #ApJ <LINK>']",https://arxiv.org/abs/2005.09477,"In this work, we present the results of our investigation into the chemistry of Z- and E-cyanomethanimine (HNCHCN), both of which are possible precursors to the nucleobase adenine. Ab initio quantum chemical calculations for a number of reactions with atomic hydrogen were carried out. We find that the reaction H + Z/E-HNCHCN leading both to H-addition as well as H$_2$-abstraction proceed via similar short-range barriers with bimolecular rate coefficients on the order of $\sim10^{-17}$ cm$^{3}$ s$^{-1}$. These results were then incorporated into astrochemical models and used in simulations of the giant molecular cloud G+0.693. The calculated abundances obtained from these models were compared with previous observational data and found to be in good agreement, with a predicted [Z/E] ratio of $\sim3$ - somewhat smaller than the previously derived value of $6.1\pm2.4$. We find that the [Z/E] ratio in our simulations is due mostly to ion-molecule destruction rates driven by the different permanent dipoles of the two conformers. Based on these results, we propose a general rule-of-thumb for estimating the abundances of isomers in interstellar environments. ","Isomers in Interstellar Environments (I): The Case of Z- and
  E-Cyanomethanimine"
45,1263412328873234432,804483812,Matteo Fasiolo,['Our paper on additive stacking for household demand forecasting is finally on <LINK> @ChristianCapez5 @YannigGoude \nThanks to @jethrobrowell  and @SteveHaben for advice on the battery scheduling application!\nNew mgcv family at <LINK> #rstats <LINK>'],https://arxiv.org/abs/2005.10092,"Future grid management systems will coordinate distributed production and storage resources to manage, in a cost effective fashion, the increased load and variability brought by the electrification of transportation and by a higher share of weather dependent production. Electricity demand forecasts at a low level of aggregation will be key inputs for such systems. We focus on forecasting demand at the individual household level, which is more challenging than forecasting aggregate demand, due to the lower signal-to-noise ratio and to the heterogeneity of consumption patterns across households. We propose a new ensemble method for probabilistic forecasting, which borrows strength across the households while accommodating their individual idiosyncrasies. In particular, we develop a set of models or 'experts' which capture different demand dynamics and we fit each of them to the data from each household. Then we construct an aggregation of experts where the ensemble weights are estimated on the whole data set, the main innovation being that we let the weights vary with the covariates by adopting an additive model structure. In particular, the proposed aggregation method is an extension of regression stacking (Breiman, 1996) where the mixture weights are modelled using linear combinations of parametric, smooth or random effects. The methods for building and fitting additive stacking models are implemented by the gamFactory R package, available at this https URL ",Additive stacking for disaggregate electricity demand forecasting
46,1263301669120548865,2888783160,Saptarshi Pal,"['<LINK>. If you are interested in discrete time discrete state dynamical systems and would love to learn a cool new method from semigroup decomposition theory to analyze their structures better here is a paper for you by @NehanivCL and I.', 'Also if you are familiar with Bootstrap Percolation in Statistical Mechanics this might be of interest to you']",https://arxiv.org/abs/2005.10078,In this paper a modification of the standard Bootstrap Percolation model is introduced. In our modification a discrete time update rule is constructed that allows for non-monotonicity - unlike its classical counterpart. External inputs to drive the system into desirable states are also included in the model. The algebraic structure and complexity properties of the system are inferred by studying the system's holonomy decomposition. We introduce methods of inferring the pools of reversibility for the system. Dependence of system complexity on process parameters is presented and discussed. ,"Algebraic Structure and Complexity of Bootstrap Percolation with
  External Inputs"
47,1263182703064748034,36386681,Prof. Aladdin Ayesh,"['A preprint of our paper (with @Dan_Schiff, Laura Musikanski @HappyCounts, and @johnchavens): \n\nIEEE 7010: A New Standard for Assessing the Well-being Implications of Artificial Intelligence \n\nis now available on arXiv: \n\n<LINK> \n\n#Wellbeing #AI  #IEEE']",http://arxiv.org/abs/2005.06620,"Artificial intelligence (AI) enabled products and services are becoming a staple of everyday life. While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue. This article introduces one of the first international standards focused on the social and ethical implications of AI: The Institute of Electrical and Electronics Engineering (IEEE) Standard (Std) 7010-2020 Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-being. Incorporating well-being factors throughout the lifecycle of AI is both challenging and urgent and IEEE 7010 provides key guidance for those who design, deploy, and procure these technologies. We begin by articulating the benefits of an approach for AI centered around well-being and the measurement of well-being data. Next, we provide an overview of IEEE 7010, including its key principles and how the standard relates to approaches and perspectives in place in the AI community. Finally, we indicate where future efforts are needed. ","IEEE 7010: A New Standard for Assessing the Well-being Implications of
  Artificial Intelligence"
48,1263166975636897792,248216566,Mozhi Zhang,['New ACL paper: “Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries” (w/ @akkikiki @mjp39 @boydgraber)\n\nCLWE are often only evaluated on BLI. See why this hurts other downstream tasks and how retrofitting helps.\n\n<LINK>'],http://arxiv.org/abs/2005.00524,"Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation. ","Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word
  Embeddings to Dictionaries"
49,1263152573126672385,119837224,Jason Baldridge,"['Happy to share a new ACL paper: Li, He, Zhou, Zhang and Baldridge ""Mapping Natural Language Instructions to Mobile UI Action Sequences"". We build datasets and models for learning to take UI actions based on NL instructions.\n\n<LINK> <LINK>', 'We evaluate the end-to-end task with a new dataset called PixelHelp, which we collected from annotators using a Pixel Phone emulator. PixelHelp is small with 187 instructions, but it has fine-grained grounding of instructions to actions (of two to eight steps). https://t.co/zXYNbGlcQ6', 'We decompose the problem into phrase-tuple extraction followed by action prediction conditioned on the phrase-tuples and the screen. The first stage uses a transformer to encode the instruction and produce a tuple sequence. We train this on annotated HowTo instructions. https://t.co/YdJB3IUSQP', 'The second stage grounds the phrase tuples using a transformer that encodes objects using both their content (metadata) and positional information (spatial and structural). We train this on synthetic command-actions on the Rico public UI corpus. https://t.co/ldtMfyrCub', 'We compare the full model with a heuristic baseline that matches phrases to object names and to object encoders using Graph Convolutional Networks. The transformer approach proves much better at both partial instruction grounding and complete grounding. https://t.co/LKgiYQloVJ', 'To be fair, we probably could have eked out a bit better performance if we had used the Ours model, but we realized that a bit too late.\n\nhttps://t.co/ZE8nivmykT', ""We hope the data, and the decomposition of the problem make it possible for others to make progress on the task! I'm keen to see if techniques and ideas from problems like Vision-and-Language Navigation (e.g. https://t.co/56JXuWLPz5) can used for this too, especially RL.""]",https://arxiv.org/abs/2005.03776,"We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PIXELHELP, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in HowTo instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PIXELHELP. ",Mapping Natural Language Instructions to Mobile UI Action Sequences
50,1263139415687335936,18045865,Shijie Wu,"['Happy to share a new paper ""Are All Languages Created Equal in Multilingual BERT?"" <LINK> \n\nThe short answer is *NO* and we provide experiments to show why.\n\nWork with @mdredze, accepted in #Repl4NLP workshop at #acl2020nlp', 'We show the bottom 30% of languages in mBERT learn poor representation. We show for low resource languages, mBERT &gt; bilingual BERT (paired w/ ling. similar language) &gt; monolingual BERT, suggesting the culprit is poor data-efficiency &amp; lack of data (two sides of the same coin) https://t.co/y2522XZvW0', 'To learn better representation for low resource languages in the future, we need to collect more data (like XLM-R https://t.co/ZSXydXjwtQ) or a more data-efficient pretraining technique (like ELECTRA https://t.co/WCBVAbgcu1)']",https://arxiv.org/abs/2005.09093,"Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data. ",Are All Languages Created Equal in Multilingual BERT?
51,1262793302388113411,2933022295,Alex Beatson,"['New paper: ""Learning Composable Energy Surrogates for PDE Order Reduction"", with Jordan Ash, @geoffrey_roeder, @TianjuXue, @ryan_p_adams: <LINK>. We use NNs to amortize solving hard mechanical meta-material PDEs, training only on data from small subdomains. 1/8 <LINK>', 'How? Solutions to hyperelasticity PDEs (and many others!) are minimizers of a total energy, which is an integral over the domain. We train NNs to predict the minimal energy in a component from a reduced-basis solution (here, a displacement field) on the component boundary. 2/8', 'When components are composed into a larger domain, the total energy is a sum of component energies. We solve the PDE in a reduced basis of component boundaries, minimizing the sum of NN surrogate energies. We only generate supervised data (via FEA) on the small components. 3/8 https://t.co/0NrQfQitq9', 'When solving PDEs with FEA, fine geometric features (such as metamaterial pores) require a fine mesh. Part of the trick here is feeding parametric representations of geometry to our component-level energy surrogate, avoiding representing geometry with a fine mesh. 4/8', 'To get accurate surrogates, we embed known structure (invariance to rigid-body transforms and a linear-elastic structural prior) into the NN surrogate, and perform Sobolev training on energy derivatives and Hessian-vector products as well as energy values. 5/8', 'The learned surrogate can solve composed systems orders-of-magnitude faster than a finite element model of similar accuracy, run on the same CPU. 6/8 https://t.co/Qmwu01ao9M', 'Limitation: we use data augmentation to capture displacements the components encounter in practice, so you have to pick a distribution of macroscopic problems to amortize. Perhaps inevitable, but we’re looking into relaxing this via active learning and learning robust models. 7/8', 'Mechanical meta-materials like the lattices we study hold great promise for engineering design (see https://t.co/GmrQEiwwuA), but are hard to simulate due to geometric complexity and nonlinear behavior. We think there’s a lot of room for ML to help harness their potential.  8/8']",https://arxiv.org/abs/2005.06549,"Meta-materials are an important emerging class of engineered materials in which complex macroscopic behaviour--whether electromagnetic, thermal, or mechanical--arises from modular substructure. Simulation and optimization of these materials are computationally challenging, as rich substructures necessitate high-fidelity finite element meshes to solve the governing PDEs. To address this, we leverage parametric modular structure to learn component-level surrogates, enabling cheaper high-fidelity simulation. We use a neural network to model the stored potential energy in a component given boundary conditions. This yields a structured prediction task: macroscopic behavior is determined by the minimizer of the system's total potential energy, which can be approximated by composing these surrogate models. Composable energy surrogates thus permit simulation in the reduced basis of component boundaries. Costly ground-truth simulation of the full structure is avoided, as training data are generated by performing finite element analysis with individual components. Using dataset aggregation to choose training boundary conditions allows us to learn energy surrogates which produce accurate macroscopic behavior when composed, accelerating simulation of parametric meta-materials. ",Learning Composable Energy Surrogates for PDE Order Reduction
52,1262662127644524544,359879856,Luca Mariot,"['The preprint of my new paper ""Exploring Semi-bent Boolean Functions Arising from Cellular Automata"", co-authored w/ Martina Saletta, Alberto Leporati and @mnzluca, is now available on arXiv: <LINK> #cellularautomata #booleanfunctions <LINK>']",https://arxiv.org/abs/2005.08300,"Semi-bent Boolean functions are interesting from a cryptographic standpoint, since they possess several desirable properties such as having a low and flat Walsh spectrum, which is useful to resist linear cryptanalysis. In this paper, we consider the search of semi-bent functions through a construction based on cellular automata (CA). In particular, the construction defines a Boolean function by computing the XOR of all output cells in the CA. Since the resulting Boolean functions have the same algebraic degree of the CA local rule, we devise a combinatorial algorithm to enumerate all quadratic Boolean functions. We then apply this algorithm to exhaustively explore the space of quadratic rules of up to 6 variables, selecting only those for which our CA-based construction always yields semi-bent functions of up to 20 variables. Finally, we filter the obtained rules with respect to their balancedness, and remark that the semi-bent functions generated through our construction by the remaining rules have a constant number of linear structures. ",Exploring Semi-bent Boolean Functions Arising from Cellular Automata
53,1262655777627467778,115190692,Matt Henderson,"['new paper - we show models for extracting information in a dialog benefit from pretraining on the response-selection task, and release our span-extraction data:\n<LINK>\ncongrats @CaHoop on the acceptance to #acl2020nlp! <LINK>']",https://arxiv.org/abs/2005.08866,"We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain. ","Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained
  Conversational Representations"
54,1262645744676331521,40639812,Colin Cotter,"[""New paper by my freshly-viva'd student @aa_bock \non the ArXiV, on finite element discretisation of image metamorphosis (a way of transforming from one image to another that combines transport with local changes that can change image topology).\n<LINK>"", '@utropstegn @aa_bock Thanks Marie! Hope you are getting on ok!', '@utropstegn @aa_bock oh and Andreas is a fabulous computational scientist and numerical analyst. I’m sure he’d be keen to hear of any opportunities in Oslo.']",https://arxiv.org/abs/2005.08743,We study the problem of registering images. The framework we use is metamorphosis and we construct a variational Eulerian space-time setting and pose the registration problem as an infinite-dimensional optimisation problem. The geodesic equations correspond to a system of advection and continuity equations and are solved analytically. Well-posedness of a primal conforming finite element method is established and its convergence is investigated numerically. This provides a discrete forward operator for the matching parameterized by a space-time velocity field. We propose a gradient descent method on this control variable and show several promising numerical results for this approach. ,Space-time metamorphosis
55,1262596885069484037,1185977761032110080,Kazumasa Ohno (大野 和正),"['My new paper about transmission spectra of exoplanetary atmosphere is out! \n<LINK>\nThe paper discusses that photochemical haze may explain the ""super-Rayleigh slopes"" observed in some exoplanets.', 'We analytically show that the steepened spectral slope emerge when the atmospheric opacity is higher at higher altitude. Such opacity gradient can be naturally generated by haze.', 'Applying a microphysical model for wide parameter spaces, we found that the transmission spectrum of hazy atmosphere can be demarcated into (at least) four typical regimes. The super-Rayleigh slope is a characteristic of one of them.', 'According to our results, there is a ""sweet spot"" for producing super-Rayleigh slopes in eddy diffusion-haze mass flux space. We derived some analytical estimations to figure out what parameters lead to yield the super-Rayleigh slopes.', 'Crudely speaking, the SRSs emerge when Kz is high and haze mass flux is moderate. If Kz is higher for hotter exoplanets as suggested by GCM study, the slope gradient can reasonably explain the trend seen in recent retrieval studies.', 'Of course, there are many uncertainties, such as haze optical constants, etc..., for interpretation of the spectral slopes. More works from several aspects (including lab experiments) will be important to better understand how haze affects observable spectra!']",https://arxiv.org/abs/2005.08880,"Spectral slopes in optical transmission spectra of exoplanetary atmospheres encapsulate information on the properties of exotic clouds. The slope is usually attributed to the Rayleigh scattering caused by tiny aerosol particles, whereas recent retrieval studies have suggested that the slopes are often steeper than the canonical Rayleigh slopes. Here, we propose that photochemical haze formed in vigorously mixing atmospheres can explain such super-Rayleigh slopes. We first analytically show that the spectral slope can be steepened by the vertical opacity gradient in which atmospheric opacity increases with altitude. Using a microphysical model, we demonstrate that such opacity gradient can be naturally generated by photochemical haze, especially when the eddy mixing is substantially efficient. The transmission spectra of hazy atmospheres can be demarcated into four typical regimes in terms of the haze mass flux and eddy diffusion coefficient. We find that the transmission spectrum can have the spectral slope 2--4 times steeper than the Rayleigh slope if the eddy diffusion coefficient is sufficiently high and the haze mass flux falls into a moderate value. Based on the eddy diffusion coefficient suggested by a recent study of atmospheric circulations, we suggest that photochemical haze preferentially generates super-Rayleigh slopes at planets with equilibrium temperature of 1000--1500 K, which might be consistent with results of recent retrieval studies. Our results would help to interpret the observations of spectral slopes from the perspective of haze formation. ","Super-Rayleigh Slopes in Transmission Spectra of Exoplanets Generated by
  Photochemical Haze"
56,1262579497951301633,1115880604560691200,NII Yamagishi Lab,"['New VoicePrivacy Challenge paper!  ""Design Choices for X-vector Based Speaker Anonymization,"" <LINK>']",https://arxiv.org/abs/2005.08601,"The recently proposed x-vector based anonymization scheme converts any input voice into that of a random pseudo-speaker. In this paper, we present a flexible pseudo-speaker selection technique as a baseline for the first VoicePrivacy Challenge. We explore several design choices for the distance metric between speakers, the region of x-vector space where the pseudo-speaker is picked, and gender selection. To assess the strength of anonymization achieved, we consider attackers using an x-vector based speaker verification system who may use original or anonymized speech for enrollment, depending on their knowledge of the anonymization scheme. The Equal Error Rate (EER) achieved by the attackers and the decoding Word Error Rate (WER) over anonymized data are reported as the measures of privacy and utility. Experiments are performed using datasets derived from LibriSpeech to find the optimal combination of design choices in terms of privacy and utility. ",Design Choices for X-vector Based Speaker Anonymization
57,1262556920365281281,42179566,Henry Mao,['My new paper on speech recognition and diarization <LINK>'],https://arxiv.org/abs/2005.08072,"Speech recognition (ASR) and speaker diarization (SD) models have traditionally been trained separately to produce rich conversation transcripts with speaker labels. Recent advances have shown that joint ASR and SD models can learn to leverage audio-lexical inter-dependencies to improve word diarization performance. We introduce a new benchmark of hour-long podcasts collected from the weekly This American Life radio program to better compare these approaches when applied to extended multi-speaker conversations. We find that training separate ASR and SD models perform better when utterance boundaries are known but otherwise joint models can perform better. To handle long conversations with unknown utterance boundaries, we introduce a striding attention decoding algorithm and data augmentation techniques which, combined with model pre-training, improves ASR and SD. ",Speech Recognition and Multi-Speaker Diarization of Long Conversations
58,1262445769514254338,3051993112,Anna Wright,"['New paper day! We use the Romulus25 cosmological simulation to study the formation and evolution of 100+ ultra-diffuse galaxies (UDGs) in the field: <LINK>\n#nbodyshopgotchu', 'Shout-out to co-authors Michael Tremmel (@MichaelTremmel), Alyson Brooks, Ferah Munshi (@fdmtweets), Daisuke Nagai, Ray Sharma (@RaySSharma), and Tom Quinn.\nHere are some of our results:', 'We search Romulus25 – one of the highest resolution volumes ever run – for isolated UDGs. That is, large, low surface brightness (LSB) galaxies that live far away from anything massive. Here are a few of our galaxies + SB profiles &amp; fits – UDGs on the right, non-UDGs on the left: https://t.co/wkdemCZ0Ci', 'Other than their large sizes and LSB, present-day UDGs aren’t really that weird. They have normal star formation rates (see below), HI masses, colors, and virial masses. They’re also pretty common: ~20% of all field galaxies with Mstar = 10^7-9 Msol are UDGs. https://t.co/l0OeHNpaGn', 'What distinguishes UDGs is their evolution! Higher mass dwarf galaxies don’t typically fade much after z~1, but UDGs become increasingly LSB. Same with size: non-UDGs don’t change much over time, but UDGs start growing and don’t stop. https://t.co/xR7cS7m4JD', 'Part of the reason for this seems to be that UDGs evolve to lower central star formation rates. As their central stars age, the cores of the galaxies get fainter, similar to what we see in galaxy cluster simulations (RomulusC; https://t.co/9oWZA88Trv) https://t.co/b8xlA1b7D4', 'However, field UDGs have typical star formation rates for their stellar masses, so what’s really happening is that star-forming gas is spreading out! This results in bright, new stars forming at larger radii, leading to an increase in size over time. But why?', 'It turns out that most of our field UDGs had early mergers (z&gt;1) that temporarily spun them up and redistributed their star formation. In many UDGs, we see bursts of star formation at high radii for billions of years post-merger: https://t.co/UTGayIlwJV']",https://arxiv.org/abs/2005.07634,"We use the \textsc{Romulus25} cosmological simulation volume to identify the largest-ever simulated sample of {\it field} ultra-diffuse galaxies (UDGs). At $z=0$, we find that isolated UDGs have average star formation rates, colors, and virial masses for their stellar masses and environment. UDGs have moderately elevated HI masses, being 70\% (300\%) more HI-rich than typical isolated dwarf galaxies at luminosities brighter (fainter) than M$_\mathrm{B}$=-14. However, UDGs are consistent with the general isolated dwarf galaxy population and make up $\sim$20\% of all field galaxies with 10$^7$<M$_\star$/M$_\odot$<10$^{9}$. The HI masses, effective radii, and overall appearances of our UDGs are consistent with existing observations of field UDGs, but we predict that many isolated UDGs have been missed by current surveys. Despite their isolation at $z=0$, the UDGs in our sample are the products of major mergers. Mergers are no more common in UDG than non-UDG progenitors, but mergers that create UDGs tend to happen earlier - almost never occurring after $z=1$, produce a temporary boost in spin, and cause star formation to be redistributed to the outskirts of galaxies, resulting in lower central star formation rates. The centers of the galaxies fade as their central stellar populations age, but their global star formation rates are maintained through bursts of star formation at larger radii, producing steeper negative g-r color gradients. This formation channel is unique relative to other proposals for UDG formation in isolated galaxies, demonstrating that UDGs can potentially be formed through multiple mechanisms. ",The Formation of Isolated Ultra-Diffuse Galaxies in Romulus25
59,1262313660027936768,40754053,stefano maria iacus,"['""Are official confirmed cases and fatalities counts good enough to study the #COVID19 pandemic dynamics?"" New paper with great team. <LINK>']",https://arxiv.org/abs/2005.07271,"As the COVID-19 outbreak is developing the two most frequently reported statistics seem to be the raw confirmed case and case fatalities counts. Focusing on Italy, one of the hardest hit countries, we look at how these two values could be put in perspective to reflect the dynamics of the virus spread. In particular, we find that merely considering the confirmed case counts would be very misleading. The number of daily tests grows, while the daily fraction of confirmed cases to total tests has a change point. It (depending on region) generally increases with strong fluctuations till (around, depending on region) 15th-22nd March and then decreases linearly after. Combined with the increasing trend of daily performed tests, the raw confirmed case counts are not representative of the situation and are confounded with the sampling effort. This we observe when regressing on time the logged fraction of positive tests and for comparison the logged raw confirmed count. Hence, calibrating model parameters for this virus's dynamics should not be done based only on confirmed case counts (without rescaling by the number of tests), but take also fatalities and hospitalization count under consideration as variables not prone to be distorted by testing efforts. Furthermore, reporting statistics on the national level does not say much about the dynamics of the disease, which are taking place at the regional level. These findings are based on the official data of total death counts up to 15th April 2020 released by ISTAT and up to 10th May 2020 for the number of cases. In this work we do not fit models but we rather investigate whether this task is possible at all. This work also informs about a new tool to collect and harmonize official statistics coming from different sources in the form of a package for the R statistical environment and presents the COVID-19 Data Hub. ","Are official confirmed cases and fatalities counts good enough to study
  the COVID-19 pandemic dynamics? A critical assessment through the case of
  Italy"
60,1262310843410264064,265414308,Atılım Güneş Baydin,['Our new workshop paper about simulation-based inference and global health is now online <LINK> <LINK>'],https://arxiv.org/abs/2005.07062,"The COVID-19 pandemic has highlighted the importance of in-silico epidemiological modelling in predicting the dynamics of infectious diseases to inform health policy and decision makers about suitable prevention and containment strategies. Work in this setting involves solving challenging inference and control problems in individual-based models of ever increasing complexity. Here we discuss recent breakthroughs in machine learning, specifically in simulation-based inference, and explore its potential as a novel venue for model calibration to support the design and evaluation of public health interventions. To further stimulate research, we are developing software interfaces that turn two cornerstone COVID-19 and malaria epidemiology models COVID-sim, (this https URL) and OpenMalaria (this https URL) into probabilistic programs, enabling efficient interpretable Bayesian inference within those simulators. ",Simulation-Based Inference for Global Health Decisions
61,1262183768950882305,1144078371338326016,Ruben Loaiza-Maya,['Our new paper on fast and accurate variational inference for models with many latent variables is now available at <LINK>.'],https://arxiv.org/abs/2005.07430,"Models with a large number of latent variables are often used to fully utilize the information in big or complex data. However, they can be difficult to estimate using standard approaches, and variational inference methods are a popular alternative. Key to the success of these is the selection of an approximation to the target density that is accurate, tractable and fast to calibrate using optimization methods. Most existing choices can be inaccurate or slow to calibrate when there are many latent variables. Here, we propose a family of tractable variational approximations that are more accurate and faster to calibrate for this case. It combines a parsimonious parametric approximation for the parameter posterior, with the exact conditional posterior of the latent variables. We derive a simplified expression for the re-parameterization gradient of the variational lower bound, which is the main ingredient of efficient optimization algorithms used to implement variational estimation. To do so only requires the ability to generate exactly or approximately from the conditional posterior of the latent variables, rather than to compute its density. We illustrate using two complex contemporary econometric examples. The first is a nonlinear multivariate state space model for U.S. macroeconomic variables. The second is a random coefficients tobit model applied to two million sales by 20,000 individuals in a large consumer panel from a marketing study. In both cases, we show that our approximating family is considerably more accurate than mean field or structured Gaussian approximations, and faster than Markov chain Monte Carlo. Last, we show how to implement data sub-sampling in variational inference for our approximation, which can lead to a further reduction in computation time. MATLAB code implementing the method for our examples is included in supplementary material. ","Fast and Accurate Variational Inference for Models with Many Latent
  Variables"
62,1262180077166563328,20703003,Peter B Denton,"['New neutrino paper: <LINK>\n\n""Visible Decay of Astrophysical Neutrinos at IceCube""\n\nwith Asli Abdullahi a student with Silvia at Durham; it\'s her first paper!\n\nWe look at the pheno of a really cool kind of BSM that can do weird stuff for IceCube: nu decay.\n\n1/9', 'Neutrinos decay in the SM but their lifetime is crazy long. If there is new physics their lifetimes may be shorter.\n\nIceCube is one of the best places to look for this since they are measuring neutrinos from really far away and they measure all three flavors(ish).\n\n2/9', ""Invisible decay (where the decay products weren't accounted for) had been discussed before for IceCube, but not really visible decay as it's rather more involved. We decided to tackle the problem and show how it works. There are lots of dials to turn, so we turned them all!\n\n3/9"", 'The neutrino signal at the Earth depends not only on the decay parameters, but also on the initial spectrum in a non-trivial way, the mass scale of neutrinos, the nature of the coupling, and other things. Asli sorted out so many things, she did an absolutely tremendous job.\n\n4/9 https://t.co/R37AopzcNO', ""These things all modify the energy spectrum in a way that's different for each flavor. It turns out that the SM predicts (fairly robustly to astro concerns) that the spectrum of each flavor should be basically the same. So any deviation from this could be a sign of decay.\n\n5/9"", ""It turns out that IceCube might be seeing this! There is weak evidence at ~3-3.3σ that the spectra are different for different flavors. It's probably fluctuations or systematics, but if it's BSM this could be nu decay! (Blue is preferred by the data over the SM to the left.)\n\n6/9 https://t.co/KWA8Niz2dW"", 'I pointed this out in a previous paper with Irene Tamborra with a simple picture of nu decay (https://t.co/GrGdRxjvZQ) but this time we do the full thing with all the bells and whistles.\n\nThere are a lot of them.\n\nThe universe is expanding, they can decay en route twice, ...\n\n7/9', ""The crazy thing about this is that assuming the normal mass ordering, neutrino decay only lets you change the spectra in a certain way - and that's the same way the data is pointing.\n\n8/9 https://t.co/foi0H5yUvr"", 'Tin foil hats aside, neutrino decay has a lot of fun phenomenology to investigate for IceCube. While the formulas are a bit involved, we put the code and the data files for all the curves online. https://t.co/RjnB0lNTdo\n\n9/9']",https://arxiv.org/abs/2005.07200,"Neutrino decay modifies neutrino propagation in a unique way; not only is there flavor changing as there is in neutrino oscillations, there is also energy transport from initial to final neutrinos. The most sensitive direct probe of neutrino decay is currently IceCube which can measure the energy and flavor of neutrinos traveling over extragalactic distances. For the first time we calculate the flavor transition probability for the cases of visible and invisible neutrino decay, including the effects of the expansion of the universe, and consider the implications for IceCube. As an example, we demonstrate how neutrino decay addresses a tension in the IceCube data. We also provide a publicly available code to calculate the effect of visible decay. ",Visible Decay of Astrophysical Neutrinos at IceCube
63,1261378663137906691,937127267850846208,Mohammad Javad Amiri,"['Check out our new paper: ""SEPAR: A Privacy-Preserving Blockchain-based System for Regulating Multi-Platform Crowdworking Environments"". SEPAR is a multi-platform crowdworking system that enforces global constraints on distributed independent entities.\n\n<LINK>']",https://arxiv.org/abs/2005.01038,"Crowdworking platforms provide the opportunity for diverse workers to execute tasks for different requesters. The popularity of the ""gig"" economy has given rise to independent platforms that provide competing and complementary services. Workers as well as requesters with specific tasks may need to work for or avail from the services of multiple platforms resulting in the rise of multi-platform crowdworking systems. Recently, there has been increasing interest by governmental, legal and social institutions to enforce regulations, such as minimal and maximal work hours, on crowdworking platforms. Platforms within multi-platform crowdworking systems, therefore, need to collaborate to enforce cross-platform regulations. While collaborating to enforce global regulations requires the transparent sharing of information about tasks and their participants, the privacy of all participants needs to be preserved. In this paper, we propose an overall vision exploring the regulation, privacy, and architecture dimensions for the future of work multi-platform crowdworking environments. We then present SEPAR, a multi-platform crowdworking system that enforces a large sub-space of practical global regulations on a set of distributed independent platforms in a privacy-preserving manner. SEPAR, enforces privacy using lightweight and anonymous tokens, while transparency is achieved using fault-tolerant blockchains shared across multiple platforms. The privacy guarantees of SEPAR against covert adversaries are formalized and thoroughly demonstrated, while the experiments reveal the efficiency of SEPAR in terms of performance and scalability. ","SEPAR: Towards Regulating Future of Work Multi-Platform Crowdworking
  Environments with Privacy Guarantees"
64,1261342487752441857,3836403113,Joe Serigano,"['Hi, I have a new paper on the arXiv! We’ve been using some of the last data sent back from @CassiniSaturn to better understand the region between Saturn’s innermost ring and upper atmosphere. Here’s a lil thread about our new paper~~\n<LINK>\n1/n', 'Let’s start in 2017. This was Cassini’s last year (😢) and the start of a new set of orbits that allowed the spacecraft to fly between the planet and the rings for the first time ever. During the last 5 orbits Cassini directly sampled Saturn’s upper atmosphere.\n2/n', 'We’ve been analyzing compositional measurements from these last orbits using data from the Ion and Neutral Mass Spectrometer (INMS) that was aboard Cassini. If you’ve seen any recent INMS studies then you already know that the results have been *pretty* surprising. \n3/n', 'First, we’ve detected an influx of material falling into Saturn’s atmosphere from the rings. Evidence of “ring rain” has actually been around since the Voyager era, and more recently @physicsJ has provided additional evidence of this phenomenon using the Keck telescope. \n4/n', 'Saturn’s rings are almost entirely water ice (&gt;95%) … but we’ve been detecting a significant influx of non-water ice material from the rings, which includes a lot of methane 🤔🤯!\n5/n', 'Analyzing mass spectra is complicated. It’s especially complicated when your mass spectrometer has low resolution and sparse calibration data, was built ~30 years ago, and has been zooming around the solar system since 1997.\n6/n https://t.co/4U1SdzWD9P', 'To overcome some of these issues, we’ve developed a mass spectral deconvolution algorithm using a Monte-Carlo approach. It allows us to use existing calibration data from different sources with more flexibility and determine the most probable concentration of each species.\n7/n', 'We hope our model can be useful for others in the future too! You can find more info about it in this paper:\nhttps://t.co/bv0vAiePz2\n8/n', 'Okay back to our new paper. It focuses on just a small (but important!) part of the mass spectrum where signal from water, methane, and ammonia all overlap. We’re able to use our model results to determine density profiles and mixing ratios of these species. \n9/n', 'Water and ammonia show much greater variability than methane from orbit to orbit. We lay out a few possible explanations for this in our paper, including differences in volatility and/or proton affinity of these species. \n10/n https://t.co/eV58n0Jj77', 'We also quantify the amount of material being deposited into Saturn’s atmosphere, which represents a lower limit since we’ve focused on just a small part of the mass spectrum. Our results agree with previous studies that Saturn’s rings are disappearing at an alarming rate!\n11/n https://t.co/hNUwLvmDmD', 'The unexpectedly large mass influx from the rings and the surprisingly complex chemical composition of this region of Saturn is still a bit puzzling and there’s plenty more work to be done. Stay tuned for more results in the coming months!\n12/12 https://t.co/iy6md23RDI', '@LeighFletcher Thanks Leigh!']",https://arxiv.org/abs/2005.06554,"The Cassini spacecraft's last orbits directly sampled Saturn's thermosphere and revealed a much more chemically complex environment than previously believed. Observations from the Ion and Neutral Mass Spectrometer (INMS) aboard Cassini provided compositional measurements of this region and found an influx of material raining into Saturn's upper atmosphere from the rings. We present here an in-depth analysis of the CH$_4$, H$_2$O, and NH$_3$ signal from INMS and provide further evidence of external material entering Saturn's atmosphere from the rings. We use a new mass spectral deconvolution algorithm to determine the amount of each species observed in the spectrum and use these values to determine the influx and mass deposition rate for these species. ","Compositional Measurements of Saturn's Upper Atmosphere and Rings from
  Cassini INMS"
65,1261235811787452417,44124642,Matthew Aldridge,"['I have a new paper up on the arXiv, for people who are interested in that sort of thing. <LINK> Weird that group testing has now become so relevant that my theorems come with an actual health warning. <LINK>']",https://arxiv.org/abs/2005.06617,"Inspired by applications in testing for Covid-19, we consider a variant of two-stage group testing called ""conservative"" (or ""trivial"") two-stage testing, where every item declared to be defective must be definitively confirmed by being tested by itself in the second stage. We study this in the linear regime where the prevalence is fixed while the number of items is large. We study various nonadaptive test designs for the first stage, and derive a new lower bound for the total number of tests required. We find that a first-stage design as studied by Broder and Kumar (arXiv:2004.01684) with constant tests per item and constant items per test is extremely close to optimal for all prevalences, and is optimal in the limit as the prevalence tends to zero. Simulations back up the theoretical results. ",Conservative two-stage group testing in the linear regime
66,1261190688684806144,157973000,Michael Pfarrhofer,"['New working paper by @FlorianHuber8 and me on using dynamic shrinkage priors for time-varying parameter stochastic volatility in mean models to forecast inflation for the United States, the United Kingdom and the Euro Area #econometrics #forecasting\n<LINK>']",https://arxiv.org/abs/2005.06851,"Successful forecasting models strike a balance between parsimony and flexibility. This is often achieved by employing suitable shrinkage priors that penalize model complexity but also reward model fit. In this note, we modify the stochastic volatility in mean (SVM) model proposed in Chan (2017) by introducing state-of-the-art shrinkage techniques that allow for time-variation in the degree of shrinkage. Using a real-time inflation forecast exercise, we show that employing more flexible prior distributions on several key parameters slightly improves forecast performance for the United States (US), the United Kingdom (UK) and the Euro Area (EA). Comparing in-sample results reveals that our proposed model yields qualitatively similar insights to the original version of the model. ","Dynamic shrinkage in time-varying parameter stochastic volatility in
  mean models"
67,1261169713855582208,24360491,Nadav Cohen,"['New paper on implicit regularization in deep learning: <LINK> (joint w/Noam Razin).  Settles open question by proving it cannot be explained through norms.  Turns out rank may be a better way to look at things.  Blog post coming soon...', '@SuryaGanguli @roydanroy Of course!  I read it in depth when it got out :)']",https://arxiv.org/abs/2005.06398,"Mathematically characterizing the implicit regularization induced by gradient-based optimization is a longstanding pursuit in the theory of deep learning. A widespread hope is that a characterization based on minimization of norms may apply, and a standard test-bed for studying this prospect is matrix factorization (matrix completion via linear neural networks). It is an open question whether norms can explain the implicit regularization in matrix factorization. The current paper resolves this open question in the negative, by proving that there exist natural matrix factorization problems on which the implicit regularization drives all norms (and quasi-norms) towards infinity. Our results suggest that, rather than perceiving the implicit regularization via norms, a potentially more useful interpretation is minimization of rank. We demonstrate empirically that this interpretation extends to a certain class of non-linear neural networks, and hypothesize that it may be key to explaining generalization in deep learning. ",Implicit Regularization in Deep Learning May Not Be Explainable by Norms
68,1261158844337721346,23980621,"Brett Morris, PhD","['New paper! Friends like @HoeijmakersJens use high resolution spectroscopy and cross correlation to recover weak signals like emission from exoplanet atmospheres. \n\nCan we use the same technique to detect small starspot coverages on Sun-like stars? \n\n<LINK>', ""TiO forms in stellar atmospheres at temperatures &lt;4000 K and has loads of absorption lines, so if you have a G or K star which has cool spots that dip below that threshold, you might be able to detect TiO with the CCF. Let's use TiO as a tracer of cool starspots!"", 'We tested this hypothesis on a bunch of HARPS spectra of Sun-like stars. We found obvious detections on the young T Tauri stars LkCa 4 (@gully_\'s star) and AA Tau, which are classified as ""K"" dwarfs but &gt;50% of their surfaces are covered in cooler regions. https://t.co/TjxBEqvtUI', ""The big dip in the CCF shows the signal, an absorption feature generated by the TiO in cool regions in the atmospheres, which coincides perfectly with the radial velocity of the star (vertical line). This is the signal we're looking for:\xa0cool regions on otherwise warmer stars."", ""Now let's look at a much less spotted star –\xa0the Sun.\xa0Can we see evidence of TiO in solar HARPS spectra when the Sun was spotted? Nope, no TiO detected here. https://t.co/AIlYyG5Kt3"", 'For the Sun, perhaps the S/N was too low to detect the &lt;1% of the stellar surface which was &lt;4000 K. So what if we stacked thousands of spectra together to build up S/N? Not all the solar data are public, but we *can* do that for 18 Scorpii, an excellent solar twin!', ""We stacked 4000 HARPS spectra of 18 Sco for a combined S/N &gt; 2000. Despite the technique's great sensitivity to weak absorption features, we don't detect any spot-tracer absorption here. 18 Sco probably isn't more spotted than the Sun at solar max. https://t.co/ueJk6WlkZ4"", ""Conclusions: \n–\xa0starspot coverages &lt;10s of % are invisible to the CCF at S/N~200\n–\xa0TiO from active photospheres *won't* create competing signals when searching for TiO from exoplanet atmospheres\n–\xa0the HARPS archive is filled with interesting experiments waiting to be crafted!"", 'Extra big thanks to my very tolerant office-mate @HoeijmakersJens who humored all of my crazy questions until this project came to fruition, and to Daniel Kitzmann for generating the spectral templates that enabled the hunt.', ""@exoZafar @HoeijmakersJens Thanks!\n\nRE HAT-P-11, it could be: \n–\xa0spot penumbra aren't cool enough to produce TiO, and only the much dimmer and smaller umbra are\n–\xa0spot coverages up to ~10% on a K dwarf aren't accessible with S/N ~ 200 over this wavelength range\n–\xa0the TiO line list could be improved"", '@exoZafar @HoeijmakersJens RE M dwarfs: there was a section of this paper on precisely this topic, but it turned out to be really difficult to do, mostly because the wavelength range of HARPS has barely any absorption features due to cool H2O. Hopefully, e.g., @CARMENES_exopl will look into this!']",https://arxiv.org/abs/2005.06749,"We present a method for detecting starspots on cool stars using the cross-correlation function (CCF) of high resolution molecular spectral templates applied to archival high-resolution spectra of G and K stars observed with HARPS/HARPS-N. We report non-detections of starspots on the Sun even when the Sun was spotted, the solar twin 18 Scorpii, and the very spotted Sun-like star HAT-P-11, suggesting that Sun-like starspot distributions will be invisible to the CCF technique, and should not produce molecular absorption signals which might be confused for signatures of exoplanet atmospheres. We detect strong TiO absorption in the T Tauri K-dwarfs LkCa 4 and AA Tau, consistent with significant coverage by cool regions. We show that despite the non-detections, the technique is sensitive to relatively small spot coverages on M dwarfs and large starspot areas on Sun-like stars. ",Hunt for Starspots in HARPS Spectra of G and K Stars
69,1261150381599490049,736541978,Jean-Claude Besse,['New paper out with a great team @qudev and quite a few entangled modes of microwave radiation. <LINK> <LINK>'],http://arxiv.org/abs/2005.07060,"Sources of entangled electromagnetic radiation are a cornerstone in quantum information processing and offer unique opportunities for the study of quantum many-body physics in a controlled experimental setting. While multi-mode entangled states of radiation have been generated in various platforms, all previous experiments are either probabilistic or restricted to generate specific types of states with a moderate entanglement length. Here, we demonstrate the fully deterministic generation of purely photonic entangled states such as the cluster, GHZ, and W state by sequentially emitting microwave photons from a controlled auxiliary system into a waveguide. We tomographically reconstruct the entire quantum many-body state for up to $N=4$ photonic modes and infer the quantum state for even larger $N$ from process tomography. We estimate that localizable entanglement persists over a distance of approximately ten photonic qubits, outperforming any previous deterministic scheme. ","Realizing a Deterministic Source of Multipartite-Entangled Photonic
  Qubits"
70,1261100792729071616,3260753346,Deepanway Ghosal,"['Excited to share our new #acl2020nlp paper KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis with @hdevamanyu @soujanyaporia @radamihalcea, Abhinaba Roy, and Navonil Majumder.\nPaper: <LINK>\nCode: <LINK>\n#NLProc <LINK>', 'In this work, we avail commonsense knowledge from ConceptNet to perform domain adaptation and show the ability of graph features helping in bridging domain gap between seen and unseen domains.']",https://arxiv.org/abs/2005.00791,"Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework. ",KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis
71,1261100300678492160,1074633382452051969,Kimin,"['New work with @young93k, Seunghyun Lee, @honglaklee, and Jinwoo Shin - Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning! \n\nPaper: <LINK> \nProject Page: <LINK> \nCode: <LINK>', 'Main problem:\nWe study how to learn a global dynamics model that can generalize across different dynamics https://t.co/ea4A9XWHIb', 'Method:\nWe separate context encoding and transition inference, and propose various auxiliary tasks to extract contextual information effectively https://t.co/q1R5jfcVYk', 'Results 1:\nThe proposed context-aware dynamics model significantly improves the generalization performances of baseline model-based methods. https://t.co/v4u66E2hdQ', 'Result 2:\nYou can also utilize the learned context latent vector to improve the generalization abilities of model-free RL methods. https://t.co/dSH0wepzsz']",https://arxiv.org/abs/2005.06800,"Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics is a challenging task. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes. ","Context-aware Dynamics Model for Generalization in Model-Based
  Reinforcement Learning"
72,1261012377987739648,1012783002789675008,Chris O'Connor,"['New paper on arXiv today, my first of grad school!🔗 <LINK>🔗In which we learn how squishy an asteroid is by watching it be eaten by a dead star. #astronomy']",https://arxiv.org/abs/2005.05977,"Several white dwarfs with atmospheric metal pollution have been found to host small planetary bodies (planetesimals) orbiting near the tidal disruption radius. We study the physical properties and dynamical origin of these bodies under the hypothesis that they underwent high-eccentricity migration from initial distances of several astronomical units. We examine two plausible mechanisms for orbital migration and circularization: tidal friction and ram-pressure drag in a compact disc. For each mechanism, we derive general analytic expressions for the evolution of the orbit that can be rescaled for various situations. We identify the physical parameters that determine whether a planetesimal's orbit can circularize within the appropriate time-scale and constrain these parameters based on the properties of the observed systems. For tidal migration to work, an internal viscosity similar to that of molten rock is required, and this may be naturally produced by tidal heating. For disc migration to operate, a minimal column density of the disc is implied; the inferred total disc mass is consistent with estimates of the total mass of metals accreted by polluted WDs. ","High-eccentricity migration of planetesimals around polluted white
  dwarfs"
73,1260939443139153923,476582730,Shakir Mohamed,"['Nowcasting was a new and fun field for us, and after a while we thought to write a paper we wished we had when we started. Thankful for hard work by @RachelPrudden &amp; team 🎉 This areas exposes many core questions in ML so a great for advancing research <LINK> <LINK>']",https://arxiv.org/abs/2005.04988,"A 'nowcast' is a type of weather forecast which makes predictions in the very short term, typically less than two hours - a period in which traditional numerical weather prediction can be limited. This type of weather prediction has important applications for commercial aviation; public and outdoor events; and the construction industry, power utilities, and ground transportation services that conduct much of their work outdoors. Importantly, one of the key needs for nowcasting systems is in the provision of accurate warnings of adverse weather events, such as heavy rain and flooding, for the protection of life and property in such situations. Typical nowcasting approaches are based on simple extrapolation models applied to observations, primarily rainfall radar. In this paper we review existing techniques to radar-based nowcasting from environmental sciences, as well as the statistical approaches that are applicable from the field of machine learning. Nowcasting continues to be an important component of operational systems and we believe new advances are possible with new partnerships between the environmental science and machine learning communities. ","A review of radar-based nowcasting of precipitation and applicable
  machine learning techniques"
74,1260921356306317314,1834931742,Yifan Qian,"['Can we leverage Graph Convolutional Networks (@thomaskipf) to the data where no graph structure exists explicitly in empirical domains? In this new paper, we explore if graphs extracted from the features themselves can aid classification performance.\n<LINK> <LINK>', 'We show that constructing optimal geometric graphs directly from data features can aid classification tasks on both synthetic and real-world data sets from different domains, ranging from text to music track features to single-cell transcriptomics. https://t.co/y94jXyugSm', 'Second, we introduce two metrics to characterize optimal graphs: i) by measuring the alignment between the subspaces spanned by the features convolved with the graph and the ground truth; https://t.co/hxYzYpF0AE', 'and ii) ratio of class separation in the output activations of Graph Convolutional Networks: this shows that the optimal graph maximally separates classes. https://t.co/ayZPqt2u40', 'Finally, we find that sparsifying the optimal graph can potentially improve classification performance. https://t.co/BnNdPow35E', 'Joint work with Paul Expert (@ExpertPol), Pietro Panzarasa and Mauricio Barahona.']",https://arxiv.org/abs/2005.04081,"Traditional classification tasks learn to assign samples to given classes based solely on sample features. This paradigm is evolving to include other sources of information, such as known relations between samples. Here we show that, even if additional relational information is not available in the data set, one can improve classification by constructing geometric graphs from the features themselves, and using them within a Graph Convolutional Network. The improvement in classification accuracy is maximized by graphs that capture sample similarity with relatively low edge density. We show that such feature-derived graphs increase the alignment of the data to the ground truth while improving class separation. We also demonstrate that the graphs can be made more efficient using spectral sparsification, which reduces the number of edges while still improving classification performance. We illustrate our findings using synthetic and real-world data sets from various scientific domains. ","Geometric graphs from data to aid classification tasks with graph
  convolutional networks"
75,1260747777446797313,43284947,Luke Hutchison ☮️,"['I just submitted a CS paper on a new parsing algorithm that parses an input *in reverse* (right-to-left, bottom-up), which (highly surprisingly) solves the left recursion and error recovery problems that have beset recursive descent parsers for years.\n<LINK>', '@earnmyturns @zakkohane Thanks. Yes, I talk about shift-reduce / recursive ascent parsing in the intro. However those parsers still work left to right, bottom up, and they run into shift-reduce and reduce-reduce conflicts, which are also resolved when parsing right to left.']",https://arxiv.org/abs/2005.06444,"A recursive descent parser is built from a set of mutually-recursive functions, where each function directly implements one of the nonterminals of a grammar. A packrat parser uses memoization to reduce the time complexity for recursive descent parsing from exponential to linear in the length of the input. Recursive descent parsers are extremely simple to write, but suffer from two significant problems: (i) left-recursive grammars cause the parser to get stuck in infinite recursion, and (ii) it can be difficult or impossible to optimally recover the parse state and continue parsing after a syntax error. Both problems are solved by the pika parser, a novel reformulation of packrat parsing as a dynamic programming algorithm, which requires parsing the input in reverse: bottom-up and right to left, rather than top-down and left to right. This reversed parsing order enables pika parsers to handle grammars that use either direct or indirect left recursion to achieve left associativity, simplifying grammar writing, and also enables optimal recovery from syntax errors, which is a crucial property for IDEs and compilers. Pika parsing maintains the linear-time performance characteristics of packrat parsing as a function of input length. The pika parser was benchmarked against the widely-used Parboiled2 and ANTLR4 parsing libraries. The pika parser performed significantly better than the other parsers for an expression grammar, although for a complex grammar implementing the Java language specification, a large constant performance impact was incurred per input character. Therefore, if performance is important, pika parsing is best applied to simple to moderate-sized grammars, or to very large inputs, if other parsing alternatives do not scale linearly in the length of the input. Several new insights into precedence, associativity, and left recursion are presented. ","Pika parsing: reformulating packrat parsing as a dynamic programming
  algorithm solves the left recursion and error recovery problems"
76,1260650742861361153,2715793145,Tirtha Banerjee,['New paper alert: Persistence analysis of velocity and temperature fluctuations in convective surface layer turbulence. Led by \u2066\u2066@Subharthi10\u2069. <LINK>'],https://arxiv.org/abs/2005.04958,"Persistence is defined as the probability that the local value of a fluctuating field remains at a particular state for a certain amount of time, before being switched to another state. The concept of persistence has been found to have many diverse practical applications, ranging from non-equilibrium statistical mechanics to financial dynamics to distribution of time scales in turbulent flows and many more. In this study, we carry out a detailed analysis of the statistical characteristics of the persistence probability density functions (PDFs) of velocity and temperature fluctuations in the surface layer of a convective boundary layer, using a field-experimental dataset. Our results demonstrate that for the time scales smaller than the integral scales, the persistence PDFs of turbulent velocity and temperature fluctuations display a clear power-law behaviour, associated with self-similar eddy cascading mechanism. Moreover, we also show that the effects of non-Gaussian temperature fluctuations act only at those scales which are larger than the integral scales, where the persistence PDFs deviate from the power-law and drop exponentially. Furthermore, the mean time scales of the negative temperature fluctuation events persisting longer than the integral scales are found to be approximately equal to twice the integral scale in highly convective conditions. However, with stability this mean time scale gradually decreases to almost being equal to the integral scale in the near neutral conditions. Contrarily, for the long positive temperature fluctuation events, the mean time scales remain roughly equal to the integral scales, irrespective of stability. ","Persistence analysis of velocity and temperature fluctuations in
  convective surface layer turbulence"
77,1260647009008017408,152143,Eszter Hargittai 😷💉🌻,"['how do accuracy, privacy &amp; public health benefits relate to willingness to adopt #Covid19 app? our new paper: <LINK> w/@eredmil1 @gkaptchuk @dggoldst @jakehofman']",https://arxiv.org/abs/2005.04343,"A growing number of contact tracing apps are being developed to complement manual contact tracing. A key question is whether users will be willing to adopt these contact tracing apps. In this work, we survey over 4,500 Americans to evaluate (1) the effect of both accuracy and privacy concerns on reported willingness to install COVID19 contact tracing apps and (2) how different groups of users weight accuracy vs. privacy. Drawing on our findings from these first two research questions, we (3) quantitatively model how the amount of public health benefit (reduction in infection rate), amount of individual benefit (true-positive detection of exposures to COVID), and degree of privacy risk in a hypothetical contact tracing app may influence American's willingness to install. Our work takes a descriptive ethics approach toward offering implications for the development of policy and app designs related to COVID19. ","How good is good enough for COVID19 apps? The influence of benefits,
  accuracy, and privacy on willingness to adopt"
78,1260606963634843650,2444302555,Ludovic Denoyer,"['Our new paper  ""Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization"" is now online: <LINK>\n\n@pa_kamienny, @teopir, Alessandro Lazaric, Thibault Lavril, Nicolas Usunier, @LudovicDenoyer <LINK>']",https://arxiv.org/abs/2005.02934,"We study the problem of learning exploration-exploitation strategies that effectively adapt to dynamic environments, where the task may change over time. While RNN-based policies could in principle represent such strategies, in practice their training time is prohibitive and the learning process often converges to poor solutions. In this paper, we consider the case where the agent has access to a description of the task (e.g., a task id or task parameters) at training time, but not at test time. We propose a novel algorithm that regularizes the training of an RNN-based policy using informed policies trained to maximize the reward in each task. This dramatically reduces the sample complexity of training RNN-based policies, without losing their representational power. As a result, our method learns exploration strategies that efficiently balance between gathering information about the unknown and changing task and maximizing the reward over time. We test the performance of our algorithm in a variety of environments where tasks may vary within each episode. ","Learning Adaptive Exploration Strategies in Dynamic Environments Through
  Informed Policy Regularization"
79,1260533396310196225,138395156,Professor Sondipon Adhikari,"['Our new paper from a collaboration between @SU_engIMPACT and U. of British Columbia entitled ""Machine learning based #digitaltwins for dynamical systems with multiple time-scales"" is now available for download from <LINK>. Please send your comments/suggestions. <LINK>']",http://arxiv.org/abs/2005.05862,"Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components -- (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-scale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the `mixture of experts using GP', an efficient framework the exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated. ","Machine learning based digital twin for dynamical systems with multiple
  time-scales"
80,1260377509092302849,927909431689596929,Matthew Mizuhara,"['Excited to announce a new paper with collaborator Georgi Medvedev:\n<LINK>\n\nWe present a method to study clusters of oscillators in the Kuramoto model with inertia. Through this framework, we can decouple the clusters and create chimera states! <LINK>', '@LongFormMath Thanks!! :)', '@jasnyder610 Thanks! Yes, I would also be really interested in learning more about your work as well!']",http://arxiv.org/abs/2005.05367,"The Kuramoto model of coupled phase oscillators with inertia on Erdos-Renyi graphs is analyzed in this work. For a system with intrinsic frequencies sampled from a bimodal distribution we identify a variety of two cluster patterns and study their stability. To this end, we decompose the description of the cluster dynamics into two systems: one governing the (macro) dynamics of the centers of mass of the two clusters and the second governing the (micro) dynamics of individual oscillators inside each cluster. The former is a low-dimensional ODE whereas the latter is a system of two coupled Vlasov PDEs. Stability of the cluster dynamics depends on the stability of the low-dimensional group motion and on coherence of the oscillators in each group. We show that the loss of coherence in one of the clusters leads to the loss of stability of a two-cluster state and to formation of chimera states. The analysis of this paper can be generalized to cover states with more than two clusters and to coupled systems on W-random graphs. Our results apply to a model of a power grid with fluctuating sources. ","Stability of clusters in the second-order Kuramoto model on random
  graphs"
81,1260224975975452673,842060300279025666,Pavel Shvechikov,"['Our new paper, Truncated Quantile Critics, improves SOTA on MuJoCo by 20-30% !   With TF and PT code.\n  \nCredit to @brickerino @shvechikov_p @AlexGrishin_ and Dmitry Vetrov. \n\nVideo: <LINK>\nProject page: <LINK>\nPaper: <LINK>', 'TQC builds heavily on the strong and stable Soft Actor Critic by @haarnoja @pabbeel @svlevine et al. \n\nWe continue the ""overestimation in the continuous control"" line of research started by Fujimoto,  et al.  in the TD3 paper and replace the min(Q1, Q2) trick.', 'For this, we adapt the QR-DQN by @wwdabney, Mark Rowland, @marcgbellemare, Remi Munos for the continuous setting.  \n\nTo control the overestimation intensity we truncate the distribution predicted by a quantile critic.', 'The last stroke to TQC is ensembling, which we do naturally as the mixture of predicted distributions.\n\nTo achieve robustness, we first compute the mixture and truncate only afterwards.', ""The TQC's estimate captures both types of uncertainty. \nAleatoric -- due to the essence of learning the distribution of returns. \nEpistemic -- due to the multiplicity of distributional critics.""]",https://arxiv.org/abs/2005.04269,"The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment. ","Controlling Overestimation Bias with Truncated Mixture of Continuous
  Distributional Quantile Critics"
82,1260140973197164544,96253726,Navin Sridhar,"[""New paper alert! <LINK>\nFor the first time, a 'Fast Radio Burst' was observed from our Galaxy, with coincident X-ray counterpart!\nIn this paper, we discuss the implications of this discovery in the context of extragalactic magnetars and magnetar-based FRB models. <LINK>""]",https://arxiv.org/abs/2005.05283,"A luminous radio burst was recently detected in temporal coincidence with a hard X-ray flare from the Galactic magnetar SGR 1935+2154 with a time and frequency structure consistent with cosmological fast radio bursts (FRB) and a fluence within a factor of $\lesssim 10$ of the least energetic extragalactic FRB previously detected. Although active magnetars are commonly invoked FRB sources, several distinct mechanisms have been proposed for generating the radio emission which make different predictions for the accompanying higher frequency radiation. We show that the properties of the coincident radio and X-ray flares from SGR 1935+2154, including their approximate simultaneity and relative fluence $E_{\rm radio}/E_{\rm X} \sim 10^{-5}$, as well as the duration and spectrum of the X-ray emission, are consistent with extant predictions for the synchrotron maser shock model. Rather than arising from the inner magnetosphere, the X-rays are generated by (incoherent) synchrotron radiation from thermal electrons heated at the same shocks which produce the coherent maser emission. Although the rate of SGR 1935+2154-like bursts in the local universe is not sufficient to contribute appreciably to the extragalactic FRB rate, the inclusion of an additional population of more active magnetars with stronger magnetic fields than the Galactic population can explain both the FRB rate as well as the repeating fraction, however only if the population of active magnetars are born at a rate that is at least two-orders of magnitude lower than that of SGR 1935+2154-like magnetars. This may imply that the more active magnetar sources are not younger magnetars formed in a similar way to the Milky Way population (e.g. via ordinary supernovae), but instead through more exotic channels such as superluminous supernovae, accretion-induced collapse or neutron star mergers. ","Implications of a ""Fast Radio Burst"" from a Galactic Magnetar"
83,1260097602814570498,974769773539155968,Daniel Baumann,"['New paper out. I am very proud of the hard work by my collaborators Guilherme Pimentel, Austin Joyce, Hayden Lee and Carlos Duaso Pueyo. <LINK> <LINK>']",https://arxiv.org/abs/2005.04234,"We extend the cosmological bootstrap to correlators involving massless particles with spin. In de Sitter space, these correlators are constrained both by symmetries and by locality. In particular, the de Sitter isometries become conformal symmetries on the future boundary of the spacetime, which are reflected in a set of Ward identities that the boundary correlators must satisfy. We solve these Ward identities by acting with weight-shifting operators on scalar seed solutions. Using this weight-shifting approach, we derive three- and four-point correlators of massless spin-1 and spin-2 fields with conformally coupled scalars. Four-point functions arising from tree-level exchange are singular in particular kinematic configurations, and the coefficients of these singularities satisfy certain factorization properties. We show that in many cases these factorization limits fix the structure of the correlators uniquely, without having to solve the conformal Ward identities. The additional constraint of locality for massless spinning particles manifests itself as current conservation on the boundary. We find that the four-point functions only satisfy current conservation if the s, t, and u-channels are related to each other, leading to nontrivial constraints on the couplings between the conserved currents and other operators in the theory. For spin-1 currents this implies charge conservation, while for spin-2 currents we recover the equivalence principle from a purely boundary perspective. For multiple spin-1 fields, we recover the structure of Yang-Mills theory. Finally, we apply our methods to slow-roll inflation and derive a few phenomenologically relevant scalar-tensor three-point functions. ","The Cosmological Bootstrap: Spinning Correlators from Symmetries and
  Factorization"
84,1260042163951460353,2819715191,Antonella Palmese,['New paper out by Alex Kim and myself on constraining the growth of structure with gravitational waves and @desisurvey and other galaxy surveys!\n<LINK> <LINK>'],http://arxiv.org/abs/2005.04325v1,"The low-redshift velocity field is a unique probe of the growth of cosmic structure and gravity. We propose to use distances from gravitational wave (GW) detections, in conjunction with the redshifts of their host galaxies from wide field spectroscopic surveys (e.g. DESI, 4MOST, TAIPAN), to measure peculiar motions within the local Universe. Such measurement has the potential to constrain the growth rate $f\sigma_8$ and test gravity through determination of the gravitational growth index $\gamma$, complementing constraints from other peculiar velocity measurements. We find that binary neutron star mergers with associated counterpart at $z\lesssim 0.2$ that will be detected by the Einstein Telescope (ET) will be able to constrain $f\sigma_8$ to $\sim 3\%$ precision after $10$ years of operations when combined with galaxy overdensities from DESI and TAIPAN. If a larger network of third generation GW detectors is available (e.g. including the Cosmic Explorer), the same constraints can be reached over a shorter timescale ($\sim 5$ years for a 3 detectors network). The same events (plus information from their hosts' redshifts) can constrain $\gamma$ to $\sigma_\gamma\lesssim 0.04$. This constraint is precise enough to discern General Relativity from other popular gravity models at $3\sigma$. This constraint is improved to $\sigma_\gamma\sim 0.02-0.03$ when combined with galaxy overdensities. The potential of combining galaxies' peculiar velocities with gravitational wave detections for cosmology highlights the need for extensive optical to near--infrared follow--up of nearby gravitational wave events, or exquisite GW localization, in the next decade. ","] Probing gravity and growth of structure with gravitational waves and
  galaxies' peculiar velocity"
85,1260041077815488513,109255123,Danny Caballero 🇲🇽,['New paper by my student @johnmaiken that quantitatively investigates aspects of Tinto’s theory of dropout. <LINK>'],http://arxiv.org/abs/2005.05104,"The time it takes a student to graduate with a university degree is mitigated by a variety of factors such as their background, the academic performance at university, and their integration into the social communities of the university they attend. Different universities have different populations, student services, instruction styles, and degree programs, however, they all collect institutional data. This study presents data for 160,933 students attending a large American research university. The data includes performance, enrollment, demographics, and preparation features. Discrete time hazard models for the time-to-graduation are presented in the context of Tinto's Theory of Drop Out. Additionally, a novel machine learning method: gradient boosted trees, is applied and compared to the typical maximum likelihood method. We demonstrate that enrollment factors (such as changing a major) lead to greater increases in model predictive performance of when a student graduates than performance factors (such as grades) or preparation (such as high school GPA). ",Predicting time to graduation at a large enrollment American university
86,1260032072615256065,326843207,Yuta Notsu,"['Our new paper ""Optical and X-ray observations of stellar flares on an active M dwarf AD Leonis with Seimei\nTelescope, SCAT, NICER and OISTER""  is accepted to PASJ and now in arXiv !!   <LINK> \n\nNamekata, Maehara, Sasaki, Kawai, Notsu, Kowalski, Allred, et al.', 'First stellar flare observation paper from @Obs_kyoto_u 3.8m Seimei Telescope https://t.co/KCI5HJHUt7, which started the science operation last year !!  \n\nTwelve flares are detected in total which include ten Hα, four X-ray, and four optical-continuum flares. https://t.co/OtkbEtKjcj', '“We found that (1) during the superflare, the Hα emission line full width at 1/8 maximum dramatically increases to 14 Å from 8 Å accompanied with the large white-light flares, (2) some weak Hα/X-ray flares are not accompanied with white-light emissions, and …. https://t.co/fjb8JnvTLw', '(3) the non-flaring emissions show clear rotational modulations in X-ray and Hα intensity in the same phase. ” https://t.co/MyuJcQbny4', 'Hα line profiles with hard and high-energy non-thermal electron beams are consistent withthe initial phase line profiles of the superflares, while those with more soft- and/or weak-energy beam are consistent with those in decay phases, indicating the changes in the energy fluxes https://t.co/A7mmxPyUzb', 'Also, we found that the relation between optical continuum and Hα intensity is nonlinear, which can be one cause of the non-white-light flares.  (Possibly helpful for understanding ongoing simultaneous observations with TESS (white-light) and chromospheric lines??) https://t.co/3E9oFtaPE4']",https://arxiv.org/abs/2005.04336,"We report multi-wavelength monitoring observations of an M-dwarf flare star AD Leonis with Seimei Telescope (6150--7930 {\AA}), SCAT (Spectroscopic Chuo-university Astronomical Telescope; 3700--7500 {\AA}), NICER (Neutron Star Interior Composition Explorer; 0.2--12.0 keV), and collaborations of OISTER (Optical and Infrared Synergetic Telescopes for Education and Research) program. Twelve flares are detected in total which include ten H$\alpha$, four X-ray, and four optical-continuum flares; one of them is a superflare with the total energy of $\sim$ 2.0$\times$10$^{33}$ erg. We found that (1) during the superflare, the H$\alpha$ emission line full width at 1/8 maximum dramatically increases to 14 {\AA} from 8 {\AA} in the low-resolution spectra (R$\sim$ 2000) accompanied with the large white-light flares, (2) some weak H$\alpha$/X-ray flares are not accompanied with white-light emissions, and (3) the non-flaring emissions show clear rotational modulations in X-ray and H$\alpha$ intensity in the same phase. To understand these observational features, one-dimensional hydrodynamic flare simulations are performed by using the RADYN code. As a result of simulations, we found the simulated H$\alpha$ line profiles with hard and high-energy non-thermal electron beams are consistent with that of the initial phase line profiles of the superflares, while those with more soft- and/or weak-energy beam are consistent with those in decay phases, indicating the changes in the energy fluxes injected to the lower atmosphere. Also, we found that the relation between optical continuum and H$\alpha$ intensity is nonlinear, which can be one cause of the non-white-light flares. The flare energy budget exhibits diversity in the observations and models, and more observations of stellar flares are necessary for constraining the occurrence of various emission line phenomena in stellar flares. ","Optical and X-ray observations of stellar flares on an active M dwarf AD
  Leonis with Seimei Telescope, SCAT, NICER and OISTER"
87,1260029362658656256,141440459,Rod Van Meter 🌻,['New paper dance!  Attacking the #QuantumInternet.\n<LINK>'],https://arxiv.org/abs/2005.04617,"The main service provided by the coming Quantum Internet will be creating entanglement between any two quantum nodes. We discuss and classify attacks on quantum repeaters, which will serve roles similar to those of classical Internet routers. We have modeled the components for and structure of quantum repeater network nodes. With this model, we point out attack vectors, then analyze attacks in terms of confidentiality, integrity and availability. While we are reassured about the promises of quantum networks from the confidentiality point of view, integrity and availability present new vulnerabilities not present in classical networks and require care to handle properly. We observe that the requirements on the classical computing/networking elements affect the systems' overall security risks. This component-based analysis establishes a framework for further investigation of network-wide vulnerabilities. ",Attacking the Quantum Internet
88,1260011707532849152,28994054,yongge wang,"['moved the paper on Ethereum’s CBC Casper from arxiv. Refused version contains math proofs and include a new interesting result. believe this is the first ever result of achieving deterministic BFT (without flipping coins) in complete asynchronous networks. <LINK> <LINK>', 'Typos:-)\nShould be: moved the paper regarding Ethereum’s CBC Casper from ePrint to arxiv. The revised paper contains......']",https://arxiv.org/abs/2005.04309,"Ethereum Research team has proposed a family of Casper blockchain consensus protocols for Ethereum 2.0. It has been shown in the literature that Casper Friendly Finality Gadget (Casper FFG) for Ethereum 2.0's beacon network cannot achieve liveness property in partially synchronous networks such as the Internet environment. The ""Correct-by-Construction"" family of Casper blockchain consensus protocols (CBC Casper) has been proposed as a finality gadget for the future release of Ethereum 2.0 blockchain. Unfortunately, neither constructive finality rule nor satisfactory liveness property has been obtained for CBC Casper, and it is commonly believed that CBC Casper could not achieve liveness property in asynchronous networks. This paper provides the first probabilistic CBC Casper protocol that achieves liveness property against (n-1)/3 Byzantine participants in complete asynchronous networks. ",Blockchain BFT Protocol for Complete Asynchronous Networks
89,1260010381386199040,2337598033,Geraint F. Lewis,"['Great new paper from PhD student, Mathew Varidel, on the arxiv today! <LINK> <LINK>']",https://arxiv.org/abs/2005.04874,"We infer the intrinsic ionised gas kinematics for 383 star-forming galaxies across a range of integrated star-formation rates (SFR $\in [10^{-3}, 10^2]$ M$_\odot$ yr$^{-1}$) at $z \lesssim 0.1$ using a consistent 3D forward-modelling technique. The total sample is a combination of galaxies from the SAMI Galaxy Survey and DYNAMO survey. For typical low-$z$ galaxies taken from the SAMI Galaxy Survey, we find the vertical velocity dispersion ($\sigma_{v, z}$) to be positively correlated with measures of star-formation rate, stellar mass, HI gas mass, and rotational velocity. The greatest correlation is with star-formation rate surface density ($\Sigma_\text{SFR}$). Using the total sample, we find $\sigma_{v, z}$ increases slowly as a function of integrated star-formation rate in the range SFR $\in$ [$10^{-3}$, 1] M$_\odot$ yr$^{-1}$ from $17\pm3$ km s$^{-1}$ to $24\pm5$ km s$^{-1}$ followed by a steeper increase up to $\sigma_{v, z}$ $\sim 80$ km s$^{-1}$ for SFR $\gtrsim 1$ M$_\odot$ yr$^{-1}$. This is consistent with recent theoretical models that suggest a $\sigma_{v, z}$ floor driven by star-formation feedback processes with an upturn in $\sigma_{v, z}$ at higher SFR driven by gravitational transport of gas through the disc. ","The SAMI Galaxy Survey: Gas velocity dispersions in low-$z$ star-forming
  galaxies and the drivers of turbulence"
90,1260010161063616514,251682258,Dr. Knicole Colón,"['I am so excited to share our new paper on the inflated sub-Saturn KELT-11b! We used Hubble, Spitzer, and TESS data and find that KELT-11b has a low-amplitude water feature with an unusual shape, suggestive of a sub-solar atmospheric water abundance. <LINK> (1/2) <LINK>', 'I am so grateful for all the work our team put into this project, a team which included @lkreidberg, Mike Line, @luis_wel, @exomadhu, @tgbeatty, @PatrickTamburo, @kevinbstevenson, Avi Mandell, @Astro_JRod, @mrtommyb, and many more! (2/2) https://t.co/B7PmzVUSpX']",https://arxiv.org/abs/2005.05153,"We present an optical-to-infrared transmission spectrum of the inflated sub-Saturn KELT-11b measured with the Transiting Exoplanet Survey Satellite (TESS), the Hubble Space Telescope (HST) Wide Field Camera 3 G141 spectroscopic grism, and the Spitzer Space Telescope (Spitzer) at 3.6 $\mu$m, in addition to a Spitzer 4.5 $\mu$m secondary eclipse. The precise HST transmission spectrum notably reveals a low-amplitude water feature with an unusual shape. Based on free retrieval analyses with varying molecular abundances, we find strong evidence for water absorption. Depending on model assumptions, we also find tentative evidence for other absorbers (HCN, TiO, and AlO). The retrieved water abundance is generally $\lesssim 0.1\times$ solar (0.001--0.7$\times$ solar over a range of model assumptions), several orders of magnitude lower than expected from planet formation models based on the solar system metallicity trend. We also consider chemical equilibrium and self-consistent 1D radiative-convective equilibrium model fits and find they too prefer low metallicities ($[M/H] \lesssim -2$, consistent with the free retrieval results). However, all the retrievals should be interpreted with some caution since they either require additional absorbers that are far out of chemical equilibrium to explain the shape of the spectrum or are simply poor fits to the data. Finally, we find the Spitzer secondary eclipse is indicative of full heat redistribution from KELT-11b's dayside to nightside, assuming a clear dayside. These potentially unusual results for KELT-11b's composition are suggestive of new challenges on the horizon for atmosphere and formation models in the face of increasingly precise measurements of exoplanet spectra. ","An Unusual Transmission Spectrum for the Sub-Saturn KELT-11b Suggestive
  of a Sub-Solar Water Abundance"
91,1260007195166433282,36396172,Diego R. Amancio,"['Our new preprint is out: ""How are scientific works viewed?""\nWe proposed a model describing the evolution of paper views along time. In collaboration with @acmbrito @moondark @hfarruda @LdaFCosta \n\n<LINK> <LINK>']",https://arxiv.org/abs/2005.04512,"With the expansion of electronic publishing, a new dynamics of scientific articles dissemination was initiated. Nowadays, many works are widely disseminated even before publication, in the form of preprints. Another important new element concerns the views of published articles. Thanks to the availability of respective data by some journals, such as PLoS ONE, it became possible to develop investigations on how scientific works are viewed along time, often before the first citations appear. This provides the main theme of the present work. More specifically, our research was motivated by preliminary observations that the view profiles along time tend to present a piecewise linear nature. A methodology was then delineated in order to identify the main segments in the view profiles, which allowed several related measurements to be derived. In particular, we focused on the inclination and length of each subsequent segment. Basic statistics indicated that the inclination can vary substantially along subsequent segments, while the segment lengths resulted more stable. Complementary joint statistics analysis, considering pairwise correlations, provided further information about the properties of the views. In order to better understand the view profiles, we performed respective multivariate statistical analysis, including principal component analysis and hierarchical clustering. The results suggest that a portion of the polygonal views are organized into clusters or groups. These groups were characterized in terms of prototypes indicating the relative increase or decrease along subsequent segments. Four respective distinct models were then developed for representing the observed segments. It was found that models incorporating joint dependencies between the properties of the segments provided the most accurate results among the considered alternatives. ","Classification of abrupt changes along viewing profiles of scientific
  articles"
92,1259905739742732291,1070545175942959105,Tom McCoy,"['New paper: “Representations of syntax [MASK] useful: Effects of constituency and dependency structure in recursive LSTMs” by Michael Lepori, @TalLinzen, &amp; me\n\narXiv: <LINK>\n\nWhich will win – dependency or constituency? And what goes in the [MASK]? Read on! \n\n1/9 <LINK>', 'We investigate which types of tree structure (constituency vs. dependency) help neural networks learn subject-verb agreement. To test for robust learning, we evaluate the models on sentences with multiple attractors (nouns intervening between the subject and verb). https://t.co/Y8bdnBKCEU', 'A plain BiLSTM (no tree structure) does poorly on naturally-occurring sentences with multiple attractors, but tree-RNNs with either constituency structure, or dependency structure, or both (the “head” model in the figure) are much more robust.\n\n3/9 https://t.co/OJXMPIJg2F', 'On a constructed evaluation set that controls for word-cooccurrence statistics, the dependency-based tree-RNN performs poorly, indicating that its strong performance on natural-language sentences is likely due to lexical heuristics.\n\n4/9', 'The constituency model does well on both evaluations. Adding dependency info to the constituency model does not improve its performance (the model labeled “head”). \n\n5/9', 'Overall, these results suggest that models need additional encouragement to robustly learn syntax from natural language, and that it is more important to encourage them to learn constituency structure than dependency structure.\n\n6/9', 'Clearly tree-structured models can introduce such encouragement. But these models are slow to train. For a more efficient approach, we show that data augmentation with only 500 automatically-generated sentences brings the BiLSTM close in performance to tree-based models. \n\n7/9 https://t.co/fbuZwW3KfV', 'Finally: the [MASK] in the title should be filled in with “are” (not “is”, and definitely not “aren’t”).\n\n8/9', 'The first author, Michael Lepori, is a very talented undergrad who will be applying to PhD programs in the fall! \n\n9/9', '@hbXNov @tallinzen @BhattGantavya @acl_srw Sounds very interesting - I look forward to reading it!']",https://arxiv.org/abs/2005.00019,"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking. ","Representations of Syntax [MASK] Useful: Effects of Constituency and
  Dependency Structure in Recursive LSTMs"
93,1259587618104213504,1187500436678266880,Frank Yue Wu,"['We just released our new paper: A Finite Time Analysis of Two Time-Scale Actor Critic Methods (<LINK>). We for the first time show that two time-scale actor-critic methods can find an eps-stationary point within eps^{-2.5} sample complexity.', 'Joint work with @WeitongZhang @PanXu_CS  @QuanquanGu']",https://arxiv.org/abs/2005.01350,"Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature. However, the non-asymptotic convergence and finite sample complexity of actor-critic methods are largely open. In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find a first-order stationary point (i.e., $\|\nabla J(\boldsymbol{\theta})\|_2^2 \le \epsilon$) of the non-concave performance function $J(\boldsymbol{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample complexity. To the best of our knowledge, this is the first work providing finite-time analysis and sample complexity bound for two time-scale actor-critic methods. ",A Finite Time Analysis of Two Time-Scale Actor Critic Methods
94,1259009592287277056,90131577,Noam Slonim 🟢,"['Can we automatically find an opinion article that specifically counters an article we just read? Check out a new ACL-2020 paper, coming out from our #ProjectDebater team. And we also release 3.6k (!) recorded debate speeches! Congrats to the authors!\n\n<LINK>']",https://arxiv.org/abs/2005.01157,"An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in ""echo chambers"" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns -- that of detecting articles that most effectively counter the arguments -- and not just the stance -- made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research. ",Out of the Echo Chamber: Detecting Countering Debate Speeches
95,1259006134469496832,90131577,Noam Slonim 🟢,"['Excited to share a new paper coming out from our #ProjectDebater team, to be presented at ACL-2020. It discusses *key-point analysis*, a new form of summarization with an important quantitative angle. Congrats to the authors, led by Roy Bar-Haim!\n<LINK>']",https://arxiv.org/abs/2005.01619,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed ""key points"", each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance. ",From Arguments to Key Points: Towards Automatic Argument Summarization
96,1258909197229363201,7062832,Yoshi Suhara,"['Check out our new #acl2020nlp paper ""OpinionDigest: A Simple Framework for Opinion Summarization"" with Xiaolan Wang, @stangelid, @wangchiewtan (@megagonlabs, @EdinburghNLP)!\nPaper: <LINK>\nCode: <LINK>\n#NLProc <LINK>', 'OpinionDigest is an unsupervised multi-document summarization framework that does not need any gold-standard summary. It relies on an aspect-based sentiment model that extracts opinions from input documents and uses the extracted opinions as input to the summarization model.', 'This idea enables OpinionDigest to easily handle a large number of input documents by selecting popular opinions. The user can also control the output summary by filtering input opinions based on their aspects and/or sentiment polarity. https://t.co/s7pwg2u3jv']",https://arxiv.org/abs/2005.01901,"We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary. OpinionDigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment. Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines. Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities. ",OpinionDigest: A Simple Framework for Opinion Summarization
97,1258851021238890497,967825967673638913,Richard Feder,"['Excited to announce our new paper on the arXiv, ""Nonlinear 3D Cosmic Web Simulation with Heavy-Tailed Generative Adversarial Networks"" <LINK> (thread below) <LINK>', ""(1/n) There's been a lot of recent interest in using deep generative modeling methods to enhance studies of large scale structure. We wanted to see how far generative adversarial networks (GANs) could go in producing fast realizations of the small-scale dark matter density field"", '(2/n) GANs are a great way to learn arbitrary probability distributions over data. Two neural networks compete with one another: a generator G tries producing realistic samples, and a discriminator network is trained to tell these synthetic samples apart from training data. https://t.co/7y3WpLubtf', '(3/n) When trained properly, GANs have proven exceptionally good at producing high-fidelity images, doing so in a fraction of a second once training is done. However, good performance on natural image data sets does not necessarily mean good performance on 3D N-body simulations.', '(4/n) Scaling our training data in a way that preserved information in high density features went a long way in producing accurate samples. This makes sense because the densest voxels disproportionately impact summary statistics. However, we still had trouble pushing to...', '(5/n) higher accuracy and capturing the variance of the full data set. Many changes to the network architecture, the training dynamics, etc. were attempted, but none of these provided the silver bullet we wanted..', '(6/n) To our surprise, modeling the latent space with a Student-t distribution instead of a standard Gaussian produced samples that 1) were much more accurate and 2) captured the sample variance of our training data. Just a one line modification of code in the end! https://t.co/2h8Y5y3e4N', '(7/n) Furthermore, conditional GANs did a pretty good job at interpolating matter density fields in redshift space. You can actually see how matter clusters into filaments over time! https://t.co/baBpkWukG6', '(8/n) We also wanted to explore bias reduction techniques from the GAN literature. In particular, we tried discriminator rejection sampling (DRS), which uses the discriminator output to estimate a likelihood ratio that, in principle, can correct ensemble based expectations..', '(9/n) Ironically enough, DRS *further* biased our model, preferentially accepting samples with higher power on average! We also learned through DRS that the latent vectors of accepted samples were larger in magnitude on average https://t.co/qldvaoUMDe', '(10/n) This work focused on fixed cosmology sims, but a model that can scale to larger volumes and be conditioned on physical parameters could be very useful as a cosmological emulator operating directly in the data space', ""(11/n) I also think there's a lot to be explored in how we choose GAN priors. We used a heavy-tailed distribution because it reflected important features of our data (at least in spirit) and was easy to sample, but it may be possible to optimize prior selection non-parametrically"", '(12/12) It was a great experience working on this project with @cosmophilippe and George Stein! Also many thanks to @yisongyue for offering CS 159, where a portion of this work was completed!', 'Also thanks to @msalbergo for sharing toy code in the early stages of this work and providing many good suggestions along the way!', '@lee_bcg Thanks Ben!']",https://arxiv.org/abs/2005.03050,"Fast and accurate simulations of the non-linear evolution of the cosmic density field are a major component of many cosmological analyses, but the computational time and storage required to run them can be exceedingly large. For this reason, we use generative adversarial networks (GANs) to learn a compressed representation of the 3D matter density field that is fast and easy to sample, and for the first time show that GANs are capable of generating samples at the level of accuracy of other conventional methods. Using sub-volumes from a suite of GADGET-2 N-body simulations, we demonstrate that a deep-convolutional GAN can generate samples that capture both large- and small-scale features of the matter density field, as validated through a variety of n-point statistics. The use of a data scaling that preserves high-density features and a heavy-tailed latent space prior allow us to obtain state of the art results for fast 3D cosmic web generation. In particular, the mean power spectra from generated samples agree to within 5% up to k=3 and within 10% for k<5 when compared with N-body simulations, and similar accuracy is obtained for a variety of bispectra. By modeling the latent space with a heavy-tailed prior rather than a standard Gaussian, we better capture sample variance in the high-density voxel PDF and reduce errors in power spectrum and bispectrum covariance on all scales. Furthermore, we show that a conditional GAN can smoothly interpolate between samples conditioned on redshift. Deep generative models, such as the ones described in this work, provide great promise as fast, low-memory, high-fidelity forward models of large-scale structure. ","Nonlinear 3D Cosmic Web Simulation with Heavy-Tailed Generative
  Adversarial Networks"
98,1258849382205710337,935376182010433536,Michael Kanaan,"['Such a pleasure to share this new paper sponsored by our partnership with the @usairforce &amp; MIT on ""Fast Mapping onto Census Blocks"" to help publicly enable the broader community to quickly integrate data for the #COVID19 response. #aiforgood ⬇️\n<LINK>']",https://arxiv.org/abs/2005.03156,"Pandemic measures such as social distancing and contact tracing can be enhanced by rapidly integrating dynamic location data and demographic data. Projecting billions of longitude and latitude locations onto hundreds of thousands of highly irregular demographic census block polygons is computationally challenging in both research and deployment contexts. This paper describes two approaches labeled ""simple"" and ""fast"". The simple approach can be implemented in any scripting language (Matlab/Octave, Python, Julia, R) and is easily integrated and customized to a variety of research goals. This simple approach uses a novel combination of hierarchy, sparse bounding boxes, polygon crossing-number, vectorization, and parallel processing to achieve 100,000,000+ projections per second on 100 servers. The simple approach is compact, does not increase data storage requirements, and is applicable to any country or region. The fast approach exploits the thread, vector, and memory optimizations that are possible using a low-level language (C++) and achieves similar performance on a single server. This paper details these approaches with the goal of enabling the broader community to quickly integrate location and demographic data. ",Fast Mapping onto Census Blocks
99,1258627924279939078,1138762581164855298,Christoph Ternes,"['New paper today with @ntinaValentina and André de Gouvêa, <LINK> . We set lower bounds on the neutrino wave packet width using data from RENO and Daya Bay and discuss sensitivities to either measure decoherence or improve this bound at the future JUNO experiment.']",https://arxiv.org/abs/2005.03022,"We explore how well reactor antineutrino experiments can constrain or measure the loss of quantum coherence in neutrino oscillations. We assume that decoherence effects are encoded in the size of the neutrino wave-packet, $\sigma$. We find that the current experiments Daya Bay and the Reactor Experiment for Neutrino Oscillation (RENO) already constrain $\sigma>1.0\times 10^{-4}$ nm and estimate that future data from the Jiangmen Underground Neutrino Observatory (JUNO) would be sensitive to $\sigma<2.1\times 10^{-3}$ nm. If the effects of loss of coherence are within the sensitivity of JUNO, we expect $\sigma$ to be measured with good precision. The discovery of nontrivial decoherence effects in JUNO would indicate that our understanding of the coherence of neutrino sources is, at least, incomplete. ",Probing neutrino quantum decoherence at reactor experiments
100,1258623604062695426,1006948726827507712,Seung-won Park,"['We\'re excited to present a new approach for Voice Conversion: use alignment from pre-trained TTS.\n\n""Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data""\npaper: <LINK>\naudio samples: <LINK>\n\n(1/6)', ""By teacher-forcing the mel spec. into the pre-trained multispeaker TTS model, the text-audio alignment can be obtained. Then, matmul of\n(1) text encoding and (2) text-audio alignment \ngives us the speaker-independent linguistic features. Let's call it Cotatron features.\n\n(2/6) https://t.co/caF4aJm0Cv"", 'Similar to previous methods, we train a decoder to reconstruct the mel spec. from the Cotatron features (L) and a speaker representation (y^id, *=s).\n\nIf the target speaker representation is fed (*=t), then we get the conversion result. A residual encoder is optional.\n\n(3/6) https://t.co/5jcGWnYG4u', 'Such a simple idea turns out to be very effective.\nOur MOS (naturalness) and DMOS (speaker similarity) result significantly outperform the previous method, Blow, when trained and evaluated with 108 speakers of the VCTK dataset.\n\n(4/6) https://t.co/ZR5qL7l1tP', ""Our model can also:\n- convert speech from arbitrary speakers (even Charlie Chaplin's speech!),\n- utilize ASR to automate the transcription process.\n\n(5/6) https://t.co/C6Cpgy7s8L"", ""We expect that the Cotatron features can be used for:\n- lip motion synthesis,\n- any speech task that can benefit from using the transcription.\n\nWe'll be releasing the code of Cotatron near InterSpeech 2020; please consider applying our idea to your task! Thank you.\n\n(6/6)"", 'One of our impressive conversion result here:\n(also available at our demo website)\n\nWe converted one of the famous memes (?) in South Korea into Korean female speaker\'s speech. (KSS dataset)\n\n""온갖 음해에 시달렸습니다. 여러분 이거 다 거짓말인거 아시죠?""\n\nhttps://t.co/7tV4R0PPGs']",https://arxiv.org/abs/2005.03295,"We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at this https URL, and the code with a pre-trained model will be made available soon. ","Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice
  Conversion without Parallel Data"
101,1258621460211552256,179823553,Dr. Bhubanjyoti Bhattacharya,['Our new paper is out! I hope some of the triple products we found are seen soon at Belle II #tripleproducts <LINK>'],https://arxiv.org/abs/2005.03032,"At present, the measurements of $R_{D^{(*)}}$ and $R_{J/\psi}$ hint at new physics (NP) in $b \to c \tau^- {\bar\nu}$ decays. The angular distribution of ${\bar B} \to D^* (\to D \pi) \, \tau^{-} {\bar\nu}_\tau$ would be useful for getting information about the NP, but it cannot be measured. The reason is that the three-momentum ${\vec p}_\tau$ cannot be determined precisely since the decay products of the $\tau^-$ include an undetected $\nu_\tau$. In this paper, we construct a measurable angular distribution by considering the additional decay $\tau^- \to \pi^- \nu_\tau$. The full process is ${\bar B} \to D^* (\to D \pi') \, \tau^{-} (\to \pi^- \nu_\tau) {\bar\nu}_\tau$, which includes three final-state particles whose three-momenta can be measured: $D$, $\pi'$, $\pi^-$. The magnitudes and relative phases of all the NP parameters can be extracted from a fit to this angular distribution. One can measure CP-violating angular asymmmetries. If one integrates over some of the five kinematic parameters parametrizing the angular distribution, one obtains (i) familiar observables such as the $q^2$ distribution and the $D^*$ polarization, and (ii) new observables associated with the $\pi^-$ emitted in the $\tau$ decay: the forward-backward asymmetry of the $\pi^-$ and the CP-violating triple-product asymmetry. ","A Measurable Angular Distribution for ${\bar B} \to D^{*} \tau^-
  {\bar\nu}_\tau$ Decays"
102,1258564811366920193,293552287,Hoan Tran,"['New paper out, main work by Kazuha Itabashi, co-authors with @unlimitcycle  @k09ht \nEvaluating the phase dynamics of coupled oscillators via time-variant topological features \n<LINK> <LINK>']",https://arxiv.org/abs/2005.03343,"By characterizing the phase dynamics in coupled oscillators, we gain insights into the fundamental phenomena of complex systems. The collective dynamics in oscillatory systems are often described by order parameters, which are insufficient for identifying more specific behaviors. To improve this situation, we propose a topological approach that constructs the quantitative features describing the phase evolution of oscillators. Here, the phase data are mapped into a high-dimensional space at each time, and the topological features describing the shape of the data are subsequently extracted from the mapped points. These features are extended to time-variant topological features by adding the evolution time as an extra dimension in the topological feature space. The time-variant features provide crucial insights into the evolution of phase dynamics. Combining these features with the kernel method, we characterize the multi-clustered synchronized dynamics during the early evolution stages. Finally, we demonstrate that our method can qualitatively explain chimera states. The experimental results confirmed the superiority of our method over those based on order parameters, especially when the available data are limited to the early-stage dynamics. ","Evaluating the phase dynamics of coupled oscillators via time-variant
  topological features"
103,1258426250445668356,1158385581476515840,Allyson Ettinger,"['New ACL paper with @josefklafka! To clarify how much contextual embeddings actually know about their surrounding words, we use fine-grained probing to track encoding of context word features across embeddings of each token in the sentence. <LINK>  1/2', 'Upshot: most contextual embeddings have information about most other words in the sentence. But different encoders vary in their distribution of information across tokens, with certain word features deprioritized, and with syntactic properties impacting distribution.  2/2']",https://arxiv.org/abs/2005.01810,"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability-but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models breakdown and prioritize word-level context information when constructing token embeddings. ","Spying on your neighbors: Fine-grained probing of contextual embeddings
  for information about surrounding words"
104,1258404137659744258,70874545,Josh Lothringer,"['New paper on the arXiv today in which we explore the short-wavelength transit spectra of hot and ultra-hot Jupiters! <LINK> (just submitted) <LINK>', 'We show that the high transit depths found in some ultra-hot Jupiters can be interpreted as absorption by a plethora of species guaranteed to be present if near chemical equilibrium. Like WASP-12b for example: https://t.co/AmT0B6MI0Y', 'We also explore how the UV-optical transit depths should vary between Teq = ~800-4000 K, with and without the rainout of condensates: https://t.co/xiYCnwNUkx', ""We're just beginning to see some of these species in both low and high-resolution observations so that's very exciting. This paper was inspired by @Guangwei_Fu's great work on WASP-76b, finding H2O, TiO, and probably a bunch of the species mentioned above: https://t.co/3hl2GDQuQw""]",https://arxiv.org/abs/2005.02528,"The low-resolution transmission spectra of ultra-hot Jupiters observed shortward of 0.5 microns indicate strong absorption at short-wavelengths. Previous explanations have included scattering, photochemistry, escaping metals, and disequilibrium chemistry. In this Letter, we show that slopes and features shortward of 0.5 microns can be caused by opacity not commonly considered in atmosphere models of exoplanets but guaranteed to be present if conditions are near chemical equilibrium including Fe I, Fe II, Ti I, Ni I, Ca I, Ca II, and SiO. Even relatively trace species (e.g., Cr I and Mn I) can contribute through strong lines in the UV and blue-optical. Using the PHOENIX atmosphere model, we describe how the short-wavelength transit spectrum varies with equilibrium temperature between 1000 K and 4000 K, as well as the effect that the rainout of condensates has at these wavelengths. We define two spectral indices to quantify the strength of the NUV and blue absorption compared to that in the red-optical, finding that the NUV transit depth will significantly exceed the transit depth from Rayleigh scattering alone for all hot Jupiters down to around 1000 K. In the blue-optical, hot Jupiters warmer than 2000 K will have transit depths larger than that from Rayleigh scattering, but below 2000 K, Rayleigh scattering can dominate, if present. We further show that these spectral indices may be used to trace the effects of rainout. We then compare our simulated transit spectra to existing observations of WASP-12b, WASP-33b, WASP-76b, and WASP-121b. ","UV Exoplanet Transmission Spectral Features as Probes of Metals and
  Rainout"
105,1258304242709823488,590290232,Dominik Batorski,"['New paper and dataset \n<LINK>: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery <LINK> #ComputerVision #MachineLearning #aerialphotography']",https://arxiv.org/abs/2005.02264,"Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads. Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56% of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at this https URL ","LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands,
  Water and Roads from Aerial Imagery"
106,1258299387366817792,2425754287,Roman Orus,"['New paper out, with Philipp Schmoll: SU(2) symmetry in 2d Tensor Network algorithms  <LINK>']",https://arxiv.org/abs/2005.02748,"We implement and benchmark tensor network algorithms with $SU(2)$ symmetry for systems in two spatial dimensions and in the thermodynamic limit. Specifically, we implement $SU(2)$-invariant versions of the infinite Projected Entangled Pair States (iPEPS) and infinite Projected Entangled Simplex States (iPESS) methods. Our implementation of $SU(2)$ symmetry follows the formalism based on fusion trees from [P. Schmoll, S. Singh, M. Rizzi, R. Or\'us, arXiv:1809.08180]. In order to assess the utility of implementing $SU(2)$ symmetry the algorithms are benchmarked for three models with different local spin: the spin-1 bilinear-biquadratic model on the square lattice, and the Kagome Heisenberg antiferromagnets (KHAF) for spin-1/2 and spin-2. We observe that the implementation of $SU(2)$ symmetry provides better energies in general than non-symmetric simulations, with smooth scalings with respect to the number of parameters in the ansatz, and with the actual improvement depending on the specifics of the model. In particular, for the spin-2 KHAF model, our $SU(2)$ simulations are compatible with a quantum spin liquid ground state. ",Benchmarking global $SU(2)$ symmetry in 2d tensor network algorithms
107,1258268537916780546,505666231,Jieyu Zhao,"['1/3 Happy to share our new paper ""Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer"" with @subho_mpi,@sagharh, @kaiwei_chang and Ahmed Hassan Awadallah at #acl2020nlp <LINK>', '2/3 We examine gender bias in multilingual word embeddings (fastText, focused on EN, ES, DE, FR) from both intrinsic and extrinsic perspectives. We collect occupation lists and bios dataset for each language for intrinsic and extrinsic analysis respectively.', '3/3 We find that 1) bias in the multilingual word embeddings changes differently when we align embeddings to different target spaces; 2) the alignment direction can also have an influence on the bias in cross-lingual transfer learning.']",https://arxiv.org/abs/2005.00699,"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks. ",Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer
108,1258205994292674560,222777271,Tan Van Vu,['New paper out (by @tanvvu and @unlimitcycle). We derive geometrical bounds on the irreversibility for both classical and open quantum systems. The bounds can also be interpreted as classical and quantum speed limits. <LINK>'],https://arxiv.org/abs/2005.02871,"We derive geometrical bounds on the irreversibility in both quantum and classical Markovian open systems that satisfy the detailed balance condition. Using information geometry, we prove that irreversible entropy production is bounded from below by a modified Wasserstein distance between the initial and final states, thus strengthening the Clausius inequality in the reversible-Markov case. The modified metric can be regarded as a discrete-state generalization of the Wasserstein metric, which has been used to bound dissipation in continuous-state Langevin systems. Notably, the derived bounds can be interpreted as the quantum and classical speed limits, implying that the associated entropy production constrains the minimum time of transforming a system state. We illustrate the results on several systems and show that a tighter bound than the Carnot bound for the efficiency of quantum heat engines can be obtained. ",Geometrical Bounds of the Irreversibility in Markovian Systems
109,1258193346620776448,1191056593476915200,Ray Bai,"['Excited to share my new paper, ""A Robust Bayesian Copas Selection Model for Quantifying and Correcting Publication Bias."" We developed an approach for correcting/quantifying publication bias in meta-analysis.  Read it on arXiv: <LINK> @chenyong1203 @MaryRBoland <LINK>', 'R package for this should be on CRAN by next week hopefully. Stay tuned!']",https://arxiv.org/abs/2005.02930,"The validity of conclusions from meta-analysis is potentially threatened by publication bias. Most existing procedures for correcting publication bias assume normality of the study-specific effects that account for between-study heterogeneity. However, this assumption may not be valid, and the performance of these bias correction procedures can be highly sensitive to departures from normality. Further, there exist few measures to quantify the magnitude of publication bias based on selection models. In this paper, we address both of these issues. First, we explore the use of heavy-tailed distributions for the study-specific effects within a Bayesian hierarchical framework. The deviance information criterion (DIC) is used to determine the appropriate distribution to use for conducting the final analysis. Second, we develop a new measure to quantify the magnitude of publication bias based on Hellinger distance. Our measure is easy to interpret and takes advantage of the estimation uncertainty afforded naturally by the posterior distribution. We illustrate our proposed approach through simulation studies and meta-analyses on lung cancer and antidepressants. To assess the prevalence of publication bias, we apply our method to 1500 meta-analyses of dichotomous outcomes in the Cochrane Database of Systematic Reviews. Our methods are implemented in the publicly available R package RobustBayesianCopas. ","A Robust Bayesian Copas Selection Model for Quantifying and Correcting
  Publication Bias"
110,1258100004310450176,1015712980275748864,Konstantin Antipin,"['My new paper on entanglement measures : ""Lower bounds on concurrence and negativity from a trace inequality"", <LINK>']",http://arxiv.org/abs/2005.01881,"For bipartite quantum states we obtain lower bounds on two important entanglement measures, concurrence and negativity, studying the inequalities for the expectation value of a projector on some subspace of the Hilbert space. Several applications, including analysis of stability of entanglement under various perturbations of a state, are discussed. ",Lower bounds on concurrence and negativity from a trace inequality
111,1258079676301443072,428833192,Dr. Bin Chen,"[""In a new paper accepted by ApJL, we used #EOVSA to image microwave emission by nonthermal electrons that follow the erupting magnetic flux rope from the central cavity to its conjugate footprints. Cartoon adapted from @MihoJnvr's 3D standard flare model.  <LINK> <LINK>"", '@MihoJnvr EOVSA has a cadence of 1 s. Insufficient to trace the propagation of the individual nonthermal electrons, but good for study their collective evolution. We have spatially resolved light curves for the central and footpoint sources, suggesting they are physically connected.']",https://arxiv.org/abs/2005.01900,"We report microwave spectral imaging observations of an erupting magnetic flux rope during the early impulsive phase of the X8.2-class limb flare on 2017 September 10, obtained by the Expanded Owens Valley Solar Array. A few days prior to the eruption, when viewed against the disk, the flux rope appeared as a reverse S-shaped dark filament along the magnetic polarity inversion line. During the eruption, the rope exhibited a ""hot channel"" structure in extreme ultraviolet and soft X-ray passbands sensitive to ~10 MK plasma. The central portion of the flux rope was nearly aligned with the line of sight, which quickly developed into a teardrop-shaped dark cavity during the early phase of the eruption. A long and thin plasma sheet formed below the cavity, interpreted as the reconnection current sheet viewed edge-on. A nonthermal microwave source was present at the location of the central current sheet, which extended upward encompassing the dark cavity. A pair of nonthermal microwave sources were observed for several minutes on both sides of the main flaring region. They shared a similar temporal behavior and spectral property to the central microwave source below the cavity, interpreted as the conjugate footpoints of the erupting flux rope. These observations are broadly consistent with the magnetic topology and the associated energy release scenario suggested in the three-dimensional standard model for eruptive solar flares. In particular, our detection of nonthermal emission at conjugate flux rope footpoints provides solid evidence of particle transport along an erupting magnetic flux rope. ","Microwave Spectral Imaging of an Erupting Magnetic Flux Rope:
  Implications for the Standard Solar Flare Model in Three Dimensions"
112,1258075541607010305,81882597,Marcos Guimaraes,"['Our new paper on spin-orbit torques using insulating antiferromagnetic #2dmaterials is now on @arxiv! We show that these materials are very promising for the manipulation of magnetic bits for future non-volatile memory devices, like M-RAM. <LINK>']",https://arxiv.org/abs/2005.01368,"Finding efficient ways of manipulating magnetic bits is one of the core goals in spintronic research. Electrically-generated spin-orbit torques (SOTs) are good candidates for this and the search for materials capable of generating highly-efficient SOTs has gained a lot of traction in the recent years. While antiferromagnet/ferromagnet bilayer structures have been employed extensively for passive applications, e.g. by using exchange bias fields, their active properties are not yet widely employed. Here we show the presence of large interfacial SOTs in bilayer of a ferromagnet and the two-dimensional layered antiferromagnetic insulator NiPS$_3$. We observe a large in-plane damping-like interfacial torque, showing a torque conductivity of $\sigma_\mathrm{DL} \approx 1 \times 10^{5} \mathrm{(\frac{\hbar}{2e}) /(\Omega m)}$ even at room temperature, comparable to the best devices reported in the literature for standard heavy-metal-based and topological insulators-based devices. Additionally, our devices also show an out-of-plane field-like torque arising from the NiPS$_3$/ferromagnet interface, further indicating the presence of an interfacial spin-orbit coupling in our structures. Temperature-dependent measurements reveal an increase of the SOTs with a decreasing temperature below the N\'eel temperature of NiPS$_3$ ($T_N \approx 170 \mathrm{K}$), pointing to a possible effect of the magnetic ordering on our measured SOTs. Our findings show the potential of antiferromagnetic insulators and two-dimensional materials for future spintronic applications. ","Large interfacial spin-orbit torques in layered antiferromagnetic
  insulator NiPS$_3$/ferromagnet bilayers"
113,1258034187107434496,48329145,Thomas Kober,"['New paper announcement: ""Data Augmentation for Hypernymy Detection"" with brilliant collaborators Julie Weeds, Lorenzo Bertolini and David Weir @SussexUni\n\narXiv: <LINK>\npdf: <LINK>\n\n1/10', 'We do data augmentation based on distributional composition and GANs and see some solid improvements for basic LR and FF models, with two different distributional vector space models, on supervised hypernymy detection. \n\n2/10', 'We compare augmentation by composition and GANs to two ways of _extending_ a given training set with either hyponym-hypernym pairs from WordNet or extracted from a large corpus with Hearst Patterns.\n\n3/10', 'We expected that extending a dataset with pairs from WordNet will likely be the upper limit of what we can expect from our data augmentation techniques, however we find that augmentation by composition and GANs frequently performs better than WordNet 😱😱😱\n\n4/10', 'Distributional composition: Given t hyponym-hypernym pair dog-animal in the training data, we collect modifiers for animal and dog (e.g. small, hungry, etc) and add small dog-dog, hungry dog-dog, small dog-animal, and hungry dog-animal as additional pairs to the training set.5/10', 'We average the vector representations for ""hungry"" and ""dog"" to get a composed vector. Negative examples are created by pairing ""small dog"" with a neighbour of ""animal"", say ""vehicle"", such that the negative pair ""small dog-vehicle"" is added to the training data.\n\n6/10', 'For the GAN based approach - GANDALF (GAN-based Data Augmentation for Lexical inFerence) - we simply aim to generate vectors that ""look like"" real nouns (i.e. that are close in terms of cosine similarity to actual word vectors).\n\n7/10', 'Given the pair dog-animal, we simply pick the top n most similar GANDALF-ed vectors to dog and animal and add those as additional positive examples to the training data. Negative examples are created by mimicking observed negative examples, e.g. dog-cat.\n\n8/10', 'We also introduce a new hand annotated dataset - HP4K - that does not rely on WordNet or other hand curated lexicons. Given that some datasets as well as models make use of WordNet, we think that this dataset will be a neat addition to existing test suites.\n\n9/10', 'Relevant links:\narXiv: https://t.co/15L2JgZSfA\npdf: https://t.co/ZXimmHwtrb\ngithub: https://t.co/anC6vR9mlr\n\nThe dataset is already on github, the code will follow soon.\n\n10/10']",https://arxiv.org/abs/2005.01854,"The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of state-of-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as ""small dog - dog"" or ""small dog - animal"", for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as WordNet. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance. ",Data Augmentation for Hypernymy Detection
114,1257994250572177408,716958933039054848,Senellart's QD Group,"['🎉 New paper on the arxiv! 🎉\n\nWe investigated how Hong-Ou-Mandel interference changes for imperfect single photon sources, and found it is critical to know where your noise comes from. \n\n<LINK> \n\n#photonics #quantum #QuantumComputing #SinglePhotons <LINK>']",https://arxiv.org/abs/2005.01743,"Hong-Ou-Mandel interference is a cornerstone of optical quantum technologies. We explore both theoretically and experimentally how the nature of unwanted multi-photon components of single photon sources affect the interference visibility. We apply our approach to quantum dot single photon sources in order to access the mean wavepacket overlap of the single-photon component - an important metric to understand the limitations of current sources. We find that the impact of multi-photon events has thus far been underestimated, and that the effect of pure dephasing is even milder than previously expected. ",Hong-Ou-Mandel Interference with Imperfect Single Photon Sources
115,1257982670484766726,312803916,Aqeel Ahmed,['New paper on “Gravitational production of vector dark matter” with an early non-standard cosmological era of reheating.(arXiv:2005.01766) \n<LINK> #darkmatter <LINK>'],http://arxiv.org/abs/2005.01766,"A model of vector dark matter that communicates with the Standard Model only through gravitational interactions has been investigated. It has been shown in detail how does the canonical quantization of the vector field in varying FLRW geometry implies a tachyonic enhancement of some of its momentum modes. Approximate solutions of the mode equation have been found and verified against exact numerical ones. De Sitter geometry has been assumed during inflation while after inflation a non-standard cosmological era of reheating with a generic equation of state has been adopted which is followed by the radiation-dominated universe. It has been shown that the spectrum of dark vectors produced gravitationally is centered around a characteristic comoving momentum $k_\star$ that is determined in terms of the mass of the vector $m_X$, the Hubble parameter during inflation $H_{\rm I}$, the equation of state parameter $w$ and the efficiency of reheating $\gamma$. Regions in the parameter space consistent with the observed dark matter relic abundance have been determined, justifying the gravitational production as a viable mechanism for vector dark matter. The results obtained in this paper are applicable within various possible models of inflation/reheating with non-standard cosmology parametrized effectively by the corresponding equation of state and efficiency of reheating. ",Gravitational production of vector dark matter
116,1257978385340354561,983186922565595137,Mujeen Sung,"['Introducing BioSyn, our new #acl2020nlp paper on learning #Biomedical entity representations with @leejnhk !\nPaper: <LINK>\nCode: <LINK>\n\n#machinelearning #NLProc <LINK>']",https://arxiv.org/abs/2005.00239,"Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset. ",Biomedical Entity Representations with Synonym Marginalization
117,1257960937547616257,717709692517163008,Stefanie Barz,['Check out our new paper on the arXiv today: what do role #quantum correlations play in computation?  <LINK> @Uni_Stuttgart @IQSTpress <LINK>'],https://arxiv.org/abs/2005.01780,"Quantum correlations are central to the foundations of quantum physics and form the basis of quantum technologies. Here, our goal is to connect quantum correlations and computation: using quantum correlations as a resource for computation - and vice versa, using computation to test quantum correlations. We derive Bell-type inequalities that test the capacity of quantum states for computing Boolean functions and experimentally investigate them using 4-photon Greenberger-Horne-Zeilinger (GHZ) states. Further, we show how the generated states can be used to specifically compute Boolean functions - which can be used to test and verify the non-classicality of the underlying quantum states. The connection between quantum correlation and computability shown here has applications in quantum technologies, and is important for networked computing being performed by measurements on distributed multipartite quantum states. ",Correlations for computation and computation for correlations
118,1257923409847226368,2844538806,Marie Van de Sande,"['New paper out! A follow-up on our work on dust-gas chemistry in AGB outflows, where we take a look at the effect of the specific grain size distribution. Maybe molecular lines could help determine this distribution! <LINK> @cwalshastrochem @stellalchemist']",https://arxiv.org/abs/2005.01553,"AGB stars are, together with supernovae, the main contributors of stellar dust to the interstellar medium (ISM). Dust grains formed by AGB stars are thought to be large. However, as dust nucleation and growth within their outflows are still not understood, the dust-grain size distribution (GSD) is unknown. This is an important uncertainty regarding our knowledge of the chemical and physical history of interstellar dust, as AGB dust forms $\sim$ 70% of the starting point of its evolution. We expand on our chemical kinetics model, which uniquely includes a comprehensive dust-gas chemistry. The GSD is now allowed to deviate from the commonly assumed canonical Mathis et al. (1977) distribution. We find that the specific GSD can significantly influence the dust-gas chemistry within the outflow. Our results show that the level of depletion of gas-phase species depends on the average grain surface area of the GSD. Gas-phase abundance profiles and their possible depletions can be retrieved from observations of molecular emission lines when using a range of transitions. Due to degeneracies within the prescription of GSD, specific parameters cannot be retrieved, only (a lower limit to) the average grain surface area. Nonetheless, this can discriminate between dust composed of predominantly large or small grains. We show that when combined with other observables such as the spectral energy distribution and polarised light, depletion levels from molecular gas-phase abundance profiles can constrain the elusive GSD of the dust delivered to the ISM by AGB outflows. ","Chemical modelling of dust-gas chemistry within AGB outflows -- II.
  Effect of the dust-grain size distribution"
119,1257843116691861506,908158289162194945,Aaron Tohuvavohu,"[""Now that I think of it, maybe it *wasn't* such a good time to publish a new paper named after bat poop?\n<LINK> <LINK> <LINK>"", 'Despite being told it was impossible for yrs, we have now demonstrated (&gt;700x) fully auto low latency commanding of a space telescope, and used the data to do cool things! more science results from the data  will be coming soon, in joint subthreshold GW/GRB searches with @LIGO', 'but (perhaps) more importantly than any data GUANO itself will ever get, this crucially demonstrates that this type of commanding can be done safely for a NASA mission, and so can be included as a capability in next generation space telescopes!', '@TheGeoffRyan This is just the intro paper, explaining the ops of how its done and a few demos of what can be done with the data. science focused analysis papers in prep!', 'There are strong science cases that require no-human-in-the-loop ultra rapid commanding of space teles.  We achieve that here, with a minimum achieved latency of 2.5 min and an avg of 15 min. Some science cases (response to early warning GW) requires O(s). Working on that now....']",https://arxiv.org/abs/2005.01751,"We introduce a new capability of the Neil Gehrels Swift Observatory, to provide event data from the Burst Alert Telescope (BAT) on demand in response to transients detected by other instruments. These event data are not continuously available due to the large telemetry load, but are critical to recovering weak or sub-threshold GRBs that are not triggered onboard, such as the likely counterparts to GW-detected off-axis binary neutron star mergers. We show that the availability of event data can effectively increase the rate of detections, and arcminute localizations, of GRB 170817-like bursts by >400%. We describe a spacecraft commanding pipeline purpose built to enable this science; to our knowledge the first fully autonomous extremely-low-latency commanding of a space telescope for scientific purposes. This pipeline has been successfully run in its complete form since early 2020, and has resulted in the recovery of BAT event data for >700 externally triggered events to date (GWs, neutrinos, GRBs triggered by other facilities, FRBs), now running with a success rate of ~90%. We exemplify the utility of this new capability by using the resultant data to (1) set the most sensitive (8 sigma) upper limits of $8.1\times10^{-8}$ erg cm$^{-2}$ s$^{-1}$ (14-195 keV) on prompt 1s duration short GRB-like emission within $\pm$ 15s of the unmodelled GW burst candidate S200114f, and (2) provide arcminute localizations for short GRB 200325A and other bursts. We show that using data from GUANO to localize GRBs discovered elsewhere, we can increase the net rate of arcminute localized GRBs by 10-20% per year. Along with the scientific yield of more sensitive searches for sub-threshold GRBs, the new capabilities designed for this project will enable further rapid response Target of Opportunity capabilities for Swift, and have implications for the design of future rapid-response space telescopes. ","Gamma-ray Urgent Archiver for Novel Opportunities (GUANO): Swift/BAT
  event data dumps on demand to enable sensitive sub-threshold GRB searches"
120,1257833500343967744,1116002690604130305,Juliette Becker,"[""Just posted to arXiv - <LINK> - my new paper 'A Coupled Analysis of Atmospheric Mass Loss and Tidal Evolution in XUV Irradiated Exoplanets: the TRAPPIST-1 Case Study', in which we use SWIFT UV+Xray data to study the present-day XUV luminosity of TRAPPIST-1!"", 'The figures and code are also available at GitHub: https://t.co/j0mi5UiMkr. In this paper, we use @VPLanetCode to study the possible water mass loss on each TRAPPIST-1 planet due to our measured luminosities.', 'We also attempt to constrain the planetary interior structures (the Q/k2 ratio) from the current day orbital elements combined with the expected past tidal evolution. We are able to find lower limits on Q/k2 for the inner planets: https://t.co/BZIREZCZ8y', '@di_goldene_pave You are correct! Sometimes I forget that not ALL astro missions/tools are acronyms and just hit that caps-lock :)']",https://arxiv.org/abs/2005.01740,"Exoplanets residing close to their stars can experience evolution of both their physical structures and their orbits due to the influence of their host stars. In this work, we present a coupled analysis of dynamical tidal dissipation and atmospheric mass loss for exoplanets in XUV irradiated environments. As our primary application, we use this model to study the TRAPPIST-1 system, and place constraints on the interior structure and orbital evolution of the planets. We start by reporting on a UV continuum flux measurement (centered around $\sim1900$ Angstroms) for the star TRAPPIST-1, based on 300 ks of Neil Gehrels Swift Observatory data, and which enables an estimate of the XUV-driven thermal escape arising from XUV photo-dissociation for each planet. We find that the X-ray flaring luminosity, measured from our X-ray detections, of TRAPPIST-1 is 5.6 $\times$10$^{-4} L_{*}$, while the full flux including non-flaring periods is 6.1 $\times$10$^{-5} L_{*}$, when $L_{*}$ is TRAPPIST-1's bolometric luminosity. We then construct a model that includes both atmospheric mass-loss and tidal evolution, and requires the planets to attain their present-day orbital elements during this coupled evolution. We use this model to constrain the ratio $Q'=3Q/2k_{2}$ for each planet. Finally, we use additional numerical models implemented with the Virtual Planet Simulator \texttt{VPLanet} to study ocean retention for these planets using our derived system parameters. ","A Coupled Analysis of Atmospheric Mass Loss and Tidal Evolution in XUV
  Irradiated Exoplanets: the TRAPPIST-1 Case Study"
121,1257816180779188224,14544467,Daniel Apai,['Excited to share our new paper – led by @azstewobs student Alex Bixel - where we propose an age-oxygen correlation in exoplanets with life &amp; a blog post on why this is cool: <LINK> @EOSNExSS @nexssinfo  <LINK>'],https://arxiv.org/abs/2005.01587,"Life has had a dramatic impact on the composition of Earth's atmosphere over time, which suggests that statistical studies of other inhabited planets' atmospheres could reveal how they co-evolve with life. While many evolutionary pathways are possible for inhabited worlds, a possible starting hypothesis is that most of them evolve similarly to Earth, which we propose could lead to a positive ""age-oxygen correlation"" between the ages of inhabited planets and the fraction which have oxygen-rich atmospheres. We demonstrate that next-generation space observatories currently under consideration could test this hypothesis, but only if the stellar age distribution of the target sample is carefully considered. We explore three possible parameterizations of the age-oxygen correlation, finding that they yield similar results. Finally, we examine how abiotic oxygen sources could affect the results, and discuss how measuring the age-dependence of oxygen could shed light on whether it is a reliable biosignature. Future efforts can expand upon this groundwork by incorporating detailed models of the redox balance of terrestrial planets and its dependence on stellar and planetary properties. ","Testing Earth-like atmospheric evolution on exo-Earths through oxygen
  absorption: required sample sizes and the advantage of age-based target
  selection"
122,1257795319997202432,865627769631264768,Jamie Tayar,"['New paper out on the arxiv today (<LINK>) led by graduate student Don Dixon (<LINK>) <LINK>', ""This paper started with a question from @chargedcurrent about whether the UV excess he found in his black hole- red giant binary was a sign of mass accretion onto the black hole, or whether it could come from the rapidly rotating red giant.  I didn't know, so Don started digging."", 'The first question he had to figure out was what is the normal amount of UV emission from a giant star. Since the UV is notoriously hard to model, he used an empirical locus to relate the J-K color to the NUV-J color, and defined NUV excess as vertical height above that line. https://t.co/PCVfqFZfVa', 'He then used a sample of TGAS stars from the @APOGEEsurvey where we could measure some amount of rotational broadening to look for a relationship between rotation rate and NUV excess. Excitingly, they seemed very correlated! https://t.co/ZNgLi1DNPH', 'Now rotation-activity people usually think about this in the context of Rossby number, and that looks good too! We have a rising activity regime for slow rotators, a saturated regime for fast rotators, and maybe even a few super saturated, less active points on the far left. https://t.co/C1F2cviHOJ', ""Our giant relationship (black) doesn't exactly match what is seen in the UV for M dwarfs (yellow and points), but it does seem to have the right shape, with saturation happening around the same place. https://t.co/0t1zSOMFFk"", ""This suggests the rotation/convection basis of the activity relationship doesn't go away for giants (😅phew all my models of stellar spin down implicitly assume this), and that there's some deep physical similarities between the chromospheres in dwarfs and giants"", 'Finally, Don was able to answer @chargedcurrent \'s original question- ""is the UV excess in his system black hole accretion?"" with a definitive no! This system is exactly as NUV active as we would expect given its rotation rate! https://t.co/f2kwmtGY0h', 'Don did a really great job leading this project, and I also want to give a shout out to Keivan Stassun, my co mentor on this project, and to the referee, who was really helpful in refining what we found.']",https://arxiv.org/abs/2005.00577,"Main sequence stars exhibit a clear rotation-activity relationship, in which rapidly rotating stars drive strong chromospheric/coronal ultraviolet and X-ray emission. While the vast majority of red giant stars are inactive, a few percent exhibit strong ultraviolet emission. Here we use a sample of 133 red giant stars observed by SDSS APOGEE and GALEX to demonstrate an empirical relationship between NUV excess and rotational velocity (vsini). Beyond this simple relationship, we find that NUV excess also correlates with rotation period and with Rossby number in a manner that shares broadly similar trends to those found in M dwarfs, including activity saturation among rapid rotators. Our data also suggest that the most extremely rapidly rotating giants may exhibit so-called ""super-saturation"", which could be caused by centrifugal stripping of these stars rotating at a high fraction of breakup speed. As an example application of our empirical rotation-activity relation, we demonstrate that the NUV emission observed from a recently reported system comprising a red giant with a black hole companion is fully consistent with arising from the rapidly rotating red giant in that system. Most fundamentally, our findings suggest a common origin of chromospheric activity in rotation and convection for cool stars from main sequence to red giant stages of evolution. ",Rotationally Driven Ultraviolet Emission of Red Giant Stars
123,1257774143413649408,60995841,Federica Tarsitano,['Check out my new paper! <LINK>. PyCosmo is user-friendly and publicly available on the PyCosmoHub: <LINK>. No need to install any software: computing #Cosmology is easy and one click away! #Python @ETH_physics @ETH_en #science #coding #physics <LINK>'],https://arxiv.org/abs/2005.00543,"Current and upcoming cosmological experiments open a new era of precision cosmology, thus demanding accurate theoretical predictions for cosmological observables. Because of the complexity of the codes delivering such predictions, reaching a high level of numerical accuracy is challenging. Among the codes already fulfilling this task, $\textsf{PyCosmo}$ is a Python based framework providing solutions to the Einstein-Boltzmann equations and accurate predictions for cosmological observables. In this work, we first describe how the observables are implemented. Then, we check the accuracy of the theoretical predictions for background quantities, power spectra and Limber and beyond-Limber angular power spectra by comparison with other codes: the Core Cosmology Library ($\texttt{CCL}$), $\texttt{CLASS}$, $\texttt{HMCode}$ and $\texttt{iCosmo}$. In our analysis we quantify the agreement of $\textsf{PyCosmo}$ with the other codes, for a range of cosmological models, monitored through a series of $\textit{unit tests}$. $\textsf{PyCosmo}$, conceived as a multi purpose cosmology calculation tool in $\texttt{Python}$, is designed to be interactive and user friendly. A current version of the code (without the Boltzmann Solver) is publicly available and can be used interactively on the platform $\textsf{PyCosmo Hub}$, all accessible from this link: this https URL . On the hub the users can perform their own computations using $\texttt{Jupyter Notebooks}$ without the need of installing any software, access to the results presented in this work and benefit from tutorial notebooks illustrating the usage of the code. The link above also redirects to the code release and documentation. ",Predicting Cosmological Observables with PyCosmo
124,1257741148086341632,84737819,Joel Fuentes,"['[New Paper] In 2016 I started working on GeantV, a high-scale particle transport simulator used in the Large Hadron Collider (@LHCbExperiment) at @CERN. The main challenge of HEP simulators is to process huge amounts of data efficiently.\n<LINK>', 'This paper presents how GeantV has been optimized for fine-grained parallelism and data locality on CPUs and accelerators. Loved to get to work with scientists from @CERN and more than 10 other institutions/labs ;-)']",http://arxiv.org/abs/2005.00949,"Full detector simulation was among the largest CPU consumer in all CERN experiment software stacks for the first two runs of the Large Hadron Collider (LHC). In the early 2010's, the projections were that simulation demands would scale linearly with luminosity increase, compensated only partially by an increase of computing resources. The extension of fast simulation approaches to more use cases, covering a larger fraction of the simulation budget, is only part of the solution due to intrinsic precision limitations. The remainder corresponds to speeding-up the simulation software by several factors, which is out of reach using simple optimizations on the current code base. In this context, the GeantV R&D project was launched, aiming to redesign the legacy particle transport codes in order to make them benefit from fine-grained parallelism features such as vectorization, but also from increased code and data locality. This paper presents extensively the results and achievements of this R&D, as well as the conclusions and lessons learnt from the beta prototype. ","GeantV: Results from the prototype of concurrent vector particle
  transport simulation in HEP"
125,1257677775630188546,894548126329184260,Noriyuki Kojima,"['New work with Hadar Averbuch-Elor, @srush_nlp and @yoavartzi to appear at ACL2020! We show building significantly less expressive models isolate the core factors of the strong performance in the black-box model trained on multimodal signals. paper: <LINK> \n(1/6) <LINK>', 'We consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. (2/6)', 'We significantly reduce the expressivity of the constituent parser in Shi et al., 2019. 1. adding a bottleneck for word-embeddings, reducing the embedding dimension drastically (i.e., 512-d ==&gt; 1-d). 2. simplifying a scoring function to merges constituents. (3/6)', 'We find our significantly less expressive versions produce similar predictions and perform just as well or even better. (4/6)', 'We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning. This can be easily visualized in 1-d embeddings space learned by a bottleneck for word-embeddings. (5/6) https://t.co/GrX1ZdMkJN', 'This is a work at @CornellCIS and @cornelltech. Code will be released soon! (6/6)', 'Hadar Averbuch-Elor @ElorHadar (6+/6)']",https://arxiv.org/abs/2005.01678,"Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning. ",What is Learned in Visually Grounded Neural Syntax Acquisition
126,1257603797519675392,84888361,Miriam Rengel,['Very proud of this new paper led by my DFG postdoc Denis Shulyak. It studies disequilibrium chemistry on the simulated spectra of Hot Jupiter atmospheres. Disequilibrium chemistry could be robustly detected with future missions like JWST and ARIEL. \n<LINK> <LINK>'],https://arxiv.org/abs/2005.01470,"In this work we study the effect of disequilibrium processes on mixing ratio profiles of neutral species and on the simulated spectra of a hot Jupiter exoplanet that orbits stars of different spectral types. We also address the impact of stellar activity that should be present to a different degree in all stars with convective envelopes. We used the VULCAN chemical kinetic code to compute number densities of species. The temperature-pressure profile of the atmosphere was computed with the HELIOS code. We also utilized the $\tau$-ReX forward model to predict the spectra of planets in primary and secondary eclipses. In order to account for the stellar activity we made use of the observed solar XUV spectrum taken from Virtual Planetary Laboratory (VPL) as a proxy for an active sun-like star. We find large changes in mixing ratios of most chemical species in planets orbiting A-type stars that radiate strong XUV flux inducing a very effective photodissociation. For some species, these changes can propagate very deep into the planetary atmosphere to pressures of around 1 bar. To observe disequilibrium chemistry we favor hot Jupiters with temperatures Teq=1000 K and ultra-hot Jupiters with Teq=3000$ K that also have temperature inversion in their atmospheres. On the other hand, disequilibrium calculations predict little changes in spectra of planets with intermediate temperatures. We also show that stellar activity similar to the one of the modern Sun drives important changes in mixing ratio profiles of atmospheric species. However, these changes take place at very high atmospheric altitudes and thus do not affect predicted spectra. We estimate that the effect of disequilibrium chemistry in planets orbiting nearby bright stars could be robustly detected and studied with future missions with spectroscopic capabilities in infrared such as, e.g., JWST and ARIEL. ","Stellar impact on disequilibrium chemistry and on observed spectra of
  hot Jupiter atmospheres"
127,1257586464872955905,633108360,Matteo Cadeddu,"['Phase II started in Italy just yesterday. Since we are following strictly the rules we decided that our new paper can finally ""go out""! Physics results from the first #COHERENT observation of #CEνNS in Ar and combination with CsI. #COVID19 \n<LINK>\n\n@COHERENT_NUS <LINK>']",https://arxiv.org/abs/2005.01645,"We present the results on the radius of the neutron distribution in $^{40}\text{Ar}$, on the low-energy value of the weak mixing angle, and on the electromagnetic properties of neutrinos obtained from the analysis of the coherent neutrino-nucleus elastic scattering data in argon recently published by the COHERENT collaboration, taking into account proper radiative corrections. We present also the results of the combined analysis of the COHERENT argon and cesium-iodide data for the determination of the low-energy value of the weak mixing angle and the electromagnetic properties of neutrinos. In particular, the COHERENT argon data allow us to improve significantly the only existing laboratory bounds on the electric charge $q_{\mu\mu}$ of the muon neutrino and on the transition electric charge $q_{\mu\tau}$. ","Physics results from the first COHERENT observation of CE$\nu$NS in
  argon and their combination with cesium-iodide data"
128,1257562616286904325,1007218217633361920,Fredrik K. Gustafsson,"['New paper: How to Train Your Energy-Based Model for Regression.\n\narXiv: <LINK>\nCode: <LINK>\nProject page: <LINK> <LINK>', 'We propose a simple yet highly effective extension of noise contrastive estimation (NCE) to train energy-based models p(y|x; theta) for regression tasks.', 'Our proposed method, termed NCE+, can be understood as a direct generalization of NCE, accounting for noise in the annotation process of real-world datasets.', 'We provide a detailed comparison of NCE+ and six popular methods from literature, the results of which suggest that NCE+ should be considered the go-to training method.', 'We also apply NCE+ to the task of visual tracking, setting a new state-of-the-art on five commonly used datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet.', '@johnmoberg Thanks!']",https://arxiv.org/abs/2005.01698,"Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves 63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at this https URL ",How to Train Your Energy-Based Model for Regression
129,1257511625352855552,1183785084152942592,Juan Camilo Zapata T.,['Looking for a better description of core-electrons?\n\nHere we present the development of three new basis sets designed to accurately describe core-electron properties. \n\nFull paper: <LINK>\naixiv link: <LINK>\n\n#compchem #basissets <LINK>'],https://arxiv.org/abs/2005.01237,"The traditional Gaussian basis sets used in modern quantum chemistry lack an electron-nuclear cusp, and hence struggle to accurately describe core electron properties. A recently introduced novel type of basis set, mixed ramp-Gaussians, introduce a new primitive function called a ramp function which addresses this issue. This paper introduces three new mixed ramp-Gaussian basis sets - STO-R, STO-RG and STO-R2G, made from a linear combination of ramp and Gaussian primitive functions - which are derived from the single-core-zeta Slater basis sets for the elements Li to Ne. This derivation is done in an analogous fashion to the famous STO-$n$G basis sets. The STO-RG basis functions are found to outperform the STO-3G basis functions and STO-R2G outperforms STO-6G, both in terms of wavefunction fit and other key quantities such as the one-electron energy and the electron-nuclear cusp. The second part of this paper performs preparatory investigations into how standard all-Gaussian basis sets can be converted to ramp-Gaussian basis sets through modifying the core basis functions. Using a test case of the 6-31G basis set for carbon, we determined that the second Gaussian primitive is less important when fitting a ramp-Gaussian core basis function directly to an all-Gaussian core basis function than when fitting to a Slater basis function. Further, we identified the basis sets that are single-core-zeta and thus should be most straightforward to convert to mixed ramp-Gaussian basis sets in the future. ","Mixed ramp-Gaussian basis sets for core-dependent properties: STO-RG and
  STO-R2G for Li-Ne"
130,1257509453022130176,1176913670057545729,Sanjay Subramanian,"['I\'m excited to share new work appearing @aclmeeting! ""Obtaining Faithful Interpretations from Compositional Neural Networks"" by @sanjayssub, @ben_bogin, @nitish_gup (co-first authors), Tomer Wolfson, @sameer_, @JonathanBerant, @nlpmattg \nPaper: <LINK> 1/5 <LINK>', ""Neural module networks (NMNs) have been applied to many tasks and provide interpretations for compositional reasoning, but are these interpretations faithful to the model's reasoning?\n\nWe show this is not necessarily the case and provide ways to measure+improve faithfulness. 2/5"", ""Evaluation of whether individual module outputs make sense is generally done qualitatively by looking at examples. By contrast, we collect annotations for module outputs and systematically evaluate whether the NMN's module outputs match the annotated ones. 3/5"", 'We find that NMNs trained with only end-task supervision  tend to have unfaithful interpretations, and we present ways to improve faithfulness: training with module-wise supervision, careful design of module architectures, and de-contextualizing word representations. 4/5', 'We show that for both visual (NLVR2) and textual (DROP) reasoning, our methods improve faithfulness of NMNs\n\nCode and data will soon be publicly available at the link in the paper. I hope others use our module-wise annotations for evaluation and build on our work! 5/5']",https://arxiv.org/abs/2005.00724,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy. ",Obtaining Faithful Interpretations from Compositional Neural Networks
131,1257408145162407938,1114262251559636992,Marten van Schijndel,"['New #acl2020 paper with Forrest Davis contrasts RNN LM high/low RC attachment vs linear recency biases. Modeling English is easy. Modeling Spanish is hard. \n\n<LINK>', ""@boknilev Fair point. We haven't looked at other genres yet, but given the comprehension bias that's been observed in the literature, I'd expect edited text such as Wikipedia and newswire would exhibit stronger native language biases than less edited genres.""]",https://arxiv.org/abs/2005.00165,"A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all. ","Recurrent Neural Network Language Models Always Learn English-Like
  Relative Clause Attachment"
132,1257354626912980992,46165258,Shane Steinert-Threlkeld,"['New paper for #acl2020nlp!\n\n""On the Spontaneous Emergence of Discrete and Compositional Signals"" \nwith @nurikolan and Emmanuel Chemla\n\nPaper: <LINK>\nCode: <LINK> <LINK>', ""We look at two of Hockett's design features for language---discreteness and displacement---in a general setting for emergent communication, where our message space is continuous instead of discrete. https://t.co/aHlwHAVLuQ"", 'This allows us to (i) explore when discrete signals emerge and (ii) avoid using tools like reinforcement learning, which are difficult in practice.  We reliably find discreteness, in both production and preception.', ""Two probes for compositional structure (vector analogies, and a composition prediction network) both fail.  There are interesting and very robust patterns during training though (see original tweet's gif), so stay tuned!"", 'We also show how the rudiments of Categorical Perception---where discriminability in a continuous space depends on discrete labels, not merely perceptual distance---can be found in our message space.  More ""psycholinguistic"" work in emergent communication would be great! https://t.co/tioN51gsVE']",https://arxiv.org/abs/2005.00110,"We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional. ",On the Spontaneous Emergence of Discrete and Compositional Signals
133,1257293116345921537,17373048,Rodrigo Nemmen,"['New paper out: Jet efficiencies and black hole spins in jetted quasars, <LINK>\nwhere we estimate quasar powers from @NASAFermi observations, constrain how efficiently BHs convert accreted mass into outflows and estimate how fast they are spinning 1/4', 'Gamma-ray luminosities correlate with BH masses, so one could try to use gamma photons to have an idea of how massive a jetted quasar is. Good way to cross-check virial estimates 2/4 https://t.co/n0ZmQACUVR', 'Here we plot the lower limit on the jet efficiencies (what comes out/what goes in) as a function of BH mass. A few of the blazars are ""overpowered"" meaning that according to existing BH models they should not be possible: those four points above the line 3/4 https://t.co/zCJotZT3xm', 'Finally, here is a distribution of BH spin lower limits for the sample. The spin is one of the two fundamental parameters of BH spacetimes besides mass (charge is unimportant).  These guys are, unsurprisingly, rotating fast! 4/4 https://t.co/mZpHwuZvf3', 'paper led by @outflows (very appropriate user name!)']",https://arxiv.org/abs/2005.00381,"The mechanisms responsible for the production of relativistic jets from supermassive black holes (SMBHs) accreting at near-Eddington rates are not well-understood. Simple theoretical expectations indicate that SMBHs in quasars accrete via thin discs which should produce at most very weak jets. This is contradicted by observations of powerful jets in flat-spectrum radio quasars (FSRQs). We use gamma-ray luminosities observed with the \emph{fermi} Large Area Telescope as a proxy of the jet power for a population of 154 FSRQs. Assuming typical quasar accretion rates and using black hole mass measurements from a variety of methods, we find a mean jet production efficiency of about 10 per cent for FSRQs, with values as high as 222 per cent. We find that this is consistent with FSRQs hosting moderately thin, magnetically arrested accretion discs around rapidly spinning black holes (BHs). Modeling our observations using general relativistic magnetohydrodynamic (GRMHD) simulations of jets from thin discs, we find an average lower limit of $a_* = 0.59$ for the SMBH spins of FSRQs, with tendency for the spins to decrease as the black hole mass increases. Our results are consistent with the merger-driven evolution of SMBHs. 3 per cent of the sample cannot be explained by current GRMHD models of jet production from Kerr BHs due to the high efficiencies. Along the way, we find a correlation between BH masses and $L_\gamma$ which may be an useful mass estimator in blazar gamma-ray studies. ",Jet efficiencies and black hole spins in jetted quasars
134,1257287512067883008,2613619922,byron wallace,"['new work on learning to provide *faithful* rationales led by @successar_nlp w/@sarahwiegreffe, &amp; @yuvalpi to appear #acl2020nlp. \n\npaper: <LINK>\n\n👇quick thread 1/5', ""we'd often like models to tell us why they made particular predictions, but it can be tricky to know which tokens influenced outputs in the era of contextualized embeddings (which allow arbitrary interactions).  2/5"", 'performing *hard* selection over inputs to extract rationales fixes this, but training discrete selection models is finicky 3/5', 'so, we propose a very simple method (FRESH) for discrete rationale selection: use arbitrary saliency scores + heuristics to derive pseudo-labels on tokens and then train an extractor on these -- an independent classifier module then *only* uses extracted rationales. 4/5 https://t.co/JQz5J2vsm1', 'it turns out that despite its simplicity, this strategy fares as well or better than, e.g., using REINFORCE to train the discrete extraction module in our experiments 5/5 https://t.co/iPJiv7ZR0M']",https://arxiv.org/abs/2005.00115,"In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text `responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to `end-to-end' approaches, while being more general and easier to train. Code is available at this https URL ",Learning to Faithfully Rationalize by Construction
135,1257272689334669314,445794479,Arnab Majumdar,"['The preprint of our new paper: ""... We use theories of disease spreading on networks to look at the COVID-19 epidemic on the basis of individual contacts ... Our second aim is to look at the role of social deprivation"" #covid19  <LINK>', 'Thanks @pooja_news for detailed lists of confinement zones in Kolkata. It was an invaluable source for the manuscript.']",https://arxiv.org/abs/2005.00491,"We have two main aims in this paper. First we use theories of disease spreading on networks to look at the COVID-19 epidemic on the basis of individual contacts -- these give rise to predictions which are often rather different from the homogeneous mixing approaches usually used. Our second aim is to look at the role of social deprivation, again using networks as our basis, in the spread of this epidemic. We choose the city of Kolkata as a case study, but assert that the insights so obtained are applicable to a wide variety of urban environments which are densely populated and where social inequalities are rampant. Our predictions of hotspots are found to be in good agreement with those currently being identifed empirically as containment zones and provide a useful guide for identifying potential areas of concern. ","Heterogeneous contact networks in COVID-19 spreading: the role of social
  deprivation"
136,1257219221555236864,2425754287,Roman Orus,"['New paper out! Thermal 3d bosons with TNs, with @saeedjahromi  <LINK>']",https://arxiv.org/abs/2005.00314,"Ultracold atoms in optical lattices are one of the most promising experimental setups to simulate strongly correlated systems. However, efficient numerical algorithms able to benchmark experiments at low-temperatures in interesting 3d lattices are lacking. To this aim, here we introduce an efficient tensor network algorithm to accurately simulate thermal states of local Hamiltonians in any infinite lattice, and in any dimension. We apply the method to simulate thermal bosons in optical lattices. In particular, we study the physics of the (soft-core and hard-core) Bose-Hubbard model on the infinite pyrochlore and cubic lattices with unprecedented accuracy. Our technique is therefore an ideal tool to benchmark realistic and interesting optical-lattice experiments. ",Thermal bosons in 3d optical lattices via tensor networks
137,1257150765694701569,976155561522794497,Juliano César Silva Neves,['My new paper is about black hole shadow and extra dimension. It is possible to evaluate the influence from a 5-dimensional universe on our 4-dimensional world. #BlackHole \n<LINK>'],https://arxiv.org/abs/2005.00483,"A constraint on the tidal charge generated within a brane world is shown. Using the shadow of a rotating black hole in a brane context in order to describe the M87* parameters recently announced by the Event Horizon Telescope Collaboration, the deviation from circularity of the reported shadow produces an upper bound on the bulk's nonlocal effect, which is conceived of as a tidal charge in the four-dimensional brane induced by the five-dimensional bulk. Therefore, a deviation from circularity $\lesssim 10\%$ leads to an upper bound on the tidal charge $\lesssim 0.004M^2$. ",Constraining the tidal charge of brane black holes using their shadows
138,1266412498581848064,1002014071,Frank Wilczek,['New paper with Hongye Yu + Biao Wu on a quantum algorithm for finding large independent sets\n<LINK> .   Uses nonabelian geometric phase + annealing in an emergent Hamiltonian.  Quantum mechanics is fun. <LINK>'],https://arxiv.org/abs/2005.13089,"We present a quantum algorithm for approximating maximum independent sets of a graph based on quantum non-Abelian adiabatic mixing in the sub-Hilbert space of degenerate ground states, which generates quantum annealing in a secondary Hamiltonian. For both sparse and dense graphs, our quantum algorithm on average can find an independent set of size very close to $\alpha(G)$, which is the size of the maximum independent set of a given graph $G$. Numerical results indicate that an $O(n^2)$ time complexity quantum algorithm is sufficient for finding an independent set of size $(1-\epsilon)\alpha(G)$. The best classical approximation algorithm can produce in polynomial time an independent set of size about half of $\alpha(G)$. ",Quantum Algorithm for Approximating Maximum Independent Sets
139,1264900708169666560,353256060,Manuel Pariente,"[""Happy to share LibriMix, a new noisy speech separation dataset. \nWe were inspired by @Dolby's paper (@jordiponsdotme @vivek_kumar) but they couldn't share the dataset so we did it, thanks for their work !\n📜Paper: <LINK>\n⌨️Code: <LINK>\n1/2"", ""We show that models trained on WHAM! (@whisperai1) don't generalize well to other similar datasets and that LibriMix partially addresses these issues. 2/3"", 'We also release a partially overlapping version of LibriMix that emulates real conversations (see the code here : https://t.co/WAFqZwKS3f)\n3/3']",https://arxiv.org/abs/2005.11262,"In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set. ",LibriMix: An Open-Source Dataset for Generalizable Speech Separation
140,1264730066010828801,4666231375,Konstantin Batygin,"['In a new paper led by newly-minted-Dr. Sarah Millholland @Yale (+Erik Petigura @UCLA +yours truly @Caltech), we argue that tidal heating routinely puffs up sub-Saturns, meaning that their envelope-to-core mass ratios are not in conflict with core accretion <LINK>']",https://arxiv.org/abs/2005.11209,"While the Solar System contains no planets between the sizes of Uranus and Saturn, our current exoplanet census includes several dozen such planets with well-measured masses and radii. These sub-Saturns exhibit a diversity of bulk densities, ranging from ~$0.1-3\ \rm{g\ cm}^{-3}$. When modeled simply as hydrogen/helium envelopes atop rocky cores, this diversity in densities translates to a diversity in planetary envelope fractions, $f_\rm{env}=M_\rm{env}/M_p$ ranging from ~$10\%$ to ~$50\%$. Planets with $f_\rm{env}\sim50\%$ pose a challenge to traditional models of giant planet formation by core-nucleated accretion, which predict the onset of runaway gas accretion when $M_\rm{env}\sim M_\rm{core}$. Here we show that many of these apparent $f_\rm{env}\sim50\%$ planets are less envelope rich than they seem, after accounting for tidal heating. We present a new framework for modeling sub-Saturn interiors that incorporates envelope inflation due to tides, which are driven by the observed non-zero eccentricities, as well as potential obliquities. Consequently, when we apply our models to known sub-Saturns, we infer lower $f_\rm{env}$ than tides-free estimates. We present a case study of K2-19 b, a moderately eccentric sub-Saturn. Neglecting tides, K2-19 b appears to have $f_\rm{env}\sim50\%$, poised precariously near the runaway threshold; by including tides, we find $f_\rm{env}\sim10\%$, resolving the tension. Through a systematic analysis of $4-8\ R_{\oplus}$ planets, we find that most (but not all) of the similarly envelope-rich planets have more modest envelopes of $f_\rm{env}\sim10\%-20\%$. Thus, many sub-Saturns may be understood as sub-Neptunes that have undergone significant radius inflation, rather than a separate class of objects. Tidal radius inflation likely plays an important role in other size classes of planets including ultra-low-density Jupiter-size planets like WASP-107 b. ",Tidal Inflation Reconciles Low-Density Sub-Saturns with Core Accretion
141,1263812509775134720,281711973,Dr. Emily Rickman,['Check out our new @SPHERE_outreach paper out today on a benchmark T-type brown dwarf with both radial velocity and direct imaging measurements!\n\n👇👇👇\n\n<LINK> <LINK>'],https://arxiv.org/abs/2005.10312,"Context. Detecting and characterizing substellar companions for which the luminosity, mass, and age can be determined independently is of utter importance to test and calibrate the evolutionary models due to uncertainties in their formation mechanisms. HD 19467 is a bright and nearby star hosting a cool brown dwarf companion detected with RV and imaging, making it a valuable object for such studies. Aims. We aim to further characterize the orbital, spectral, and physical properties of the HD 19467 system. Methods. We present new high-contrast imaging data with the SPHERE and NaCo instruments. We also analyze archival data from HARPS, NaCo, HIRES, UVES, and ASAS. We also use proper motion data of the star from Hipparcos and Gaia. Results. We refine the properties of the host star and derive an age of 8.0$^{+2.0}_{-1.0}$ Gyr based on isochrones, gyrochronology, and chemical and kinematic arguments. This estimate is slightly younger than previous estimates of ~9-11 Gyr. No orbital curvature is seen in the current imaging, RV, and astrometric data. From a joint fit of the data, we refine the orbital parameters for HD 19467B: period 398$^{+95}_{-93}$ yr, inclination 129.8$^{+8.1}_{-5.1}$ deg, eccentricity 0.56$\pm$0.09, longitude of the ascending node 134.8$\pm$4.5 deg, and argument of the periastron 64.2$^{+5.5}_{-6.3}$ deg. We assess a dynamical mass of 74$^{+12}_{-9}$ MJ. The fit with atmospheric models of the spectrophotometric data of HD 19467B indicates an atmosphere without clouds or with very thin clouds, an effective temperature of 1042$^{+77}_{-71}$ K, and a large surface gravity of 5.34$^{+0.08}_{-0.09}$ dex. The comparison to model predictions of the bolometric luminosity and dynamical mass of HD 19467B, assuming our system age estimate, indicates a better agreement with the Burrows et al. models; whereas the other evolutionary models used tend to underestimate its cooling rate. ","Orbital and spectral characterization of the benchmark T-type brown
  dwarf HD 19467B"
142,1263281166221639682,1139739755510243328,Yonglong Tian,"['Check this new paper: what makes for good views for contrastive learning?\npdf: <LINK>\ncode: <LINK>\n- If you like analysis, there are fun experiments and intuitive theory.\n- If you like SoTA, there are best-performing models to play with.']",https://arxiv.org/abs/2005.10243,"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:this http URL ",What Makes for Good Views for Contrastive Learning?
143,1263198794004471809,1001323493336662016,Noga Zaslavsky,"['Very excited to share our new paper: “A Rate-Distortion view of human pragmatic reasoning”  <LINK>\nJoint work with @_jennhu and @roger_p_levy.  (1/) <LINK>', 'Our results suggest that pragmatic reasoning may emerge from efficient data compression, as characterized by Shannon’s Rate-Distortion theory. (2/)', 'First, we show that the prominent Rational Speech Act (RSA) framework for recursive pragmatic reasoning implements an alternating maximization algorithm for optimizing a type of least-effort tradeoff. (3/)', 'We use this result to disconfirm the conjecture that the expected utility in RSA is guaranteed to improve with recursion depth. (4/)', 'Next, we show that RSA can be grounded in Rate-Distortion theory, yielding a slightly modified model of pragmatic reasoning. We refer to this new model as RD-RSA. (5/)', 'On that basis, we study the dynamics of RSA and RD-RSA and compare their predictions. We find that RD-RSA is similar to RSA in its ability to account for human behavior, while avoiding a bias of RSA toward random utterance production.']",https://arxiv.org/abs/2005.06641,"What computational principles underlie human pragmatic reasoning? A prominent approach to pragmatics is the Rational Speech Act (RSA) framework, which formulates pragmatic reasoning as probabilistic speakers and listeners recursively reasoning about each other. While RSA enjoys broad empirical support, it is not yet clear whether the dynamics of such recursive reasoning may be governed by a general optimization principle. Here, we present a novel analysis of the RSA framework that addresses this question. First, we show that RSA recursion implements an alternating maximization for optimizing a tradeoff between expected utility and communicative effort. On that basis, we study the dynamics of RSA recursion and disconfirm the conjecture that expected utility is guaranteed to improve with recursion depth. Second, we show that RSA can be grounded in Rate-Distortion theory, while maintaining a similar ability to account for human behavior and avoiding a bias of RSA toward random utterance production. This work furthers the mathematical understanding of RSA models, and suggests that general information-theoretic principles may give rise to human pragmatic reasoning. ",A Rate-Distortion view of human pragmatic reasoning
144,1262596157823348736,957685323198164992,Ziwei Liu,"['#MMFashion toolbox has been updated to v0.4, now supporting two new fashion analysis tasks:\n* Fashion Segmentation and Parsing\n* Fashion Compatibility and Recommendation\n\n- Code: <LINK>\n- Paper: <LINK>\n\nFeedback and contributions are welcome! <LINK>', 'In addition, #DeepFashion Database (In-shop Clothes Retrieval Benchmark) has added two new types of annotations:\n\n* Parsing Mask Annotations\n* Dense Pose Annotations\n\n- Dataset: https://t.co/OFMXSJO8YV \n\nFeel free to download and play :) https://t.co/a3dPP5TrpY']",https://arxiv.org/abs/2005.08847,"We present MMFashion, a comprehensive, flexible and user-friendly open-source visual fashion analysis toolbox based on PyTorch. This toolbox supports a wide spectrum of fashion analysis tasks, including Fashion Attribute Prediction, Fashion Recognition and Retrieval, Fashion Landmark Detection, Fashion Parsing and Segmentation and Fashion Compatibility and Recommendation. It covers almost all the mainstream tasks in fashion analysis community. MMFashion has several appealing properties. Firstly, MMFashion follows the principle of modular design. The framework is decomposed into different components so that it is easily extensible for diverse customized modules. In addition, detailed documentations, demo scripts and off-the-shelf models are available, which ease the burden of layman users to leverage the recent advances in deep learning-based fashion analysis. Our proposed MMFashion is currently the most complete platform for visual fashion analysis in deep learning era, with more functionalities to be added. This toolbox and the benchmark could serve the flourishing research community by providing a flexible toolkit to deploy existing models and develop new ideas and approaches. We welcome all contributions to this still-growing efforts towards open science: this https URL ",MMFashion: An Open-Source Toolbox for Visual Fashion Analysis
145,1262457294614417413,923231130383536128,Eduardo Fonseca,['🔊New preprint out! We address missing labels in AudioSet using a teacher-student framework with loss masking. Based on part of the work during my internship at @GoogleAI NYC!\n\nPaper: <LINK>\nBlog: <LINK>\n#dcase #machinelistening <LINK>'],https://arxiv.org/abs/2005.00878,"The study of label noise in sound event recognition has recently gained attention with the advent of larger and noisier datasets. This work addresses the problem of missing labels, one of the big weaknesses of large audio datasets, and one of the most conspicuous issues for AudioSet. We propose a simple and model-agnostic method based on a teacher-student framework with loss masking to first identify the most critical missing label candidates, and then ignore their contribution during the learning process. We find that a simple optimisation of the training label set improves recognition performance without additional computation. We discover that most of the improvement comes from ignoring a critical tiny portion of the missing labels. We also show that the damage done by missing labels is larger as the training set gets smaller, yet it can still be observed even when training with massive amounts of audio. We believe these insights can generalize to other large-scale datasets. ","Addressing Missing Labels in Large-Scale Sound Event Recognition Using a
  Teacher-Student Framework With Loss Masking"
146,1258704851724369920,432812106,Corrado Monti,"[""🔴New paper published at @WebSciConf!\n\nWho were Trump's supporters before Trump? We use Reddit to study which traits in 2012 anticipated the emergence of Trumpism in 2016.\n\nPreprint: <LINK>\nWith @gdfm7 @FrancescoBonchi and Joan Massachs.\n\n[1/2] Short summary ⬇️ <LINK>"", 'We find that Trump support emerged from both libertarian and conservative camps, and even ancaps. Many were interested in guns, entrepreneurship, shock humor and so-called ""Men\'s rights"".\nAlso, they often negative feedback from mainstream communities.\nSee paper for more!\n\n[2/2] https://t.co/8X2HZyKIkg', '@mtizzoni We also find r/DeadBedrooms there... 😅', 'Forgot to add: we open-sourced our data set as a prepared and easy-to-use CSV to study the change in political opinions on Reddit between 2012 and 2016\n\nHere it is:  https://t.co/DTrhTVdDXV']",https://arxiv.org/abs/2005.01790,"We study the emergence of support for Donald Trump in Reddit's political discussion. With almost 800k subscribers, ""r/The_Donald"" is one of the largest communities on Reddit, and one of the main hubs for Trump supporters. It was created in 2015, shortly after Donald Trump began his presidential campaign. By using only data from 2012, we predict the likelihood of being a supporter of Donald Trump in 2016, the year of the last US presidential elections. To characterize the behavior of Trump supporters, we draw from three different sociological hypotheses: homophily, social influence, and social feedback. We operationalize each hypothesis as a set of features for each user, and train classifiers to predict their participation in r/The_Donald. We find that homophily-based and social feedback-based features are the most predictive signals. Conversely, we do not observe a strong impact of social influence mechanisms. We also perform an introspection of the best-performing model to build a ""persona"" of the typical supporter of Donald Trump on Reddit. We find evidence that the most prominent traits include a predominance of masculine interests, a conservative and libertarian political leaning, and links with politically incorrect and conspiratorial content. ","Roots of Trumpism: Homophily and Social Feedback in Donald Trump Support
  on Reddit"
147,1258555474775064576,35926248,Thomas Paula,"['Our paper ""A Proposal for Intelligent Agents with Episodic Memory"" is on Arxiv! We propose an alternative look at the role of episodic memory for intelligent agents. We hope this view can help to spark new discussions and ideas! @dfm794  @JulianoVacaro \n\n<LINK>']",https://arxiv.org/abs/2005.03182,"In the future we can expect that artificial intelligent agents, once deployed, will be required to learn continually from their experience during their operational lifetime. Such agents will also need to communicate with humans and other agents regarding the content of their experience, in the context of passing along their learnings, for the purpose of explaining their actions in specific circumstances or simply to relate more naturally to humans concerning experiences the agent acquires that are not necessarily related to their assigned tasks. We argue that to support these goals, an agent would benefit from an episodic memory; that is, a memory that encodes the agent's experience in such a way that the agent can relive the experience, communicate about it and use its past experience, inclusive of the agents own past actions, to learn more effective models and policies. In this short paper, we propose one potential approach to provide an AI agent with such capabilities. We draw upon the ever-growing body of work examining the function and operation of the Medial Temporal Lobe (MTL) in mammals to guide us in adding an episodic memory capability to an AI agent composed of artificial neural networks (ANNs). Based on that, we highlight important aspects to be considered in the memory organization and we propose an architecture combining ANNs and standard Computer Science techniques for supporting storage and retrieval of episodic memories. Despite being initial work, we hope this short paper can spark discussions around the creation of intelligent agents with memory or, at least, provide a different point of view on the subject. ",A Proposal for Intelligent Agents with Episodic Memory
148,1258034187107434496,48329145,Thomas Kober,"['New paper announcement: ""Data Augmentation for Hypernymy Detection"" with brilliant collaborators Julie Weeds, Lorenzo Bertolini and David Weir @SussexUni\n\narXiv: <LINK>\npdf: <LINK>\n\n1/10', 'We do data augmentation based on distributional composition and GANs and see some solid improvements for basic LR and FF models, with two different distributional vector space models, on supervised hypernymy detection. \n\n2/10', 'We compare augmentation by composition and GANs to two ways of _extending_ a given training set with either hyponym-hypernym pairs from WordNet or extracted from a large corpus with Hearst Patterns.\n\n3/10', 'We expected that extending a dataset with pairs from WordNet will likely be the upper limit of what we can expect from our data augmentation techniques, however we find that augmentation by composition and GANs frequently performs better than WordNet 😱😱😱\n\n4/10', 'Distributional composition: Given t hyponym-hypernym pair dog-animal in the training data, we collect modifiers for animal and dog (e.g. small, hungry, etc) and add small dog-dog, hungry dog-dog, small dog-animal, and hungry dog-animal as additional pairs to the training set.5/10', 'We average the vector representations for ""hungry"" and ""dog"" to get a composed vector. Negative examples are created by pairing ""small dog"" with a neighbour of ""animal"", say ""vehicle"", such that the negative pair ""small dog-vehicle"" is added to the training data.\n\n6/10', 'For the GAN based approach - GANDALF (GAN-based Data Augmentation for Lexical inFerence) - we simply aim to generate vectors that ""look like"" real nouns (i.e. that are close in terms of cosine similarity to actual word vectors).\n\n7/10', 'Given the pair dog-animal, we simply pick the top n most similar GANDALF-ed vectors to dog and animal and add those as additional positive examples to the training data. Negative examples are created by mimicking observed negative examples, e.g. dog-cat.\n\n8/10', 'We also introduce a new hand annotated dataset - HP4K - that does not rely on WordNet or other hand curated lexicons. Given that some datasets as well as models make use of WordNet, we think that this dataset will be a neat addition to existing test suites.\n\n9/10', 'Relevant links:\narXiv: https://t.co/15L2JgZSfA\npdf: https://t.co/ZXimmHwtrb\ngithub: https://t.co/anC6vR9mlr\n\nThe dataset is already on github, the code will follow soon.\n\n10/10']",https://arxiv.org/abs/2005.01854,"The automatic detection of hypernymy relationships represents a challenging problem in NLP. The successful application of state-of-the-art supervised approaches using distributed representations has generally been impeded by the limited availability of high quality training data. We have developed two novel data augmentation techniques which generate new training examples from existing ones. First, we combine the linguistic principles of hypernym transitivity and intersective modifier-noun composition to generate additional pairs of vectors, such as ""small dog - dog"" or ""small dog - animal"", for which a hypernymy relationship can be assumed. Second, we use generative adversarial networks (GANs) to generate pairs of vectors for which the hypernymy relation can also be assumed. We furthermore present two complementary strategies for extending an existing dataset by leveraging linguistic resources such as WordNet. Using an evaluation across 3 different datasets for hypernymy detection and 2 different vector spaces, we demonstrate that both of the proposed automatic data augmentation and dataset extension strategies substantially improve classifier performance. ",Data Augmentation for Hypernymy Detection
149,1257985932374036485,123440015,P. Ajie Utama,"['A new paper in #acl2020nlp paper about ""Debiasing NLU Models without Degrading the In-distribution Performance""! Here we address the issue of out-of-distribution/adversarial improvement which is at odds with the in-distribution performance. <LINK> <LINK>', 'We propose a confidence regularization method that helps models to avoid exploiting biases by discouraging them to be overconfident on examples containing artifacts https://t.co/fwXStC2GNj', 'This simple method is shown to be effective in obtaining equally high accuracy on 3 OOD evaluations, e.g., MNLI &gt; HANS, Fever &gt; SymmetricFever, QQP &gt; PAWS; while maintaining its in-distribution performance https://t.co/pxiWrA2IHx', 'By preventing overconfidence, we also show that the resulting models are being better calibrated, i.e., probabilities assigned to the predicted label are more aligned with their accuracy https://t.co/Na80t5902j', 'Joint work with amazing collaborators in UKP @NafiseSadat &amp; Iryna Gurevych!']",https://arxiv.org/abs/2005.00315,"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. ","Mind the Trade-off: Debiasing NLU Models without Degrading the
  In-distribution Performance"
150,1268165429379375106,1267746246963400704,Lukas Pukelis,['We have used the Fields of Study data from Microsoft Academic (@MSFTAcademic ) to create a tool that allows to identify research related to #SustainableDevelopmentGoals and to assign it #SDG labels. Check out our paper on ArXiv: <LINK>'],https://arxiv.org/abs/2005.14569,"Sustainable Development Goals (SDGs) bring together the diverse development community and provide a clear set of development targets for 2030. Given a large number of actors and initiatives related to these goals, there is a need to have a way to accurately and reliably assign text to different input: scientific research, research projects, technological output or documents to specific SDGs. In this paper we present Open Source SDG (OSDG) project and tool which does so by integrating existing research and previous classification into a robust and coherent framework. This integration is based on linking the features from the variety of previous approaches, like ontology items, keywords or features from machine-learning models, to the topics in Microsoft Academic Graph. ","OSDG -- Open-Source Approach to Classify Text Data by UN Sustainable
  Development Goals (SDGs)"
151,1266306830340370432,1525220317,Lena Frischlich,"['(1/10) Follow up to #PandemicPopulism (summary thread: <LINK>) is online- again as preprint so not formally reviewed so far. In #PandemicNews (<LINK>) we use computational content analysis to study GER news papers #CoronaVirusDE coverage <LINK>', '(2/10) @thorstenquandt @ZwiZwaSvens and @Kudusch and I analysed &gt;100k Facebook posts of German newspapers (national + regional) between January and end of March 2020, looking at topics (including emergence over time), central actors, overall negativity, &amp; treatement of falsehoods', '(3/10) Results show how reporting about the crisis evolved over time: From focussing on core information in the beginning, over context and background info up to offering multiple perspectives and insights into how #CoronaVirusDE  impacted peoples lifes later on https://t.co/0LyCa0vvgq', '(4/10) In comparison to our https://t.co/NOHyFBuMyw analysis in #PandemicPopulism we found a larger variety of voices in established journalistic media, although prominent political figures are mentioned particularly often.', '(5/10) Notably: Relatively spoken, chancellor Merkel is *more* present in alternative media compared to the established press- in contrast to alternative news medias self-positioning as a place for unheard voices... https://t.co/91t8MfeZMO', '(6/10) The overall tone of reporting is more critical in alternative news media but we do not see that this difference increased over time (contradicting accusations that  professional news media became uncritical) https://t.co/1CkVprxyMF', '(7/10) Share of #FakeNews in the overall reporting of established media is marginal, overall more info about medical #misinformation than #ConspiracyTheory - all discussed only when #FactChecked. But see https://t.co/8bKTejtxw0 for risks of journalistic coverage of #fakeNews https://t.co/9BbYrIeKIg', '(8/10) In sum: Our  data doesn\'t point at a radical failure of the journalistic system. Much of the stark criticism of ""the media"" seems to be based on a narrow understanding of journalism as permanent opposition. https://t.co/domfxhlfwf', '(9/10) But thats only one  part of the journalistic role as the #WorldOfJournalism Study shows (https://t.co/50UzYNNY4x). Instead, our data shows that German online journalism responded to #coronavirus with a multi-perspective coverage acting within the given societal system. https://t.co/eY8mWK8276', '(10/10) Caution! We focussed only on Facebook (i.e. can differ on other plattforms) and Germany (can differ in other countries), only data till the end of March 2020 (might be different now). Study has not been peer reviewed so far. We hope you like it nonetheless! https://t.co/MZej5B1hdQ']",https://arxiv.org/abs/2005.13290,"The unfolding of the COVID-19 pandemic has been an unprecedented challenge for news media around the globe. While journalism is meant to process yet unknown events by design, the dynamically evolving situation affected all aspects of life in such profound ways that even the routines of crisis reporting seemed to be insufficient. Critics noted tendencies to horse-race reporting and uncritical coverage, with journalism being too close to official statements and too affirmative of political decisions. However, empirical data on the performance of journalistic news media during the crisis has been lacking thus far. The current study analyzes the Facebook messages of journalistic news media during the early Coronavirus crisis, based on a large German data set from January to March 2020. Using computational content analysis methods, reach and interactions, topical structure, relevant actors, negativity of messages, as well as the coverage of fabricated news and conspiracy theories were examined. The topical structure of the near-time Facebook coverage changed during various stages of the crisis, with just partial support for the claims of critics. The initial stages were somewhat lacking in topical breadth, but later stages offered a broad range of coverage on Corona-related issues and societal concerns. Further, journalistic media covered fake news and conspiracy theories during the crisis, but they consistently contextualized them as what they were and debunked the false claims circulating in public. While some criticism regarding the performance of journalism during the crisis received mild empirical support, the analysis did not find overwhelming signs of systemic dysfunctionalities. Overall, journalistic media did not default to a uniform reaction nor to sprawling, information-poor pandemic news, but they responded with a multi-perspective coverage of the crisis. ","Pandemic News: Facebook Pages of Mainstream News Media and the
  Coronavirus Crisis -- A Computational Content Analysis"
152,1266188687219384320,51590414,Tom Brown,"['Language models are few shot learners! We find that larger models can often (but not always) perform NLP tasks given only natural language prompt and a few examples in the context. No fine-tuning.\n\nPaper: <LINK>\nIllustrated summary ⬇️ (1/12) <LINK>', 'This ""in-context learning"" happens entirely within the forward-pass on a single sequence. We study this in the zero-, one- and few-shot settings.\n(2/N) https://t.co/3X5oM43a5f', 'One way to think about this: In-context learning is the inner loop of meta-learning, and unsupervised pre-training is the outer loop. To do well at pre-training, a language model needs to learn to quickly recognize patterns within the context of a given sequence.\n(3/12) https://t.co/XQSSHITT6O', 'We apply our few-shot learning to a large number of tasks, and find evidence that bigger models are better at in-context learning. There are too many results to fit in this thread, so I’ll show some highlights &amp; lowlights (and you can see the paper for the rest🙂)\n(4/12) https://t.co/Q9reTryY1P', ""GPT-3 seems to be quite strong at question answering and trivia. We find that it can be competitive with the fine-tuned SOTA despite being shown only a few examples of the task and performing no gradient updates after pre-training. Here's TriviaQA as one instance. https://t.co/alEx4nB7J4"", 'For SuperGLUE, we find that GPT-3 with 32 examples per task performs slightly better than a fine-tuned BERT-large (which is far smaller than GPT-3). So in this case, massively scaling up the model massively reduced the data requirements. \n(6/12) https://t.co/5W0bG89zF8', 'Few-shot GPT-3 seems to struggle on some NLI-style tasks that involve comparing two sentences and applying a label. For example, we see near chance performance on ANLI for all but our largest model. We discuss some theories for why this happens in the limitations section.\n(7/12) https://t.co/ufAzAeoOiU', 'When we use larger models to generate news articles, we find that humans have an increasingly difficult time distinguishing human articles from machine generated ones. For the largest models, we find that it is approaching random chance.\n(8/12) https://t.co/iN4GtQ27wJ', 'Finally, we look at some ways where these models might go wrong. We look at the potential for misuse and study the biases of the model. My personal hope is that by studying these current weaknesses, we can develop solutions that will scale with more powerful systems.\n(9/12)', 'This paper took a village, and I’m glad to have played a part in it. It could not have happened without the hard work of everyone on the team.\n(10/12) https://t.co/CsyKjT15mJ', 'Special shout out to joint first authors @8enmann, Nick Ryder and Melanie Subbiah, to @AlecRad for research guidance and to @Dario_Amodei for leading the project.\n(11/12) https://t.co/RaKY6R9wLB', 'I encourage y’all to read (or at least skim) the paper. I’m really proud to have had a part in creating this work over the last 18 months and am glad to get to share it with you.\n\nPaper: https://t.co/6FWzo3au8t\nSamples &amp; Data: https://t.co/ze8LRhhFGW\n(12/12)', '@hardmaru Thanks, David!', '@TinDan_ Thanks, will push a fix', '@srchvrs We gave it a couple examples of converting from poor to good English in the prompt, and my intuition is that it just picked up the pattern.', ""@iantbd I'm optimistic about external knowledge bases. Intuitively it seems like letting the model offload some information should be able to give gains in terms of memory or performance/flop.""]",http://arxiv.org/abs/2005.14165,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ",Language Models are Few-Shot Learners
153,1265647301537796097,1216318368678141954,Georgios Sidiropoulos,"['""Knowledge Graph Simple Question Answering for Unseen Domains"",\xa0 w/ @nickvosk &amp; @ekanou\xa0 accepted @akbc_conf. We propose a data-centric domain adaptation framework that relies on question generation and distant supervision. Preprint available at\xa0<LINK>']",https://arxiv.org/abs/2005.12040,"Knowledge graph simple question answering (KGSQA), in its standard form, does not take into account that human-curated question answering training data only cover a small subset of the relations that exist in a Knowledge Graph (KG), or even worse, that new domains covering unseen and rather different to existing domains relations are added to the KG. In this work, we study KGSQA in a previously unstudied setting where new, unseen domains are added during test time. In this setting, question-answer pairs of the new domain do not appear during training, thus making the task more challenging. We propose a data-centric domain adaptation framework that consists of a KGSQA system that is applicable to new domains, and a sequence to sequence question generation method that automatically generates question-answer pairs for the new domain. Since the effectiveness of question generation for KGSQA can be restricted by the limited lexical variety of the generated questions, we use distant supervision to extract a set of keywords that express each relation of the unseen domain and incorporate those in the question generation method. Experimental results demonstrate that our framework significantly improves over zero-shot baselines and is robust across domains. ",Knowledge Graph Simple Question Answering for Unseen Domains
154,1265635419699515396,1403815458,Amir Siraj,"['In a new paper on the arXiv today, we propose a way to figure out whether or not Planet Nine, if it exists, could be a black hole: <LINK>']",https://arxiv.org/abs/2005.12280,"Planet Nine has been proposed to potentially be a black hole in the outer solar system. We investigate the accretion flares that would result from impacts of small Oort cloud objects, and find that the upcoming LSST observing program will be able to either rule out or confirm Planet Nine as a black hole within a year. We also find that LSST could rule out or confirm the existence of trapped planet-mass black holes out to the edge of the Oort cloud, indirectly probing the dark matter fraction in subsolar mass black holes and potentially improving upon current limits by orders of magnitude. ",Searching for Black Holes in the Outer Solar System with LSST
155,1265574709439205379,2297512354,Bastien Mallein,"['In this paper, we constructed a mathematical model  to study the possible use of group testing for the detection of SARS-COV-2 <LINK> 1/7', 'A difficulty if you want to estimate the number of currently infected individuals is that you have either to count the number of detected cases, and estimate the proportion of cases that go undetected, or test people at random and count the number of infected in your sample. 2/7', 'If you use the second method, another obstacle is that there are not that many infected people at a given time, so you would have to test a lot to give a good estimate of the prevalence (number of infected), which costs a lot of tests. 3/7', 'To avoid using too many tests, one option is group testing: you mix samples together, and test the mix. In theory, it will be positive when one individual in the group is positive. For ex. if you make groups of 10 individuals, you divide by 10 the number of tests needed. 4/7', 'The problem is that the method is not perfect. If you mix samples, you dilute virus from the positive sample, possibly below its detection threshold. To take this into account, we modelled the viral load of a typical contaminated individual as a Gaussian mixture model. 5/7', 'With this model, we estimated the false negative rate created by group testing, and use it to construct estimators for the prevalence. There is an optimal group size to use if you want to minimize the number of tests to use, probably in the 10-50 range. 6/7', 'We also investigated the potential use of group testing for the early detection of an outbreak in a community. Under reasonable conditions, it would take at most a couple of tests a day to detect an outbreak in a group of 1000 before first symptoms show up! 7/7', 'Thanks @Lionning13 and JFR for working with me on that project, and to everyone in the MODCOV initiative for their help!']",https://arxiv.org/abs/2005.06776,"Sample pooling consists in combining samples from multiple individuals into a single pool that is then tested using a unique test-kit. A positive test means that at least one individual within the pool is infected. Here, we propose an analysis and applications of sample pooling to the epidemiologic monitoring of COVID-19. We first introduce a model of the RT-qPCR process used to test for the presence of virus in a sample and construct a statistical model for the viral load in a typical infected individual inspired by the clinical data from Jones et. al. (2020). We then propose a method for the measure of the prevalence in a population, based on group testing, taking into account the increased number of false negatives associated with this method. Finally, we present an application of sample pooling for the prevention of epidemic outbreak in closed connected communities (e.g. nursing homes). ",Group testing as a strategy for the epidemiologic monitoring of COVID-19
156,1265481552860811264,795877354266456064,KoheiKamadaPhys,"['Our new paper has appeared! \n<LINK>\nWe studied the false vacuum decay (Minkowski to AdS) catalyzed by a black hole, which found to be enhanced compared to the usual CdL.', 'We took into account the thermal mass to the potential originated by the Hawking radiation to construct the bounce solution. We expected that it would reduce the decay rate.', 'But it turned out its effect is negligibly small unless the coupling between the tunneling scalar and the Hawking radiation is extremely large.', 'This result suggests together with the electroweak vacuum metastability, small primordial BHs should not be formed in the entire history of the Universe otherwise our Universe should have been collapsed into the unwanted AdS vacuum. \nOr the electroweak vacuum is completely stable']",https://arxiv.org/abs/2005.12808,"False vacuum decay is a key feature in quantum field theories and exhibits a distinct signature in the early Universe cosmology. It has recently been suggested that the false vacuum decay is catalyzed by a black hole (BH), which might cause the catastrophe of the Standard Model Higgs vacuum if primordial BHs are formed in the early Universe. We investigate vacuum phase transition of a scalar field around a radiating BH with taking into account the effect of Hawking radiation. We find that the vacuum decay rate slightly decreases in the presence of the thermal effect since the scalar potential is stabilized near the horizon. However, the stabilization effect becomes weak at the points sufficiently far from the horizon. Consequently, we find that the decay rate is not significantly changed unless the effective coupling constant of the scalar field to the radiation is extremely large. This implies that the change of the potential from the Hawking radiation does not help prevent the Standard Model Higgs vacuum decay catalyzed by a BH. ","On catalyzed vacuum decay around a radiating black hole and the crisis
  of the electroweak vacuum"
157,1265443043848552454,1264414190887817216,Behnam Hedayatnia,"['Excited to share new work from my team at Alexa \n\n""Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems""\n<LINK>\n\nWe propose using a dialogue policy to plan the content and style of our responses for open-domain response generation <LINK>']",https://arxiv.org/abs/2005.12529?context=cs.CL,"Open-domain dialogue systems aim to generate relevant, informative and engaging responses. Seq2seq neural response generation approaches do not have explicit mechanisms to control the content or style of the generated response, and frequently result in uninformative utterances. In this paper, we propose using a dialogue policy to plan the content and style of target responses in the form of an action plan, which includes knowledge sentences related to the dialogue context, targeted dialogue acts, topic information, etc. The attributes within the action plan are obtained by automatically annotating the publicly released Topical-Chat dataset. We condition neural response generators on the action plan which is then realized as target utterances at the turn and sentence levels. We also investigate different dialogue policy models to predict an action plan given the dialogue context. Through automated and human evaluation, we measure the appropriateness of the generated responses and check if the generation models indeed learn to realize the given action plans. We demonstrate that a basic dialogue policy that operates at the sentence level generates better responses in comparison to turn level generation as well as baseline models with no action plan. Additionally the basic dialogue policy has the added effect of controllability. ","Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue
  Systems"
158,1265242361661186048,16145248,Future Prof. Cori Faklaris,"['We have a paper out today re #covid19 contact-tracing apps at <LINK> - see this thread for a summary 👇🏻Many thanks to Tianshi and Jackie for doing the heavy lifting on this study! <LINK>', 'In our recent Mturk survey (N=208), many preferred to install a #ContactTracingApp with a centralized architecture, rather than a decentralized architecture (which some in CS have advocated). See our full paper on #ContactTracing app preferences here: https://t.co/J6Qhqx2S4a']",https://arxiv.org/abs/2005.11957,"Contact-tracing apps have potential benefits in helping health authorities to act swiftly to halt the spread of COVID-19. However, their effectiveness is heavily dependent on their installation rate, which may be influenced by people's perceptions of the utility of these apps and any potential privacy risks due to the collection and releasing of sensitive user data (e.g., user identity and location). In this paper, we present a survey study that examined people's willingness to install six different contact-tracing apps after informing them of the risks and benefits of each design option (with a U.S.-only sample on Amazon Mechanical Turk, $N=208$). The six app designs covered two major design dimensions (centralized vs decentralized, basic contact tracing vs. also providing hotspot information), grounded in our analysis of existing contact-tracing app proposals. Contrary to assumptions of some prior work, we found that the majority of people in our sample preferred to install apps that use a centralized server for contact tracing, as they are more willing to allow a centralized authority to access the identity of app users rather than allowing tech-savvy users to infer the identity of diagnosed users. We also found that the majority of our sample preferred to install apps that share diagnosed users' recent locations in public places to show hotspots of infection. Our results suggest that apps using a centralized architecture with strong security protection to do basic contact tracing and providing users with other useful information such as hotspots of infection in public places may achieve a high adoption rate in the U.S. ","Decentralized is not risk-free: Understanding public perceptions of
  privacy-utility trade-offs in COVID-19 contact-tracing apps"
159,1265121469157130240,2915749124,Dhiraj Hazra,"['We find several correlations between reionization and other cosmological parameters are substantially reduced - with the Planck 2018 - early onsets of reionization are strongly disfavoured.. <LINK>... always a pleasure working with Daniela, Fabio and @georgesmoot', 'https://t.co/1LvVZIa9Ar']",https://arxiv.org/abs/2005.12222,"We provide an update on the constraints on extended reionization histories with the Planck 2018 cosmic microwave background anisotropy data. The Planck 2018 data on large angular scales improve the measurement of the $E$-mode polarization reionization bump at low multipoles providing the possibility to improve our previous results. Using a minor modification to the original Poly-reion model for the reionization history, we find that the Planck 2018 data significantly improve all our previous results: we find as optical depth of $\tau=0.0572_{-0.0075}^{+0.0064}$ at 68% CL, that early onsets of reionization are strongly disfavoured, i.e. redshift when the reionization begins, $z_{xe=0}=18.18_{-10.89}^{+1.61}$ at 68% CL,and that reionization duration (defined between 10% and 99% reionization) is significantly reduced, i.e. $\Delta_z^{Reion}=4.59_{-2.45}^{+1.67}$ at 68% CL. We explore possible correlations between reionization histories and cosmological parameters, including important extensions beyond $\Lambda$CDM. We find that the degeneracy between reionization and scalar spectral index,neutrino mass sum, spatial curvature, dark matter annihilation and other non-standard models are significantly reduced.The reduction of the error bars and the degeneracies, together with the shift towards lower values of the optical depth that we observe in the Poly-reion model are mainly driven by the new low-$\ell$ polarization likelihood of Planck 2018 baseline based on the HFI data. This is confirmed also by the results derived without this likelihood and the ones with different alternatives to the baseline that are presented for a subset of models. ","Extended reionization in models beyond $\Lambda$CDM with Planck 2018
  data"
160,1264843208854568963,738769492122214400,Johannes Lischner,"['In our new paper, we study how plasmon decay in Ag nanoparticles can be controlled by dielectric embedding in different host materials. Last paper of my fantastic student Lara who now works for @DeepMind. 🥲  <LINK> #plasmonics #compchem <LINK>']",https://arxiv.org/abs/2005.10932,"Understanding and controlling properties of plasmon-induced hot carriers is a key step towards next-generation photovoltaic and photocatalytic devices. Here, we uncover a route to engineering hot-carrier generation rates of silver nanoparticles by designed embedding in dielectric host materials. Extending our recently established quantum-mechanical approach to describe the decay of quantized plasmons into hot carriers we capture both external screening by the nanoparticle environment and internal screening by silver d-electrons through an effective electron-electron interaction. We find that hot-carrier generation can be maximized by engineering the dielectric host material such that the energy of the localized surface plasmon coincides with the highest value of the nanoparticle joint density of states. This allows us to uncover a path to control the energy of the carriers and the amount produced, for example a large number of relatively low-energy carriers are obtained by embedding in strongly screening environments. ","Dielectric engineering of hot carrier generation by quantized plasmons
  in embedded silver nanoparticles"
161,1264705883662987265,2228815292,Lewis Mitchell,"['Curtis Murray tracks the ‘arc’ of patient experience with #Covid19 using Reddit posts to /r/COVIDPositive — we find that you can extract symptoms, and the emotions associated with them. Check it out on the arxiv: <LINK> <LINK>', 'Using sentiment analysis, we find that there are two main symptom-emotion clusters (+ and -), and map the landscape of these in the narratives. Pretty pictures abound! https://t.co/gCQAURAgLP', 'We hope that this might be able to help with rapid identification of symptoms, and also of emotional/mental health issues associated with #Covid19. Joint work with @jonotuke and Mark Mackay. @MathsUOA @ACEMathStats https://t.co/ct7N1akalN']",https://arxiv.org/abs/2005.10454,"Social media discussion of COVID-19 provides a rich source of information into how the virus affects people's lives that is qualitatively different from traditional public health datasets. In particular, when individuals self-report their experiences over the course of the virus on social media, it can allow for identification of the emotions each stage of symptoms engenders in the patient. Posts to the Reddit forum r/COVID19Positive contain first-hand accounts from COVID-19 positive patients, giving insight into personal struggles with the virus. These posts often feature a temporal structure indicating the number of days after developing symptoms the text refers to. Using topic modelling and sentiment analysis, we quantify the change in discussion of COVID-19 throughout individuals' experiences for the first 14 days since symptom onset. Discourse on early symptoms such as fever, cough, and sore throat was concentrated towards the beginning of the posts, while language indicating breathing issues peaked around ten days. Some conversation around critical cases was also identified and appeared at a roughly constant rate. We identified two clear clusters of positive and negative emotions associated with the evolution of these symptoms and mapped their relationships. Our results provide a perspective on the patient experience of COVID-19 that complements other medical data streams and can potentially reveal when mental health issues might appear. ","Symptom extraction from the narratives of personal experiences with
  COVID-19 on Reddit"
162,1263888683385552898,722842576479322114,Anthony Aguirre,"['Very excited about a new paper with J. Schindler and D. Safranek in which we propose a preferred generalization of entanglement entropy to multipartite and/or mixed systems, based on our previous work on coarse-grained entropy. <LINK>']",https://arxiv.org/abs/2005.05408,"We study quantum coarse-grained entropy and demonstrate that the gap in entropy between local and global coarse-grainings is a natural generalization of entanglement entropy to mixed states and multipartite systems. This ""quantum correlation entropy"" $S^{\rm QC}$ is additive over independent systems, is invariant under local unitary operations, measures total nonclassical correlations (vanishing on states with strictly classical correlation), and reduces to the entanglement entropy for bipartite pure states. It quantifies how well a quantum system can be understood via local measurements, and ties directly to non-equilibrium thermodynamics, including representing a lower bound on the quantum part of thermodynamic entropy production. We discuss two other measures of nonclassical correlation to which this entropy is equivalent, and argue that together they provide a unique thermodynamically distinguished measure. ",Quantum correlation entropy
163,1263629331605204993,96892174,Aldo Batta,"['Check out this awesome paper led by Ariadna Murguia at @ucsc, where we study the dead of massive stars disappearing without producing a luminous transient. \n\n<LINK>']",https://arxiv.org/abs/2005.10212,"The collapse of a massive star with low angular momentum content is commonly argued to result in the formation of a black hole without an accompanying bright transient. Our goal in this Letter is to understand the flow in and around a newly-formed black hole, involving accretion and rotation, via general relativistic hydrodynamics simulations aimed at studying the conditions under which infalling material can accrete without forming a centrifugally supported structure and, as a result, generate no effective feedback. If the feedback from the black hole is, on the other hand, significant, the collapse would be halted and we suggest that the event is likely to be followed by a bright transient. We find that feedback is only efficient if the specific angular momentum of the infalling material at the innermost stable circular orbit exceeds that of geodesic circular flow at that radius by at least $\approx 20\%$. We use the results of our simulations to constrain the maximal stellar rotation rates of the disappearing massive progenitors PHL293B-LBV and N6946-BH1, and to provide an estimate of the overall rate of disappearing massive stars. We find that about a few percent of single O-type stars with measured rotational velocities are expected to spin below the critical value before collapse and are thus predicted to vanish without a trace. ","On the maximum stellar rotation to form a black hole without an
  accompanying luminous transient"
164,1263499423683883009,160687843,Alexey Melnikov,"['Our new paper on designing better Bell test experiments, where we propose new unintuitive experiments with Bell inequality violations higher than the best known setups. Here we go towards device-independent quantum information processing <LINK> @UniBasel <LINK>']",https://arxiv.org/abs/2005.01697,"Finding optical setups producing measurement results with a targeted probability distribution is hard as a priori the number of possible experimental implementations grows exponentially with the number of modes and the number of devices. To tackle this complexity, we introduce a method combining reinforcement learning and simulated annealing enabling the automated design of optical experiments producing results with the desired probability distributions. We illustrate the relevance of our method by applying it to a probability distribution favouring high violations of the Bell-CHSH inequality. As a result, we propose new unintuitive experiments leading to higher Bell-CHSH inequality violations than the best currently known setups. Our method might positively impact the usefulness of photonic experiments for device-independent quantum information processing. ",Setting up experimental Bell test with reinforcement learning
165,1263062585672437762,38562652,Matthew Roddy,"['Our #acl2020nlp paper: ""Neural Generation of Dialogue Response Timings""\n\nSpoken dialogue systems often generate unnatural response timings. We propose models that generate timings based on the conversational context.\n\nPaper: <LINK>\nCode: <LINK> <LINK>']",http://arxiv.org/abs/2005.09128,"The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions. ",Neural Generation of Dialogue Response Timings
166,1262926406541021184,1047899041311412224,Francois Grondin,"['Preprint of our interspeech paper. We propose to train a NN on pairs of mics with different spacing and acoustic conditions, and use it to estimate a time-freq mask from all the pairs of mics forming the array to perform separation with GEV beamforming\n\n<LINK> <LINK>']",https://arxiv.org/abs/2005.09587,"Distant speech processing is a challenging task, especially when dealing with the cocktail party effect. Sound source separation is thus often required as a preprocessing step prior to speech recognition to improve the signal to distortion ratio (SDR). Recently, a combination of beamforming and speech separation networks have been proposed to improve the target source quality in the direction of arrival of interest. However, with this type of approach, the neural network needs to be trained in advance for a specific microphone array geometry, which limits versatility when adding/removing microphones, or changing the shape of the array. The solution presented in this paper is to train a neural network on pairs of microphones with different spacing and acoustic environmental conditions, and then use this network to estimate a time-frequency mask from all the pairs of microphones forming the array with an arbitrary shape. Using this mask, the target and noise covariance matrices can be estimated, and then used to perform generalized eigenvalue (GEV) beamforming. Results show that the proposed approach improves the SDR from 4.78 dB to 7.69 dB on average, for various microphone array geometries that correspond to commercially available hardware. ","GEV Beamforming Supported by DOA-based Masks Generated on Pairs of
  Microphones"
167,1262912253101256707,2228370313,Damien Teney,"['If you work on visual question answering with the VQA-CP dataset, drop everything and check our latest study ! We show that random predictions with a few heuristics surpass the state of the art on a good fraction of the dataset.\n<LINK> <LINK>']",https://arxiv.org/abs/2005.09241,"Out-of-distribution (OOD) testing is increasingly popular for evaluating a machine learning system's ability to generalize beyond the biases of a training set. OOD benchmarks are designed to present a different joint distribution of data and labels between training and test time. VQA-CP has become the standard OOD benchmark for visual question answering, but we discovered three troubling practices in its current use. First, most published methods rely on explicit knowledge of the construction of the OOD splits. They often rely on ``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the common training answer is 'no'. Second, the OOD test set is used for model selection. Third, a model's in-domain performance is assessed after retraining it on in-domain splits (VQA v2) that exhibit a more balanced distribution of labels. These three practices defeat the objective of evaluating generalization, and put into question the value of methods specifically designed for this dataset. We show that embarrassingly-simple methods, including one that generates answers at random, surpass the state of the art on some question types. We provide short- and long-term solutions to avoid these pitfalls and realize the benefits of OOD evaluation. ","On the Value of Out-of-Distribution Testing: An Example of Goodhart's
  Law"
168,1262855399805300736,14485282,rahuljha,"[""Find how we're bringing summarization to Office using state-of-the-art deep learning models: <LINK>. To collect data for our models, we came up with a unique hierarchical annotation methodology called Artemis, find more details here: <LINK>.""]",https://arxiv.org/abs/2005.02146,"We describe Artemis (Annotation methodology for Rich, Tractable, Extractive, Multi-domain, Indicative Summarization), a novel hierarchical annotation process that produces indicative summaries for documents from multiple domains. Current summarization evaluation datasets are single-domain and focused on a few domains for which naturally occurring summaries can be easily found, such as news and scientific articles. These are not sufficient for training and evaluation of summarization models for use in document management and information retrieval systems, which need to deal with documents from multiple domains. Compared to other annotation methods such as Relative Utility and Pyramid, Artemis is more tractable because judges don't need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents. ","Artemis: A Novel Annotation Methodology for Indicative Single Document
  Summarization"
169,1262825372338528257,221381860,Diogo Pacheco,"['Excited about this new work ""Neutral Bots Reveal Political Bias on Social Media"" <LINK>\n\nWe found Partisan accounts, tend to receive more followers, follow more automated accounts, are exposed to more low-credibility content, and find themselves in echo chambers. <LINK>']",https://arxiv.org/abs/2005.08141,"Social media platforms attempting to curb abuse and misinformation have been accused of political bias. We deploy neutral social bots who start following different news sources on Twitter, and track them to probe distinct biases emerging from platform mechanisms versus user interactions. We find no strong or consistent evidence of political bias in the news feed. Despite this, the news and information to which U.S. Twitter users are exposed depend strongly on the political leaning of their early connections. The interactions of conservative accounts are skewed toward the right, whereas liberal accounts are exposed to moderate content shifting their experience toward the political center. Partisan accounts, especially conservative ones, tend to receive more followers and follow more automated accounts. Conservative accounts also find themselves in denser communities and are exposed to more low-credibility content. ",Neutral bots probe political bias on social media
170,1262726253733388294,45179715,Phil Wiseman,"[""It's paper day - myself and a group of @theDESurvey colleagues have submitted a paper about some weird star explosions to @RAS_Journals! You can find the arXiv pre-print here:\n<LINK>\nWhat have we found? Read on... 1/n <LINK>"", 'During the @theDESurvey, Miika Pursiainen from @sotonastro discovered 72 weird flashes. They looked a lot like supernovae - dying stars - but faded away much faster. https://t.co/MdVyvO42DJ', ""After the survey finished, we went back to the data and found a few more - 106 in total. Some were as bright as the very brightest 'superluminous' supernovae (SNe), while some were fainter, like 'normal' SNe from the core-collapse of massive stars."", 'Because they fade away so fast, it was almost impossible to point any larger telescopes, like the @ESO Very Large Telescope, at these Rapidly Evolving Transients quickly enough to get any useful information about what they might be ... https://t.co/JBJYPMg2Qb', ""But! All was not lost. To measure the size, age and expansion rate of our Universe, @theDESurvey attempted to measure 'redshift' - the stretching of light due to the Universe expanding - of every SN. That meant taking a spectrum of the host galaxy, where the SN lived &amp; exploded. https://t.co/Flz0Hqt9fA"", 'As well as redshift, those spectra also contain information about the galaxy. Along with images at different wavelengths, we were able to work out how heavy each RET host galaxy is as well as the rate at which each galaxy is forming stars! https://t.co/1tXTHYm5nO', 'We could also, sorry physicists, stray into the realms of chemistry. Using different features in the spectra, we can measure abundances of different elements. This allows us to know how enriched, or evolves, the galaxy is. This is important for explosions! https://t.co/QVURkCoNlQ', 'We know from previous work that some REALLY big explosions, like superluminous SNe and gamma-ray bursts, need stars to be very big and have very few heavy elements*\n\n*heavy elements in astronomy = anything heavier than helium', 'A high star-formation rate means there are more likely to be more very big stars in a galaxy, and a low heavy element content means they might be more likely to blow up as extreme events. Less star formation and more heavy elements would suppress this rate. https://t.co/Vf7XpIvFui', ""So what did we see? \n\nThe RET host galaxies DO show more star formation than 'normal' core-collapse SNe, but not as much as in the host galaxies of superluminous SNe or gamma-ray bursts."", ""And the RET host galaxies DO show lower metallicity than 'normal' core-collapse SNe, but not as low as the hosts of superluminous SNe or gamma-ray bursts. Intriguing! What could that mean? https://t.co/KQcK9vACOw"", ""We think it means something about the type of stars that are exploding as RETs. They are extreme. They are big, they are not very enriched. But they are not quite big enough to explode as superluminous SNe or gamma-ray bursts. Could they be 'failed' versions of those explosions?"", ""We'll need to wait for bigger samples to come along to find out - looking at you @VRubinObs! https://t.co/p9eU53lnce""]",https://arxiv.org/abs/2005.08653,"Rapidly evolving transients (RETs), also termed fast blue optical transients (FBOTs), are a distinct class of astrophysical event. They are characterised by lightcurves that decline much faster than common classes supernovae (SNe), span vast ranges in peak luminosity and can be seen to redshifts greater than 1. Their evolution on fast timescales has hindered high quality follow-up observations, and thus their origin and explosion/emission mechanism remains unexplained. In this paper we define the largest sample of RETs to date, comprising 106 objects from the Dark Energy Survey, and perform the most comprehensive analysis of RET host galaxies. Using deep-stacked photometry and emission-lines from OzDES spectroscopy, we derive stellar masses and star-formation rates (SFRs) for 49 host galaxies, and metallicities for 37. We find that RETs explode exclusively in star-forming galaxies and are thus likely associated with massive stars. Comparing RET hosts to samples of host galaxies of other explosive transients as well as field galaxies, we find that RETs prefer galaxies with high specific SFRs, indicating a link to young stellar populations, similar to stripped-envelope SNe. RET hosts appear to show a lack of chemical enrichment, their metallicities akin to long duration gamma-ray bursts and superluminous SN host galaxies. There are no clear relationships between properties of the host galaxies and the peak magnitudes or decline rates of the transients themselves. ","The Host Galaxies of Rapidly Evolving Transients in the Dark Energy
  Survey"
171,1262567404883894275,741340014878064640,🎃 Adina 💀 Feinstein 🎃,"[""🚨🚨New ArXiv posting🚨🚨\n\nPretty stoked - we've developed a neural network to find flares on 3200 young stars and evaluate spot coverage and flare rates!\n\nHere's the link: <LINK>\n\nAnd below find the TL;DR 👇👇 <LINK>"", ""We created a NEW convolutional neural network with @TensorFlow to find flares in @NASA_TESS light curves. It's called stella! It was trained, validated, and tested on @M_N_Guenther's Sectors 1 &amp; 2 flare catalog\n\nstella is open-source and can be found here: https://t.co/CfTldjBMWf https://t.co/DMelJ925Pg"", 'The CNN is used as a sliding box detector. It assigns a probably to each cadence to if it is or is not part of a flare and creates a probability light curve. It takes a second or so to make the probability light curve for a single observation!', 'The sample -- We used TESS 2-min light curves from Sectors 1-20 and assign membership to 3200 to nearby young moving\rgroups, young open clusters, OB associations, and star\nforming regions using Banyan-Sigma, resulting in high-probability young stars with ages &lt; 800 Myr https://t.co/mDnLlxAzzs', ""Here are some example light curves where we found flares! We don't remove the spot variability. Yellow = low probability of flare; purple = high probability. It works on a range of rotation periods and doesn't get confused between spots and flares https://t.co/edWfg7QpjX"", ""First question -- what is the relationship between spots and flares?\n\n We measured rotation periods for 1500 stars and plot flare number as a function of spot phase.\n\nIt's flat! Real flat. Only a 3% difference in num of flares between the spotted and less spotted hemispheres https://t.co/wEtiKAXEX6"", ""This small phase dependence means the stellar active regions are constant along our line of sight!\n\nThis doesn't account for circumpolar or inclination dependencies. But one thing's for sure -- they're very different from the Sun!"", 'But really this whole study was for naught. Matt Groening showed in the Futurama episode ""A Farewell to Arms"" that a cataclysmic sunspot cycle would bring about a solar flare that would literally break Mars in half. He knew spots == flares. Such accuracy! https://t.co/KvKXpYgHHB', 'We also looked at Gaia color vs. rotation period colored by the maximum flare amplitude found on that star. There are lots of high amplitude flares on Bp-Rp&gt;2 (~4000K) and not so high amplitude flares on hotter stars. Neato! https://t.co/uTCSDmcTIb', ""Second question -- what can we uncover about flare rates? \n\nLooking at all temps, we see clear evolution in rates and amplitudes for hot stars younger and older than 50 Myr years old. While for cool stars they're relatively consistent across the first 800 Myr https://t.co/kTuKG9dFEq"", 'We also split the sample up by effective temperature (purple is &lt;= 4000K, green is &gt; 4000K). Cool stars dominate flare rates and the high amplitude regime across allllllllll age bins. All of them! Cool stars are ~flarey bois~ https://t.co/jbfPwpbA3L', 'Last question -- what do these flare rates mean for young exoplanets? \n\nLess atmosphere! We modified the equations of Owen &amp; Wu (2017) to account for flares. When flares are present in the first 200 Myr and 1000 Myr, the planet loses 4% and 7% more atm, respectively https://t.co/aSy8VYq1xs', ""That's it! There are way more details in the paper so definitely check it out \n👉 https://t.co/IAfVI0HpG9👈\n\nAnd a huge shout out to my awesome advisor @benmontet and collaborators @megaparsec808 @iamstarnord @M_N_Guenther #JacobBean @gully_ &amp; @JoshuaSchlieder https://t.co/1ANLhNbymQ""]",http://arxiv.org/abs/2005.07710,"All-sky photometric time-series missions have allowed for the monitoring of thousands of young ($t_{\rm age} < 800$Myr) to understand the evolution of stellar activity. Here we developed a convolutional neural network (CNN), $\texttt{stella}$, specifically trained to find flares in $\textit{Transiting Exoplanet Survey Satellite}$ ($\textit{TESS}$) short-cadence data. We applied the network to 3200 young stars to evaluate flare rates as a function of age and spectral type. The CNN takes a few seconds to identify flares on a single light curve. We also measured rotation periods for 1500 of our targets and find that flares of all amplitudes are present across all spot phases, suggesting high spot coverage across the entire surface. Additionally, flare rates and amplitudes decrease for stars $t_{\rm age} > 50$Myr across all temperatures $T_{\rm eff} \geq 4000$K, while stars from $2300 \leq T_{\rm eff} < 4000$K show no evolution across 800 Myr. Stars of $T_{\rm eff} \leq 4000$K also show higher flare rates and amplitudes across all ages. We investigate the effects of high flare rates on photoevaporative atmospheric mass loss for young planets. In the presence of flares, planets lose 4-7% more atmosphere over the first 1 Gyr. $\texttt{stella}$ is an open-source Python tool-kit hosted on GitHub and PyPI. ","Flare Statistics for Young Stars from a Convolutional Neural Network
  Analysis of $\textit{TESS}$ Data"
172,1262445769514254338,3051993112,Anna Wright,"['New paper day! We use the Romulus25 cosmological simulation to study the formation and evolution of 100+ ultra-diffuse galaxies (UDGs) in the field: <LINK>\n#nbodyshopgotchu', 'Shout-out to co-authors Michael Tremmel (@MichaelTremmel), Alyson Brooks, Ferah Munshi (@fdmtweets), Daisuke Nagai, Ray Sharma (@RaySSharma), and Tom Quinn.\nHere are some of our results:', 'We search Romulus25 – one of the highest resolution volumes ever run – for isolated UDGs. That is, large, low surface brightness (LSB) galaxies that live far away from anything massive. Here are a few of our galaxies + SB profiles &amp; fits – UDGs on the right, non-UDGs on the left: https://t.co/wkdemCZ0Ci', 'Other than their large sizes and LSB, present-day UDGs aren’t really that weird. They have normal star formation rates (see below), HI masses, colors, and virial masses. They’re also pretty common: ~20% of all field galaxies with Mstar = 10^7-9 Msol are UDGs. https://t.co/l0OeHNpaGn', 'What distinguishes UDGs is their evolution! Higher mass dwarf galaxies don’t typically fade much after z~1, but UDGs become increasingly LSB. Same with size: non-UDGs don’t change much over time, but UDGs start growing and don’t stop. https://t.co/xR7cS7m4JD', 'Part of the reason for this seems to be that UDGs evolve to lower central star formation rates. As their central stars age, the cores of the galaxies get fainter, similar to what we see in galaxy cluster simulations (RomulusC; https://t.co/9oWZA88Trv) https://t.co/b8xlA1b7D4', 'However, field UDGs have typical star formation rates for their stellar masses, so what’s really happening is that star-forming gas is spreading out! This results in bright, new stars forming at larger radii, leading to an increase in size over time. But why?', 'It turns out that most of our field UDGs had early mergers (z&gt;1) that temporarily spun them up and redistributed their star formation. In many UDGs, we see bursts of star formation at high radii for billions of years post-merger: https://t.co/UTGayIlwJV']",https://arxiv.org/abs/2005.07634,"We use the \textsc{Romulus25} cosmological simulation volume to identify the largest-ever simulated sample of {\it field} ultra-diffuse galaxies (UDGs). At $z=0$, we find that isolated UDGs have average star formation rates, colors, and virial masses for their stellar masses and environment. UDGs have moderately elevated HI masses, being 70\% (300\%) more HI-rich than typical isolated dwarf galaxies at luminosities brighter (fainter) than M$_\mathrm{B}$=-14. However, UDGs are consistent with the general isolated dwarf galaxy population and make up $\sim$20\% of all field galaxies with 10$^7$<M$_\star$/M$_\odot$<10$^{9}$. The HI masses, effective radii, and overall appearances of our UDGs are consistent with existing observations of field UDGs, but we predict that many isolated UDGs have been missed by current surveys. Despite their isolation at $z=0$, the UDGs in our sample are the products of major mergers. Mergers are no more common in UDG than non-UDG progenitors, but mergers that create UDGs tend to happen earlier - almost never occurring after $z=1$, produce a temporary boost in spin, and cause star formation to be redistributed to the outskirts of galaxies, resulting in lower central star formation rates. The centers of the galaxies fade as their central stellar populations age, but their global star formation rates are maintained through bursts of star formation at larger radii, producing steeper negative g-r color gradients. This formation channel is unique relative to other proposals for UDG formation in isolated galaxies, demonstrating that UDGs can potentially be formed through multiple mechanisms. ",The Formation of Isolated Ultra-Diffuse Galaxies in Romulus25
173,1262268308352630785,1212606587644178432,Romero-Isart Group,"['In #HarryPotter and the philosopher’s stone, Prof. Albus Dumbledore uses his magic deluminator to turn on and off, from his distant position, each streetlight in Private Drive one by one. In <LINK> we propose a way to implement a “qubit deluminator” <LINK>', 'That is, we propose to remotely address quantum emitters with sub-wavelength resolution using self-focusing chirped pulses propagating near a bandgap of a structured reservoir. Work done in collaboration with @iqoqi fellow @jjgarciaripoll https://t.co/7oFKK3eHGv']",https://arxiv.org/abs/2005.07506,We propose to use chirped pulses propagating near a bandgap to remotely address quantum emitters. We introduce a particular family of chirped pulses that dynamically self-compress to sub-wavelength spot sizes during their evolution in a medium with a quadratic dispersion relation. We analytically describe how the compression distance and width of the pulse can be tuned through its initial parameters. We show that the interaction of such pulses with a quantum emitter is highly sensitive to its position due to effective Landau-Zener processes induced by the pulse chirping. Our results propose pulse engineering as a powerful control and probing tool in the field of quantum emitters coupled to structured reservoirs. ,Remote Individual Addressing of Quantum Emitters with Chirped Pulses
174,1262196719791431681,1109864785355702274,Jared Goldberg,"['In the saga of ""Massive Stars and Explosions: What can we know? Can we know things? Let\'s find out!"" My most recent paper with Lars Bildsten was accepted and is now on arXiv: <LINK>. 17 explosions selected from &gt;2000 stellar models vs 5 observed supernovae.', ""The twitter summary: Sometimes, different stars exploding can look the same. If we see an explosion and already know how big the star was, then we're in business. Otherwise, it might be harder than we thought to tell just what went into that explosion.""]",https://arxiv.org/abs/2005.07290,"Using Modules for Experiments in Stellar Astrophysics (MESA)+STELLA, we show that very different physical models can adequately reproduce a specific observed Type II-Plateau Supernova (SN). We consider SN2004A, SN2004et, SN2009ib, SN2017eaw, and SN2017gmr, Nickel-rich ($M_\mathrm{Ni}>0.03M_\odot$) events with bolometric lightcurves and a well-sampled decline from the plateau. These events also have constraints on the progenitor radius, via a progenitor image, or, in the case of SN2017gmr, a radius from fitting shock-cooling models. In general, many explosions spanning the parameter space of progenitors can yield excellent lightcurve and Fe line velocity agreement, demonstrating the success of scaling laws in motivating models which match plateau properties for a given radius and highlighting the degeneracy between plateau luminosity and velocity in models and observed events, which can span over 50% in ejecta mass, radius, and explosion energy. This can help explain disagreements in explosion properties reported for the same event using different model calculations. Our calculations yield explosion properties when combined with pre-explosion progenitor radius measurements or a robust understanding of the outermost $<0.1\,M_\odot$ of material that quantifies the progenitor radius from SN observations a few days after explosion. ","The Value of Progenitor Radius Measurements for Explosion Modeling of
  Type II-Plateau Supernovae"
175,1261353563114156032,837079877501218817,Kaden Hazzard,"['Theory &amp; expt for ultracold magnetic superconductors <LINK> Magnetism usually destroys SC, but they can coexist in an elusive “FFLO” state. Coupled 1D tubes are great for FFLO. We find thry &amp; expt agree in broad strokes, but expts always look surprisingly 1D-like', 'some notes:\n\nFFLO = Fulde-Ferrell-Larkin-Ovchinnokov after the discoveres\n\nthe theory was BdG = Bogoliubov-de Gennes\n\nIn the experiments, these superconductors are formed by neutral atoms -- it\'s mass they\'re conducting in such a super way, so you might prefer ""superfluid"".', 'I think one of the key conclusions (besides the utility of BdG for treating much of this and aiding the search for FFLO) is that in some relevant cases, a theory starting from the 1D limit and perturbing around this may offer a better approach', 'The RG treatments of Zhao &amp; Liu PRA 78, 063605 (2008) or recently-used ""variational Bethe ansatz"" approaches (I\'ll have to dig out a reference) would be strong candidates.', '@excitedstoat Conceptually yes: PRL 99, 250403 (2007) by my former boss (&amp; our mutual friends) does BdG of this. The new things @quantumbhu:\n(1) The PRL calculated for one inter-tube tunneling value. None of the expts are at that tube coupling, so need new calculations to compare w/ expt.', ""@excitedstoat @quantumbhu (2) They calculated phase boundaries, but did not calculate the full density profiles. Moreover, to get critical radii separating different phases (measured in experiment), you need the full density profiles (they're what tell you the central chemical potential in LDA)"", '@excitedstoat @quantumbhu density profiles can be more challenging to calculate than boundaries.\n\n(3) The normal polarized-FFLO phase boundary in the PRL differs from ours, which we believe is because they used a simple ansatz rather than full self-consistency. The NP is much smaller than they predicted.', '@excitedstoat @quantumbhu (4) There\'s new experimental data! \n\n(5) And lots of comparisons/analysis of these things.\n\nSo ""do BdG on coupled 1D tubes"" isn\'t groundbreaking. But there\'s a large amount of serious work in order to check how well this works compared to experiment.', '@excitedstoat @quantumbhu ah, I see. In my followup to the original post, I mentioned Erhai Zhao and Vincent Liu\'s nice work. Turning it into a quantitative calculation would be do-able, but I don\'t think it\'s been done. I think one could also employ these ""variational Bethe-ansatz"" I\'ve read about,', ""@excitedstoat @quantumbhu but I don't think it's been done. There is some literature that I should read, but I don't think it's been explicitly worked out. [ Well, I'm certain it hasn't for the detailed parameters studied in experiment! That would be quite the coincidence. :) ]"", '@excitedstoat @quantumbhu assume BA wavefunction for non-integrable system; minimize energy. (I have no idea how to carry it out.)\n\nBut some version of it has been applied to one of your favorite systems! https://t.co/zldcOXIc2v', '@excitedstoat @quantumbhu I mean, just taking the 1D solution and *not( optimizing does a not-bad job for the phase boundaries of coupled tubes, even at pretty sizable inter-tube coupling.']",https://arxiv.org/abs/2005.01826,"Motivated by a recent experiment [Revelle et al. Phys. Rev. Lett. 117, 235301 (2016)] that characterized the one- to three-dimensional crossover in a spin-imbalanced ultracold gas of $^6$Li atoms trapped in a two-dimensional array of tunnel-coupled tubes, we calculate the phase diagram for this system using Hartree-Fock Bogoliubov-de Gennes mean-field theory, and compare the results with experimental data. Mean-field theory predicts fully spin-polarized normal, partially spin-polarized normal, spin-polarized superfluid, and spin-balanced superfluid phases in a homogeneous system. We use the local density approximation to obtain density profiles of the gas in a harmonic trap. We compare these calculations with experimental measurements in Revelle {\em et al.} as well as previously unpublished data. Our calculations qualitatively agree with experimentally-measured densities and coordinates of the phase boundaries in the trap, and quantitatively agree with experimental measurements at moderate-to-large polarizations. Our calculations also reproduce the experimentally-observed universal scaling of the phase boundaries for different scattering lengths at a fixed value of scaled inter-tube tunneling. However, our calculations have quantitative differences with experimental measurements at low polarization, and fail to capture important features of the one- to three-dimensional crossover observed in experiments. These suggest the important role of physics beyond-mean-field theory in the experiments. We expect that our numerical results will aid future experiments in narrowing the search for the FFLO phase. ","Spin-imbalanced ultracold Fermi gases in a two-dimensional array of
  tubes"
176,1260981661073911813,169399653,Jens Peter Andersen,"['1/8 Our new preprint looks at the widening gender gap in research on #COVID19. We find women in first author position publish 23% less than expected.\n<LINK> (submitted to @eLife) #gendergap #genderinSTEM', '2/8 Paper is published together with @mathiaswullum, Resa Lewiss, @NickiSimone4, @reshmajagsi', '3/8 We compare COVID-19 papers (which are all published in 2020) to papers from 2019, published in the same journals. Taking the journal into account, women on average publish 16% less than they did in 2019. As first authors, they publish 23% less. https://t.co/WNli8IBx2k', '4/8 As first authors tend to be more early career researchers than in other positions, this raises concerns for future career possibilities for women in medical research.', '5/8 Others have reported similar stories, e.g. editors seeing fewer submissions from women: \nhttps://t.co/daMdsQfEpA', '6/8 https://t.co/aCPBhQcWoX', '7/8 https://t.co/5S9884cgzG', '8/8 CC #TIMESUPHC @TIMESUPHC @arghavan_salles @choo_ek']",https://arxiv.org/abs/2005.06303,"The COVID-19 pandemic has resulted in school closures and distancing requirements that have disrupted both work and family life for many. Concerns exist that these disruptions caused by the pandemic may not have influenced men and women researchers equally. Many medical journals have published papers on the pandemic, which were generated by researchers facing the challenges of these disruptions. Here we report the results of an analysis that compared the gender distribution of authors on 1,893 medical papers related to the pandemic with that on papers published in the same journals in 2019, for papers with first authors and last authors from the United States. Using mixed-effects regression models, we estimated that the proportion of COVID-19 papers with a woman first author was 19% lower than that for papers published in the same journals in 2019, while our comparisons for last authors and overall proportion of women authors per paper were inconclusive. A closer examination suggested that women's representation as first authors of COVID-19 research was particularly low for papers published in March and April 2020. Our findings are consistent with the idea that the research productivity of women, especially early-career women, has been affected more than the research productivity of men. ","Meta-Research: COVID-19 medical papers have fewer women first authors
  than expected"
177,1260633238302646272,1578756350,Laura V. Sales,"['How galaxies accrete into clusters ? We find ~38% come in as part of groups in Illustris. Why we care?: These groups allow mergers to happen even within clusters. --&gt; formation path for S0s and compact dwarfs! on Arxiv today: <LINK> <LINK>', '@SheerPriya Awesome! Priya send me the link to paper, if possible, so we can add a reference to it after referee process. Thanks!!!']",https://arxiv.org/abs/2005.05344,"We study the role of group infall in the assembly and dynamics of galaxy clusters in $\Lambda$CDM. We select $10$ clusters with virial mass $M_{\rm 200} \sim 10^{14} \, M_{\odot}$ from the cosmological hydrodynamical simulation Illustris and follow their galaxies with stellar mass $M_{\star} \geq 1.5 \times 10^8 \, M_{\odot}$. A median of $\sim 38\%$ of surviving galaxies at $z=0$ are accreted as part of groups and did not infall directly from the field, albeit with significant cluster-to-cluster scatter. The evolution of these galaxy associations is quick, with observational signatures of their common origin eroding rapidly in $1$-$3$ Gyr after infall. Substructure plays a dominant role in fostering the conditions for galaxy mergers to happen, even within the cluster environment. Integrated over time, we identify (per cluster) an average of $17 \pm 6$ mergers that occur in infalling galaxy associations, of which $7 \pm 3$ occur well within the virial radius of their cluster hosts. The number of mergers show large dispersion from cluster to cluster, with our most massive system having $42$ mergers above our mass cut-off. These mergers, which are typically gas rich for dwarfs and a combination of gas rich and gas poor for $M_{\star} \sim 10^{11} \, M_{\odot}$, may contribute significantly within $\Lambda$CDM to the formation of specific morphologies, such as lenticulars (S0) and blue compact dwarfs in groups and clusters. ",Accretion of Galaxy Groups into Galaxy Clusters
178,1260377509092302849,927909431689596929,Matthew Mizuhara,"['Excited to announce a new paper with collaborator Georgi Medvedev:\n<LINK>\n\nWe present a method to study clusters of oscillators in the Kuramoto model with inertia. Through this framework, we can decouple the clusters and create chimera states! <LINK>', '@LongFormMath Thanks!! :)', '@jasnyder610 Thanks! Yes, I would also be really interested in learning more about your work as well!']",http://arxiv.org/abs/2005.05367,"The Kuramoto model of coupled phase oscillators with inertia on Erdos-Renyi graphs is analyzed in this work. For a system with intrinsic frequencies sampled from a bimodal distribution we identify a variety of two cluster patterns and study their stability. To this end, we decompose the description of the cluster dynamics into two systems: one governing the (macro) dynamics of the centers of mass of the two clusters and the second governing the (micro) dynamics of individual oscillators inside each cluster. The former is a low-dimensional ODE whereas the latter is a system of two coupled Vlasov PDEs. Stability of the cluster dynamics depends on the stability of the low-dimensional group motion and on coherence of the oscillators in each group. We show that the loss of coherence in one of the clusters leads to the loss of stability of a two-cluster state and to formation of chimera states. The analysis of this paper can be generalized to cover states with more than two clusters and to coupled systems on W-random graphs. Our results apply to a model of a power grid with fluctuating sources. ","Stability of clusters in the second-order Kuramoto model on random
  graphs"
179,1260375448577388545,3229862700,Aravind Rajeswaran,"['Can model-based RL be effective for offline RL? Absolutely! We develop a new framework and algorithm called MOReL that has both strong theoretical guarantees as well as SOTA results in widely studied benchmarks.  (1/n)\n<LINK>', 'Key idea is to learn a pessimistic MDP that partitions the state space into ""known"" and ""unknown"" regions based on learned model confidence. It forces transition to a low reward absorbing state from ""unknown"" regions, providing regularizing effect.', 'We derive information theoretic lower bounds for sub-optimality of any RL algorithm. We show MOReL matches this lower bound upto log factors, effectively making it minimax optimal. (3/n)', 'We develop a practical variant of MOReL that achieves SOTA on 12 out of 20 environments. Next best algorithm obtains SOTA on 4 out of 20. (4/n)', ""Algorithmic approach largely inspired by @ShamKakade6's metric E3 and our recent MBRL work in the online RL setting (https://t.co/kxNsTLedRr. (5/n)"", 'Joint work with awesome collaborators: Rahul Kidambi, @PNetrapalli, and @thorstenjoachim (6/6)', ""@nanjiang_cs Thanks Nan, these are really cool works! We'll include them in the next revision. Our view of pes-MDP was inspired by E3, but it doesn't exactly treat it pessimistically. Maybe we should think of a cool descriptive name for it. P-MDP, R-Min, C-Max all sound great! :)"", '@vvanirudh @nanjiang_cs Thanks @vvanirudh and thanks for CMax pointer. Happy to chat! I think MBRL, offline RL, and planning community should all move closer together :)']",https://arxiv.org/abs/2005.05951,"In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL. ",MOReL : Model-Based Offline Reinforcement Learning
180,1260219028066512899,1015270156501647362,Karan Molaverdikhani,"['In 2019, we published an extensive study on the role of vertical mixing: <LINK>\n\nShulyak et al. extend this to investigate the ""Stellar impact on disequilibrium chemistry and on observed spectra of hot Jupiter atmospheres"" <LINK> 💫 <LINK>']",https://arxiv.org/abs/1908.09847,"Almost all planetary atmospheres are affected by disequilibrium chemical processes. In this paper we introduce our recently developed Chemical Kinetic Model (\texttt{ChemKM}). We show that the results of our HD189733b model are in good agreement with previously published results, except at $\mu$bar regime, where molecular diffusion and photochemistry are the dominant processes. We thus recommend careful consideration of these processes when abundances at the top of the atmosphere are desired. We also propose a new metric for a quantitative measure of quenching levels. By applying this metric, we find that quenching pressure decreases with the effective temperature of planets, but it also varies significantly with other atmospheric parameters such as [Fe/H], log(g), and C/O. In addition, we find that the ""Methane Valley"", a region between 800 and 1500K where above a certain C/O threshold value a greater chance of CH$_4$ detection is expected, still exists after including the vertical mixing. The first robust CH$_4$ detection on an irradiated planet (HD102195b) places this object within this region; supporting our prediction. We also investigate the detectability of disequilibrium spectral fingerprints by JWST, and suggest focusing on the targets with T$_{eff}$ between 1000 and 1800K, orbiting around M-dwarfs, having low surface gravity but high metallicity and a C/O ratio value around unity. Finally, constructing Spitzer color-maps suggests that the main two color-populations are largely insensitive to the vertical mixing. Therefore any deviation of observational points from these populations are likely due to the presence of clouds and not disequilibrium processes. However, some cold planets (T$_{eff}<$900K) with very low C/O ratios ($<$0.25) show significant deviations; making these planets interesting cases for further investigation. ","From cold to hot irradiated gaseous exoplanets: Fingerprints of chemical
  disequilibrium in atmospheric spectra"
181,1260151580667662336,1360175358,Michele Starnini,"['Our last preprint about the prediction of new collaborations in multiplex scientific interaction networks is out! We propose a new metric by generalizing the Adamic-Adar method to multiplex graphs, that encode diverse forms of scientific interactions 1/3\n <LINK> <LINK>', 'The Multiplex Adamic Adar metric outperforms\nother single-layered, similarity-based scores. We found that scientific credit, represented by citations among authors, and common interests, measured by the usage of common keywords in papers, can be predictive of new collaborations. https://t.co/f6Ti0Asn09', ""Many thanks to @ISI_Fondazione's colleagues @danielapaolotti @cosnet_bifi @SrAleta and Marta Tuninetti. Comments welcome!""]",https://arxiv.org/abs/2005.04432,"Link prediction algorithms can help to understand the structure and dynamics of scientific collaborations and the evolution of Science. However, available algorithms based on similarity between nodes of collaboration networks are bounded by the limited amount of links present in these networks. In this work, we reduce the latter intrinsic limitation by generalizing the Adamic-Adar method to multiplex networks composed by an arbitrary number of layers, that encode diverse forms of scientific interactions. We show that the new metric outperforms other single-layered, similarity-based scores and that scientific credit, represented by citations, and common interests, measured by the usage of common keywords, can be predictive of new collaborations. Our work paves the way for a deeper understanding of the dynamics driving scientific collaborations, and provides a new algorithm for link prediction in multiplex networks that can be applied to a plethora of systems. ","Prediction of scientific collaborations through multiplex interaction
  networks"
182,1260098137127075840,1036512677051412480,Claudio Gallicchio,"['We study super-simple #GraphNeuralNetworks based on Ring #ReservoirComputing, where all aspects of stochasticity in the initialization are removed. \n<LINK> <LINK>']",https://arxiv.org/abs/2005.05294,"Machine Learning for graphs is nowadays a research topic of consolidated relevance. Common approaches in the field typically resort to complex deep neural network architectures and demanding training algorithms, highlighting the need for more efficient solutions. The class of Reservoir Computing (RC) models can play an important role in this context, enabling to develop fruitful graph embeddings through untrained recursive architectures. In this paper, we study progressive simplifications to the design strategy of RC neural networks for graphs. Our core proposal is based on shaping the organization of the hidden neurons to follow a ring topology. Experimental results on graph classification tasks indicate that ring-reservoirs architectures enable particularly effective network configurations, showing consistent advantages in terms of predictive performance. ",Ring Reservoir Neural Networks for Graphs
183,1260010161063616514,251682258,Dr. Knicole Colón,"['I am so excited to share our new paper on the inflated sub-Saturn KELT-11b! We used Hubble, Spitzer, and TESS data and find that KELT-11b has a low-amplitude water feature with an unusual shape, suggestive of a sub-solar atmospheric water abundance. <LINK> (1/2) <LINK>', 'I am so grateful for all the work our team put into this project, a team which included @lkreidberg, Mike Line, @luis_wel, @exomadhu, @tgbeatty, @PatrickTamburo, @kevinbstevenson, Avi Mandell, @Astro_JRod, @mrtommyb, and many more! (2/2) https://t.co/B7PmzVUSpX']",https://arxiv.org/abs/2005.05153,"We present an optical-to-infrared transmission spectrum of the inflated sub-Saturn KELT-11b measured with the Transiting Exoplanet Survey Satellite (TESS), the Hubble Space Telescope (HST) Wide Field Camera 3 G141 spectroscopic grism, and the Spitzer Space Telescope (Spitzer) at 3.6 $\mu$m, in addition to a Spitzer 4.5 $\mu$m secondary eclipse. The precise HST transmission spectrum notably reveals a low-amplitude water feature with an unusual shape. Based on free retrieval analyses with varying molecular abundances, we find strong evidence for water absorption. Depending on model assumptions, we also find tentative evidence for other absorbers (HCN, TiO, and AlO). The retrieved water abundance is generally $\lesssim 0.1\times$ solar (0.001--0.7$\times$ solar over a range of model assumptions), several orders of magnitude lower than expected from planet formation models based on the solar system metallicity trend. We also consider chemical equilibrium and self-consistent 1D radiative-convective equilibrium model fits and find they too prefer low metallicities ($[M/H] \lesssim -2$, consistent with the free retrieval results). However, all the retrievals should be interpreted with some caution since they either require additional absorbers that are far out of chemical equilibrium to explain the shape of the spectrum or are simply poor fits to the data. Finally, we find the Spitzer secondary eclipse is indicative of full heat redistribution from KELT-11b's dayside to nightside, assuming a clear dayside. These potentially unusual results for KELT-11b's composition are suggestive of new challenges on the horizon for atmosphere and formation models in the face of increasingly precise measurements of exoplanet spectra. ","An Unusual Transmission Spectrum for the Sub-Saturn KELT-11b Suggestive
  of a Sub-Solar Water Abundance"
184,1259863998234857473,401811181,Walter Tangarife,"['Our paper ""Origin of Sterile Neutrino Dark Matter via Vector Secret Neutrino Interactions"" (with @kjkelly_physics, Manibrata Sen, and Yue Zhang) <LINK>\nWe study the production of Sterile Neutrino Dark Matter: dark matter that mixes a tiny bit with neutrinos', 'These sterile neutrinos can easily be produced via neutrino oscillations in the early Universe. If neutrinos can ""talk"" to one another significantly more strongly than what is predicted by the Standard Model, then the oscillation to sterile neutrinos is enhanced', 'The focus of this paper is the case when active neutrinos interact via vector mediators (something like a massive photon). \nI a previous work, the case of a scalar mediator was also studied (https://t.co/EbwblWwQzR)']",https://arxiv.org/abs/2005.03681,"Secret neutrino interactions can play an essential role in the origin of dark matter. We present an anatomy of production mechanisms for sterile neutrino dark matter, a keV-scale gauge-singlet fermion that mixes with active neutrinos, in the presence of a new vector boson mediating secret interactions among active neutrinos. We identify three regimes of the vector boson's mass and coupling where it makes distinct impact on dark matter production through the dispersion relations and/or scattering rates. We also analyze models with gauged $L_\mu-L_\tau$ and $B-L$ numbers which have a similar dark matter cosmology but different vector boson phenomenology. We derive the parameter space in these models where the observed relic abundance is produced for sterile neutrino dark matter. They serve as well-motivated target for the upcoming experimental searches. ","Origin of Sterile Neutrino Dark Matter via Vector Secret Neutrino
  Interactions"
185,1259587618104213504,1187500436678266880,Frank Yue Wu,"['We just released our new paper: A Finite Time Analysis of Two Time-Scale Actor Critic Methods (<LINK>). We for the first time show that two time-scale actor-critic methods can find an eps-stationary point within eps^{-2.5} sample complexity.', 'Joint work with @WeitongZhang @PanXu_CS  @QuanquanGu']",https://arxiv.org/abs/2005.01350,"Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature. However, the non-asymptotic convergence and finite sample complexity of actor-critic methods are largely open. In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find a first-order stationary point (i.e., $\|\nabla J(\boldsymbol{\theta})\|_2^2 \le \epsilon$) of the non-concave performance function $J(\boldsymbol{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample complexity. To the best of our knowledge, this is the first work providing finite-time analysis and sample complexity bound for two time-scale actor-critic methods. ",A Finite Time Analysis of Two Time-Scale Actor Critic Methods
186,1259009592287277056,90131577,Noam Slonim 🟢,"['Can we automatically find an opinion article that specifically counters an article we just read? Check out a new ACL-2020 paper, coming out from our #ProjectDebater team. And we also release 3.6k (!) recorded debate speeches! Congrats to the authors!\n\n<LINK>']",https://arxiv.org/abs/2005.01157,"An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in ""echo chambers"" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns -- that of detecting articles that most effectively counter the arguments -- and not just the stance -- made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research. ",Out of the Echo Chamber: Detecting Countering Debate Speeches
187,1258829209306267654,73090248,Prof. Danushka Bollegala,"['How can we train a dialogue engine using multiple response models? In this paper, we propose a method that does is agnostic to the response models used by the agents and also do not require ratings for the constituent agents. <LINK> <LINK>']",https://arxiv.org/abs/2005.03066,"Dialogue engines that incorporate different types of agents to converse with humans are popular. However, conversations are dynamic in the sense that a selected response will change the conversation on-the-fly, influencing the subsequent utterances in the conversation, which makes the response selection a challenging problem. We model the problem of selecting the best response from a set of responses generated by a heterogeneous set of dialogue agents by taking into account the conversational history, and propose a \emph{Neural Response Selection} method. The proposed method is trained to predict a coherent set of responses within a single conversation, considering its own predictions via a curriculum training mechanism. Our experimental results show that the proposed method can accurately select the most appropriate responses, thereby significantly improving the user experience in dialogue systems. ","Weakly-Supervised Neural Response Selection from an Ensemble of
  Task-Specialised Dialogue Agents"
188,1257984205629214720,1253638786812014592,Tatsuki Kuribayashi,"['How to test linguistic generalizations?\nA. Psychological experiments with humans 🧠\nB. Corpus study 📃\n\nWe explore the third option:\nC. Using neural language models 🤖\n\nIn Japanese word order analysis, option C can be valid.\n#acl2020nlp <LINK> <LINK>', 'We discuss the advantages and limitations of option C.\nWe reveal that Japanese neural language models have a word order preference consistent with native speakers and existing studies on word order, which support option C.\nw/ @t_ito0516 @drJunSuzuki @inuikentaro at @NlpTohoku']",https://arxiv.org/abs/2005.00842,"We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments. ","Language Models as an Alternative Evaluator of Word Order Hypotheses: A
  Case Study in Japanese"
189,1257833500343967744,1116002690604130305,Juliette Becker,"[""Just posted to arXiv - <LINK> - my new paper 'A Coupled Analysis of Atmospheric Mass Loss and Tidal Evolution in XUV Irradiated Exoplanets: the TRAPPIST-1 Case Study', in which we use SWIFT UV+Xray data to study the present-day XUV luminosity of TRAPPIST-1!"", 'The figures and code are also available at GitHub: https://t.co/j0mi5UiMkr. In this paper, we use @VPLanetCode to study the possible water mass loss on each TRAPPIST-1 planet due to our measured luminosities.', 'We also attempt to constrain the planetary interior structures (the Q/k2 ratio) from the current day orbital elements combined with the expected past tidal evolution. We are able to find lower limits on Q/k2 for the inner planets: https://t.co/BZIREZCZ8y', '@di_goldene_pave You are correct! Sometimes I forget that not ALL astro missions/tools are acronyms and just hit that caps-lock :)']",https://arxiv.org/abs/2005.01740,"Exoplanets residing close to their stars can experience evolution of both their physical structures and their orbits due to the influence of their host stars. In this work, we present a coupled analysis of dynamical tidal dissipation and atmospheric mass loss for exoplanets in XUV irradiated environments. As our primary application, we use this model to study the TRAPPIST-1 system, and place constraints on the interior structure and orbital evolution of the planets. We start by reporting on a UV continuum flux measurement (centered around $\sim1900$ Angstroms) for the star TRAPPIST-1, based on 300 ks of Neil Gehrels Swift Observatory data, and which enables an estimate of the XUV-driven thermal escape arising from XUV photo-dissociation for each planet. We find that the X-ray flaring luminosity, measured from our X-ray detections, of TRAPPIST-1 is 5.6 $\times$10$^{-4} L_{*}$, while the full flux including non-flaring periods is 6.1 $\times$10$^{-5} L_{*}$, when $L_{*}$ is TRAPPIST-1's bolometric luminosity. We then construct a model that includes both atmospheric mass-loss and tidal evolution, and requires the planets to attain their present-day orbital elements during this coupled evolution. We use this model to constrain the ratio $Q'=3Q/2k_{2}$ for each planet. Finally, we use additional numerical models implemented with the Virtual Planet Simulator \texttt{VPLanet} to study ocean retention for these planets using our derived system parameters. ","A Coupled Analysis of Atmospheric Mass Loss and Tidal Evolution in XUV
  Irradiated Exoplanets: the TRAPPIST-1 Case Study"
190,1257816180779188224,14544467,Daniel Apai,['Excited to share our new paper – led by @azstewobs student Alex Bixel - where we propose an age-oxygen correlation in exoplanets with life &amp; a blog post on why this is cool: <LINK> @EOSNExSS @nexssinfo  <LINK>'],https://arxiv.org/abs/2005.01587,"Life has had a dramatic impact on the composition of Earth's atmosphere over time, which suggests that statistical studies of other inhabited planets' atmospheres could reveal how they co-evolve with life. While many evolutionary pathways are possible for inhabited worlds, a possible starting hypothesis is that most of them evolve similarly to Earth, which we propose could lead to a positive ""age-oxygen correlation"" between the ages of inhabited planets and the fraction which have oxygen-rich atmospheres. We demonstrate that next-generation space observatories currently under consideration could test this hypothesis, but only if the stellar age distribution of the target sample is carefully considered. We explore three possible parameterizations of the age-oxygen correlation, finding that they yield similar results. Finally, we examine how abiotic oxygen sources could affect the results, and discuss how measuring the age-dependence of oxygen could shed light on whether it is a reliable biosignature. Future efforts can expand upon this groundwork by incorporating detailed models of the redox balance of terrestrial planets and its dependence on stellar and planetary properties. ","Testing Earth-like atmospheric evolution on exo-Earths through oxygen
  absorption: required sample sizes and the advantage of age-based target
  selection"
191,1257783060214530048,38844142,kartik goyal,"['In our #acl2020nlp paper, we propose a generative model to analyze glyph shapes in early-modern printing. The interpretable variables account for spatial noise while the neurally parametrized uninterpretable latent variable explains other variation. <LINK> 1/6 <LINK>', 'This model learns the underlying glyph templates and allows to cluster on the basis of subtle differences in glyph shapes in presence of multiple confounding sources of variance like inking. 2/6 https://t.co/DYqNiQvRWr', 'In order to ensure that the expressive neurally parametrized variable does not learn to explain the spatial adjustment noise (translation, shear, rotation, scale) captured by the interpretable variables, it is crucial to restrict the inference network for q(z). 3/6 https://t.co/43ShyS8BqS', 'Our model discovers different fonts used in early modern documents in a completely unsupervised manner! These are the discovered glyph shapes for Fs and Rs in Leviathan (1651?) 4/6 https://t.co/KTbKblska3', 'The interpretable variables also provide explicit control. We used the estimated offsets, rotation/shear angles and scaling factors to project the noisy images to a fixed size frame resulting in aligned/comparable input images. 5/6 https://t.co/PAMjeykCVj', ""This is joint work with fantastic collaborators @BergKirkpatrick, @ChrisVVarren, @redpony and Max G'Sell. Code coming soon! 6/6""]",https://arxiv.org/abs/2005.01646,"We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing. Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures. We show that our approach outperforms rigid interpretable clustering baselines (Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-font documents. ","A Probabilistic Generative Model for Typographical Analysis of Early
  Modern Printing"
192,1257520110853476358,119013247,Aditya Vijaykumar,"['We put a preprint paper on arXiv! This one talks about the possibility of probing the Large Scale Structure of the Universe with future gravitational-wave observations. You can find the paper here - <LINK>\n\nRead on for a summary 😃 [1/n] <LINK>', 'Our Universe consists of very dense, compact objects like black holes and neutron stars that form binary systems (like the earth and the sun). As they go around each other, they emit gravitational waves (GW) and lose energy, until they merge and form a new compact object. [2/n]', 'On average, we detect a Binary Black Hole (BBH) event about once every week when the detectors are on. With future improvements in detectors, we expect to detect around half a million binary black holes in a year: roughly one every minute! GWs be poppin’ like popcorn [4/n] https://t.co/rpxmvqsVL9', 'The events that we have already detected have enabled us to make measurements of various cosmological parameters, most interestingly the value of the Hubble constant ie. the rate at which the universe is expanding.\n\nWhat more cosmology can we do with these events? [5/n] https://t.co/zd3Tdg6Xfa', 'We looked for an answer in the field of galaxy surveys!\n\nGalaxy surveys map out galaxies in the Universe. Using properties of the distribution of galaxies in the Universe, one can draw conclusions about how galaxies formed and how the Universe evolved. [6/n] https://t.co/8vuNXZ9Oas', 'One important statistic that we can measure from galaxy surveys is the “two-point correlation function” (2PCF). The 2PCF encodes information of how many galaxies are separated by a particular distance in the survey. This is what it looks like for galaxies - https://t.co/7gMTgsVONV', 'The galaxies that we observe have matter not unlike what we see around us; but most matter in the Universe happens to be “dark” matter. Funnily enough, in a few cases, we know more about dark matter than we know about normal matter. [8/n] https://t.co/TlUOEjY1Qp', 'We have theoretical expectations of the 2PCF for “dark” matter, and the 2PCF of galaxies can be approximated to a constant times that of dark matter. This constant is the bias factor and it encodes information about the properties of the galaxies. [9/n]', 'So much about galaxies. In the future, we will have a “survey” of BBHs, and we attempt to calculate the 2PCF of BBHs and extract physics out of it in the same way as we do with galaxies. [10/n] https://t.co/xoJDptCnj0', 'But there is a catch. When we detect BBH events, we cannot ascertain our distance to them and their position in the sky with absolute precision. This means the 2PCF we measure will be “smeared” due to these position uncertainties, ie. we lose some information (see fig). [11/n] https://t.co/R24Iik9dUK', 'We simulate fake future BBH events with a given bias factor and test whether we can recover their 2PCF. We also calculate how the theoretical 2PCF gets smeared due to the position uncertainties and match it with the 2PCF from fake events to calculate the bias factor. [12/n] https://t.co/6bKV17syqh', 'We see that with 3-5 years of observation with future GW telescopes, we can constrain the bias factor to a decent accuracy!\n\nBut why is the bias factor of BBHs interesting?\xa0 [13/n] https://t.co/dt7xqa7Hey', 'BBHs form inside galaxies, and comparing the calculated bias factor from BBHs with those from galaxies can help ascertain the type of galaxies these BBHs form in.\n\nBroadly, BBHs provide a complementary way to test our theories of the distribution of matter in the Universe. [14/n] https://t.co/zaJrFoAASN', 'This work was done with @sumit_tk among others. \n\nBiggest takeaway - science takes time. https://t.co/JCz7lLp0Xz', '@sumit_tk इसके बाद तो बिलकुल नही!', 'There is a lot more physics we hope to explore in future work :)\n\nThanks for reading, and stay safe! Here’s another thread about another paper that came out last month. [15/15]\n\nhttps://t.co/LlQrh0zC4C', 'Thanks @tisanskritea for help in putting this up 😃', '@skau98 Thanks buddy!', '@ananya_tweeted Thanks Ananya🥺 https://t.co/YlYJCMjknu']",https://arxiv.org/abs/2005.01111,"Third generation gravitational-wave (GW) detectors are expected to detect a large number of binary black holes (BBHs) to large redshifts, opening up an independent probe of the large scale structure using their clustering. This probe will be complementary to the probes using galaxy clustering -- GW events could be observed up to very large redshifts ($z \sim 10$) although the source localization will be much poorer at large distances ($\sim$ tens of square degrees). We explore the possibility of probing the large scale structure from the spatial distribution of the observed BBH population, using their two-point (auto)correlation function. We find that we can estimate the bias factor of population of BBH (up to $z \sim 1$) with a few years of observations with these detectors. Our method relies solely on the source-location posteriors obtained the GW events and does not require any information from electromagnetic observations. This will help in identifying the type of galaxies that host the BBH population, thus shedding light on their origins. ","Probing the large scale structure using gravitational-wave observations
  of binary black holes"
193,1266654987540746240,46856175,Pedram Daee,"['How users collaborate with an intelligent system? We designed a function optimization task where a human and an optimization algorithm collaborate to find the max of a 1-d function. With @fcole90 Jussi Jokinen @oulasvirta  @samikaski. preprint: <LINK>  @UMAPconf <LINK>', ""Users are not passive feedback sources, rather they are in control, actively steering the system towards their goal. This behaviour violates system's assumptions about the user and suggests the need for novel types of user models. Demo: https://t.co/BgkkdXIqkt""]",https://arxiv.org/abs/2005.01291,"A central concern in an interactive intelligent system is optimization of its actions, to be maximally helpful to its human user. In recommender systems for instance, the action is to choose what to recommend, and the optimization task is to recommend items the user prefers. The optimization is done based on earlier user's feedback (e.g. ""likes"" and ""dislikes""), and the algorithms assume the feedback to be faithful. That is, when the user clicks ""like,"" they actually prefer the item. We argue that this fundamental assumption can be extensively violated by human users, who are not passive feedback sources. Instead, they are in control, actively steering the system towards their goal. To verify this hypothesis, that humans steer and are able to improve performance by steering, we designed a function optimization task where a human and an optimization algorithm collaborate to find the maximum of a 1-dimensional function. At each iteration, the optimization algorithm queries the user for the value of a hidden function $f$ at a point $x$, and the user, who sees the hidden function, provides an answer about $f(x)$. Our study on 21 participants shows that users who understand how the optimization works, strategically provide biased answers (answers not equal to $f(x)$), which results in the algorithm finding the optimum significantly faster. Our work highlights that next-generation intelligent systems will need user models capable of helping users who steer systems to pursue their goals. ","Human Strategic Steering Improves Performance of Interactive
  Optimization"
194,1265433415781842945,17261743,Emad Aghayi,"['We ( @ThomasLaToza ,  @aaronemassey , and I) are happy to announce our paper entitled to \n”Find Unique Usages: Helping Developers Understand Common Usages” \nwill appear at @vlhcc 20. \n\nPreprint: <LINK>']",https://arxiv.org/abs/2005.11474,"When working in large and complex codebases, developers face challenges using \textit{Find Usages} to understand how to reuse classes and methods. To better understand these challenges, we conducted a small exploratory study with 4 participants. We found that developers often wasted time reading long lists of similar usages or prematurely focused on a single usage. Based on these findings, we hypothesized that clustering usages by the similarity of their surrounding context might enable developers to more rapidly understand how to use a function. To explore this idea, we designed and implemented \textit{Find Unique Usages}, which extracts usages, computes a diff between pairs of usages, generates similarity scores, and uses these scores to form usage clusters. To evaluate this approach, we conducted a controlled experiment with 12 participants. We found that developers with Find Unique Usages were significantly faster, completing their task in 35% less time. ",Find Unique Usages: Helping Developers Understand Common Usages
195,1262773897755328512,51169895,Gianluca Stringhini,"['Our @WebSciConf paper ""Measuring and Characterizing Hate Speech on News Websites"" is available here: <LINK>\n\nWe find that hateful comments are likely to appear in the comment section shortly after the link to a news article is posted on online communities <LINK>']",https://arxiv.org/abs/2005.07926,"The Web has become the main source for news acquisition. At the same time, news discussion has become more social: users can post comments on news articles or discuss news articles on other platforms like Reddit. These features empower and enable discussions among the users; however, they also act as the medium for the dissemination of toxic discourse and hate speech. The research community lacks a general understanding on what type of content attracts hateful discourse and the possible effects of social networks on the commenting activity on news articles. In this work, we perform a large-scale quantitative analysis of 125M comments posted on 412K news articles over the course of 19 months. We analyze the content of the collected articles and their comments using temporal analysis, user-based analysis, and linguistic analysis, to shed light on what elements attract hateful comments on news articles. We also investigate commenting activity when an article is posted on either 4chan's Politically Incorrect board (/pol/) or six selected subreddits. We find statistically significant increases in hateful commenting activity around real-world divisive events like the ""Unite the Right"" rally in Charlottesville and political events like the second and third 2016 US presidential debates. Also, we find that articles that attract a substantial number of hateful comments have different linguistic characteristics when compared to articles that do not attract hateful comments. Furthermore, we observe that the post of a news articles on either /pol/ or the six subreddits is correlated with an increase of (hateful) commenting activity on the news articles. ",Measuring and Characterizing Hate Speech on News Websites
196,1260035034053898245,1199782808501153793,Andrew Lampinen,"['How can deep learning models flexibly reuse their knowledge? How can they adapt to new tasks zero-shot, as humans can? In our new preprint (<LINK>), we propose a new approach based on learning to transform task representations: meta-mapping. Preview in thread:', 'Our approach can make drastic adaptations zero-shot, like switching from winning at (simplified) poker to trying to lose. It can allow a visual classification system to recognize new concepts, and can adapt a model-free reinforcement learning to new tasks, without data from them.', 'It accomplishes this without prior domain knowledge, based only on the relationships between tasks. Specifically, it learns basic task representations, e.g. for poker, via meta-learning. It also learns meta-mappings, higher order tasks which transform these basic task reps.', 'For example, it might learn a ""lose"" meta-mapping, from the relationship between winning and losing at games like blackjack. This meta-mapping could then be applied to the model\'s representation of poker, in order to lose at poker zero-shot.', 'We show this method can allow 80-90% performance, zero-shot, in domains ranging from polynomial regression to visual classification to reinforcement learning, outperforming baselines (sometimes substantially). It even exhibits some intriguing signatures of being more systematic.', 'This zero-shot adaptation then allows the system to master the new tasks much more efficiently. It makes an order of magnitude fewer mistakes (cumulative loss) on the way to mastering the tasks than the next-best approach we considered.', 'We implement this all in a parsimonious, homoiconic architecture that reuses the same networks for basic tasks and meta-mappings. This improves generalization! We also show our approach works with task representations constructed from either examples of the task or language.', 'I think that meta-mapping may offer a useful concept for building more flexible artificial intelligence systems, and better cognitive models.', ""Thanks for making it through this thread! There's a lot more detail, experiments, and related work/implications in the paper, please check it out! I hope it will be interesting and understandable to researchers in both AI/ML and cognitive science. https://t.co/HRgMsycNQt""]",https://arxiv.org/abs/2005.04318,"An important aspect of intelligence is the ability to adapt to a novel task without any direct experience (zero-shot), based on its relationship to previous tasks. Humans can exhibit this cognitive flexibility. By contrast, models that achieve superhuman performance in specific tasks often fail to adapt to even slight task alterations. To address this, we propose a general computational framework for adapting to novel tasks based on their relationship to prior tasks. We begin by learning vector representations of tasks. To adapt to new tasks, we propose meta-mappings, higher-order tasks that transform basic task representations. We demonstrate the effectiveness of this framework across a wide variety of tasks and computational paradigms, ranging from regression to image classification and reinforcement learning. We compare to both human adaptability and language-based approaches to zero-shot learning. Across these domains, meta-mapping is successful, often achieving 80-90% performance, without any data, on a novel task, even when the new task directly contradicts prior experience. We further show that meta-mapping can not only generalize to new tasks via learned relationships, but can also generalize using novel relationships unseen during training. Finally, using meta-mapping as a starting point can dramatically accelerate later learning on a new task, and reduce learning time and cumulative error substantially. Our results provide insight into a possible computational basis of intelligent adaptability and offer a possible framework for modeling cognitive flexibility and building more flexible artificial intelligence systems. ",Transforming task representations to perform novel tasks
197,1259829851898421250,981929532474183682,Ines Chami,"['Working with graph-structured data? Check out our recent survey for Machine Learning on Graphs: \n<LINK>\n\nWe propose a simple framework (GraphEDM) and a comprehensive Taxonomy to review and unify several graph representation learning methods. <LINK>', 'GraphEDM encapsulates over thirty graph embedding methods: from graph regularization algorithms (Label Propagation, ...) to more recent advances such as random walks (DeepWalk, node2vec, ...) or GNNs (GCN, GAT, ...). https://t.co/KBb4icnUpo', ""@PetarV_93 @HazyResearch @phanein @abu_usc Thanks for sharing these exciting works! We'll definitely include these in the next version:)"", ""@balasrini32 @HazyResearch @phanein @abu_usc Thanks for sharing this work, looks very relevant and we'll definitely take a look!"", ""@thomaskipf @HazyResearch @phanein @abu_usc Thanks for the helpful feedback, really appreciated!! We tried to distinguish between GCNs and spectral methods using the spectrum-dependant/specific notions, but we'll think about how to collapse local message passing methods even further:)"", '@thomaskipf @HazyResearch @phanein @abu_usc Makes perfect sense! We actually had a discussion on whether to categorize GCNs/ChebyNets under spectral or message passing algorithms, so your feedback is really helpful:)', '@thomaskipf @HazyResearch @phanein @abu_usc @danieldazac Really nice work and framework by @danieldazac! We plan to add more methods including works like DGI in the next version so this is very helpful, thanks for sharing!']",https://arxiv.org/abs/2005.03675,"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area. ",Machine Learning on Graphs: A Model and Comprehensive Taxonomy
198,1258704851724369920,432812106,Corrado Monti,"[""🔴New paper published at @WebSciConf!\n\nWho were Trump's supporters before Trump? We use Reddit to study which traits in 2012 anticipated the emergence of Trumpism in 2016.\n\nPreprint: <LINK>\nWith @gdfm7 @FrancescoBonchi and Joan Massachs.\n\n[1/2] Short summary ⬇️ <LINK>"", 'We find that Trump support emerged from both libertarian and conservative camps, and even ancaps. Many were interested in guns, entrepreneurship, shock humor and so-called ""Men\'s rights"".\nAlso, they often negative feedback from mainstream communities.\nSee paper for more!\n\n[2/2] https://t.co/8X2HZyKIkg', '@mtizzoni We also find r/DeadBedrooms there... 😅', 'Forgot to add: we open-sourced our data set as a prepared and easy-to-use CSV to study the change in political opinions on Reddit between 2012 and 2016\n\nHere it is:  https://t.co/DTrhTVdDXV']",https://arxiv.org/abs/2005.01790,"We study the emergence of support for Donald Trump in Reddit's political discussion. With almost 800k subscribers, ""r/The_Donald"" is one of the largest communities on Reddit, and one of the main hubs for Trump supporters. It was created in 2015, shortly after Donald Trump began his presidential campaign. By using only data from 2012, we predict the likelihood of being a supporter of Donald Trump in 2016, the year of the last US presidential elections. To characterize the behavior of Trump supporters, we draw from three different sociological hypotheses: homophily, social influence, and social feedback. We operationalize each hypothesis as a set of features for each user, and train classifiers to predict their participation in r/The_Donald. We find that homophily-based and social feedback-based features are the most predictive signals. Conversely, we do not observe a strong impact of social influence mechanisms. We also perform an introspection of the best-performing model to build a ""persona"" of the typical supporter of Donald Trump on Reddit. We find evidence that the most prominent traits include a predominance of masculine interests, a conservative and libertarian political leaning, and links with politically incorrect and conspiratorial content. ","Roots of Trumpism: Homophily and Social Feedback in Donald Trump Support
  on Reddit"
199,1258555474775064576,35926248,Thomas Paula,"['Our paper ""A Proposal for Intelligent Agents with Episodic Memory"" is on Arxiv! We propose an alternative look at the role of episodic memory for intelligent agents. We hope this view can help to spark new discussions and ideas! @dfm794  @JulianoVacaro \n\n<LINK>']",https://arxiv.org/abs/2005.03182,"In the future we can expect that artificial intelligent agents, once deployed, will be required to learn continually from their experience during their operational lifetime. Such agents will also need to communicate with humans and other agents regarding the content of their experience, in the context of passing along their learnings, for the purpose of explaining their actions in specific circumstances or simply to relate more naturally to humans concerning experiences the agent acquires that are not necessarily related to their assigned tasks. We argue that to support these goals, an agent would benefit from an episodic memory; that is, a memory that encodes the agent's experience in such a way that the agent can relive the experience, communicate about it and use its past experience, inclusive of the agents own past actions, to learn more effective models and policies. In this short paper, we propose one potential approach to provide an AI agent with such capabilities. We draw upon the ever-growing body of work examining the function and operation of the Medial Temporal Lobe (MTL) in mammals to guide us in adding an episodic memory capability to an AI agent composed of artificial neural networks (ANNs). Based on that, we highlight important aspects to be considered in the memory organization and we propose an architecture combining ANNs and standard Computer Science techniques for supporting storage and retrieval of episodic memories. Despite being initial work, we hope this short paper can spark discussions around the creation of intelligent agents with memory or, at least, provide a different point of view on the subject. ",A Proposal for Intelligent Agents with Episodic Memory
200,1258519208133922816,1214840725440999424,Masoud Mansoury,['Our #UMAP2020 paper with @HAbdollahpouri et al. is now online:\n<LINK> \nWe propose a graph-based technique based on the maximum flow theory to give higher visibility to items that are ignored by the algorithms due to popularity bias. <LINK>'],https://arxiv.org/abs/2005.01148,"Recommender systems are often biased toward popular items. In other words, few items are frequently recommended while the majority of items do not get proportionate attention. That leads to low coverage of items in recommendation lists across users (i.e. low aggregate diversity) and unfair distribution of recommended items. In this paper, we introduce FairMatch, a general graph-based algorithm that works as a post-processing approach after recommendation generation for improving aggregate diversity. The algorithm iteratively finds items that are rarely recommended yet are high-quality and add them to the users' final recommendation lists. This is done by solving the maximum flow problem on the recommendation bipartite graph. While we focus on aggregate diversity and fair distribution of recommended items, the algorithm can be adapted to other recommendation scenarios using different underlying definitions of fairness. A comprehensive set of experiments on two datasets and comparison with state-of-the-art baselines show that FairMatch, while significantly improving aggregate diversity, provides comparable recommendation accuracy. ","FairMatch: A Graph-based Approach for Improving Aggregate Diversity in
  Recommender Systems"
