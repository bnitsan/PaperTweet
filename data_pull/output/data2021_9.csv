,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1448592644888793089,935236213,Gavin Lamb,"['NEW PAPER! Where we ask, ""what is shaping the jets in neutron-star mergers?"" <LINK>\n\nWork led by Lorenzo Nativi (@Stockholm_Uni), with me (@PhysicsUoL), S. Rosswog and C. Lundman (@Stockholm_Uni), and G. Kowal (@usponline)', 'We inject 4 jets with one of 2 powers and 2 structures into a realistic neutron star merger ejecta with neutrino-driven winds. We evolve these systems in 3D relativistic hydrodynamics (using AMUN) to find the resultant jet profile.\n\nThe initial jet-power has the biggest effect!', ""Lower powered jets result in a more collimated outflow.\n\nResultant profile differences between the jets with varying initial structure jets are largely due to chaotic processes due to the jet-ejecta interaction. \n\nBut this isn't the end of the story! Afterglow modellers take note"", 'Despite the ""small"" variation between fixed power jet\'s resultant outflow profiles. When we fit afterglows using these structures to data, we find a significant difference in the inferred parameters, such as the system inclination!\n\nUsed to infer the H_0 from a single GWEM event', 'So, chaotic processes in the jet-ejecta interaction shape the outflow that results in the afterglows to Gamma-ray Bursts and particularly for Gravitational-wave counterparts.\n\nVariability in the profile due to these processes is significant, in terms of afterglow parameter fits', 'Lower powered jets are more significantly collimated.\n\nWhere we used a relatively low-mass ejecta/wind environment -- so, for GW170817 (with a higher ejected mass), the resultant jet structure seen in the afterglow was a result of the jet-ejecta interaction']",http://arxiv.org/abs/2109.00814,"Jets can become collimated as they propagate through dense environments and understanding such interactions is crucial for linking physical models of the environments to observations. In this work, we use 3D special-relativistic simulations to study how jets propagate through the environment created around a neutron star merger remnant by neutrino-driven winds. We simulate four jets with two different initial structures, top-hat and Gaussian, and two luminosities. After jet breakout, we study the angular jet structures and the resulting afterglow light curves. We find that the initial angular structures are efficiently washed out during the propagation, despite the small wind mass of only $\sim 10^{-3}$ M$_\odot$. The final structure depends on the jet luminosity as less energetic jets are more strongly collimated, and entrainment of baryons leads to a moderate outflow Lorentz factor ($\approx 40$). Although our jets are not specifically intended to model the outflows of the GW170817 event, we show that they can be used to produce light curves consistent with the afterglow observed in the aftermath of GW170817. Using this procedure we show how the inferred physical parameters e.g., inclination angle, ambient particle number density, can vary substantially between independent fits of the same dataset and appear to be sensitive to smaller details of the angular jet shape, indicating that observationally inferred parameters may depend sensitively on the employed jet models. ",Are Interactions with Neutron Star Merger Winds Shaping the Jets?
1,1447755432714330114,2577596593,Chelsea Finn,"['Robot learning is bottlenecked on good, reusable datasets\n\nWe introduce:\n* a new dataset with demos of 71 tasks over 10 envs, all w/ a low-cost arm\n* find the data can be used to help solve new tasks in new envs\n\nPaper: <LINK>\nData &amp; Code: <LINK> <LINK>', 'Led by @febert8888 &amp; Yanlai Yang\nIn collab with Schmeckpeper, Bucher, Georgakis, @KostasPenn, @svlevine']",https://arxiv.org/abs/2109.13396,"Robot learning holds the promise of learning policies that generalize broadly. However, such generalization requires sufficiently diverse datasets of the task of interest, which can be prohibitively expensive to collect. In other fields, such as computer vision, it is common to utilize shared, reusable datasets, such as ImageNet, to overcome this challenge, but this has proven difficult in robotics. In this paper, we ask: what would it take to enable practical data reuse in robotics for end-to-end skill learning? We hypothesize that the key is to use datasets with multiple tasks and multiple domains, such that a new user that wants to train their robot to perform a new task in a new domain can include this dataset in their training process and benefit from cross-task and cross-domain generalization. To evaluate this hypothesis, we collect a large multi-domain and multi-task dataset, with 7,200 demonstrations constituting 71 tasks across 10 environments, and empirically study how this data can improve the learning of new tasks in new environments. We find that jointly training with the proposed dataset and 50 demonstrations of a never-before-seen task in a new domain on average leads to a 2x improvement in success rate compared to using target domain data alone. We also find that data for only a few tasks in a new domain can bridge the domain gap and make it possible for a robot to perform a variety of prior tasks that were only seen in other domains. These results suggest that reusing diverse multi-task and multi-domain datasets, including our open-source dataset, may pave the way for broader robot generalization, eliminating the need to re-collect data for each new robot learning project. ","Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain
  Datasets"
2,1446408915747495942,17819190,Vaishak Belle,['Interested in synthesizing the semantics of programming languages? \n\nWe have a new paper on that! Accepted at OOPSLA: \n<LINK>'],https://arxiv.org/abs/2109.06114v1,"Programming or scripting languages used in real-world systems are seldom designed with a formal semantics in mind from the outset. Therefore, developing well-founded analysis tools for these systems requires reverse-engineering a formal semantics as a first step. This can take months or years of effort. Can we (at least partially) automate this process? Though desirable, automatically reverse-engineering semantics rules from an implementation is very challenging, as found by Krishnamurthi et al. [2019]. In this paper, we highlight that scaling methods with the size of the language is very difficult due to state space explosion, so we propose to learn semantics incrementally. We give a formalisation of Krishnamurthi et al.'s desugaring learning framework in order to clarify the assumptions necessary for an incremental learning algorithm to be feasible. We show that this reformulation allows us to extend the search space and express rules that Krishnamurthi et al. described as challenging, while still retaining feasibility. We evaluate enumerative synthesis as a baseline algorithm, and demonstrate that, with our reformulation of the problem, it is possible to learn correct desugaring rules for the example source and core languages proposed by Krishnamurthi et al., in most cases identical to the intended rules. In addition, with user guidance, our system was able to synthesize rules for desugaring list comprehensions and try/catch/finally constructs. ","] One Down, 699 to Go: or, synthesising compositional desugarings"
3,1446326187416961026,990433714948661250,Sergey Levine,"['Is there a principled way to adapt a model to distributional shift without labels? In our new paper, ""Training on Test Data"", we propose a Bayesian adaptation strategy based on BNNs and entropy minimization. w/ Aurick Zhou: <LINK>\n\nA thread:', 'First: what information do we gain from observing unlabeled datapoints from a new distribution? We can draw a graphical model for this: x is input, y is label, theta is classifier params, phi parameterizes x distro. Unfortunately, if y is unobserved, x tells nothing about theta! https://t.co/E2w5hxaYyV', ""We need a better graphical model. What if we assume new datapoints are not arbitrary: if we are asked to classify a new OOD, it likely belongs to *one of* the classes, we just don't know which one! Now there is a relationship between theta and phi for each distribution! https://t.co/2BPaQCR0yh"", ""This naturally leads to an entropy minimization procedure at test time: get some unlabeled points, and then update the parameter posterior to get lower entropy on test points, but don't stray too far from parameter distribution on training set! https://t.co/TdgRlHmEZQ"", 'To avoid needing to store all training data, we can learn posterior q(theta) using any BNN approach, and then incorporate this as a regularizer when minimizing entropy at test time on unlabeled data. https://t.co/cHlCQLDQRu', 'This leads to better accuracy *and* better calibration on unlabeled test points.\n\nInspired by some classics on entropy minimization:\n\nY. Grandvalet, Y. Bengio. Semi-supervised Learning by Entropy Minimization\nM. Seeger. Input-dependent Regularization of Conditional Density Models']",https://arxiv.org/abs/2109.12746,"When faced with distribution shift at test time, deep neural networks often make inaccurate predictions with unreliable uncertainty estimates. While improving the robustness of neural networks is one promising approach to mitigate this issue, an appealing alternate to robustifying networks against all possible test-time shifts is to instead directly adapt them to unlabeled inputs from the particular distribution shift we encounter at test time. However, this poses a challenging question: in the standard Bayesian model for supervised learning, unlabeled inputs are conditionally independent of model parameters when the labels are unobserved, so what can unlabeled data tell us about the model parameters at test-time? In this paper, we derive a Bayesian model that provides for a well-defined relationship between unlabeled inputs under distributional shift and model parameters, and show how approximate inference in this model can be instantiated with a simple regularized entropy minimization procedure at test-time. We evaluate our method on a variety of distribution shifts for image classification, including image corruptions, natural distribution shifts, and domain adaptation settings, and show that our method improves both accuracy and uncertainty estimation. ",Training on Test Data with Bayesian Adaptation for Covariate Shift
4,1446264438340816923,240851505,Tomas Pfister,"['""Fast Sample Reweighting"" is a new paper from our research group @GoogleCloud that allows you to re-weight training samples effectively without the need for additional unbiased reward data.  <LINK> PS: We’re hiring!  @GoogleAI\xa0@googlecloud\xa0#ML\xa0#research\xa0#ICCV2021 <LINK>']",https://arxiv.org/abs/2109.03216,"Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at this https URL ",Learning Fast Sample Re-weighting Without Reward Data
5,1446193981092732935,228792418,Tim Rocktäschel,"[""Great summary video about @samveIyan @_robertkirk @y0b1byte @MinqiJiang @jparkerholder's MiniHack sandbox RL environment @NeurIPSConf paper based on @NetHack_LE!\n\n👉 <LINK>\n\nCode <LINK>\nPaper <LINK>\nBlog <LINK> <LINK>""]",https://arxiv.org/abs/2109.13202,"Progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity. ","MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning
  Research"
6,1446143854588035083,280403336,Sean Welleck,"['new paper:\n\n""Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics""\n\nSequence models show amazing performance on many tasks. Does perfect test accuracy tell the full story?\n\nw/ @PeterWestTM, @JizeCao, @YejinChoinka \n\n<LINK> <LINK>', 'We consider symbolic integration, as it requires generalizing systematically beyond the test set and is verifiable.\n\nDespite high test accuracy, we find deficiencies in robustness, compositionality, and OOD generalization in a state-of-the-art MLE seq2seq model for this task. https://t.co/ErGkN7ve9w', 'We develop a genetic algorithm 🧬 (SAGGA) which automatically discovers (thousands of) failures that highlight each type of generalization, and test suites that perturb and compose validation problems &amp; simple functions. https://t.co/ofikc0H0HQ', 'Robustness tells us whether the model systematically solves all problems in a neighborhood, typically governed by a generalizable pattern.\n\nThe model is surprisingly brittle when test problems or simple functions are slightly changed. https://t.co/qWAQ51ZPIC', 'Regarding compositionality, successfully integrating two functions did not imply that the model learned to integrate their sum \n\n(recall the sum rule of integration https://t.co/Oo2XOVaUKv) https://t.co/78QyBDe1sf', 'Moving further from the training distribution:\n\nPerformance degrades for integers and problem sizes larger than those typically encountered in training (extrapolation)\n\nAnd functions not covered in the training set (""exploits"") https://t.co/I57XDM5KrH', 'We also study the effect of increasing the search budget and whether it is a search problem alone -- check out the paper! https://t.co/wIpDcDRL0r', 'Stay tuned for code, which we plan to release.']",https://arxiv.org/abs/2109.13986,"Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is defined by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the test set. We develop a methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier. Despite promising in-distribution performance of sequence-to-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and out-of-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically finds large collections of failures in a controllable manner. Our investigation highlights the difficulty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization. ","Symbolic Brittleness in Sequence Models: on Systematic Generalization in
  Symbolic Mathematics"
7,1445394405104525318,1228624747,Lars Ruthotto,"['Training deep neural networks can be challenging! Think learning rates, regularization, other hyperparameters... In her new paper, @MathEmory postdoc Liz Newman splits off the weights of the last layer to improve training: <LINK>']",https://arxiv.org/abs/2109.14002,"Deep neural networks (DNNs) have shown their success as high-dimensional function approximators in many applications; however, training DNNs can be challenging in general. DNN training is commonly phrased as a stochastic optimization problem whose challenges include non-convexity, non-smoothness, insufficient regularization, and complicated data distributions. Hence, the performance of DNNs on a given task depends crucially on tuning hyperparameters, especially learning rates and regularization parameters. In the absence of theoretical guidelines or prior experience on similar tasks, this requires solving many training problems, which can be time-consuming and demanding on computational resources. This can limit the applicability of DNNs to problems with non-standard, complex, and scarce datasets, e.g., those arising in many scientific applications. To remedy the challenges of DNN training, we propose slimTrain, a stochastic optimization method for training DNNs with reduced sensitivity to the choice hyperparameters and fast initial convergence. The central idea of slimTrain is to exploit the separability inherent in many DNN architectures; that is, we separate the DNN into a nonlinear feature extractor followed by a linear model. This separability allows us to leverage recent advances made for solving large-scale, linear, ill-posed inverse problems. Crucially, for the linear weights, slimTrain does not require a learning rate and automatically adapts the regularization parameter. Since our method operates on mini-batches, its computational overhead per iteration is modest. In our numerical experiments, slimTrain outperforms existing DNN training methods with the recommended hyperparameter settings and reduces the sensitivity of DNN training to the remaining hyperparameters. ","slimTrain -- A Stochastic Approximation Method for Training Separable
  Deep Neural Networks"
8,1445350214127206404,1280041792122093568,Yi-Ling Chung,"['New #ArgMining #NLProc paper  🤗\n\n🚨Multilingual Counter Narrative Type Classification 🚨\n\nis now out: <LINK>\n\nJoint work with @m_guerini and @ragerri\n\nA short thread:', '@m_guerini @ragerri Multilingual and diverse Counter Narratives (CNs) are needed to fight online hate and develop automatic CN evaluation metrics.\n\nWe conducted CN type classification for EN, IT and FR, evaluating SoTA pre-trained LMs in monolingual, multilingual and cross-lingual settings.', 'Some key findings:\n\n1) The performance is promising for the majority classes (facts, question, denouncing)\n\n2) Classifying humor is still challenging especially for Italian and French, due to the use of figurative language and few instances in training data', '3) Combining training data from the three source languages improves performance over the monolingual evaluation\n\n4) The best overall results are obtained if we translate every language to English before cross-lingual prediction']",https://arxiv.org/abs/2109.13664,"The growing interest in employing counter narratives for hatred intervention brings with it a focus on dataset creation and automation strategies. In this scenario, learning to recognize counter narrative types from natural text is expected to be useful for applications such as hate speech countering, where operators from non-governmental organizations are supposed to answer to hate with several and diverse arguments that can be mined from online sources. This paper presents the first multilingual work on counter narrative type classification, evaluating SoTA pre-trained language models in monolingual, multilingual and cross-lingual settings. When considering a fine-grained annotation of counter narrative classes, we report strong baseline classification results for the majority of the counter narrative types, especially if we translate every language to English before cross-lingual prediction. This suggests that knowledge about counter narratives can be successfully transferred across languages. ",Multilingual Counter Narrative Type Classification
9,1444997170420199424,2796071287,Eran Hirsch,"['New #EMNLP2021 demo paper 🤩\niFᴀᴄᴇᴛSᴜᴍ: interactive #faceted summarization for large multi-document exploration!\n\nPaper: <LINK>\nDemo+code: <LINK>\n\n@AlonEirew @obspp18 @clu_avi @ArieCattan @oriern1 @ramakanth1729 HRonen @mohitban47 IdoDagan <LINK>', ""Joint work of @biunlp @IntelAI @uncnlp\n\nStatic #summarization is limited! A user can't request more information on subtopics of interest. It's also computationally difficult to process all the information in multiple long documents."", 'Our faceted navigation design provides an overview of the topic and the ability to gradually investigate subtopics of interest.\nConcise information is communicated via multi-facet abstractive summarization.\nFacet-values are generated based on cross-document coreference pipelines. https://t.co/Aw4QnBZaSt', 'Small-scale eval w. human subjects, as a preliminary examination of our system, yielded satisfactory results wrt facets quality &amp; summary’s coherence, informativeness, non-redundancy, length. On a System Usability Scale (SUS) questionnaire, avg score=82.9 (considered excellent). https://t.co/6xNGOjfG5Q']",https://arxiv.org/abs/2109.11621,"We introduce iFacetSum, a web application for exploring topical document sets. iFacetSum integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user's selections. This approach offers both a comprehensive overview as well as concise details regarding subtopics of choice. Fine-grained facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies, which suggest the usefulness of our approach. ","iFacetSum: Coreference-based Interactive Faceted Summarization for
  Multi-Document Exploration"
10,1444983345755172865,3306943245,Konstantin Klemmer,"['New preprint out today on generative modeling of spatio-temporal patterns! 🎉🤖🌎\n\nJointly lead by @linylinx and me, with Beatrice Acciaio (@ETH_en) and Daniel Neill (@nyuniversity) .\n\nPaper: <LINK>\nCode: <LINK>\nThread: 👇\n\n(1/11) <LINK>', 'Spatio-temporal dynamics can be hard to learn - but explicitly providing models with information on present autocorrelation has shown to help training of NNs for purely spatial data (see: https://t.co/OkOxY6YGXc).\n\nWe expand this idea to the spatio-temporal domain!\n\n(2/11)', ""We propose SPATE, a new measure of spatio-temporal association by expanding the local Moran's I measure to account for spatio-temporal expectations, rather than purely spatial ones. \n\n(3/11) https://t.co/ztSO8sPVkF"", ""We propose 3 ways of obtaining spatio-temporal expectations, following Kulldorff et al's work on space-time scan statistics (https://t.co/hZn5RaNoeO):\n1. No temporal weighting\n2. With temporal weighting\n3. Sequential (no looking into the future) &amp; with temporal weighting\n\n(4/11) https://t.co/Qm3XBUkXVL"", ""SPATE is much more sensitive to temporal changes than the (static) Moran's I; the temporal weight lengthscale allows for focusing on the detection of more short or more long term spatio-temporal clusters. \n\n(5/11) https://t.co/hh81MPustn"", 'We combine SPATE with the state-of-the art GAN for sequential data, COT-GAN (by @linylinx et al: https://t.co/uTZFrDjHr3) to create SPATE-GAN.\n\n(6/11) https://t.co/NzE7v0S9Wo', 'SPATE-GAN uses the SPATE embeddings of real and generated data to create an embedding loss which nudges the GAN to produce synthetic data faithful to the spatio-temporal associations observed in the real data.\n\n(7/11) https://t.co/J3EdVoALLe', 'We test our approach on complex, real-world spatio-temporal patterns: (1) global surface temperatures, (2) Log-Gaussian Cox Processes (e.g. used in epidemiology) and (3) turbulent flows.\n\n(8/11) https://t.co/DP5q0CJSBY', 'The SPATE embedding loss consistently improves the performance of baseline GANs for sequential data. Particularly, SPATE using sequential, weighted spatio-temporal expetations works particularly well with the causal optimal transport loss of COT-GAN\n\n(9/11) https://t.co/gs1XdzOKEc', 'Very happy that this paper once again shows how well GIS and Neural Nets go together! #GeoAI #GeoML #SpatialDataScience #geospatial \n\n(10/11) https://t.co/oo20zDPQyM', ""We provide all our code (in PyTorch) and data on our GitHub Page: https://t.co/u07Gw478wJ\n\nWe also include the (to our knowledge) first, fast PyTorch implementation of the local Moran's I metric 🚀.\n\n(11/11)""]",https://arxiv.org/abs/2109.15044,"From ecology to atmospheric sciences, many academic disciplines deal with data characterized by intricate spatio-temporal complexities, the modeling of which often requires specialized approaches. Generative models of these data are of particular interest, as they enable a range of impactful downstream applications like simulation or creating synthetic training data. Recent work has highlighted the potential of generative adversarial nets (GANs) for generating spatio-temporal data. A new GAN algorithm COT-GAN, inspired by the theory of causal optimal transport (COT), was proposed in an attempt to better tackle this challenge. However, the task of learning more complex spatio-temporal patterns requires additional knowledge of their specific data structures. In this study, we propose a novel loss objective combined with COT-GAN based on an autoregressive embedding to reinforce the learning of spatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new metric measuring spatio-temporal autocorrelation by using the deviance of observations from their expected values. We compute SPATE for real and synthetic data samples and use it to compute an embedding loss that considers space-time interactions, nudging the GAN to learn outputs that are faithful to the observed dynamics. We test this new objective on a diverse set of complex spatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and global weather data. We show that our novel embedding loss improves performance without any changes to the architecture of the COT-GAN backbone, highlighting our model's increased capacity for capturing autoregressive structures. We also contextualize our work with respect to recent advances in physics-informed deep learning and interdisciplinary work connecting neural networks with geographic and geophysical sciences. ","SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal
  Patterns with an Autoregressive Embedding Loss"
11,1444011135989161988,207494657,Anabel Quan-Haase,"['Our new paper on social support, #ICTs, #socialmedia and policy for #COVID19  is now on ArXiv:\nThe role of communication technology across the life course: A field guide to social support in East York\n@m_harper93 @barrywellman \n<LINK>']",https://arxiv.org/abs/2109.13907,"We examine how Canadians living in the East York section of Toronto exchange social support. Just as we have had to deconstruct social support to understand its component parts, we now deconstruct how different types of communication technologies play socially supportive roles. We draw on 101 in-depth interviews conducted in 2013-2014 to shed light on the support networks of a sample of East York residents and discern the role of communication technologies in the exchange of different types of social support across age groups. Our findings show that not much has changed since the 1960s in terms of the social ties that our sample of East Yorkers have, and the types of support mobilized via social networks: companionship, small and large services, emotional aid, and financial support. What has changed is how communication technologies interweave in complex ways with different types of social ties (partners, siblings, friends, etc.) to mobilize social support. We found that with siblings and extended kin communication technologies could boost the frequency of interaction and help exchange support at a distance. With friendship ties, communication technologies provide a continuous, constant flow of interaction. We draw implications for theories of social support and for social policy linked to interventions aimed at helping vulnerable groups during the COVID-19 pandemic. ","The Role of Communication Technology Across the Life Course: A Field
  Guide to Social Support in East York"
12,1443957112917434378,3433220662,Anthony Bonato,['New paper on arXiv! Joint work with undergrads as part of the @FieldsInstitute Undergraduate Summer Research Program. We find bounds and exact values for pursuit-evasion parameters on graphs derived from latin squares and MOLS.\n<LINK> <LINK>'],https://arxiv.org/abs/2109.14669,"We investigate various pursuit-evasion parameters on latin square graphs, including the cop number, metric dimension, and localization number. The cop number of latin square graphs is studied, and for $k$-MOLS$(n),$ bounds for the cop number are given. If $n>(k+1)^2,$ then the cop number is shown to be $k+2.$ Lower and upper bounds are provided for the metric dimension and localization number of latin square graphs. The metric dimension of back-circulant latin squares shows that the lower bound is close to tight. Recent results on covers and partial transversals of latin squares provide the upper bound of $n+O\left(\frac{\log{n}}{\log{\log{n}}}\right)$ on the localization number of a latin square graph of order $n.$ ",Pursuit-evasion games on latin square graphs
13,1443874003618353191,196589843,Helga Dénes,"['The newest Apertif paper is out 🎉 <LINK>\nThis paper is describing the awesome new receiver system on the Westerbork Telescope, which is now doing the Apertif sky surveys.']",https://arxiv.org/abs/2109.14234,"We describe the APERture Tile In Focus (Apertif) system, a phased array feed (PAF) upgrade of the Westerbork Synthesis Radio Telescope which has transformed this telescope into a high-sensitivity, wide field-of-view L-band imaging and transient survey instrument. Using novel PAF technology, up to 40 partially overlapping beams can be formed on the sky simultaneously, significantly increasing the survey speed of the telescope. With this upgraded instrument, an imaging survey covering an area of 2300 deg2 is being performed which will deliver both continuum and spectral line data sets, of which the first data has been publicly released. In addition, a time domain transient and pulsar survey covering 15,000 deg2 is in progress. An overview of the Apertif science drivers, hardware and software of the upgraded telescope is presented, along with its key performance characteristics. ","Apertif, Phased Array Feeds for the Westerbork Synthesis Radio Telescope"
14,1443832227423531043,1258092781232427009,Paul Breiding,"['New preprint online: <LINK>\n➡️ a paper, in which we show how to multiply zonoids (a special class on convex bodies). This is my first work in convex geometry! 🙋\u200d♂️']",https://arxiv.org/abs/2109.14996,"We show that every multilinear map between Euclidean spaces induces a unique, continuous, Minkowski multilinear map of the corresponding real cones of zonoids. Applied to the wedge product of the exterior algebra of a Euclidean space, this yields a multiplication of zonoids, defining the structure of a commutative, associative, and partially ordered ring, which we call the zonoid algebra. This framework gives a new perspective on classical objects in convex geometry, and it allows to introduce new functionals on zonoids, in particular generalizing the notion of mixed volume. We also analyze a similar construction based on the complex wedge product, which leads to the new notion of mixed $J$-volume. These ideas connect to the theory of random determinants. ","The zonoid algebra, generalized mixed volumes, and random determinants"
15,1443585448991809550,1258845082297475072,Nick André G. Johnson,['Happy to share a new paper with @dbertsim  and @RyanCoryWright on computing certifiable near optimal solutions to the Sparse Plus Low-Rank Matrix Decomposition problem: <LINK>'],https://arxiv.org/abs/2109.12701,"We study the Sparse Plus Low Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix $\mathbf{D}$ into a sparse matrix $\mathbf{Y}$ containing the perturbations plus a low rank matrix $\mathbf{X}$. SLR is a fundamental problem in Operations Research and Machine Learning arising in many applications such as data compression, latent semantic indexing, collaborative filtering and medical imaging. We introduce a novel formulation for SLR that directly models the underlying discreteness of the problem. For this formulation, we develop an alternating minimization heuristic to compute high quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We further develop a custom branch and bound routine that leverages our heuristic and convex relaxation that solves small instances of SLR to certifiable near-optimality. Our heuristic can scale to $n=10000$ in hours, our relaxation can scale to $n=200$ in hours, and our branch and bound algorithm can scale to $n=25$ in minutes. Our numerical results demonstrate that our approach outperforms existing state-of-the-art approaches in terms of the MSE of the low rank matrix and that of the sparse matrix. ","Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization
  Approach"
16,1443585421363986443,3319563187,Xiaohui Fan,"['New day, new paper: Probing Early Supermassive Black Hole Growth and Quasar Evolution with Near-infrared Spectroscopy of 37 Reionization-era Quasars at 6.3 &lt; z ≤ 7.64, <LINK>, led by @AstroJinyi and @feigewang: supermassive black holes grew early and fast..1/2 <LINK>', 'Yet quasar spectra at z~7 look almost the same as those at z~1.. 2/2 https://t.co/GL24cktlj8']",https://arxiv.org/abs/2109.13942,"We report the results of near-infrared spectroscopic observations of 37 quasars in the redshift range $6.3< z\le7.64$, including 32 quasars at $z>6.5$, forming the largest quasar near-infrared spectral sample at this redshift. The spectra, taken with Keck, Gemini, VLT, and Magellan, allow investigations of central black hole mass and quasar rest-frame ultraviolet spectral properties. The black hole masses derived from the MgII emission lines are in the range $(0.3-3.6)\times10^{9}\,M_{\odot}$, which requires massive seed black holes with masses $\gtrsim10^{3-4}\,M_{\odot}$, assuming Eddington accretion since $z=30$. The Eddington ratio distribution peaks at $\lambda_{\rm Edd}\sim0.8$ and has a mean of 1.08, suggesting high accretion rates for these quasars. The CIV - MgII emission line velocity differences in our sample show an increase of CIV blueshift towards higher redshift, but the evolutionary trend observed from this sample is weaker than the previous results from smaller samples at similar redshift. The FeII/MgII flux ratios derived for these quasars up to $z=7.6$, compared with previous measurements at different redshifts, do not show any evidence of strong redshift evolution, suggesting metal-enriched environments in these quasars. Using this quasar sample, we create a quasar composite spectrum for $z>6.5$ quasars and find no significant redshift evolution of quasar broad emission lines and continuum slope, except for a blueshift of the CIV line. Our sample yields a strong broad absorption line quasar fraction of $\sim$24%, higher than the fractions in lower redshift quasar samples, although this could be affected by small sample statistics and selection effects. ","Probing Early Super-massive Black Hole Growth and Quasar Evolution with
  Near-infrared Spectroscopy of 37 Reionization-era Quasars at 6.3 &lt; z &lt;= 7.64"
17,1443557266070851588,2432886163,Oscar Barragán,"['The paper I have been working for more than two years is out today! A new pyaneti that uses multi-dimensional Gaussian processes to constrain stellar signals in RV time-series. WHAT?! If you want to know more, check this 🧵 (and the paper!)\n\n<LINK>', 'We all know that stars are great, but they also can jeopardise exoplanetary signal detection due to their intrinsic variability. Therefore, we need techniques that allow us to disentangle stellar and planetary signals from our data. https://t.co/CAFAOfeQqJ', 'Fortunately, there are ways to mitigate activity in RV time series. One of the most sophisticated methods is the multi-dimensional Gaussian Process (MDGP) framework  of Rajpaul et al., (2015,https://t.co/bJFHPcoCtQ) to use activity indicators to constrain the stellar signal', 'In this new pyaneti paper, we aim for a more qualitative description of the MDGP method and its physical motivations.  The main idea is that evolving active regions generate stellar signals in our spectroscopic time series (RVs and activity indicators) https://t.co/NyMzLP76VL', 'Such active regions affect our time-series differently, some of them are affected by the area that is covered by active regions, i.e., they can be described with a function G(t); while others are affected by how these regions evolve in time, i.e., as G(t) AND ITS TIME DERIVATIVE https://t.co/YUKd3idAM8', 'The thing is that stellar signals in RVs behave as the derivate function (as a first approach) of the function describing photometric-like activity indicators ( e.g., Calcium II H &amp; K, FWHM). This is important when using activity indicators to constrain stellar signals in RVs! https://t.co/rGd4tB6xha', 'Well, the MDGP approach, allows one to use the same underlying function to describe the stellar signal in all time-series, and also to include time-derivatives of that function, this is ideal to jointly model spectroscopic time-series https://t.co/uueefol1rm', 'And we have seen this behaviour of the RVs acting as the derivative of the photometric-like activity indicator. For K2-100, the RV stellar signal behaves as the derivative of the function describing the stellar signal in the log R_HK, as you can see in this animation https://t.co/zpal0oMreS', 'And how to know when is relevant to model the stellar signal in the RVs as the derivate of the process describing the photometric-like time-series? Well, this may depend on the harmonic complexity of your stellar signal...', 'In the high harmonic complexity scenario, the derivative of a function with high harmonic complexity is a function with higher harmonic complexity. While in the low harmonic complexity case, stellar signals and their derivatives behave as sinusoids (with some phase shift)', 'And now you will be able to reproduce the results on K2-100 just by cloning pyaneti from its repository (to see how you need to read the paper 😉). With the new pyaneti, you can apply this for any star and any number of RVs and activity indicators.', 'The new pyaneti also runs on python 3, and allows one to do multi-band transit fitting, single transit fitting, and multi-dimensional Gaussian Process regression. Ah, and it is also faster!', 'We also present two codes called citlalicue and citlalatonac (in honour of the goddess and god who created the stars in Aztec mythology). These codes allow you to create (synthetic) photometric and spectroscopic stellar-like time-series with stellar and planetary-like signals.', 'And of course, I am not alone in this 😉 this paper is the result of a collaboration with the brilliant @AirborneGrain and @nzicher from @OxfordPhysics @oxoplanets and V. M. Rajpaul from @Cambridge_Uni.', 'And as a spoiler, stay alert that there are some upcoming new exoplanet discoveries done with this new version of pyaneti... https://t.co/Yw6FMwMNnA']",https://arxiv.org/abs/2109.14086,"The two most successful methods for exoplanet detection rely on the detection of planetary signals in photometric and radial velocity time-series. This depends on numerical techniques that exploit the synergy between data and theory to estimate planetary, orbital, and/or stellar parameters. In this work, we present a new version of the exoplanet modelling code pyaneti. This new release has a special emphasis on the modelling of stellar signals in radial velocity time-series. The code has a built-in multidimensional Gaussian process approach to modelling radial velocity and activity indicator time-series with different underlying covariance functions. This new version of the code also allows multi-band and single transit modelling; it runs on Python 3, and features overall improvements in performance. We describe the new implementation and provide tests to validate the new routines that have direct application to validate the new routines that have direct application to exoplanet detection and characterisation. We have made the code public and freely available at this https URL We also present the codes citlalicue and citlalatonac that allow one to create synthetic photometric and spectroscopic time-series, respectively, with planetary and stellar-like signals. ","pyaneti II: A multidimensional Gaussian process approach to analysing
  spectroscopic time-series"
18,1443537952626331655,3087935529,Drew Stommes,"['In a new working paper with P. M. Aronow and Fredrik Sävje, we reanalyze all studies published in the top journals in political science that use a regression discontinuity (RD) design. <LINK> [1/4]', 'We show that the literature demonstrates some pathological behavior consistent with selective reporting of findings. The figure below demonstrates that findings cluster just at or just above the t-statistic threshold of 1.96 (i.e. a p-value of less than or equal to 0.05). [2/4] https://t.co/AiONuxYIEu', 'Reanalysis of these studies using modern, automated methods [see: https://t.co/EPIW73zx1W] shows that, while the point estimates are relatively stable, uncertainty has been been systematically understated. [3/4]', 'Retrospective power analyses demonstrate that most studies were underpowered to detect all but large effect sizes. We conclude that many published findings using the RD design are exaggerated if not altogether spurious. [4/4]', ""@guygrossman We included filtering by reviewers/editors in the phrase 'selective reporting.' Perhaps 'selective reporting or publishing' would have been clearer. That said, we do explicitly note publication filtering and selective reporting in the body of the paper.""]",https://arxiv.org/abs/2109.14526,"The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009--2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious. ","On the reliability of published findings using the regression
  discontinuity design in political science"
19,1443486942788792322,1176755442,Prof Aimee Morgans,['Our new paper on acoustic absorption/generation in axially varying duct flows is out - Journal of Sound &amp; Vibration.\n<LINK>\nOA: <LINK>\nWell done to PhD researcher Sai Reddy Yeddular @sairickyking and postdoc Renaud Gaudron!'],https://arxiv.org/abs/2109.13612,"In ducts with varying cross-sectional area and sustaining a subsonic non-isentropic mean flow, the axially varying flow conditions affect the acoustic energy balance of the system. This is significant in understanding and controlling thermo-acoustic phenomena, particularly in combustors. This work aims at quantifying the acoustic energy change in such configurations, using the acoustic absorption coefficient, $\Delta$. The acoustic response of the duct to acoustic forcing is determined using an analytical model, neglecting the effect of entropy fluctuations on the acoustic field, and subsequently, $\Delta$ is estimated. The model predictions of $\Delta$ are validated using a linearised Euler equations (LEEs) solver. The model was found to be accurate for Mach numbers below $0.25$, provided the lower frequency limit set by the analytical solution is satisfied. For conically varying area ducts with linear mean temperature gradient, it was observed that $\Delta$ showed very little dependence on frequency, and that the absolute value of $\Delta$ tended to be maximised when the upstream boundary was anechoic rather than non-anechoic. More importantly, $\Delta$ was also observed to show stronger dependence on the mean temperature gradient than area gradient variation for such configurations. Further parametric and optimisation studies for $\Delta$ revealed a crucial finding that a positive mean temperature gradient, representing a heated duct caused acoustic energy absorption. Similarly, a negative mean temperature gradient, representing a cooled duct caused acoustic energy generation -- a key result of this analysis. This behaviour was shown to be consistent with a simplified analysis of the acoustic energy balance. Based on this finding, a linearly proportional reduction in acoustic energy generation was achieved by changing the mean temperature gradient. ","Acoustic absorption and generation in ducts of smoothly varying area
  sustaining a mean flow and a mean temperature gradient"
20,1443359096774742022,1137635579271831552,Ravi Kunjwal,"['We have a new paper out on the nonclassicality of multiqubit systems, demonstrating the necessity of entanglement for the Kochen-Specker theorem and related results: <LINK>\n\n#newpaper #quantum #contextuality #entanglement']",https://arxiv.org/abs/2109.13594,"The Kochen-Specker (KS) theorem reveals the nonclassicality of single quantum systems. In contrast, Bell's theorem and entanglement concern the nonclassicality of composite quantum systems. Accordingly, unlike incompatibility, entanglement and Bell non-locality are not necessary to demonstrate KS-contextuality. However, here we find that for multiqubit systems, entanglement and the violation of Bell inequalities are key to proofs of the Kochen-Specker theorem, i.e., multiqubit proofs of KS-contextuality require not only incompatibility but also entanglement. Specifically, we show that any logical proof of the KS theorem -- explicitly, any KS set -- on a multiqubit system necessarily requires entangled projections. This also implies that proving Gleason's theorem on a multiqubit system necessarily requires entangled projections, a result originally due to Wallach [Contemp Math, 305: 291-298 (2002)]. We then turn to statistical proofs of KS-contextuality and show that a multiqubit state admits such a proof if and only if it can violate a Bell inequality with projective measurements. We also establish the relationship between entanglement and the theorems of both Kochen-Specker and Gleason more generally in multiqudit systems by constructing new examples of KS sets. Finally, we discuss how our results shed new light on the role of multiqubit contextuality as a resource within the paradigm of quantum computation with state injection. ","Contextuality in composite systems: the role of entanglement in the
  Kochen-Specker theorem"
21,1443319871257948160,1079592050973061122,Pieter van Dokkum,"['A new implementation of a powerful method to fit galaxy images, by Tim Miller. Tim optimized the multi-Gaussian expansion method for faint objects and placed it in a Bayesian framework. See <LINK> for the code and <LINK>  for the paper!', 'Imcascade measures radial profiles of faint galaxies with realistic errorbars and taking the PSF into account. It also provides traditional measures such as the half-light radius, derived without assuming a particular functional form of the profile.', 'The method is demonstrated with model galaxies and also with real nearby galaxies, degraded so the data mimic HST observations of distant ones. This figure shows that both the light profiles and color profiles are reproduced quite well. https://t.co/YXFZz4Pyb6', ""Tim is now applying the method to the images of galaxies in the 3D-HST survey. The goal is to see if we can reproduce (and hopefully expand upon) the striking and very cool results of Wren Suess - if you haven't read her papers yet, I recommend it! https://t.co/34oFPyhQCe"", ""It has been tricky to extract the full information in the beautiful HST images that we've been looking at for so many years now. We hope that this will help the community - also with an eye to the riches that will hopefully come our way with the James Webb Space Telescope!""]",https://arxiv.org/abs/2109.13262,"Fitting parameterized models to images of galaxies has become the standard for measuring galaxy morphology. This forward modelling technique allows one to account for the PSF to effectively study semi-resolved galaxies. However, using a specific parameterization for a galaxy's surface brightness profile can bias measurements if it is not an accurate representation. Furthermore, it can be difficult to assess systematic errors in parameterized profiles. To overcome these issues we employ the Multi-Gaussian expansion (MGE) method of representing a galaxy's profile together with a Bayesian framework for fitting images. MGE flexibly represents a galaxy's profile using a series of Gaussians. We introduce a novel Bayesian inference approach which uses pre-rendered Gaussian components, which greatly speeds up computation time and makes it feasible to run the fitting code on large samples of galaxies. We demonstrate our method with a series of validation tests. By injecting galaxies, with properties similar to those observed at $z\sim1.5$, into deep HST observations we show that it can accurately recover total fluxes and effective radii of realistic galaxies. Additionally we use degraded images of local galaxies to show that our method can recover realistic galaxy surface brightness and color profiles. Our implementation is available in an open source python package $\texttt{imcascade}$, which contains all methods needed for the preparation of images, fitting and analysis of results. ",Bayesian fitting of multi-Gaussian expansion models to galaxy images
22,1443268136053067776,4639078397,John Wise,"[""New paper day! Many thanks to my co-authors (@jaregan @bwoshea @TurloughDownes, Norman, Willott), in particular the lead author Tyrone Woods\n\nIf some of the first stars were really massive, this is how we'd detect them with JWST and LUVOIR <LINK> <LINK>""]",https://arxiv.org/abs/2109.13279,"Identifying stars formed in pristine environments (Pop III) within the first billion years is vital to uncovering the earliest growth and chemical evolution of galaxies. Pop III galaxies, however, are typically expected to be too faint and too few in number to be detectable by forthcoming instruments without extremely long integration times and/or extreme lensing. In an environment, however, where star formation is suppressed until a halo crosses the atomic cooling limit (e.g., by a modest Lyman-Werner flux, high baryonic streaming velocities, and/or dynamical heating effects),primordial halos can form substantially more numerous and more massive stars. Some of these stars will in-turn be accreting more rapidly than they can thermally relax at any given time. Using high resolution cosmological zoom-in simulations of massive star formation in high-z halos, we find that such rapidly accreting stars produce prominent spectral features which would be detectable by {\it JWST}. The rapid accretion episodes within the halo lead to stochastic reprocessing of 0--20\% of the total stellar emission into the rest-frame optical over long timescales, a unique signature which may allow deep observations to identify such objects out to $z \sim 10-13$ using mid- and wide-band NIRCam colors alone. ","Some First Stars Were Red: Detecting Signatures of Massive Population
  III Formation Through Long-Term Stochastic Color Variations"
23,1443259060476776451,101171811,LIGO Hanford,['Check out the new results of a search for subsolar-mass #blackholes from #LIGOVirgo data from the first part of Observing Run #3 (aka O3 from April to Oct 2019).\nRead more here:\nThe Paper (<LINK>)\n&amp;\nScience Summary (<LINK>)\n#GravitationalWaves <LINK>'],https://arxiv.org/abs/2109.12197,"We report on a search for compact binary coalescences where at least one binary component has a mass between 0.2 $M_\odot$ and 1.0 $M_\odot$ in Advanced LIGO and Advanced Virgo data collected between 1 April 2019 1500 UTC and 1 October 2019 1500 UTC. We extend previous analyses in two main ways: we include data from the Virgo detector and we allow for more unequal mass systems, with mass ratio $q \geq 0.1$. We do not report any gravitational-wave candidates. The most significant trigger has a false alarm rate of 0.14 $\mathrm{yr}^{-1}$. This implies an upper limit on the merger rate of subsolar binaries in the range $[220-24200] \mathrm{Gpc}^{-3} \mathrm{yr}^{-1}$, depending on the chirp mass of the binary. We use this upper limit to derive astrophysical constraints on two phenomenological models that could produce subsolar-mass compact objects. One is an isotropic distribution of equal-mass primordial black holes. Using this model, we find that the fraction of dark matter in primordial black holes is $f_\mathrm{PBH} \equiv \Omega_\mathrm{PBH} / \Omega_\mathrm{DM} \lesssim 6\%$. The other is a dissipative dark matter model, in which fermionic dark matter can collapse and form black holes. The upper limit on the fraction of dark matter black holes depends on the minimum mass of the black holes that can be formed: the most constraining result is obtained at $M_\mathrm{min}=1 M_\odot$, where $f_\mathrm{DBH} \equiv \Omega_\mathrm{PBH} / \Omega_\mathrm{DM} \lesssim 0.003\%$. These are the tightest limits on spinning subsolar-mass binaries to date. ","Search for subsolar-mass binaries in the first half of Advanced LIGO and
  Virgo's third observing run"
24,1443245005150330882,958312958593064961,Mikayel Samvelyan,"[""Creating rich and complex environments for RL has never been easier!\n\nI'm excited to introduce MiniHack: A Sandbox for Open-Ended Reinforcement Learning Research.\n\nCode: <LINK>\nPaper: <LINK>\nBlogpost: <LINK> <LINK> <LINK>"", 'MiniHack provides a toolbox for easily and effortlessly creating custom RL environments ranging from small rooms to complex, procedurally generated worlds that use a wealth of pre-existing assets (&gt;500 monsters &amp; &gt;400 objects) from NetHack, one of the hardest game in the world. https://t.co/IFFmajmDxp', 'With just a few lines of human-readable description files, people can generate a large variety of environments, controlling every little detail, from the location and types of monsters, to the traps, objects, and terrain of the level, all while introducing stochasticity to tasks https://t.co/e1jBCIMZ6j', 'Engineers can easily create a universe of tasks that challenge RL methods and are targeted at specific problems within RL. \n\nThe availability of multi-modal observations, its speed and ease of use, make MiniHack an appealing framework for a variety of different RL problems. https://t.co/XY0JFzBs5V', 'MiniHack also enables the porting of existing grid-based benchmarks under one roof. We show how prior testbeds such as MiniGrid and Boxoban can be ported to MiniHack. and made more challenging by adding additional entities, environment features, and randomness. https://t.co/GbqmD1h8MZ', 'This is a joint work with an amazing group of people: \n@_robertkirk, @y0b1byte, @jparkerholder, @MinqiJiang, @erichammy, @Fabio_Petroni, @HeinrichKuttler, @egrefen &amp; @_rockt.\n\nP.S. MiniHack has been accepted to @NeurIPSConf 2021 (Datasets and Benchmarks) 🎉 \n\n#NeurIPS2021']",https://arxiv.org/abs/2109.13202,"Progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity. ","MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning
  Research"
25,1443244962296971265,68538286,Dan Hendrycks,"['How can we productively work toward creating safe machine learning models?\nAfter struggling with this question for the past several years, we have developed a new roadmap for ML safety.\n\nPost: <LINK>\nPaper: <LINK> <LINK>']",http://arxiv.org/abs/2109.13916,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), reducing inherent model hazards (""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions. ",Unsolved Problems in ML Safety
26,1443238280510668805,4747768463,Cam Buzard,"['New paper out today! We looked back at some previous work from our group and came to some different conclusions. I suppose science is meant to be a learning process after all..\n <LINK>', 'I thought it was important to get this out, but it was a terribly disheartening process. I dunno what else to say about it..']",https://arxiv.org/abs/2109.13275,"We reanalyze the multi-epoch direct detections of HD 88133 b and ups And b that were published in Piskorz et al. 2016 and Piskorz et al. 2017, respectively. Using simulations to attempt to reproduce the detections, we find that with the 6 and 7 $L$ band Keck/NIRSPEC epochs analyzed in the original works, the planets would not have been detectable unless they had unreasonably large radii. HD88133 and ups And both have fairly large stellar radii, which contributed to the difficulty in detecting the planets. We take this opportunity to consider how these planets may have been detectable with the small number of epochs originally presented by running simulations both with the upgraded NIRSPEC instrument and with near-zero primary velocities, as recommended by Buzard et al. 2021. While 7 $L$ band NIRSPEC2.0 epochs with near-zero primary velocities would have allowed a strong ($10.8\sigma$) detection of ups And b, many more than 6 $L$ band epochs would have been required for a strong detection of HD88133b, which could be due in part to both this system's large stellar radius and low stellar temperature. This work stresses the importance of careful analytic procedures and the usefulness of simulations in understanding the expected sensitivity of high-resolution spectroscopic data. ","Reinvestigation of the Multi-Epoch Direct Detections of HD 88133 b and
  Upsilon Andromedae b"
27,1443225724559806467,356676252,John Regan,"['New paper on the @arxiv - <LINK> Some of the most massive first stars in the Universe were red and very bright! Tyrone Woods led a project where we looked at the emission spectrum from rapidly accreting massive stars and the great news is that @JWSTObserver <LINK>', 'should be able to see these distant early massive galaxies (with a bit of luck). Thanks to all of the co-authors as always: Tyrone Woods (lead author), Chris Willott, @AstroAhura @TurloughDownes @bwoshea &amp; Mike Norman @MUTheorPhys @royalsociety @scienceirel https://t.co/fedKi7r0Bg']",https://arxiv.org/abs/2109.13279,"Identifying stars formed in pristine environments (Pop III) within the first billion years is vital to uncovering the earliest growth and chemical evolution of galaxies. Pop III galaxies, however, are typically expected to be too faint and too few in number to be detectable by forthcoming instruments without extremely long integration times and/or extreme lensing. In an environment, however, where star formation is suppressed until a halo crosses the atomic cooling limit (e.g., by a modest Lyman-Werner flux, high baryonic streaming velocities, and/or dynamical heating effects),primordial halos can form substantially more numerous and more massive stars. Some of these stars will in-turn be accreting more rapidly than they can thermally relax at any given time. Using high resolution cosmological zoom-in simulations of massive star formation in high-z halos, we find that such rapidly accreting stars produce prominent spectral features which would be detectable by {\it JWST}. The rapid accretion episodes within the halo lead to stochastic reprocessing of 0--20\% of the total stellar emission into the rest-frame optical over long timescales, a unique signature which may allow deep observations to identify such objects out to $z \sim 10-13$ using mid- and wide-band NIRCam colors alone. ","Some First Stars Were Red: Detecting Signatures of Massive Population
  III Formation Through Long-Term Stochastic Color Variations"
28,1443215691721674764,22046694,Jonathan Mummolo,"[""Our new working paper: tell the computer what you know (and don't know!) about a causal question w/ discrete data → automatically get most precise possible answer (bounds, or a point estimate). Joint w/ @guilhermejd1 @nsfinkelstein @dean_c_knox Shpitser.🧵<LINK> <LINK>"", 'Scholars have developed many causal research designs (SOO, IV, DiD, RD, RCTs, etc.). All rely on bundles of assumptions. But applied settings are messy. What if an assumption fails? Most common choices: give up, or ignore the problem. 2/', 'But you learned a ton about a topic. You designed a study. You gathered data. Even if things didn’t go perfectly, you ought to be able to learn something. Data and area expertise should at least allow you to rule out some possible answers. 3/', 'For that, we need partial identification: finding the range of possible causal effects (given what we know from data &amp; defensible assumptions). Even if we can’t obtain a single answer, (a point-identified solution), we can make a contribution by narrowing the possibilities. 4/', 'Problem: bounding causal quantities is hard! Sometimes intractable. Here are the derivations of bounds on racial bias in policing in a previous paper w/ @conjugateprior &amp; @dean_c_knox. Change an assumption, &amp; we’d have to do it all over again. 5/ https://t.co/33tinJtbIj https://t.co/ki9uKCGlRr', 'To accelerate scientific progress, we need an automated solution. Scholars have been seeking one for decades. Balke &amp; @yudapearl (1997) shows some causal questions can be expressed as linear programming problems &amp; solved computationally. But many problems don’t fit this mold. 6/ https://t.co/Us3CtDZ5vK', 'Our algorithm provides a solution. It automatically derives causal bounds for problems involving discrete data. You supply a causal theory (DAG), estimand, assumptions &amp; data. “Autobounds” does the rest. It even returns a point-identified solution, if one exists. 7/', 'How does this work? Intuitively, if data are discrete, unobservables can only distort the observed data in so many ways. Assumptions restrict unobservables even further. The question then becomes: what’s the largest/smallest possible causal effect given all these constraints? 8/', 'This means causal inference boils down to a constrained optimization problem. If we can figure out how to ask a computer to find the most extreme values of a causal quantity that are consistent w/ data &amp; assumptions, we can automate causal discovery in any setting. 9/', 'We first show how to rewrite essentially every causal question w/ discrete data as a polynomial programming problem. Once written this way, causal queries become a tractable (but still hard!) optimization task. 10/', 'Our algo uses primal-dual methods, which have desirable properties. 1) Run algo to completion &amp; get sharpest possible answer. 2) The algo is “anytime”: no matter when you stop running it, bounds contain the truth (but might not be as sharp as possible).https://t.co/IM17gJLrNM 11/', 'My favorite feature: it can alert you to faulty theory! If you declare a causal theory &amp; its observable implications are falsified by the data, the algo will tell you that no solution exists. We show this w/ classic instrumental variables simulations. 12/ https://t.co/9ooiUdEDM1', 'In addition, if you don’t run the algo to completion, we show how to characterize the worst-case looseness of the resulting non-sharp bounds. (We call this ε-sharpness). We also show how to characterize uncertainty in the estimated bounds due to sampling error. 13/ https://t.co/na5slx8zqG', 'We validate our approach against several known analytic solutions, and show how it easily accommodates common research obstacles including confounding, measurement error, missing data, and nonresponse/attrition. 14/ https://t.co/DVV0Hwp56J', 'Here is one example of outcome-based selection. An analytic derivation of sharp bounds was recently published in JASA: https://t.co/Tcp0XTRhY2 \n \nAfter supplying the parameters of the problem, our algo recovered the previously derived sharp bounds in 0.6 seconds. 15/ https://t.co/Ik8NwSGDKv', 'Here’s one w/ dependent missingness on X and Y. An analytic solution was recently in Biometrika https://t.co/ikYpsXuvBE \n \nOur algo found the previously derived point-identified answer in 4 seconds. Manski bounds (black error bar below) are much wider. 16/ https://t.co/dsngNxDb4b', 'There are a number of reasonable critiques people might raise here. What if we don’t know the true causal model? What about continuous variables? What if the bounds are too wide to be informative? We discuss these concerns, and are eager to hear more. 17/ https://t.co/rkAd5wbgsy', 'Our hope is that this approach helps researchers better navigate the peculiarities of applied settings and avoid abandoning imperfect projects, or worse, pretending obstacles to inference don’t exist. 18/', 'It may also lead to more fruitful debates about the credibility of research. If an assumption is contested, analysts can relax it &amp; quickly see how much it affects empirical conclusions. If it’s consequential, they can design a targeted follow-up study to investigate further. 19/', 'Finally, we hope this approach aids research transparency. Research rests on assumptions which often go unstated. Now we have a new, principled approach to explore their empirical consequences, hopefully accelerating the pace of discovery. (end) https://t.co/82R9FD2OqI', ""@guilhermejd1 @maqartan @nsfinkelstein @dean_c_knox To expand on this, our approach will never give you an invalid answer, no matter how hard the problem is. MCMC based approaches don't have that, they can only guarantee correctness when run for infinite time. https://t.co/8EoPPWlve0"", ""@SimLabURI @guilhermejd1 @nsfinkelstein @dean_c_knox Thanks! A bit of both. You describe what you know/believe about the DGP to use the tool, but we've already found that the algo can alert you to a faulty assumption if it's inconsistent with data &amp; other constraints. See IV simulations in paper.""]",https://arxiv.org/abs/2109.13471,"When causal quantities cannot be point identified, researchers often pursue partial identification to quantify the range of possible values. However, the peculiarities of applied research conditions can make this analytically intractable. We present a general and automated approach to causal inference in discrete settings. We show causal questions with discrete data reduce to polynomial programming problems, and we present an algorithm to automatically bound causal effects using efficient dual relaxation and spatial branch-and-bound techniques. The user declares an estimand, states assumptions, and provides data (however incomplete or mismeasured). The algorithm then searches over admissible data-generating processes and outputs the most precise possible range consistent with available information -- i.e., sharp bounds -- including a point-identified solution if one exists. Because this search can be computationally intensive, our procedure reports and continually refines non-sharp ranges that are guaranteed to contain the truth at all times, even when the algorithm is not run to completion. Moreover, it offers an additional guarantee we refer to as $\epsilon$-sharpness, characterizing the worst-case looseness of the incomplete bounds. Analytically validated simulations show the algorithm accommodates classic obstacles, including confounding, selection, measurement error, noncompliance, and nonresponse. ",An Automated Approach to Causal Inference in Discrete Settings
29,1443208642203947015,2530947115,Max Tegmark,"['Our new #AI paper shows how physics can improve #MachineLearning by complementing physics-informed learning (#PIL) with physics-augmented learning (#PAL), taking advantage of simplifying data properties that are easier to generate than test.\nThe paper: <LINK> <LINK>']",https://arxiv.org/abs/2109.13901,"Integrating physical inductive biases into machine learning can improve model generalizability. We generalize the successful paradigm of physics-informed learning (PIL) into a more general framework that also includes what we term physics-augmented learning (PAL). PIL and PAL complement each other by handling discriminative and generative properties, respectively. In numerical experiments, we show that PAL performs well on examples where PIL is inapplicable or inefficient. ","Physics-Augmented Learning: A New Paradigm Beyond Physics-Informed
  Learning"
30,1443195708614475776,21859920,Ben Rubinstein,"['🎺 new work with @NGMarchant @ScottAlfeld on machine *un*learning 🤖 and an unusual kind of adversarial learning attack that increases compute time 😴. ""Hard to Forget: Poisoning Attacks on Certified Machine Unlearning"". Paper link: <LINK> A short 🧵⬇️ 1/ <LINK>', 'The right to erasure is enshrined in legislation (GDPR) ⚖️. While the FTC this year ruled that an app developer had to not only remove photos from its possession, but also any downstream algorithms/models trained on them 🗑️. https://t.co/PAJsJkNVBd 2/', 'When consent is withdrawn from previous shared data, an organisation may be faced with the task of retraining machine learning models 🤖. Some of these models are big and expense to train 💸⌛️. https://t.co/EEY9oKbzrb What to do? 3/', 'Enter ""machine unlearning"": learn an approx model similar to an original model (utility! 😎), updatable with minimal fuss (speed! 🏎️), can remove a requested training point - update is *certifiably* indistinguishable from retrained model (privacy! 🕵️) https://t.co/sMsc104rqP 4/', 'Not so fast! We find a problem: privacy-preserving noise accumulates in the unlearned model over time, can mean full retraining 😓. While still a net time savings in ideal settings, we find strategically chosen points can poison unlearning, forcing frequent retrainings ⌛️. 5/', 'What does this mean? We can still have privacy through retraining (yay we love privacy!), but this might come at more compute cost than previously thought. 6/6']",https://arxiv.org/abs/2109.08266,"The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine ""unlearning"" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed. ",Hard to Forget: Poisoning Attacks on Certified Machine Unlearning
31,1443195150122029062,17880404,Relinde Jurrius,"[""New paper alert! The direct sum of q-matroids: <LINK>\n\nNow I hear you thinking: a whole paper on the direct sum? Isn't that, like, one of the easiest constructions in mathematics? Well, not if you consider q-analogues..."", 'For sets, the disjoint union E=E1⊔E2 has the nice property that we can write any subset A⊆E as a disjoint union A=A1⊔A2 with A1⊆E1 and A2⊆E2.\n\nIn vector spaces: not so much... Most subspaces of the direct sum E1⊕E2 of vector spaces cannot be decomposed in such a way.', ""For classical matroids, you define the rank of a set in the direct sum as the sum of the ranks in the parts. You can do something similar for vector spaces but then you are left with a whole lot of spaces that don't get a rank. So you have to `fill in the gaps'. That's hard."", ""A significant part of this paper explains why seemingly obvious constructions don't work, so you don't have to try them again. But it also contains a solution! And a whole appendix of small examples.\n\nSo, please read and enjoy! (And tell us what you think!)""]",https://arxiv.org/abs/2109.13637,"For classical matroid, the direct sum is one of the most straightforward methods to make a new matroid out of existing ones. This paper defines a direct sum for $q$-matroids, the $q$-analogue of matroids. This is a lot less straightforward than in the classical case, as we will try to convince the reader. With the use of $q$-polymatroids and the $q$-analogue of matroid union we come to a definition of the direct sum of $q$-matroids. As a motivation for this definition, we show it has some desirable properties. ",The direct sum of $q$-matroids
32,1443150600376565765,31130787,Max Jaderberg,"['We released a new paper improving Population Based Training (PBT): Faster Improvement Rate (FIRE) PBT <LINK>. PBT allows automated adaptation of hyperparameters within the time of a single experiment run (the image shows learning rate on ResNet-50 ImageNet) [1/n] <LINK>', 'However PBT is inherently greedy, making changes to hyperparameters that are good in the short-term but may not be best in the long-term. In this new method, the assumption is that networks with a faster rate of improvement will lead to better long-term performance [2/n]', 'By using a hierarchy of sub-populations and a robust measure of improvement rate, FIRE PBT can evolve networks and hyperparameters during training to get better long-term training performance [3/n] https://t.co/U8aYj2CsO7', 'The result is an automated way to match highly tuned hyperparameter schedules in the time of a single experiment run. This is especially difficult to achieve on well established, highly tuned supervised learning setups such as ResNet-50 ImageNet shown here https://t.co/lHRJWEhiqJ', 'The same method can be applied identically to reinforcement learning problems such as these continuous control domains https://t.co/c2kMxrxBIS', ""There's many more details in the paper. Congrats to Valentin Dalibard at @DeepMind for this work! https://t.co/djQWeBxKQN""]",https://arxiv.org/abs/2109.13800,"The successful training of neural networks typically involves careful and time consuming hyperparameter tuning. Population Based Training (PBT) has recently been proposed to automate this process. PBT trains a population of neural networks concurrently, frequently mutating their hyperparameters throughout their training. However, the decision mechanisms of PBT are greedy and favour short-term improvements which can, in some cases, lead to poor long-term performance. This paper presents Faster Improvement Rate PBT (FIRE PBT) which addresses this problem. Our method is guided by an assumption: given two neural networks with similar performance and training with similar hyperparameters, the network showing the faster rate of improvement will lead to a better final performance. Using this, we derive a novel fitness metric and use it to make some of the population members focus on long-term performance. Our experiments show that FIRE PBT is able to outperform PBT on the ImageNet benchmark and match the performance of networks that were trained with a hand-tuned learning rate schedule. We apply FIRE PBT to reinforcement learning tasks and show that it leads to faster learning and higher final performance than both PBT and random hyperparameter search. ",Faster Improvement Rate Population Based Training
33,1443130921700536321,4201214013,James Wootton,"['New paper today!\n\nMy take on QEC in heavy-hexagon lattices. In the 3 years that I’ve procrastinated the threshold calculations, the hardware has developed enough to let me actually run the code. So I rounded out the paper by ranking the devices instead.\n\n<LINK> <LINK>']",http://arxiv.org/abs/2109.13308,"Matching codes are stabilizer codes based on Kitaev's honeycomb lattice model. The hexagonal form of these codes are particularly well-suited to the heavy-hexagon device layouts currently pursued in the hardware of IBM Quantum. Here we show how the stabilizers of the code can be measured solely through the 2-body measurements that are native to the architecture. The process is then run on 27 and 65 qubit devices, to compare results with simulations for a standard error model. It is found that the results correspond well to simulations where the noise strength is similar to that found in the benchmarking of the devices. The best devices show results consistent with a noise model with an error probability of around $1.5\%-2\%$. ",Hexagonal matching codes with 2-body measurements
34,1443044522380779525,1015190157845258240,Yuji Matsumoto,['Our new paper! <LINK> \nWe calculate the size evolution of protoplanets through giant impacts and photoevaporation. We show the size distribution and the size ratios of planets as functions of the initial envelope fraction and mass of protoplanets. <LINK>'],https://arxiv.org/abs/2109.13487,"The Kepler transit survey with follow-up spectroscopic observations has discovered numerous super-Earth sized planets and revealed intriguing features of their sizes, orbital periods, and their relations between adjacent planets. For the first time, we investigate the size evolution of planets via both giant impacts and photoevaporation to compare with these observed features. We calculate the size of a protoplanet, which is the sum of its core and envelope sizes, by analytical models. $N$-body simulations are performed to evolve planet sizes during the giant impact phase with envelope stripping via impact shocks. We consider the initial radial profile of the core mass and the initial envelope mass fractions as parameters. Inner planets can lose their whole envelopes via giant impacts, while outer planets can keep their initial envelopes since they do not experience giant impacts. Photoevaporation is simulated to evolve planet sizes afterward. Our results suggest that the period-radius distribution of the observed planets would be reproduced if we perform simulations in which the initial radial profile of the core mass follows a wide range of power-law distributions and the initial envelope mass fractions are $\sim0.1$. Moreover, our model shows that the adjacent planetary pairs have similar sizes and regular spacings, with slight differences from detailed observational results such as the radius gap. ","Size evolution of close-in super-Earths through giant impacts and
  photoevaporation"
35,1443022428951785475,61623544,Dr./Prof. Renée Hložek,"['New paper alert! In this work led by awesome student Gerrit Farren we estimate the kSZ signature of ultra light axions. <LINK>', 'If you care about dark matter, you know the name of the game is distinguishing axions from dark matter - so we either want to detect it or we want to rule out a universe that contains a lot of axions! This plot shows how our limits will keep getting tighter #darkmatter https://t.co/vUlB1jrANB', 'Future experiments like @SimonsObs, CMBS4 and  @desisurvey will be pivotal for these constraints', 'It’s a great time to be a cosmologist!']",https://arxiv.org/abs/2109.13268,"Measurements of secondary cosmic microwave background (CMB) anisotropies, such as the Sunyaev-Zel'dovich (SZ) effect, will enable new tests of neutrino and dark sector properties. The kinetic SZ (kSZ) effect is produced by cosmological flows, probing structure growth. Ultra-light axions (ULAs) are a well-motivated dark-matter candidate. Here the impact of ULA dark matter (with mass $10^{-27}~{\rm eV}$ to $10^{-23}~{\rm eV}$) on kSZ observables is determined, applying new analytic expressions for pairwise cluster velocities and Ostriker-Vishniac signatures in structure-suppressing models. For the future CMB-S4 and ongoing DESI galaxy surveys, the kSZ effect (along with primary anisotropies) will probe ULA fractions $\eta_a = \Omega_{\rm{axion}}/\Omega_{\rm DM}$ as low as $\sim 5\%$ if $m_{a}\simeq 10^{-27}~{\rm eV}$ (at 95\% C.L.), with sensitivity extending up to $m_{a}\simeq 10^{-25}~{\rm eV}$. If reionization and the primary CMB can be adequately modeled, Ostriker-Vishniac measurements could probe values $\eta_{a}\simeq 10^{-3}$ if $10^{-27}~{\rm eV}\lesssim m_{a}\lesssim 10^{-24}~{\rm eV}$, or $\eta_{a}\simeq 1$ if $m_{a}\simeq 10^{-22}~{\rm eV}$, within the fuzzy dark matter window. ",Ultra-light axions and the kinetic Sunyaev-Zel'dovich Effect
36,1443004288129667073,15163166,Sherri Rose,"['Our new paper, led by @IDegtiar, develops machine learning estimators for generalizability with observational &amp; randomized data <LINK>\n\nThese methods were motivated by our interest in assessing plan-specific effects on 💲 in Medicaid\n\nCode <LINK> <LINK>', '@BhramarBioStat @IDegtiar @StanfordHP @timothyjlayton @jwswallace @StanfordHAI @StanfordMed @HarvardBiostats @Stanford @MathematicaNow 👏👏👏 @IDegtiar just joined @MathematicaNow following her doctoral graduation from @HarvardBiostats!']",https://arxiv.org/abs/2109.13288,"While much of the causal inference literature has focused on addressing internal validity biases, both internal and external validity are necessary for unbiased estimates in a target population of interest. However, few generalizability approaches exist for estimating causal quantities in a target population when the target population is not well-represented by a randomized study but is reflected when additionally incorporating observational data. To generalize to a target population represented by a union of these data, we propose a class of novel conditional cross-design synthesis estimators that combine randomized and observational data, while addressing their respective biases. The estimators include outcome regression, propensity weighting, and double robust approaches. All use the covariate overlap between the randomized and observational data to remove potential unmeasured confounding bias. We apply these methods to estimate the causal effect of managed care plans on health care spending among Medicaid beneficiaries in New York City. ","Conditional Cross-Design Synthesis Estimators for Generalizability in
  Medicaid"
37,1442665425741905921,1128508767404838912,Thayne Currie,"['Our new SCExAO/CHARIS paper reporting the discovery of a low-mass companion to the A star HD 91312, led by Jeff Chilcote, Taylor Tobin, &amp; myself.\n<LINK> <LINK>']",https://arxiv.org/abs/2109.12124,"We present the SCExAO direct imaging discovery and characterization of a low-mass companion to the nearby young A7IV star, HD 91312. SCExAO/CHARIS $JHK$ (1.1-2.4 $\mu m$) spectra and SCExAO/HiCIAO $H$ band imaging identify the companion over a two year baseline in a highly inclined orbit with a maximum projected separation of 8 au. The companion, HD 91312 B, induces an 8.8-$\sigma$ astrometric acceleration on the star as seen with the Gaia & Hipparcos satellites and a long-term radial velocity trend as previously identified by Borgniet et al. (2019). HD 91312 B's spectrum is consistent with that of an early-to-mid M dwarf. Hipparcos and Gaia absolute astrometry, radial-velocity data, and SCExAO/CHARIS astrometry constrain its dynamical mass to be $0.337^{+0.042}_{-0.044}$M$_\odot$, consistent with - but far more precise than - masses derived from spectroscopy, and favors a nearly edge-on orbit with a semi-major axis of $\sim$9.7 au. This work is an example of precisely characterizing properties of low-mass companions at solar system-like scales from a combination of direct imaging, astrometry, and radial-velocity methods. ","SCExAO/CHARIS Direct Imaging of A Low-Mass Companion At A Saturn-Like
  Separation from an Accelerating Young A7 Star"
38,1442535536246222855,900584226730512384,Shiyue Zhang,"['Interested in finding a balance btw automation and human correlation for summary evaluation? Happy to share our #EMNLP2021 paper which introduces new metrics: Lite2Pyramid, Lite3Pyramid, Lite2.xPyramid\n\nw @MohitBan47 (@uncnlp)\n\n<LINK>\n\n<LINK>\n1/7 <LINK>', 'Human eval. for summarization is reliable but non-reproducible and expensive. Automatic metrics are cheap and reproducible but poorly correlated with human judgment. We propose flexible semi-automatic to automatic metrics, following Pyramid/LitePyramid human eval. protocol. 2/7', 'Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs’ presence in system summaries with a natural language inference (NLI) model. 3/7', 'Fully-automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. 4/7', 'Third, we propose in-between metrics Lite2.xPyramid, where inspired by active learning, we use a simple regressor to predict how well the STUs can simulate SCUs &amp; retain SCUs more difficult to simulate, which provides a smooth transition+balance btw automation and manual eval 5/7', 'Comparing to 15 metrics, we evaluate human-metric correlations on 4 meta-eval datasets (incl. our new PyrXSum). Lite2Pyr consistently has best summ-level correlations; Lite3Pyr works competitively; Lite2.xPyr trades off small correlation drops for larger manual effort reduction. https://t.co/ipMWfxM1N5', 'SCUs annotation or STUs extraction only needs to be done once, so they can come with data or be done in pre-processing. When they are ready &amp; one TITAN V GPU is available, it takes around 2.5mins to evaluate 500 CNN/DM examples. We provide support at: https://t.co/bWOLWVdsIN \n7/7', '(Thanks to @ani_nenkova @RPassonneau for Pyramid and @obspp18, Gabay, @AlexGaoYang, Ronen, @ramakanth1729, Bansal, Amsterdamer, Ido Dagan for LitePyramid! Thank @EasonNie @huggingface for NLI models, @_danieldeutsch @DanRothNLP for SacreROUGE, and @ai2_allennlp for SRL tools 😀)']",https://arxiv.org/abs/2109.11503,"Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs' presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose in-between metrics, Lite2.xPyramid, where we use a simple regressor to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between automation and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly-collected PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid consistently has the best summary-level correlations; Lite3Pyramid works better than or comparable to other automatic metrics; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection. Our code and data are publicly available at: this https URL ",Finding a Balanced Degree of Automation for Summary Evaluation
39,1442522254051405829,1116002690604130305,Juliette Becker,"['New on arXiv last night (and accepted to AJ last week): undergraduate Lucas Brefka’s first first-author paper: <LINK> In this paper, he studied how multi-planet systems change as their stars evolve and secular resonances sweep through the systems.', 'In this work started as part of the @UROPumich  program, he used a combination of secular theory and numerical simulations to show that for systems with ultra-short-period planets and extra outer planets, this dynamical process can recreate the observed geometry of the system.', 'Lucas went from first-day @UROPumich freshman to published author in less than two years. Lucas is applying to grad school this fall, even though he is only a third-year undergrad (graduating a year early). If you’re looking for grad students this fall, watch for his application!']",https://arxiv.org/abs/2109.12054,"Ultra-short period (USP) planets are exoplanets which have orbital periods of less than one day and are unique because they orbit inside the nominal magnetic truncation gap of their host stars. In some cases, USP planets have also been observed to exhibit unique dynamical parameters such as significant misalignments in inclination angle with respect to nearby planets. In this paper, we explore how the geometry of a multi-planet system hosting a USP planet can be expected to evolve as a star ages. In particular, we explore the relationship between the mutual inclination of the USP planet and the quadrupole moment ($J_2$) of the host star. We use secular perturbation theory to predict the past evolution of the example TOI-125 system, and then confirm the validity of our results using long-term N-body simulations. Through investigating how the misalignment between the candidate USP planet and the three other short-period planets in the TOI-125 system arose, we intend to derive a better understanding of the population of systems with misaligned USP planets and how their observed parameters can be explained in the context of their dynamical histories. ","A General Origin for Multi-Planetary Systems With Significantly
  Misaligned USP Planets"
40,1442445769571700737,251604578,Gernot Maier,['New Paper published! Gamma-ray observations of the binary HESS J0632+057 with a beautiful phase-folded light curve. <LINK> <LINK>'],https://arxiv.org/abs/2109.11894,"The results of gamma-ray observations of the binary system HESS J0632+057 collected during 450 hours over 15 years, between 2004 and 2019, are presented. Data taken with the atmospheric Cherenkov telescopes H.E.S.S., MAGIC, and VERITAS at energies above 350 GeV were used together with observations at X-ray energies obtained with Swift-XRT, Chandra, XMM-Newton, NuSTAR, and Suzaku. Some of these observations were accompanied by measurements of the H{\alpha} emission line. A significant detection of the modulation of the VHE gamma-ray fluxes with a period of 316.7+-4.4 days is reported, consistent with the period of 317.3+-0.7 days obtained with a refined analysis of X-ray data. The analysis of data of four orbital cycles with dense observational coverage reveals short timescale variability, with flux-decay timescales of less than 20 days at very high energies. Flux variations observed over the time scale of several years indicate orbit-to-orbit variability. The analysis confirms the previously reported correlation of X-ray and gamma-ray emission from the system at very high significance, but can not find any correlation of optical H{\alpha} parameters with X-ray or gamma-ray energy fluxes in simultaneous observations. The key finding is that the emission of HESS J0632+057 in the X-ray and gamma-ray energy bands is highly variable on different time scales. The ratio of gamma-ray to X-ray flux shows the equality or even dominance of the gamma-ray energy range. This wealth of new data is interpreted taking into account the insufficient knowledge of the ephemeris of the system, and discussed in the context of results reported on other gamma-ray binary systems. ","Observation of the gamma-ray binary HESS J0632+057 with the H.E.S.S.,
  MAGIC, and VERITAS telescopes"
41,1442097175454908417,226785003,Raghav Kunnawalkam Elayavalli,"['New paper time from @RHIC_STAR! <LINK> The preliminary results have been public for a few years now so i thought i would just highlight the main takeaways from the paper :) 1/x', 'This is essentially the first measurement that looks at a canonical energy loss observable (dijet asymmetry) as a function of an actual resolution scale (subjet opening angle) in the medium 2/x', 'There have been other measurements of energy loss (such as RAA) vs jet mass or mass/pT (handle of virtuality) and the Softdrop groomed jet radius but its not a scale in the medium 3/x', 'thats where this measurements comes into play. we highlight the importance of an angle observable thats sensitive to physics but also simultaneously robust to the heavy ion underlying event 4/x', 'subjets to the rescue! using a formation time argument, we come to our interpretation of the data is that a shorter medium path length and the dijet selection bias at RHIC energies, leads to ... 5/x', 'an observation of jet quenching of a single color charge! this is the QCD analogue of the LPM effect in action! Like i said before this is our understanding of the data and i would love for our community to use this data and study different hypothesis 6/x', ""my view - these types of differential measurements are how we will understand the QGP's transport properties. isolate specific populations of jet topology and systematically explore modifications in the emissions phase space! this will lead us to space-time evolution 7/x""]",https://arxiv.org/abs/2109.09793,"The STAR collaboration presents jet substructure measurements related to both the momentum fraction and the opening angle within jets in \pp and \AuAu collisions at \sqrtsn $= 200$ GeV. The substructure observables include SoftDrop groomed momentum fraction (\zg), groomed jet radius (\rg), and subjet momentum fraction (\zsj) and opening angle (\tsj). The latter observable is introduced for the first time. Fully corrected subjet measurements are presented for \pp collisions and are compared to leading order Monte Carlo models. The subjet \tsj~distributions reflect the jets leading opening angle and are utilized as a proxy for the resolution scale of the medium in \AuAu collisions. We compare data from \AuAu collisions to those from \pp which are embedded in minimum-bias \AuAu events in order to include the effects of detector smearing and the heavy-ion collision underlying event. The subjet observables are shown to be more robust to the background than \zg~and \rg. We observe no significant modifications of the subjet observables within the two highest-energy, back-to-back jets, resulting in a distribution of opening angles and the splittings that are vacuum-like. We also report measurements of the differential di-jet momentum imbalance ($A_{\rm{J}}$) for jets of varying \tsj. We find no qualitative differences in energy loss signatures for varying angular scales in the range $0.1 < $ \tsj $ < 0.3$, leading to the possible interpretation that energy loss in this population of high momentum di-jet pairs, is due to soft medium-induced gluon radiation from a single color-charge as it traverses the medium. ","Differential measurements of jet substructure and partonic energy loss
  in Au$+$Au collisions at $\sqrt{s_{\rm{NN}}} =200$ GeV"
42,1442094253417529346,1246756760146391040,Oleg Brodt,"['Apparently, a lot of AV / EDR vendors abuse DNS as a communication channel, despite known vulnerabilities. We explore this phenomenon in a new paper:\n\nOn The Vulnerability of Anti-Malware Solutions to DNS Attacks\n\n<LINK>']",https://arxiv.org/abs/2109.11342,"Anti-malware agents typically communicate with their remote services to share information about suspicious files. These remote services use their up-to-date information and global context (view) to help classify the files and instruct their agents to take a predetermined action (e.g., delete or quarantine). In this study, we provide a security analysis of a specific form of communication between anti-malware agents and their services, which takes place entirely over the insecure DNS protocol. These services, which we denote DNS anti-malware list (DNSAML) services, affect the classification of files scanned by anti-malware agents, therefore potentially putting their consumers at risk due to known integrity and confidentiality flaws of the DNS protocol. By analyzing a large-scale DNS traffic dataset made available to the authors by a well-known CDN provider, we identify anti-malware solutions that seem to make use of DNSAML services. We found that these solutions, deployed on almost three million machines worldwide, exchange hundreds of millions of DNS requests daily. These requests are carrying sensitive file scan information, oftentimes - as we demonstrate - without any additional safeguards to compensate for the insecurities of the DNS protocol. As a result, these anti-malware solutions that use DNSAML are made vulnerable to DNS attacks. For instance, an attacker capable of tampering with DNS queries, gains the ability to alter the classification of scanned files, without presence on the scanning machine. We showcase three attacks applicable to at least three anti-malware solutions that could result in the disclosure of sensitive information and improper behavior of the anti-malware agent, such as ignoring detected threats. Finally, we propose and review a set of countermeasures for anti-malware solution providers to prevent the attacks stemming from the use of DNSAML services. ",On The Vulnerability of Anti-Malware Solutions to DNS Attacks
43,1441514153261211648,195474093,Jelmer Wolterink,"['This Monday, @sukjulian will present new work on 3D wall shear stress estimation with rotation equivariant mesh CNNs at the #MICCAI STACOM workshop (<LINK>). Paper: <LINK> Code: <LINK> @pimdehaan @phillip_lippe @ChristophBrune <LINK>']",https://arxiv.org/abs/2109.04797,"Computational fluid dynamics (CFD) is a valuable tool for personalised, non-invasive evaluation of hemodynamics in arteries, but its complexity and time-consuming nature prohibit large-scale use in practice. Recently, the use of deep learning for rapid estimation of CFD parameters like wall shear stress (WSS) on surface meshes has been investigated. However, existing approaches typically depend on a hand-crafted re-parametrisation of the surface mesh to match convolutional neural network architectures. In this work, we propose to instead use mesh convolutional neural networks that directly operate on the same finite-element surface mesh as used in CFD. We train and evaluate our method on two datasets of synthetic coronary artery models with and without bifurcation, using a ground truth obtained from CFD simulation. We show that our flexible deep learning model can accurately predict 3D WSS vectors on this surface mesh. Our method processes new meshes in less than 5 [s], consistently achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5 [%] median approximation accuracy over the held-out test set, comparing favourably to previously published work. This demonstrates the feasibility of CFD surrogate modelling using mesh convolutional neural networks for hemodynamic parameter estimation in artery models. ","Mesh convolutional neural networks for wall shear stress estimation in
  3D artery models"
44,1441487726075949061,485856511,Channon Visscher,['New paper posted on the arXiv! (<LINK>)! Great work by @EGonzales788 and a nice thread here summarizing the results. <LINK>'],https://arxiv.org/abs/2109.11000,"We present the first retrieval analysis of a substellar subdwarf, SDSS J125637.13-022452.4 (SDSS J1256-0224), using the Brewster retrieval code base. We find SDSS J1256-0224 is best fit by a cloud-free model with an ion (neutral H, H-, and electron) abundance corresponding to ion [Fe/H]=-1.5. However, this model is indistinguishable from a cloud-free model with ion [Fe/H]=-2.0 and a cloud-free model with ion Fe/H]=-1.5 assuming a subsolar carbon-to-oxygen ratio. We are able to constrain abundances for water, FeH, and CrH, with an inability to constrain any carbon-bearing species likely due to the low-metallicity of SDSS J1256-0224. We also present an updated spectral energy distribution (SED) and semi-empirical fundamental parameters. Our retrieval- and SED-based fundamental parameters agree with the Baraffe low-metallicity evolutionary models. From examining our ""rejected"" models (those with $\Delta$BIC>45), we find that we are able to retrieve gas abundances consistent with those of our best-fitting model. We find the cloud in these poorer fitting ""cloudy"" models is either pushed to the bottom of the atmosphere or made optically thin. ","The first retrieval of a substellar subdwarf: A cloud-free SDSS
  J125637.13-022452.4"
45,1441444078185496577,202420697,Jeff Carver,"['Interested in how peer code review is working in the research software community? Check out this new paper with @nasireisty, which will appear in Empirical Software Engineering.\n\n<LINK>\n\n#RSE, #ResearchSoftware, #CodeReview, # SoftwareQuality,  @Se4Science <LINK>']",https://arxiv.org/abs/2109.10971,"Background: Research software is software developed by and/or used by researchers, across a wide variety of domains, to perform their research. Because of the complexity of research software, developers cannot conduct exhaustive testing. As a result, researchers have lower confidence in the correctness of the output of the software. Peer code review, a standard software engineering practice, has helped address this problem in other types of software. Aims: Peer code review is less prevalent in research software than it is in other types of software. In addition, the literature does not contain any studies about the use of peer code review in research software. Therefore, through analyzing developers perceptions, the goal of this work is to understand the current practice of peer code review in the development of research software, identify challenges and barriers associated with peer code review in research software, and present approaches to improve the peer code review in research software. Method: We conducted interviews and a community survey of research software developers to collect information about their current peer code review practices, difficulties they face, and how they address those difficulties. Results: We received 84 unique responses from the interviews and surveys. The results show that while research software teams review a large amount of their code, they lack formal process, proper organization, and adequate people to perform the reviews. Conclusions: Use of peer code review is promising for improving the quality of research software and thereby improving the trustworthiness of the underlying research results. In addition, by using peer code review, research software developers produce more readable and understandable code, which will be easier to maintain. ","Developers Perception of Peer Code Review in Research Software
  Development"
46,1441420471807852551,16837428,John Stott,['New paper out using QSAGE data to look for metal-enriched halo gas associated with galaxy overdensities <LINK>'],https://arxiv.org/abs/2109.10927,"We present a study of metal-enriched halo gas traced by MgII and CIV absorption at z<2 in the MUSE Analysis of Gas around Galaxies survey and the Quasar Sightline and Galaxy Evolution survey. Using these large and complete galaxy surveys in quasar fields, we study the dependence of the metal distribution on galaxy properties and overdensities, out to physical projected separations of 750 kpc. We find that the cool, low-ionization gas is significantly affected by the environment across the full redshift range probed, with ~2-3 times more prevalent and stronger MgII absorption in higher overdensity group environments and in regions with greater overall stellar mass and star formation rates. Complementary to these results, we have further investigated the more highly ionized gas as traced by CIV absorption, and found that it is likely to be more extended than the MgII gas, with ~2 times higher covering fraction at a given distance. We find that the strength and covering fraction of CIV absorption show less significant dependence on galaxy properties and environment than the MgII absorption, but more massive and star-forming galaxies nevertheless also show ~2 times higher incidence of CIV absorption. The incidence of MgII and CIV absorption within the virial radius shows a tentative increase with redshift, being higher by a factor of ~1.5 and ~4, respectively, at z>1. It is clear from our results that environmental processes have a significant impact on the distribution of metals around galaxies and need to be fully accounted for when analyzing correlations between gaseous haloes and galaxy properties. ","Metal-enriched halo gas across galaxy overdensities over the last 10
  billion years"
47,1441409889155301377,1559281832,Vishvas Pandey,"[""The FPF at @CERN paper is out today 👉 <LINK> A lot of SM &amp; BSM physics coming up at this new facility, including lots of exciting Neutrino Physics. 🤩 It's been a pleasure to co-convene the Neutrino part of the effort! Congrats to all the amazing authors! 🙌 <LINK>""]",https://arxiv.org/abs/2109.10905,"The Forward Physics Facility (FPF) is a proposal to create a cavern with the space and infrastructure to support a suite of far-forward experiments at the Large Hadron Collider during the High Luminosity era. Located along the beam collision axis and shielded from the interaction point by at least 100 m of concrete and rock, the FPF will house experiments that will detect particles outside the acceptance of the existing large LHC experiments and will observe rare and exotic processes in an extremely low-background environment. In this work, we summarize the current status of plans for the FPF, including recent progress in civil engineering in identifying promising sites for the FPF and the experiments currently envisioned to realize the FPF's physics potential. We then review the many Standard Model and new physics topics that will be advanced by the FPF, including searches for long-lived particles, probes of dark matter and dark sectors, high-statistics studies of TeV neutrinos of all three flavors, aspects of perturbative and non-perturbative QCD, and high-energy astroparticle physics. ","The Forward Physics Facility: Sites, Experiments, and Physics Potential"
48,1441405995968864257,1324428524,Rikard Enberg,"['New paper, with lots of people: a proposal for a facility for forward experiments at the high luminosity LHC. (I only worked on a very small part of this with implications for astroparticle physics.) <LINK>', 'If you want to calculate how much neutrinos are produced by cosmic rays that collide with the atmosphere, something that the @uw_icecube experiment is interested in, then you need to know about the particles produced in the flight direction of the beam particle.', ""We have models based on a lot of known physics, but we can't check those models against data as far out as we want, because the LHC experiments aren't sensitive there. Basically they have holes in their detectors where the beamline enters. So we have to extrapolate."", ""That's where the Forward Physics Facility would be useful. It would allow us to better pin down our predictions. Such as this one: https://t.co/u87lFVwTPM"", ""But that's only part of the interesting stuff you can do with the FPF. See this thread for example: https://t.co/j6gOLbwY13""]",https://arxiv.org/abs/2109.10905,"The Forward Physics Facility (FPF) is a proposal to create a cavern with the space and infrastructure to support a suite of far-forward experiments at the Large Hadron Collider during the High Luminosity era. Located along the beam collision axis and shielded from the interaction point by at least 100 m of concrete and rock, the FPF will house experiments that will detect particles outside the acceptance of the existing large LHC experiments and will observe rare and exotic processes in an extremely low-background environment. In this work, we summarize the current status of plans for the FPF, including recent progress in civil engineering in identifying promising sites for the FPF and the experiments currently envisioned to realize the FPF's physics potential. We then review the many Standard Model and new physics topics that will be advanced by the FPF, including searches for long-lived particles, probes of dark matter and dark sectors, high-statistics studies of TeV neutrinos of all three flavors, aspects of perturbative and non-perturbative QCD, and high-energy astroparticle physics. ","The Forward Physics Facility: Sites, Experiments, and Physics Potential"
49,1441340184679772166,127490159,Jitka Polechova,"['New paper out, with multiple #variants and #vaccines: responding to the reproduction number is significantly more efficient in preventing future outbreaks. We define a new measure rho which highlights changing relative advantage of #VOCs:  <LINK>  @MBeiglboeck <LINK>']",https://arxiv.org/abs/2109.11156,"In light of the continuing emergence of new SARS-CoV-2 variants and vaccines, we create a simulation framework for exploring possible infection trajectories under various scenarios. The situations of primary interest involve the interaction between three components: vaccination campaigns, non-pharmaceutical interventions (NPIs), and the emergence of new SARS-CoV-2 variants. Additionally, immunity waning and vaccine boosters are modeled to account for their growing importance. New infections are generated according to a hierarchical model in which people have a random, individual infectiousness. The model thus includes super-spreading observed in the COVID-19 pandemic. Our simulation functions as a dynamic compartment model in which an individual's history of infection, vaccination, and possible reinfection all play a role in their resistance to further infections. We present a risk measure for each SARS-CoV-2 variant, $\rho^\V$, that accounts for the amount of resistance within a population and show how this risk changes as the vaccination rate increases. Furthermore, by considering different population compositions in terms of previous infection and type of vaccination, we can learn about variants which pose differential risk to different countries. Different control strategies are implemented which aim to both suppress COVID-19 outbreaks when they occur as well as relax restrictions when possible. We demonstrate that a controller that responds to the effective reproduction number in addition to case numbers is more efficient and effective in controlling new waves than monitoring case numbers alone. This is of interest as the majority of the public discussion and well-known statistics deal primarily with case numbers. ",Robust models of SARS-CoV-2 heterogeneity and control
50,1441293797556686848,44583052,Bernhard Haslhofer,"['I am happy to announce our new working paper (<LINK>) on the adoption of Wasabi and Samourai, two popular #Bitcoin wallets with built-in #CoinJoin functionality. Great collaboration with Johann Stockinger, @pedrorechez, and @matteo_maffei']",https://arxiv.org/abs/2109.10229,"We present a first measurement study on two popular wallets with built-in distributed CoinJoin functionality, Wasabi and Samourai, in the context of the broader Bitcoin ecosystem. By applying two novel heuristics, we can effectively pinpoint 25,070 Wasabi and 134,569 Samourai transactions within the first 689,255 (2021-07-01) blocks. Our study reveals a somewhat steady adoption of these services and found a growing trend with a total amount of 190,777.11 mixed BTC with a value of ca. 3.02 B USD. Within the recent six months, we measured an average monthly mixing throughput of 5410.98 BTC (ca. 240.14 M USD). Among all actors, which were directly or indirectly involved in CoinJoins, we also found a lower-bound of 32 distinct exchanges and traced a lower-bound of 6683.19 BTC (ca. 95.98 M USD) mixed coins received by exchanges. Our analysis further shows that linking heuristics over Wasabi and Samourai transactions allows us to narrow down the anonymity set provided by these wallets over time. Furthermore, we estimate the number of mixing outputs that are handled in Wasabi and Samourai correspondingly over time. Overall, this is the first paper to provide a comprehensive picture of the adoption of distributed CoinJoin and to discuss implications for end-users, cryptoasset exchanges, and regulatory bodies. ","Pinpointing and Measuring Wasabi and Samourai CoinJoins in the Bitcoin
  Ecosystem"
51,1441292337456369671,1162541483431301120,James Beattie,['🧲🚨 Paper day 🚨🧲\n\nHere we update the DCF method for measuring B-field strength from the dispersion of dust polarisation angles to include the affects of compressibility in MHD turbulence.  We find significantly better performance using our new method.\n\n<LINK>'],https://arxiv.org/abs/2109.10925,"The magnetic field strength in interstellar clouds can be estimated indirectly by using the spread of dust polarization angles ($\delta \theta$). The method developed by Davis 1951 and by Chandrasekhar and Fermi 1953 (DCF) assumes that incompressible magnetohydrodynamic (MHD) fluctuations induce the observed dispersion of polarization angles, deriving $B\propto 1/\delta \theta$ (or, $\delta \theta \propto M_{A}$, in terms of the Alfv\'{e}nic Mach number). However, observations show that the interstellar medium (ISM) is highly compressible. Recently, Skalidis & Tassis 2021 (ST) relaxed the incompressibility assumption and derived instead $B\propto 1/\sqrt{\delta \theta}$ ($\delta \theta \propto M_{A}^2$). We explored what the correct scaling is in compressible and magnetized turbulence with numerical simulations. We used 26 magnetized, ideal-MHD numerical simulations with different types of forcing. The range of $M_{A}$ and sonic Mach numbers $M_{s}$ explored are $0.1 \leq M_{A} \leq 2.0$ and $0.5 \leq M_{s} \leq 20$. We created synthetic polarization maps and tested the assumptions and accuracy of the two methods. The synthetic data have a remarkable consistency with the $\delta \theta \propto M_{A}^{2}$ scaling, which is inferred by ST, while the DCF scaling fails to follow the data. The ST method shows an accuracy better than $50\%$ over the entire range of $M_{A}$ explored; DCF performs adequately only in the range of $M_{A}$ for which it has been optimized through the use of a ""fudge factor"". For low $M_{A}$, DCF is inaccurate by factors of tens. The assumptions of the ST method reflect better the physical reality in clouds with compressible and magnetized turbulence, and for this reason the method provides a much better estimate of the magnetic field strength over the DCF method. ","Why take the square root? An assessment of interstellar magnetic field
  strength estimation methods"
52,1441201023385694209,442597508,Genta Indra Winata,"['🚨New Paper Alert🚨\n""Language Models are Few-shot Multilingual Learners""\n\nWe show the cross-lingual transfer of GPT and T5 in French, German, and Spanish without any tuning!\n📰<LINK>\nA joint work with @AndreaMadotto @zlinao_lin @savvyRL @jasonyo #nlproc @mrl_2021 <LINK>', '@AndreaMadotto @zlinao_lin @savvyRL @jasonyo @mrl_2021 @pascalefung thanks @ml_collective for a great collaboration!']",https://arxiv.org/abs/2109.07684,"General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models. ",Language Models are Few-shot Multilingual Learners
53,1441085126574698496,990433714948661250,Sergey Levine,"['Offline RL lets us run RL without active online interaction, but tuning hyperparameters, model capacity etc. still requires rollouts, or validation tasks. In our new paper, we propose guidelines for *fully offline* tuning for algorithms like CQL <LINK>\n\nA thread: <LINK>', 'In supervised learning, we have training and validation sets, and this works great for tuning. There is no equivalent in RL, making tuning hard. However, with CQL, when there is too much or too little model capacity, we do get very characteristic estimated vs true Q-value curves https://t.co/s5YE6O7Ger', 'Of course, the true return is unknown during offline training, but we can still use our understanding of the trends of estimated Q-values to provide guidelines for how to adjust model capacity. These guidelines are not guaranteed to work, but seem to work well in practice. https://t.co/TTaI5x2iwP', 'We evaluate these guidelines on a simulated robotic task, and two different real-world robots, and find that it works well across the board, using the same alpha=1.0 CQL parameter and fully offline selection of model capacity, regularization, etc. https://t.co/zeCp1Gdhzt', 'A few things that I think are interesting: (1) we can do capacity/arch/hyperparam tuning *without* full OPE (which is very hard); (2) we can tune fully offline for three very different domains. But this is far from perfect, and more research is needed on better workflows.', 'This paper will be presented at CoRL 2021, with @aviral_kumar2, Anikait Singh, Stephen Tian, @chelseabfinn \n\nhttps://t.co/Yg9YDMrIA0\nhttps://t.co/WqeCOQeGQ5']",https://arxiv.org/abs/2109.10813,"Offline reinforcement learning (RL) enables learning control policies by utilizing only prior experience, without any online interaction. This can allow robots to acquire generalizable skills from large and diverse datasets, without any costly or unsafe online data collection. Despite recent algorithmic advances in offline RL, applying these methods to real-world problems has proven challenging. Although offline RL methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. In this paper, our aim is to develop a practical workflow for using offline RL analogous to the relatively well-understood workflows for supervised learning problems. To this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. Our workflow is derived from a conceptual understanding of the behavior of conservative offline RL algorithms and cross-validation in supervised learning. We demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards. Explanatory video and additional results can be found at sites.google.com/view/offline-rl-workflow ",A Workflow for Offline Model-Free Robotic Reinforcement Learning
54,1441068497031741442,790033937531703296,Yi Tay,"['New paper alert! 😀 ""Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers""\n\nWe study scaling laws of Transformers pertaining to both upstream &amp; downstream transfer by pretraining over 200+ T5 models.\n\nPaper: <LINK>\n@GoogleAI @DeepMind <LINK>', 'What we found:\n1) Scaling Laws differ in upstream/downstream. While upstream pre-training performance measured by perplexity scales with model size quite\nindependently from the model shape, the downstream performance does not. Be cautious about over indexing on perplexity scores! https://t.co/uRYjHzX99D', '2) Scaling protocols can differ in diff compute regions! A scaling strategy may work at a smaller compute region but not at large-scale (or vice versa). So iterating on scaling strategy at a smaller size and scaling up later may be a challenging endeavor.', '3) We analyze the pareto-frontier of the compute-performance trade-off of over 200+ pretrained models. We find that not all knobs are created equal. Some influence the pareto-frontier greatly while some not so much. Depth is easily one of the biggest influencers. https://t.co/F7FYcWVuLm', '4) By analyzing the pareto-frontier, we find that existing canonical configs (base/large etc) of T5 are slightly pareto-inefficient and one could achieve a much more efficient model with better performance.', '5) From our analysis we propose a scaling protocol which we call ""DeepNarrow"". We show that this protocol applies to all sizes (small-&gt;XXL). At base, we arrive at a model with 50% less parameters and 40% faster with better downstream performance. https://t.co/4pJItFrPkI', '6) To further test the generality and robustness of these findings, we also conduct experiments on the vision domain (ViT) and also 12 other diverse NLP tasks to make it a total of almost 30 NLP tasks. https://t.co/VVlH4TT4Gu', '7) Finally, we will release and open source all 100+ pretrained checkpoints to the community. This is slated for early Q4 2021. Release link in the paper PDF itself.', 'Thanks to all amazing collaborators at @GoogleAI and @DeepMind , @m__dehghani (co-first author), @Jeffy_Sailing @LiamFedus @samiraabnar @hwchung27 @sharan0909 @DaniYogatama @ashVaswani @metzlerd']",http://arxiv.org/abs/2109.10686,"There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis. ","Scale Efficiently: Insights from Pre-training and Fine-tuning
  Transformers"
55,1441046099951816705,1314936675777294336,Gregory Kell,['Excited to announce a new position paper on requirements for getting biomedical QA systems into practice.  This was written in conjunction with @ijmarshall and @byron_c_wallace and has been accepted to the MRQA workshop of EMNLP 2021.\n\narXiv link: <LINK>'],https://arxiv.org/abs/2109.10415,"Medical question answering (QA) systems have the potential to answer clinicians uncertainties about treatment and diagnosis on demand, informed by the latest evidence. However, despite the significant progress in general QA made by the NLP community, medical QA systems are still not widely used in clinical environments. One likely reason for this is that clinicians may not readily trust QA system outputs, in part because transparency, trustworthiness, and provenance have not been key considerations in the design of such models. In this paper we discuss a set of criteria that, if met, we argue would likely increase the utility of biomedical QA systems, which may in turn lead to adoption of such systems in practice. We assess existing models, tasks, and datasets with respect to these criteria, highlighting shortcomings of previously proposed approaches and pointing toward what might be more usable QA systems. ",What Would it Take to get Biomedical QA Systems into Practice?
56,1440921144387723270,199409067,Alex Teachey 齊孝嵐,"['PAPER DAY!\n \nIn this new work, @david_kipping and I leverage the power of Convolutional Neural Networks (CNNs) to search for exomoon transit signals!\n \nWe achieved up to 97% detection accuracy, and then vetted 1880 KOIs in search of exomoons!\n\nTHREAD\n<LINK>', 'What are CNNs?\n \nThink ""computer vision"". A neural network is trained iteratively to identify relevant, coherent features in the data.\n \nIn this case, we are looking for small dips in the light curve near the planet\'s transit that could indicate the transit of a moon. https://t.co/j4pnRmXwMO', ""Why CNNs?\n \nThere's a *huge* amount of data to go through in the search for exomoons, so we just can't go through it by eye.\n \nWe needs tools that are straightforward and fast to identify potentially interesting signals, which we can then follow up with detailed analysis. https://t.co/g51ShUMysk"", 'CNNs have now been employed by several teams to search for planets in the Kepler, K2, and TESS data. It works pretty well!\n \nAs we were working on this paper, we also found out another team (Alshehhi et al) was simultaneously working on applying CNNs to the exomoon search. 👍👍 https://t.co/4Q5lweR3R7', 'Our implementation is somewhat different from theirs, though, as we employ an *ensemble* of CNNs to boost classification accuracy (kind of like a random forest). It works nicely.\n \nWe also go beyond proof of concept to actually go out and look for these signals with Kepler! https://t.co/FVvJNpxT8y', 'We are capable of detecting fairly low SNR moon signals in the training / validation dataset, including moons that almost certainly would not be flagged with a ""by eye"" approach.\n \nOf course, larger moons are easier to see than small moons. https://t.co/qf7X6cpLpQ', ""So what did we find?\n \nWell, even though the pipeline performs well on the sims, we don't see many signals in the real Kepler data.\n \nWe analyzed some 57,000 transit events (all planets with P &gt; 10d), but only about 1% were flagged as containing a moon transit.\n \nIs that weird? https://t.co/OMy0aRBG25"", ""Maybe! \n\nWe generally expect moons are out there and not rare, but we don't yet know much about their occurrence rate, or how large they will be.\n \nAlso, Kepler was a fabulous telescope, but exomoons are *just barely* within reach of detection in some cases. https://t.co/B6oOAtxHFz"", 'We briefly examined several systems produced by the pipeline that looked favorable by the numbers.\n \nThe transits appear to show hints of asymmetry, but there are no super obvious signs of moon transits by eye. A couple look like they might be eclipsing binaries. https://t.co/tBYXkKFv60', 'We generate these feature maps that color code the regions of the light curve that are contributing to the moon classification -- often it\'s the planet\'s transit morphology that the CNN ""sees"" as indicative of a moon.\n \nAnyway, we stop short of calling these systems ""candidates"".', 'As the first such application of CNNs to search the real Kepler data for exomoons, we caution against reading too much into the low detection rate. It shouldn\'t be conflated with an ""occurrence rate"".\n \nBut we\'re pretty sure our pipeline is robust, so...', 'it suggests that finding exomoons is still pretty tough. Coming up with novel approaches to the problem are still needed!\n \nEven so, CNNs should continue to be a powerful tool for a first-pass on large datsets like K2 and TESS, so we can zero in on the most interesting systems.', ""Thanks for reading! \n\nIf you got this far, maybe you will enjoy reading our paper, now accepted in MNRAS. A twitter thread is, after all, a mere shadow of the paper's content. \n\nhttps://t.co/ZiISmx2Kg9"", 'And if you want to dig into the KOI vetting results, you can find them here: https://t.co/E6ljWLWzTy']",https://arxiv.org/abs/2109.10503,"Targeted observations of possible exomoon host systems will remain difficult to obtain and time-consuming to analyze in the foreseeable future. As such, time-domain surveys such as Kepler, K2 and TESS will continue to play a critical role as the first step in identifying candidate exomoon systems, which may then be followed-up with premier ground- or space-based telescopes. In this work, we train an ensemble of convolutional neural networks (CNNs) to identify candidate exomoon signals in single-transit events observed by Kepler. Our training set consists of ${\sim}$27,000 examples of synthetic, planet-only and planet+moon single transits, injected into Kepler light curves. We achieve up to 88\% classification accuracy with individual CNN architectures and 97\% precision in identifying the moons in the validation set when the CNN ensemble is in total agreement. We then apply the CNN ensemble to light curves from 1880 Kepler Objects of Interest with periods $>10$ days ($\sim$57,000 individual transits), and further test the accuracy of the CNN classifier by injecting planet transits into each light curve, thus quantifying the extent to which residual stellar activity may result in false positive classifications. We find a small fraction of these transits contain moon-like signals, though we caution against strong inferences of the exomoon occurrence rate from this result. We conclude by discussing some ongoing challenges to utilizing neural networks for the exomoon search. ",Identifying Potential Exomoon Signals with Convolutional Neural Networks
57,1440914035797549057,1110110589202956289,Alessandro Strumia,['New paper: \n- Inflation can come from points in field space where the Planck mass is small. \n- A non-generic potential is needed. \n- Quantum field theories with dimension-less couplings generically give the needed potential.\n<LINK> <LINK>'],https://arxiv.org/abs/2109.10367,"Transforming canonical scalars to the Einstein frame can give a multi-field generalization of pole inflation (namely, a scalar with a divergent kinetic term) at vanishing field-dependent Planck mass. However, to obtain an attractor, the scalar potential must obey certain non-generic conditions. These are automatically satisfied in Quantum Field Theories with dimension-less couplings. The resulting models of pole inflation have special inflationary predictions determined by the full RG running of couplings. Acceptable predictions for the tensor/scalar ratio arise for perturbative but moderately large couplings, so we explore the possible QFT runnings: to confinement, to an IR fixed point, and to a UV fixed point. ",Pole inflation from non-minimal coupling to gravity
58,1440831535788093447,1129375875856773120,Efrat Shimron,"['Check out our new paper - Subtle Inverse Crimes!\n\nThis work shows why naïve usage of open datasets leads to overly-optimistic performance of Compressed Sensing, Dictionary Learning and Deep Learning algorithms.\n\nPaper: <LINK>\nco-authors @jtsense1 @KewangKe @sm313 <LINK> <LINK>']",https://arxiv.org/abs/2109.08237,"While open databases are an important resource in the Deep Learning (DL) era, they are sometimes used ""off-label"": data published for one task are used for training algorithms for a different one. This work aims to highlight that in some cases, this common practice may lead to biased, overly-optimistic results. We demonstrate this phenomenon for inverse problem solvers and show how their biased performance stems from hidden data preprocessing pipelines. We describe two preprocessing pipelines typical of open-access databases and study their effects on three well-established algorithms developed for Magnetic Resonance Imaging (MRI) reconstruction: Compressed Sensing (CS), Dictionary Learning (DictL), and DL. In this large-scale study we performed extensive computations. Our results demonstrate that the CS, DictL and DL algorithms yield systematically biased results when naively trained on seemingly-appropriate data: the Normalized Root Mean Square Error (NRMSE) improves consistently with the preprocessing extent, showing an artificial increase of 25%-48% in some cases. Since this phenomenon is generally unknown, biased results are sometimes published as state-of-the-art; we refer to that as subtle data crimes. This work hence raises a red flag regarding naive off-label usage of Big Data and reveals the vulnerability of modern inverse problem solvers to the resulting bias. ","Subtle Data Crimes: Naively training machine learning algorithms could
  lead to overly-optimistic results"
59,1440713386224848905,21133252,Chiara Boldrini,"['Can shoe leather anthropology🐒 + interpretable ML beat black-box◼️ approaches to link prediction? Find out in our new paper w/ @_mustafatoprak, @andrepassarella, Marco Conti @IITCNR.\n(<LINK>).\n\nA thread below 🧵', 'We wanted to test whether knowledge of an individual’s social circles (as defined by Dunbar’s model, illustrated below) could prove useful in predicting new links between the nodes of a graph. https://t.co/0TsWBKQlAS', 'To verify this hypothesis, we enriched standard supervised and unsupervised learning algorithms with social circle information. As benchmarks, we used their baseline w/o circle info and two feature learning methods (node2vec/node embedding and SEAL/GNN).', 'The latter automatically learn graph properties conducive to the formation of new links.', 'For the evaluation, we downloaded two longitudinal datasets from Twitter: a thematic one involving gamers and one containing generic users. Then, using the first temporal snapshot of each dataset for training, we tried to predict the newly formed links in the second snapshot.', 'Take-home messages 🏡📚: \n\n1. Supervised circle-aware link prediction algorithms that leverage only the **2-3 strongest social relationships** (those in the first circle, C1) consistently beat SoA feature-learning methods like node2vec (node-embedding) and SEAL (GNN). https://t.co/Ot2J1K349m', '2. Social circle information proves very effective wrt the baselines in which this information is not used. https://t.co/TKZztUDH25']",https://arxiv.org/abs/2109.09190,"Being able to recommend links between users in online social networks is important for users to connect with like-minded individuals as well as for the platforms themselves and third parties leveraging social media information to grow their business. Predictions are typically based on unsupervised or supervised learning, often leveraging simple yet effective graph topological information, such as the number of common neighbors. However, we argue that richer information about personal social structure of individuals might lead to better predictions. In this paper, we propose to leverage well-established social cognitive theories to improve link prediction performance. According to these theories, individuals arrange their social relationships along, on average, five concentric circles of decreasing intimacy. We postulate that relationships in different circles have different importance in predicting new links. In order to validate this claim, we focus on popular feature-extraction prediction algorithms (both unsupervised and supervised) and we extend them to include social-circles awareness. We validate the prediction performance of these circle-aware algorithms against several benchmarks (including their baseline versions as well as node-embedding- and GNN-based link prediction), leveraging two Twitter datasets comprising a community of video gamers and generic users. We show that social-awareness generally provides significant improvements in the prediction performance, beating also state-of-the-art solutions like node2vec and SEAL, and without increasing the computational complexity. Finally, we show that social-awareness can be used in place of using a classifier (which may be costly or impractical) for targeting a specific category of users. ","Harnessing the Power of Ego Network Layers for Link Prediction in Online
  Social Networks"
60,1440676948460462100,738769492122214400,Johannes Lischner,"['In our new paper with the Crommie group from @BerkeleyPhysics, we demonstrate how to control the density of adsorbed molecules on #graphene via an applied electric field. Read here: <LINK> @ImpMaterials <LINK>']",https://arxiv.org/abs/2109.07631,"The spatial arrangement of adsorbates deposited onto a clean surface in vacuum typically cannot be reversibly tuned. Here we use scanning tunneling microscopy to demonstrate that molecules deposited onto graphene field-effect transistors exhibit reversible, electrically-tunable surface concentration. Continuous gate-tunable control over the surface concentration of charged F4TCNQ molecules was achieved on a graphene FET at T = 4.5K. This capability enables precisely controlled impurity doping of graphene devices and also provides a new method for determining molecular energy level alignment based on the gate-dependence of molecular concentration. The gate-tunable molecular concentration can be explained by a dynamical molecular rearrangement process that reduces total electronic energy by maintaining Fermi level pinning in the device substrate. Molecular surface concentration in this case is fully determined by the device back-gate voltage, its geometric capacitance, and the energy difference between the graphene Dirac point and the molecular LUMO level. ","Imaging reconfigurable molecular concentration on a graphene
  field-effect transistor"
61,1440664454178377734,1254620813036158977,Shunya Noda,"['Junpei Komiyama and I released a new working paper, ""Deviation-Based Learning""! In this paper, we consider a method to train recommender systems based on whether users followed or deviated from recommendations. Check it out! <LINK>']",https://arxiv.org/abs/2109.09816,"We propose deviation-based learning, a new approach to training recommender systems. In the beginning, the recommender and rational users have different pieces of knowledge, and the recommender needs to learn the users' knowledge to make better recommendations. The recommender learns users' knowledge by observing whether each user followed or deviated from her recommendations. We show that learning frequently stalls if the recommender always recommends a choice: users tend to follow the recommendation blindly, and their choices do not reflect their knowledge. Social welfare and the learning rate are improved drastically if the recommender abstains from recommending a choice when she predicts that multiple arms will produce a similar payoff. ",Deviation-Based Learning
62,1440653985883459584,123172471,홍성욱 Sungwook E Hong,['New ApJ accepted paper: Panspermia in a Milky Way-like Galaxy <LINK> <LINK>'],https://arxiv.org/abs/2109.08926,"We study the process of panspermia in Milky Way-like galaxies by modeling the probability of successful travel of organic compounds between stars harboring potentially habitable planets. To this end, we apply the modified habitability recipe of Gobat & Hong (2016) to a model galaxy from the MUGS suite of zoom-in cosmological simulations. We find that, unlike habitability, which only occupies narrow dynamic range over the entire galaxy, the panspermia probability can vary be orders of magnitude between the inner ($R, b = 1-4 {\rm kpc}$) and outer disk. However, only a small fraction of star particles have very large values of panspermia probability and, consequently, the fraction of star particles where the panspermia process is more effective than prebiotic evolution is much lower than from na\""ive expectations based on the ratio between panspermia probability and natural habitability. ",Panspermia in a Milky Way-like Galaxy
63,1440605094068703234,15242431,André Meyer-Vitali 👁️👁️,['Pre-print of our new paper to extend Modular Design Patterns for Hybrid Learning and Reasoning to Actors. #NeurIPS2021 #neurosymbolicai #multiagentsystems #mas #ai #patterns #softwareengineering #hmi #trustworthyai #trust #transparency #hybrid #agents  <LINK>'],https://arxiv.org/abs/2109.09331,"Recently, a boxology (graphical language) with design patterns for hybrid AI was proposed, combining symbolic and sub-symbolic learning and reasoning. In this paper, we extend this boxology with actors and their interactions. The main contributions of this paper are: 1) an extension of the taxonomy to describe distributed hybrid AI systems with actors and interactions; and 2) showing examples using a few design patterns relevant in multi-agent systems and human-agent interaction. ",Modular Design Patterns for Hybrid Actors
64,1440582755025244165,735386827578875904,siegfried Vanaverbek,['A new paper on the discovery of the transiting exoplanet TOI-1201b with Diana Kossakowski and the CARMENES team is on archiv today: <LINK>'],https://arxiv.org/abs/2109.09346,"We present the discovery of a transiting mini-Neptune around TOI-1201, a relatively bright and moderately young early M dwarf ($J \approx$ 9.5 mag, $\sim$600-800 Myr) in an equal-mass $\sim$8 arcsecond-wide binary system, using data from the Transiting Exoplanet Survey Satellite (TESS), along with follow-up transit observations. With an orbital period of 2.49 d, TOI-1201 b is a warm mini-Neptune with a radius of $R_\mathrm{b} = 2.415\pm0.090 R_\oplus$. This signal is also present in the precise radial velocity measurements from CARMENES, confirming the existence of the planet and providing a planetary mass of $M_\mathrm{b} = 6.28\pm0.88 M_\oplus$ and, thus, an estimated bulk density of $2.45^{+0.48}_{-0.42}$ g cm$^{-3}$. The spectroscopic observations additionally show evidence of a signal with a period of 19 d and a long periodic variation of undetermined origin. In combination with ground-based photometric monitoring from WASP-South and ASAS-SN, we attribute the 19 d signal to the stellar rotation period ($P_{rot}=$ 19-23 d), although we cannot rule out that the variation seen in photometry belongs to the visually close binary companion. We calculate precise stellar parameters for both TOI-1201 and its companion. The transiting planet is an excellent target for atmosphere characterization (the transmission spectroscopy metric is $97^{+21}_{-16}$) with the upcoming James Webb Space Telescope. It is also feasible to measure its spin-orbit alignment via the Rossiter-McLaughlin effect using current state-of-the-art spectrographs with submeter per second radial velocity precision. ","TOI-1201 b: A mini-Neptune transiting a bright and moderately young M
  dwarf"
65,1440540709455470593,2577596593,Chelsea Finn,"['RL methods so often learn from _scratch_. Can they leverage offline experience from previous tasks?\n\nThey can. And if they do, they will learn new tasks ~2x faster.\n\nPaper: <LINK>\nWebsite: <LINK>\n\nLed by Annie Xie. 🧵👇(1/4) <LINK>', 'Many prior transfer learning methods try to transfer weights, e.g through fine-tuning.\n\nWe consider whether we can also transfer past *experiences*, rather than throwing away the prior data.\n(2/4) https://t.co/2GKQh4yyBK', 'Retaining &amp; filtering experiences performs *substantially* better than fine-tuning &amp; other prior methods\n\nIt also outperforms learning from scratch, even when learning from scratch with 2x the data.\n(3/4) https://t.co/4nRQtJBrEy', 'Six years ago, @svlevine and I had the PR2 robot to learn to insert a block from scratch. \n\nNow, robots can finally leverage experience from inserting one block to much more quickly learn to insert other blocks. :) https://t.co/pnJRHcLTvI']",https://arxiv.org/abs/2109.09180,"Multi-task learning ideally allows robots to acquire a diverse repertoire of useful skills. However, many multi-task reinforcement learning efforts assume the robot can collect data from all tasks at all times. In reality, the tasks that the robot learns arrive sequentially, depending on the user and the robot's current environment. In this work, we study a practical sequential multi-task RL problem that is motivated by the practical constraints of physical robotic systems, and derive an approach that effectively leverages the data and policies learned for previous tasks to cumulatively grow the robot's skill-set. In a series of simulated robotic manipulation experiments, our approach requires less than half the samples than learning each task from scratch, while avoiding impractical round-robin data collection. On a Franka Emika Panda robot arm, our approach incrementally learns ten challenging tasks, including bottle capping and block insertion. ",Lifelong Robotic Reinforcement Learning by Retaining Experiences
66,1440508057591582721,849787456568340480,dan shiebler,"['Excited to share a new Category Theory+ML paper! In this work I explore which properties of gradient descent and Newton’s method hold when we replace derivatives with Cartesian derivatives <LINK>!', 'TLDR: Transformation invariance holds up surprisingly well. Convergence properties require some bending over backwards to reason about.', 'This paper will appear in Intelligent Computing and Optimization (https://t.co/U0qABVLmsX)', 'Huge thanks to @bgavran3 @statusfailed @jer_gib Cezar Ionescu and many others for their thoughts and feedback on this work']",http://arxiv.org/abs/2109.10262,"The Cartesian reverse derivative is a categorical generalization of reverse-mode automatic differentiation. We use this operator to generalize several optimization algorithms, including a straightforward generalization of gradient descent and a novel generalization of Newton's method. We then explore which properties of these algorithms are preserved in this generalized setting. First, we show that the transformation invariances of these algorithms are preserved: while generalized Newton's method is invariant to all invertible linear transformations, generalized gradient descent is invariant only to orthogonal linear transformations. Next, we show that we can express the change in loss of generalized gradient descent with an inner product-like expression, thereby generalizing the non-increasing and convergence properties of the gradient descent optimization flow. Finally, we include several numerical experiments to illustrate the ideas in the paper and demonstrate how we can use them to optimize polynomial functions over an ordered ring. ","Generalized Optimization: A First Step Towards Category Theoretic
  Learning Theory"
67,1440497364934873098,31503155,Dr. Pedro Bernardinelli,"['New paper on arXiv tonight, analyzing the DES (and some others) data on C/2014 UN271 (BB), with a special contribution from @benmontet. \n\nWe took a deep dive into the data, and figured some interesting things out. \n\n<LINK> (1/n) <LINK>', 'First, astrometry. What can we say about the past dybamics of BB? It came from the Oort cloud, and by including both the Galactic tide and stellar passages, we found out that its previous perihelion was about 18 au, ~3.5 Myr ago, and this is its closest passage to date\n(2/n) https://t.co/f8VppBxHkw', 'Then, photometry. Our data is consistent with an unresolved nucleus (H ~ 8.0, so 150km diameter with a typical albedo), and the object has colors similar to other OC comets, and a variability amplitude of 0.2 magnitudes. Nothing surprising here yet! (3/n) https://t.co/xnyoYrvDub', 'Now take a look at @benmontet’s thread, as things will get quite interesting from here onwards (4/n) https://t.co/5EN3PCH3lO', 'Basically, TESS measured BB in 2018 (and so did DES), and these magnitudes are incompatible with each other - TESS measured BB ~1 magnitude brighter! Did we miss something in DES? Turns out we did! We looked for a concentrated coma in a small region of the image. \n\n(5/n) https://t.co/ZVoAILojSM', 'But it turns out this coma is very diffuse and faint. We had to look for an extended emission in a &gt;10” radius to detect something (that’s &gt;40 DECam pixels, or half a TESS pixel) with surface brightness ~30 mag/arcsec^2\n\n(6/n - fig caption is not finished, oops) https://t.co/3kMP4Ln37i', 'And also a weak tail whose detection required not only stacking all of our 250x250 pixel scene-modeled (precise static source subtraction) images, but also binning the pixels themselves. This is my favorite figure of this paper!\n\n(7/n) https://t.co/c77wrM5rx4', 'The coma is consistent with a stationary model, and it follows simple sublimation thermodynamics (hence the spherical cow title for the paper!). We were able to determine that this is probably consistent with CO2 or NH3 sublimation, but couldn’t do much more than that\n\n(8/n) https://t.co/0k8Lc4UuTG', 'This was a fun, quick project to work in. Special thanks to all of you comet people for pointing out we had something really interesting in our hands (and essentially telling us to do a deep dive in our data), and we hope this all makes sense! \n\n(9/9)', 'Ok, one other tweet: An interesting detail from the text is that we’re referencing non-traditional sources (i.e. not papers). Notes, MPECs, ATels, and even blog posts from people who measured this object. I find this fascinating: if the information is online, it can be useful!']",https://arxiv.org/abs/2109.09852,"C/2014 UN271 (Bernardinelli-Bernstein) is a comet incoming from the Oort cloud which is remarkable in having the brightest (and presumably largest) nucleus of any well-measured comet, and having been discovered at heliocentric distance $r_h\approx29$ au farther than any Oort-cloud member. We describe the properties that can be inferred from images recorded until the first reports of activity in June 2021. The orbit has $i=95^\circ,$ with perihelion of 10.97 au to be reached in 2031, and previous aphelion at $40,400\pm260$ au. Backwards integration of the orbit under a standard Galactic tidal model and known stellar encounters suggests this is a pristine new comet, with a perihelion of $q\approx18$ au on its previous perihelion passage 3.5 Myr ago. The photometric data show an unresolved nucleus with absolute magnitude $H_r=8.0,$ colors that are typical of comet nuclei or Damocloids, and no secular trend as it traversed the range 34--23 au. For $r$-band geometric albedo $p_r,$ this implies a diameter of $150 (p_r/0.04)^{-0.5}$ km. There is strong evidence of brightness fluctuations at $\pm0.2$ mag level, but no rotation period can be discerned. A coma consistent with a ``stationary' $1/\rho$ surface-brightness distribution grew in scattering cross-section at an exponential rate from $A f \rho\approx1$ m to $\approx150$ m as the comet approached from 28 to 20 au. The activity is consistent with a simple model of sublimation of a surface species in radiative equilibrium with the Sun. The inferred enthalpy of sublimation matches those of $CO_2$ and $NH_3$. More-volatile species -- $N_2,$ $CH_4,$ and $CO$ -- must be far less abundant on the sublimating surfaces. ","C/2014 UN271 (Bernardinelli-Bernstein): the nearly spherical cow of
  comets"
68,1440480117612969992,1338201043,Koichi Hamaguchi,"['New Paper! with Kento Asai, Natsumi Nagata, Shih-Yen Tseng @shihyen_phys , and Juntaro Wada.\n<LINK>\nWe show that the Lμ-Lτ gauge boson, which is one of the simplest BSM solutions to the muon g-2 anomaly, can be probed at the MUonE experiment.']",https://arxiv.org/abs/2109.10093,"We discuss the prospects of probing the $L_\mu - L_\tau$ gauge boson at the MUonE experiment. The $L_\mu - L_\tau$ gauge boson $Z^\prime$ with a mass of $\lesssim 200$ MeV, which can explain the discrepancy between the measured value of the muon $g-2$ and the value calculated in the Standard Model, can be produced at the MUonE experiment through the process $\mu e \to \mu e Z^\prime$. The $Z^\prime$ in the final state decays into a pair of neutrinos, and therefore we cannot observe the decay of $Z^\prime$ directly. It is, however, still possible to probe this signature by searching for events with a large scattering angle of muon and a less energetic final-state electron. The background events coming from the elastic scattering $\mu e \to \mu e$ as well as radiative process $\mu e \to \mu e \gamma$ can be removed by the kinematical cuts on the muon scattering angle and the electron energy, in addition to a photon veto. The background events from the electroweak process $\mu e \to \mu e \nu \bar{\nu}$ are negligible. With our selection criteria, the number of signal events $\mu e \to \mu e Z^\prime$ is found to be as large as $\sim 10^3$ in the parameter region motivated by the muon $g-2$ discrepancy. It is, therefore, quite feasible to probe the $L_\mu - L_\tau$ gauge boson at the MUonE experiment -- without introducing additional devices -- and we strongly recommend recording the events relevant to this $Z^\prime$ production process. ",Probing the $L_\mu$-$L_\tau$ Gauge Boson at the MUonE Experiment
69,1440428521118048256,3327515352,M.P. Ross,"['New paper out today by my colleague Erik Shaw! Places new limits on ultra-light dark matter using a torsion balance. The experiment searched through 491,019 different dark matter masses and was up to two times better than past searches (mass dependent).\n<LINK>']",https://arxiv.org/abs/2109.08822,We used a stationary torsion balance with a beryllium-aluminum composition dipole to search for ultra low-mass bosonic dark matter coupled to baryon minus lepton number. We set 95% confidence limits on the coupling constant $g_{\rm B-L}$ for bosons with masses between $10^{-18}$ and $10^{-16}$ eV/$c^2$ with the best performance at $m_{\rm DM} = 8\times 10^{-18}$ eV/$c^2$ constraining $g_{B-L}(\hbar c)^{-1/2} < 1 \times 10^{-25}$. This provides a complimentary limit to equivalence-principle experiments that search for ultra low-mass bosons as force-mediating particles. ,A torsion-balance search for ultra low-mass bosonic dark matter
70,1440330167709175810,399569532,Dr. Soheb Mandhai,"['I recently submitted a new paper to MNRAS, which you can find here:\n<LINK>\nTitle: ""Exploring compact binary merger host galaxies and environments with zELDA"" 1/N', 'Technical Summary: Using publicly available stellar evolution codes such as BPASS and COSMIC in tandem with galaxies from the EAGLE simulations. We evolve compact binary systems (in this context: systems composed of two neutron stars or a neutron star + black-hole)* within 2/n', 'their parent galaxy using our zELDA pipeline. By constructing a gravitational potential based on the EAGLE galaxies, we can also simulate the orbits of each binary within our binary populations. 3/n', 'The compact binaries considered can potentially produce short-lived bursts of gamma-ray radiation (aptly named short-duration gamma-ray bursts or SGRBs). We identify a sub-population of simulated binaries that would be able to produce these events and be detected by the 4/n https://t.co/IPcjl6OF9O', 'Burst Alert Telescope (BAT) on the Swift telescope. We then compare this sub-population against observed well-localised SGRBs to gauge the consistency of the zELDA pipeline + binary models with reality. 5/n https://t.co/JIpGr5xEXI', ""Key results: \n- The redshift distribution for the binary populations that we've considered indicate a peak between 0.5 &lt; z &lt; 1. 6/n"", '- The majority of binaries migrate ~ 10 kpc ( about 33 000 light-years) from the centre of the host galaxy before merging. (eBHNS = EM Bright BHNS systems) 7/n https://t.co/fZmgcE943d', ""- If 16-40% (depending on the binary simulation used) of mergers were observed in reality (either with gravitational waves/SGRBs/both/other), they'd either have a host galaxy that's too faint or they'd merge far enough from the galaxy such that they appear rogue. 8/n"", ""This project was a big undertaking and I'm glad I managed to see it through to its first major milestone.\nFour years ago, I had the inspiration to simulate the evolution of binary systems within their galaxies (following on from my Master's research). There were many..."", ""challenges along the way relating to coding, mental health (depression/grief/trauma), pandemic etc. but we got there in the end. I have presented this and related work around the world - which is humbling (I never thought I'd reach his point) to think about. It now has..."", ""a fixed format in the form of a manuscript. There's a lot of research that I plan to build on this but first - I need to secure a research position and secondly - take a break.""]",https://arxiv.org/abs/2109.09714,"Compact binaries such as double neutron stars or a neutron star paired with a black-hole, are strong sources of gravitational waves during coalescence and also the likely progenitors of various electromagnetic phenomena, notably short-duration gamma-ray bursts (SGRBs), and kilonovae. In this work, we generate populations of synthetic binaries and place them in galaxies from the large-scale hydrodynamical galaxy evolution simulation $\rm{EAGLE}$. With our $\rm{zELDA}$ code, binaries are seeded in proportion to star formation rate, and we follow their evolution to merger using both the $\rm{BPASS}$ and $\rm{COSMIC}$ binary stellar evolution codes. We track their dynamical evolution within their host galaxy potential, to estimate the galactocentric distance at the time of the merger. Finally, we apply observational selection criteria to allow comparison of this model population with the legacy sample of SGRBs. We find a reasonable agreement with the redshift distribution (peaking at $0.5<z<1$), host morphologies and projected galactocentric offsets (modal impact parameter $\sim10$ kpc). Depending on the binary simulation used, we predict $16-40\%$ of SGRB events would appear ""host-less"", i.e. sources that merge with high impact parameters or have hosts fainter than the detection limit ($H>26$). ","Exploring compact binary merger host galaxies and environments with
  $\rm{zELDA}$"
71,1440320886406463488,1192399858092523520,Hyunwoo Kim,"[""💜 New EMNLP 2021 paper 🚨\n<LINK>\nHow do you express stronger empathy in dialogues?\nWe argue that we should focus on the cause of the emotion🎯\nThen do we ever learn to recognize the cause of others' emotions with word-level labels?🤔\nWe don't, right? See below⬇️ <LINK>"", 'Humans reason others\' emotional states by putting ourselves in others\' shoes. This is coined ""perspective-taking"" in social cognition. We imagine or simulate what it would be like if we\'re in that situation. This mental imitation is known to pave the way for understanding others.', 'We implement this idea with a generative emotion estimator (GEE) and reason emotion cause words with only emotion labels. Interestingly, GEE, which models P(C, E) = P(E)P(C|E) with context C and emotion E, can reason words strongly related to the emotion for free by simulating C.', 'We release 💛 EmoCause dataset 💜 to evaluate our approach on weakly-supervised emotion cause recognition (requiring only emotion labels and no word-level labels). We find the generative capability indeed helps GEE to outperform other methods (e.g., BERT, EmpDG).', 'We also propose a pragmatics-based generation method to focus on targeted words. Experiment results on EmpatheticDialogues show that our method helps dialogue agents to focus on the given target words (e.g., emotion cause words) and be perceived as more empathetic.', 'Code and the EmoCause dataset are available at https://t.co/c6YCmQC9zJ \nJoint work with @ByeongchangKim and @gunheekim 🙌🏻🙌🏻\nOur code is based on @parlai_parley. \nCheck it out 🥳']",https://arxiv.org/abs/2109.08828,"Empathy is a complex cognitive ability based on the reasoning of others' affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other's emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level annotations, which can be demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on pragmatics to make dialogue models focus on targeted words in the input during generation. Our method is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation. ","Perspective-taking and Pragmatics for Generating Empathetic Responses
  Focused on Emotion Causes"
72,1440298641168601097,780011537738174464,Dennis Soemers,"['Check out our new paper ""Automatic Generation of Board Game Manuals"" (led by @matthew_stephe) on arXiv (<LINK>)! <LINK>', 'It describes how the @LudiiGames general game system can automatically generate manuals for *any* of our &gt;900 games (as well as, potentially, procedurally generated ones). Some examples of generated manuals can be found on https://t.co/9fUbgYoPmx. https://t.co/6YzSvFknjl', ""This includes rule descriptions in English, automatically generated from game description files in Ludii's game description language, as well as images and GIFs showing examples of legal moves, win/loss conditions, initial board setups, and so on. https://t.co/3G3RCAajzP""]",https://arxiv.org/abs/2109.09507,"In this paper we present a process for automatically generating manuals for board games within the Ludii general game system. This process requires many different sub-tasks to be addressed, such as English translation of Ludii game descriptions, move visualisation, highlighting winning moves, strategy explanation, among others. These aspects are then combined to create a full manual for any given game. This manual is intended to provide a more intuitive explanation of a game's rules and mechanics, particularly for players who are less familiar with the Ludii game description language and grammar. ",Automatic Generation of Board Game Manuals
73,1440293764950474760,765860018872840192,Sebastian Cioaba,"['New paper with one of my Ph.D. students, Himanshu Gupta, and Ferdinand Ihringer on least Euclidean distortion of distance-regular graphs: <LINK> Comments and suggestions welcome!']",https://arxiv.org/abs/2109.09708,"In 2008, Vallentin made a conjecture involving the least distortion of an embedding of a distance-regular graph into Euclidean space. Vallentin's conjecture implies that for a least distortion Euclidean embedding of a distance-regular graph of diameter $d$, the most contracted pairs of vertices are those at distance $d$. In this paper, we confirm Vallentin's conjecture for several families of distance-regular graphs. We also provide counterexamples to this conjecture, where the largest contraction occurs between pairs of vertices at distance $d{-}1$. We suggest three alternative conjectures and prove them for several families of distance-regular graphs. ",The least Euclidean distortion constant of a distance-regular graph
74,1440247370663428106,192826908,Jorge Lillo-Box,"['A new @CARMENES_exopl paper is out today with the discovery of a close-in (P=2.5d) mini-Neptune around an M-dwarf (TOI-1201), detected by @NASA_TESS.  Congrats to Diana Kossakowski for the great leading work! Happy to have contributed.\n<LINK> <LINK>']",https://arxiv.org/abs/2109.09346,"We present the discovery of a transiting mini-Neptune around TOI-1201, a relatively bright and moderately young early M dwarf ($J \approx$ 9.5 mag, $\sim$600-800 Myr) in an equal-mass $\sim$8 arcsecond-wide binary system, using data from the Transiting Exoplanet Survey Satellite (TESS), along with follow-up transit observations. With an orbital period of 2.49 d, TOI-1201 b is a warm mini-Neptune with a radius of $R_\mathrm{b} = 2.415\pm0.090 R_\oplus$. This signal is also present in the precise radial velocity measurements from CARMENES, confirming the existence of the planet and providing a planetary mass of $M_\mathrm{b} = 6.28\pm0.88 M_\oplus$ and, thus, an estimated bulk density of $2.45^{+0.48}_{-0.42}$ g cm$^{-3}$. The spectroscopic observations additionally show evidence of a signal with a period of 19 d and a long periodic variation of undetermined origin. In combination with ground-based photometric monitoring from WASP-South and ASAS-SN, we attribute the 19 d signal to the stellar rotation period ($P_{rot}=$ 19-23 d), although we cannot rule out that the variation seen in photometry belongs to the visually close binary companion. We calculate precise stellar parameters for both TOI-1201 and its companion. The transiting planet is an excellent target for atmosphere characterization (the transmission spectroscopy metric is $97^{+21}_{-16}$) with the upcoming James Webb Space Telescope. It is also feasible to measure its spin-orbit alignment via the Rossiter-McLaughlin effect using current state-of-the-art spectrographs with submeter per second radial velocity precision. ","TOI-1201 b: A mini-Neptune transiting a bright and moderately young M
  dwarf"
75,1440223335292424207,54849207,Ian Harrison,"['New paper day..! From Juan Pablo Cordero, on a way to efficiently marginalise over uncertainties in redshift distributions in cosmology analyses where you have access to realisations of the full n(z)\n<LINK>', 'By ordering the discrete realisations according to a few summary values you can present the sampler with a smoother likelihood it can more easily explore. We talk about how to choose the summary values and how to order them in n-dimensional space.', 'We show that for DES-Y3, the scope of uncertainty on redshift distributions given by the SOMPz+3sDIR+SR (https://t.co/OCACtfjP7d) realisations is small enough that the traditional ""mean shifting"" method is still valid (does not degrade the cosmology parameter estimation).', 'If you wanted to use it, the code is on the des-y3 branch of cosmosis-standard-library (which, yes, is not the best place it could be 😝)\nhttps://t.co/PkssHiBqlD', '@cosmic_mar Thanks! I was mostly shepherd... It is driven by the fact that for n(z) in weak lensing we have a bunch of samples of nuisance parameters (the set of histogram bin heights of the discretised n(z)) which are generated independently of the main cosmology chain. https://t.co/DoqjrKzC2F', ""@cosmic_mar Feeding them into the chain in a random order can mean small steps (to the 'next' realisation of n(z)) can create a very jumpy likelihood surface which is harder to explore."", '@cosmic_mar We make a choice of summary parameters for the n(z) (the per-bin mean unsurprisingly turns out to be a very good one) which we expect to correlate with likelihood, then order the realisations according to that, with the rank becoming the nuisance parameter in the chain. https://t.co/ili3Qmm3zQ', '@cosmic_mar The other slight subtlety was how to do such a ranking in &gt;1D (when we have more than one tomographic bin) but Gary B had the insight this was a version of the Linear Sum Assignment Problem, and solving that allows you to place similar n(z) both nearby and on a regular grid.', '@cosmic_mar ...so, not specific to n(z) really, but situations where you have a bunch of realisations of nuisance parts of your theory already sitting round and want to use them in a chain.', '@cosmic_mar ...which I guess may not be a common situation 😆  It was definitely developed as an ad-hoc solution to this particular problem.']",https://arxiv.org/abs/2109.09636,"Cosmological information from weak lensing surveys is maximised by dividing source galaxies into tomographic sub-samples for which the redshift distributions are estimated. Uncertainties on these redshift distributions must be correctly propagated into the cosmological results. We present hyperrank, a new method for marginalising over redshift distribution uncertainties in cosmological analyses, using discrete samples from the space of all possible redshift distributions. This is demonstrated in contrast to previous highly simplified parametric models of the redshift distribution uncertainty. In hyperrank the set of proposed redshift distributions is ranked according to a small (in this work between one and four) number of summary values, which are then sampled along with other nuisance parameters and cosmological parameters in the Monte Carlo chain used for inference. This can be regarded as a general method for marginalising over discrete realisations of data vector variation with nuisance parameters, which can consequently be sampled separately to the main parameters of interest, allowing for increased computational efficiency. We focus on the case of weak lensing cosmic shear analyses and demonstrate our method using simulations made for the Dark Energy Survey (DES). We show the method can correctly and efficiently marginalise over a range of models for the redshift distribution uncertainty. Finally, we compare hyperrank to the common mean-shifting method of marginalising over redshift uncertainty, validating that this simpler model is sufficient for use in the DES Year 3 cosmology results presented in companion papers. ","Dark Energy Survey Year 3 results: Marginalisation over redshift
  distribution uncertainties using ranking of discrete realisations"
76,1440222180550197249,204261944,Matthew Kenworthy,"['New paper day! @ajbohn discusses ""Unveiling wide-orbit companions to K-type stars in Sco-Cen with Gaia EDR3"" <LINK> where he selected common proper motion companions (including stars from the YSES survey that has discovered 3 directly imaged exoplanets) /1 <LINK>', '@ajbohn Alex noted that one of the brown dwarfs detected in the YSES survey was bright enough to be detected in the GAIA catalogue - which made us wonder about more distant comoving companions, so he searched and found many more! The astrometric precision of GAIA enabled us to test /2 https://t.co/PXpqn3IUwo', '@ajbohn whether they were bound by comparing the projected velocity between the star and the candidate companion with the maximum bound velocity, enabling us to reject unbound companions. The GAIA photometry also allowed us to estimate the mass of the companions too. /3 https://t.co/boXDZEJ7Yt', '@ajbohn Looking at 480 K-type stars, there are 163 potential companions to 142 stars including 21 candidate triple systems, and about a dozen brown dwarf candidates. One BD was seen by SPHERE and Gaia, and the SED from SPHERE agrees with the GAIA photometric mass estimate. /4 https://t.co/uPCnPk5IOm', ""@ajbohn It was great to see this paper get accepted before @ajbohn's thesis defence tomorrow, and I'm very proud of all of Alex's work on this and the other papers.""]",https://arxiv.org/abs/2109.09185,"Abbreviated. We aim to identify new low-mass companions to young stars using the astrometric measurements provided by the Gaia space mission and complementary VLT/SPHERE data. We identify companion candidates from a sample of K-type, pre-main sequence stars in the Scorpius Centaurus association using the early version of the third data release of the Gaia space mission. Based on the provided positions, proper motions, and magnitudes, we identify all objects within a predefined radius whose differential proper motions are consistent with a gravitationally bound system. We derive companion masses through comparison with evolutionary tracks. For seven identified companion candidates we use additional data collected with VLT/SPHERE and VLT/NACO to assess the accuracy of the properties of the companions based on Gaia photometry alone. We identify 110 comoving companions that have a companionship likelihood of more than $95\,\%$. We identify ten especially intriguing companions that have masses in the brown dwarf regime down to $20\,M_\mathrm{Jup}$. Our high-contrast imaging data confirm both astrometry and photometric masses derived from Gaia alone. We discover a new brown dwarf companion, TYC 8252-533-1 B, with a projected separation of approximately $570\,\mathrm{au}$ from its Sun-like primary. SED modeling provides a companion mass of $52^{+17}_{-11}\,M_\mathrm{Jup}$. We show that the Gaia database can identify low-mass companions at wide separations from their host stars. For K-type Sco-Cen members Gaia can detect sub-stellar objects at projected separations larger than $300\,\mathrm{au}$ and is sensitivity limited beyond $1,000\,\mathrm{au}$ with a lower mass limit down to $20\,M_\mathrm{Jup}$. A similar analysis of other star-forming regions could significantly enlarge the sample size of such objects and test formation and evolution theories of planetary systems. ","Unveiling wide-orbit companions to K-type stars in Sco-Cen with Gaia
  EDR3"
77,1440208926906060809,263647843,Lukas Daniel Klausner,"['New paper alert: Together with several colleagues, both here at @FH_StPoelten and from Italy, we investigated current research trends and gaps in resilient AI. <LINK> <LINK> <LINK>', ""@fh_stpoelten Personally, I'm particularly proud of the brief introduction on the topic. https://t.co/zll7eWjPKF""]",https://arxiv.org/abs/2109.08904,"Artificial intelligence (AI) systems are becoming critical components of today's IT landscapes. Their resilience against attacks and other environmental influences needs to be ensured just like for other IT assets. Considering the particular nature of AI, and machine learning (ML) in particular, this paper provides an overview of the emerging field of resilient AI and presents research issues the authors identify as potential future work. ",Towards Resilient Artificial Intelligence: Survey and Research Issues
78,1440160643953291266,66452419,Alexey Svyatkovskiy,"['Our new work on modeling long-range aspects of source code has been accepted to #EMNLP2021\n\nPaper is available on arXiv: <LINK>\n\nSummary (1/4)', 'eWASH is an architecture-independent approach to incorporating entire file-level context into a fixed length window for learning, that compares favorably to memory-efficient transformers (Reformer, Performer). (2/4)', 'Our hypothesis was that the\nsyntax hierarchy imposed by developers is a real\nsignal of importance in a task context, and that\nmethods, containing most lines of code, are most\ndependent on the higher-level scopes of their file-level attributes (3/4)', 'We applied eWASH to GPT-C and PyMT5 models. (4/4)']",https://arxiv.org/abs/2109.08780,"Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context. ","Long-Range Modeling of Source Code Files with eWASH: Extended Window
  Access by Syntax Hierarchy"
79,1440119159057096706,1128508767404838912,Thayne Currie,"['Our new SCExAO paper on multi-band imaging of the HD 36546 debris disk, in a work led by @kellen_lawson \n<LINK> <LINK>']",https://arxiv.org/abs/2109.08984,"We present the first multi-wavelength (near-infrared; $1.1 - 2.4$ $\mu m$) imaging of HD 36546's debris disk, using the Subaru Coronagraphic Extreme Adaptive Optics (SCExAO) system coupled with the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS). As a 3-10 Myr old star, HD 36546 presents a rare opportunity to study a debris disk at very early stages. SCExAO/CHARIS imagery resolves the disk over angular separations of $\rho \sim 0.25"" - 1.0""$ (projected separations of $\rm{r_{proj}} \sim 25 - 101$ $\rm{au}$) and enables the first spectrophotometric analysis of the disk. The disk's brightness appears symmetric between its eastern and western extents and it exhibits slightly blue near-infrared colors on average (e.g. $J-K =-0.4\pm0.1$) $-$ suggesting copious sub-micron sized or highly porous grains. Through detailed modeling adopting a Hong scattering phase function (SPF), instead of the more common Henyey-Greenstein function, and using the differential evolution optimization algorithm, we provide an updated schematic of HD 36546's disk. The disk has a shallow radial dust density profile ($\alpha_{in} \approx 1.0$ and $\alpha_{out} \approx -1.5$), a fiducial radius of $r_0 \approx 82.7$ au, an inclination of $i \approx 79.1^\circ$, and a position angle of $\rm PA \approx 80.1^\circ$. Through spine tracing, we find a spine that is consistent with our modeling, but also with a ""swept-back wing"" geometry. Finally, we provide constraints on companions, including limiting a companion responsible for a marginal Hipparcos-Gaia acceleration to a projected separation of $\lesssim 0.2''$ and to a minimum mass of $\lesssim 11$ $\rm M_{Jup}$. ","Multiband imaging of the HD 36546 debris disk: a refined view from
  SCExAO/CHARIS"
80,1440116058120245251,549460404,吉田 紅 (Beni Yoshida),"['My new paper on monitored quantum circuits is out. \n\nThe paper is a bit long (68 pages) and has several results. Here are some highlights. \n\n<LINK>', '1) Entanglement in a monitored Clifford circuit can be studied by looking at a certain classical error-correction problem. This “dual” code is constructed from the spacetime pattern of the operator growth.', 'So, yes, entanglement in monitored circuits indeed emerges from quantum information scrambling, in a sense of OTOCs.', '2) Entanglement in monitored Clifford circuit can be distilled by “decoding” this dual classical code. This gives you a simple way to verify entanglement in monitored circuits.', 'So, try this in your laboratory!', '3) Due to decoupling from scrambling dynamics, entanglement structure below certain polynomial length scale is independent of the initial states of the circuit, as well as the measurement outcomes in the distant past.', 'The length scale is essentially the code distance. Below this, entanglement is state-independent. Above this, it is state-dependent.', 'So, there will be a sudden transition on the “entanglement distillation complexity”, I suspect.', '4) One philosophical speculation. Obviously, entanglement in monitored circuits depends on measurement outcomes in the past. Presumably, to see the entanglement, you need to know measurement results from the exponentially distant past.', 'But if you believe entanglement in monitored circuits is relevant to any naturally arising phenomena, it should not depend on the measurement outcomes. The nature won’t keep a record of exponentially many measurement results in her notebook!', 'The point is that, below the code distance scale, entanglement can be verified by knowing only the very recent measurement outcomes. Perhaps, this portion of entanglement may be relevant to naturally occurring phenomena... ?', ""@quantum_graeme Oh, I somehow thought I cited it, but apparently I didn't."", '@quantum_graeme Now I know what happened. I cited it when I discussed stabilizer sizes in the previous version. But then, I removed that entire section.']",https://arxiv.org/abs/2109.08691,"Given an output wavefunction of a monitored quantum circuit consisting of both unitary gates and projective measurements, we ask whether two complementary subsystems are entangled or not. For Clifford circuits, we find that this question can be mapped to a certain classical error-correction problem where various entanglement measures can be explicitly computed from the recoverability. The dual classical code is constructed from spacetime patterns of out-of-time ordered correlation functions among local operators and measured Pauli operators in the past, suggesting that the volume-law entanglement in a monitored circuit emerges from quantum information scrambling, namely the growth of local operators. We also present a method of verifying quantum entanglement by providing a simple deterministic entanglement distillation algorithm, which can be interpreted as decoding of the dual classical code. Discussions on coding properties of a monitored Clifford circuit, including explicit constructions of logical and stabilizer operators, are also presented. Applications of our framework to various physical questions, including non-Clifford systems, are discussed as well. Namely, we argue that the entanglement structure of a monitored quantum circuit in the volume-law phase is largely independent of the initial states and past measurement outcomes except recent ones, due to the decoupling phenomena from scrambling dynamics, up to a certain polynomial length scale which can be identified as the code distance of the circuit. We also derive a general relation between the code distance and the sub-leading contribution to the volume-law entanglement entropy. Applications of these results to black hole physics are discussed as well. ",Decoding the Entanglement Structure of Monitored Quantum Circuits
81,1439975250565816323,935991962460721153,Aishik Ghosh,"['New paper! Led by @BPNachman, a Cautionary Tale of Decorrelating Theory Uncertainties:\n<LINK>\n\nCould making ourselves insensitive to uncertainties just be hiding the truth?\n1/n', 'What if decorrelation shrinks only our ad-hoc estimate of the uncertainty, while the actual uncertainty remains large? Intuitive illustration:\n2/n https://t.co/jrV8JCVs8h', 'Example 1: We painstakingly sacrifice separation power for reduced difference between Pythia and Herwig. Alas, the difference to a third generator (Sherpa) remains large.\n3/n https://t.co/LVdS1hCEa7', 'Example 2: Decorrelating scale uncertainty at LO reduces the error bands, but we’re only fooling ourselves, the difference to NLO remains large.\n4/n https://t.co/VxiDHwdtCJ', 'Message: Decorrelation only does what it is trained to do, doesn’t solve the general problem. Until we have a better description of these uncertainties, think carefully before decorrelating!\n\n5/5', 'Learning twitter from the boss @DanielWhiteson :P']",https://arxiv.org/abs/2109.08159,"A variety of techniques have been proposed to train machine learning classifiers that are independent of a given feature. While this can be an essential technique for enabling background estimation, it may also be useful for reducing uncertainties. We carefully examine theory uncertainties, which typically do not have a statistical origin. We will provide explicit examples of two-point (fragmentation modeling) and continuous (higher-order corrections) uncertainties where decorrelating significantly reduces the apparent uncertainty while the actual uncertainty is much larger. These results suggest that caution should be taken when using decorrelation for these types of uncertainties as long as we do not have a complete decomposition into statistically meaningful components. ",A Cautionary Tale of Decorrelating Theory Uncertainties
82,1439872546745327617,27522184,Jessica May Hislop,"['Check out our new paper on the challenge of simulating the star cluster population of dwarf galaxies in simulations! The key points are summarised in the thread below 👇<LINK> 1/7', 'Simulations of galaxy formation use subgrid models for star formation. But as we go to higher resolution simulations and start to resolve individual stars, we need to make sure we still agree with observations 2/7', 'So in this study we looked at varying the star formation efficiency (SFE), that is, how much of the star forming gas is actually turned into stars. Turns out varying the SFE makes a huge difference to the distribution of stars 3/7 https://t.co/7FlGXaXtE4', 'Varying the SFE doesn’t make any major differences to the star formation rate or the outflow rate but it does make a big difference to the star clusters 4/7', 'Low SFE produces many tightly bound clusters with a high cluster formation efficiency (CFE) and high SFE produces fewer loosely bound clusters with a much lower CFE. Confusingly, no SFE reproduced what is observed in the Universe perfectly 5/7 https://t.co/pG4Z1lJXzD', 'This motivates us to now investigate alternatives to this particular subgrid model because it seems it doesn’t really work that well as we go to higher resolution simulations 6/7', 'Tl;dr: We ran simulations of isolated galaxies and found that varying the star formation efficiency makes very little difference to the global properties but makes a huge difference to the star cluster populations produced 7/7']",https://arxiv.org/abs/2109.08160,"We present results on the star cluster properties from a series of high resolution smoothed particles hydrodynamics (SPH) simulations of isolated dwarf galaxies as part of the GRIFFIN project. The simulations at sub-parsec spatial resolution and a minimum particle mass of 4 $\mathrm{M_\odot}$ incorporate non-equilibrium heating, cooling and chemistry processes, and realise individual massive stars. All the simulations follow feedback channels of massive stars that include the interstellar-radiation field, that is variable in space and time, the radiation input by photo-ionisation and supernova explosions. Varying the star formation efficiency per free-fall time in the range $\epsilon_\mathrm{ff}$ = 0.2 - 50$\%$ neither changes the star formation rates nor the outflow rates. While the environmental densities at star formation change significantly with $\epsilon_\mathrm{ff}$, the ambient densities of supernovae are independent of $\epsilon_\mathrm{ff}$ indicating a decoupling of the two processes. At low $\epsilon_\mathrm{ff}$, more massive, and increasingly more bound star clusters are formed, which are typically not destroyed. With increasing $\epsilon_\mathrm{ff}$ there is a trend for shallower cluster mass functions and the cluster formation efficiency $\Gamma$ for young bound clusters decreases from $50 \%$ to $\sim 1 \%$ showing evidence for cluster disruption. However, none of our simulations form low mass ($< 10^3$ $\mathrm{M_\odot}$) clusters with structural properties in perfect agreement with observations. Traditional star formation models used in galaxy formation simulations based on local free-fall times might therefore not be able to capture low mass star cluster properties without significant fine-tuning. ","The challenge of simulating the star cluster population of dwarf
  galaxies with resolved interstellar medium"
83,1438907533641478147,14761223,Shiri Dori-Hacohen ✡️♿️,"['Our new paper (with @DrRMontenegro @fabriciomurai @computermacgyve @KeenSung @MichelaBlainMD @MSUDrEJ), ""Fairness via AI: Bias Reduction in Medical Information"", is now available. Spoiler alert: we coined #bisinformation to refer to biased information.\n<LINK> 1/2', 'I will present the paper at @FAccTRec 2021 (https://t.co/W1U6frzcX9), part of @ACMRecSys 2021. Come hear about our novel ""#Fairness via #AI"" framework inspired by insights from #medical education, #sociology and #antiracism, plus the definition &amp; examples of #bisinformation. 2/2']",https://arxiv.org/abs/2109.02202,"Most Fairness in AI research focuses on exposing biases in AI systems. A broader lens on fairness reveals that AI can serve a greater aspiration: rooting out societal inequities from their source. Specifically, we focus on inequities in health information, and aim to reduce bias in that domain using AI. The AI algorithms under the hood of search engines and social media, many of which are based on recommender systems, have an outsized impact on the quality of medical and health information online. Therefore, embedding bias detection and reduction into these recommender systems serving up medical and health content online could have an outsized positive impact on patient outcomes and wellbeing. In this position paper, we offer the following contributions: (1) we propose a novel framework of Fairness via AI, inspired by insights from medical education, sociology and antiracism; (2) we define a new term, bisinformation, which is related to, but distinct from, misinformation, and encourage researchers to study it; (3) we propose using AI to study, detect and mitigate biased, harmful, and/or false health information that disproportionately hurts minority groups in society; and (4) we suggest several pillars and pose several open problems in order to seed inquiry in this new space. While part (3) of this work specifically focuses on the health domain, the fundamental computer science advances and contributions stemming from research efforts in bias reduction and Fairness via AI have broad implications in all areas of society. ",Fairness via AI: Bias Reduction in Medical Information
84,1438890172721074176,3740077215,Fan Bai,"['When adapting NLP models to a new domain, does it help more to annotate more data or pre-training an in-domain LM? Check out our #EMNLP2021 paper (w/ @alan_ritter &amp; @cocoweixu): Pre-train or Annotate? Domain Adaptation with a Constrained Budget (<LINK>).\n\n[1/7] <LINK>', 'Pre-training in-domain language models have been proved to be an effective domain adaptation strategy. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? \n\n[2/7]', 'In this paper, we study domain adaptation under a constrained budget, and approach it as a consumer choice problem.\n\n[3/7]', 'We measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain LMs. Then we evaluate the utility of different combinations of pre-training and data annotation under varying budget constraints to assess which one works best.\n\n[4/7] https://t.co/IEmry7oCOm', 'We find that, for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally.\n\n[5/7] https://t.co/IWKuCOfFb6', 'We, therefore, suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.\n\n[6/7]', 'Data and code are coming soon: https://t.co/MwVdO2gwgv.\n\n[7/7]']",http://arxiv.org/abs/2109.04711,"Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we view domain adaptation with a constrained budget as a consumer choice problem, where the goal is to select an optimal combination of data annotation and pre-training. We measure annotation costs of three procedural text datasets, along with the pre-training costs of several in-domain language models. The utility of different combinations of pre-training and data annotation are evaluated under varying budget constraints to assess which combination strategy works best. We find that for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, however, a combination of data annotation and in-domain pre-training yields better performance. Our experiments suggest task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain. ",Pre-train or Annotate? Domain Adaptation with a Constrained Budget
85,1438884403221049349,1069608017841262593,Joseph Ortiz,"['Excited to share new work entitled: Incremental Abstraction in Distributed Probabilistic SLAM Graphs. \n\nWork with: @talfanevans, @SucarEdgar, @AjdDavison \n\nProject page: <LINK>\nPaper: <LINK>\nVideo: <LINK>', 'Takeaway 1:\nWe abstract the SLAM factor graph into a more compact + semantic graph using network predictions that are accepted / rejected through inference.', 'Takeaway 2:\nWe use GBP (https://t.co/ymjBGC5cnG) for distributed inference on a graph processor (@graphcoreai IPU). We make GBP work for dynamic graphs and show for complex graphs with many different types of factors, GBP is very efficient and faster than the Ceres Solver!']",https://arxiv.org/abs/2109.06241,"Scene graphs represent the key components of a scene in a compact and semantically rich way, but are difficult to build during incremental SLAM operation because of the challenges of robustly identifying abstract scene elements and optimising continually changing, complex graphs. We present a distributed, graph-based SLAM framework for incrementally building scene graphs based on two novel components. First, we propose an incremental abstraction framework in which a neural network proposes abstract scene elements that are incorporated into the factor graph of a feature-based monocular SLAM system. Scene elements are confirmed or rejected through optimisation and incrementally replace the points yielding a more dense, semantic and compact representation. Second, enabled by our novel routing procedure, we use Gaussian Belief Propagation (GBP) for distributed inference on a graph processor. The time per iteration of GBP is structure-agnostic and we demonstrate the speed advantages over direct methods for inference of heterogeneous factor graphs. We run our system on real indoor datasets using planar abstractions and recover the major planes with significant compression. ",Incremental Abstraction in Distributed Probabilistic SLAM Graphs
86,1438781329789886466,135845460,Barry Haddow,"['New parallel corpus. Over 40M parallel sentences from 506 translation directions,  all official EU languages. Derived from the reports of the European Court of Auditors. Paper: <LINK> , Corpus: <LINK>']",https://arxiv.org/abs/2109.07351,"We present the ELITR ECA corpus, a multilingual corpus derived from publications of the European Court of Auditors. We use automatic translation together with Bleualign to identify parallel sentence pairs in all 506 translation directions. The result is a corpus comprising 264k document pairs and 41.9M sentence pairs. ",The ELITR ECA Corpus
87,1438673180009377793,1108100809596813314,Ming Sun,"['Our new paper, “The BIG X-ray tail”, <LINK>. (one of the most favorite titles I made 😀)', 'https://t.co/Mv7uTJp4Mo']",https://arxiv.org/abs/2109.07964,"Galaxy clusters grow primarily through the continuous accretion of group-scale haloes. Group galaxies experience preprocessing during their journey into clusters. A star-bursting compact group, the Blue Infalling Group (BIG), is plunging into the nearby cluster A1367. Previous optical observations reveal rich tidal features in the BIG members, and a long H$\alpha$ trail behind. Here we report the discovery of a projected $\sim 250$ kpc X-ray tail behind the BIG using Chandra and XMM-Newton observations. The total hot gas mass in the tail is $\sim 7\times 10^{10}\ {\rm M}_\odot$ with an X-ray bolometric luminosity of $\sim 3.8\times 10^{41}$ erg s$^{-1}$. The temperature along the tail is $\sim 1$ keV, but the apparent metallicity is very low, an indication of the multi-$T$ nature of the gas. The X-ray and H$\alpha$ surface brightnesses in the front part of the BIG tail follow the tight correlation established from a sample of stripped tails in nearby clusters, which suggests the multiphase gas originates from the mixing of the stripped interstellar medium (ISM) with the hot intracluster medium (ICM). Because thermal conduction and hydrodynamic instabilities are significantly suppressed, the stripped ISM can be long lived and produce ICM clumps. The BIG provides us a rare laboratory to study galaxy transformation and preprocessing. ",The BIG X-ray tail
88,1438666854407548928,1284565916,Chris Adami,"['New paper: we show that symbolic sequences can be classified by creating models directly from the multiple-sequence alignment. Far superior to Potts- or Ising models. We model correlations up to 4 variables (previous methods could only do up to 2). <LINK>', 'Work with my student Nitash C G, who is a wizard with the computer.']",https://arxiv.org/abs/2109.07933,"The information content of symbolic sequences (such as nucleic- or amino acid sequences, but also neuronal firings or strings of letters) can be calculated from an ensemble of such sequences, but because information cannot be assigned to single sequences, we cannot correlate information to other observables attached to the sequence. Here we show that an information score obtained from multivariate (multiple-variable) correlations within sequences of a ""training"" ensemble can be used to predict observables of out-of-sample sequences with an accuracy that scales with the complexity of correlations, showing that functional information emerges from a hierarchy of multi-variable correlations. ",Emergence of functional information from multivariate correlations
89,1438627519784275969,741340014878064640,🎃 Adina 💀 Feinstein 🎃,"['haavee you ever wondered if stellar flares behave like solar flares? what about earthquakes? \n\nin our (@DarrylSeligman @M_N_Guenther, &amp; Fred Adams) new (submitted) paper, we tie it all back to the theory of self-organized criticality\n\nfull paper here --&gt; <LINK>', ""the easiest system to understand self-organized criticality (SOC) is a sandpile\n\nthe addition of a sand grain could trigger a range of avalanche energies. the key is that the system, once it's finished releasing energy, goes back to maintaining itself at some critical slope https://t.co/XMgmr4VTCz"", ""but in Sun, the picture isn't quite the same. here:\n\nnew sand grain = injection of energy from the interior dynamo\n\navalanche = flare\n\na small flare is equivalent to a grain causing a small avalanche. a large flare triggers a larger event, with more sympathetic flares https://t.co/zvhreJOdOB"", 'for our analysis, @M_N_Guenther ran all of the TESS 2-minute targets from years 1&amp;2 through my machine learning code, stella, with some false-positive filters to create a catalog of ~10^6 high-probability flares \n\ncheck out how flare rate changes as a function of color &amp; mag👇🏻 https://t.co/Zhrzz9jEaG', ""then, we plot some lines! in log-log space, of course! we measured the flare rate as a function of stellar mass\n\nwhat's more indicative that these coronae are in self-organized critical states is that they look like featureless power-laws (even with lack of low-amp flares) https://t.co/2jMXnNfOBH"", ""we compared our flare rates to that found in the literature. we find low-mass stars and giants have shallower flare rates, potentially because they have larger convective regions (??) but we'll leave that to the theorists now to figure out 😉 https://t.co/nLoLGpNbbp"", 'if all stellar coronae exist in a self-organized critical state, then we can infer properties of magnetic fields, interior structure, and dynamo mechanisms for *all* of these point sources 😱🥳⭐\n\nfor more deets, the paper is on arxiv --&gt; https://t.co/QyNmu5PXgz']",https://arxiv.org/abs/2109.07011,"Self-organized criticality describes a class of dynamical systems that maintain themselves in an attractor state with no intrinsic length or time scale. Fundamentally, this theoretical construct requires a mechanism for instability that may trigger additional instabilities locally via dissipative processes. This concept has been invoked to explain nonlinear dynamical phenomena such as featureless energy spectra that have been observed empirically for earthquakes, avalanches, and solar flares. If this interpretation proves correct, it implies that the solar coronal magnetic field maintains itself in a critical state via a delicate balance between the dynamo-driven injection of magnetic energy and the release of that energy via flaring events. All-sky high-cadence surveys like the Transiting Exoplanet Survey Satellite (TESS) provide the necessary data to compare the energy distribution of flaring events in stars of different spectral types to that observed in the Sun. We identified $\sim 10^6$ flaring events on $\sim 10^5$ stars observed by TESS at 2-minute cadence. By fitting the flare frequency distribution for different mass bins, we find that all main sequence stars exhibit distributions of flaring events similar to that observed in the Sun, independent of their mass or age. This may suggest that stars universally maintain a critical state in their coronal topologies via magnetic reconnection events. If this interpretation proves correct, we may be able to infer properties of magnetic fields, interior structure, and dynamo mechanisms for stars that are otherwise unresolved point sources. ","Testing Self-Organized Criticality Across the Main Sequence using
  Stellar Flares from TESS"
90,1438586147073052675,1319101874532978690,jason wei,"['New paper to appear in #emnlp2021! <LINK>\n\nWe study the syntactic abilities of BERT by manipulating the training corpus and retraining BERT.', 'Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules.', 'We investigate this question using BERT’s performance on English subject-verb agreement (SVA) as a case study. https://t.co/prIIvyA4l1', 'We train multiple instances of BERT from scratch that allow us to perform a series of controlled interventions at pre-training time.', 'BERT often generalizes well to subject-verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. https://t.co/3sH1i26xio', 'We also find, however, that performance is heavily influenced by word frequency. BERT performs better on verbs that occur more often. https://t.co/DRYGRduxFM', 'When one verb form occurs more often during training than the other (e.g., if “run” occurs more often than “runs”), BERT tends to predict the more frequent word form. https://t.co/npflW1ubYS', 'Closer analysis via probing reveals that much of the observed error rate can be attributed to errors in predicting the agreement feature of the subject or verb (singular/plural), as opposed to not following SVA when the subject and verb have been correctly identified. https://t.co/iKLRkgVFyE', 'Hence, BERT follows the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) for infrequent lexical items”\n\nThis finding provides insight into the conditions needed for LMs to exhibit rule-learning behavior.', 'Thanks @Brown_NLP @tallinzen @dhgarrette for hosting me for this AI residency project!']",https://arxiv.org/abs/2109.07020,"Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject-verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject-verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items. ",Frequency Effects on Syntactic Rule Learning in Transformers
91,1438550289255055361,20926161,Michael Ekstrand,"['🚨 new preprint 🚨\n\nMy student Ngozi Ihemelandu is works on statistical inference for #recsys evaluation; her first paper, at the #RecSys2021 PERSPECTIVES workshop, is on current practices in statistical techniques in RecSys experiments. 1/4 <LINK>', 'The heart of the paper: she surveyed #RecSys2019 and #RecSys2020 papers that present a new algorithm and compare it to a baseline. **59%** of these papers (65/111) simply compare metric values, with no inference for significance or confidence intervals. We need to do better. 2/4', 'There are a lot of unknowns about *how* we should conduct rigorous analysis of experimental outcomes, but doing nothing is not an option. Immediately, the field should be using established tests &amp; fully reporting the inference methods and results. 3/4', 'Going forward, further research is needed to develop evidence-based best practices for analyzing recsys experiment results, and those practices need to be documented and disseminated to the community.\n\nAll this and more in the paper! 4/4 @PIReTship @BoiseState_Grad @ComputingPhD']",https://arxiv.org/abs/2109.06424,"This paper calls attention to the missing component of the recommender system evaluation process: Statistical Inference. There is active research in several components of the recommender system evaluation process: selecting baselines, standardizing benchmarks, and target item sampling. However, there has not yet been significant work on the role and use of statistical inference for analyzing recommender system evaluation results. In this paper, we argue that the use of statistical inference is a key component of the evaluation process that has not been given sufficient attention. We support this argument with systematic review of recent RecSys papers to understand how statistical inference is currently being used, along with a brief survey of studies that have been done on the use of statistical inference in the information retrieval community. We present several challenges that exist for inference in recommendation experiment which buttresses the need for empirical studies to aid with appropriately selecting and applying statistical inference techniques. ","Statistical Inference: The Missing Piece of RecSys Experiment
  Reliability Discourse"
92,1438543852109897734,2853379350,Siddhant Garg,"['🚨#EMNLP2021 Paper\n\nNew💡for Efficient QA➡️\nFilter questions that will not be answered by QA system\n\nInteresting 🔎: Transformer-based QA scores can be approximated only using question text via ""partial-input distillation""\n\n@AmazonScience @amoschitti1\n\n📰: <LINK> <LINK>', '@AmazonScience @amoschitti1 Practical QA systems operate at high Prec. (for customer req.) and end up not answering a large % of ques by failing system threshold on answer confidence score\n\nWe train filters to preemptively remove ques that are not answered by the system, saving (retrieval+answering) compute', '@AmazonScience @amoschitti1 We propose 2 loss objectives for learning 2 types of filters: regression &amp; classification head by distilling knowledge of QA system scores\n\nTraining does not require any human labels, only system generated scores\n\nDifferent from KD since teacher &amp; student use different inputs https://t.co/dqjrB8Hncz', '@AmazonScience @amoschitti1 Experimental results show: \n\n(i) Question filters can approximate Pr/Re of QA system very well\n\n(ii) Filters can provide large efficiency gains, with only a small drop in Recall (user-tunable tradeoff) https://t.co/WxV4o5ecr6', '@AmazonScience @amoschitti1 Code to be released soon at https://t.co/ucyWnAqajH\n\nPaper #AmazonScience: https://t.co/PnHOgoaL68\n\nFeel free to reach out to us in case of any questions 😀']",https://arxiv.org/abs/2109.07009,"In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision/Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower Recall, e.g., reducing computation by ~60%, while only losing ~3-4% of Recall. ","Will this Question be Answered? Question Filtering via Answer Model
  Distillation for Efficient Question Answering"
93,1438534504382189568,774299949580357632,Milan Curcic,"['Our new paper on @fortranlang is up: <LINK>.\n\nWe explain why and how we started the Fortran-lang community, progress made so far, and next steps.\n\nSubmitted for publication to ACM Fortran Forum.', ""Thanks to my co-authors @OndrejCertik, @everythingfunct, Sebastian Ehlert, @laurencekedward, @ArjenMarkus, @IvanPribec, and @jeremievdp for all their hard work and leadership, and all other @fortranlang contributors (300+ people), without whom this wouldn't happen."", 'Discussion on Hacker News: https://t.co/cIcAGTPXdC.']",https://arxiv.org/abs/2109.07382,"Fortran is the oldest high-level programming language that remains in use today and is one of the dominant languages used for compute-intensive scientific and engineering applications. However, Fortran has not kept up with the modern software development practices and tooling in the internet era. As a consequence, the Fortran developer experience has diminished. Specifically, lack of a rich general-purpose library ecosystem, modern tools for building and packaging Fortran libraries and applications, and online learning resources, has made it difficult for Fortran to attract and retain new users. To address this problem, an open source community has formed on GitHub in 2019 and began to work on the initial set of core tools: a standard library, a build system and package manager, and a community-curated website for Fortran. In this paper we report on the progress to date and outline the next steps. ",Toward Modern Fortran Tooling and a Thriving Developer Community
94,1438494336115642368,1128863592210608128,Eva Portelance,"[""Exciting stuff: I have a new paper out which was recently accepted at #CoNLL2021 called 'The Emergence of the Shape Bias results from Communicative Efficiency'. <LINK> This is work with @jurafsky @mcxfrank @murefil and @LarocheRomain."", '@jurafsky @mcxfrank @murefil @LarocheRomain We use neural emergent communication agents to model how a shape bias can emerge and persist across generations of language learners.', '@jurafsky @mcxfrank @murefil @LarocheRomain First, we show that the shape bias emerges as a result of efficient communication strategies employed by agents; efficient communication strategies in one semantic domain – shape – can affect the overall lexicon and lead to categorical biases, like the shape bias.', ""@jurafsky @mcxfrank @murefil @LarocheRomain Second, we show that pressure brought on by communicative need is also necessary for it to persist across generations; simply having a shape bias in an agent's input language is insufficient."", '@jurafsky @mcxfrank @murefil @LarocheRomain These results suggest that, over and above the operation of other learning strategies, the shape bias in human learners may emerge and be sustained by communicative pressures.']",https://arxiv.org/abs/2109.06232,"By the age of two, children tend to assume that new word categories are based on objects' shape, rather than their color or texture; this assumption is called the shape bias. They are thought to learn this bias by observing that their caregiver's language is biased towards shape based categories. This presents a chicken and egg problem: if the shape bias must be present in the language in order for children to learn it, how did it arise in language in the first place? In this paper, we propose that communicative efficiency explains both how the shape bias emerged and why it persists across generations. We model this process with neural emergent language agents that learn to communicate about raw pixelated images. First, we show that the shape bias emerges as a result of efficient communication strategies employed by agents. Second, we show that pressure brought on by communicative need is also necessary for it to persist across generations; simply having a shape bias in an agent's input language is insufficient. These results suggest that, over and above the operation of other learning strategies, the shape bias in human learners may emerge and be sustained by communicative pressures. ",The Emergence of the Shape Bias Results from Communicative Efficiency
95,1438422825283375104,1282107171518349312,Jacob Simpson,"['Proud to announce that I have written my first paper. In it I propose a new strategy for C2. Could not have done it without the support from @HusseinAbbassOz, @AusAirForce, @WarintheFuture and @WilliamsFnd. Thank you all very much. <LINK>']",https://arxiv.org/abs/2109.06874,"Artificial Intelligence (AI) is rapidly becoming integrated into military Command and Control (C2) systems as a strategic priority for many defence forces. The successful implementation of AI is promising to herald a significant leap in C2 agility through automation. However, realistic expectations need to be set on what AI can achieve in the foreseeable future. This paper will argue that AI could lead to a fragility trap, whereby the delegation of C2 functions to an AI could increase the fragility of C2, resulting in catastrophic strategic failures. This calls for a new framework for AI in C2 to avoid this trap. We will argue that antifragility along with agility should form the core design principles for AI-enabled C2 systems. This duality is termed Agile, Antifragile, AI-Enabled Command and Control (A3IC2). An A3IC2 system continuously improves its capacity to perform in the face of shocks and surprises through overcompensation from feedback during the C2 decision-making cycle. An A3IC2 system will not only be able to survive within a complex operational environment, it will also thrive, benefiting from the inevitable shocks and volatility of war. ","Agile, Antifragile, Artificial-Intelligence-Enabled, Command and Control"
96,1438419428094496772,1194556060641705984,Marco Gaido,"['We have put on #arXiv our paper on the translation and transcription of NE and terminology that we will present at #EMNLP2021: <LINK>\nIt contains the link to our new benchmark.\nJoint work with Susana Rodríguez, @negri_teo @luisabentivogli  @Turchi_Marco\n@fbk_mt']",https://arxiv.org/abs/2109.07439,"Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en->es/fr/it) show that ST systems correctly translate 75-80% of terms and 65-70% of NEs, with very low performance (37-40%) on person names. ","Is ""moby dick"" a Whale or a Bird? Named Entities and Terminology in
  Speech Translation"
97,1438374660081692672,314171681,Laura Baudis,"['New paper: with a small Xe time projection chamber called Xurich we measured the W-value in liquid xenon - the mean energy required to produce single excitation quanta (scintillation photons or ionisation electrons) by particles interacting in the liquid: <LINK>', 'The kinetic energy of a particle interacting in xenon results in scintillation photons in the vacuum ultraviolet range (~178 nm) and ionisation electrons; their total number is directly proportional to the energy, and W is the proportionality constant E = W (n_ph + n_e)', 'Two-phase xenon TPCs measure the prompt scintillation signal (S1) and the ionisation signal (S2, via electroluminescence in a gaseous region above the liquid) and use these signals to reconstruct, among other things such as position, the deposited energy in the medium https://t.co/QJUshuozUC', 'The efficiencies to observe the charge and light signals are detector-dependent and lead to the definition of so-called scintillation g1 = S1/n_ph and ionisation g2 = S2/n_e gains, which are characteristic for a particular xenon detector', 'Rewriting the first expression in terms of g1, g2 yields a simple equation for W, telling us how to measure it: we need an event population of known energy E in S1-S2 space, an independent measurement of g2 &amp; the negative slope g2/g1 of the line in charge versus light yield space https://t.co/LpLJeWp0JX', 'This is a representation of the charge and light are anti-correlation (the more e- recombine with ions, the more light and vice versa) in xenon TPCs, illustrated in the so-called Doke plot, after Tadoyashi Doke, Waseda, who performed some of the earliest measurements of this kind https://t.co/6lVAy0GCfH', 'To quantify the anti-correlation we need at least S1 and S2 data at a single energy line and two different electric drift fields (the e-ion recombination is field dependent) or data at two different known energies. We use multiple lines and a range of drift fields here', 'In addition, we must know the absolute charge gain g2, which we determined from a population of single electrons extracted into the gas phase. For these electrons, assuming 100% extraction efficiently from liquid to gas at an extraction field of 10 kV/cm, the charge gain is S2', 'This figure shows the anti-correlated S1 and S2 data from our internal calibration sources (37-Ar with 2.82 keV from K-shell electron capture and 83m-Kr with transition lines at 32.15 keV and 9.41 keV) acquired at electric drift fields between 80-968 V/cm https://t.co/D2gQfmZpkK', 'The measurement principle is simple, but the devil is in the systematics; their description takes the largest part of the paper. We obtain a W-value of (11.5+0.2-0.3) eV, which is lower than the commonly used value of (13.7±0.2) eV measured by E. Dahl in his PhD thesis in 2009', 'Our W-value is however compatible with a more recent (2020) measurement by EXO-200, a double beta decay experiment. With their larger, single-phase liquid xenon detector with charge readout and using lines at MeV-energies, they obtain W = [11.5 +- 0.1 (stat) +- 0.5 (syst)] eV', 'To be clear: a change in the numerical value of W will not influence the translation from S1 and S2 measured in, e.g., dark matter detectors, into deposited energy E when measurements relative to known energies of calibration sources are performed', 'A lower W-value will however reduce the detector specific parameters g1 and g2, for given S1, S2 and E, and thus the absolute energy response to excitation quanta. Because of this and other implications (see paper) we hope for further measurements of W in LXe detectors', 'Main work by PhD student Kevin Thieme, who will submit his thesis and graduate later this year :-). Xurich is a small two-phase xenon TPC equipped with a 2-inch PMT and an array of SiPMs. The detector, its data acquisitions &amp; processing are described here https://t.co/3gE2o4cMQk']",https://arxiv.org/abs/2109.07151,"Detectors using liquid xenon as target are widely deployed in rare event searches. Conclusions on the interacting particle rely on a precise reconstruction of the deposited energy which requires calibrations of the energy scale of the detector by means of radioactive sources. However, a microscopic calibration, i.e. the translation from the number of excitation quanta into deposited energy, also necessitates good knowledge of the energy required to produce single scintillation photons or ionisation electrons in liquid xenon. The sum of these excitation quanta is directly proportional to the deposited energy in the target. The proportionality constant is the mean excitation energy and is commonly known as $W$-value. Here we present a measurement of the $W$-value with electronic recoil interactions in a small dual-phase xenon time projection chamber with a hybrid (photomultiplier tube and silicon photomultipliers) photosensor configuration. Our result is based on calibrations at $\mathcal{O}(1-10 \, \mathrm{keV})$ with internal $^{37}$Ar and $^{83\text{m}}$Kr sources and single electron events. We obtain a value of $W=11.5 \, ^{+0.2}_{-0.3} \, \mathrm{(syst.)} \, \mathrm{eV}$, with negligible statistical uncertainty, which is lower than previously measured at these energies. If further confirmed, our result will be relevant for modelling the absolute response of liquid xenon detectors to particle interactions. ",A measurement of the mean electronic excitation energy of liquid xenon
98,1438350879556440069,983186922565595137,Mujeen Sung,"['I\'m so happy to announce that our new work ""Can Language Models be Biomedical Knowledge Bases?"" has been accepted to #EMNLP2021. \n\nPaper: <LINK>\nCode: <LINK>\n\n[1/N] <LINK>', 'To evaluate the possibilities of LMs as biomedical KBs, we created and released 49K biomedical factual triples from three sources (CTD, UMLS, and Wikidata).\n\n[2/N] https://t.co/YhKd7jtUEq', 'We compared three different LMs (BERT, BioBERT, and Bio-LM) on BioLAMA using two existing probing methods (manual prompt and OptiPrompt), and figured out that Bio-LM (w/ OptiPrompt) outperformed the existing information extraction system.\n\n[3/N] https://t.co/ie7t75aEJ0', 'We also analyzed whether LMs are ready for being biomedical KBs in terms of two aspects (Prompt Bias and Synonym Variance) and demonstrated that the current LMs tended to be highly biased towards relation information.\n\n[4/N] https://t.co/HsYSkng4A6', 'We expect that stronger knowledge-intensive LMs and more sophisticated probing methods can overcome this phenomenon, and leave this as an open challenge.\n\n[5/N]', 'This is joint work with @leejnhk\xa0@seanswyi\xa0Minji Jeon @tjdehd1222\xa0@jkang101.\n\n[6/N]']",https://arxiv.org/abs/2109.07154,"Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing. ",Can Language Models be Biomedical Knowledge Bases?
99,1438218008837693450,39332031,Judit Acs,['Our new paper with Dániel Lévai and @AKornai on transfering BERT models to low resource Uralic languages is available now. Conclusion: it works surprisingly well within the same alphabet even between unrelated languages like Russian and Erzya. <LINK>'],https://arxiv.org/abs/2109.06327,"Transformer-based language models such as BERT have outperformed previous models on a large number of English benchmarks, but their evaluation is often limited to English or a small number of well-resourced languages. In this work, we evaluate monolingual, multilingual, and randomly initialized language models from the BERT family on a variety of Uralic languages including Estonian, Finnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian, Northern S\'ami, and Skolt S\'ami. When monolingual models are available (currently only et, fi, hu), these perform better on their native language, but in general they transfer worse than multilingual models or models of genetically unrelated languages that share the same character set. Remarkably, straightforward transfer of high-resource models, even without special efforts toward hyperparameter optimization, yields what appear to be state of the art POS and NER tools for the minority Uralic languages where there is sufficient data for finetuning. ",Evaluating Transferability of BERT Models on Uralic Languages
100,1438215447783030786,346719335,Keyon Vafa,"['New paper: <LINK>\n\nConsider a sequence generated by a language model. Which words were most important for generating each word?\n\nWe propose greedy rationalization: greedily finding the smallest subset of words that would make the same prediction as the full text. <LINK>', 'Consider a sequence generated by GPT-2: ""The court struck down the law because it was unconstitutional""\n\nWhich words were most important for predicting ""unconstitutional""?\n\nThe greedy algorithm starts with an empty set and adds words until ""unconstitutional"" is the top prediction https://t.co/a9NXHFTjuR', ""How do we evaluate sequential rationales? \n\nThere are some datasets with annotated rationales for classification, but these don't extend to sequence models.\n\nSo we collected our own sequential rationale dataset based on Lambada. https://t.co/liPVCfNoLM"", 'Paper: https://t.co/8CjhtqB1j8\nGithub: https://t.co/4NzpZ255bg\nDemo: https://t.co/C9dmp255VV\n\nWith: @yuntiandeng, @blei_lab, @srush_nlp']",https://arxiv.org/abs/2109.06387,"Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the combinatorial objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales. ",Rationales for Sequential Predictions
101,1438197765280919554,1182194132309012481,Michihiro Yasunaga,"['Excited to share our new #EMNLP2021 paper ""LM-Critic: Language Models for Unsupervised Grammatical Error Correction"" with @percyliang @jure @StanfordAILab @StanfordNLP!\n\nPaper: <LINK>\nGithub: <LINK>\n\nThread below [1/7] ⤵️ <LINK>', 'A big challenge in learning grammatical error correction (GEC) is to get labeled bad-good sentence pairs. Manual labeling is expensive. Synthetic data (e.g. perturbing good sentences) does not reflect real errors. \nHow can we get cheap yet realistic training data for GEC?\n\n[2/7] https://t.co/2l97ok3rXA', 'We develop a new solution to this problem leveraging language models (LMs). Two core insights:\n\n1) LM-Critic: We can get an unsupervised critic that assesses the grammaticality of a sentence by checking if LMs assign it a higher probability than its local perturbations.\n\n[3/7] https://t.co/2O0qJ4NEVN', 'How BIFI works is to jointly train the desired fixer that corrects a bad sentence into a good one and a breaker that corrupts a good sentence into a bad one, while using them in conjunction to generate more realistic parallel data from unlabeled raw text.\n[5/7] https://t.co/alhh7dngYV', 'In summary, using pretrained LMs, our method (LM-Critic + BIFI) can obtain usable parallel data from unlabeled data alone!\n\nWe use the generated parallel data to augment the training of GEC models. This offers substantial performance boost across various GEC benchmarks.\n\n[6/7] https://t.co/Y0twpCydPf', 'For more details, please check out our paper at https://t.co/Fvi34s4LuQ\n\nHuge thanks to the collaborators and all who gave us feedback!\n[7/7]']",https://arxiv.org/abs/2109.06822,"Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets across multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting (+0.5 F0.5). ",LM-Critic: Language Models for Unsupervised Grammatical Error Correction
102,1438193129903906818,1591862592,Jenny Calahan,"['🚨New paper alert🚨\n\nMolecules with ALMA at Planet Forming Scales, or the MAPS collaboration has just dropped 20 (😱) papers on the ArXiV about protoplanetary disks and constraining their chemistry and physics.\n\nA 🧵 about my contribution: <LINK>', 'My paper focuses on the protoplanetary disk around the  star HD 163296. This disk has been observed A LOT with ALMA. ALMA is a giant radio interferometer telescope that is able to resolve down to 10s of au for an object that is hundreds of light years away. https://t.co/pMz7zCrrpq', 'In my paper I use observations of the dust and CO gas within the disk to help constrain the 2D temperature (height vs radius). Using a thermo-chemical model, I was able to reproduce all of the radial intensity profiles of CO and its friends 13CO, C18O, and C17O.', 'Reproducing the emission from these lines was not straightforward! I found that I had to increase the temperature in the upper atmosphere of the disk, this was the only way I could reproduce all the radial profiles. https://t.co/7lMIrQhXRR', 'Using the 3D images (x, y, velocity) of CO from MAPS data, the team was able to determine the actual heights at which each molecule is emitting from. We can also estimate the temperature of the disk directly from observations. (See Law &amp; MAPS team et al. 2021 papers!)', 'So why bother using a fancy thermo-chemical code figure out the 2D temperature information in a disk? https://t.co/L4RZYvofdL', 'Well! Turns out by matching the CO lines, dust emission, and some other upper limits, my thermo-chemical model was able to reproduce the emitting heights of CO and  temperatures derived directly from the CO emission! https://t.co/Bbwe88ksZY', 'This not only strengthens my model, but my model fills in the gaps* in the empirically derived temperature structure. The modeling work was hard (took ~a year 🙃) but we have more information about the disk, and a super fun final model to play around with! https://t.co/wkN91h7KsM', ""Speaking of *gaps!! \n\nThe disk around HD 163296 has been shown to have pretty significant gaps in the dusty part of the disk.\n\nWe aren't certain if those gaps exist in the gas as well. I wanted to see what effect those possible gas gaps could have on the temperature structure."", 'IF there was a planet forming in these dust gaps, we can make a prediction as to how depleted of gas these gaps would be.\n\nI ""put in"" a Jupiter mass planet and a &gt;4 Jupiter mass planet and made changes to the CO distribution to reproduce all of our observations. https://t.co/NBOzdLTDVQ', ""I found that only a &gt;4 Jupiter mass planet would create such a change in the CO observations that we'd have to alter the CO abundance in the gap, and that taking away a significant amount of gas only changed the temperature by ~5%."", ""What did I learn from this experiment?\n1. It's hard to determine if there is a gap in the gas corresponding to gaps in the dust.  Only a really massive planet, or a large depletion of gas would be in the realm of detectability. (**with my methods + current observations)"", '2. If there is a gas gap of any sort located on order of a few tens of au, the temperature should not change significantly. https://t.co/x8qqEqBHzM', 'Thanks for reading ❤️ and be sure to check out other MAPS papers as well! I love talking about my research, and am always happy to present my work at journal clubs or small talks 👉👈']",https://arxiv.org/abs/2109.06202,"Understanding the temperature structure of protoplanetary disks is key to interpreting observations, predicting the physical and chemical evolution of the disk, and modeling planet formation processes. In this study, we constrain the two-dimensional thermal structure of the disk around Herbig Ae star HD 163296. Using the thermo-chemical code RAC2D, we derive a thermal structure that reproduces spatially resolved ALMA observations (~0.12 arcsec (13 au) - 0.25 arcsec (26 au)) of CO J = 2-1, 13CO J = 1-0, 2-1, C18O J = 1-0, 2-1, and C17O J = 1-0, the HD J = 1-0 flux upper limit, the spectral energy distribution (SED), and continuum morphology. The final model incorporates both a radial depletion of CO motivated by a time scale shorter than typical CO gas-phase chemistry (0.01 Myr) and an enhanced temperature near the surface layer of the the inner disk (z/r <= 0.21). This model agrees with the majority of the empirically derived temperatures and observed emitting surfaces derived from the J = 2-1 CO observations. We find an upper limit for the disk mass of 0.35 Msun, using the upper limit of the HD J = 1-0 and J = 2-1 flux. With our final thermal structure, we explore the impact that gaps have on the temperature structure constrained by observations of the resolved gaps. Adding a large gap in the gas and small dust additionally increases gas temperature in the gap by only 5-10%. This paper is part of the MAPS special issue of the Astrophysical Journal Supplement. ","Molecules with ALMA at Planet-forming Scales (MAPS) XVII: Determining
  the 2D Thermal Structure of the HD 163296 Disk"
103,1438145148433666052,3075669963,Filip,"[""I'm glad I finally uploaded this paper that emerged from my master thesis: \nAn effective 3-d dS cosmology in string theory.  <LINK> \n\nNow my PhD begins with new adventures.""]",https://arxiv.org/abs/2109.06801,"Motivated by the lack of consensus on whether or not de Sitter (dS) lies in the Swampland, we use a recently developed braneworld construction and known Anti-de Sitter (AdS) vacua to compute an explicit effective dS cosmology in three dimensions. We consider a non-perturbative AdS$_4$ vacuum decaying to another lower AdS$_4$ vacuum via bubble-nucleation. We also consider the more speculative case where a dS$_4$ decay to a Minkowski$_4$. The Israel junction conditions are solved across the bubble and we obtain the Friedmann equations from which the cosmological constants can be read off, in the respective cases. The cosmological constants are computed in a flux background yielding small positive values admitting a dS cosmology. However, we find that an energetically viable model in the AdS to AdS case requires more fine-tuning. ",An effective three-dimensional de Sitter cosmology in string theory
104,1438107325470969857,562815963,Carmine Ventre,"['New paper on systemic risk in financial systems out at <LINK>. Assume we have a network modelling assets and liabilities of the bank in the system. Question of interest: Can we quickly compute the exposure rate of each bank to defaults in the system? [1/6]', 'The answer is yes if there are no financial derivatives (conditional obligations) in the network. [2/6]', 'When we introduce derivatives, specifically Credit Default Swaps (CDS), the problem is complete for PPAD if one is content with ""almost"" solutions that could grossly under- and over-estimate each bank\'s exposure. [3/6]', 'What about solutions where the rate is precise to say 1%? We prove that computing these ""strong"" approximations up to any given precision is FIXP-complete. [4/6]', 'This captures hardness due to numerical aspects (in particular, the irrationality of the actual solution) in addition to the combinatorial issues modelled by PPAD. [5/6]', 'We also study the relationship between the network structure and the (ir)rationality of the solution. Our results support a ban of purely speculative naked CDS and uncover a connection between FIXP and efficient algorithms in the real computational model of Blum-Shub-Smale. [6/6]']",https://arxiv.org/abs/2109.06608,"Financial networks model a set of financial institutions (firms) interconnected by obligations. Recent work has introduced to this model a class of obligations called credit default swaps, a certain kind of financial derivatives. The main computational challenge for such systems is known as the clearing problem, which is to determine which firms are in default and to compute their exposure to systemic risk, technically known as their recovery rates. It is known that the recovery rates form the set of fixed points of a simple function, and that these fixed points can be irrational. Furthermore, Schuldenzucker et al. (2016) have shown that finding a weakly (or ""almost"") approximate (rational) fixed point is PPAD-complete. We further study the clearing problem from the point of view of irrationality and approximation strength. Firstly, we observe that weakly approximate solutions may misrepresent the actual financial state of an institution. On this basis, we study the complexity of finding a strongly (or ""near"") approximate solution, and show FIXP-completeness. We then study the structural properties required for irrationality, and we give necessary conditions for irrational solutions to emerge: The presence of certain types of cycles in a financial network forces the recovery rates to take the form of roots of non-linear polynomials. In the absence of a large subclass of such cycles, we study the complexity of finding an exact fixed point, which we show to be a problem close to, albeit outside of, PPAD. ","Strong Approximations and Irrationality in Financial Networks with
  Financial Derivatives"
105,1438106066462593027,891694882934341632,Yupei Du,"['Glad to share our new EMNLP paper “Assessing the Reliability of Word Embedding Gender Bias Measures” #EMNLP2021! Joint work w/ @qixiangfang @dongng \nWe investigate the consistency of word embedding gender bias scores from various measures.\nPre-print: <LINK>\n(1/7) <LINK>', 'A key challenge in developing bias measures is that social biases are not directly observable but have to be *inferred* from data, making the resulting bias scores more prone to measurement errors. In this paper, we focus on one aspect of measurement quality: reliability.\n(2/7)', 'Reliability concerns the extent to which a measure produces consistent results (for embedding bias measures, results are bias scores). By drawing from measurement theory, we propose an approach to evaluate three types of reliability. \n(3/7)', 'These are:\n1. consistency over different random seeds used in word embedding training (test-retest reliability).\n2. consistency across different scoring rules (inter-rater consistency).\n3. consistency within query and gender base pairs (internal consistency).\n(4/7)', 'Our results show that bias scores are mostly consistent across random seeds (i.e. high test-retest reliability) and target words in the same query (i.e. high internal consistency). However, scoring rules fail to agree with each other (i.e. low inter-rater consistency)\n(5/7)', 'We also analyze the effects of various factors (e.g. word properties, embedding algorithms, training corpora) on the reliability scores of target words. We find that word embedding algorithms have a large influence. \n(6/7)', 'Want to know more? Check out our paper!!\n\nCode:  https://t.co/JKJedvcXzC\n(7/7)']",https://arxiv.org/abs/2109.04732,"Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures' reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures. ",Assessing the Reliability of Word Embedding Gender Bias Measures
106,1438098267569537024,945445796098473984,Patrick Schnider,"['A new paper on arXiv. In this one, I show that computing a set of cuts to fairly divide a pizza is PPA-complete. So if you‘re ever accused of not sharing your pizzas fairly, now you have a mathematical excuse :)\n\n<LINK>']",https://arxiv.org/abs/2109.06752,"Assume you have a 2-dimensional pizza with $2n$ ingredients that you want to share with your friend. For this you are allowed to cut the pizza using several straight cuts, and then give every second piece to your friend. You want to do this fairly, that is, your friend and you should each get exactly half of each ingredient. How many cuts do you need? It was recently shown using topological methods that $n$ cuts always suffice. In this work, we study the computational complexity of finding such $n$ cuts. Our main result is that this problem is PPA-complete when the ingredients are represented as point sets. For this, we give a new proof that for point sets $n$ cuts suffice, which does not use any topological methods. We further prove several hardness results as well as a higher-dimensional variant for the case where the ingredients are well-separated. ",The complexity of sharing a pizza
107,1438050997730004992,776765039726460929,Carlo Felice Manara,['Do not just look at #MAPS papers.... Today there is also #PENELLOPELP paper II on @arxiv \n<LINK>\nPENELLOPE II. CVSO 104: a pre-main sequence close binary with an optical companion in Ori OB1 by Frasca et al.\nA new spectroscopic binary was discovered in our data! <LINK>'],https://arxiv.org/abs/2109.06305,"We present results of our study of the close pre-main sequence spectroscopic binary CVSO 104 in Ori OB1, based on data obtained within the PENELLOPE legacy program. We derive, for the first time, the orbital elements of the system and the stellar parameters of the two components. The system is composed of two early M-type stars and has an orbital period of about 5 days and a mass ratio of 0.92, but contrarily to expectations does not appear to have a tertiary companion. Both components have been (quasi-)synchronized, but the orbit is still very eccentric. The spectral energy distribution clearly displays a significant infrared excess compatible with a circumbinary disk. The analysis of HeI and Balmer line profiles, after the removal of the composite photospheric spectrum, reveals that both components are accreting at a similar level. We also observe excess emission in H$\alpha$ and H$\beta$, which appears redshifted or blueshifted by more than 100 km/s with respect to the mass center of the system depending on the orbital phase. This additional emission could be connected with accretion structures, such as funnels of matter from the circumbinary disk. We also analyze the optical companion located at about 2"".4 from the spectroscopic binary. This companion, that we named CVSO 104B, turns out to be a background Sun-like star not physically associated with the PMS system and not belonging to Ori OB1. ","PENELLOPE II. CVSO 104: a pre-main sequence close binary with an optical
  companion in Ori OB1"
108,1438035367886790658,204501916,Francesco Sannino,"['Do we need a  Health (Corona) Pass?          \nWhat kind of Pass is most effective and why?\n\nWe answer all of the above and much more in our new paper!\n#coronarivus #pandemics <LINK>', '@ewa_szczurek Thanx, for the reference. Very relevant indeed!']",https://arxiv.org/abs/2109.06525,"We study the impact on the epidemiological dynamics of a class of restrictive measures that are aimed at reducing the number of contacts of individuals who have a higher risk of being infected with a transmittable disease. Such measures are currently either implemented or at least discussed in numerous countries worldwide to ward off a potential new wave of COVID-19 across Europe. They come in the form of Health Passes (HP), which grant full access to public life only to individuals with a certificate that proves that they have either been fully vaccinated, have recovered from a previous infection or have recently tested negative to SARS-Cov-19 . We develop both a compartmental model as well as an epidemic Renormalisation Group approach, which is capable of describing the dynamics over a longer period of time, notably an entire epidemiological wave. Introducing different versions of HPs in this model, we are capable of providing quantitative estimates on the effectiveness of the underlying measures as a function of the fraction of the population that is vaccinated and the vaccination rate. We apply our models to the latest COVID-19 wave in several European countries, notably Germany and Austria, which validate our theoretical findings. ",Effective Mathematical Modelling of Health Passes during a Pandemic
109,1437975500341714949,1658162341,Narayanan Rengaswamy,"['First paper out of postdoc @azengineering out! See <LINK>. We propose a QEC based GHZ distillation protocol, inspired by the Bell pair distillation of @markwilde in <LINK>. Byproduct: new method to generate logical Pauli operators for codes. <LINK>', 'Joint work with @rainarocks @nithinitzme and Prof. Bane Vasić. Implementation available online: https://t.co/oyd2cEZs9i', 'Besides the main protocol, we also discuss variations that might be more practical for certain network topologies. We work out a detailed example with a 3 qubit code to show the subtleties in the steps of the protocol. This example should help develop protocol variations.', 'The key step to the protocol is a new property of GHZ states that forms our main result in Theorem 6. It considers stabilizer measurements on one subsystem and shows the equivalent code on the remaining two subsystems. It builds on an ""extended"" transpose trick from Bell pairs.']",https://arxiv.org/abs/2109.06248,"Entanglement distillation is a well-studied problem in quantum information, where one typically starts with $n$ noisy Bell pairs and distills $k$ Bell pairs of higher fidelity. While distilling Bell pairs is the canonical setting, it is important to study the distillation of multipartite entangled states because these can be useful for realizing distributed algorithms on quantum networks. In this paper, we study the distillation of GHZ states using quantum error correcting codes (QECCs). Using the stabilizer formalism, we begin by explaining the QECC-based Bell pair distillation protocol in arXiv:0708.3699, which relies particularly on the transpose symmetry between Alice's and Bob's qubits in Bell states. Extending this idea, we show that, given $n$ GHZ states, performing a matrix on Alice's qubits is equivalent to performing a ""stretched"" version of the transpose of the matrix on the qubits of Bob and Charlie. We call this mapping to the stretched version of the matrix the GHZ-map, and show that it is an algebra homomorphism. Using this property, we show that Alice projecting her qubits onto an $[[n,k]]$ stabilizer code implies the simultaneous projection of Bob's and Charlie's qubits onto an induced $[[2n,k]]$ stabilizer code. Guided by this insight, we develop a GHZ distillation protocol based on local operations and classical communication that uses any stabilizer code. Inspired by stabilizer measurements on GHZ states, we also develop a new algorithm to generate logical Pauli operators of any stabilizer code and use it in the protocol. Since quantum codes with finite rate and almost linear minimum distance have recently been discovered, this paper paves the way for high-rate high-output-fidelity GHZ distillation. We provide simulation results on the $5$-qubit perfect code to emphasize the importance of the placement of a certain local Clifford operation in the protocol. ",Distilling GHZ States using Stabilizer Codes
110,1437951318862221320,767085056636522497,Udit Arora,"['1/5 New paper @emnlpmeeting!\n\n“Types of Out-of-distribution Texts and How to Detect Them” with @WillHuang93 and @hhexiy: <LINK>.\n\nTL;DR: Our results call for an explicit definition of OOD examples when evaluating different detection methods.', '2/5 There is little consensus on a rigorous definition of OOD examples in text. We categorize these as exhibiting a background or semantic shift and evaluate the performance of 2 common methods, density estimation and calibration, on 14 ID/OOD dataset pairs and 8 challenge pairs. https://t.co/vbPSe1E2Lk', '3/5 We also construct a toy OOD detection problem using a binary classification setting to remove estimation errors and study optimal calibration and density estimation detectors under controlled semantic and background shifts. https://t.co/PgUOvD4CKs', '4/5 For both simulated and natural text, we find that density estimation methods outperform calibration methods under background shifts while the opposite is true under semantic shifts. However, we find several failure cases from challenge examples that target model shortcomings. https://t.co/mXzwyPkHtR', '5/5 While this simplified framework explains much of the differences between the two methods, challenge examples highlight the room for better frameworks and a more explicit definition of OOD to progress the development of OOD detection methods and benchmarks.', '@ramitsawhney @emnlpmeeting @WillHuang93 @hhexiy Thanks Ramit! Hope to see you at EMNLP again this year :)', '@ramitsawhney @emnlpmeeting @WillHuang93 @hhexiy Hopefully in-person, if everything works out. What about you?', ""@ramitsawhney @emnlpmeeting @WillHuang93 @hhexiy Haha that's fair. Looking forward to it!""]",https://arxiv.org/abs/2109.06827,"Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of OOD examples and how to best detect them. We categorize these examples by whether they exhibit a background shift or a semantic shift, and find that the two major approaches to OOD detection, model calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings, while performing worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, highlighting a weak spot for current methods. Since no single method works well across all settings, our results call for an explicit definition of OOD examples when evaluating different detection methods. ",Types of Out-of-Distribution Texts and How to Detect Them
111,1437822574369460224,954465907539152897,Dr. Doctor,"[""It's the 6 year anniversary of GW150914, the first gravitational wave ever directly detected, so what better time than now for a new paper led by @EdelmanBruce! \n<LINK>\nWe measure the binary black hole merger mass distribution using a new semi-parametric model!"", 'The cool thing about the semi-parametric model that we developed is that it can fit the larger trends of the black hole mass distribution with a power-law (lots of low-mass BHs, not as many high-mass), while simultaneously fitting smaller scale features with a spline.', 'The model is ""data-driven"" rather than ""astrophysics driven"" which enables us to measure the mass distribution in an agnostic manner. Yet we recover the same features seen when using the physics-inspired models!', 'In particular, we find the same bump in the mass distribution around 35 Msun that was found by the LIGO-Virgo Collaboration via a model (from Talbot and Thrane) with an explicit peak in it! https://t.co/vQrQjTgicJ', ""So what is the bump? Probably the best guess out there now is that it's a feature due to the pair instability in massive stars.  Electron-positron pair production in a massive star causes the star to either lose mass down to a core of ~35M or blow up entirely."", 'This ""(Pulsational) Pair Instability Supernova"" phenomenon leads to a bump and fall-off in the mass distribution. Whether that\'s *actually* causing the peak in the observed mass distribution of black holes is still an open question, but it\'s a good guess based on theory.', ""Our analysis is also finding some possible structure at lower masses ~10 Msun. However, there isn't enough data now to resolve that structure, if it exists at all. Like always, we just want more data!!""]",https://arxiv.org/abs/2109.06137,"We introduce a semi-parametric model for the primary mass distribution of binary black holes (BBHs) observed with gravitational waves (GWs) that applies a cubic-spline perturbation to a power law. We apply this model to the 46 BBHs included in the second gravitational wave transient catalog (GWTC-2). The spline perturbation model recovers a consistent primary mass distribution with previous results, corroborating the existence of a peak at $35\,M_\odot$ ($>97\%$ credibility) found with the \textsc{Powerlaw+Peak} model. The peak could be the result pulsational pair-instability supernovae (PPISNe). The spline perturbation model finds potential signs of additional features in the primary mass distribution at lower masses similar to those previously reported by Tiwari and Fairhurst (2021). However, with fluctuations due to small number statistics, the simpler \textsc{Powerlaw+Peak} and \textsc{BrokenPowerlaw} models are both still perfectly consistent with observations. Our semi-parametric approach serves as a way to bridge the gap between parametric and non-parametric models to more accurately measure the BBH mass distribution. With larger catalogs we will be able to use this model to resolve possible additional features that could be used to perform cosmological measurements, and will build on our understanding of BBH formation, stellar evolution and nuclear astrophysics. ","Ain't No Mountain High Enough: Semi-Parametric Modeling of LIGO-Virgos
  Binary Black Hole Mass Distribution"
112,1437794712027533313,458731223,Alberto Testoni,"['Happy to share our (with @raffagbernardi) new paper ""Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy"" that will appear at the #EMNLP2021 main conference. The preprint is available here: <LINK>', 'Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model’s conjecture. https://t.co/lr1ACOr11U', 'We take the GuessWhat task as a test-bed. Confirm-it simulates an internal Oracle agent, and it selects the question that, if answered as expected by the model, helps the most in confirming the initial conjecture about the target. https://t.co/2LWeU6Shi4', 'By running a human-based evaluation, we show that dialogues generated by Confirm-it are more natural and effective. Interestingly, our re-ranking algorithm reduces repetitions and hallucinations in the output, while increasing the accuracy and the number of novel questions.']",https://arxiv.org/abs/2109.05312,"Generating goal-oriented questions in Visual Dialogue tasks is a challenging and long-standing problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model's conjecture about the referent. We take the GuessWhat?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking. ","Looking for Confirmations: An Effective and Human-Like Visual Dialogue
  Strategy"
113,1437768571149508620,112654363,Torsten Scholak,"['New paper: <LINK>!\nIntroducing PICARD - a simple &amp; effective constrained beam search algorithm for any language model.\nPICARD helps with generating valid code, which is useful for program synthesis and semantic parsing.\nWe achieve SoTA on both Spider and CoSQL\n🧵 <LINK>', ""Generating valid code is hard for language models. On Spider, even a fine-tuned T5-3B model generates about 12% invalid SQL queries! That's why models like BART and T5 have not been competitive with the best models on Spider - yet.\nThis changes with PICARD."", 'We find that a T5-Base model with PICARD can outperform a T5-Large model without it, and likewise for a T5-Large and T5-3B model. By using PICARD, a T5-3B model is improved to SoTA performance on Spider and also CoSQL. https://t.co/31AjCMh3IH', 'PICARD is easier to use than existing constrained decoding methods:\nIt integrates with beam search. It doesn’t need training or changing the model. It works with any model generating token sequences (including language models). It doesn’t need a special vocabulary or tokenizer. https://t.co/wCoNOoAJSz', ""PICARD doesn't need large beam sizes to work well. Improvements are substantial for beam sizes of 2 and 4, and larger beam sizes are not significantly better. https://t.co/6XWPzmdAg7"", 'We only show results on SQL, but in principle we can also use PICARD to generate valid code for other languages (e.g., Python) and DSLs (e.g., Spark). The method is that general.', 'There are more results and ablations in the paper. Thanks to my coauthors @nschucher and @DBahdanau! 🙏\nThere will be more content and updates on PICARD from @elementai and @servicenow.\nCode and models will be released soon at https://t.co/9Ls4TGZUXt, so stay tuned!', 'Thanks also to @lzamparo, @ptshaw2, Yusen Zhang, and @taoyds. Lee helped with the CoSQL experiments, Pete gave instructions on how to reproduce his earlier T5 results on Spider, and Yusen and Tao worked hard on evaluating the model on the hidden Spider and CoSQL test sets.']",https://arxiv.org/abs/2109.05093,"Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at this https URL), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions. ","PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding
  from Language Models"
114,1437674071244771329,3867814873,Prathamesh,['New paper on over-the-air optimization with @shubhamjha23  and @hstyagi.\n<LINK> <LINK>'],https://arxiv.org/abs/2109.05222,"We consider over-the-air convex optimization on a $d-$dimensional space where coded gradients are sent over an additive Gaussian noise channel with variance $\sigma^2$. The codewords satisfy an average power constraint $P$, resulting in the signal-to-noise ratio (SNR) of $P/\sigma^2$. We derive bounds for the convergence rates for over-the-air optimization. Our first result is a lower bound for the convergence rate showing that any code must slowdown the convergence rate by a factor of roughly $\sqrt{d/\log(1+\mathtt{SNR})}$. Next, we consider a popular class of schemes called $analog$ $coding$, where a linear function of the gradient is sent. We show that a simple scaled transmission analog coding scheme results in a slowdown in convergence rate by a factor of $\sqrt{d(1+1/\mathtt{SNR})}$. This matches the previous lower bound up to constant factors for low SNR, making the scaled transmission scheme optimal at low SNR. However, we show that this slowdown is necessary for any analog coding scheme. In particular, a slowdown in convergence by a factor of $\sqrt{d}$ for analog coding remains even when SNR tends to infinity. Remarkably, we present a simple quantize-and-modulate scheme that uses $Amplitude$ $Shift$ $Keying$ and almost attains the optimal convergence rate at all SNRs. ","Fundamental limits of over-the-air optimization: Are analog schemes
  optimal?"
115,1437655382994956292,795620317532143616,Jonas Pfeiffer,"['In our paper\n\nxGQA: Cross-Lingual Visual Question Answering\n\nWe propose a new multilingual multimodal benchmark, covering 7 new typologically diverse languages\n📃 <LINK>\n🌐 <LINK>\n@GregorGeigle @ashkamath20 @jmsteitz @stefanroth @licwu @IGurevych <LINK>', 'We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. https://t.co/omuxSK3E17', 'While our adapter-based architecture outperforms the SotA  M3P model in cross-lingual zero-shot scenarios, the overall transfer performance remains low across the board, with an average drop of around 38 accuracy points across target languages. https://t.co/EIjLKsQ01e', 'This demonstrates the inherent difficulty of the task, even though the corresponding questions are ar- guably simple, containing only 8.5 words on average.', 'In few-shot scenarios we find that utilizing an increasing amount of data instances in the target language consistently improves accuracy, culminating in an improvement of up to 20 accuracy points when specializing the model with only 48 images in the target language. https://t.co/PL68ttBlgT', 'Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, which can only be partially recovered through few-shot learning, calling for more sophisticated methods for vision and multilingual language modeling.', '@srchvrs I agree that it’s surprising that it works well for text-only tasks—which has been throughly investigated in the past (i.e. https://t.co/gDLr8gFZeK @PDufter)—however, what surprised me even more, was this doesn’t seem to translate to scenarios where we add an additional modality.']",https://arxiv.org/abs/2109.06082,"Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and -- vice versa -- multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. ",xGQA: Cross-Lingual Visual Question Answering
116,1437619518927302656,1248312505782460416,Daichi Kashino | 柏野 大地,['Our new paper showed up on arXiv!\nWe measured the stellar mass vs. stellar metallicity (Fe abundance) relation using 1336 galaxies at z~2.\n<LINK>'],https://arxiv.org/abs/2109.06044,"We measure the relationship between stellar mass and stellar metallicity, the stellar mass--metallicity relation (MZR), for 1336 star-forming galaxies at $1.6\le z\le3.0$ (<z>=2.2) using rest-frame far-ultraviolet spectra from the zCOSMOS-deep survey. High signal-to-noise composite spectra containing stellar absorption features are fit with population synthesis model spectra of a range of metallicity. We find stellar metallicities, which mostly reflect iron abundances, scaling as $(Z_{Fe,\ast}/Z_{Fe,\odot})=-(0.81\pm0.01)+(0.32+0.03)\log(M_\ast/10^{10}M_\odot)$ across the mass range of $10^9\lesssim M_\ast/M_\odot\lesssim10^{11}$, being $\approx6\times$ lower than seen locally at the same masses. The instantaneous oxygen-to-iron ratio ($\alpha$-enhancement) inferred using the gas-phase oxygen MZRs, is on average found to be [O/Fe]$\approx0.47$, being higher than the local [O/Fe]$\approx0$. The observed changes in [O/Fe] and [Fe/H] are reproduced in simple flow-through gas-regulator models with steady star-formation histories (SFHs) that follow the evolving main sequence. Our models show that the [O/Fe] is determined almost entirely by the instantaneous specific star formation rate alone while being independent of the SFHs, mass, and the gas-regulation characteristics of the systems. We find that the locations of $\sim10^{10}M_\odot$ galaxies at z~2 in the [O/Fe]--metallicity planes are in remarkable agreement with the sequence of low-metallicity thick-disk stars in our Galaxy. This manifests a beautiful concordance between the results of Galactic archaeology and observations of high-redshift Milky Way progenitors. However, there remains a question of how and when the old metal-rich, low-$\alpha$/Fe stars seen in the bulge had formed by z~2 because such a stellar population is not seen in our data and difficult to explain in the context of our models. ","The stellar mass versus stellar metallicity relation of star-forming
  galaxies at $1.6\le z\le3.0$ and implications for the evolution of the
  $\alpha$-enhancement"
117,1437535827177836545,1196266674954985472,Nirmal Raj,"['1/n\nMy new friend &amp; collaborator @BradleyKavanagh has written a nice, compact thread,\n<LINK>,\n\n on our paper with @josephbramante,\n <LINK>\n\nHere is a wordier and more layperson version. <LINK>', '2/n\nDark matter particles in the Galaxy can be thought of as interstellar flour. Our paper poses and tries to answer a simple question. If the dark flour is lumped into balls, and the Earth is in a void between lumps, can we ever know if #darkmatter could touch visible objects?', ""3/n \nDiscovering dark matter's ability to touch (versus just going through) would greatly help in identifying it. Detectors on Earth look for this dark touch ('dark matter astronomy' in my former thread) by implicitly assuming that dark particles are flying in as unlumped flour."", '4/n\nBut we have no evidence for that: all we know about the consistency of dark matter is that it is smooth over size resolutions larger than ~1000 light-years. Whether or not at smaller scales it is lumped depends on its genesis story, one of the great unknowns of cosmology.', '5/n\nWe came up with 3 strategies to probe the  scattering nature of far-flung dark lumps. The first is original, the next 2 are adaptations of neat ideas.', '6/n\n(1) Neutron stars. From time to time, these ubiquitous super-dense neutron balls could run into dark lumps, which would smack into and brighten them. Depending on the frequency of these encounters, neutron stars either shine steady and lukewarm...', '7/n \n...to be picked up by a soon-to-launch telescope like @NASAWebb, or they blaze hot briefly before cooling down: findable in sky surveys. We show that via this method, the coldest known neutron star already sets limits on dark lumps in which the dark particles self-interact.', '8/n\n(2) Cosmic rays. The Galaxy is filled with a fog of free-flying protons, and these can strike distant lumps. That would either eject dark particles and send them the way of  Earth-bound detectors, or would burst into flashes of light.', '9/n \n(3) Old rocks. Perhaps the Earth is floating in an inter-lump void today, but this need not have always been so. Over its few billion years of existence, it may have encountered many lumps, acquiring telltale scars findable in ancient minerals from deep underground.', ""10/n\n(There's a fun paper by Juan Collar on how these encounters could have triggered mass extinctions in the past: https://t.co/hyTQVl1tmQ.)"", '11/n\nAll in all, these strategies would help us find lumps over a pretty vast range in masses, between the mass of a typical lake to that of the Sun. https://t.co/wcStAQmcWs', ""12/n\nOne of the funnest projects I've worked on, due in no small part to my collaborators' all-round awesomeness. To balance things out @arxiv auto-re-classified our paper to hep-ex after making us wait for 5 days. This is becoming all too common?!"", '13/n\nOur research was performed at @TRIUMF, @queensu, and @IFCA_CSIC_UC.', '@davemckeen @triumf @queensu @IFCA_CSIC_UC Crap, I did it again! 🤣']",https://arxiv.org/abs/2109.04582,"In many cosmologies dark matter clusters on sub-kiloparsec scales and forms compact subhalos, in which the majority of Galactic dark matter could reside. Null results in direct detection experiments since their advent four decades ago could then be the result of extremely rare encounters between the Earth and these subhalos. We investigate alternative and promising means to identify subhalo dark matter interacting with Standard Model particles: (1) subhalo collisions with old neutron stars can transfer kinetic energy and brighten the latter to luminosities within the reach of imminent infrared, optical, and ultraviolet telescopes; we identify new detection strategies involving single-star measurements and Galactic disk surveys, and obtain the first bounds on self-interacting dark matter in subhalos from the coldest known pulsar, PSR J2144-3933, (2) subhalo dark matter scattering with cosmic rays results in detectable effects, (3) historic Earth-subhalo encounters can leave dark matter tracks in paleolithic minerals deep underground. These searches could discover dark matter subhalos weighing between gigaton and solar masses, with corresponding dark matter cross sections and masses spanning tens of orders of magnitude. ","Scattering searches for dark matter in subhalos: neutron stars, cosmic
  rays, and old rocks"
118,1437415180644847619,561899047,Aki Vehtari,"['New paper ""Latent space projection predictive inference""\nwith @AleexCatalina and @paulbuerkner  <LINK> <LINK>', 'Projection predictive inference project the posterior to a restricted parameter space, e.g., restricting some coefficients to 0. This can be used for low variance variable selection and inference after the selection.', 'Lindley (1968) presented the approach for normal linear model with known variances. Goutis &amp; Robert (1988) and presented computationally feasible approach for generalized linear models. Piironen &amp; Vehtari (2017), and Piironen et al. (2020) presented many practical improvements.', 'So far the approach has been based on the projection minimizing KL-divergence from the projected predictive distribution to the full posterior predictive distribution. In case of exponential family, these projections can be solved with maximum likelihood for each posterior draw.', 'Minimizing KL-divergence for non-exponential family distributions is more difficult. We present here that we can do the projection by minimizing the KL divergence from the approximate latent posterior to the restricted latent posterior.', 'With many non-exponential family models, the parameterization is already chosen so that the latent posterior is close to normal. If we approximate the latent posterior with normal, we get back to fast projection for each posterior draw of the other parameters.', 'With this, we can extend our projpred package to handle, e.g., ordinal and survival models, which users have been asking many times. It turns out that the approach improves also projection predictive inference for hierarchical models and non-normal exponential family models.', ""For eager ones, the code is in laten_projection branch of the projpred github repo. We're working on a vignette and additional testing, before merging to main branch.""]",https://arxiv.org/abs/2109.04702,"Given a reference model that includes all the available variables, projection predictive inference replaces its posterior with a constrained projection including only a subset of all variables. We extend projection predictive inference to enable computationally efficient variable and structure selection in models outside the exponential family. By adopting a latent space projection predictive perspective we are able to: 1) propose a unified and general framework to do variable selection in complex models while fully honouring the original model structure, 2) properly identify relevant structure and retain posterior uncertainties from the original model, and 3) provide an improved approach also for non-Gaussian models in the exponential family. We demonstrate the superior performance of our approach by thoroughly testing and comparing it against popular variable selection approaches in a wide range of settings, including realistic data sets. Our results show that our approach successfully recovers relevant terms and model structure in complex models, selecting less variables than competing approaches for realistic datasets. ",Latent space projection predictive inference
119,1437393852227276801,3378033143,Katerina Margatina,"['💥Our #EMNLP2021 paper is now on Arxiv! We propose a new acquisition function for active learning that leverages both uncertainty and diversity sampling by acquiring ⚡️contrastive⚡️ examples.⬇️\n\n💻code: <LINK>\n📝pre-print: <LINK>\n\n(1/n)', 'We hypothesize that data points that are close in the model feature space but the model produces different predictive likelihoods, should be good candidates for data acquisition. We define such examples as contrastive. (2/n) https://t.co/WOtJaclHyS', 'Our method, Contrastive Active Learning (CAL), selects unlabeled data points from the pool, whose predictive likelihoods diverge the most from their neighbors in the training set. (3/n) https://t.co/I4j2H7iCJO', 'This way, CAL shares similarities with diversity sampling, but instead of performing clustering it uses the feature space to create neighborhoods. CAL also leverages uncertainty, by using predictive likelihoods to rank the unlabeled data. (4/n)', 'We empirically show that CAL performs consistently better or equal compared to all baseline acquisition functions, in 7 datasets from 4 NLP tasks, when evaluated on in-domain and out-of-domain settings. (5/n) https://t.co/55Me01ODA9', 'We finally conduct a thorough analysis of our method showing that CAL achieves a better trade-off between diversity and uncertainty compared to the other acquisition functions. (6/n) https://t.co/QQ2gcFfsig', 'Feel free to check our paper for more details🔍! I would like to thank my co-authors again for their incredible help &amp; support on this work! @gvernikos @LoicBarrault @nikaletras 😊\n \n(7/7) n=7']",https://arxiv.org/abs/2109.03764,"Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies. ",Active Learning by Acquiring Contrastive Examples
120,1437388857415188489,13800042,Lukas Heinrich,"['Our new paper on publishing statistical models is out! This has been a long time coming with lots of progress in the last years to make it a reality. This is *one of the best* data products we have and making them public is the right thing to do.\n<LINK> <LINK>', '@TJGershon an important  point is ""we are!"" https://t.co/PXkXGjR1cQ . It\'s not a pure hypothetical but a reality already. The paper a plea to make it more standard practice. Admittedly, it happened only recently. We discuss reasons a bit (closed/open world) but it also is a lot of sociology', '@TJGershon it might not have been as obvious to everyone (often heard weariness that information is too detailed, difficult to interpret, etc) but shared tooling and documentation imho are the way to go. The community can handle the truth.']",https://arxiv.org/abs/2109.04981,"The statistical models used to derive the results of experimental analyses are of incredible scientific value and are essential information for analysis preservation and reuse. In this paper, we make the scientific case for systematically publishing the full statistical models and discuss the technical developments that make this practical. By means of a variety of physics cases -- including parton distribution functions, Higgs boson measurements, effective field theory interpretations, direct searches for new physics, heavy flavor physics, direct dark matter detection, world averages, and beyond the Standard Model global fits -- we illustrate how detailed information on the statistical modelling can enhance the short- and long-term impact of experimental results. ","Publishing statistical models: Getting the most out of particle physics
  experiments"
121,1437372545091637251,2548806176,Jonathan Freundlich,"['In our new paper with @benfamaey, P.A. Oria, M. Bilek, @VoltarCH and R. Ibata, we explored how well the ultra-diffuse galaxies (UDGs) of the Coma cluster agree (or not) with MOND expectations, and with scaling relations observed for field galaxies: <LINK>', '1. Overall, we find that the measured velocity dispersions are in impressive agreement with isolated MOND predictions: if these objects would not be residing in clusters, it seems to us they would represent a resounding success for MOND. https://t.co/DQbTa4scGO', '2. This means that those UDGs appear to follow the same scaling relations (predicted by MOND) as field galaxies, e.g. the radial acceleration relation. https://t.co/p7fjlJtsSO', ""3. However, MOND predictions with the expected 'external field effect' (EFE) from the cluster are not that successful. https://t.co/uDX2FHrZ6b"", '4. Since the on-sky distribution of the sample is not compatible with a large hole in the center of the cluster, we reject the possibility that all the UDGs are actually far away enough to fully avoid the EFE.', '5. We note that the galaxies should be disrupted by tides in the MOND EFE context, which could inflate their velocity dispersions. But it would then appear as a bit of a conspiracy that the resulting velocity dispersions would be so well in line with the isolated MOND predictions', '6. We then discuss the possibility of higher stellar mass-to-light ratios and/or baryonic dark matter in light of these new results.', ""7. A mass-follows light 'boost' of the stellar mass ratio by a factor ~10 would be in line with the data, but it may again appear as a strange coincidence that it leads exactly to the isolated MOND predictions."", '8. We also discuss the fact that some MOND theories could avoid the EFE altogether, but then it would cause problems with, e.g. wide binaries in the Milky Way.', ""9. Then we note that 'extended MOND' (EMOND, https://t.co/ledSNX2OiV) would rather naturally predict a 10 times higher value of the acceleration a0, which would quite systematically solve the problem."", '10. The last speculative hypothesis that we put forward in the MOND context, which is our currently preferred one, is that maybe the inefficiency of the EFE and the residual missing mass in clusters are two faces of the same coin.', '11. The fact that the EFE appears to be screened in clusters, precisely where MOND needs additional mass, might not be a coincidence? This would be the case with superfluid dark matter, but also perhaps in a theory like that of Skordis &amp; Zlosnik (https://t.co/Fz1l8WLpi5)', '12. In this theory, the scalar field would play both the role of additional ""dust"" in clusters and the role of the TeVeS scalar field within galaxies: it is plausible it cannot do both at the same time.', '13. This would kill the EFE in clusters but not in regions where the density of ""scalar field dust"" is low, as in the field or in small groups.', '14. Of course, we do not exclude that the isolated MOND predictions, both in the field and in galaxy clusters, would just be an emergent dark matter scaling relation. But even in that context, the fact that UDGs and field spirals share such a tight relation is surprising!', '15. It will be important and exciting to repeat similar analyses for dwarf elliptical and irregular galaxies in the outer regions of clusters, to continue to explore this fascinating question.']",http://arxiv.org/abs/2109.04487,"The tight radial acceleration relation (RAR) obeyed by rotationally supported disk galaxies is one of the most successful a priori predictions of the modified Newtonian dynamics (MOND) paradigm on galaxy scales. Another important consequence of MOND as a classical modification of gravity is that the strong equivalence principle (SEP) -- which requires the dynamics of a small, free-falling, self-gravitating system not to depend on the external gravitational field in which it is embedded -- should be broken. Multiple tentative detections of this so-called external field effect (EFE) of MOND have been made in the past, but the systems that should be most sensitive to it are galaxies with low internal gravitational accelerations residing in galaxy clusters within a strong external field. Here, we show that ultra-diffuse galaxies (UDGs) in the Coma cluster do lie on the RAR, and that their velocity dispersion profiles are in full agreement with isolated MOND predictions, especially when including some degree of radial anisotropy. However, including a breaking of the SEP via the EFE seriously deteriorates this agreement. We discuss various possibilities to explain this within the context of MOND, including a combination of tidal heating and higher baryonic masses. We also speculate that our results could mean that the EFE is screened in cluster UDGs. The fact that this would happen precisely within galaxy clusters, where classical MOND fails, could be especially relevant to the nature of the residual MOND missing mass in clusters of galaxies. ","Probing the radial acceleration relation and the strong equivalence
  principle with the Coma cluster ultra-diffuse galaxies"
122,1437350656071852032,4866589137,Dr. Nathan Adams,"['New MIGHTEE paper on arXiv this morning! Led by Anastasia Ponomareva, MeerKAT data shows a lack of any evolution in the baryonic Tully-Fisher relation for z&lt;0.08. Full details are presented here: <LINK>']",https://arxiv.org/abs/2109.04992,"Using a sample of 67 galaxies from the MIGHTEE Survey Early Science data we study the HI-based baryonic Tully-Fisher relation (bTFr), covering a period of $\sim$one billion years ($0 \leq z \leq 0.081 $). We consider the bTFr based on two different rotational velocity measures: the width of the global HI profile and $\rm V_{out}$, measured as the outermost rotational velocity from the resolved HI rotation curves. Both relations exhibit very low intrinsic scatter orthogonal to the best-fit relation ($\sigma_{\perp}=0.07\pm0.01$), comparable to the SPARC sample at $z \simeq 0$. The slopes of the relations are similar and consistent with the $ z \simeq 0$ studies ($3.66^{+0.35}_{-0.29}$ for $\rm W_{50}$ and $3.47^{+0.37}_{-0.30}$ for $\rm V_{out}$). We find no evidence that the bTFr has evolved over the last billion years, and all galaxies in our sample are consistent with the same relation independent of redshift and the rotational velocity measure. Our results set up a reference for all future studies of the HI-based bTFr as a function of redshift that will be conducted with the ongoing deep SKA pathfinders surveys. ","MIGHTEE-HI: The baryonic Tully-Fisher relation over the last billion
  years"
123,1437343941691588608,1415338317764349959,Nuno Guerreiro,"['I am happy to announce our new #EMNLP2021 paper ""SPECTRA: Sparse Structured Text Rationalization"" w/ @andre_t_martins .\n\n📰 Paper: <LINK>', '@andre_t_martins Most work on selective rationalization focus on highlights extraction. Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. [1/n]', 'Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction. In our work, we present a framework for deterministic rationale extraction via constrained inference on a factor graph, forming a differentiable layer. [2/n]', 'By leveraging LP-SparseMAP (@vnfrombucharest 🙌) we are able to extract differently constrained structured explanations, such as sparse alignments between two documents. [3/n]', 'Finally, we also provide a comparative study between stochastic 🎲 and deterministic 🎯 methods for rationale extraction for both highlights and matchings extraction.', 'We also share the code for our library for selective rationalization  of highlights and matchings.\n\n💻️Code: https://t.co/4rkd4d0BTU']",https://arxiv.org/abs/2109.04552,"Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability. ",SPECTRA: Sparse Structured Text Rationalization
124,1437310404766486532,807327556072402945,Machel Reid,"['Super happy to finally share our new #EMNLP2021 paper ""AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages""! This has been a long time in the making!\n\nw/@JunjieHu12, @gneubig, @ymatsuo\n\n📃 <LINK>\n\n🧵Thread (1/N) <LINK>', 'We provide heuristics for cleaning and normalizing noisy data (such as JW300) --- e.g. removing data leakage, tokenization, non-sentences, etc.. --- and use this to develop a simple and reproducible benchmark for the 8 languages. https://t.co/nrBLTRvuga', 'We also investigate ""low-resource focused pre-training"" and develop two techniques to tackle this: (1) dictionary augmentation and (2) using pseudo-monolingual data. https://t.co/Qq8Z4OT9C8', 'Using these techniques we pre-train AfroBART for sequence generation in these languages. We show really cool results when constraining the number of parallel sentences at finetuning. With AfroBART (and 10k pairs) we can outperform a model using 5x the data! 🤯 https://t.co/hA4irepYrU', 'Also, we developed a tool to see performance taking into account noun classes/formatives! We can actually see that cross-lingual transfer helps mainly with noun classes/formatives shared between Xhosa/Zulu, but for unshared classes, it hurts performance. https://t.co/NLLrfXTuDh', 'Code, models, and data will be open-sourced at https://t.co/zPMBJN9aIg', 'Finally thank you to @anas_ant, @AditiC123, Edison Marrese-Taylor, and @siminyu_kat for helpful discussions and comments! And more broadly, thank you to @MasakhaneNLP for being a great inspiration -- being part African myself, #AfricanNLP has a special place in my heart!\n(N/N)']",https://arxiv.org/abs/2109.04715,"Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages. ","AfroMT: Pretraining Strategies and Reproducible Benchmarks for
  Translation of 8 African Languages"
125,1437263577140772868,1115880604560691200,NII Yamagishi Lab,"['Preprint of our new paper under review for T-BIOM, ""Master Face Attacks on Face Recognition Systems,"" an extension of our 2019 IJCB paper, is online:  <LINK>', 'We study latent variable evolution, a method for generating master faces, in various scenarios and with multiple databases &amp; face recognition systems to study the properties of master faces and to understand in which conditions strong master faces can be generated. https://t.co/R0DbhLLVHY', 'We hypothesize that master faces come from dense areas in the embedding space. Simulated presentation attacks generally preserve the false-matching ability of their original digital forms, thus demonstrating that the existence of master faces poses an actual threat. https://t.co/SLVVLOfKQp']",https://arxiv.org/abs/2109.03398,"Face authentication is now widely used, especially on mobile devices, rather than authentication using a personal identification number or an unlock pattern, due to its convenience. It has thus become a tempting target for attackers using a presentation attack. Traditional presentation attacks use facial images or videos of the victim. Previous work has proven the existence of master faces, i.e., faces that match multiple enrolled templates in face recognition systems, and their existence extends the ability of presentation attacks. In this paper, we perform an extensive study on latent variable evolution (LVE), a method commonly used to generate master faces. We run an LVE algorithm for various scenarios and with more than one database and/or face recognition system to study the properties of the master faces and to understand in which conditions strong master faces could be generated. Moreover, through analysis, we hypothesize that master faces come from some dense areas in the embedding spaces of the face recognition systems. Last but not least, simulated presentation attacks using generated master faces generally preserve the false-matching ability of their original digital forms, thus demonstrating that the existence of master faces poses an actual threat. ",Master Face Attacks on Face Recognition Systems
126,1437201099157643272,57793813,Teppei Katori (香取哲平),['New #nuxsec paper with Tianlu Yuan @uw_wipac and Juan Pablo Yanez @UAlbertaScience. Review of neutrino cross-section measurements using neutrino telescopes such as IceCube. A new field but fast growing!  @uw_icecube \n<LINK>'],https://arxiv.org/abs/2109.04430,"Neutrino telescopes can observe neutrino interactions starting at GeV energies by sampling a small fraction of the Cherenkov radiation produced by charged secondary particles. These experiments instrument volumes massive enough to collect substantial samples of neutrinos up to the TeV scale as well as small samples at the PeV scale. This unique ability of neutrino telescopes has been exploited to study the properties of neutrino interactions across energies that cannot be accessed with man-made beams. Here we present the methods and results obtained by IceCube, the most mature neutrino telescope in operation, and offer a glimpse of what the future holds in this field. ",Neutrino Interaction Physics in Neutrino Telescopes
127,1436392734080700420,928595497748541441,Ali Hassani,"['Check out our new paper! We brought convolution into the ""Mix"", reducing compute and increasing flexibility &amp; transferability.\nCode, training scripts and checkpoints are all available:\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2109.04454,"MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are publicly available at this https URL ",ConvMLP: Hierarchical Convolutional MLPs for Vision
128,1436313532186963982,1435616683062878215,David Bossens,['Want to build a repertoire of solutions that generalises well to new problems? Or customise a repertoire of solutions to any kind of meta-objective? QD-Meta might be your best bet. Paper available now at <LINK>.'],https://arxiv.org/abs/2109.03918,"Quality-Diversity (QD) algorithms evolve behaviourally diverse and high-performing solutions. To illuminate the elite solutions for a space of behaviours, QD algorithms require the definition of a suitable behaviour space. If the behaviour space is high-dimensional, a suitable dimensionality reduction technique is required to maintain a limited number of behavioural niches. While current methodologies for automated behaviour spaces focus on changing the geometry or on unsupervised learning, there remains a need for customising behavioural diversity to a particular meta-objective specified by the end-user. In the newly emerging framework of QD Meta-Evolution, or QD-Meta for short, one evolves a population of QD algorithms, each with different algorithmic and representational characteristics, to optimise the algorithms and their resulting archives to a user-defined meta-objective. Despite promising results compared to traditional QD algorithms, QD-Meta has yet to be compared to state-of-the-art behaviour space automation methods such as Centroidal Voronoi Tessellations Multi-dimensional Archive of Phenotypic Elites Algorithm (CVT-MAP-Elites) and Autonomous Robots Realising their Abilities (AURORA). This paper performs an empirical study of QD-Meta on function optimisation and multilegged robot locomotion benchmarks. Results demonstrate that QD-Meta archives provide improved average performance and faster adaptation to a priori unknown changes to the environment when compared to CVT-MAP-Elites and AURORA. A qualitative analysis shows how the resulting archives are tailored to the meta-objectives provided by the end-user. ","Quality-Diversity Meta-Evolution: customising behaviour spaces to a
  meta-objective"
129,1436309336003788803,1473984524,Keisuke Okumura,"['My new paper ""Solving Simultaneous Target Assignment and Path Planning Efficiently with Time-Independent Execution,"" w/Défago-sensei, is out on arXiv!\npaper: <LINK>\nwebsite: <LINK>\nrobot demo: <LINK> <LINK>']",https://arxiv.org/abs/2109.04264,"Real-time planning for a combined problem of target assignment and path planning for multiple agents, also known as the unlabeled version of Multi-Agent Path Finding (MAPF), is crucial for high-level coordination in multi-agent systems, e.g., pattern formation by robot swarms. This paper studies two aspects of unlabeled-MAPF: (1) offline scenario: solving large instances by centralized approaches with small computation time, and (2) online scenario: executing unlabeled-MAPF despite timing uncertainties of real robots. For this purpose, we propose TSWAP, a novel sub-optimal complete algorithm, which takes an arbitrary initial target assignment then repeats one-timestep path planning with target swapping. TSWAP can adapt to both offline and online scenarios. We empirically demonstrate that Offline TSWAP is highly scalable; providing near-optimal solutions while reducing runtime by orders of magnitude compared to existing approaches. In addition, we present the benefits of Online TSWAP, such as delay tolerance, through real-robot demos. ","Solving Simultaneous Target Assignment and Path Planning Efficiently
  with Time-Independent Execution"
130,1436237195074015233,16389141,Massimo Nicosia,['📄 Our new EMNLP paper is on arXiv! 📄\n\n1⃣ Train an mT5 filler model to reconstruct full parses from English utterances + parse signatures\n2⃣ Run it on translations and parse signatures to obtain high quality i18n synthetic data!\n\nMore here:👉 <LINK> 👈\n\n @Google'],https://arxiv.org/abs/2109.04319,"While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel Translate-and-Fill (TaF) method to produce silver training data for a multilingual semantic parser. This method simplifies the popular Translate-Align-Project (TAP) pipeline and consists of a sequence-to-sequence filler model that constructs a full parse conditioned on an utterance and a view of the same parse. Our filler is trained on English data only but can accurately complete instances in other languages (i.e., translations of the English training utterances), in a zero-shot fashion. Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques. ","Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with
  Synthetic Data"
131,1436210551739215872,4807828837,Vishal Upendran,"['☀️ New paper with @dktripathi accepted in ApJ, and out on arxiv !☀️\n<LINK>\n\nIn this work, we study the properties of the C II 1334 line which forms in the upper chromosphere, as a function of photospheric magnetic flux density (|B|) in Cor hole &amp; Quiet Sun (1/5)', 'We find CHs to show reduced intensity, excess blueshifts (only blueshifted pixels), excess redshifts (only redshifted pixels), and excess width over QS for similar |B|. These properties are also found to increase with |B| for both CH and QS. (2/5)', 'We also find that the spectral profiles are flatter, and more skewed than a Gaussian, with a dependence on |B|. However, the ""flatness"" and ""skewness"" are consistent between CH and QS. Why are these important? (3/5)', ""This analysis tells us that the fundamental processes giving rise to spectral profiles are similar in both CH and QS. The difference seems to arise mainly due to the magnetic topology, and that's why gross chromospheric features look similar in both CH and QS. (4/5)"", 'Hope you enjoy reading this work. However, this is just a part of the work  - the remaining contains some interesting implications of these observations. A glimpse on what is yet to come may be known to those who checked out my poster at the IIA 50 conference. Stay tuned! (5/5)', '@ydnad0 @dktripathi Thank you @ydnad0 !']",https://arxiv.org/abs/2109.04287,"Coronal Holes (CHs) have subdued intensity and net blueshifts when compared to Quiet Sun (QS) at coronal temperatures. At transition region temperatures, such differences are obtained for regions with identical photospheric absolute magnetic flux density ($\vert$B$\vert$). In this work, we use spectroscopic measurements of the \car 1334~{\AA} line from Interface Region Imaging Spectrograph (IRIS), formed at chromospheric temperatures, to investigate the intensity, Doppler shift, line width, skew, and excess kurtosis variations with $\vert$B$\vert$. We find the intensity, Doppler shift, and line widths to increase with $\vert$B$\vert$ for CHs and QS. The CHs show deficit in intensity and excess total widths over QS for regions with identical $\vert$B$\vert$. For pixels with only upflows, CHs show excess upflows over QS, while for pixels with only downflows, CHs show excess downflows over QS that cease to exist at $\vert$B$\vert$ $\le$ 40. Finally, the spectral profiles are found to be more skewed and flatter than a Gaussian, with no difference between CH and QS. These results are important in understanding the heating of the atmosphere in CH and QS, including solar wind formation, and provide further constraints on the modeling of the solar atmosphere. ","Properties of the C II 1334 {\AA} line in Coronal Hole and Quiet Sun as
  observed by IRIS"
132,1436034888579289089,1202760024,Stacy McGaugh,"['New paper on the arxiv. The equivalent to the flat rotation speed of disk galaxies for dwarf spheroidals is basically twice the velocity dispersion. This averages over anisotropy, and accounts for the larger radius at which Vflat typically occurs. \n<LINK>', 'This plot show the rotation curve of the dwarf irregular WLM and the inferred equivalent in the dwarf spheroidal Leo I. The outer, quasi-flat rotation speed Vo=2sigma on average. This lets us compare rotating and pressure supported galaxies on a level playing field. https://t.co/8TYE2YAm3I', 'The factor of 2 places dSphs on the same baryonic Tully-Fisher relation as spirals. Note also that Local Group rotators fall neatly on the line calibrated by galaxies external to the Local Group. This gives a nice independent corroboration of the calibration. https://t.co/EPDwdAqboU', 'Like many other local measurements, the BTFR gives H0=75. Reconciling that with 67 isn’t a simple recalibration of the LMC distance or TRGB calibration. All of the Local Group rotators would have to shift in unison by an uncomfortable amount. \nhttps://t.co/44hfiqJM68']",https://arxiv.org/abs/2109.03251,"We explore the Baryonic Tully-Fisher Relation in the Local Group. Rotationally supported Local Group galaxies adhere precisely to the relation defined by more distant galaxies. For pressure supported dwarf galaxies, we determine the scaling factor $\beta_c$ that relates their observed velocity dispersion to the equivalent circular velocity of rotationally supported galaxies of the same mass such that $V_o = \beta_c \sigma_*$. For a typical mass-to-light ratio $\Upsilon_* = 2\;\mathrm{M}_{\odot}/\mathrm{L}_{\odot}$ in the $V$-band, we find that $\beta_c = 2$. More generally, $\log \beta_c = 0.25 \log \Upsilon_* +0.226$. This provides a common kinematic scale relating pressure and rotationally supported dwarf galaxies. ","The Baryonic Tully-Fisher Relation in the Local Group and the Equivalent
  Circular Velocity of Pressure Supported Dwarfs"
133,1436000039705403393,704533062860681216,Patrick Coles,"['🔥New paper from our summer school on quantum datasets for QML (see thread for details)\n\n<LINK>\n\nHere’s @louis_schatzki, @MvsCerezo, and me enjoying the New Mexico views while thinking about quantum datasets <LINK> <LINK>']",https://arxiv.org/abs/2109.03400,"High-quality, large-scale datasets have played a crucial role in the development and success of classical machine learning. Quantum Machine Learning (QML) is a new field that aims to use quantum computers for data analysis, with the hope of obtaining a quantum advantage of some sort. While most proposed QML architectures are benchmarked using classical datasets, there is still doubt whether QML on classical datasets will achieve such an advantage. In this work, we argue that one should instead employ quantum datasets composed of quantum states. For this purpose, we introduce the NTangled dataset composed of quantum states with different amounts and types of multipartite entanglement. We first show how a quantum neural network can be trained to generate the states in the NTangled dataset. Then, we use the NTangled dataset to benchmark QML models for supervised learning classification tasks. We also consider an alternative entanglement-based dataset, which is scalable and is composed of states prepared by quantum circuits with different depths. As a byproduct of our results, we introduce a novel method for generating multipartite entangled states, providing a use-case of quantum neural networks for quantum entanglement theory. ",Entangled Datasets for Quantum Machine Learning
134,1435983189151866880,1101770645476528128,Shashank Dholakia,"[""Check out this new way to leverage photometry to learn about planetary formation and evolution!\n\nThis is an idea I'm particularly proud of and have worked on with @rodluger and @AstroShishir for the past year, so here's a thread! \n\nPaper link: <LINK>\n\n🧵(1/N) <LINK>"", ""First some background: \n\nIn our solar system, planets all orbit in a plane that's nearly aligned with the Sun's rotation. But is this the case for planets outside the solar system?\n\nThis 💫 spin-orbit angle 💫 can tell us a lot about how planets form and develop! \n\n(2/N)"", 'So how do we measure spin-orbit angles for exoplanets?\n\nThe standard transit method, which assumes a spherical star, renders all spin-orbit angles the same in transit. Usually, we use Doppler tomography (DT), which requires time on big, ground-based telescopes.\n\n(3/N)', 'Stars that rotate rapidly are actually spheroidal 🏈, and the less dense material at their equator is cooler and darker. Hence, the symmetry in transit is broken!\n\nWe can exploit these two effects of stellar rotation to measure spin-orbit angles photometrically.\n\n(4/N) https://t.co/SzadbpuzHO', ""Using gravity darkening and oblateness to measure spin-orbit angles is not new. But it's really challenging and has only been tried on a handful of planets.\n\nCurrent methods are computationally expensive! They rely on slow 2-D integrals at every step of the light curve. \n\n(5/N)"", ""But there's a much better way! In the code package starry, gravity darkening can easily be modeled using spherical harmonics.\n\nIn this work, we extend starry to oblate stars by describing and solving integrals when a circular planet intersects an elliptical star.\n\n(6/N)"", 'Now for some real data from TESS! \n\nA new method like this needs a good test case. We chose the well-known hot Jupiter WASP-33b, which has a rapidly rotating host star. The star is also a delta Scuti variable, which required careful treatment to detrend the pulsations.\n\n(7/N)', 'The TESS light curve of WASP-33 has distinct asymmetry after the pulsation removal. We use this to place a novel constraint on the true spin-orbit of this system!\n\nWASP-33 also has measurements of projected spin-orbit, which is useful to benchmark our method against.\n\n(8/N) https://t.co/3cpq6glnfX', 'Our value for projected spin-orbit using only the TESS photometry matches those from DT measurements! We also make a direct constraint on true spin orbit angle, which is discrepant from prior dynamical constraints. \n\n(9/N) https://t.co/qy3GQK6heX', 'Given that there are likely to be ~2000 exoplanets orbiting hot stars in TESS, this method could be used to find spin-orbits on a large sample of planets in the future!\n\n(10/N)', 'The implementation in starry is also designed for posterior inference. This means that gravity-darkened and oblate transit fits can easily be combined with existing DT data. It also allows us to push to exploring aligned systems without perceptible asymmetry in transit.\n\n(11/N)', 'Lastly, the implementation in starry is easy to use, extensible, and orders of magnitude faster than 2D integration. Model light curves can be generated for arbitrary surface maps on oblate bodies, which could be used for modeling eclipsing binaries, heartbeat stars, etc.\n\n(12/N)', ""And just for fun: here's a to scale model of the WASP-33 system we analyze in the paper! Everything in the animation reflects what we know about the system, from the star's rotation rate, (subtle!) oblateness and gravity darkening to the planet's transit geometry.\n\n(13/N) https://t.co/YEBbBorliM""]",http://arxiv.org/abs/2109.03250,"We derive solutions to transit light curves of exoplanets orbiting rapidly-rotating stars. These stars exhibit significant oblateness and gravity darkening, a phenomenon where the poles of the star have a higher temperature and luminosity than the equator. Light curves for exoplanets transiting these stars can exhibit deviations from those of slowly-rotating stars, even displaying significantly asymmetric transits depending on the system's spin-orbit angle. As such, these phenomena can be used as a protractor to measure the spin-orbit alignment of the system. In this paper, we introduce a novel semi-analytic method for generating model light curves for gravity-darkened and oblate stars with transiting exoplanets. We implement the model within the code package starry and demonstrate several orders of magnitude improvement in speed and precision over existing methods. We test the model on a TESS light curve of WASP-33, whose host star displays rapid rotation ($v \sin i_* = 86.4$ km/s). We subtract the host's $\delta$-Scuti pulsations from the light curve, finding an asymmetric transit characteristic of gravity darkening. We find the projected spin orbit angle is consistent with Doppler tomography and constrain the true spin-orbit angle of the system as $\varphi=108.3^{+19.0}_{-15.4}$~$^{\circ}$. We demonstrate the method's uses in constraining spin-orbit inclinations of such systems photometrically with posterior inference. Lastly, we note the use of such a method for inferring the dynamical history of thousands of such systems discovered by TESS. ","Efficient and precise transit light curves for rapidly-rotating, oblate
  stars"
135,1435970617832984590,3079023467,Dr. Emma Beasor,"['Our new paper just accepted to ApJ! We updated the mass loss rate prescription for cool supergiants to our recently published prescription, which showed quiescent mass loss is actually quite weak…\n<LINK>', 'Two key takeaways… 1) the mass loss is so weak that RSGs cannot return to the blue of the HR diagram, and hence the single star pathway for Wolf-Rayets is effectively ruled out…', 'And 2) we find a direct correlation between H-envelope mass at core-collapse with initial mass for single RSGs, whereas MIST (and other) models predict a plateau… this could be useful!']",https://arxiv.org/abs/2109.03239,"Accurate mass-loss rates are essential for meaningful stellar evolutionary models. For massive single stars with initial masses between 8 - 30\msun the implementation of cool supergiant mass loss in stellar models strongly affects the resulting evolution, and the most commonly used prescription for these cool-star phases is that of de Jager. Recently, we published a new \mdot\ prescription calibrated to RSGs with initial masses between 10 - 25\msun, which unlike previous prescriptions does not over estimate \mdot\ for the most massive stars. Here, we carry out a comparative study to the MESA-MIST models, in which we test the effect of altering mass-loss by recomputing the evolution of stars with masses 12-27\msun\ with the new \mdot-prescription implemented. We show that while the evolutionary tracks in the HR diagram of the stars do not change appreciably, the mass of the H-rich envelope at core-collapse is drastically increased compared to models using the de Jager prescription. This increased envelope mass would have a strong impact on the Type II-P SN lightcurve, and would not allow stars under 30\msun\ to evolve back to the blue and explode as H-poor SN. We also predict that the amount of H-envelope around single stars at explosion should be correlated with initial mass, and we discuss the prospects of using this as a method of determining progenitor masses from supernova light curves. ",The impact of realistic red supergiant mass-loss on stellar evolution
136,1435934105049444352,966054811740360704,Nicola De Cao,"['Happy to announce my new #EMNLP2021 paper: Highly Parallel Autoregressive Entity Linking with Discriminative Correction\n\nSoTA performance while being &gt;70x faster than previous generative formulations! 🤯\n\n📄<LINK>\n💻<LINK> <LINK>', 'The model generates mention-entity pairs conditionally independently given the input and thus allowing high parallelism.\n\nIn addition, a Longformer is used as the encoder to handle long input documents, and a shallow LSTM is used as the decoder to make generation super fast! ⚡️ https://t.co/ZZWfAwiBlC', 'As always I want to thank my amazing collaborators @iatitov and @wilkeraziz ♥️']",https://arxiv.org/abs/2109.03792,"Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is >70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at this https URL ","Highly Parallel Autoregressive Entity Linking with Discriminative
  Correction"
137,1435891550303793152,494212643,Aayush Saxena,"[""After over a year of being in the works (counting the pandemic), I'm excited that our paper reporting the discovery of 11 new spectroscopically confirmed candidate Lyman continuum (LyC) leakers in the GOODS-S field is out now!\n\n<LINK>\n\nShort thread below (1/5)"", 'We use ground based LyC imaging and compile spectra from publicly available surveys, mainly from VANDELS (LBGs) and MUSE (blind) to measure the LyC escape fractions. only 6% of galaxies have any LyC leakage, with the majority of the sample giving an upper limit of fesc&lt;7% (2/5)', 'Interestingly, we do not find any strong dependence of the measured LyC escape fraction for 11 new candidate leakers on their stellar masses or specific star-formation rates, as can be seen in the plots below: (3/5) https://t.co/a5e89E4pzS', 'At these redshifts, the [OIII]+Hb lines fall in the observed K band, and we measure these line strengths by including nebular emission in SED fitting. Once again, no strong dependence of fesc is found on the [OIII]+Hb line strengths: (4/5) https://t.co/srfjTSN0nO', 'It remains observationally unclear which physical property regulates high LyC leakage. We argue that orientation and timescales may play a role in actually detecting LyC leakage, and the presence of young clusters within galaxies could be important. (5/5)', ""@jorryt_m Cheers! Typical MUV is ~ -21, so comparable to Steidel+2018 I'd say and its encouraging to get similar results.\nIndeed IGM stochasticity may play a role in masking correlations, which we touch upon in the paper too!"", ""@astrobellatrix Ooh so sorry about that -including MUSE redshifts thanks to your and your team's amazing work was just too tempting ;) I hope we'll arrive at similar conclusions and will keep an eye out on your results too! Thanks :)"", '@astrobellatrix Yes that was a really cool paper. MUSE results are generally absolutely fantastic I must say!', ""@maximetrebitsch That is an excellent point actually - we haven't explored the dust content angle here, but it is indeed something that could definitely play a role in an orientation-based scenario... Kind of similar to IGM stochasticity in effect(?) More zoom-in simulations please!!""]",https://arxiv.org/abs/2109.03662,"We present Lyman continuum (LyC) radiation escape fraction $f_{\rm{esc}}$ measurements for 183 spectroscopically confirmed star-forming galaxies in the redshift range $3.11 < z < 3.53$ in the \textit{Chandra} Deep Field South. We use ground-based imaging to measure $f_{\rm{esc}}$, and use ground- and space-based photometry to derive galaxy physical properties using spectral energy distribution (SED) fitting. We additionally derive [O III]+H$\beta$ equivalent widths (that fall in the observed K band) by including nebular emission in the SED fitting. After removing foreground contaminants, we report the discovery of 11 new candidate LyC leakers, with absolute LyC escape fractions, $f_{\rm{esc}}$ in the range $0.14-0.85$. From non-detections, we place $1\sigma$ upper limits of $f_{\rm{esc}}<0.12$, where the Lyman-break selected galaxies have $f_{\rm{esc}} < 0.11$ and `blindly' discovered galaxies with no prior photometric selection have $f_{\rm{esc}}<0.13$. We find a slightly higher $1\sigma$ limit of $f_{\rm{esc}}<0.20$ for extreme emission line galaxies with rest-frame [O III]+H$\beta$ equivalent widths $>300$A. For candidate LyC leakers, we find a weak negative correlation between $f_{\rm{esc}}$ and galaxy stellar masses, no correlation between $f_{\rm{esc}}$ specific star-formation rates (sSFRs) and a positive correlation between $f_{\rm{esc}}$ and EW$_0$([O III]+H$\beta$). The weak/no correlations between stellar mass and sSFRs may be explained by misaligned viewing angles and/or non-coincident timescales of starburst activity and periods of high $f_{\rm{esc}}$. Alternatively, escaping radiation may predominantly occur in highly localised star-forming regions, or $f_{\rm{esc}}$ measurements may be impacted by stochasticity of the intervening neutral medium, obscuring any global trends with galaxy properties. These hypotheses have important consequences for models of reionisation. ","No strong dependence of Lyman continuum leakage on physical properties
  of star-forming galaxies at $\mathbf{3.1 \lesssim z \lesssim 3.5}$"
138,1435874838774788101,702876848657661952,Oscar Sainz,"[""I'm glad to share a new paper about zero-shot RE that has been accepted at #EMNLP2021 .\n\nLabel Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction.\n\nCode: <LINK> \nPaper: <LINK>\n\nw/ @oierldl @glabaka @4nderB @eagirre"", 'Recent methods on prompt-based learning has shown to be very effective at few-shot. However, using other supervised pivot tasks such as NLI, QA, ... instead of (masked) language modeling increases the zero-shot performance. Thus, we reformulate the RE task as NLI. https://t.co/Ee0bYD8MA9', 'To do it, we simply verbalize the relations by defining some simple templates. Also, we add some entity type constraints to the relations.  The inference is done by selecting the most probable (to be entailed) hypothesis among all valid verbalizations. https://t.co/X6NoumTHfg', 'This approach can work out of the box, but, what if you have some data to train it? Easy! You can map the training examples to NLI format by simple heuristics! https://t.co/fyfP8WURCL', 'We evaluated this simple yet effective method on TACRED dataset by defining several scenarios: zero-shot, few-shot, full training and data-aug. The proposed method outperforms the state-of-the-art by a large margin in zero- and few-shot, and scales up to large training as well. https://t.co/FG65gt1Dhe']",https://arxiv.org/abs/2109.03659,"Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are specially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases. ","Label Verbalization and Entailment for Effective Zero- and Few-Shot
  Relation Extraction"
139,1435644742616813569,1174529814264332289,Leo Gao,"['New paper: <LINK>\n\ntl;dr: If your filtering objective is some kind of quality proxy, then filtering too aggressively goodharts your proxy and actually hurts downstream performance. <LINK>']",https://arxiv.org/abs/2109.00698,"While conventional wisdom suggests that more aggressively filtering data from low-quality sources like Common Crawl always monotonically improves the quality of training data, we find that aggressive filtering can in fact lead to a decrease in model quality on a wide array of downstream tasks for a GPT-like language model. We speculate that this is because optimizing sufficiently strongly for a proxy metric harms performance on the true objective, suggesting a need for more robust filtering objectives when attempting to filter more aggressively. We hope this work leads to detailed analysis of the effects of dataset filtering design choices on downstream model performance in future work. ",An Empirical Exploration in Quality Filtering of Text Data
140,1435567098739269635,322636963,Jonathan Berant,"['Another piece of evidence in the quest for compositional generalization: \nNew paper by Inbar Oren and w/ @jonherzig <LINK> (to appear in #emnlp2021): \nsampling examples with high structural diversity across examples from a SCFG dramatically improves comp. gen 1/2 <LINK>', 'We double accuracy on a compositional split of Schema2QA by sampling only 5K synthetic examples from the synchronous grammar, check it out!\nOne more #emnlp2021 paper on compositional generalization in a visually grounded setup coming up next week, stay tuned.', 'Oh and Shana Tova!']",https://arxiv.org/abs/2109.02575,"Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing. Given a small training set of annotated examples and an ""infinite"" pool of synthetic examples, we select a subset of synthetic examples that are structurally-diverse and use them to improve compositional generalization. We evaluate our approach on a new split of the schema2QA dataset, and show that it leads to dramatic improvements in compositional generalization as well as moderate improvements in the traditional i.i.d setup. Moreover, structurally-diverse sampling achieves these improvements with as few as 5K examples, compared to 1M examples when sampling uniformly at random -- a 200x improvement in data efficiency. ","Finding needles in a haystack: Sampling Structurally-diverse Training
  Sets from Synthetic Data for Compositional Generalization"
141,1435521859227901953,2434383363,Phoebe Pearce,"['New on arXiv, paper on my PhD work very long in the making! <LINK>']",https://arxiv.org/abs/2109.02782,"We present a combined experimental and theoretical analysis of the evolution of the near-band gap electronic and optical properties of Si$_{x}$Ge$_{1-x-y}$Sn$_{y}$ alloys lattice-matched to Ge and GaAs substrates. We perform photoreflectance (PR) and photoluminescence (PL) measurements on Si$_{x}$Ge$_{1-x-y}$Sn$_{y}$ epitaxial layers grown via chemical vapour deposition, for Si (Sn) compositions up to $x =$ 9.6% ($y =$ 2.5%). Our measurements indicate the presence of an indirect fundamental band gap, with PL observed $\approx$ 200-250 meV lower in energy than the direct $E_0$ transition identified by PR measurements. The measured PL is Ge-like, suggesting that the alloy conduction band (CB) edge is primarily derived from the Ge L-point CB minimum. Interpretation of the PR and PL measurements is supported by atomistic electronic structure calculations. Effective alloy band structures calculated via density functional theory confirm the presence of an indirect fundamental band gap, and reveal the origin of the observed inhomogeneous broadening of the measured optical spectra as being alloy-induced band hybridisation occurring close in energy to the CB edge. To analyze the evolution of the band gap, semi-empirical tight-binding (TB) calculations are employed to enable calculations for large supercell sizes. TB calculations reveal that the alloy CB edge is hybridized in nature, consisting at low Si and Sn compositions of an admixture of Ge L-, $\Gamma$- and X-point CB edge states, and confirm that the alloy CB edge retains primarily Ge L-point CB edge character. Our experimental measurements and theoretical calculations confirm a direct transition energy close to 1 eV in magnitude for Si and Sn compositions $x =$ 6.8 - 9.6% and $y =$ 1.6 - 2.2%. ","Electronic and optical properties of Si$_{x}$Ge$_{1-x-y}$Sn$_{y}$ alloys
  lattice-matched to Ge"
142,1435467635966230533,10666172,Sabine Hossenfelder,['We have written a new paper <LINK>'],https://arxiv.org/abs/2109.02676,"In quantum mechanics, the wave-function only predicts probabilities of measurement outcomes, not individual outcomes. This suggests that it describes an ensemble of states with different values of a hidden variable. Here, we analyse this idea with reference to currently known theorems and experiments. We argue that the $\psi$-ontic/epistemic distinction fails to properly identify ensemble interpretations and propose a more useful definition. We then show that all $\psi$-ensemble interpretations which reproduce quantum mechanics violate Statistical Independence. Finally, we explain how this interpretation helps make sense of some otherwise puzzling phenomena in quantum mechanics, such as the delayed choice experiment, the Elitzur-Vaidman bomb detector, and the Extended Wigner's Friends Scenario. ",The wave-function as a true ensemble
143,1435430187378819074,1095182290689318912,Jon Zink,"['This NEW PAPER was a labor of love! We discovered 366 additional K2 planets through an automated search of the transit campaigns, enabling completeness assessment of the 747 candidate sample. Robust K2 exoplanet demographic studies can now be achieved.\n<LINK> <LINK>']",https://arxiv.org/abs/2109.02675,"We provide the first full K2 transiting exoplanet sample, using photometry from Campaigns 1-8 and 10-18, derived through an entirely automated procedure. This homogeneous planet candidate catalog is a crucial to perform a robust demographic analysis of transiting exoplanets with K2. We identify 747 unique planet candidates and 57 multi-planet systems. Of these candidates, 366 have not been previously identified, including one resonant multi-planet system and one system with two short-period gas giants. By automating the construction of this list, measurements of sample biases (completeness and reliability) can be quantified. We carried out a light curve-level injection/recovery test of artificial transit signals and found a maximum completeness of 61%, a consequence of the significant detrending required for K2 data analysis. Through this operation we attained measurements of the detection efficiency as a function of signal strength, enabling future population analysis using this sample. We assessed the reliability of our planet sample by testing our vetting software EDI-Vetter against inverted transit-free light curves. We estimate 91% of our planet candidates are real astrophysical signals, increasing up to 94% when limited to the FGKM dwarf stellar population. We also constrain the contamination rate from background eclipsing binaries to less than 5%. The presented catalog, along with the completeness and reliability measurements, enable robust exoplanet demographic studies to be carried out across the fields observed by the K2 mission for the first time. ",Scaling K2. IV. A Uniform Planet Sample for Campaigns 1-8 and 10-18
144,1435424928753278978,1115880604560691200,NII Yamagishi Lab,"['Our new paper, ""Benchmarking and challenges in security and privacy for voice biometrics,"" is online:  <LINK>', 'This paper provides a high-level overview of benchmark-based evaluation, the task of Automatic Speaker Verification and its vulnerabilities, and the task of Speaker Anonymization. https://t.co/MrU33UnY6D', 'Discussion of the ASVspoof and VoicePrivacy challenges notes the progress that has been made, but there is still room for improvement and many unanswered research questions in these relatively new areas.']",https://arxiv.org/abs/2109.00281,"For many decades, research in speech technologies has focused upon improving reliability. With this now meeting user expectations for a range of diverse applications, speech technology is today omni-present. As result, a focus on security and privacy has now come to the fore. Here, the research effort is in its relative infancy and progress calls for greater, multidisciplinary collaboration with security, privacy, legal and ethical experts among others. Such collaboration is now underway. To help catalyse the efforts, this paper provides a high-level overview of some related research. It targets the non-speech audience and describes the benchmarking methodology that has spearheaded progress in traditional research and which now drives recent security and privacy initiatives related to voice biometrics. We describe: the ASVspoof challenge relating to the development of spoofing countermeasures; the VoicePrivacy initiative which promotes research in anonymisation for privacy preservation. ",Benchmarking and challenges in security and privacy for voice biometrics
145,1435338464711614465,1319101874532978690,jason wei,"['New paper: <LINK>\n\nWe teach a 137B parameter language model to follow instructions using “instruction tuning”, which finetunes the model on a collection of tasks described via instructions. \n\nThis improves the zero-shot abilities of the model on unseen tasks. <LINK>', 'We call the 137B instruction-tuned language model FLAN, for Finetuned Language Net.', 'FLAN substantially improves the zero-shot performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 held-out tasks that we evaluate.\n\nFLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. https://t.co/q7FR09Fz9y', 'In ablation studies, we find that adding additional task clusters to instruction tuning improves zero-shot performance on held-out task types. https://t.co/svfl6WNLQr', 'Interestingly, the benefits of instruction tuning emerge only with sufficient model scale. https://t.co/QCBtZrbPTe', 'FLAN also responds better to prompt tuning than the unmodified base model. https://t.co/QHgoivFxFO', 'Thanks to my co-authors: @MaartenBosma, Vincent Zhao, @kelvin_guu, @AdamsYu, @blester125, Nan Du, @iamandrewdai, @quocleix', 'We hope our work motivates future research on instructions-based NLP. \n\nAnd I’d be remiss to not mention other work in this direction. \n\nMy favorites are CrossFit: https://t.co/ej8kimENvl\n\nAs well as this paper: https://t.co/jf2HbSqTn6']",https://arxiv.org/abs/2109.01652,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. ",Finetuned Language Models Are Zero-Shot Learners
146,1435289450544869377,1060906446001442818,Athanasios Vlontzos,"[""🚨 New Paper !\nIn our most recent work with @BernhardKainz1 and @quantumciaran we evaluate probabilities of causation and perform counterfactual-level causal inf by combining @yudapearl's Twin Networks and Deep NNs <LINK> #CausalML\n #MachineLearning"", 'Twin Nets present an alt to the usual Abduction-Action-Prediction framework and are more computationally efficient. By building a neural network that abides by their rules and pairing it with identifiability constraints, we are able to estimate accurate&amp;efficient counterfactuals!', 'A big thanks to my co-authors, code will be available soon and stay tuned for more exciting causality-based research coming up.']",http://arxiv.org/abs/2109.01904,"There has been much recent work using machine learning to answer causal queries. Most focus on interventional queries, such as the conditional average treatment effect. However, as noted by Pearl, interventional queries only form part of a larger hierarchy of causal queries, with counterfactuals sitting at the top. Despite this, our community has not fully succeeded in adapting machine learning tools to answer counterfactual queries. This work addresses this challenge by showing how to implement twin network counterfactual inference -- an alternative to abduction, action, & prediction counterfactual inference -- with deep learning to estimate counterfactual queries. We show how the graphical nature of twin networks makes them particularly amenable to deep learning, yielding simple neural network architectures that, when trained, are capable of counterfactual inference. Importantly, we show how to enforce known identifiability constraints during training, ensuring the answer to each counterfactual query is uniquely determined. We demonstrate our approach by using it to accurately estimate the probabilities of causation -- important counterfactual queries that quantify the degree to which one event was a necessary or sufficient cause of another -- on both synthetic and real data. ","Estimating the probabilities of causation via deep monotonic twin
  networks"
147,1435247754091974665,270754217,Isobel Kolbe,"['Mawande and I have a new paper on the arXiv today! We came up with a rather nifty (and actually quite universal) way of adjusting classical calculations of bremsstrahlung to account for the recoil of the target, which is a quantum effect.\n<LINK>\nNow back to JEWEL', ""Also - for a person who has done almost nothing BUT bremsstrahlung since my MSc, I can't believe I misspelt it in the caption of Fig.2. But if I'm honest, I'm so proud of Fig. 3 that  I'm kind of OK with it - for now...""]",https://arxiv.org/abs/2109.01736,"We propose a formula for gluon radiation in the fragmentation region. By comparing tree-level bremsstrahlung of a spin-less quark to the known result for gluon radiation from a classical particle struck by a sheet of colored glass arXiv:1903.01381, arXiv:1911.12738 we modify the classical formula to take into account the recoil of the struck particle. The new formula produces the correct perturbative behaviour at high momentum of the radiated gluon. ",Gluon radiation from a classical point particle: Recoil effects
148,1435189824424710145,2370131880,Andy Keller,"['Together with @wellingmax, we think deep learning needs more organization and structure... topographic organization and equivariant structure 😁\n\nIntroducing our new paper:\nTopographic VAEs learn Equivariant Capsules\n📃<LINK>\n🧬<LINK>\n\n1/6 <LINK>', 'Topographic organization in the brain describes the observation that nearby neurons on the cortical surface tend to have more strongly correlated activations than spatially distant neurons. What is the advantage of such organization and what is the relation to equivariance?\n\n2/6', 'To answer these questions we introduce the Topographic VAE, a deep generative model with topographically organized latent variables, and show that it indeed learns to organize its activations according to salient characteristics such as class, width, and style on MNIST.\n\n3/6 https://t.co/7XRFazsSb8', ""We then show, by extension of such organization over time, it is possible learn sets of approximately equivariant features (aka 'capsules') directly from sequences -- where observed transformations become encoded as cyclic permutations within the capsule dimension. (Fig. 1)\n\n4/6"", ""Experimentally we observe that our model yields higher likelihoods than non-topographic baselines on transforming test sequences while additionally being able to decode unseen future sequence elements through a 'capsule roll' in latent space.\n\n5/6 https://t.co/LqaAA2XVIH"", ""We hope this work enables further exploration of the computational benefits of topographic organization in deep neural networks, and further advances in the domain learned approximate equivariance. Please see our paper for more experiments, and don't hesitate to reach out!\n\n6/6"", 'This work could not have been done without the prior works of Aapo Hyvarinen, @pohoyer, Jarmo Hurri, Mika Inki, Jaakko Väyrynen, @wellingmax, @sindero, @geoffreyhinton, Laurenz Wiskott, @sejnowski, @TacoCohen, @dpkingma, @DaniloJRezende, @shakir_za and so many more. Thank you!']",https://arxiv.org/abs/2109.01394,"In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. ""capsules"") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks. ",Topographic VAEs learn Equivariant Capsules
149,1435062722912133122,1055973579467059200,Rajsekhar Mohapatra,['We have a new paper out on arxiv today! We have studied velocity structure functions in high-res simulations of the multiphase intracluster medium in galaxy halos. \nCheck it out at:  <LINK>\nSimulation movies at: <LINK>.'],https://arxiv.org/abs/2109.01771,"The central regions of cool-core galaxy clusters harbour multiphase gas, with gas temperatures ranging from $10$ $\mathrm{K}$--$10^7$$\mathrm{K}$. Feedback from active galactic nuclei (AGNs) jets prevents the gas from undergoing a catastrophic cooling flow. However, the exact mechanism of this feedback energy input is unknown, mainly due to the lack of velocity measurements of the hot phase gas. However, recent observations have measured the velocity structure functions ($\mathrm{VSF}$s) of the cooler molecular ($\sim10$$\mathrm{K}$) and H$\alpha$ filaments ($\sim10^4$$\mathrm{K}$) and used them to indirectly estimate the motions of the hot phase. In the first part of this study, we conduct high-resolution ($384^3$--$1536^3$ resolution elements) simulations of homogeneous isotropic subsonic turbulence, without radiative cooling. We analyse the second-order velocity structure functions ($\mathrm{VSF}_2$) in these simulations and study the effects of varying spatial resolution, the introduction of magnetic fields, and the effect of projection along the line of sight (LOS) on it. In the second part of the study, we analyse high-resolution ($768^3$ resolution elements) idealised simulations of multiphase turbulence in the intracluster medium (ICM) from Mohapatra et al 2021. We compare the $\mathrm{VSF}_2$ for both the hot ($T\sim10^7$$\mathrm{K}$) and cold ($T\sim10^4$$\mathrm{K}$) phases and find that their amplitude depends on the density contrast between the phases. They have similar scaling with separation, but introducing magnetic fields steepens the $\mathrm{VSF}_2$ of only the cold phase. We also find that projection along the LOS steepens the $\mathrm{VSF}_2$ for the hot phase and mostly flattens it for the cold phase. ","Velocity structure functions in multiphase turbulence: interpreting
  kinematics of H$\alpha$ filaments in cool core clusters"
150,1435058394256355328,20703003,Peter B Denton,"['New paper out tonight with Hooman Davoudiasl and @j_gehrlein called: ""Connecting the Extremes: A Story of Supermassive Black Holes and Ultralight Dark Matter"" <LINK> 1/8 <LINK>', 'In it we connect the largest objects (super massive black holes SMBHs) and the lightest dark matter candidates via a model with a first order phase transition 2/8', 'The existence of SMBHs in the early (z=5-7) universe is a puzzle that has received a fair bit of attention in the last few years including in popular press. Our model proposes a solution to this 3/8 https://t.co/4UbqV08MWi', 'At the same time there have been some hints in ultra faint dwarf galaxies suggesting that DM is ultralight: m~1e-20 eV. While not yet confirmed, we took it as a hint worth paying attention to 4/8', 'Basically, with an SU(3) dark sector and an axion coupling near the Planck scale (as suggested should exist by string theory), you get primordial BHs with the right mass and density AND a dark matter candidate compatible with small scale structure hints 5/8\n\n(also dark glueballs)', 'How to confirm this? Obviously the small scale structure hints would have to become fully realized. We also predict a slight deviation in N_eff that could be measured in coming years. Then again, so do lots of models... 6/8', 'We also predict a gravitational wave signature from the first order phase transition that @SKAO could measure, subject to theory uncertainties (the model is essentially constrained from the SMBH masses, the fuzzy DM mass, and the axion mass near the Planck scale) 7/8 https://t.co/WAjBRiyLaE', 'Thanks to my collaborators on this fun project!\n\n(Due to the title I couldn\'t get Long Island legend Billy Joel\'s ""I Go To Extremes"" out of my head while working on this project https://t.co/m6JFJbZTN4) 8/8']",https://arxiv.org/abs/2109.01678,"The formation of ultra rare supermassive black holes (SMBHs), with masses of $\mathcal O(10^9\,M_\odot)$, in the first billion years of the Universe remains an open question in astrophysics. At the same time, ultralight dark matter (DM) with mass in the vicinity of $\mathcal O(10^{-20}~\text{eV})$ has been motivated by small scale DM distributions. Though this type of DM is constrained by various astrophysical considerations, certain observations could be pointing to modest evidence for it. We present a model with a confining first order phase transition at $\sim 10$ keV temperatures, facilitating production of $\mathcal O(10^9\,M_\odot)$ primordial SMBHs. Such a phase transition can also naturally lead to the implied mass for a motivated ultralight axion DM candidate, suggesting that SMBHs and ultralight DM may be two sides of the same cosmic coin. We consider constraints and avenues to discovery from superradiance and a modification to $N_{\rm eff}$. On general grounds, we also expect primordial gravitational waves -- from the assumed first order phase transition -- characterized by frequencies of $\mathcal O(10^{-12}-10^{-9}~\text{Hz})$. This frequency regime is largely uncharted, but could be accessible to pulsar timing arrays if the primordial gravitational waves are at the higher end of this frequency range, as could be the case in our assumed confining phase transition. ","Connecting the Extremes: A Story of Supermassive Black Holes and
  Ultralight Dark Matter"
151,1435021241891672064,1278771726877437955,Luc Combi,"['new paper today on the arxiv from our team (@fedelopezar1 , @ManuelaCampane1 , @therealscn + ): \n\n<LINK> \nwe simulate mini-disk accretion around a binary black hole system approaching merger; turns out these disks are pretty different from single BH disks!  (1/2) <LINK>', 'we investigate how these disks vary in time and how they are affected when the BHs have spin. we also show how black holes can power a jet in this system. we will use these models to make concrete predictions of spectra/lightcurves in our next paper (by @egutierrezposse) (2/2)']",https://arxiv.org/abs/2109.01307,"We perform a full 3D general relativistic magnetohydrodynamical (GRMHD) simulation of an equal-mass, spinning, binary black hole approaching merger, surrounded by a circumbinary disk and with mini-disks around each black hole. For this purpose, we evolve the ideal GRMHD equations on top of an approximated spacetime for the binary that is valid in every position of space, including the black hole horizons, during the inspiral regime. We use relaxed initial data for the circumbinary disk from a previous long-term simulation, where the accretion is dominated by an $m=1$ overdensity called the lump. We compare our new spinning simulation with a previous non-spinning run, studying how spin influences the mini-disk properties. We analyze the accretion from the inner edge of the lump to the black hole, focusing on the angular momentum budget of the fluid around the mini-disks. We find that mini-disks in the spinning case have more mass over a cycle than the non-spinning case. However, in both cases, we find most of the mass received by the black holes is delivered by the direct plunging of material from the lump. We also analyze the morphology and variability of the electromagnetic fluxes and we find they share the same periodicities of the accretion rate. In the spinning case, we find that the outflows are $8$ times stronger than the non-spinning case. Our results will be useful to understand and produce realistic synthetic light curves and spectra, which can be used in future observations. ","Mini-disk accretion onto spinning black hole binaries:
  quasi-periodicities and outflows"
152,1434891467600941056,1141772501472727040,Lena Voita,"['[1/5] New #EMNLP2021 paper (and a blog post!) with @iatitov and @RicoSennrich:\n\nLanguage Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT\n\nPaper: <LINK>\nBlog: <LINK> #NLProc <LINK>', '@iatitov @RicoSennrich [2/5] In SMT, model competences are modelled with distinct models (e.g., target-side LM, lexical translation, alignments). In NMT, the whole translation task is modelled with a single neural network. \n\nHow and when does NMT get to learn all the competences?\n\n#EMNLP2021 #NLProc https://t.co/BY4JzURznm', '[3/5] Target-side LM -&gt; lexical translation -&gt; reordeings:\n\nNMT first learns target LM, then lexical translation and comes close to word-by-word translation, and finally learns more complicated reorderings.\n\nExample: how translations change at the last stage\n#EMNLP2021 #NLProc https://t.co/bpQxlnHxP2', '[4/5] Our analysis helps in practice!\n\nFor example, it can improve a vanilla non-autoregressive model by guiding teacher model selection. Instead of a fully converged autoregressive teacher, we take an earlier checkpoint and get more monotonic targets.\n\n#EMNLP2021 #NLProc https://t.co/VTg97gxvSz', '[5/5] #ACL2021 vs #EMNLP2021: results agree!\n\nFinally, the stages agree well with our ACL2021 paper analyzing source and target contributions. For that paper, see the earlier blog post:\nhttps://t.co/uRCpSufIST\n\n#NLProc https://t.co/oYv01UNQae', '@josephimperial_ Thank you😊']",https://arxiv.org/abs/2109.01396,"Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection. ","Language Modeling, Lexical Translation, Reordering: The Training Process
  of NMT through the Lens of Classical SMT"
153,1434875424123801600,1339508444764786694,Gregor Kasieczka,"['New Paper ""Classifying Anomalies THrough Outer Density Estimation (CATHODE)"" <LINK> with @BPNachman @tobi_quatsch and others 1/3', 'We improve sideband-based, unsupervised, anomaly detection by combining density estimation (i.e. learning a normalising flow in a sideband and transporting it into a signal region) with a classifier that distinguishes this estimate from the actual data in the signal region 2/3 https://t.co/EPvoMHd90A', 'This works pretty well compared to other unsupervised anomaly detectors  and comes even close to the performance of a supervised tagger in some cases. 3/3 https://t.co/ecEL40wRI0']",https://arxiv.org/abs/2109.00546,"We propose a new model-agnostic search strategy for physics beyond the standard model (BSM) at the LHC, based on a novel application of neural density estimation to anomaly detection. Our approach, which we call Classifying Anomalies THrough Outer Density Estimation (CATHODE), assumes the BSM signal is localized in a signal region (defined e.g. using invariant mass). By training a conditional density estimator on a collection of additional features outside the signal region, interpolating it into the signal region, and sampling from it, we produce a collection of events that follow the background model. We can then train a classifier to distinguish the data from the events sampled from the background model, thereby approaching the optimal anomaly detector. Using the LHC Olympics R&D dataset, we demonstrate that CATHODE nearly saturates the best possible performance, and significantly outperforms other approaches that aim to enhance the bump hunt (CWoLa Hunting and ANODE). Finally, we demonstrate that CATHODE is very robust against correlations between the features and maintains nearly-optimal performance even in this more challenging setting. ",Classifying Anomalies THrough Outer Density Estimation (CATHODE)
154,1434790083593396225,1134375290581524480,Kai Schmitz,"['New paper on the arXiv <LINK>, together with three new collaborators from Lausanne/Kyiv! We study gauge-field production during inflation, incl. nonlinear Schwinger pair production and backreaction effects, which enables us to compute EM power spectra in k space. <LINK>']",https://arxiv.org/abs/2109.01651,"We study the explosive production of gauge fields during axion inflation in a novel gradient expansion formalism that describes the time evolution of a set of bilinear electromagnetic functions in position space. Based on this formalism, we are able to simultaneously account for two important effects that have thus far been mostly treated in isolation: (i) the backreaction of the produced gauge fields on the evolution of the inflaton field and (ii) the Schwinger pair production of charged particles in the strong gauge-field background. This allows us to show that the suppression of the gauge-field production due to the Schwinger effect can prevent the backreaction in scenarios in which it would otherwise be relevant. Moreover, we point out that the induced current, $\boldsymbol{J} = \sigma \boldsymbol{E}$, also dampens the Bunch-Davies vacuum fluctuations deep inside the Hubble horizon. We describe this suppression by a new parameter $\Delta$ that is related to the time integral over the conductivity $\sigma$ which hence renders the description of the entire system inherently nonlocal in time. Finally, we demonstrate how our formalism can be used to construct highly accurate solutions for the mode functions of the gauge field in Fourier space, which serves as a starting point for a wealth of further phenomenological applications, including the phenomenology of primordial perturbations and baryogenesis. ","Gauge-field production during axion inflation in the gradient expansion
  formalism"
155,1433867010711031808,1172321171670360064,Suraj Nair,"['Thrilled to share our new work on learning language-conditioned visuomotor skills on real robots! \nPaper: <LINK>\nIn collaboration with @_eric_mitchell_, Kevin Chen, @brian_ichter, @silviocinguetta, @chelseabfinn \n👇1/9 <LINK>', 'Easily specifying tasks and acquiring rewards for learning can be challenging on real robots. Motivated by the flexibility and ease of natural language, we aim to learn language-conditioned rewards and skills on robots. 2/9', 'To scalably learn language-conditioned behavior on robots, we leverage pre-existing (potentially highly sub-optimal) robot datasets, such as autonomous exploration data, or replay buffers of trained RL agents. We then crowdsource natural language descriptions of each episode. 3/9 https://t.co/SUKgqKXZrk', 'Since the actions in the data may not be good enough to imitate, we propose to learn a language-conditioned reward function in the form of a classifier which predicts if transitioning between images completes an instruction. 4/9 https://t.co/IWogi5ZXAz', 'We can then combine the learned reward with a task-agnostic visual dynamics model to perform model predictive control to complete language specified tasks. 5/9 https://t.co/XkioNiREg9', 'We find that this approach is able to learn from data collected by even a random policy, which may often not complete any task, outperforming both language-conditioned imitation techniques and goal image based task specification. 6/9 https://t.co/m2aEbTXVfW', 'We additionally observe that by virtue of using pre-trained language models, the method is able to generalize to unseen rephrasings of tasks, including natural language, while only being trained on procedurally generated language. 7/9 https://t.co/muAJav46ts', 'Lastly, we deploy our method on a real robot, with an offline dataset taken directly from the replay buffer of a different project and crowdsourced annotations. We find that the agent is able to complete a set of 5 language-specified visuomotor skills on the real robot. 8/9 https://t.co/vMHYSs4WzM', 'For more details, please see the\nWebsite: https://t.co/FBs4SHXu0S\nPaper: https://t.co/xZWNJqWfIr\nCode/Data: https://t.co/KC0CuZg1s5\n9/9']",https://arxiv.org/abs/2109.01115,"We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot's observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. To scalably learn this grounding we propose to leverage offline robot datasets (including highly sub-optimal, autonomously collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%, and is able to perform visuomotor tasks from natural language, such as ""open the right drawer"" and ""move the stapler"", on a Franka Emika Panda robot. ","Learning Language-Conditioned Robot Behavior from Offline Data and
  Crowd-Sourced Annotation"
156,1433866566056087559,1210312444221935616,Cyrus Rashtchian,"['Total variation distance is important in theory+apps but hard to analyze\n\nNew paper: tight TV bounds for mixtures of 2 Gaussians with shared covariance. Joint w/ Sami Davies, Arya Mazumdar (@MountainOfMoon), Soumyabrata Pal (@soumyabrata_pal)\n<LINK>\n\n[1/9] 🧵👇', '[2/9] First, some background. What is a mixture? You combine two distributions by adding half the mass of one to half the mass of the other. Here’s a nice picture by @AngusTurner9 for a mixture of 2 Gaussians, which is what we study https://t.co/qWD8I3jx8C', '[3/9] Our work is inspired by a paper of Devroye, @abbas_mehrabian, Reddad that studies TV distance between single Gaussians (not mixtures)\nhttps://t.co/fpZ49C3qwS\n\nSurprisingly, in 2018 they’re first to get tight TV bounds as functions of means/covariances in high dimensions!!', '[4/9] This prior paper has tons of applications (check Google scholar, etc) and opened the door for many new results. \n\nSo we thought, can we do the same thing for mixtures? https://t.co/eIGZOZ0WdM', '[5/9] Turns out, this is tricky in general, and a well known open question. But after a lot of hard work, we found a way to analyze mixtures of two components assuming the components have shared (co)variance, which is an important case. https://t.co/UZFv2FQlcl', '[6/9] The coolest part of our paper is that we use complex analysis! \n\nWe start by writing the TV distance in terms of the characteristic function, then we bring out fun tools like sin/cos inequalities and Taylor series https://t.co/X0Ubsadbuj', '[7/9]  The complex analysis is pretty elementary, but it gets the job done, and maybe it also sheds some new light on how to analyze the TV distance. A big part of it is a case analysis based on the layout of the means https://t.co/DsGZWVtQff', '[8/9] We start with this approach for 1D, then we get the tight higher dimensional lower bound by projecting to 1D in the right way and invoking the 1D result. Again, not as easy as it sounds, but it works!', '[9/9] Please check out our paper and let us know if you have any questions or references. It’s super hard to find all the relevant papers in this very popular area, so send us anything we missed!', '@proneat @MountainOfMoon @soumyabrata_pal Good question! Prior work settles sample complexity of learning/testing Gaussians (or we can recover some results using our bounds + standard tools). \n\nOur new TV bounds may be useful for other settings (eg private or distributed) plus TV dist is just a fundamental quantity =)']",http://arxiv.org/abs/2109.01064,"Mixtures of high dimensional Gaussian distributions have been studied extensively in statistics and learning theory. While the total variation distance appears naturally in the sample complexity of distribution learning, it is analytically difficult to obtain tight lower bounds for mixtures. Exploiting a connection between total variation distance and the characteristic function of the mixture, we provide fairly tight functional approximations. This enables us to derive new lower bounds on the total variation distance between pairs of two-component Gaussian mixtures that have a shared covariance matrix. ","Lower Bounds on the Total Variation Distance Between Mixtures of Two
  Gaussians"
157,1433824253686788099,789125136481910784,Raphael Shirley,['The Galaxy Evolution Probe is an exciting new proposal for a space telescope observing in the mid to far infrared. The paper by @jasonglenn999 and others has just been published on the arXiv  <LINK>'],https://arxiv.org/abs/2109.00614,"The Galaxy Evolution Probe (GEP) is a concept for a mid- and far-infrared space observatory to measure key properties of large samples of galaxies with large and unbiased surveys. GEP will attempt to achieve zodiacal light and Galactic dust emission photon background-limited observations by utilizing a 6 Kelvin, 2.0 meter primary mirror and sensitive arrays of kinetic inductance detectors. It will have two instrument modules: a 10 - 400 micron hyperspectral imager with spectral resolution R = 8 (GEP-I) and a 24 - 193 micron, R = 200 grating spectrometer (GEP-S). GEP-I surveys will identify star-forming galaxies via their thermal dust emission and simultaneously measure redshifts using polycyclic aromatic hydrocarbon emission lines. Galaxy luminosities derived from star formation and nuclear supermassive black hole accretion will be measured for each source, enabling the cosmic star formation history to be measured to much greater precision than previously possible. Using optically thin far-infrared fine-structure lines, surveys with GEP-S will measure the growth of metallicity in the hearts of galaxies over cosmic time and extraplanar gas will be mapped in spiral galaxies in the local universe to investigate feedback processes. The science case and mission architecture designed to meet the science requirements are described, and the kinetic inductance detector and readout electronics state of the art and needed developments are described. This paper supersedes the GEP concept study report cited in it by providing new content, including: a summary of recent mid-infrared KID development, a discussion of microlens array fabrication for mid-infrared KIDs, and additional context for galaxy surveys. The reader interested in more technical details may want to consult the concept study report. ",The Galaxy Evolution Probe
158,1433816036617359369,2530947115,Max Tegmark,"[""Our new paper shows how to measure #MediaBias without bias, using #MachineLearning. We auto-discover not only familiar left-right bias, but also establishment bias, where NPR &amp; Breitbart score almost identically. In big data, bias can't hide! \n<LINK> <LINK>""]",https://arxiv.org/abs/2109.00024,"We present an automated method for measuring media bias. Inferring which newspaper published a given article, based only on the frequencies with which it uses different phrases, leads to a conditional probability distribution whose analysis lets us automatically map newspapers and phrases into a bias space. By analyzing roughly a million articles from roughly a hundred newspapers for bias in dozens of news topics, our method maps newspapers into a two-dimensional bias landscape that agrees well with previous bias classifications based on human judgement. One dimension can be interpreted as traditional left-right bias, the other as establishment bias. This means that although news bias is inherently political, its measurement need not be. ",Machine-Learning media bias
159,1433807598675955738,942694791707545600,Thomas Scialom,"['A very new way to reduce the attention cost in transformer inspired by co. science.\n\nSkim-Attention: Learning to Focus via Document Layout\n<LINK>\n\nAccepted at #EMNLP2021, and first paper for the great Laura Nguyen!\n\nThread\n1/ <LINK>', ""Language Models can't attend long contexts because of the (quadratic) cost of the attention. \n\nSolutions so far: let's optimize the attention formula from O(N²) -&gt; O(N), see BigBird, Reformer etc.\n\nIn this paper, we propose a totally different (&amp; orthogonal) method: \n\n2/"", 'The idea is to learn what to focus on, by leveraging the layout of a document. \nVery human-like: we can guess the structure of a doc at a glance, without reading each token one by one... why should Transformers?\n\n3/', 'By doing so, our model learns in an unsupervised way the document structure, as we qualitatively observed with its attention;\n\nThe final model obtains SOTA results while requiring a shorter window to attend to. \n\n4/ https://t.co/DNcbuijq4x', 'Collaboration with @mlia_lip6 and @RecitalAI. \n\nWith Laura Nguyen, @stjaco and @bpiwowar \n\n5/']",https://arxiv.org/abs/2109.01078,"Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention. ",Skim-Attention: Learning to Focus via Document Layout
160,1433804864828608524,328326288,Mikhail Kats,"['Our new preprint, led by the @RonningGroup: ""Fast recovery of ion-irradiation-induced defects in Ge2Sb2Te5 (GST) thin films at room temperature"" <LINK>\n\nThis paper studies ion-induced defects in GST, a phase-change material used in photonics to enable tunability', 'We explore how GST transitions phases upon ion irradiation, and the differences between defect creation and annealing between hexagonal and rock-salt GST, via optical, electrical, and x-ray experiments\n\nThe @KatsGroup work was supported by @USNavyResearch and in part by NG Next https://t.co/cztqhErNY9']",https://arxiv.org/abs/2109.00716,"Phase-change materials serve a broad field of applications ranging from non-volatile electronic memory to optical data storage by providing reversible, repeatable, and rapid switching between amorphous and crystalline states accompanied by large changes in the electrical and optical properties. Here, we demonstrate how ion irradiation can be used to tailor disorder in initially crystalline Ge2Sb2Te5 (GST) thin films via the intentional creation of lattice defects. We found that continuous Ar ion irradiation at room temperature of GST films causes complete amorphization of GST when exceeding 0.6 (for rock-salt GST) and 3 (for hexagonal GST) displacements per atom (n_dpa). While the transition from rock-salt to amorphous GST is caused by progressive amorphization via the accumulation of lattice defects, several transitions occur in hexagonal GST upon ion irradiation. In hexagonal GST, the creation of point defects and small defect clusters leads to disordering of intrinsic vacancy layers (van der Waals gaps) that drives the electronic metal-insulator transition. Increasing disorder then induces a structural transition from hexagonal to rock-salt and then leads to amorphization. Furthermore, we observed different annealing behavior of defects for rock-salt and hexagonal GST. The higher amorphization threshold in hexagonal GST compared to rock-salt GST is caused by an increased defect-annealing rate, i.e., a higher resistance against ion-beam-induced disorder. Moreover, we observed that the recovery of defects in GST is on the time scale of seconds or less at room temperature. ","Fast recovery of ion-irradiation-induced defects in Ge2Sb2Te5 thin films
  at room temperature"
161,1433799994159497221,60893773,James Bullock,"['New paper w @AstronoMouse_ @cooperUCI @sentientbaryons @MBKplus @laura__sales @joshuadsimon - velocities of satellite galaxies gives Milky Way halo mass of 1−1.2×10^12 M⊙\n<LINK> <LINK>', 'Also @alexanderpji !']",https://arxiv.org/abs/2109.00633,"As the Milky Way and its satellite system become more entrenched in near field cosmology efforts, the need for an accurate mass estimate of the Milky Way's dark matter halo is increasingly critical. With the second and early third data releases of stellar proper motions from {\it Gaia}, several groups calculated full $6$D phase-space information for the population of Milky Way satellite galaxies. Utilizing these data in comparison to subhalo properties drawn from the Phat ELVIS simulations, we constrain the Milky Way dark matter halo mass to be $\sim 1-1.2\times10^{12}~\msun$. We find that the kinematics of subhalos drawn from more- or less-massive hosts (i.e. $>1.2\times10^{12}~\msun$ or $<10^{12}~\msun$) are inconsistent, at the $3\sigma$ confidence level, with the observed velocities of the Milky Way satellites. The preferred host halo mass for the Milky Way is largely insensitive to the exclusion of systems associated with the Large Magellanic Cloud, changes in galaxy formation thresholds, and variations in observational completeness. As more Milky Way satellites are discovered, their velocities (radial, tangential, and total) plus Galactocentric distances will provide further insight into the mass of the Milky Way dark matter halo. ",Sizing from the Smallest Scales: The Mass of the Milky Way
162,1433723891264929794,885598151423520768,Pierre Nyquist,"['🚨New work alert 🚨 Today one of my grad students, Federica Milinanni, got her first paper up on arXiv. Together with our collaborators at @scilifelab we develop a new method (1/n)\n\n[2109.00067] Sensitivity Approximation by the Peano-Baker Series <LINK>']",https://arxiv.org/abs/2109.00067,"In this paper we develop a new method for numerically approximating sensitivities in parameter-dependent ordinary differential equations (ODEs). Our approach, intended for situations where the standard forward and adjoint sensitivity analysis become too computationally costly for practical purposes, is based on the Peano-Baker series from control theory. We give a representation, using this series, for the sensitivity matrix $\boldsymbol{S}$ of an ODE system and use the representation to construct a numerical method for approximating $\boldsymbol{S}$. We prove that, under standard regularity assumptions, the error of our method scales as $O(\Delta t ^2 _{max})$, where $\Delta t _{max}$ is the largest time step used when numerically solving the ODE. We illustrate the performance of the method in several numerical experiments, taken from both the systems biology setting and more classical dynamical systems. The experiments show the sought-after improvement in running time of our method compared to the forward sensitivity approach. For example, in experiments involving a random linear system, the forward approach requires roughly $\sqrt{n}$ longer computational time, where $n$ is the dimension of the parameter space, than our proposed method. ",Sensitivity Approximation by the Peano-Baker Series
163,1433630032077885451,1106481910853701632,Kazuyuki Sekizawa,"['A new #preprint from our collaboration with #BARC, India!\n\nWe report an exploration of orientation effects in multinucleon transfer reaction: <LINK>\n\nMore ambitious we are, more delay in publishing a paper - we have to take a compromise between hope and reality..']",https://arxiv.org/abs/2109.00674,"Background: Multinucleon transfer reactions at energies around the Coulomb barrier offer a vital opportunity to study rich physics of nuclear structure and dynamics. Despite the continuous development in the field, we have still limited knowledge about how deformation - one of the representative nuclear structures - affects multinucleon transfer reactions. Purpose: To shed light on the effect of deformation in multinucleon transfer processes, we study the $^{16}$O+$^{154}$Sm reaction at $E_{\rm lab}$=85 MeV (around the Coulomb barrier) and 134 MeV (substantially above the Coulomb barrier), where the target nucleus, $^{154}$Sm, is a well-established, deformed nucleus. Results: Angular distributions for elastic scattering and for various transfer channels were measured over a wide angular range at the BARC-TIFR pelletron-Linac accelerator facility, Mumbai. The $Q$-value- and angle-integrated isotope production cross sections have been extracted from the measured angular distributions. The experimental data are analyzed along with time-dependent Hartree-Fock calculations. For the lower incident energy case, we find a reasonable agreement between the measurements and the TDHF calculations for a-few-nucleon transfer channels; whereas TDHF underestimates cross sections for many-nucleon transfers, consistent with earlier works. On the other side, we find that calculated cross sections for secondary reaction products for the higher incident energy case, qualitatively explains the measured trends of isotopic distributions observed for the lower energy. The latter observation indicates possible underestimation of excitation energies in the present TDHF+GEMINI analysis. Although certain orientation effects were observed in TDHF results, it is difficult to disentangle them from the $Q$-value- and angle-integrated production cross sections. (Shortened due to the arXiv's length limit.) ","Reaction mechanism study for multinucleon transfer processes in
  collisions of spherical and deformed nuclei at energies near and above the
  Coulomb barrier: The $^{16}$O+$^{154}$Sm reaction"
164,1433625928920305697,3241924438,Akshansh Mishra,['Check out our new paper on arXiv:\n<LINK>\n#MachineLearning #optimization #frictionstirwelding #solidstate #arXiv <LINK>'],https://arxiv.org/abs/2109.00570,"The highest strength-to-weight ratio criterion has fascinated curiosity increasingly in virtually all areas where heft reduction is indispensable. Lightweight materials and their joining processes are also a recent point of research demands in the manufacturing industries. Friction Stir Welding (FSW) is one of the recent advancements for joining materials without adding any third material (filler rod) and joining below the melting point of the parent material. The process is widely used for joining similar and dissimilar metals, especially lightweight non-ferrous materials like aluminum, copper, and magnesium alloys. This paper presents verdicts of optimum process parameters on attaining enhanced mechanical properties of the weld joint. The experiment was conducted on a 5 mm 6061 aluminum alloy sheet. Process parameters; tool material, rotational speed, traverse speed, and axial forces were utilized. Mechanical properties of the weld joint are examined employing a tensile test, and the maximum joint strength efficiency was reached 94.2%. Supervised Machine Learning based Regression algorithms such as Decision Trees, Random Forest, and Gradient Boosting Algorithm were used. The results showed that the Random Forest algorithm yielded highest coefficient of determination value of 0.926 which means it gives a best fit in comparison to other algorithms. ","Process parameter optimization of Friction Stir Welding on 6061AA using
  Supervised Machine Learning Regression-based Algorithms"
165,1433526764710596608,358306755,Jani Kastikainen,['New paper today with Sanjit Shashi! We studied boundary CFT 1- and 2-point functions in a holographic model with an end-of-the-world brane. For this we used the geodesic approximation that requires taking into account geodesics reflecting off the brane.\n\n<LINK>'],https://arxiv.org/abs/2109.00079,"We compute correlation functions, specifically 1-point and 2-point functions, in holographic boundary conformal field theory (BCFT) using geodesic approximation. The holographic model consists of a massive scalar field coupled to a Karch-Randall brane -- a rigid boundary in the bulk AdS space. Geodesic approximation requires the inclusion of paths reflecting off of this brane, which we show in detail. For the 1-point function, we find agreement between geodesic approximation and the harder $\Delta$-exact calculation, and we give a novel derivation of boundary entropy using the result. For the 2-point function, we find a factorization phase transition and a mysterious set of anomalous boundary-localized BCFT operators. We also discuss some puzzles concerning these operators. ",Structure of Holographic BCFT Correlators from Geodesics
166,1433509995279273984,325194378,Antonio Valerio Miceli Barone,"['Check out our new survey paper on low-resource machine translation: <LINK>\n\nWe explore:\n- Data collection\n- Linguistic resources\n- Mono and multilingual training\n- Architectures, training and inference algorithms', 'We also review the performing techniques for multiple language pairs used in the best systems presented in shared tasks\n\nJoint work with @bazril @RABawden @jindra_helcl @alexandrabirch1']",https://arxiv.org/abs/2109.00486,We present a survey covering the state of the art in low-resource machine translation research. There are currently around 7000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a summary of this topical research field and provide a description of the techniques evaluated by researchers in several recent shared tasks in low-resource MT. ,Survey of Low-Resource Machine Translation
167,1433429327002046465,10580512,Stanislas Polu,"['📔 New MiniF2F paper! <LINK>\n\nIntroduces MiniF2F a benchmark of Olympiad-level problem statements formalized in Lean/Metamath/Isabelle.\n\nGPT-f applied to MiniF2F/Metamath ~ 2% 🥶\nGPT-f applied to MiniF2F/Lean ~ 29% 🔥\n\nCode: <LINK>\n\n👇 <LINK>', ""This work and paper was led by @KunhaoZ during his 5 months scientific internship at @Polytechnique in collaboration with @OpenAI helped with @jessemhan's Lean superpowers.\n\nWay to go Kunhao! What an achievement 🙌"", ""Why a benchmark of maths exercises? Because it's very hard to compare neural theorem provers since they are generally tied to specific formal systems and hence specific math libraries and their splits."", 'Also this benchmark is definitely out of distribution compared to typical maths libraries, so hopefully it will serve as a useful measure of mathematical reasoning and generalization in the context of formal maths.', 'This benchmark has been super useful to us @OpenAI and hopefully it will be equally useful to other teams interested in neural theorem proving. We see it as a stepping stone towards the IMO Grand Challenge (https://t.co/BXAU3le1YR) 🦾', 'Looking forward to adding HOL Light, Coq and other systems coverage. Contributions are welcome! (big thanks to @WendaLi8 for his contribution to Isabelle coverage!)']",https://arxiv.org/abs/2109.00110,"We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving. ",MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics
168,1433399816050987010,565140816,K-G Lee,"['Paper Day! In which I again say: ""Give me ALL your multi-object spectroscopy"" and introduce the FLIMFLAM Survey to implement the new technique of FRB foreground mapping. \n\nThis will yield, over the next few yrs, a comprehensive census of cosmic baryons.\n\n<LINK>']",https://arxiv.org/abs/2109.00386,"The dispersion measures (DM) of fast radio bursts (FRBs) encode the integrated electron density along the line-of-sight, which is typically dominated by the intergalactic medium (IGM) contribution in the case of extragalactic FRBs. In this paper, we show that incorporating wide-field spectroscopic galaxy survey data in the foreground of localized FRBs can significantly improve constraints on the partition of diffuse cosmic baryons. Using mock DMs and realistic lightcone galaxy catalogs derived from the Millennium simulation, we define spectroscopic surveys that can be carried out with 4m and 8m-class wide field spectroscopic facilities. On these simulated surveys, we carry out Bayesian density reconstructions in order to estimate the foreground matter density field. In comparison with the `true' matter density field, we show that these can help reduce the uncertainties in the foreground structures by $\sim 2-3\times$ compared to cosmic variance. We calculate the Fisher matrix to forecast that $N=30\: (96)$ localized FRBs should be able to constrain the diffuse cosmic baryon fraction to $\sim 10\%\: (\sim 5\%) $, and parameters governing the size and baryon fraction of galaxy circumgalactic halos to within $\sim 20-25\%\: (\sim 8-12\%)$. From the Fisher analysis, we show that the foreground data increases the sensitivity of localized FRBs toward our parameters of interest by $\sim 25\times$. We briefly introduce FLIMFLAM, an ongoing galaxy redshift survey that aims to obtain foreground data on $\sim 30$ localized FRB fields. ","Constraining the Cosmic Baryon Distribution with Fast Radio Burst
  Foreground Mapping"
169,1433364867688321027,840969572677238790,Sebastian Siol,['Our group just submitted its first (lead) paper. 🎉🥳🥳 Shout-out to @siarhei_zhuk for the great work. We synthesize a new ternary nitride Zn2VN3 &amp; show epitaxial growth at low T. Read the preprint here: <LINK> #nitrides #materials discovery #haxpes #pvd'],https://arxiv.org/abs/2109.00365,"Computationally guided high-throughput synthesis is used to explore the Zn-V-N phase space, resulting in the synthesis of a novel ternary nitride Zn$_2$VN$_3$. Following a combinatorial PVD screening, we isolate the phase and synthesize polycrystalline Zn$_2$VN$_3$ thin films with wurtzite structure on conventional borosilicate glass substrates. In addition, we demonstrate that cation-disordered, but phase-pure (002)-textured, Zn$_2$VN$_3$ thin films can be grown using epitaxial stabilization on {\alpha}-Al2O3 (0001) substrates at remarkably low growth temperatures well below 200 {\deg}C. The structural properties and phase composition of the Zn$_2$VN$_3$ films are studied in detail using XRD and (S)TEM techniques. The composition as well as chemical state of the constituent elements are studied using RBS/ERDA as well as XPS/HAXPES methods. These analyses reveal a stoichiometric material with no oxygen contamination, besides a thin surface oxide. We find that Zn$_2$VN$_3$ is a weakly-doped p-type semiconductor demonstrating broadband room-temperature photoluminescence spanning the range between 2 eV and 3 eV. In addition, the electronic properties can be tuned over a wide range via isostructural alloying on the cation site, making this a promising material for optoelectronic applications. ","Synthesis and Characterization of the Ternary Nitride Semiconductor
  Zn$_2$VN$_3$: Theoretical Prediction, Combinatorial Screening and Epitaxial
  Stabilization"
170,1448327774721036290,1148870802852347904,Yufei Tian,"['Is generating hyperboles easy? Our machine says yes!\n\nCheck our new #EMNLP2021 Findings paper ""HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge"" with Arvind and @VioletNPeng!🧾<LINK>\nCode and data coming soon! <LINK>', '@VioletNPeng What leads to an intentional and creative exaggeration called a hyperbole? We investigate the sensical (commonsense and counterfactual) relationships of sentence-level hyperboles, and propose a generation model accordingly!', '@VioletNPeng More fun examples generated by our HypoGen Model: \n1) The party is so lit that even the street wants to have fun! 🥳\n2) His drawing is so bright that even stars fade away!🌟\n3) The young artist is so productive that Botticelli wants to retire! 🎨']",https://arxiv.org/abs/2109.05097,"A hyperbole is an intentional and creative exaggeration not to be taken literally. Despite its ubiquity in daily life, the computational explorations of hyperboles are scarce. In this paper, we tackle the under-explored and challenging task: sentence-level hyperbole generation. We start with a representative syntactic pattern for intensification and systematically study the semantic (commonsense and counterfactual) relationships between each component in such hyperboles. Next, we leverage the COMeT and reverse COMeT models to do commonsense and counterfactual inference. We then generate multiple hyperbole candidates based on our findings from the pattern, and train neural classifiers to rank and select high-quality hyperboles. Automatic and human evaluations show that our generation method is able to generate hyperboles creatively with high success rate and intensity scores. ","HypoGen: Hyperbole Generation with Commonsense and Counterfactual
  Knowledge"
171,1448046444628221952,1173629422751977473,Ayush Chopra,"['Our new paper at @ICCV_2021. One of my last projects @Adobe before starting at @medialab \nlink: <LINK>\n\nFor product folks: also see my talk on user experiences this can enable, at Adobe Marketing \nSummit <LINK> <LINK>']",https://arxiv.org/abs/2109.07001,"Image-based virtual try-on involves synthesizing perceptually convincing images of a model wearing a particular garment and has garnered significant research interest due to its immense practical applicability. Recent methods involve a two stage process: i) warping of the garment to align with the model ii) texture fusion of the warped garment and target model to generate the try-on output. Issues arise due to the non-rigid nature of garments and the lack of geometric information about the model or the garment. It often results in improper rendering of granular details. We propose ZFlow, an end-to-end framework, which seeks to alleviate these concerns regarding geometric and textural integrity (such as pose, depth-ordering, skin and neckline reproduction) through a combination of gated aggregation of hierarchical flow estimates termed Gated Appearance Flow, and dense structural priors at various stage of the network. ZFlow achieves state-of-the-art results as observed qualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and FID). The paper presents extensive comparisons with other existing solutions including a detailed user study and ablation studies to gauge the effect of each of our contributions on multiple datasets. ",ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors
172,1445780093394042895,978071611143319552,Evan Munro,"['Excited to share a new working paper with S. Wager and @xu_kuang on estimating treatment effects in a market equilibrium! The paper applies a mix of game theory, asymptotic theory, and experiment design to tackle a problem in causal inference. \n\n<LINK>\n(1/3).', 'Programs are often evaluated within a market economy, where individuals buy and sell at the prevailing market price. Equilibrium prices introduce interference, so usual estimators of the ATE do not correctly estimate the average total effect of a treatment. (2/3)', 'We provide estimators for the average direct and indirect treatment effects based on an augmented unit-level randomized experiment. We provide methods for asymptotically exact inference, and describe how\nequilibrium interference impacts the variance of the estimators. (3/3)']",https://arxiv.org/abs/2109.11647,"In evaluating social programs, it is important to measure treatment effects within a market economy, where interference arises due to individuals buying and selling various goods at the prevailing market price. We introduce a stochastic model of potential outcomes in market equilibrium, where the market price is an exposure mapping. We prove that average direct and indirect treatment effects converge to interpretable mean-field treatment effects, and provide estimators for these effects through a unit-level randomized experiment augmented with randomization in prices. We also provide a central limit theorem for the estimators that depends on the sensitivity of outcomes to prices. For a variant where treatments are continuous, we show that the sum of direct and indirect effects converges to the total effect of a marginal policy change. We illustrate the coverage and consistency properties of the estimators in simulations of different interventions in a two-sided market. ",Treatment Effects in Market Equilibrium
173,1445041875065327626,942238055607435264,Luca Carlone,"['in this new survey paper with @aprorok, Matt Malencia, @gauravsukhatme, Brian Sadler, and @vijay_r_kumar, we discuss the importance of resilience in multi-robot systems. The survey provides a taxonomy and a list of open problems: <LINK> #multirobot #resilience']",https://arxiv.org/abs/2109.12343,"Robustness is key to engineering, automation, and science as a whole. However, the property of robustness is often underpinned by costly requirements such as over-provisioning, known uncertainty and predictive models, and known adversaries. These conditions are idealistic, and often not satisfiable. Resilience on the other hand is the capability to endure unexpected disruptions, to recover swiftly from negative events, and bounce back to normality. In this survey article, we analyze how resilience is achieved in networks of agents and multi-robot systems that are able to overcome adversity by leveraging system-wide complementarity, diversity, and redundancy - often involving a reconfiguration of robotic capabilities to provide some key ability that was not present in the system a priori. As society increasingly depends on connected automated systems to provide key infrastructure services (e.g., logistics, transport, and precision agriculture), providing the means to achieving resilient multi-robot systems is paramount. By enumerating the consequences of a system that is not resilient (fragile), we argue that resilience must become a central engineering design consideration. Towards this goal, the community needs to gain clarity on how it is defined, measured, and maintained. We address these questions across foundational robotics domains, spanning perception, control, planning, and learning. One of our key contributions is a formal taxonomy of approaches, which also helps us discuss the defining factors and stressors for a resilient system. Finally, this survey article gives insight as to how resilience may be achieved. Importantly, we highlight open problems that remain to be tackled in order to reap the benefits of resilient robotic systems. ","Beyond Robustness: A Taxonomy of Approaches towards Resilient
  Multi-Robot Systems"
174,1443994172935966722,19089454,Dr. Teddy Kareta,"[""This press release for our exciting new paper on metal-rich objects near the Earth has some excellent quotes by our group's very own @Asteroid_Adam and @astro_cantillo. It's a neat paper, take a look!\n(It's also on ArXiV at: <LINK>) <LINK> <LINK>""]",https://arxiv.org/abs/2109.13950,"Metal-rich near-Earth asteroids (NEAs) represent a small fraction of the NEA population that is mostly dominated by S- and C-type asteroids. Because of this, their identification and study provide us with a unique opportunity to learn more about the formation and evolution of this particular type of bodies, as well as their relationship with meteorites found on Earth. We present near-infrared (NIR) spectroscopic data of NEAs 6178 (1986 DA) and 2016 ED85. We found that the spectral characteristics of these objects are consistent with those of metal-rich asteroids, showing red slopes, convex shapes, and a weak pyroxene absorption band at $\sim$0.93 $\mu$m. The compositional analysis showed that they have a pyroxene chemistry of Fs$_{40.6\pm3.3}$Wo$_{8.9\pm1.1}$ and a mineral abundance of $\sim$15% pyroxene and 85% metal. We determined that these objects were likely transported to the near-Earth space via the 5:2 mean motion resonance with Jupiter. Asteroid spectra were compared with the spectra of mesosiderites and bencubbinites. Differences in the NIR spectra and pyroxene chemistry suggest that bencubbinites are not good meteorite analogs. Mesosiderites were found to have a similar pyroxene chemistry and produced a good spectral match when metal was added to the silicate component. We estimated that the amounts of Fe, Ni, Co, and the platinum group metals present in 1986 DA could exceed the reserves worldwide. ","Physical Characterization of Metal-rich Near-Earth Asteroids 6178 (1986
  DA) and 2016 ED85"
175,1443285707842334734,846477429669593088,Savvas Kyriacou,"['New paper posted on arxiv! \n\nPheno work with folks here at JHU, focusing on the Higgs anomalous couplings to virtual photons. \n\n<LINK>\n\n#particlephysics #Higgs #LHC #Pheno']",https://arxiv.org/abs/2109.13363,"We present a study of Higgs boson production in vector boson fusion and in association with a vector boson and its decay to two vector bosons, with a focus on the treatment of virtual loops and virtual photons. Our analysis is performed with the JHU generator framework. Comparisons are made to several other frameworks, and the results are expressed in terms of an effective field theory. New features of this study include a proposal on how to handle singularities involving Higgs boson decays to light fermions via photons, calculation of the partial Higgs boson width in the presence of anomalous couplings to photons, a comparison of the next-to-leading-order electroweak corrections to effects from effective couplings, and phenomenological observations regarding the special role of intermediate photons in analysis of LHC data in the effective field theory framework. Some of these features are illustrated with projections for experimental measurements with the full LHC and HL-LHC datasets. ",Constraining anomalous Higgs boson couplings to virtual photons
176,1443062326295506944,1235230714922184712,Nupoor Gandhi,"['Happy to share our new paper:\xa0Improving Span Representation for Domain-adapted Coreference Resolution (<LINK>) in the\xa0Fourth Workshop on Computational Models of Reference, Anaphora and Coreference #EMNLP2021 !\n\njoint work with @anjalie_f and Yulia Tsvetkov', 'We propose two new losses to incorporate external knowledge for more data-efficient fine-tuning of coreference models.']",https://arxiv.org/abs/2109.09811,"Recent work has shown fine-tuning neural coreference models can produce strong performance when adapting to different domains. However, at the same time, this can require a large amount of annotated target examples. In this work, we focus on supervised domain adaptation for clinical notes, proposing the use of concept knowledge to more efficiently adapt coreference models to a new domain. We develop methods to improve the span representations via (1) a retrofitting loss to incentivize span representations to satisfy a knowledge-based distance function and (2) a scaffolding loss to guide the recovery of knowledge from the span representation. By integrating these losses, our model is able to improve our baseline precision and F-1 score. In particular, we show that incorporating knowledge with end-to-end coreference models results in better performance on the most challenging, domain-specific spans. ",Improving Span Representation for Domain-adapted Coreference Resolution
177,1442888852859879430,1003938762382905344,Dheeraj Mekala,"['Paper alert❗️: We introduce a new problem called coarse-to-fine grained classification which aims to perform fine-grained classification on coarsely annotated data without any fine-grained annotations.\n👉 <LINK> (1/2)\n#EMNLP2021 #NLProc <LINK>', 'Instead of asking for new fine-grained annotations, we leverage label surface names and weave in pre-trained generative LMs. \n\nJoint work with @VarunGangal and @shangjingbo (2/2)']",https://arxiv.org/abs/2109.10856,"Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only human guidance and weave in rich pre-trained generative language models into the iterative weak supervision strategy. Specifically, we first propose a label-conditioned finetuning formulation to attune these generators for our task. Furthermore, we devise a regularization objective based on the coarse-fine label constraints derived from our problem setting, giving us even further improvements over the prior formulation. Our framework uses the fine-tuned generative models to sample pseudo-training data for training the classifier, and bootstraps on real unlabeled data for model refinement. Extensive experiments and case studies on two real-world datasets demonstrate superior performance over SOTA zero-shot classification baselines. ","Coarse2Fine: Fine-grained Text Classification on Coarsely-grained
  Annotated Data"
178,1441445140644696064,796952127771918336,Pierre Darancet,"['New paper alert: Near-field electrostatic effects of molecular layers on 2D materials.  With Qunfei Zhou, Michele Kotiuga, and two amazing REU students (at the time): Bukuru Anacket and Trevor Steiner\n <LINK> <LINK>', 'We looked at how the electrostatic potential molecular assemblies can change the properties of nearby (say, within 1 nm) electrons --for example, electrons in a 2D substrate. https://t.co/K34ffXnZUi', 'Some of the effects are already well-known e.g. any dipole in the molecular layer can dramatically impact surface properties. But what about higher moments of the charge density? We find that while they can be quite significant.', 'In particular, for some common molecular assemblies, they ""imprint"" a periodic potential fairly reminiscent of some simple models (Muffin tin, Kronig-Penney).. Have a look! Modulation of the in-plane potential is on the scale of 0.5eV over a nm. https://t.co/7Itz2DMmP3', ""Side note: These effects can't be described by a multipole expansion of the potential due to the lateral extent of the molecule being comparable to the molecule-2D distance, so we had to develop our own theory --based on point charges in a neutral plane. It works beautifully! https://t.co/4YRY5GVh62"", '(at least at these lengthscales). For example, it predicts the near- to far-field transition of the potential (for C8-BTBT molecule shown here and others in the paper) https://t.co/1L1kxOo2kM', 'and the in-plane profile with a handful of point charges.  (DFT top, point charges in a neutral plane ~4 Angstroms above at the bottom). https://t.co/AGLaLzC7qY', 'As always, thanks to @NSF, CNM at @argonne, @NUMRSEC, @nu_argonne_inst  and the wonderful REU program brilliantly managed by Kathleen Stair for making this research possible.']",https://arxiv.org/abs/2109.09990,"We compute the electronic structure of two-dimensional (2D) materials decorated with self-assembled organic monolayers using density functional theory. We find that 2D materials are strongly impacted by near-field electrostatic effects resulting from high multipoles of the organic layer electronic density. We show that this effect can lead to significant (~0.5V) modulation of the in-plane potential experienced by electrons in 2D materials within ~4\AA from the molecular layer, with a transition between near- and far-field depending on the lateral extent of the molecules. We develop a theory of this effect, showing that the electrostatic potential of the molecular layer can be approximated by a discretized planar charge density derived from the molecular structure and multipoles. Solving this model computationally and analytically, we propose implementations of this effect to generate novel electronic properties for electrons in 2D materials, such as band gap opening and anisotropic group velocity modulation for monolayer graphene from experimentally achievable molecular assemblies. ","Engineering the Electronic Structure of Two-Dimensional Materials with
  Near-Field Electrostatic Effects of Self-Assembled Organic Layers"
179,1441106413745414154,190779003,Semih Yavuz,"['Excited to share our Rank-and-Generate (RnG-KBQA) approach sets new SoTA for knowledge base question answering. Paper, code, and models are released. Welcome to give it a try :) \nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2109.08678,"Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization. ","RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base
  Question Answering"
180,1441074229366059008,2550133394,Mostafa Dehghani,"['Check out our new paper, presenting insights on scaling Transformers/T5 within the context of the pretraining-finetuning setup.  \n<LINK>\n\nWith @YiTayML, @Jeffy_Sailing, @LiamFedus, @samiraabnar, @hwchung27, @sharan0909, @DaniYogatama, @ashVaswani, and @metzlerd. <LINK>', 'In search of the best scaling recipe, we ran an extensive search over different knobs and evaluated our modes w.r.t not only upstream performance (perplexity) but also the performance on different downstream tasks/benchmarks after finetuning. \nA few cool take-home messages:', '(1) If you, by default, take the T5-Base for your next amazing project to build on top of it, you may want to reconsider that. Transformer/T5 Base is not necessarily the Pareto-efficient configuration. We present some better alternatives.', '(2) Although model size is a key factor determining the scaling behavior of Transformers in pretraining, the model shape matters a lot for fine-tuning. So blindly scaling up may look good upstream, but can be disappointing when you finetune your model on a downstream task.', '(3) ""How to scale up?"" is pretty much depending on the region you\'re in. If you\'re working with Tiny models and want to go to Base size, the best recipe differs from the case of going from Large to XL.', 'We have many more observations and insights. You should check the paper.', 'Oh, btw, we are releasing 100+ pretrained checkpoints from our experiments, which will be an amazing playground for doing further analysis and interesting research :)', ""You should also check @YiTayML's great thread that highlights more points from the paper:\nhttps://t.co/MLj7rdrq1b""]",https://arxiv.org/abs/2109.10686,"There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis. ","Scale Efficiently: Insights from Pre-training and Fine-tuning
  Transformers"
181,1438457730621128710,2546561779,Masoud,"['Check our new paper at #EMNLP2021 ""Graph Algorithms for Multiparallel Word Alignment"". We get better results compared to bilingual aligners even when we use noisy translations.\nJoint work: @imani_ayyoob\n@lksenel @PDufter @HinrichSchuetze\nPDF: <LINK>\n#NLProc']",https://arxiv.org/abs/2109.06283,"With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit the multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements in $F_1$ of up to 28% over the baseline bilingual word aligner in different datasets. ",Graph Algorithms for Multiparallel Word Alignment
182,1438142884914548739,1171357907574824961,Łukasz Tychoniec,"['Releasing a paper today feels like a premiere of the movie close to the new Star Wars (#MAPS). However, this is a impressive first-paper of a PhD student in Toruń, Aga Mirocha on CN/HCN as tracers of UV in star-forming regions <LINK> <LINK>']",https://arxiv.org/abs/2109.06793,"Context: Ultraviolet radiation (UV) influences the physics and chemistry of star-forming regions, but its properties and significance in the immediate surroundings of low-mass protostars are still poorly understood. Aims: We aim to extend the use of the CN/HCN ratio, already established for high-mass protostars, to the low-mass regime to trace and characterize the UV field around low-mass protostars on $\sim 0.6\times0.6$ pc scales. Methods: We present $5'\times5'$ maps of the Serpens Main Cloud encompassing 10 protostars observed with the EMIR receiver at the IRAM 30 m telescope in CN 1-0, HCN 1-0, CS 3-2, and some of their isotopologues. The radiative-transfer code RADEX and the chemical model Nahoon are used to determine column densities of molecules, gas temperature and density, and the UV field strength, $G_\mathrm{0}$. Results: The spatial distribution of HCN and CS are well-correlated with CO 6-5 emission that traces outflows. The CN emission is extended from the central protostars to their immediate surroundings also tracing outflows, likely as a product of HCN photodissociation. The ratio of CN to HCN total column densities ranges from $\sim$1 to 12 corresponding to G$_0$ $\approx$ $10^{1}-10^{3}$ for gas densities and temperatures typical for outflows of low-mass protostars. Conclusions: UV radiation associated with protostars and their outflows is indirectly identified in a significant part of the Serpens Main low-mass star-forming region. Its strength is consistent with the values obtained from the OH and H$_2$O ratios observed with Herschel and compared with models of UV-illuminated shocks. From a chemical viewpoint, the CN to HCN ratio is an excellent tracer of UV fields around low- and intermediate-mass star-forming regions. ","Signatures of UV radiation in low-mass protostars I. Origin of HCN and
  CN emission in the Serpens Main region"
183,1437773070283026437,932462804056936448,Kung-Hsiang Steeve Huang,"['Excited to share our new #EMNLP2021 paper ""Document-level Entity-based Extraction as Template Generation"". \nJoint work with Sam Tang and @VioletNPeng.\n\nPre-print: <LINK>\nCode: <LINK>\n#NLProc <LINK>', '@VioletNPeng Conventional classification-based approach to document-level IE tasks often struggle to capture long-term dependencies among entity types. By framing the problems as a template generate task, we significantly reduce distances between entities in the decoding targets.\n\n1/', '@VioletNPeng We use a pre-trained seq2seq model to learn the template generation task. To enhance its capabilities of identifying salient mentions in the input document, a novel copy mechanism guided by cross-attention, TopK Copy, is integrated into BART.\n\n2/', '@VioletNPeng Specifically, we hypothesize that some of the cross-attention heads are less important, and thus mislead the model in copying the incorrect tokens. Hence, TopK Copy only focus on the top-k most important heads. Experiment results confirm this hypothesis.\n\n3/ https://t.co/c3tXHYz5VA']",https://arxiv.org/abs/2109.04901,"Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%), binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score. ",Document-level Entity-based Extraction as Template Generation
184,1437691753654591490,72781449,Nikos Aletras,['New paper w/ Samuel Mensah &amp; Kai Sun:  large empirical study on Target-oriented Opinion Words Extraction showing that (1) position &gt; syntactic info (encoded using GCNs) and (2) simple baselines &gt; complex models #EMNLP2021\n\n💾: <LINK>\n\n📰: <LINK>'],https://arxiv.org/abs/2109.01238,"Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on pretrained word embeddings or language models that leverage part-of-speech and position embeddings, aiming to examine the actual contribution of each component in TOWE. We also adapt a graph convolutional network (GCN) to enhance word representations by incorporating syntactic information. Our experimental results demonstrate that BiLSTM-based models can effectively encode position information into word representations while using a GCN only achieves marginal gains. Interestingly, our simple methods outperform several state-of-the-art complex neural structures. ","An Empirical Study on Leveraging Position Embeddings for Target-oriented
  Opinion Words Extraction"
185,1437474244515057665,850415526602059777,Vikram Dwarkadas,"[""Can we really infer the progenitor of a supernova remnant using the Fe Kalpha line centroid? Our new paper, accepted to ApJ, <LINK> explores that in detail. Led by Jared Siegel, 4th year undergrad. His 3rd Ist author paper with me. And he's got one on exoplanets.""]",https://arxiv.org/abs/2109.01157,"The centroid energy of the Fe K$\alpha$ line has been used to identify the progenitors of supernova remnants (SNRs). These investigations generally considered the energy of the centroid derived from the spectrum of the entire remnant. Here we use {\it XMM-Newton} data to investigate the Fe K$\alpha$ centroid in 6 SNRs: 3C~397, N132D, W49B, DEM L71, 1E 0102.2-7219, and Kes 73. In Kes 73 and 1E 0102.2-7219, we fail to detect any Fe K$\alpha$ emission. We report a tentative first detection of Fe K$\alpha$ emission in SNR DEM L71, with a centroid energy consistent with its Type Ia designation. In the remaining remnants, the spatial and spectral sensitivity is sufficient to investigate spatial variations of the Fe K$\alpha$ centroid. We find in N132D and W49B that the centroids in different regions are consistent with that derived from the overall spectrum, although not necessarily with the remnant type identified via other means. However, in SNR 3C~397, we find statistically significant variation in the centroid of up to 100 eV, aligning with the variation in the density structure around the remnant. These variations span the intermediate space between centroid energies signifying core-collapse and Type Ia remnants. Shifting the dividing line downwards by 50 eV can place all the centroids in the CC region, but contradicts the remnant type obtained via other means. Our results show that caution must be used when employing the Fe K$\alpha$ centroid of the entire remnant as the sole diagnostic for typing a remnant. ",Can the Fe K-alpha Line Reliably Predict Supernova Remnant Progenitors?
186,1436713016041541641,796224233781104641,Trevor Incerti,"['Lead author, package writer, and *brilliant* recent Yale S&amp;DS and EP&amp;E grad @J_S_Carlson provides a great overview of our new working paper with P.M. Aronow. \n\nPaper here: <LINK> <LINK>']",https://arxiv.org/abs/2109.03774,"Quantitative empirical inquiry in international relations often relies on dyadic data. Standard analytic techniques do not account for the fact that dyads are not generally independent of one another. That is, when dyads share a constituent member (e.g., a common country), they may be statistically dependent, or ""clustered."" Recent work has developed dyadic clustering robust standard errors (DCRSEs) that account for this dependence. Using these DCRSEs, we reanalyzed all empirical articles published in International Organization between January 2014 and January 2020 that feature dyadic data. We find that published standard errors for key explanatory variables are, on average, approximately half as large as DCRSEs, suggesting that dyadic clustering is leading researchers to severely underestimate uncertainty. However, most (67% of) statistically significant findings remain statistically significant when using DCRSEs. We conclude that accounting for dyadic clustering is both important and feasible, and offer software in R and Stata to facilitate use of DCRSEs in future research. ",Dyadic Clustering in International Relations
187,1436336395698655233,2918290813,Dr./Prof. Sam Lawler,"['New paper thread: ""Visibility Predictions for Near-Future Satellite Megaconstellations: Latitudes near 50 Degrees will Experience the Worst Light Pollution""\n\n<LINK>\n\nAlternate title: ""3 Canadian astro profs are REALLY TICKED OFF about satellites in the night sky""', '(The three ticked-off Canadian astro profs are me, Aaron Boley of @OuterSpaceInst , and @hannorein)', ""The reason I wrote this paper is because I want to know how worried to be about future megaconstellations of satellites.  \n\nThe answer: REALLY worried.  These things are bright, and there's a LOT of them planned to launch."", ""I know, just what you need, something ELSE to worry about besides climate change and covid and everything else.  \n\nHere's a baby guinea and her two turkey mamas for courage. https://t.co/5WtGKimG6P"", ""We used 65,000 satellites (Starlink, OneWeb, Kuiper, StarNet/GW) on their filed or predicted orbits.  \n\nThat's a LOT of satellites - there are currently about 3,500 operating satellites (almost half are Starlinks now) https://t.co/MGlel4ozDg"", 'How many out of these 65,000 are sunlit and in your sky?  Depends on the season, your latitude, and the time of night. https://t.co/xdidsQ0ec1', 'But how many of these sunlit sats are visible?  This is much more complicated.  \n\nDepends on the shapes and reflectivities of the satellites, which is not public info.  Astronomers have to spend good observing time to find this out. \n\nBut we need a model to make predictions.', ""So, like good physicists, we used a sphere - we know this is a terrible approximation, but more complicated models didn't actually work any better. So, go with simple.\n\nWe fit this model to observations of real Starlink satellites from the Plaskett Telescope in Victoria, BC https://t.co/Bs7pqcUgvD"", ""Is this a good approximation of all future satellites?  Don't know.  \n\nStarlink HAS changed their design to make it fainter, which is great! (It's still naked-eye visible, but better than it was).  \n\nWill other companies do this?  Or will they be brighter?  Don't know."", ""We applied this model to the 65,000 satellites, and made predictions for all over the world.  Hawaii at latitude 20N (where lots of big telescopes are) isn't so bad: lots of satellites close to dusk and dawn, but drops quickly so lots of satellite-free observing time. https://t.co/p0UtsSqJpz"", ""Here's a movie of a night from Hawaii on the equinox: https://t.co/H9TULxddAV\n\nAnd here's the summer solstice:\nhttps://t.co/PKEaZIrXQV"", ""50 degrees latitude (where I live) is a different story. The equinox isn't too bad, there is some time during the night with only a few faint satellites close to the horizon.\nhttps://t.co/u0XzgCvmLb"", 'But the summer solstice is terrible from 50 degrees latitude.  There will be hundreds of bright, naked-eye visible sunlit satellites near the zenith all night long.  This really sucks.  (This is equally applicable to 50 degrees south)\nhttps://t.co/Z2ClvC5flD', 'Here are some cute ducklings.  You probably need them by now.  (I do.) https://t.co/jIIF0oFI5L', 'Changing the altitude of satellites just changes the problem.  Higher orbits make satellites harder to see naked eye, but spreads the problem across more of the earth (not just 50 degrees latitude).', 'Lower orbits are much brighter, so more disruptive to naked-eye stargazing, but better for big research telescopes because the satellites move across the field of view faster, smearing their light across more pixels.', ""But there isn't a magic altitude that's great for everyone.  It's all terrible, just terrible in different ways. https://t.co/8H70yi3BaW"", 'There are loads of other problems with thousands of sats in orbit: Kessler syndrome, atmospheric pollution from launches, satellite re-entries depositing literally tons of aluminum in the upper atmosphere every single day. Radio astronomy is even worse off than optical astronomy.', ""This future sky really sucks. I do not want it.  \n\nBut without regulation of satellites, or voluntary industry standards for faint satellties RIGHT NOW, this is exactly where we're headed."", ""So... what can you do? Giant corporations will only respond to 1) legislation, and 2) consumer pressure.  \n\nTell your local/prov./state/fed gov't that you want them to support other ways of getting internet (fibreoptic, cell networks - requires infrastructure investment)"", ""If you are in a place where you have other internet options, don't buy satellite internet (I am very aware this is a privilege not everyone has).  \n\nIf you do buy satellite internet, tell your provider (repeatedly!) that you are concerned about the night sky and pollution."", ""Thanks for reading, here's a cute floofy dog and a snuggly goat. https://t.co/oKtq1LzmeL"", ""Go outside and enjoy your night sky!  \n\nIt's changing right now.""]",https://arxiv.org/abs/2109.04328,"Megaconstellations of thousands to tens of thousands of artificial satellites (satcons) are rapidly being developed and launched. These satcons will have negative consequences for observational astronomy research, and are poised to drastically interfere with naked-eye stargazing worldwide should mitigation efforts be unsuccessful. Here we provide predictions for the optical brightnesses and on-sky distributions of several satcons, including Starlink, OneWeb, Kuiper, and StarNet/GW, for a total of 65,000 satellites on their filed or predicted orbits. We develop a simple model of satellite reflectivity, which is calibrated using published Starlink observations. We use this model to estimate the visible magnitudes and on-sky distributions for these satellites as seen from different places on Earth, in different seasons, and different times of night. For latitudes near 50 degrees North and South, satcon satellites make up a few percent of all visible point sources all night long near the summer solstice, as well as near sunrise and sunset on the equinoxes. Altering the satellites' altitudes only changes the specific impacts of the problem. Without drastic reduction of the reflectivities, or significantly fewer total satellites in orbit, satcons will significantly change the night sky worldwide. ","Visibility Predictions for Near-Future Satellite Megaconstellations:
  Latitudes near 50 Degrees will Experience the Worst Light Pollution"
188,1435664887007506433,2437293979,Nathan Ratledge,"[""Pleased to share our working paper on the economic effects of grid-based electrification. A few thoughts to complement @MarshallBBurke's great thread. \n\n1. Satellite imagery and machine learning represent a new frontier for economics &amp; policy evaluation.\n\n<LINK>"", '2. Huge thanks to @atlasai_co, especially co-author Gabe Cadamuro, for their ground breaking work using satellite imagery &amp; AI to literally create new data in really challenging settings. \n\nHere, we use imputed outcomes to create a balanced dataset suitable for causal inference.', '3. One takeaway for other researches using similar methods is to be cognizant of the downstream task.  Optimizing for r2 in the CNN process may not be the best metric, as we show. \n\nThanks to Marshall, Brandon de la Cuesta and Matthieu Stigler for their brainpower on this.', '4. Ultimately, we find that new grid-based electrification had a substantial, statistically significant positive effect on asset wealth in Uganda, with electrified communities growing nearly 2x as fast as un-electrified comms.  \n\nThese results are robust to varying parameters.', '5. Stepping away from the paper &amp; putting on my policy hat, our results suggest that investments in grid extensions (while costly) have measurable, near-term impacts, even in low income settings.   \n\nAs 600m people in SSA lack electricity, we should accelerate grid extensions.']",https://arxiv.org/abs/2109.02890,"In many regions of the world, sparse data on key economic outcomes inhibits the development, targeting, and evaluation of public policy. We demonstrate how advancements in satellite imagery and machine learning can help ameliorate these data and inference challenges. In the context of an expansion of the electrical grid across Uganda, we show how a combination of satellite imagery and computer vision can be used to develop local-level livelihood measurements appropriate for inferring the causal impact of electricity access on livelihoods. We then show how ML-based inference techniques deliver more reliable estimates of the causal impact of electrification than traditional alternatives when applied to these data. We estimate that grid access improves village-level asset wealth in rural Uganda by 0.17 standard deviations, more than doubling the growth rate over our study period relative to untreated areas. Our results provide country-scale evidence on the impact of a key infrastructure investment, and provide a low-cost, generalizable approach to future policy evaluation in data sparse environments. ","Using Satellite Imagery and Machine Learning to Estimate the Livelihood
  Impact of Electricity Access"
189,1435634029504589824,921385730,Marshall Burke,"['Check out our new paper (preprint aka working paper) using satellites and ML to measure the impact of electrification on economic livelihoods, led by @NWRAT with colleagues at @atlasai_co . Quick 🧵 <LINK>', 'Billions of $ going into expansion of energy grid across emerging markets, but lack of consensus on what this expansion means for livelihoods. Challenging research question: grid placement not random, hard to measure livelihood outcomes at scale. 2/n', 'To solve measurement prob, we use daytime satellite imagery (Landsat) and deep learning to measure evolution of local-level asset wealth over two decades across Uganda. Model works well: high predictive performance in handful of locations/yrs where we have ground data. 3/n', 'We pair sat-based estimates of wealth with constructed dataset of grid expansion. Then use new-ish ML estimators (matrix completion) to help deal with potentially endogenous grid placement. Show multiple tests to convince ourselves (refs) that it might have worked. 4/n', 'Critically, we show that livelihood prediction step cannot be divorced from causal inference step.  Standard loss fnc in deep learning models generates output with too low variance in this setting; this attenuates causal estimates. Custom loss fnc in step 1 model fixes it. 5/n', 'We find large positive effects of electrification on community-level wealth in Uganda: +0.2sd in the 3-4 years after grid access.  Seems good.  We believe suite of methods we use could be useful for large-scale impact evaluation in a lot of settings. /n']",https://arxiv.org/abs/2109.02890,"In many regions of the world, sparse data on key economic outcomes inhibits the development, targeting, and evaluation of public policy. We demonstrate how advancements in satellite imagery and machine learning can help ameliorate these data and inference challenges. In the context of an expansion of the electrical grid across Uganda, we show how a combination of satellite imagery and computer vision can be used to develop local-level livelihood measurements appropriate for inferring the causal impact of electricity access on livelihoods. We then show how ML-based inference techniques deliver more reliable estimates of the causal impact of electrification than traditional alternatives when applied to these data. We estimate that grid access improves village-level asset wealth in rural Uganda by 0.17 standard deviations, more than doubling the growth rate over our study period relative to untreated areas. Our results provide country-scale evidence on the impact of a key infrastructure investment, and provide a low-cost, generalizable approach to future policy evaluation in data sparse environments. ","Using Satellite Imagery and Machine Learning to Estimate the Livelihood
  Impact of Electricity Access"
190,1435182143810850816,4035203532,Linqing Liu,"['What aspects of novel questions are challenging for Open Domain Question Answering models? We explore it by introducing and annotating questions into three levels of generalization in our new work w/ @PSH_Lewis  @riedelcastro, Pontus Stenetorp. 1/N\nPaper: <LINK> <LINK>', 'Experiments show non-parametric models can handle novel question entities comparatively well, but struggle to generalize compositionally. 2/N', 'We identify key factors that impact generalization performance are: cascading errors from the retrieval component, frequency of question pattern, and frequency of the entity. 3/N', ""Perhaps surprisingly, we find the frequency of entities mentioned in a question is strongly negatively correlated with accuracy. It's likely due to large number of superficially relevant distractor passages being retrieved for questions with highly frequent entities. 4/N""]",https://arxiv.org/abs/2109.01156,"Recent work on Open Domain Question Answering has shown that there is a large discrepancy in model performance between novel test questions and those that largely overlap with training questions. However, it is unclear which aspects of novel questions make them challenging. Drawing upon studies on systematic generalization, we introduce and annotate questions according to three categories that measure different levels and kinds of generalization: training set overlap, compositional generalization (comp-gen), and novel-entity generalization (novel-entity). When evaluating six popular parametric and non-parametric models, we find that for the established Natural Questions and TriviaQA datasets, even the strongest model performance for comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the full test set -- indicating the challenge posed by these types of questions. Furthermore, we show that whilst non-parametric models can handle questions containing novel entities relatively well, they struggle with those requiring compositional generalization. Lastly, we find that key question difficulty factors are: cascading errors from the retrieval component, frequency of question pattern, and frequency of the entity. ",Challenges in Generalization in Open Domain Question Answering
191,1447755432714330114,2577596593,Chelsea Finn,"['Robot learning is bottlenecked on good, reusable datasets\n\nWe introduce:\n* a new dataset with demos of 71 tasks over 10 envs, all w/ a low-cost arm\n* find the data can be used to help solve new tasks in new envs\n\nPaper: <LINK>\nData &amp; Code: <LINK> <LINK>', 'Led by @febert8888 &amp; Yanlai Yang\nIn collab with Schmeckpeper, Bucher, Georgakis, @KostasPenn, @svlevine']",https://arxiv.org/abs/2109.13396,"Robot learning holds the promise of learning policies that generalize broadly. However, such generalization requires sufficiently diverse datasets of the task of interest, which can be prohibitively expensive to collect. In other fields, such as computer vision, it is common to utilize shared, reusable datasets, such as ImageNet, to overcome this challenge, but this has proven difficult in robotics. In this paper, we ask: what would it take to enable practical data reuse in robotics for end-to-end skill learning? We hypothesize that the key is to use datasets with multiple tasks and multiple domains, such that a new user that wants to train their robot to perform a new task in a new domain can include this dataset in their training process and benefit from cross-task and cross-domain generalization. To evaluate this hypothesis, we collect a large multi-domain and multi-task dataset, with 7,200 demonstrations constituting 71 tasks across 10 environments, and empirically study how this data can improve the learning of new tasks in new environments. We find that jointly training with the proposed dataset and 50 demonstrations of a never-before-seen task in a new domain on average leads to a 2x improvement in success rate compared to using target domain data alone. We also find that data for only a few tasks in a new domain can bridge the domain gap and make it possible for a robot to perform a variety of prior tasks that were only seen in other domains. These results suggest that reusing diverse multi-task and multi-domain datasets, including our open-source dataset, may pave the way for broader robot generalization, eliminating the need to re-collect data for each new robot learning project. ","Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain
  Datasets"
192,1447493409359470592,1393782502737776641,Mahmoud shoush,['Looking forward to presenting the first paper of my Ph.D. research at the #ML4PM workshop @icpm_conf with @marlon_dumas. We propose a prescriptive monitoring approach that triggers intervention to optimize a cost function under fixed resource constraints. \n<LINK>'],https://arxiv.org/abs/2109.02894,"Prescriptive process monitoring is a family of techniques to optimize the performance of a business process by triggering interventions at runtime. Existing prescriptive process monitoring techniques assume that the number of interventions that may be triggered is unbounded. In practice, though, specific interventions consume resources with finite capacity. For example, in a loan origination process, an intervention may consist of preparing an alternative loan offer to increase the applicant's chances of taking a loan. This intervention requires a certain amount of time from a credit officer, and thus, it is not possible to trigger this intervention in all cases. This paper proposes a prescriptive process monitoring technique that triggers interventions to optimize a cost function under fixed resource constraints. The proposed technique relies on predictive modeling to identify cases that are likely to lead to a negative outcome, in combination with causal inference to estimate the effect of an intervention on the outcome of the case. These outputs are then used to allocate resources to interventions to maximize a cost function. A preliminary empirical evaluation suggests that the proposed approach produces a higher net gain than a purely predictive (non-causal) baseline. ","Prescriptive Process Monitoring Under Resource Constraints: A Causal
  Inference Approach"
193,1446852182846820353,1446849828638449664,Minhyuk Sung,['#ICCV 2021 (1/2) Session 6\n\nCPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Cloud\n\nWe propose a cascaded network to handle high-res 3D point clouds in the primitive fitting problem.\n\nwebpage: <LINK>\narXiv: <LINK>'],https://arxiv.org/abs/2109.00113,"Representing human-made objects as a collection of base primitives has a long history in computer vision and reverse engineering. In the case of high-resolution point cloud scans, the challenge is to be able to detect both large primitives as well as those explaining the detailed parts. While the classical RANSAC approach requires case-specific parameter tuning, state-of-the-art networks are limited by memory consumption of their backbone modules such as PointNet++, and hence fail to detect the fine-scale primitives. We present Cascaded Primitive Fitting Networks (CPFN) that relies on an adaptive patch sampling network to assemble detection results of global and local primitive detection networks. As a key enabler, we present a merging formulation that dynamically aggregates the primitives across global and local scales. Our evaluation demonstrates that CPFN improves the state-of-the-art SPFN performance by 13-14% on high-resolution point cloud datasets and specifically improves the detection of fine-scale primitives by 20-22%. ","CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point
  Clouds"
194,1446326187416961026,990433714948661250,Sergey Levine,"['Is there a principled way to adapt a model to distributional shift without labels? In our new paper, ""Training on Test Data"", we propose a Bayesian adaptation strategy based on BNNs and entropy minimization. w/ Aurick Zhou: <LINK>\n\nA thread:', 'First: what information do we gain from observing unlabeled datapoints from a new distribution? We can draw a graphical model for this: x is input, y is label, theta is classifier params, phi parameterizes x distro. Unfortunately, if y is unobserved, x tells nothing about theta! https://t.co/E2w5hxaYyV', ""We need a better graphical model. What if we assume new datapoints are not arbitrary: if we are asked to classify a new OOD, it likely belongs to *one of* the classes, we just don't know which one! Now there is a relationship between theta and phi for each distribution! https://t.co/2BPaQCR0yh"", ""This naturally leads to an entropy minimization procedure at test time: get some unlabeled points, and then update the parameter posterior to get lower entropy on test points, but don't stray too far from parameter distribution on training set! https://t.co/TdgRlHmEZQ"", 'To avoid needing to store all training data, we can learn posterior q(theta) using any BNN approach, and then incorporate this as a regularizer when minimizing entropy at test time on unlabeled data. https://t.co/cHlCQLDQRu', 'This leads to better accuracy *and* better calibration on unlabeled test points.\n\nInspired by some classics on entropy minimization:\n\nY. Grandvalet, Y. Bengio. Semi-supervised Learning by Entropy Minimization\nM. Seeger. Input-dependent Regularization of Conditional Density Models']",https://arxiv.org/abs/2109.12746,"When faced with distribution shift at test time, deep neural networks often make inaccurate predictions with unreliable uncertainty estimates. While improving the robustness of neural networks is one promising approach to mitigate this issue, an appealing alternate to robustifying networks against all possible test-time shifts is to instead directly adapt them to unlabeled inputs from the particular distribution shift we encounter at test time. However, this poses a challenging question: in the standard Bayesian model for supervised learning, unlabeled inputs are conditionally independent of model parameters when the labels are unobserved, so what can unlabeled data tell us about the model parameters at test-time? In this paper, we derive a Bayesian model that provides for a well-defined relationship between unlabeled inputs under distributional shift and model parameters, and show how approximate inference in this model can be instantiated with a simple regularized entropy minimization procedure at test-time. We evaluate our method on a variety of distribution shifts for image classification, including image corruptions, natural distribution shifts, and domain adaptation settings, and show that our method improves both accuracy and uncertainty estimation. ",Training on Test Data with Bayesian Adaptation for Covariate Shift
195,1445420505742315523,87818714,Tiago Pimentel,"['A surprisal–duration trade-off across and within the world’s languages!\nAnalysing 600 languages, we find evidence of this trade-off: cross-linguistically; and within 319 of them.\nWe conclude less surprising phones are produced faster.\n\n#EMNLP2021\n<LINK> <LINK>', 'Natural languages differ considerably from one another. Nonetheless, we may predict human cognition constrains how each of them is implemented. In this paper, we assume that the capacity to process information is roughly constant across human populations.', 'Given this assumption, we expect a surprisal–duration trade-off to arise both across and within languages. This trade-off is connected (cross-linguistically) to the compensation hypothesis and (within languages) to the uniform information density hypothesis.', 'This trade-off may arise for trivial reasons, though, and we control for such potential confounds in our analysis. E.g., marked phones are both longer and less frequent. Further, word-initial positions are longer and more surprising, potentially for uncorrelated reasons.', 'Experimentally, we find that there is a modest but significant trade-off between phone duration and surprisal. We find a significant cross-linguistic trade-off, and significant trade-offs in 319 of the 600 analysed languages. Moreover, no language presents an ""inverse trade-off""! https://t.co/OcnQKEpQ3D', 'Shout-out to these great (and recurring) collaborators :) @clara__meister, @esalesk, Simone Teufel, @blasi_lang and @ryandcotterell!\n\nShout-out to the VoxClamantis dataset as well, which although noisy allows such massive cross-linguistic analysis!\nhttps://t.co/HXJNeQV2CW']",https://arxiv.org/abs/2109.15000,"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages. ",A surprisal--duration trade-off across and within the world's languages
196,1444090911550439424,113264888,Aaswath Raman,"['Do you use adjoint-optimization to inverse design your photonic structures/ devices? You might find our new preprint (led by Chris Yeung in our group) interesting! We show how combining it with explainable ML allows us to dramatically enhance performance: <LINK> <LINK>', 'Adjoint and topology optimization can end up trapped in local minima, limiting the performance of the final design output of the algorithm. We train ML models on the output of the optimization algorithms, explain them using SHAP, and use that to improve designs further. https://t.co/Liln4SDUaX', 'For the canonical example of a Y-splitter, our approach yields designs w/ 75% improvements in FOM relative to  adjoint opt (LumOpt) . Looking ahead, combining optimization with data-driven, explainable ML seems a fruitful path to building truly generalizable inverse design tools.']",https://arxiv.org/abs/2109.14886,"A fundamental challenge in the design of photonic devices, and electromagnetic structures more generally, is the optimization of their overall architecture to achieve a desired response. To this end, topology or shape optimizers based on the adjoint variables method have been widely adopted due to their high computational efficiency and ability to create complex freeform geometries. However, the functional understanding of such freeform structures remains a black box. Moreover, unless a design space of high-performance devices is known in advance, such gradient-based optimizers can get trapped in local minima valleys or saddle points, which limits performance achievable through this inverse design process. To elucidate the relationships between device performance and nanoscale structuring while mitigating the effects of local minima trapping, we present an inverse design framework that combines adjoint optimization, automated machine learning (AutoML), and explainable artificial intelligence (XAI). Integrated with a numerical electromagnetics simulation method, our framework reveals structural contributions towards a figure-of-merit (FOM) of interest. Through an explanation-based reoptimization process, this information is then leveraged to minimize the FOM further than that obtained through adjoint optimization alone, thus overcoming the optimization's local minima. We demonstrate our framework in the context of waveguide design and achieve between 39% and 74% increases in device performance relative to state-of-the-art adjoint optimization-based inverse design across a range of telecom wavelengths. Results of this work therefore highlight machine learning strategies that can substantially extend and enhance the capabilities of a conventional, optimization-based inverse design algorithm while revealing deeper insights into the algorithm's designs. ","Enhancing Adjoint Optimization-based Photonics Inverse Design with
  Explainable Machine Learning"
197,1443957112917434378,3433220662,Anthony Bonato,['New paper on arXiv! Joint work with undergrads as part of the @FieldsInstitute Undergraduate Summer Research Program. We find bounds and exact values for pursuit-evasion parameters on graphs derived from latin squares and MOLS.\n<LINK> <LINK>'],https://arxiv.org/abs/2109.14669,"We investigate various pursuit-evasion parameters on latin square graphs, including the cop number, metric dimension, and localization number. The cop number of latin square graphs is studied, and for $k$-MOLS$(n),$ bounds for the cop number are given. If $n>(k+1)^2,$ then the cop number is shown to be $k+2.$ Lower and upper bounds are provided for the metric dimension and localization number of latin square graphs. The metric dimension of back-circulant latin squares shows that the lower bound is close to tight. Recent results on covers and partial transversals of latin squares provide the upper bound of $n+O\left(\frac{\log{n}}{\log{\log{n}}}\right)$ on the localization number of a latin square graph of order $n.$ ",Pursuit-evasion games on latin square graphs
198,1443817420028792833,907745906610630656,Manel Perucho Pla,"['This contribution we have placed today in arXiv shows the first numerical simulations that study the interaction of a relativistic jet with an inhomogeneous medium, including atomic hydrogen and ionisation physics. Proud of it.\n\n<LINK>', 'A small step towards realistic scenarios that allow us to study the exact role of extragalactic jets on their host galaxies. This is a crucial process in galaxy evolution. https://t.co/CWO5BiacS5']",https://arxiv.org/abs/2109.15234,"In this contribution we present the first numerical simulations of a relativistic outflow propagating through the inner hundreds of parsecs of its host galaxy, including atomic and ionised hydrogen, and the cooling effects of ionisation. Our results are preliminary, but we observe efficient shock ionization of atomic hydrogen in interstellar clouds. The mean density of the interstellar medium in these initial simulations is lower than that expected in typical galaxies, which makes cooling times longer and thus no recombination is observed inside the shocked region. The velocities achieved by the shocked gas in the simulations are in agreement with observational results, although with a wide spectrum of values. ",Jet propagation through inhomogeneous media and shock ionization
199,1443804218817716226,1185964566867595265,Nitish Govindarajan,['In our recent work we study the mechanistic differences in  CO electro-oxidation on Cu and Au electrodes by combining single crystal experiments and microkinetic modeling. @arxiv \n\n<LINK>'],http://arxiv.org/abs/2109.15269,"Understanding CO electro-oxidation is crucial towards designing catalysts for electrochemically oxidizing complex organic molecules. Earth-abundant Copper (Cu) has recently been demonstrated to exhibit high alkaline CO electro-oxidation activity, rivaling the previously acclaimed Gold (Au). Herein, we combine single crystal rotating disc electrode (RDE) experiments and ab initio microkinetic modeling to understand the underlying reaction mechanisms on Cu and Au surfaces. Cu exhibits a facet-dependent activity with Cu(111) having a 0.27 V lower overpotential than Au(111) and a comparable CO oxidation current density. Using Koutecky-Levich analysis and DFT based microkinetic modeling, we identify the rate-limiting pathway to be Langmuir-Hinshelwood on Cu whereas Eley-Rideal on Au. We additionally present strikingly variant RDE responses on four Cu facets (111, 100, 110 and 211) and long-term stability analysis on Cu(111) and Au(111). We find a combined reset-reaction profile helps Cu retain its high activity and pose a strong competition to the expensive Au catalysts. ","Alkaline CO electro-oxidation: Mechanistic Differences between Copper
  and Gold Single Crystals and Peculiarities of various Copper Facets"
200,1443057631380135943,1004105220505456641,Paola Domínguez,"['Today on astro-ph paper by PhD student Salome Mtchedlidze🎉""Evolution of primordial magnetic fields during large-scale structure formation"" (<LINK>). We (@HambObs, @aghatubrid +) study inflationary- and phase-transitional-alike PMFs with the ENZO code <LINK>', 'Rotation measure maps from the simulated cosmic\nweb shown above!', '@astro_magnetism @HambObs @aghatubrid Cool! Feel free to email us with comments/feedback if you want.', ""@amitseta90 no (sub-grid) dynamo action here indeed. Yes I've seen your paper, I'll read it !!""]",https://arxiv.org/abs/2109.13520,"Primordial magnetic fields could explain the large-scale magnetic fields present in the Universe. Inflation and phase transitions in the early Universe could give rise to such fields with unique characteristics. We investigate the magneto-hydrodynamic evolution of these magnetogenesis scenarios with cosmological simulations. We evolve inflation-generated magnetic fields either as (i) uniform (homogeneous) or as (ii) scale-invariant stochastic fields, and phase transition-generated ones either as (iii) helical or as (iv) non-helical fields from the radiation-dominated epoch. We find that the final distribution of magnetic fields in the simulated cosmic web shows a dependence on the initial strength and the topology of the seed field. Thus, the observed field configuration retains information on the initial conditions at the moment of the field generation. If detected, primordial magnetic field observations would open a new window for indirect probes of the early universe. The differences between the competing models are revealed on the scale of galaxy clusters, bridges, as well as filaments and voids. The distinctive spectral evolution of different seed fields produces imprints on the correlation length today. We discuss how the differences between rotation measures from highly ionized regions can potentially be probed with forthcoming surveys. ","Evolution of primordial magnetic fields during large-scale structure
  formation"
201,1442950230169702400,1345856121660178433,Annabelle Bohrdt,"['In our most recent work, impressively led by undergrad Lauritz Hahn, we look at the dynamics of a hole in a finite temperature Ising AFM ⬆️⬇️ and find a change of behavior at a temperature T* 🌡 indicating spin charge deconfinement: <LINK> <LINK>']",https://arxiv.org/abs/2109.09732,"The mechanism underlying charge transport in strongly correlated quantum systems, such as doped antiferromagnetic Mott insulators, remains poorly understood. Here we study the expansion dynamics of an initially localized hole inside a two-dimensional (2D) Ising antiferromagnet at variable temperature. Using a combination of classical Monte Carlo and a truncated basis method, we reveal two dynamically distinct regimes: A spin-charge confined region below a critical temperature $T^*$, characterized by slow spreading, and a spin-charge deconfined region above $T^*$, characterized by an unbounded diffusive expansion. The deconfinement temperature $T^*\approx 0.65 J_z$ we find is around the N\'eel temperature $T_{\rm N} = 0.567 J_z$ of the Ising background in 2D, but we expect $T^* < T_{\rm N}$ in higher dimensions. In both regimes we find that the mobile hole does not thermalize with the Ising spin background on the considered time scales, indicating weak effective coupling of spin- and charge degrees of freedom. Our results can be qualitatively understood by an effective parton model, and can be tested experimentally in state-of-the-art quantum gas microscopes. ","Dynamical signatures of thermal spin-charge deconfinement in the doped
  Ising model"
202,1442518690050854914,1290365889544691712,Deringer Group Oxford,['New preprint! We studied the structure of amorphous phosphorus - and how it changes under pressure - with ML-driven #compchem simulations. Read more at @arxiv: <LINK>'],https://arxiv.org/abs/2109.09794,"Amorphous phosphorus (a-P) has long attracted interest because of its complex atomic structure, and more recently as an anode material for batteries. However, accurately describing and understanding a-P at the atomistic level remains a challenge. Here we show that large-scale molecular-dynamics simulations, enabled by a machine learning (ML)-based interatomic potential for phosphorus, can give new insights into the atomic structure of a-P and how this structure changes under pressure. The structural model so obtained contains abundant five-membered rings, as well as more complex seven- and eight-atom clusters. Changes in the simulated first sharp diffraction peak during compression and decompression indicate a hysteresis in the recovery of medium-range order. An analysis of cluster fragments, large rings, and voids suggests that moderate pressure (up to about 5 GPa) does not break the connectivity of clusters, but higher pressure does. Our work provides a starting point for further computational studies of the structure and properties of a-P, and more generally it exemplifies how ML-driven modeling can accelerate the understanding of disordered functional materials. ","Cluster Fragments in Amorphous Phosphorus and their Evolution under
  Pressure"
203,1441292337456369671,1162541483431301120,James Beattie,['🧲🚨 Paper day 🚨🧲\n\nHere we update the DCF method for measuring B-field strength from the dispersion of dust polarisation angles to include the affects of compressibility in MHD turbulence.  We find significantly better performance using our new method.\n\n<LINK>'],https://arxiv.org/abs/2109.10925,"The magnetic field strength in interstellar clouds can be estimated indirectly by using the spread of dust polarization angles ($\delta \theta$). The method developed by Davis 1951 and by Chandrasekhar and Fermi 1953 (DCF) assumes that incompressible magnetohydrodynamic (MHD) fluctuations induce the observed dispersion of polarization angles, deriving $B\propto 1/\delta \theta$ (or, $\delta \theta \propto M_{A}$, in terms of the Alfv\'{e}nic Mach number). However, observations show that the interstellar medium (ISM) is highly compressible. Recently, Skalidis & Tassis 2021 (ST) relaxed the incompressibility assumption and derived instead $B\propto 1/\sqrt{\delta \theta}$ ($\delta \theta \propto M_{A}^2$). We explored what the correct scaling is in compressible and magnetized turbulence with numerical simulations. We used 26 magnetized, ideal-MHD numerical simulations with different types of forcing. The range of $M_{A}$ and sonic Mach numbers $M_{s}$ explored are $0.1 \leq M_{A} \leq 2.0$ and $0.5 \leq M_{s} \leq 20$. We created synthetic polarization maps and tested the assumptions and accuracy of the two methods. The synthetic data have a remarkable consistency with the $\delta \theta \propto M_{A}^{2}$ scaling, which is inferred by ST, while the DCF scaling fails to follow the data. The ST method shows an accuracy better than $50\%$ over the entire range of $M_{A}$ explored; DCF performs adequately only in the range of $M_{A}$ for which it has been optimized through the use of a ""fudge factor"". For low $M_{A}$, DCF is inaccurate by factors of tens. The assumptions of the ST method reflect better the physical reality in clouds with compressible and magnetized turbulence, and for this reason the method provides a much better estimate of the magnetic field strength over the DCF method. ","Why take the square root? An assessment of interstellar magnetic field
  strength estimation methods"
204,1441085126574698496,990433714948661250,Sergey Levine,"['Offline RL lets us run RL without active online interaction, but tuning hyperparameters, model capacity etc. still requires rollouts, or validation tasks. In our new paper, we propose guidelines for *fully offline* tuning for algorithms like CQL <LINK>\n\nA thread: <LINK>', 'In supervised learning, we have training and validation sets, and this works great for tuning. There is no equivalent in RL, making tuning hard. However, with CQL, when there is too much or too little model capacity, we do get very characteristic estimated vs true Q-value curves https://t.co/s5YE6O7Ger', 'Of course, the true return is unknown during offline training, but we can still use our understanding of the trends of estimated Q-values to provide guidelines for how to adjust model capacity. These guidelines are not guaranteed to work, but seem to work well in practice. https://t.co/TTaI5x2iwP', 'We evaluate these guidelines on a simulated robotic task, and two different real-world robots, and find that it works well across the board, using the same alpha=1.0 CQL parameter and fully offline selection of model capacity, regularization, etc. https://t.co/zeCp1Gdhzt', 'A few things that I think are interesting: (1) we can do capacity/arch/hyperparam tuning *without* full OPE (which is very hard); (2) we can tune fully offline for three very different domains. But this is far from perfect, and more research is needed on better workflows.', 'This paper will be presented at CoRL 2021, with @aviral_kumar2, Anikait Singh, Stephen Tian, @chelseabfinn \n\nhttps://t.co/Yg9YDMrIA0\nhttps://t.co/WqeCOQeGQ5']",https://arxiv.org/abs/2109.10813,"Offline reinforcement learning (RL) enables learning control policies by utilizing only prior experience, without any online interaction. This can allow robots to acquire generalizable skills from large and diverse datasets, without any costly or unsafe online data collection. Despite recent algorithmic advances in offline RL, applying these methods to real-world problems has proven challenging. Although offline RL methods can learn from prior data, there is no clear and well-understood process for making various design choices, from model architecture to algorithm hyperparameters, without actually evaluating the learned policies online. In this paper, our aim is to develop a practical workflow for using offline RL analogous to the relatively well-understood workflows for supervised learning problems. To this end, we devise a set of metrics and conditions that can be tracked over the course of offline training, and can inform the practitioner about how the algorithm and model architecture should be adjusted to improve final performance. Our workflow is derived from a conceptual understanding of the behavior of conservative offline RL algorithms and cross-validation in supervised learning. We demonstrate the efficacy of this workflow in producing effective policies without any online tuning, both in several simulated robotic learning scenarios and for three tasks on two distinct real robots, focusing on learning manipulation skills with raw image observations with sparse binary rewards. Explanatory video and additional results can be found at sites.google.com/view/offline-rl-workflow ",A Workflow for Offline Model-Free Robotic Reinforcement Learning
205,1441068497031741442,790033937531703296,Yi Tay,"['New paper alert! 😀 ""Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers""\n\nWe study scaling laws of Transformers pertaining to both upstream &amp; downstream transfer by pretraining over 200+ T5 models.\n\nPaper: <LINK>\n@GoogleAI @DeepMind <LINK>', 'What we found:\n1) Scaling Laws differ in upstream/downstream. While upstream pre-training performance measured by perplexity scales with model size quite\nindependently from the model shape, the downstream performance does not. Be cautious about over indexing on perplexity scores! https://t.co/uRYjHzX99D', '2) Scaling protocols can differ in diff compute regions! A scaling strategy may work at a smaller compute region but not at large-scale (or vice versa). So iterating on scaling strategy at a smaller size and scaling up later may be a challenging endeavor.', '3) We analyze the pareto-frontier of the compute-performance trade-off of over 200+ pretrained models. We find that not all knobs are created equal. Some influence the pareto-frontier greatly while some not so much. Depth is easily one of the biggest influencers. https://t.co/F7FYcWVuLm', '4) By analyzing the pareto-frontier, we find that existing canonical configs (base/large etc) of T5 are slightly pareto-inefficient and one could achieve a much more efficient model with better performance.', '5) From our analysis we propose a scaling protocol which we call ""DeepNarrow"". We show that this protocol applies to all sizes (small-&gt;XXL). At base, we arrive at a model with 50% less parameters and 40% faster with better downstream performance. https://t.co/4pJItFrPkI', '6) To further test the generality and robustness of these findings, we also conduct experiments on the vision domain (ViT) and also 12 other diverse NLP tasks to make it a total of almost 30 NLP tasks. https://t.co/VVlH4TT4Gu', '7) Finally, we will release and open source all 100+ pretrained checkpoints to the community. This is slated for early Q4 2021. Release link in the paper PDF itself.', 'Thanks to all amazing collaborators at @GoogleAI and @DeepMind , @m__dehghani (co-first author), @Jeffy_Sailing @LiamFedus @samiraabnar @hwchung27 @sharan0909 @DaniYogatama @ashVaswani @metzlerd']",http://arxiv.org/abs/2109.10686,"There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis. ","Scale Efficiently: Insights from Pre-training and Fine-tuning
  Transformers"
206,1440907821894156292,2691042313,David Howard,"[""Good Vibrations or We Jammin'? \nEver wondered which song provides the highest grip strength?  Check out our latest publication on vibration-enhanced granular jamming soft grippers and find out! Collab with UQ.\n\n<LINK>\n\n#softrobotics #robotics @CSIRORobotics <LINK>""]",https://arxiv.org/abs/2109.10496,"Granular jamming is a popular soft robotics technology that has seen recent widespread applications including industrial gripping, surgical robotics and haptics. However, to date the field has not fully exploited the fundamental science of the jamming phase transition, which has been rigorously studied in the field of statistical and condensed matter physics. This work introduces vibration as a means to improve the properties of granular jamming grippers through vibratory fluidisation and the exploitation of resonant modes within the granular material. We show that vibration in soft jamming grippers can improve holding strength, reduce the downwards force needed for the gripping action, and lead to a simplified setup where the second air pump, generally used for unjamming, could be removed. In a series of studies, we show that frequency and amplitude of the waveforms are key determinants to performance, and that jamming performance is also dependent on temporal properties of the induced waveform. We hope to encourage further study in transitioning fundamental jamming mechanisms into a soft robotics context to improve performance and increase diversity of applications for granular jamming grippers. ",Vibration Improves Performance in Granular Jamming Grippers
207,1440763422124085264,941178306744872970,Behrooz Ghorbani,"['Scaling Laws for NMT\n\n<LINK>\n\nWe study the model scaling behavior of encoder-decoder Transformers in NMT. We show that scaling the encoder yields different loss dynamics compared to scaling the decoder. We provide scaling laws that capture this behavior (1/5) <LINK>', 'It turns out that the scaling behavior is heavily influenced by the test set composition! As model size increases, loss consistently improves on test sets with natural target sentences. In contrast, improvements quickly saturate on test sets with translationese targets. (2/5) https://t.co/0wrc2xKIU2', 'Interestingly, we observe that scaling DOES NOT always improve the generation quality of the model output: On test sets with translationese targets, scaling beyond a certain point hurts BLEU scores even though test loss keeps improving with model size! (3/5) https://t.co/E80yvehpy6', 'Finally, we observe that severe contamination of the training data with (forward or backward) machine translated content can completely derail the scaling behavior. Careful what you build on: the data foundation matters a lot for scaling up NMT models: (4/5) https://t.co/ER56WbQTBa', 'More details in our paper! Joint work with @orf_bnw, @markuseful, @ankurbpn, @xgarcia238, Maxim Krikun, @ciprian_chelba, and @ColinCherry. (5/5)']",http://arxiv.org/abs/2109.07740,"We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study. ",Scaling Laws for Neural Machine Translation
208,1440733242659799042,36262769,Laurent Lessard,"['New preprint! <LINK>\n\nWe study the trade-off between worst-case convergence rate and robustness to additive gradient noise for first-order methods, and present novel near-Pareto-optimal algorithm designs.\n\nInformal summary here:\n<LINK>']",https://arxiv.org/abs/2109.05059,"We study the trade-off between convergence rate and sensitivity to stochastic additive gradient noise for first-order optimization methods. Ordinary Gradient Descent (GD) can be made fast-and-sensitive or slow-and-robust by increasing or decreasing the stepsize, respectively. However, it is not clear how such a trade-off can be navigated when working with accelerated methods such as Polyak's Heavy Ball (HB) or Nesterov's Fast Gradient (FG) methods, or whether any of these methods can achieve an optimal trade-off. We consider three classes of functions: (1) strongly convex quadratics, (2) smooth strongly convex functions, and (3) nonconvex functions that satisfy a weak notion of strong convexity. For each function class, we present a tractable way to compute convergence rate and sensitivity to additive gradient noise for a broad family of first-order methods, and we present near-Pareto-optimal algorithm designs. Each design consists of a simple analytic update rule with two states of memory, similar to HB and FG. Moreover, each design has a scalar tuning parameter that explicitly trades off convergence rate and sensitivity to additive gradient noise. When tuned as aggressively as possible, our proposed algorithms recover the algorithms with fastest-known convergence rates for each function class. When tuned to be more robust, our algorithms are novel and provide a practical way to control noise sensitivity while maintaining the fastest possible convergence rate. We validate the performance and near-optimality of our designs through numerous numerical simulations. ","The Speed-Robustness Trade-Off for First-Order Methods with Additive
  Gradient Noise"
209,1440707394162421764,3025082120,Shayne Longpre,"['📢📜#NLPaperAlert 🌟Knowledge Conflicts in QA🌟- what happens when facts learned in training contradict facts given at inference time? 🤔\n \nHow can we mitigate hallucination + improve OOD generalization? 📈\n \nFind out in our #EMNLP2021 paper! [1/n]\n \n<LINK> <LINK>', 'Knowledge conflicts occur when a model encounters contextual knowledge that conflicts with it’s parametric knowledge. \n\nTo test this, we create “substitute” QA examples, where the answer to the “original” example has been replaced in the context passage. [2/n]', 'When we create substitute instances on Natural Questions, a T5 model hallucinates the original answer ~20% of the time, despite the given (substitute) context providing another answer!\n\n(This is bad! Why? Static models cannot generalize to new/temporal information!) [3/n] https://t.co/NZo9NhYtg2', 'What factors exacerbate this hallucination / memorization phenomena? \n\nModel size (↑size →  ↑hallucination)\nRetriever quality in training (↑quality →↓hallucination )\nPopularity of answer (↑popularity →↓hallucination )\n\n[4/n]', 'As log(model size) increases, models generate more parametric (or “memorized”) answers -- even for questions not seen during fine-tuning. [5/n] https://t.co/WXsgkxTsNT', 'We also show there is a strong inverse correlation between the quality of the retriever at training (Recall@K) and the model’s tendency to generate memorized answers (MR, or % predicting “Original” answer). [6/n] https://t.co/UBOPS7RhOI', 'We show that simply augmenting the training set with modified examples from our framework minimizes over-reliance on memorized facts, reducing hallucination, and improving OOD generalization 4-7%! [7/n] https://t.co/IPUxS3OIsh', 'We release our repo that constructs knowledge conflict examples for QA.\n\nThis framework can be used to interpret model behaviour and mitigate hallucination (as we’ve done here). [8/n]\n\nhttps://t.co/QIDH97zEJj https://t.co/v4yxds6cen', '@kartikperisetla, @_anthonychen, @nikutopian, @chrisdubois, @sameer_ and I are super proud to release this work! \n\nPlease don’t hesitate to follow up with questions/comments, and see you all at EMNLP! [n/n]', 'Lastly, thank you to @lao_ni, @LeBrave2016, Russ Webb, @adamjfisch, @nlpmattg, Dheeru Dua, and Sanjay Subramanian for their guidance and support! [n+1/n]', '@WilliamWangNLP @sameer_ Would love to hear more about it! Shoot us an email any time']",https://arxiv.org/abs/2109.05052,"Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4%-7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts. ",Entity-Based Knowledge Conflicts in Question Answering
210,1440487863796207624,359216663,Nicolás Guarín-Zapata,"[""I'm happy to share our latest preprint. We propose a qualitative method to analyze directionality of periodic materials.\n\n<LINK>\n\nCC @Gomez_Juan_dita"", 'All the figures are available with a #CreativeCommons license here:\n\nhttps://t.co/Ed8hhZ0MZO']",https://arxiv.org/abs/2109.09893,We present a method to visualize the directionality of periodic materials. This method takes as input dispersion (hyper-) surfaces obtained using a Bloch analysis and outputs a curve/surface with the bulk directionality encoded on it. Although the examples shown are in the context of elastodynamics the method can be used without need of consideration for the underlying physical phenomena. ,Analysis of the directionality on periodic materials
211,1440384033255407625,3050272306,John Hewitt,"[""How do I 'probe' a representation for just the aspects of a property (like part-of-speech) that aren't captured by a baseline (like word identity?) In #emnlp2021 paper, we propose conditional probing, which does this!\n\npaper: <LINK>\nblog: <LINK>"", 'An intuition of probing is that if a neural representation (like BERT) makes a property (like part-of-speech) better+more easily predictable than do baselines (like word identity), then ""useful work"" was performed by the neural net.', 'So what if BERT only captured the ambiguous POS cases? It would explain _less_ about POS than does the word identity. Probing would indicate this with a negative result.\n\nBut really, in this case BERT captures an important part of POS! So, how do we measure it? https://t.co/PFJbNmXe1G', 'Our conditional probing method does this, and it\'s as simple as training probe 1 on the baseline, and probe 2 on the concatenation of baseline and representation! Intuitively, now we measure just what ""extra"" the representation provides, not what it lacks. https://t.co/BEWuqqML9y', ""Further, we show that conditional probing actually can estimate the conditional _usable_ information from the representation to the property, after conditioning on the baseline!   I(BERT-&gt;POS | word identity) So it's not just an intuitive tool; it has a clear interpretation."", 'We also propose usable (𝒱-information) as a basis for probing in general. The probe family (𝒱) is a hypothesis for the structure of the mapping from representation to property. V-info can be constructed by ""useful work"" by BERT that makes property more predictable using 𝒱.', ""In my blog post, I argue that probing is a clear tool to characterize knowledge in neural networks when we didn't tell the network how to represent that knowledge. https://t.co/EQifl9OcEa\nThe code should be very useful for probing studies! https://t.co/ChDvFmRK0f"", 'This work with fellow Stanford NLP PhD @ethayarajh, and my advisors @percyliang and @chrmanning; my thanks to them for enduring a long and winding process with this paper, e.g., reformulating 𝒱-information to capture conditional information quantities (see Appendix!)', ""@tpimentelms @ryandcotterell Hi Tiago! Still chewing on it :) -- maybe these frameworks are complementary? (Though I realize you show Bayesian MI converges to V-info, the frameworks seem to capture distinct intuitions?) That's what we thought about the great MDL framework of @lena_voita and @iatitov!"", ""@tpimentelms @ryandcotterell @lena_voita @iatitov Nice. Sample efficiency of learning the mapping from repr to property is an interesting thing that V-info definitely doesn't capture. V-info studies the structure of the mapping alone; even with infinite data, the family is still a constraint/hypothesis -- so seems complementary!""]",https://arxiv.org/abs/2109.09234,"Probing experiments investigate the extent to which neural representations make properties -- like part-of-speech -- predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we're interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called $\mathcal{V}$-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought. ",Conditional probing: measuring usable information beyond a baseline
212,1440128876466917376,2852215580,Ferdinando Fioretto,"['New preprint 👉 <LINK>\n\nWe show that differentially private voting may introduce disparate impacts in semi-supervised #ML settings.\nWe isolate and analyze key model and data properties responsible for exacerbating unfairness and propose a mitigation method. <LINK>', 'This work builds on the results of a growing and exciting research area at the intersection between privacy, fairness, and learning! https://t.co/uLapvcdo8G']",https://arxiv.org/abs/2109.08630,"The Private Aggregation of Teacher Ensembles (PATE) is an important private machine learning framework. It combines multiple learning models used as teachers for a student model that learns to predict an output chosen by noisy voting among the teachers. The resulting model satisfies differential privacy and has been shown effective in learning high-quality private models in semisupervised settings or when one wishes to protect the data labels. This paper asks whether this privacy-preserving framework introduces or exacerbates bias and unfairness and shows that PATE can introduce accuracy disparity among individuals and groups of individuals. The paper analyzes which algorithmic and data properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings. ",A Fairness Analysis on Private Aggregation of Teacher Ensembles
213,1440009709197545476,112156645,Giulia Guidi,"['Last fall, @rogarcia_sanz and I studied a representative workload (k-mer counting) to evaluate the fitness of #CALM —consistency and logical monotonicity— programming in the #cloud for computational science, and we just uploaded a preprint: <LINK> 1/n <LINK>', 'First, we compared a Bud (https://t.co/MCt4Xo6Hiu) implementation with a UPC++ one and found that both are equally expressive —although a detailed performance comparison remains a future work, Bud provides enough low-level control to assume similar scaling behavior as UPC++ 2/n', 'Bud’s low-level characteristics complicate distributed programming by offloading performance and correctness considerations to the user —Our case study motivated the design of #BuDDI, a more declarative #Bloom language that provides Distributed Data Independence 3/n', 'Our declarative abstractions rely on #CRDTs and Iterators to ensure acceptable consistency and performance with the many benefits of declarative logic programming 4/n', 'Our hope is that the design of #BuDDI will motivate our readers to work with us to implement the compiler and runtime 5/n', ""A few useful resources (since there's not much background in our preprint at the moment): \n(a) https://t.co/iVVPj4TtMO \n(b) https://t.co/VIg172Tutd \n(c) https://t.co/sVH97eRrtY\n(d) https://t.co/liFOnySwiF""]",https://arxiv.org/abs/2109.08192,"Coordination protocols help programmers of distributed systems reason about the effects of transactions on the state of the system, but they're not cheap. Coordination protocols may involve multiple rounds of communication, which can hurt system responsiveness. There exist many efforts in distributed computing for managing the coordination-performance trade-off. More recent is a line of work that characterizes the class of workloads for which coordination is not necessary for consistency: namely, logically monotonic programs. In this paper, we present a case study of logical monotonicity in workloads typical to computational biology. We leverage the Bloom language to write efficient distributed programs, and compare their performance to equivalent programs written in UPC++, a popular language for writing distributed programs. Additionally, we leverage Bloom's analysis tools to identify points-of-coordination, and use our own experience using Bloom to recommend some higher-level abstractions for users without strong distributed computing backgrounds. ",BuDDI: A Declarative Bloom Language for CALM Programming
214,1439985963040526336,97939183,Yuandong Tian,"['We introduce CompilerGym, a fast &amp; robust gym-like environment that enables simple integration of existing  ML/RL techniques for compiler optimization (i.e., find customized compiler flags to make program smaller / run faster). Many RL baselines included. <LINK>']",https://arxiv.org/abs/2109.08267,"Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field. We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API. We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27x more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers. In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains. ","CompilerGym: Robust, Performant Compiler Optimization Environments for
  AI Research"
215,1439892667454926852,423024537,Alessandro Rizzo,['Our new projective study on #covid19 progression in the US suggests that we should keep vaccinating at the rate of the first wave and do extensive testing and contact tracing if we want to curb the spreading. Details here <LINK>'],https://arxiv.org/abs/2109.08660,"The potential waning of the vaccination immunity to COVID-19 could pose threats to public health, as it is tenable that the timing of such waning would synchronize with the near-complete restoration of normalcy. Should also testing be relaxed, we might witness a resurgent COVID-19 wave in winter 2021/2022. In response to this risk, an additional vaccine dose, the booster shot, is being administered worldwide. In a projected study with an outlook of six months, we explore the interplay between the rate at which boosters are distributed and the extent to which testing practices are implemented, using a highly granular agent-based model tuned on a medium-sized U.S. town. Theoretical projections indicate that the administration of boosters at the rate at which the vaccine is currently administered could yield a severe resurgence of the pandemic. Projections suggest that the peak levels of mid spring 2021 in the vaccination rate may prevent such a scenario to occur, although exact agreement between observations and projections should not be expected due to continuously evolving nature of the pandemics. Our study highlights the importance of testing, especially to detect asymptomatic individuals in the near future, as the release of the booster reaches full speed. ","Predicting the effects of waning vaccine immunity against COVID-19
  through high-resolution agent-based modeling"
216,1438889075969232898,780828430325600256,Artjoms Šeļa,"['Our preprint is out! \n\nTogether with Petr Plecháč (@versotym) and @AWLassche we try to show that  poetic meters historically tend to maintain distinct semantic ranges. Effect is studied in several 18-20th c. European traditions (cs,ger,rus + en &amp; nl*) \n<LINK>', ""It highlights structural limitations that meters impose on meaning (re)production in poetry. You can't copy something for ~hundreds of years and pretend it's OK 😌 Some overarching forces would kick in. https://t.co/K0G9K5Wzx6"", ""Why does the thing happen? Well, we &amp; those before don't have answers! Meter-meaning association could be driven by intrinsic features (meter=mnemonic device), social environment (canon &amp; teaching), extreme inequality in cultural prestige distribution. Our study is only a start.""]",https://arxiv.org/abs/2109.07148,"Recent advances in cultural analytics and large-scale computational studies of art, literature and film often show that long-term change in the features of artistic works happens gradually. These findings suggest that conservative forces that shape creative domains might be underestimated. To this end, we provide the first large-scale formal evidence of the persistent association between poetic meter and semantics in 18-19th European literatures, using Czech, German and Russian collections with additional data from English poetry and early modern Dutch songs. Our study traces this association through a series of clustering experiments using the abstracted semantic features of 150,000 poems. With the aid of topic modeling we infer semantic features for individual poems. Texts were also lexically simplified across collections to increase generalizability and decrease the sparseness of word frequency distributions. Topics alone enable recognition of the meters in each observed language, as may be seen from highly robust clustering of same-meter samples (median Adjusted Rand Index between 0.48 and 1). In addition, this study shows that the strength of the association between form and meaning tends to decrease over time. This may reflect a shift in aesthetic conventions between the 18th and 19th centuries as individual innovation was increasingly favored in literature. Despite this decline, it remains possible to recognize semantics of the meters from past or future, which suggests the continuity of semantic traditions while also revealing the historical variability of conditions across languages. This paper argues that distinct metrical forms, which are often copied in a language over centuries, also maintain long-term semantic inertia in poetry. Our findings, thus, highlight the role of the formal features of cultural items in influencing the pace and shape of cultural evolution. ","Semantics of European poetry is shaped by conservative forces: The
  relationship between poetic meter and meaning in accentual-syllabic verse"
217,1438813159553384452,356645746,Blas Kolic,"['Happy to share our fresh #preprint, where we study how to estimate the initial conditions of chaotic dynamical systems with incomplete information.\n\nCheck it out :)\n\nHuge thanks to @juan_sabuco and #DoyneFarmer for a great collaboration!\n<LINK>']",https://arxiv.org/abs/2109.06825,"In this paper we study the problem of inferring the initial conditions of a dynamical system under incomplete information. Studying several model systems, we infer the latent microstates that best reproduce an observed time series when the observations are sparse,noisy and aggregated under a (possibly) nonlinear observation operator. This is done by minimizing the least-squares distance between the observed time series and a model-simulated time series using gradient-based methods. We validate this method for the Lorenz and Mackey-Glass systems by making out-of-sample predictions. Finally, we analyze the predicting power of our method as a function of the number of observations available. We find a critical transition for the Mackey-Glass system, beyond which it can be initialized with arbitrary precision. ","Estimating initial conditions for dynamical systems with incomplete
  information"
218,1438764065984184320,1105488204,Shanika Galaudage,"['Thanks @LIGO + panel of judges — the poster is all about my latest first-author paper!\n\nWe find evidence for two sub-populations for merging binary black holes: ~80% non-spinning, remaining rapidly spinning + nearly aligned! 😎\n\nCheck it out on the arXiv:\n<LINK> <LINK>', 'Work done in collaboration with @ColmMTalbot, Tushar Nagar, Deepnika Jain, @EHThrane &amp; Ilya Mandel 🌟\n\nAnd be sure to check out the poster, you can find it on my webpage: https://t.co/v3hv0ETzze', '@AstroKirsten Thanks KB! ✨', '@TheAstroPhoenix Cheers Soheb!']",https://arxiv.org/abs/2109.02424,"Recent work paints a conflicting portrait of the distribution of black hole spins in merging binaries measured with gravitational waves. Some analyses find that a significant fraction of merging binaries contain at least one black hole with a spin tilt $>90^\circ$ with respect to the orbital angular momentum vector, which has been interpreted as a signature for dynamical assembly. Other analyses find the data are consistent with a bimodal population in which some binaries contain black holes with negligible spin while the rest contain black holes with spin vectors preferentially aligned with the orbital angular momentum vector. In this work, we scrutinize models for the distribution of black hole spins to pinpoint possible failure modes in which the model yields a faulty conclusion. We reanalyze data from the second LIGO--Virgo gravitational-wave transient catalog (GWTC-2) using a revised spin model, which allows for a sub-population of black holes with negligible spins. In agreement with recent results by Roulet et al., we show that the GWTC-2 detections are consistent with two distinct sub-populations. We estimate that $29-75\%$ (90\% credible interval) of merging binaries contain black holes with negligible spin $\chi \approx 0$. The remaining binaries are part of a second sub-population in which the spin vectors are preferentially (but not exactly) aligned to the orbital angular momentum. The black holes in this second sub-population are characterized by spins of $\chi\sim0.45$. We suggest that the inferred spin distribution is consistent with the hypothesis that all merging binaries form via the field formation scenario. ","Building better spin models for merging binary black holes: Evidence for
  non-spinning and rapidly spinning nearly aligned sub-populations"
219,1438586147073052675,1319101874532978690,jason wei,"['New paper to appear in #emnlp2021! <LINK>\n\nWe study the syntactic abilities of BERT by manipulating the training corpus and retraining BERT.', 'Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules.', 'We investigate this question using BERT’s performance on English subject-verb agreement (SVA) as a case study. https://t.co/prIIvyA4l1', 'We train multiple instances of BERT from scratch that allow us to perform a series of controlled interventions at pre-training time.', 'BERT often generalizes well to subject-verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. https://t.co/3sH1i26xio', 'We also find, however, that performance is heavily influenced by word frequency. BERT performs better on verbs that occur more often. https://t.co/DRYGRduxFM', 'When one verb form occurs more often during training than the other (e.g., if “run” occurs more often than “runs”), BERT tends to predict the more frequent word form. https://t.co/npflW1ubYS', 'Closer analysis via probing reveals that much of the observed error rate can be attributed to errors in predicting the agreement feature of the subject or verb (singular/plural), as opposed to not following SVA when the subject and verb have been correctly identified. https://t.co/iKLRkgVFyE', 'Hence, BERT follows the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) for infrequent lexical items”\n\nThis finding provides insight into the conditions needed for LMs to exhibit rule-learning behavior.', 'Thanks @Brown_NLP @tallinzen @dhgarrette for hosting me for this AI residency project!']",https://arxiv.org/abs/2109.07020,"Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject-verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject-verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items. ",Frequency Effects on Syntactic Rule Learning in Transformers
220,1438307278571065350,1347114826267504641,Sarah Betti,['Astronomy paper #3 now on arxiv!-- A study of how stars form in molecular clouds and how we can see them with radio/millimeter telescopes. 🌠⭐️💥\n<LINK>'],https://arxiv.org/abs/2109.06916,"We use hydrodynamical simulations of star-forming gas with stellar feedback and sink particles (proxies for young stellar objects, i.e., YSOs) to produce and analyze synthetic 1.1mm continuum observations at different distances (150 - 1000pc) and ages (0.49 - 1.27 Myr). We characterize how the inferred core properties, including mass, size, and clustering with respect to diffuse natal gas structure, change with distance, cloud evolution, and the presence of YSOs. We find that atmospheric filtering and core segmentation treatments have distance-dependent impacts on the resulting core properties for d < 300pc and 500pc, respectively, which dominate over evolutionary differences. Concentrating on synthetic observations at further distances (650-1000pc), we find a growing separation between the inferred sizes and masses of cores with and without YSOs in the simulations, which is not seen in recent observations of the Mon R2 cloud at 860pc. We find that the synthetic cores cluster in smaller groups, and their mass densities are correlated with gas column density over a much narrower range, than the Mon R2 observations. Such differences limit applicability of the evolutionary predictions we report here and motivate future efforts to adapt our synthetic observation and analysis framework to next generation simulations such as STARFORGE. These predictions and systematic characterizations will help guide analysis of cores for the upcoming TolTEC Clouds to Cores Legacy Survey on the Large Millimeter Telescope Alfonso Serrano (LMT). ","Robustness of Synthetic Observations in Producing Observed Core
  Properties: Predictions for the TolTEC Clouds to Cores Legacy Survey"
221,1438215447783030786,346719335,Keyon Vafa,"['New paper: <LINK>\n\nConsider a sequence generated by a language model. Which words were most important for generating each word?\n\nWe propose greedy rationalization: greedily finding the smallest subset of words that would make the same prediction as the full text. <LINK>', 'Consider a sequence generated by GPT-2: ""The court struck down the law because it was unconstitutional""\n\nWhich words were most important for predicting ""unconstitutional""?\n\nThe greedy algorithm starts with an empty set and adds words until ""unconstitutional"" is the top prediction https://t.co/a9NXHFTjuR', ""How do we evaluate sequential rationales? \n\nThere are some datasets with annotated rationales for classification, but these don't extend to sequence models.\n\nSo we collected our own sequential rationale dataset based on Lambada. https://t.co/liPVCfNoLM"", 'Paper: https://t.co/8CjhtqB1j8\nGithub: https://t.co/4NzpZ255bg\nDemo: https://t.co/C9dmp255VV\n\nWith: @yuntiandeng, @blei_lab, @srush_nlp']",https://arxiv.org/abs/2109.06387,"Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the combinatorial objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales. ",Rationales for Sequential Predictions
222,1438058237128056840,627820771,Peter T Gallagher,"[""Our latest paper by @laura_hayess @Oscar_S_OHara  @drsophiemurray <LINK>. We use X-ray data from the GOES satellite and very low frequency (VLF) data from Birr, Ireland - Maine, USA to study the impacts of solar flares on Earth's ionosphere. 1/4 <LINK>"", ""From a sample of 334 flares, we find a close relationship between X-ray flares and ionospheric disturbances. The Earth's atmosphere is a huge solar solar X-ray detector! But can we calibrate and use it? Maybe ... 2/4 https://t.co/cIHtDK3cc6"", 'Left: The location of flares on the solar disk, the colour and size represent GOES X-ray magnitude. Right: The position of a flare on disk does not affect the impact on the ionosphere ... 3/4 https://t.co/TJiwdfQe0g', 'Also investigated the impact on the ionosphere of hard X-rays (25-50 keV) detected by the RHESSI spacecraft. Can see that ionospheric response peaks with hard X-rays ... one for further study.  4/4 https://t.co/zjoStxgGAC', '@ryanomilligan Ye, HXR/UV/EUV from flare ribbons have similar time profiles. HXR penetrate deeper into ionosphere, so would think that VLF reflected by lower layers. Defo need to model response of ionosphere to a time-varying UV/EUV/X-ray spectrum.']",https://arxiv.org/abs/2109.06558,"Solar flares significantly impact the conditions of the Earth's ionosphere. In particular, the sudden increase in X-ray flux during a flare penetrates down to the lowest-lying D-region and dominates ionization at these altitudes (60-100 km). Measurements of very low frequency (VLF: 3-30kHz) radio waves that reflect at D-region altitudes provide a unique remote-sensing probe to investigate the D-region response to solar flare emissions. Here, using a combination of VLF amplitude measurements at 24kHz together with X-ray observations from the Geostationary Operational Environment Satellite (GOES) X-ray sensor, we present a large-scale statistical study of 334 solar flare events and their impacts on the D-region over the past solar cycle. Focusing on both GOES broadband X-ray channels, we investigate how the flare peak fluxes and position on the solar disk dictate an ionospheric response and extend this to investigate the characteristic time delay between incident X-ray flux and the D-region response. We show that the VLF amplitude linearly correlates with both the 1-8 A and 0.5-4 A channels, with correlation coefficients of 0.80 and 0.79, respectively. Unlike higher altitude ionospheric regions for which the location of the flare on the solar disk affects the ionospheric response, we find that the D-region response to solar flares does not depend on the flare location. By comparing the time delays between the peak X-ray fluxes in both GOES channels and VLF amplitudes, we find that there is an important difference between the D-region response and the X-ray spectral band. We also demonstrate for several flare events that show a negative time delay, the peak VLF amplitude matches with the impulsive 25-50 keV hard X-ray fluxes measured by the Ramaty High Energy Solar Spectroscopic Imager (RHESSI). ",Solar Flare Effects on the Earth's Lower Ionosphere
223,1438035955630366721,1213098396765937664,Gemma De las Cuevas,['How can Gala be decomposed into spheres? How can polynomials be decomposed while preserving invariance and positivity? With Andreas Klingler and Tim Netzer (@AlgebraUibk) we’ve studied the latter question <LINK> <LINK>'],https://arxiv.org/abs/2109.06680,"We present a framework to decompose real multivariate polynomials while preserving invariance and positivity. This framework has been recently introduced for tensor decompositions, in particular for quantum many-body systems. Here we transfer results about decomposition structures, invariance under permutations of variables, positivity, rank inequalities and separations, approximations, and undecidability to real polynomials. Specifically, we define invariant decompositions of polynomials and characterize which polynomials admit such decompositions. We then include positivity: We define invariant separable and sum-of-squares decompositions, and characterize the polynomials similarly. We provide inequalities and separations between the ranks of the decompositions, and show that the separations are not robust with respect to approximations. For cyclically invariant decompositions, we show that it is undecidable whether the polynomial is nonnegative or sum-of-squares for all system sizes. Our work sheds new light on polynomials by putting them on an equal footing with tensors, and opens the door to extending this framework to other tensor product structures. ","Polynomial decompositions with invariance and positivity inspired by
  tensors"
224,1437975500341714949,1658162341,Narayanan Rengaswamy,"['First paper out of postdoc @azengineering out! See <LINK>. We propose a QEC based GHZ distillation protocol, inspired by the Bell pair distillation of @markwilde in <LINK>. Byproduct: new method to generate logical Pauli operators for codes. <LINK>', 'Joint work with @rainarocks @nithinitzme and Prof. Bane Vasić. Implementation available online: https://t.co/oyd2cEZs9i', 'Besides the main protocol, we also discuss variations that might be more practical for certain network topologies. We work out a detailed example with a 3 qubit code to show the subtleties in the steps of the protocol. This example should help develop protocol variations.', 'The key step to the protocol is a new property of GHZ states that forms our main result in Theorem 6. It considers stabilizer measurements on one subsystem and shows the equivalent code on the remaining two subsystems. It builds on an ""extended"" transpose trick from Bell pairs.']",https://arxiv.org/abs/2109.06248,"Entanglement distillation is a well-studied problem in quantum information, where one typically starts with $n$ noisy Bell pairs and distills $k$ Bell pairs of higher fidelity. While distilling Bell pairs is the canonical setting, it is important to study the distillation of multipartite entangled states because these can be useful for realizing distributed algorithms on quantum networks. In this paper, we study the distillation of GHZ states using quantum error correcting codes (QECCs). Using the stabilizer formalism, we begin by explaining the QECC-based Bell pair distillation protocol in arXiv:0708.3699, which relies particularly on the transpose symmetry between Alice's and Bob's qubits in Bell states. Extending this idea, we show that, given $n$ GHZ states, performing a matrix on Alice's qubits is equivalent to performing a ""stretched"" version of the transpose of the matrix on the qubits of Bob and Charlie. We call this mapping to the stretched version of the matrix the GHZ-map, and show that it is an algebra homomorphism. Using this property, we show that Alice projecting her qubits onto an $[[n,k]]$ stabilizer code implies the simultaneous projection of Bob's and Charlie's qubits onto an induced $[[2n,k]]$ stabilizer code. Guided by this insight, we develop a GHZ distillation protocol based on local operations and classical communication that uses any stabilizer code. Inspired by stabilizer measurements on GHZ states, we also develop a new algorithm to generate logical Pauli operators of any stabilizer code and use it in the protocol. Since quantum codes with finite rate and almost linear minimum distance have recently been discovered, this paper paves the way for high-rate high-output-fidelity GHZ distillation. We provide simulation results on the $5$-qubit perfect code to emphasize the importance of the placement of a certain local Clifford operation in the protocol. ",Distilling GHZ States using Stabilizer Codes
225,1437655382994956292,795620317532143616,Jonas Pfeiffer,"['In our paper\n\nxGQA: Cross-Lingual Visual Question Answering\n\nWe propose a new multilingual multimodal benchmark, covering 7 new typologically diverse languages\n📃 <LINK>\n🌐 <LINK>\n@GregorGeigle @ashkamath20 @jmsteitz @stefanroth @licwu @IGurevych <LINK>', 'We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and—vice versa—multilingual models to become multimodal. https://t.co/omuxSK3E17', 'While our adapter-based architecture outperforms the SotA  M3P model in cross-lingual zero-shot scenarios, the overall transfer performance remains low across the board, with an average drop of around 38 accuracy points across target languages. https://t.co/EIjLKsQ01e', 'This demonstrates the inherent difficulty of the task, even though the corresponding questions are ar- guably simple, containing only 8.5 words on average.', 'In few-shot scenarios we find that utilizing an increasing amount of data instances in the target language consistently improves accuracy, culminating in an improvement of up to 20 accuracy points when specializing the model with only 48 images in the target language. https://t.co/PL68ttBlgT', 'Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, which can only be partially recovered through few-shot learning, calling for more sophisticated methods for vision and multilingual language modeling.', '@srchvrs I agree that it’s surprising that it works well for text-only tasks—which has been throughly investigated in the past (i.e. https://t.co/gDLr8gFZeK @PDufter)—however, what surprised me even more, was this doesn’t seem to translate to scenarios where we add an additional modality.']",https://arxiv.org/abs/2109.06082,"Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and -- vice versa -- multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. ",xGQA: Cross-Lingual Visual Question Answering
226,1437535379532386310,1409589671194025984,Jeremy R Cole,"['Happy to announce our paper, Graph-Based Decoding for Task-Oriented Semantic Parsing, was accepted to Findings of EMNLP 2021! We propose an adaptation of edge-factored models for the TOP dataset. \n\n<LINK>\n\n1/4 <LINK>', 'We evaluate the proposed model standard setting, finding it competitive, and also evaluate when training sizes are limited, including proposing a setting where partially annotated examples are available.\n\n2/4 https://t.co/5nXuB5Ew8z', 'In these settings, the proposed model is able to outperform even strong baselines like T5, though T5 is able to use partially annotated examples surprisingly well.\n\n3/4 https://t.co/fjsvayRczp', 'This was joint work with @mumblamb, @IcePasupat, @LuhengH, and @ptshaw2. Check it out!\n\n4/4']",https://arxiv.org/abs/2109.04587,"The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available. ",Graph-Based Decoding for Task Oriented Semantic Parsing
227,1437393852227276801,3378033143,Katerina Margatina,"['💥Our #EMNLP2021 paper is now on Arxiv! We propose a new acquisition function for active learning that leverages both uncertainty and diversity sampling by acquiring ⚡️contrastive⚡️ examples.⬇️\n\n💻code: <LINK>\n📝pre-print: <LINK>\n\n(1/n)', 'We hypothesize that data points that are close in the model feature space but the model produces different predictive likelihoods, should be good candidates for data acquisition. We define such examples as contrastive. (2/n) https://t.co/WOtJaclHyS', 'Our method, Contrastive Active Learning (CAL), selects unlabeled data points from the pool, whose predictive likelihoods diverge the most from their neighbors in the training set. (3/n) https://t.co/I4j2H7iCJO', 'This way, CAL shares similarities with diversity sampling, but instead of performing clustering it uses the feature space to create neighborhoods. CAL also leverages uncertainty, by using predictive likelihoods to rank the unlabeled data. (4/n)', 'We empirically show that CAL performs consistently better or equal compared to all baseline acquisition functions, in 7 datasets from 4 NLP tasks, when evaluated on in-domain and out-of-domain settings. (5/n) https://t.co/55Me01ODA9', 'We finally conduct a thorough analysis of our method showing that CAL achieves a better trade-off between diversity and uncertainty compared to the other acquisition functions. (6/n) https://t.co/QQ2gcFfsig', 'Feel free to check our paper for more details🔍! I would like to thank my co-authors again for their incredible help &amp; support on this work! @gvernikos @LoicBarrault @nikaletras 😊\n \n(7/7) n=7']",https://arxiv.org/abs/2109.03764,"Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies. ",Active Learning by Acquiring Contrastive Examples
228,1436455367693590529,3250001664,Colin Hill,"['Using the latest DR4 data from @ACT_Pol, we have put constraints on a candidate model for increasing the value of H0 inferred from CMB data -- the early dark energy (EDE) scenario: <LINK>  Intriguingly, we find hints of a signal, with an according increase in H0', 'However, @Planck does not prefer a non-zero EDE signal, and indeed the Planck data are sufficiently constraining that even the joint analysis of Planck + ACT yields no evidence for EDE, and very little change in H0', 'Understanding whether the differences in ACT-driven and Planck-driven EDE constraints are physical in nature, due to systematics, or simply a rare statistical fluctuation is of high priority', 'The moderate EDE preference in ACT is driven by the several lowest multipole bins in our EE power spectrum, in particular from ACT\'s ""wide patch"".  So it is very localized, which gives us some pause, as systematics often have this property', 'In joint fits, the ACT-driven EDE preference arises from the EE data mentioned above, as well as the ACT TE data, which shows a coherent residual w.r.t. LCDM that the EDE model is able to absorb', 'ACTors are hard at work -- and indeed put a huge amount of work into the DR4 analysis (https://t.co/HkSdGw6ofh / https://t.co/YoRUTreSGQ) -- to ensure robustness against systematic effects', 'Near-future CMB data will have sufficient (statistical) constraining power to distinguish between the best-fit LCDM and EDE models here, and will strongly constrain other candidate H0-increasing models as well.  Stay tuned!', ""Many thanks to my @ACT_Pol collaborators, especially @ermcalabrese!  And thanks as well to everyone for their interest in our work today (@DScol, @cosmosgalli, @MBKplus, and many others I'm forgetting)!"", 'Also, if you are interested in learning more, I discussed these results in a colloquium @AspenPhysics yesterday, which is online here: https://t.co/g5LPTpsWpO']",https://arxiv.org/abs/2109.04451,"The early dark energy (EDE) scenario aims to increase the value of the Hubble constant ($H_0$) inferred from cosmic microwave background (CMB) data over that found in $\Lambda$CDM, via the introduction of a new form of energy density in the early universe. The EDE component briefly accelerates cosmic expansion just prior to recombination, which reduces the physical size of the sound horizon imprinted in the CMB. Previous work has found that non-zero EDE is not preferred by Planck CMB power spectrum data alone, which yield a 95% confidence level (CL) upper limit $f_{\rm EDE} < 0.087$ on the maximal fractional contribution of the EDE field to the cosmic energy budget. In this paper, we fit the EDE model to CMB data from the Atacama Cosmology Telescope (ACT) Data Release 4. We find that a combination of ACT, large-scale Planck TT (similar to WMAP), Planck CMB lensing, and BAO data prefers the existence of EDE at $>99.7$% CL: $f_{\rm EDE} = 0.091^{+0.020}_{-0.036}$, with $H_0 = 70.9^{+1.0}_{-2.0}$ km/s/Mpc (both 68% CL). From a model-selection standpoint, we find that EDE is favored over $\Lambda$CDM by these data at roughly $3\sigma$ significance. In contrast, a joint analysis of the full Planck and ACT data yields no evidence for EDE, as previously found for Planck alone. We show that the preference for EDE in ACT alone is driven by its TE and EE power spectrum data. The tight constraint on EDE from Planck alone is driven by its high-$\ell$ TT power spectrum data. Understanding whether these differing constraints are physical in nature, due to systematics, or simply a rare statistical fluctuation is of high priority. The best-fit EDE models to ACT and Planck exhibit coherent differences across a wide range of multipoles in TE and EE, indicating that a powerful test of this scenario is anticipated with near-future data from ACT and other ground-based experiments. ","The Atacama Cosmology Telescope: Constraints on Pre-Recombination Early
  Dark Energy"
229,1436211773229600775,775630411863130112,Sven Buder,"['How much did accreted stars contribute to the build-up/shape of the Milky Way? We all found many accreted stars via their distinct orbits thanks to @ESAGaia. In our latest study (<LINK>), we identify/“tag” accreted stars via their chemistry with @galahsurvey. 1/5 <LINK>', 'Galactic archaeology with accreted stars made immense progress thanks to work by P. E. Nissen, @amina_helmi, @BelokurovVasily, @drpayeldas, @AGalactichawk, H. Koppelman, D. Feuillet, @Rohan_Naidu, @anabonaca, G. C. Myeong, and many more (see paper for more refs)! Exciting! 2/5', 'Countless new techniques have been developed to identify accreted stars. We try to give an overview in our Table A1 and hope it will be useful for anyone interesting in joining our search for accreted stars! 3/5 https://t.co/rrjyxrLft6', 'Thanks to the combination of @GaiaESA and @galahsurvey DR3 (https://t.co/okDy7tBfr2), we can identify stars through chemistry via their distinct [Na/Fe] vs. [Mg/Mn] abundances (see also Das+2020) and compare with dynamical selections via L_Z vs. sqrt{J_R} (Feuillet+2020). 4/5 https://t.co/tP3tI6NJbm', 'This allows us to study the dynamical properties (via chemical selected stars) and chemical properties (via dynamically selected stars). For everyone who wants to follow up this study, we provide the code on https://t.co/Syrb44GwaN (GALAH+ DR3 data is public!) 5/5 https://t.co/MP3TkQCrS4']",https://arxiv.org/abs/2109.04059,"Since the advent of $Gaia$ astrometry, it is possible to identify massive accreted systems within the Galaxy through their unique dynamical signatures. One such system, $Gaia$-Sausage-Enceladus (GSE), appears to be an early ""building block"" given its virial mass $> 10^{10}\,\mathrm{M_\odot}$ at infall ($z\sim1-3$). In order to separate the progenitor population from the background stars, we investigate its chemical properties with up to 30 element abundances from the GALAH+ Survey Data Release 3 (DR3). To inform our choice of elements for purely chemically selecting accreted stars, we analyse 4164 stars with low-$\alpha$ abundances and halo kinematics. These are most different to the Milky Way stars for abundances of Mg, Si, Na, Al, Mn, Fe, Ni, and Cu. Based on the significance of abundance differences and detection rates, we apply Gaussian mixture models to various element abundance combinations. We find the most populated and least contaminated component, which we confirm to represent GSE, contains 1049 stars selected via [Na/Fe] vs. [Mg/Mn] in GALAH+ DR3. We provide tables of our selections and report the chrono-chemodynamical properties (age, chemistry, and dynamics). Through a previously reported clean dynamical selection of GSE stars, including $30 < \sqrt{J_R~/~\mathrm{kpc\,km\,s^{-1}}} < 55$, we can characterise an unprecedented 24 abundances of this structure with GALAH+ DR3. Our chemical selection allows us to prevent circular reasoning and characterise the dynamical properties of the GSE, for example mean $\sqrt{J_R~/~\mathrm{kpc\,km\,s^{-1}}} = 26_{-14}^{+9}$. We find only $(29\pm1)\%$ of the GSE stars within the clean dynamical selection region. Our methodology will improve future studies of accreted structures and their importance for the formation of the Milky Way. ","The GALAH Survey: Chemical tagging and chrono-chemodynamics of accreted
  halo stars with GALAH+ DR3 and $Gaia$ eDR3"
230,1436210551739215872,4807828837,Vishal Upendran,"['☀️ New paper with @dktripathi accepted in ApJ, and out on arxiv !☀️\n<LINK>\n\nIn this work, we study the properties of the C II 1334 line which forms in the upper chromosphere, as a function of photospheric magnetic flux density (|B|) in Cor hole &amp; Quiet Sun (1/5)', 'We find CHs to show reduced intensity, excess blueshifts (only blueshifted pixels), excess redshifts (only redshifted pixels), and excess width over QS for similar |B|. These properties are also found to increase with |B| for both CH and QS. (2/5)', 'We also find that the spectral profiles are flatter, and more skewed than a Gaussian, with a dependence on |B|. However, the ""flatness"" and ""skewness"" are consistent between CH and QS. Why are these important? (3/5)', ""This analysis tells us that the fundamental processes giving rise to spectral profiles are similar in both CH and QS. The difference seems to arise mainly due to the magnetic topology, and that's why gross chromospheric features look similar in both CH and QS. (4/5)"", 'Hope you enjoy reading this work. However, this is just a part of the work  - the remaining contains some interesting implications of these observations. A glimpse on what is yet to come may be known to those who checked out my poster at the IIA 50 conference. Stay tuned! (5/5)', '@ydnad0 @dktripathi Thank you @ydnad0 !']",https://arxiv.org/abs/2109.04287,"Coronal Holes (CHs) have subdued intensity and net blueshifts when compared to Quiet Sun (QS) at coronal temperatures. At transition region temperatures, such differences are obtained for regions with identical photospheric absolute magnetic flux density ($\vert$B$\vert$). In this work, we use spectroscopic measurements of the \car 1334~{\AA} line from Interface Region Imaging Spectrograph (IRIS), formed at chromospheric temperatures, to investigate the intensity, Doppler shift, line width, skew, and excess kurtosis variations with $\vert$B$\vert$. We find the intensity, Doppler shift, and line widths to increase with $\vert$B$\vert$ for CHs and QS. The CHs show deficit in intensity and excess total widths over QS for regions with identical $\vert$B$\vert$. For pixels with only upflows, CHs show excess upflows over QS, while for pixels with only downflows, CHs show excess downflows over QS that cease to exist at $\vert$B$\vert$ $\le$ 40. Finally, the spectral profiles are found to be more skewed and flatter than a Gaussian, with no difference between CH and QS. These results are important in understanding the heating of the atmosphere in CH and QS, including solar wind formation, and provide further constraints on the modeling of the solar atmosphere. ","Properties of the C II 1334 {\AA} line in Coronal Hole and Quiet Sun as
  observed by IRIS"
231,1436137135011155975,1133260452388122625,Mohannad Alhanahnah,"['Our paper is on the arxiv. We propose LMCAS, a lightweight approach for debloating programs. This work has been accepted for technology transfer. Please feel free to send me feedback and questions.  \n<LINK>']",https://arxiv.org/abs/2109.02775,"Program debloating aims to enhance the performance and reduce the attack surface of bloated applications. Several techniques have been recently proposed to specialize programs. These approaches are either based on unsound strategies or demanding techniques, leading to unsafe results or a high overhead debloating process. In this paper, we address these limitations by applying partial-evaluation principles to generate specialized applications. Our approach relies on a simple observation that an application typically consists of configuration logic, followed by the main logic of the program. The configuration logic specifies what functionality in the main logic should be executed. LMCAS performs partial interpretation to capture a precise program state of the configuration logic based on the supplied inputs. LMCAS then applies partial-evaluation optimizations to generate a specialized program by propagating the constants in the captured partial state, eliminating unwanted code, and preserving the desired functionalities. Our evaluation of LMCAS on commonly used benchmarks and real-world applications shows that it successfully removes unwanted features while preserving the functionality and robustness of the deblated programs, runs faster than prior tools, and reduces the attack surface of specialized programs. LMCAS runs 1500x, 4.6x, and 1.2x faster than the state-of-the-art debloating tools CHISEL, RAZOR, and OCCAM, respectively; achieves 25% reduction in the binary size; reduces the attack surface of code-reuse attacks by removing 51.7% of the total gadgets and eliminating 83% of known CVE vulnerabilities ","Lightweight, Multi-Stage, Compiler-Assisted Application Specialization"
232,1435880620216012803,1237350319891374081,Dimitris Papoulias,['Paper day! <LINK>\nWe used data from @COHERENT_NUS to calculate the neutrino floor. We also studied the impact of subdominant electroweak/nuclear uncertainties and BSM models!\nSpecial thanks to my awesome collaborators: @ntinaValentina @LuisJFloresS D. Aristizabal <LINK>'],https://arxiv.org/abs/2109.03247,"We reconsider the discovery limit of multi-ton direct detection dark matter experiments in the light of recent measurements of the coherent elastic neutrino-nucleus scattering process. Assuming the cross section to be a parameter entirely determined by data, rather than using its Standard Model prediction, we use the COHERENT CsI and LAr data sets to determine WIMP discovery limits. Being based on a data-driven approach, the results are thus free from theoretical assumptions and fall within the WIMP mass regions where XENONnT and DARWIN have best expected sensitivities. We further determine the impact of subleading nuclear form factor and weak mixing angle uncertainties effects on WIMP discovery limits. We point out that these effects, albeit small, should be taken into account. Moreover, to quantify the impact of new physics effects in the neutrino background, we revisit WIMP discovery limits assuming light vector and scalar mediators as well as neutrino magnetic moments/transitions. We stress that the presence of new interactions in the neutrino sector, in general, tend to worsen the WIMP discovery limit. ","Impact of COHERENT measurements, cross section uncertainties and new
  interactions on the neutrino floor"
233,1435513485178384387,1166800207427883009,Pierpaolo Vivo,"['🚨 Newest on the arXiv 🚨 We use Random Matrix Theory to study instabilities of complex fluids with many constituents (liquid-liquid phase separation), with an application to membraneless organelles in the cytoplasm. <LINK> @KCLDisSyst @kclmathematics @CANES_CDT']",https://arxiv.org/abs/2109.03164,"We develop a theory for thermodynamic instabilities of complex fluids composed of many interacting chemical species organised in families. This model includes partially structured and partially random interactions and can be solved exactly using tools from random matrix theory. The model exhibits three kinds of fluid instabilities: one in which the species form a condensate with a local density that depends on their family (family condensation); one in which species demix in two phases depending on their family (family demixing); and one in which species demix in a random manner irrespective of their family (random demixing). We determine the critical spinodal density of the three types of instabilities and find that the critical spinodal density is finite for both family condensation and family demixing, while for random demixing the critical spinodal density grows as the square root of the number of species. We use the developed framework to describe phase-separation instability of the cytoplasm induced by a change in pH. ","Instabilities of complex fluids with partially structured and partially
  random interactions"
234,1435350231437045762,1387514922,Irwan Bello,"['Wondering how simple 3D-ResNets perform on video recognition given all the recent architecture craze?\n\nIn Revisiting 3D ResNets for Video Recognition, we study the impact of improved training and scaling methods on 3D ResNets.\n\n<LINK> <LINK>', 'We present revised 3D-ResNet baselines, termed 3D-ResNet-RS, and show that they are on par with more recent works. https://t.co/Fa1nSl9loe', 'This follows a series of works revisiting “older” architectures and showing that they can be competitive with recent alternatives, when scaled and trained properly.\n\nObject Detection:\nhttps://t.co/VrSbG9RuvM\n\nImage classification:\nhttps://t.co/K7hMTtS0eR', 'Work led by @Phyyysalis in collaboration with @yeqing133, @YinCui1, @RuiQian3 and Jing Li.']",https://arxiv.org/abs/2109.01696,"A recent work from Bello shows that training and scaling strategies may be more significant than model architectures for visual recognition. This short note studies effective training and scaling strategies for video recognition models. We propose a simple scaling strategy for 3D ResNets, in combination with improved training strategies and minor architectural changes. The resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0 on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated in a self-supervised setup using contrastive learning, demonstrating improved performance. Code is available at: this https URL ",Revisiting 3D ResNets for Video Recognition
235,1435191716277981185,1112312168270061569,Atsuki Yamaguchi,['Our #EMNLP paper is now on arXiv!\n-----\nWe propose five simple pretraining objectives and test their effectiveness on GLUE and SQuAD benchmarks using a BERT architecture.\n\npaper: <LINK>\ncode: <LINK>\n\nCc: @soon1otis @katemargatina @nikaletras'],https://arxiv.org/abs/2109.01819,"Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE's parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective. ","Frustratingly Simple Pretraining Alternatives to Masked Language
  Modeling"
236,1435157833989902338,1305853736850534400,Paweł Swoboda,"['Is it the fastest multicut (a.k.a. correlation clustering) solver? We propose a primal-dual highly parallel solver running on GPU that outperforms CPU counterparts regarding runtime and computes comparable results, see <LINK>']",https://arxiv.org/abs/2109.01838,"We propose a highly parallel primal-dual algorithm for the multicut (a.k.a. correlation clustering) problem, a classical graph clustering problem widely used in machine learning and computer vision. Our algorithm consists of three steps executed recursively: (1) Finding conflicted cycles that correspond to violated inequalities of the underlying multicut relaxation, (2) Performing message passing between the edges and cycles to optimize the Lagrange relaxation coming from the found violated cycles producing reduced costs and (3) Contracting edges with high reduced costs through matrix-matrix multiplications. Our algorithm produces primal solutions and lower bounds that estimate the distance to optimum. We implement our algorithm on GPUs and show resulting one to two orders-of-magnitudes improvements in execution speed without sacrificing solution quality compared to traditional sequential algorithms that run on CPUs. We can solve very large scale benchmark problems with up to $\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. Our code is available at this https URL ",RAMA: A Rapid Multicut Algorithm on GPU
237,1435152181817188355,2917879835,Claudia Cicone,"['When we first looked at the data, fresh out of the archive, we almost could not believe it. We suspected this quasar hosted extended CO-emitting gas beyond the ISM, but we did not expect to find such a giant molecular gas halo: <LINK> @almaobs @ESO_Chile @EsoARC', 'The molecular halo, detected in CO(3-2), extends out to a radius of ~200 kpc from the quasar position, the largest molecular CGM ever imaged. We do not know how it originated, but we think powerful AGN-driven outflows may have helped seeding metal enriched gas out to large scales', 'Understanding this kind of structures requires a new, large sub-mm single dish such as AtLAST, and new high-res cosmological simulations capable of tracing cold molecular gas on large scales (both are goals of @atlast_design)', 'This work is part of the SUPER project https://t.co/3COkwBVjil, a huge collaborative effort led by @VincenzoMainie2 @ESO with the help of @DARSHANKAKKAD Chiara Circosta and many more amazing collaborators']",https://arxiv.org/abs/2109.02269,"We present the discovery of copious molecular gas in the halo of cid_346, a $z=2.2$ quasar studied as part of the SINFONI survey for Unveiling the Physics and Effect of Radiative feedback (SUPER). New Atacama Compact Array (ACA) CO(3-2) observations detect a much higher flux (by a factor of $14\pm5$) than measured on kiloparsec scales ($r\lesssim8$ kpc) using previous snapshot Atacama Large Millimeter/submillimeter Array data. Such additional CO(3-2) emission traces a structure that extends out to $r\sim200$ kpc in projected size, as inferred through direct imaging and confirmed by an analysis of the uv visibilities. This is the most extended molecular circumgalactic medium (CGM) reservoir that has ever been mapped. It shows complex kinematics, with an overall broad line profile (FWHM $= 1000$ km/s) that is skewed towards redshifted velocities up to at least $v\sim1000$ km/s. Using the optically thin assumption, we estimate a strict lower limit for the total molecular CGM mass observed by ACA of $M_{mol}^{CGM}>10^{10}~M_{\odot}$. There is however room for up to $M^{CGM}_{mol}\sim 1.7\times 10^{12}$ $M_{\odot}$, once optically thick CO emission with $\alpha_{\rm CO}=3.6$ $\rm M_{\odot}~(K~km~s^{-1}~pc^2)^{-1}$ and $L^{\prime}_{CO(3-2)}/L^{\prime}_{CO(1-0)}=0.5$ are assumed. Since cid_346 hosts quasar-driven ionised outflows and since there is no evidence of merging companions or an overdensity, we suggest that outflows may have played a crucial rule in seeding metal-enriched, dense gas on halo scales. However, the origin of such an extended molecular CGM remains unclear. ",SUPER. VI. A giant molecular halo around a z~2 quasar
238,1435131668965339137,829359351450185728,Tero Heikkilä,"['Diodes are key elements in electronics. Typically they are based on semiconductors, which cease to function at the low temperatures relevant for example in #quantum technology. We @SupertedP propose and demonstrate a diode based on #superconductors. <LINK>']",https://arxiv.org/abs/2109.01061,"Diodes are key elements for electronics, optics, and detection. The search for a material combination providing the best performances for the required application is continuously ongoing. Here, we present a superconducting spintronic tunnel diode based on the strong spin filtering and splitting generated by an EuS thin film between a superconducting Al and a normal metal Cu layer. The Cu/EuS/Al tunnel junction achieves a large rectification (up to $\sim40$\%) already for a small voltage bias ($\sim 200$ $\mu$V) thanks to the small energy scale of the system: the Al superconducting gap. With the help of an analytical theoretical model we can link the maximum rectification to the spin polarization of the barrier and describe the quasi-ideal Schottky-diode behavior of the junction. This cryogenic spintronic rectifier is promising for the application in highly-sensitive radiation detection for which two different configurations are evaluated. In addition, the superconducting diode may pave the way for future low-dissipation and fast superconducting electronics. ",Rectification in a Eu-chalcogenide-based superconducting diode
239,1435062722912133122,1055973579467059200,Rajsekhar Mohapatra,['We have a new paper out on arxiv today! We have studied velocity structure functions in high-res simulations of the multiphase intracluster medium in galaxy halos. \nCheck it out at:  <LINK>\nSimulation movies at: <LINK>.'],https://arxiv.org/abs/2109.01771,"The central regions of cool-core galaxy clusters harbour multiphase gas, with gas temperatures ranging from $10$ $\mathrm{K}$--$10^7$$\mathrm{K}$. Feedback from active galactic nuclei (AGNs) jets prevents the gas from undergoing a catastrophic cooling flow. However, the exact mechanism of this feedback energy input is unknown, mainly due to the lack of velocity measurements of the hot phase gas. However, recent observations have measured the velocity structure functions ($\mathrm{VSF}$s) of the cooler molecular ($\sim10$$\mathrm{K}$) and H$\alpha$ filaments ($\sim10^4$$\mathrm{K}$) and used them to indirectly estimate the motions of the hot phase. In the first part of this study, we conduct high-resolution ($384^3$--$1536^3$ resolution elements) simulations of homogeneous isotropic subsonic turbulence, without radiative cooling. We analyse the second-order velocity structure functions ($\mathrm{VSF}_2$) in these simulations and study the effects of varying spatial resolution, the introduction of magnetic fields, and the effect of projection along the line of sight (LOS) on it. In the second part of the study, we analyse high-resolution ($768^3$ resolution elements) idealised simulations of multiphase turbulence in the intracluster medium (ICM) from Mohapatra et al 2021. We compare the $\mathrm{VSF}_2$ for both the hot ($T\sim10^7$$\mathrm{K}$) and cold ($T\sim10^4$$\mathrm{K}$) phases and find that their amplitude depends on the density contrast between the phases. They have similar scaling with separation, but introducing magnetic fields steepens the $\mathrm{VSF}_2$ of only the cold phase. We also find that projection along the LOS steepens the $\mathrm{VSF}_2$ for the hot phase and mostly flattens it for the cold phase. ","Velocity structure functions in multiphase turbulence: interpreting
  kinematics of H$\alpha$ filaments in cool core clusters"
240,1434894801086910473,1014454910346285057,Daniel Coelho de Castro,"['Our new study demonstrates noisy labels impacts training, eval &amp; selection of ML models. Reannotating large datasets requires lots of expert time (eg in healthcare), so we also show data-driven strategies enable fixing more labels using fewer resources: <LINK>', 'Great collab with Melanie Bernhardt, @RyutaroT92, Anton Schwaighofer, Kerem Tezcan, @mabmonteiro, Shruthi Bannur, @mattlungrenMD, Aditya Nori, @GlockerBen, @alvarezvalle, @ozanoktay__! @MSFTResearch', 'That should be @mel_bernhardt! Somehow Twitter struggled to find you 😀']",https://arxiv.org/abs/2109.00574,"Imperfections in data annotation, known as label noise, are detrimental to the training of machine learning models and have an often-overlooked confounding effect on the assessment of model performance. Nevertheless, employing experts to remove label noise by fully re-annotating large datasets is infeasible in resource-constrained settings, such as healthcare. This work advocates for a data-driven approach to prioritising samples for re-annotation - which we term ""active label cleaning"". We propose to rank instances according to estimated label correctness and labelling difficulty of each sample, and introduce a simulation framework to evaluate relabelling efficacy. Our experiments on natural images and on a new medical imaging benchmark show that cleaning noisy labels mitigates their negative impact on model training, evaluation, and selection. Crucially, the proposed active label cleaning enables correcting labels up to 4 times more effectively than typical random selection in realistic conditions, making better use of experts' valuable time for improving dataset quality. ","Active label cleaning for improved dataset quality under resource
  constraints"
241,1434790083593396225,1134375290581524480,Kai Schmitz,"['New paper on the arXiv <LINK>, together with three new collaborators from Lausanne/Kyiv! We study gauge-field production during inflation, incl. nonlinear Schwinger pair production and backreaction effects, which enables us to compute EM power spectra in k space. <LINK>']",https://arxiv.org/abs/2109.01651,"We study the explosive production of gauge fields during axion inflation in a novel gradient expansion formalism that describes the time evolution of a set of bilinear electromagnetic functions in position space. Based on this formalism, we are able to simultaneously account for two important effects that have thus far been mostly treated in isolation: (i) the backreaction of the produced gauge fields on the evolution of the inflaton field and (ii) the Schwinger pair production of charged particles in the strong gauge-field background. This allows us to show that the suppression of the gauge-field production due to the Schwinger effect can prevent the backreaction in scenarios in which it would otherwise be relevant. Moreover, we point out that the induced current, $\boldsymbol{J} = \sigma \boldsymbol{E}$, also dampens the Bunch-Davies vacuum fluctuations deep inside the Hubble horizon. We describe this suppression by a new parameter $\Delta$ that is related to the time integral over the conductivity $\sigma$ which hence renders the description of the entire system inherently nonlocal in time. Finally, we demonstrate how our formalism can be used to construct highly accurate solutions for the mode functions of the gauge field in Fourier space, which serves as a starting point for a wealth of further phenomenological applications, including the phenomenology of primordial perturbations and baryogenesis. ","Gauge-field production during axion inflation in the gradient expansion
  formalism"
242,1433526764710596608,358306755,Jani Kastikainen,['New paper today with Sanjit Shashi! We studied boundary CFT 1- and 2-point functions in a holographic model with an end-of-the-world brane. For this we used the geodesic approximation that requires taking into account geodesics reflecting off the brane.\n\n<LINK>'],https://arxiv.org/abs/2109.00079,"We compute correlation functions, specifically 1-point and 2-point functions, in holographic boundary conformal field theory (BCFT) using geodesic approximation. The holographic model consists of a massive scalar field coupled to a Karch-Randall brane -- a rigid boundary in the bulk AdS space. Geodesic approximation requires the inclusion of paths reflecting off of this brane, which we show in detail. For the 1-point function, we find agreement between geodesic approximation and the harder $\Delta$-exact calculation, and we give a novel derivation of boundary entropy using the result. For the 2-point function, we find a factorization phase transition and a mysterious set of anomalous boundary-localized BCFT operators. We also discuss some puzzles concerning these operators. ",Structure of Holographic BCFT Correlators from Geodesics
243,1433349541911318530,54812076,Ibrahim Alabdulmohsin,"['In this work (with H. Maennel and @keysers), we study the impact of “reinitialization” on the generalization risk of CNNs. One-sentence summary: you can improve generalization in CNNs when your data is very small using bottom-up reinitialization. (1/5)\n<LINK> <LINK>', 'Why? We show that layerwise reinitialization boosts the margin on the training data without increasing the size of the weights, hence improving margin-based generalization bounds. Also, it settles on flatter local minima and discourages memorization at the upper layers (2/5). https://t.co/fvE4QTV95y', 'We also study the impact of other reinitialization techniques in recent works. In general, they all tend to improve performance but layerwise reinitialization has the biggest (and most statistically significant) effect (3/5). https://t.co/ztR0IwEvjp', 'We analyzed failures using decision trees to predict when one reinitialization method outperforms others by looking into the impact of the choice of the architecture, training data size, number of classes, and others. Layerwise reinit seems to be a safe bet in all cases (4/5) https://t.co/PQNObuV9Ya', 'Please check out our work. Special thanks to our colleagues @GoogleAI, particularly @tolstikhini, @Robert_Baldock, @HanieSedghi, @bneyshabur, @hugo_larochelle, Chiyuan Zhang, and Mike Mozer for their valuable comments (5/5).']",https://arxiv.org/abs/2109.00267,"Recent results suggest that reinitializing a subset of the parameters of a neural network during training can improve generalization, particularly for small training sets. We study the impact of different reinitialization methods in several convolutional architectures across 12 benchmark image classification datasets, analyzing their potential gains and highlighting limitations. We also introduce a new layerwise reinitialization algorithm that outperforms previous methods and suggest explanations of the observed improved generalization. First, we show that layerwise reinitialization increases the margin on the training examples without increasing the norm of the weights, hence leading to an improvement in margin-based generalization bounds for neural networks. Second, we demonstrate that it settles in flatter local minima of the loss surface. Third, it encourages learning general rules and discourages memorization by placing emphasis on the lower layers of the neural network. Our takeaway message is that the accuracy of convolutional neural networks can be improved for small datasets using bottom-up layerwise reinitialization, where the number of reinitialized layers may vary depending on the available compute budget. ","The Impact of Reinitialization on Generalization in Convolutional Neural
  Networks"
244,1433259215557177344,724116390127480833,Subhayan Sahu,['New work shedding light on quantum measurement-induced phases of matter in the presence of long-range interactions. We introduce models which we can study analytically to find interesting and novel entanglement phases! @condensed_the @JQInews  \n\n<LINK>'],https://arxiv.org/abs/2109.00013,"We study the effects of power-law long-range couplings on measurement-induced phases and transitions in tractable large-$N$ models, including a Brownian qubit model and a Brownian SYK model. In one dimension, the long-range coupling is irrelevant for $\alpha>3/2$, with $\alpha$ being the power-law exponent, thus the volume-law and area-law entanglement phases and the phase transition remain intact. For $\alpha<3/2$ the long-range coupling becomes relevant, leading to a nontrivial dynamical exponent at the measurement-induced phase transition. More interestingly, for $\alpha<1$ the entanglement pattern receives a sub-volume correction for both area-law and volume-law phases. The volume-law phase with such a sub-volume correction realizes a novel quantum error correcting code whose code distance scales as $L^{2-2\alpha}$. We further extend the calculation to a quadratic SYK model, where two distinct fractal entangled phases emerge, leading to a complete phase diagram of the long-range free fermion model under monitoring. ","Entanglement Phases in large-N hybrid Brownian circuits with long-range
  couplings"
245,1445031024551862277,2491688827,Dr. Maria Gorlatova,"['We did a benchmarking study of whether holograms appear to stay where placed, in modern markerless smartphone augmented reality. Got lots of data on spatial instability for different user motions and in different environments: <LINK> #AugmentedReality']",https://arxiv.org/abs/2109.14757,"Markerless augmented reality (AR) has the potential to provide engaging experiences and improve outcomes across a wide variety of industries; the overlaying of virtual content, or holograms, onto a view of the real world without the need for predefined markers provides great convenience and flexibility. However, unwanted hologram movement frequently occurs in markerless smartphone AR due to challenging visual conditions or device movement, and resulting error in device pose tracking. We develop a method for measuring hologram positional errors on commercial smartphone markerless AR platforms, implement it as an open-source AR app, HoloMeasure, and use the app to conduct systematic quantitative characterizations of hologram stability across 6 different user actions, 3 different smartphone models, and over 200 different environments. Our study demonstrates significant levels of spatial instability in holograms in all but the simplest settings, and underscores the need for further enhancements to pose tracking algorithms for smartphone-based markerless AR. ","Here To Stay: Measuring Hologram Stability in Markerless Smartphone
  Augmented Reality"
246,1435877684081565700,885070653363298305,Anne Lauscher (she/her),"[""Most debiasing methods adjust all of the models' params. This can be computationally expensive and risks forgetting the already acquired knowledge. For more sustainable debasing, we propose *debiasing adapters*! @MilaNLProc @dwsunima #Findings @gg42554  <LINK> <LINK>""]",https://arxiv.org/abs/2109.03646,"Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -- besides being computationally expensive -- comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that -- due to its modular nature -- ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages. ",Sustainable Modular Debiasing of Language Models
247,1434905466656727046,794303906901880840,Ozan Oktay,"[""Check out our recent study exploring why data quality matters -- its impact on (I) model's predictive performance, (II) model evaluation and (III) selection for deployment purposes.\n\nHere we explore resource-effective solutions to tackle these challenges: <LINK> <LINK> <LINK>""]",https://arxiv.org/abs/2109.00574,"Imperfections in data annotation, known as label noise, are detrimental to the training of machine learning models and have an often-overlooked confounding effect on the assessment of model performance. Nevertheless, employing experts to remove label noise by fully re-annotating large datasets is infeasible in resource-constrained settings, such as healthcare. This work advocates for a data-driven approach to prioritising samples for re-annotation - which we term ""active label cleaning"". We propose to rank instances according to estimated label correctness and labelling difficulty of each sample, and introduce a simulation framework to evaluate relabelling efficacy. Our experiments on natural images and on a new medical imaging benchmark show that cleaning noisy labels mitigates their negative impact on model training, evaluation, and selection. Crucially, the proposed active label cleaning enables correcting labels up to 4 times more effectively than typical random selection in realistic conditions, making better use of experts' valuable time for improving dataset quality. ","Active label cleaning for improved dataset quality under resource
  constraints"
