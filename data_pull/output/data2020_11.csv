,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1337259170114850819,3375538456,Ali Mottaghi,"['Check out our work on medical symptom recognition at #ML4H workshop #NeurIPS2020 tomorrow. We developed a new active learning method for long-tailed multilabel distributions. Joint with Prathusha Sarma, @xamat, @syeung10, and @anithakan at @CuraiHQ.\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2011.06874,"We study the problem of medical symptoms recognition from patient text, for the purposes of gathering pertinent information from the patient (known as history-taking). A typical patient text is often descriptive of the symptoms the patient is experiencing and a single instance of such a text can be ""labeled"" with multiple symptoms. This makes learning a medical symptoms recognizer challenging on account of i) the lack of availability of voluminous annotated data as well as ii) the large unknown universe of multiple symptoms that a single text can map to. Furthermore, patient text is often characterized by a long tail in the data (i.e., some labels/symptoms occur more frequently than others for e.g ""fever"" vs ""hematochezia""). In this paper, we introduce an active learning method that leverages underlying structure of a continually refined, learned latent space to select the most informative examples to label. This enables the selection of the most informative examples that progressively increases the coverage on the universe of symptoms via the learned model, despite the long tail in data distribution. ","Medical symptom recognition from patient text: An active learning
  approach for long-tailed multilabel distributions"
1,1336703845322330113,16077656,Bear Braumoeller,"[""Important new paper: Underspecification (multiple sets of parameter values that fit the data equally well) is a major cause of poor performance in machine-learning models. (They don't mention theory once, but I have to think it'd help in some cases.) <LINK>"", 'Useful summary of the paper and explanation of its importance, w/one commenter calling it a “wrecking ball”: https://t.co/1CPwM2lQUb', '@aaronclauset Useful, thanks! I’ve run into similar problems even in the partial-identification models I’ve been toying with for years.\n\nDoes that mean the paper’s novelty for machine learning is, ah, a bit overstated in the Technology Review piece?', ""@jcanfil Novice answer: Overfitting happens when you try to fit the data too well. Underspecification happens when you have multiple sets of parameter values that fit the data equally well. Doesn't require data shift bc. your algorithm may just select the wrong parameter values.""]",https://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain. ","Underspecification Presents Challenges for Credibility in Modern Machine
  Learning"
2,1335050517316300801,46554460,Pulkit Agrawal,"[""Long term planning for object manipulation is hard! Turns out if you only want to manipulate rigid objects -- its very much possible. Check out our new framework in Anthony's CoRL 2020 paper <LINK> @MIT_CSAIL @corl_conf <LINK>""]",https://arxiv.org/abs/2011.08177,"We present a framework for solving long-horizon planning problems involving manipulation of rigid objects that operates directly from a point-cloud observation, i.e. without prior object models. Our method plans in the space of object subgoals and frees the planner from reasoning about robot-object interaction dynamics by relying on a set of generalizable manipulation primitives. We show that for rigid bodies, this abstraction can be realized using low-level manipulation skills that maintain sticking contact with the object and represent subgoals as 3D transformations. To enable generalization to unseen objects and improve planning performance, we propose a novel way of representing subgoals for rigid-body manipulation and a graph-attention based neural network architecture for processing point-cloud inputs. We experimentally validate these choices using simulated and real-world experiments on the YuMi robot. Results demonstrate that our method can successfully manipulate new objects into target configurations requiring long-term planning. Overall, our framework realizes the best of the worlds of task-and-motion planning (TAMP) and learning-based approaches. Project website: this https URL ","A Long Horizon Planning Framework for Manipulating Rigid Pointcloud
  Objects"
3,1334590008964100096,70874545,Josh Lothringer,"['In the chaos that was the JWST proposal deadline, we dropped a new paper on the arXiv that tries to figure out what to do with all those awesome metal detections in ultra-hot Jupiters: <LINK>', 'If we can measure both refractory/metal *and* volatile elemental abundances, we realized we can tie that to the rocky and icy components that may have built some of these planets, as represented by the rock-to-ice ratio. https://t.co/casPR4SBij', 'The connection between the observable, present-day atmosphere and the bulk composition from formation is likely pretty complex (e.g., sequestration in the interior, nightside condensation, escaping atmospheres, etc.), but we hope this motivates future research in those areas.', ""This also emphasizes that we shouldn't necessarily expect any single element to be representative of the bulk composition/metallicity, especially if we have no clue what the rock-to-ice ratio is. https://t.co/L9JaPNDZmR"", ""Ultra-hot Jupiters are hot enough that we can largely avoid condensation of metals so these awesome planets offer our first chance to do this. For all other planets, from Solar System gas giants to 'normal' hot Jupiters, refractory elements are tied up into condensates."", 'So we try it out with WASP-121b. The low-res short-wavelength transit spectrum indicates absorption by metals (as confirmed at hi-resolution). Combined with H2O at 1.4 microns, we can compare refractory and volatile abundances! https://t.co/eJFoyIotbF', ""Our PETRA retrievals estimate a refractory-to-volatile ratio of about 3x solar (+/- ~3) and a rock-to-ice ratio &gt;2/3. This indicates rocky planetesimals must have made up a significant amount of W-121b's atmospheric enrichment and evidence for planetesimal-driven core accretion. https://t.co/IzAEpxsfmd"", ""The most exciting part for me is that this is just the tip of the iceberg- adding in information from high-resolution observations + future JWST measurements can really help us piece together the story of these planet's formation and migration."", 'It just so happens that WASP-121b is up-to-bat in the ExoCup: https://t.co/mv3jsb8uxL\n\nThis wonderful, spectrally-rich planet is just beginning to tell us how it grew up- WASP-121b has my vote! #ExoCup2020', 'Also, shout-out to @exoZafar, @ExoSing, and all my other co-authors for helping turn this idea into reality rather quickly! Read the paper here: https://t.co/Qnmh84GGMu https://t.co/aiyGMy7Z8U', '@WASPplanets @LaetitiaDelrez Absolutely!']",https://arxiv.org/abs/2011.10626,"A primary goal of exoplanet characterization is to use a planet's current composition to understand how that planet formed. For example, the C/O ratio has long been recognized as carrying important information on the chemistry of volatile species. Refractory elements, like Fe, Mg, and Si, are usually not considered in this conversation because they condense into solids like Fe(s) or MgSiO$_3$ and would be removed from the observable, gaseous atmosphere in exoplanets cooler than about 2000~K. However, planets hotter than about 2000~K, called ultra-hot Jupiters (UHJs), are warm enough to largely avoid the condensation of refractory species. In this paper, we explore the insight that the measurement of refractory abundances can provide into a planet's origins. Through refractory-to-volatile elemental abundance ratios, we can estimate a planet's atmospheric rock-to-ice fraction and constrain planet formation and migration scenarios. We first relate a planet's present-day refractory-to-volatile ratio to its rock-to-ice ratio from formation using various compositional models for the rocky and icy components of the protoplanetary disk. We discuss potential confounding factors like the sequestration of heavy metals in the core and condensation. We then show such a measurement using atmospheric retrievals of the low-resolution UV-IR transmission spectrum of WASP-121b with PETRA, from which we estimate a refractory-to-volatile ratio of 5.0$^{+6.0}_{-2.7}\times$ solar and a rock-to-ice ratio greater than 2/3. This result is consistent with significant atmospheric enrichment by rocky planetismals. Lastly, we discuss the rich future potential for measuring refractory-to-volatile ratios in ultra-hot Jupiters with the arrival of JWST and by combining observations at low- and high-resolution. ","A New Window into Planet Formation and Migration: Refractory-to-Volatile
  Elemental Ratios in Ultra-hot Jupiters"
4,1334476940535795714,1092693586263457792,Greg Yang,"[""What's the right way to think about feature learning in wide neural networks?\n\nI'll talk about my new paper <LINK> next Wednesday at 12 EDT at <LINK>. Sign up here <LINK> Come w/ good questions! Let's make it a lively discussion! <LINK>""]",https://arxiv.org/abs/2011.14522,"As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4. ",Feature Learning in Infinite-Width Neural Networks
5,1334191213306654720,143430352,Rachael Meager,"['Hello! Tamara Broderick, Ryan Giordano and I have a new working paper out!! It\'s called ""An Automatic Finite-Sample Robustness Metric: Can Dropping a Little Data Change Conclusions?"" <LINK> <LINK>', 'Here comes the paper thread!!! Aaaaaah!!!', 'We propose a way to measure the dependence of research findings on the particular realisation of the sample. We find that several results from big papers in empirical micro can be overturned by dropping less than 1% of the data -- or even 1-10 points, even when samples are large.', ""We do find some results are robust, and can simulate totally robust results, so this thing genuinely varies in practice. You might think that the sensitive cases must have outliers or spec problems, but they don't need to, and our headline application has binary outcome data!"", 'So what is this metric? Ok, for any given result (sign of an effect, significance, whatever) we ask if there is a small set of observations in the sample with a large influence on that result, in the sense that removing them would overturn it.', 'This is like asking how bad it could be for your result if a small percentage of the data set was lost, or a small percentage of your population of interest was not in the sample.', 'Such exclusions are pretty common, both because of practical difficulties of perfectly randomly sampling the real world (and humans processing the\xa0data), and b/c the world is always changing across space and time, even if only a little.', ""Typically it's not reasonable to assume these deviations from the ideal thought experiment are random: there's usually a reason you can't perfectly sample people or places, or interpret everyone's data intelligibly, or predict the future!"", ""So we want to know if there's a small number of highly influential points in the sample, capable of overturning our result if dropped. Finding them exactly is possible but usually computationally\xa0prohibitive\xa0 -- you have to cycle through too many combinations most of the time."", 'We develop an approximation to the influence of removing any given set of points. It\'s a Taylor expansion type of thing, but what\'s exciting is YOU CAN *ALWAYS* CHECK IF THIS APPROXIMATION WORKED IN *EVERY* GIVEN SAMPLE! So it\'s not the usual bullshit ""trust my big math"" thing.', 'Our approach identifies these approximate-high-influence points, so you can always remove them, re-run the analysis once, and see if the result changes. Whatever you get is an exact lower bound on the true sensitivity, since at worst we missed out on some higher-influence points.', 'In our applications we almost always achieve the claimed reversal (tho we discuss exceptions in the paper, and it seems like having true parameter values lying near the boundaries of their spaces is a problem even if you transform the parameter).', ""Now of course if you'd like some big math, we do have some big math for you. We formally derive the approximation for Z estimators (like GMM, OLS, IV, MLE) under regularity conditions."", ""We have explicit bounds on the approximation error for OLS and IV - it's small relative to the real change in the result. We show our metric is a semi-norm on the Influence Function, linking it to standard errors and gross error sensitivity, which are different norms on the IF."", 'Why are some analyses so sensitive? It turns out to be linked to the signal to noise ratio, where the signal is the strength\xa0of the empirical result, and the noise is large\xa0when the\xa0influence function is ""big"" in a specific sense.', ""For OLS, the value of the influence function for each data point is just that point's sample regression error times its leverage. One or the other is not enough. You need both at once, on the same point. That's part of why you can't eyeball this thing in the outcome or errors."", 'Wouldn\'t that ""noise"" show up in standard errors? No, because standard errors are divided through by root-N. Big N can make SEs small even when the noise is large. That\'s also why SEs disappear asymptotically, but our metric won\'t. Important as we move into the ""big data"" space.', ""Also, this noise reflects a distributional shape component that SEs don't, but that is NOT just about outliers: we show that this sensitivity to 1% of the sample can arise even in perfectly specified OLS inference on Gaussian data,\xa0and it also arises in practice on binary data."", ""This links up to something we were discussing on twitter earlier this year: what's intuitively wrong with running an OLS linear reg when the X-Y data scatter is basically vertical? Well, many things, but one of them is that the signal to noise ratio is *probably*\xa0quite low."", 'The fact that this sensitivity can arise even when nothing is formally ""wrong"" with the classical inference can feel weird, because we are used to thinking of our SEs and performance metrics like bias, test size, etc as capturing all the uncertainty we have -- but they don\'t!', ""classical procedures are only designed to capture one type of uncertainty: the variation in an estimator's finite sample value across perfectly random resamplings of the exact same population."", ""But this hypothetical\xa0perfect resampling experiment doesn't really capture all the relevant uncertainty about results in social science. We're not physicists or crop yield analysts, so we shouldn't expect\xa0their statistical tools to be suitable for us."", 'We need to ask about data-dependence in ways that make more sense given how we actually generate and use empirical results!', ""*Bayesian whisper* also wouldn't you rather know about the dependence of a research result on the sample you HAVE rather than the dependence you could imagine having in some hypothetical thought\xa0experiment based on a resampling exercise you could never do? You would, come on!!"", ""But let me be super clear: my own bayesian papers don't escape this problem and you should check out the paper if you want to see me dunk on myself for several pages. (My hunch is that I have been using overly weak priors.)"", ""We think you should report our metric. We definitely don't think you should abandon results that are not robust, but it should prompt a desire to understand the\xa0estimation procedure more deeply, and promote caution in generalizing them too broadly."", 'We wrote you an R package to compute and report it automatically! It all uses Python at the moment, so you need to have that installed. Future versions will be able to do OLS and IV without Python\xa0though!\xa0https://t.co/4c9GbZstUf', 'We really hope this paper can be part of a broader conversation about empirical social science that leads us all to try out new ways of interrogating our data and understanding our conclusions a lot more deeply.', 'Don;t think of this new metric as ""yet another thing you have to report (sigh)"" but as yet another tool to illuminate the way in which your procedure is constructing information about the population from your sample. And what could be more important than that!? Nothing!', 'FIN!!!!', '@socio_steve Alas not indeed!!! But now you know it! progress!!', ""@afranks53 Yah that's never ok in my view, although this paper is more general than just about outliers, and sensitivity here can happen even if there's no bad behaviour like that."", '@jiafengchen42 CORRECT :) !!', ""@Dunkin_Donuts_2 You can think of it that way, yes! And we think that's one of its big useful interpretastions. But you can also think of it as measuring a result's exposure to small changes in the population, across space or time."", ""@ryancbriggs You would ideally want that, but if the exact lower bound is such that you can fully reverse the paper's result by removing 1% of the sample we think that is already informative enough, and that seems to happen quite a bit."", '@Dunkin_Donuts_2 Thank you! :)', ""@jiafengchen42 Yes absolutely I think there is a LOT to unpack here with the empirical IF! Such a cool object, really strange that they don't teach it!"", ""@sacksdaniel I'm surprised if people feel ok with compliers being less than 1% of the sample! If that's the case then harder to argue we care about this group AND harder to argue that we should scale SEs on the LATE by root N!"", '@sacksdaniel Yeah that is quite interesting, i guess we will see if the needle moves on this...', '@jiafengchen42 I should have paid more attention in 385 when i audited it :(', '@vientsek shut up witold 😂', '@ben_golub @S_Stantcheva lmaoooo', '@modrak_m Conceptually there is a relationship but LOO is only going to find the highest influence subset if it is 1 point. Leave K out is typically done by randomly dropping and cycling through subsets, whereas we are searching for and learning about the highest influence subset of data.', '@dballaelliott You can! We have an example in our repo!! https://t.co/lKaSN7KMI4', ""@tjmahr lean in tj it's all good we're gonna learn stuff"", ""@mattkahn1966 We haven't investigated that at all, and we'd be extremely excited if someone did start digging into it!!"", '@Corey_Yanofsky yuuuuuuup', '@stern_tomer hopefully never lmaooooo', ""@jonpcohen Yaaay i'm so glad you like it!!! :)) Yes, it's very related to Alwyn's work, I think we cited him like four times lol."", ""@modrak_m Seems like usually yes from what I've seen, and from how our metric is constructed (just adding the individual bits up, shockingly it works, god loves the taylor expansion....)"", ""@everspinning close, but we can find you who the group is exactly! well, approximately, and then you can check exactly what that group's influence is. :)"", ""@peterbergman_ @Baladevan I sincerely hope there is and that we will see a bunch of work on this coming soon -- we didn't have any good ideas on it (and we are very tired) tell your student to work on it!!"", '@everspinning yes correct!!! :)', ""@geomblog @ben_golub breakdown points in general are about what happens when any observation's value is changed arbitrarily. we're just asking what happens if the observation gets dropped, rather than changed (and we are looking for the worst case drop set)."", '@Woody_WongE thank you for this gif, I will cherish it', '@dbergstresser THANK YOU that means a lot!!', ""@fburlig @BerkouwerS Currently we do not have a recommendation on that, we had no ideas (and we are tired), but we hope that other people will (or us in the future!). BUT here's what I will say very VERY loosely: quantile estimators basically can't be this sensitive unless something DID go wrong"", ""@fburlig @BerkouwerS basically for a median TE sign to flip removing like 10 points or half a percent or something, there's got to be a discontinuity in the underlying distribution, for which you shouldn't really be using quantile inference anyway."", ""@fburlig @BerkouwerS formally i can't show this because quantile estimators are not smooth functions of anything which is what we need for the theory to work"", ""@fburlig @BerkouwerS but if i personally wanted to be sure I wasn't vulnerable to this, i'd do simple median comparisons and just look at the data points in the 2 groups around the medians, and then you can start to roughly bound the damage from removal."", ""@NicDuquette I don't know! you will have to ask Ryan who does all the python things, I try to stay away from it because it scares me lol"", '@ZacGross we try to find the most influential set, so adversely chosen!', '@evanjfields prior dependence can ""crowd out"" data dependence -- as i make the priors stronger, I eat into the data\'s influence on my result. so i would expect less data dependence in general when priors are stronger, but that is just my hunch!', ""@Apoorva__Lal oooo i didnt know about this, i LOVE roger koenker's work though ---  thank you for the link!!!"", '@Apoorva__Lal LMAO', '@Chris_Auld @geomblog @ben_golub it is related to a thing called a higher order infinitesimal jackknife yes but to say exactly how in a tweet is beyond my capabilities https://t.co/XpTAW6ipn2', '@camaradewill 💖💖💖💖💖💖💖💖 https://t.co/qe7sdCmcRf', '@camaradewill the endorsement of the transmasc they/thems is the only demographic i care about on this earth but shhhh', ""@otis_reid i haven't checked specifically for this but i do think it's true that if your intervention actually only affects some tiny fraction of the population, you will tend to be vulnerable to this sensitivity (but you can also be vulnerable to it outside of that case too)"", '@camaradewill even pokemon gifs cannot express the depth of my gratitude here omg', '@daniel_egan thank you!! :)']",https://arxiv.org/abs/2011.14999,"We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the ""Approximate Maximum Influence Perturbation,"" is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1% of the sample. ","An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little
  Data Make a Big Difference?"
6,1333883811562549248,1240603790,Victor Fragoso 💻,['Curious to learn how to use 4-pt congruence constraints to estimate the pose and scale of a generalized camera? Checkout our new gP4Pc solver presented at #3dv2020 \n\n👉 Paper: <LINK>\n👉 Code: <LINK> <LINK>'],https://arxiv.org/abs/2011.13817,"We present gP4Pc, a new method for computing the absolute pose of a generalized camera with unknown internal scale from four corresponding 3D point-and-ray pairs. Unlike most pose-and-scale methods, gP4Pc is based on constraints arising from the congruence of shapes defined by two sets of four points related by an unknown similarity transformation. By choosing a novel parametrization for the problem, we derive a system of four quadratic equations in four scalar variables. The variables represent the distances of 3D points along the rays from the camera centers. After solving this system via Groebner basis-based automatic polynomial solvers, we compute the similarity transformation using an efficient 3D point-point alignment method. We also propose a specialized variant of our solver for the case of coplanar points, which is computationally very efficient and about 3x faster than the fastest existing solver. Our experiments on real and synthetic datasets, demonstrate that gP4Pc is among the fastest methods in terms of total running time when used within a RANSAC framework, while achieving competitive numerical stability, accuracy, and robustness to noise. ","Generalized Pose-and-Scale Estimation using 4-Point Congruence
  Constraints"
7,1333880631629991936,1240603790,Victor Fragoso 💻,['Wishing for a smaller 3D map for visual-based localization? Check out our new scene compression algorithm #3dv2020 paper that is\n\n☑️Easy to implement;\n☑️Efficient; and\n☑️Effective\n\n👉Paper: <LINK>\n👉Code: <LINK> <LINK>'],https://arxiv.org/abs/2011.13894,"Estimating the pose of a camera with respect to a 3D reconstruction or scene representation is a crucial step for many mixed reality and robotics applications. Given the vast amount of available data nowadays, many applications constrain storage and/or bandwidth to work efficiently. To satisfy these constraints, many applications compress a scene representation by reducing its number of 3D points. While state-of-the-art methods use $K$-cover-based algorithms to compress a scene, they are slow and hard to tune. To enhance speed and facilitate parameter tuning, this work introduces a novel approach that compresses a scene representation by means of a constrained quadratic program (QP). Because this QP resembles a one-class support vector machine, we derive a variant of the sequential minimal optimization to solve it. Our approach uses the points corresponding to the support vectors as the subset of points to represent a scene. We also present an efficient initialization method that allows our method to converge quickly. Our experiments on publicly available datasets show that our approach compresses a scene representation quickly while delivering accurate pose estimates. ",Efficient Scene Compression for Visual-based Localization
8,1333851658418221056,24501055,Kevin Carlberg,['New paper on preserving physical properties in model reduction via constrained-optimization projection. Summary: formulating projection as a constrained-optimization problem enables enforcing general kinematic and dynamic constraints <LINK>'],https://arxiv.org/abs/2011.13998,"Model-reduction techniques aim to reduce the computational complexity of simulating dynamical systems by applying a (Petrov-)Galerkin projection process that enforces the dynamics to evolve in a low-dimensional subspace of the original state space. Frequently, the resulting reduced-order model (ROM) violates intrinsic physical properties of the original full-order model (FOM) (e.g., global conservation, Lagrangian structure, state-variable bounds) because the projection process does not generally ensure preservation of these properties. However, in many applications, ensuring the ROM preserves such intrinsic properties can enable the ROM to retain physical meaning and lead to improved accuracy and stability properties. In this work, we present a general constrained-optimization formulation for projection-based model reduction that can be used as a template to enforce the ROM to satisfy specific properties on the kinematics and dynamics. We introduce constrained-optimization formulations at both the time-continuous (i.e., ODE) level, which leads to a constrained Galerkin projection, and at the time-discrete level, which leads to a least-squares Petrov-Galerkin (LSPG) projection, in the context of linear multistep schemes. We demonstrate the ability of the proposed formulations to equip ROMs with desired properties such as global energy conservation and bounds on the total variation. ","Preserving general physical properties in model reduction of dynamical
  systems via constrained-optimization projection"
9,1333804386439446537,2621989106,Yang Song,"['Happy to announce our new work on score-based generative modeling: high quality samples, exact log-likelihoods, and controllable generation, all available through score matching and Stochastic Differential Equations (SDEs)!\n\nPaper: <LINK> <LINK>', 'We can perturb data to noise using an SDE, and create samples from noise by solving the reverse SDE. This reverse SDE can be estimated by training a neural network (score-based model) to approximate the score function with score matching. No need for adversarial training! https://t.co/0oydAQZYWK', 'Every SDE has an associated probability flow ODE, which yields deterministic processes that sample from the same distribution as the SDE at each timestep. This establishes an equivalence to neural ODEs, allowing sampling via ODE solvers and exact computation of log-likelihoods. https://t.co/V2Err8cLZ0', 'We achieved a record-breaking FID of 2.20 and inception score of 9.89 on CIFAR-10. Although not trained by maximum likelihood, our model obtains a negative log-likelihood of 3.10 bits/dim, better than any other result we know for uniformly dequantized CIFAR-10 images. https://t.co/WcUf2VbDdx', 'With new architectures and better samplers enabled by our framework, we were able to demonstrate high-fidelity generation of 1024 x 1024 images for the first time from a score-based generative model. https://t.co/TjRMs7d5TO', 'Our framework unifies and generalizes previous work on score matching + Langevin dynamics (Song &amp; Ermon 2019, 2020) and denoising diffusion probabilistic models (Ho et al., 2020). They correspond to discretizations of different SDEs under our framework.', 'One more thing: We can modulate the generation process by conditioning on information not available during training. This enables controllable generation, leading to applications such as class-conditional generation, image inpainting, and colorization. https://t.co/GQ2m9HWl5h', 'In summary, we present a new framework for score-based generative modeling that allows high sample quality, exact likelihood computation, and controllable generation. There are other good properties, such as uniquely identifiable encoding. Check out our paper for more details!', 'This is a joint work with awesome collaborators: @jaschasd @dpkingma Abhishek Kumar @StefanoErmon &amp; @poolio !\nWe will release the code soon. Stay tuned for updates!', '@_dim_ma_ Thanks for the inspiring work!']",https://arxiv.org/abs/2011.13456,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model. ","Score-Based Generative Modeling through Stochastic Differential
  Equations"
10,1333787946743316487,202420697,Jeff Carver,"['Check out new paper with @LucasPCordova, @Gursimr09066201, and Noah Gershmel on conceptual feedback to help students learn #testing. Paper will appear at the 2021 @SIGCSE_TS. Preprint available: <LINK>']",https://arxiv.org/abs/2011.13004,"The feedback provided by current testing education tools about the deficiencies in a student's test suite either mimics industry code coverage tools or lists specific instructor test cases that are missing from the student's test suite. While useful in some sense, these types of feedback are akin to revealing the solution to the problem, which can inadvertently encourage students to pursue a trial-and-error approach to testing, rather than using a more systematic approach that encourages learning. In addition to not teaching students why their test suite is inadequate, this type of feedback may motivate students to become dependent on the feedback rather than thinking for themselves. To address this deficiency, there is an opportunity to investigate alternative feedback mechanisms that include a positive reinforcement of testing concepts. We argue that using an inquiry-based learning approach is better than simply providing the answers. To facilitate this type of learning, we present Testing Tutor, a web-based assignment submission platform that supports different levels of testing pedagogy via a customizable feedback engine. We evaluated the impact of the different types of feedback through an empirical study in two sophomore-level courses. We use Testing Tutor to provide students with different types of feedback, either traditional detailed code coverage feedback or inquiry-based learning conceptual feedback, and compare the effects. The results show that students that receive conceptual feedback had higher code coverage (by different measures), fewer redundant test cases, and higher programming grades than the students who receive traditional code coverage feedback. ","A Comparison of Inquiry-Based Conceptual Feedback vs. Traditional
  Detailed Feedback Mechanisms in Software Testing Education: An Empirical
  Investigation"
11,1333768498099548161,167395734,Daiki Nishiguchi,"['Cute flocking colloids in our new paper with @jiwasawa !\nThey show long-range order, algebraic correlations, giant fluctuations, enhanced diffusion etc. Many implications and challenges on the connection between Vicsek World &amp; Active Brownian Physics.\n<LINK> <LINK>']",https://arxiv.org/abs/2011.14548,"We study the polar collective dynamics of Janus colloidal particles fueled by an AC electric field. When the density is high enough, the polar interactions between the particles induce a polar orientationally ordered state which exhibits features reminiscent of the Vicsek model such as true long-range order and giant number fluctuations. Independent measurements of the polarity and velocity at the single particle level allowed us to investigate the single particle dynamics within the ordered state. We discovered theoretically-unaddressed statistical properties of the ordered state such as the asymmetric relation of polarity and velocity, enhanced rotational diffusion stronger than in the disordered state, and an algebraic auto-correlation of the polarity. Our experimental findings, at the crossroad of the Vicsek physics and the Active Brownian Particles physics, shed light on the so-far-unexplored physics arising from the interplay between the polarity and the velocity. ","Algebraic correlations and anomalous fluctuations in ordered flocks of
  Janus particles fueled by an AC electric field"
12,1333730907900010499,909073399770738694,Giulia De Rosi,"['Our new paper is online: <LINK>\n\nExotic liquids recently emerged from ultracold atomic gases. Their temperature is nK and they are 100 million times less dense than water. Their existence is a pure quantum effect. Their thermal effects were unknown until our work <LINK>', 'We have predicted two thermal mechanisms driving the liquid-gas transition: the dynamical instability and the evaporation. We have provided the phase diagram suggesting the realization of the liquid by cooling the gas. We have computed the thermodynamic quantities of the liquid.', 'We have proposed novel and precise methods to measure the temperature in quantum liquids: \n\n1) the strong dependence of the critical temperature on the interactions which can be finely tuned, \n\n2) the thermodynamic quantities of the liquids suggest future in-situ measurements.']",https://arxiv.org/abs/2011.14353,"We study the low-temperature thermodynamics of weakly-interacting uniform liquids in one-dimensional attractive Bose-Bose mixtures.~The Bogoliubov approach is used to simultaneously describe quantum and thermal fluctuations. First, we investigate in detail two different thermal mechanisms driving the liquid-to-gas transition, the dynamical instability and the evaporation, and we draw the phase diagram. Then, we compute the main thermodynamic quantities of the liquid, such as the chemical potential, the Tan's contact, the adiabatic sound velocity and the specific heat at constant volume. The strong dependence of the thermodynamic quantities on the temperature may be used as a precise temperature probe for experiments on quantum liquids. ","Thermal instability, evaporation and thermodynamics of one-dimensional
  liquids in weakly-interacting Bose-Bose mixtures"
13,1333726886543945728,487990723,Gianfranco Bertone,"['New paper today w/ @joeri_hermans, Nil Banik, @C_Weniger and @glouppe: a likelihood-free Bayesian inference pipeline to infer properties of dark matter from observations of stellar streams <LINK> <LINK>']",https://arxiv.org/abs/2011.14923,"A statistical analysis of the observed perturbations in the density of stellar streams can in principle set stringent contraints on the mass function of dark matter subhaloes, which in turn can be used to constrain the mass of the dark matter particle. However, the likelihood of a stellar density with respect to the stream and subhaloes parameters involves solving an intractable inverse problem which rests on the integration of all possible forward realisations implicitly defined by the simulation model. In order to infer the subhalo abundance, previous analyses have relied on Approximate Bayesian Computation (ABC) together with domain-motivated but handcrafted summary statistics. Here, we introduce a likelihood-free Bayesian inference pipeline based on Amortised Approximate Likelihood Ratios (AALR), which automatically learns a mapping between the data and the simulator parameters and obviates the need to handcraft a possibly insufficient summary statistic. We apply the method to the simplified case where stellar streams are only perturbed by dark matter subhaloes, thus neglecting baryonic substructures, and describe several diagnostics that demonstrate the effectiveness of the new method and the statistical quality of the learned estimator. ","Towards constraining warm dark matter with stellar streams through
  neural simulation-based inference"
14,1333721342605201409,127070843,Michael Sentef,"[""Tao Yu's new paper on an interaction limit for spin currents @MPSDHamburg @TohokuUniPR Tianjin University <LINK>""]",https://arxiv.org/abs/2011.15008,The Doppler shift of the quasiparticle dispersion by charge currents is responsible for the critical supercurrents in superconductors and instabilities of the magnetic ground state of metallic ferromagnets. Here we predict an analogous effect in thin films of magnetic insulators in which microwaves emitted by a proximity stripline generate coherent chiral spin currents that cause a Doppler shift in the magnon dispersion. The spin-wave instability is suppressed by magnon-magnon interactions that limit spin currents to values close to but below the threshold for the instability. The spin current limitations by the backaction of magnon currents on the magnetic order should be considered as design parameters in magnonic devices. ,Spin-Wave Doppler Shift by Magnon Drag in Magnetic Insulators
15,1333601885840834560,1148910974218321920,Dr. Isobel Romero-Shaw,"[""🎶 On the first day of Advent, my PhD gave to me, a new paper on the arXiv pre-print repository 🎶\n<LINK>\n(It's Dec 1 in Australia...)\n\nIn this paper we show how future gravitational wave detections can unveil the formation histories of globular clusters! 1/3"", 'This work uses globular cluster simulations by co-author Kyle Kremer to generate realistic metallicity-dependent delay-time distributions, mapping black-hole merger detections back to the birth of their host clusters. 2/3', 'Many thanks to Kyle, as well as other co-authors @LaskyPaul, @EHThrane and Johan Samsing, for the many Zoom calls across awkward timezones that it took to pull this off! 3/3']",http://arxiv.org/abs/2011.14541,"Globular clusters are considered to be likely breeding grounds for compact binary mergers. In this paper, we demonstrate how the gravitational-wave signals produced by compact object mergers can act as tracers of globular cluster formation and evolution. Globular cluster formation is a long-standing mystery in astrophysics, with multiple competing theories describing when and how globular clusters formed. The limited sensitivity of electromagnetic telescopes inhibits our ability to directly observe globular cluster formation. However, with future audio-band detectors sensitive out to redshifts of $z \approx 50$ for GW150914-like signals, gravitational-wave astronomy will enable us to probe the Universe when the first globular clusters formed. We simulate a population of binary black hole mergers from theoretically-motivated globular cluster formation models, and construct redshift measurements consistent with the predicted accuracy of third-generation detectors. We show that we can locate the peak time of a cluster formation epoch during reionisation to within 0.05Gyr after one year of observations. The peak of a formation epoch that coincides with the Universal star formation rate can be measured to within 0.4Gyr-10.5Gyr after one year of observations, depending on the relative weighting of the model components. ","Gravitational Waves as a Probe of Globular Cluster Formation and
  Evolution"
16,1333597424242331652,4438354094,Tom Wong,"[""New paper! It's 32 pages of me explaining this table, which summarizes how classical random walks and quantum walks solve the unstructured search problem (i.e., search on the complete graph or all-to-all network). <LINK> <LINK>""]",https://arxiv.org/abs/2011.14533,"The task of finding an entry in an unsorted list of $N$ elements famously takes $O(N)$ queries to an oracle for a classical computer and $O(\sqrt{N})$ queries for a quantum computer using Grover's algorithm. Reformulated as a spatial search problem, this corresponds to searching the complete graph, or all-to-all network, for a marked vertex by querying an oracle. In this tutorial, we derive how discrete- and continuous-time (classical) random walks and quantum walks solve this problem in a thorough and pedagogical manner, providing an accessible introduction to how random and quantum walks can be used to search spatial regions. Some of the results are already known, but many are new. For large $N$, the random walks converge to the same evolution, both taking $N \ln(1/\epsilon)$ time to reach a success probability of $1-\epsilon$. In contrast, the discrete-time quantum walk asymptotically takes $\pi\sqrt{N}/2\sqrt{2}$ timesteps to reach a success probability of $1/2$, while the continuous-time quantum walk takes $\pi\sqrt{N}/2$ time to reach a success probability of $1$. ",Unstructured Search by Random and Quantum Walk
17,1333596338529984519,996928553038966784,Samuel Yen-Chi Chen,"['Our new paper ""Hybrid quantum-classical classifier based on tensor network and variational quantum circuit"" is accepted by the First Workshop on Quantum Tensor Networks in Machine Learning in NeurIPS 2020\n<LINK>']",https://arxiv.org/abs/2011.14651,"One key step in performing quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices is the dimension reduction of the input data prior to their encoding. Traditional principle component analysis (PCA) and neural networks have been used to perform this task; however, the classical and quantum layers are usually trained separately. A framework that allows for a better integration of the two key components is thus highly desirable. Here we introduce a hybrid model combining the quantum-inspired tensor networks (TN) and the variational quantum circuits (VQC) to perform supervised learning tasks, which allows for an end-to-end training. We show that a matrix product state based TN with low bond dimensions performs better than PCA as a feature extractor to compress data for the input of VQCs in the binary classification of MNIST dataset. The architecture is highly adaptable and can easily incorporate extra quantum resource when available. ","Hybrid quantum-classical classifier based on tensor network and
  variational quantum circuit"
18,1333594655481503746,140385235,J. Iwasawa,"['Our new paper, ""Algebraic correlations and anomalous fluctuations in ordered flocks of Janus particles fueled by an AC electric field"" is now available in arXiv. It was wonderful being able to collaborate with @daikinish.\n<LINK>', 'We study the polar collective motion of Janus particles and find true long-range order and Giant Number Fluctuations (Fig. 4, 8). We also study single particle dynamics and discuss theoretically dismissed features such as asymmetric coupling between polarity and velocity (Fig. 7)']",https://arxiv.org/abs/2011.14548,"We study the polar collective dynamics of Janus colloidal particles fueled by an AC electric field. When the density is high enough, the polar interactions between the particles induce a polar orientationally ordered state which exhibits features reminiscent of the Vicsek model such as true long-range order and giant number fluctuations. Independent measurements of the polarity and velocity at the single particle level allowed us to investigate the single particle dynamics within the ordered state. We discovered theoretically-unaddressed statistical properties of the ordered state such as the asymmetric relation of polarity and velocity, enhanced rotational diffusion stronger than in the disordered state, and an algebraic auto-correlation of the polarity. Our experimental findings, at the crossroad of the Vicsek physics and the Active Brownian Particles physics, shed light on the so-far-unexplored physics arising from the interplay between the polarity and the velocity. ","Algebraic correlations and anomalous fluctuations in ordered flocks of
  Janus particles fueled by an AC electric field"
19,1333516537467002881,1291134968907698176,Jennifer J. Sun,"['Our framework, using self- and programmatic supervision, improves data efficiency for behavioral experiments by up to 10x.\n\nNew preprint with @Antihebbiann, Eric Zhan, @yisongyue &amp; Pietro Perona\n\nPaper: <LINK>\nCode release in the works :) <LINK>', '@mikemcdannald @Antihebbiann @yisongyue Thank you! :)']",https://arxiv.org/abs/2011.13917,"Specialized domain knowledge is often necessary to accurately annotate training sets for in-depth analysis, but can be burdensome and time-consuming to acquire from domain experts. This issue arises prominently in automated behavior analysis, in which agent movements or actions of interest are detected from video tracking data. To reduce annotation effort, we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning. The tasks in our method can be efficiently engineered by domain experts through a process we call ""task programming"", which uses programs to explicitly encode structured knowledge from domain experts. Total domain expert effort can be reduced by exchanging data annotation time for the construction of a small number of programmed tasks. We evaluate this trade-off using data from behavioral neuroscience, in which specialized domain knowledge is used to identify behaviors. We present experimental results in three datasets across two domains: mice and fruit flies. Using embeddings from TREBA, we reduce annotation burden by up to a factor of 10 without compromising accuracy compared to state-of-the-art features. Our results thus suggest that task programming and self-supervision can be an effective way to reduce annotation effort for domain experts. ",Task Programming: Learning Data Efficient Behavior Representations
20,1333498158853222400,846245360,Alex Tamkin,"['What happens when you Fourier Transform a BERT neuron?\n\nSignal processing can reveal (+manipulate!) multiscale linguistic structure in BERT neurons!\n\nNew #NeurIPS2020 paper w/ @jurafsky and Noah Goodman \n\nPaper: <LINK> \n@stanfordnlp &amp; @StanfordAILab\n\n👇 1/ <LINK>', 'Linguistic phenomena occur at different scales, including\n– within a word (e.g. morphology)\n– between words (syntax)\n– across utterances (discourse) \n– across paragraphs (topic)\n\nBut to what extent are these scales captured in the representations of pretrained models?\n\n2/ https://t.co/3yj11c1tPK', 'It turns out that many BERT neurons exhibit *multiscale* structure across an input—this neuron changes both rapidly between adjacent tokens and gradually across the input\n\nSpectral analysis lets us disentangle these scales by treating these activations as a digital signal!\n\n3/ https://t.co/b5t2VdkkJ4', ""After doing this, we show through probing experiments that different parts of a neuron's frequency spectrum capture knowledge of NLP tasks at different scales! \n\nLow frequencies correspond to topic, higher frequencies to part of speech, and middle ones to dialog acts.\n\n4/ https://t.co/gOvTR0u304"", 'We can also use spectral filters to *specialize* neurons to different scales of structure during pretraining, using what we call a prism layer.\n\nThis produces single multiscale representations which perform comparably or better on all tasks.\n\n5/ https://t.co/7q1YTx71T1', 'The prism layer also improves modeling of long-range dependencies—the BERT + Prism model outperforms BERT on masked language modeling problems without local context!\n\n6/ https://t.co/5ypM5EhM2e', 'Spectral filters are super easy to incorporate into existing models—it just takes a couple lines of code and an off-the-shelf PyTorch library (https://t.co/NK9ZMyHdiO)\n\n7/ https://t.co/TScZp0SkLt', ""It was great working on this with @jurafsky and Noah Goodman, and we're excited to see what folks do with these methods, both for interpretability and in building better models!\n\nPaper: https://t.co/kTO0WGjUqZ\n\n🌇 8/8""]",https://arxiv.org/abs/2011.04823,"Language exhibits structure at different scales, ranging from subwords to words, sentences, paragraphs, and documents. To what extent do deep models capture information at these scales, and can we force them to better capture structure across this hierarchy? We approach this question by focusing on individual neurons, analyzing the behavior of their activations at different timescales. We show that signal processing provides a natural framework for separating structure across scales, enabling us to 1) disentangle scale-specific information in existing embeddings and 2) train models to learn more about particular scales. Concretely, we apply spectral filters to the activations of a neuron across an input, producing filtered embeddings that perform well on part of speech tagging (word-level), dialog speech acts classification (utterance-level), or topic classification (document-level), while performing poorly on the other tasks. We also present a prism layer for training models, which uses spectral filters to constrain different neurons to model structure at different scales. Our proposed BERT + Prism model can better predict masked tokens using long-range context and produces multiscale representations that perform better at utterance- and document-level tasks. Our methods are general and readily applicable to other domains besides language, such as images, audio, and video. ","Language Through a Prism: A Spectral Approach for Multiscale Language
  Representations"
21,1333474402235609088,546427546,Andrea Tagliasacchi,"['Our new paper ""𝐔𝐧𝐬𝐮𝐩𝐞𝐫𝐯𝐢𝐬𝐞𝐝 𝐩𝐚𝐫𝐭 𝐫𝐞𝐩𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧 𝐛𝐲 𝐅𝐥𝐨𝐰 𝐂𝐚𝐩𝐬𝐮𝐥𝐞𝐬"" is out <LINK>\n\nTL;DR: are newborns exposed to 14 million (i.e. ImageNet) labeled images? No! They learn by observing motion... in an unsupervised fashion <LINK>', '@krematas @geoffreyhinton @sabour_sara 🤣😂🤣😂😅']",https://arxiv.org/abs/2011.13920,"Capsule networks aim to parse images into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a way to learn primary capsule encoders that detect atomic parts from a single image. During training we exploit motion as a powerful perceptual cue for part definition, with an expressive decoder for part generation within a layered image model with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and occlusion. The part decoder infers the underlying shape masks, effectively filling in occluded regions of the detected shapes. We evaluate FlowCapsules on unsupervised part segmentation and unsupervised image classification. ",Unsupervised part representation by Flow Capsules
22,1333450081060724738,61831354,Eline Maaike de Weerd,"['New paper (somehow)! We include local primordial non-Gaussianity in the relativistic bispectrum, and show that the bias from using a Newtonian analysis instead could be as large as fnl~5, highlighting the importance of including these effects in modelling: <LINK>']",http://arxiv.org/abs/2011.13660,"Next-generation galaxy and 21cm intensity mapping surveys will rely on a combination of the power spectrum and bispectrum for high-precision measurements of primordial non-Gaussianity. In turn, these measurements will allow us to distinguish between various models of inflation. However, precision observations require theoretical precision at least at the same level. We extend the theoretical understanding of the galaxy bispectrum by incorporating a consistent general relativistic model of galaxy bias at second order, in the presence of local primordial non-Gaussianity. The influence of primordial non-Gaussianity on the bispectrum extends beyond the galaxy bias and the dark matter density, due to redshift-space effects. The standard redshift-space distortions at first and second order produce a well-known primordial non-Gaussian imprint on the bispectrum. Relativistic corrections to redshift-space distortions generate new contributions to this primordial non-Gaussian signal, arising from: (1)~a coupling of first-order scale-dependent bias with first-order relativistic observational effects, and (2)~linearly evolved non-Gaussianity in the second-order velocity and metric potentials which appear in relativistic observational effects. Our analysis allows for a consistent separation of the relativistic `contamination' from the primordial signal, in order to avoid biasing the measurements by using an incorrect theoretical model. We show that the bias from using a Newtonian analysis of the squeezed bispectrum could be $\Delta \fnl\sim 5$ for a Stage IV H$\alpha$ survey. ",Local primordial non-Gaussianity in the relativistic galaxy bispectrum
23,1333437271450664962,69202541,Jonathan Le Roux,"['Our new paper w/ Sameer Khurana, Niko Moritz, Takaaki Hori, presents DUST, a Dropout-based Uncertainty-driven Self-Training method for unsupervised domain adaptation. DUST can recover 80% performance of supervised system on WSJ-&gt;TED-LIUM3/SWITCHBOARD tasks\n<LINK>']",https://arxiv.org/abs/2011.13439,"The performance of automatic speech recognition (ASR) systems typically degrades significantly when the training and test data domains are mismatched. In this paper, we show that self-training (ST) combined with an uncertainty-based pseudo-label filtering approach can be effectively used for domain adaptation. We propose DUST, a dropout-based uncertainty-driven self-training technique which uses agreement between multiple predictions of an ASR system obtained for different dropout settings to measure the model's uncertainty about its prediction. DUST excludes pseudo-labeled data with high uncertainties from the training, which leads to substantially improved ASR results compared to ST without filtering, and accelerates the training time due to a reduced training data set. Domain adaptation experiments using WSJ as a source domain and TED-LIUM 3 as well as SWITCHBOARD as the target domains show that up to 80% of the performance of a system trained on ground-truth data can be recovered. ","Unsupervised Domain Adaptation for Speech Recognition via Uncertainty
  Driven Self-Training"
24,1333406688880300036,461851789,Tanel Alumäe,"['VoxLingua107: new dataset for training spoken language identification models, covering 107 languages, scraped automatically from YouTube \n<LINK>\nPaper: <LINK>']",https://arxiv.org/abs/2011.12998,"This paper investigates the use of automatically collected web audio data for the task of spoken language recognition. We generate semi-random search phrases from language-specific Wikipedia data that are then used to retrieve videos from YouTube for 107 languages. Speech activity detection and speaker diarization are used to extract segments from the videos that contain speech. Post-filtering is used to remove segments from the database that are likely not in the given language, increasing the proportion of correctly labeled segments to 98%, based on crowd-sourced verification. The size of the resulting training set (VoxLingua107) is 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances. We use the data to build language recognition models for several spoken language identification tasks. Experiments show that using the automatically retrieved training data gives competitive results to using hand-labeled proprietary datasets. The dataset is publicly available. ",VoxLingua107: a Dataset for Spoken Language Recognition
25,1333403358917496838,887278045761077248,Dmytro Mishkin 🇺🇦,"['Efficient Initial Pose-graph Generation for Global SfM\n<LINK>\n\nOur new paper without deep learning invoved!\n\nTl;dr: Bag of tricks for 7x speed-up camera pose graph generation (29 hours for 402 130 image pairs vs 202 hours originally)\n\nDetails in the thread 1/6 <LINK>', 'Idea 1: Do not match hard pairs, unless necessary. Implementation:\n- calculate global similarity with GeM retrieval\n- use A* to find the easiest path between not-yet-matched pairs via already matched ones. Get the camera pose estimation from graph. Verify by guided matching 2/6 https://t.co/aRsIkPbz4u', 'Idea 2: to get the correspondence for the A*-based pose, use guided matching.\nImplementation: Get the tentative corresponces using known F. Verify by SIFT distance ratio (among only epipolar consistent only, ~2..30).\n3/6 https://t.co/BaLzo1X9J2', 'Idea 2 does not work naively: default  Lowe SNN ratio of 0.8..0.9 is too permissive. Solution: Adaptive SNN ratio based on the number of descriptor pool \n4/6 https://t.co/aMqfiFZLfU', 'Idea 3: if some features useful for match image A, they are are likely to be useful to match image B as well and that is stronger signal than Lowe SNN ratio. \nUse it to speed-up the RANSAC via PROSAC sampling\n\n5/6 https://t.co/pg2pIt30o4', 'All these things together lead to almost 7x speed-up of getting the initial pose graph for the global SfM bundle adjustment without compromise in quality: see exhaustive matching (EM) vs A*+EH.\n6/6 https://t.co/dQb5vCOjjJ']",https://arxiv.org/abs/2011.11986,"We propose ways to speed up the initial pose-graph generation for global Structure-from-Motion algorithms. To avoid forming tentative point correspondences by FLANN and geometric verification by RANSAC, which are the most time-consuming steps of the pose-graph creation, we propose two new methods - built on the fact that image pairs usually are matched consecutively. Thus, candidate relative poses can be recovered from paths in the partly-built pose-graph. We propose a heuristic for the A* traversal, considering global similarity of images and the quality of the pose-graph edges. Given a relative pose from a path, descriptor-based feature matching is made ""light-weight"" by exploiting the known epipolar geometry. To speed up PROSAC-based sampling when RANSAC is applied, we propose a third method to order the correspondences by their inlier probabilities from previous estimations. The algorithms are tested on 402130 image pairs from the 1DSfM dataset and they speed up the feature matching 17 times and pose estimation 5 times. ",Efficient Initial Pose-graph Generation for Global SfM
26,1333366370562699270,93411059,Bob Stienen,"['Today we got a new #paper about sampling high-dimensional distributions efficiently😀!\nPhase Space Sampling and Inference from Weighted Events with Autoregressive Flows\n<LINK>\n\nMore information in paper (well… yeah, of course…), and a summary in thread👇 <LINK>', 'Comparing theoretical predictions with measurements is difficult in particle physics, due to the probabilistic nature of the theory. We therefore use computer programs to simulate what measurements would look like for our theories.', 'The distributions of these simulations are then compared to our actual measurements. Using fancy statistics we can then make statements about the theory which was used in the simulations.', 'An example of such a comparison is shown here: the black dots are the actual measurements, the bars of the histogram are the simulation. They match very well, so the theory used in the simulation can explain the measurements. (source: https://t.co/Ivk7qHBEc2) https://t.co/xfrjYzO68Z', 'However, simulating these measurements is difficult, not the least because the particle processes are not just distributed normally or uniformly.', 'The distributions are very complicated and the simulation of a single data point can be very costly because of this.', 'One of the standard algorithms used to work around this limitation is the VEGAS algorithm. It works decently, but its failure in capturing correlations well hinders its performance.', 'However, in 2015, Rezende &amp; Mohamed (https://t.co/SmIP59GuDZ) proposed a technique called Normalising Flows. This is a machine learning model that can learn complicated distributions based on example data. (image: left is data, right 3 columns are increasingly better flows) https://t.co/3NCC5AWXSa', 'This technique has recently (i.e. this year) been investigated for particle physics, for example by https://t.co/nJ9i06PHws, https://t.co/JLw0Tz6z59, https://t.co/SXNpHkPsSn and https://t.co/Wlma82Fpqw.', 'We wondered if this technique also works in higher dimensions and if so, if it continues to work even when using data with weights. And if that is the case, what about negative weights (which show up in higher order simulations)? 🤔', 'In the normalizing flows we created we made use of autoregressive transformations, which are well-suited to capture correlations between parameters. https://t.co/rY7YsDltKo', 'In addition, we used spline transformations, which make sure that our samples will not be sampled in unphysical regions of parameter space. https://t.co/9onDqNC63O', 'In our paper we investigate 2 scenarios. Scenario 1 is a 14-dimensional parameter space of electron positron annihilation to a top + anti-top pair. For this we created weighted and unweighted data and trained flow models for both.', 'This figure shows a variable that is a combination of some of the 14 dimensions of the problem (i.e. correlations are important). We see that our flow model outperforms VEGAS and just naïve flat sampling of parameter space. https://t.co/xEDiITERZK', 'Flow models do not only sample data, they also give access to the likelihood of the taken samples (something for example a GAN can’t do), so they can be used as proposal distribution in rejection sampling.', 'The just shown model would have an acceptance rate in rejection sampling of approximately 32%. For comparison: the acceptance of VEGAS’ points is at 25% and for the flat distribution at 0.18%.', 'For weighted data we get similar distributions. This is convenient, because weighted data can be generated more efficiently! The unweighted data is here sampled via rejection sampling, leaving less training data in the process, to show this. https://t.co/68zE3eJ3Oz', 'We can sample complex distributions in high-dimensional parameter space at a rate of 1M data points in 24 seconds.  🏎', 'For negatively weighted data, we used an 8-dimensional subspace of the proton + proton to top + anti-top process. In this process the transverse momentum of the top quarks is relatively sensitive to the sign of the weights.', 'Does it work? Yes! Here the blue curve consists out of the same data as the red curve, but the weights (which are +1 and -1) are ignored. The transverse momentum is properly learned by the flow, taking the negative weights correctly into account. https://t.co/0zALrGpUj9', 'The cool thing is: the samples from all these flows are unweighted, so based on (negatively) weighted data, we can sample new, unweighted data that is distributed the same.', 'Our research show that autoregressive flows have to potential to make the simulation of particle physics more efficient, also in high-dimensional parameter spaces, which could significantly speed up research in our field.', 'More information, more plots and more detailed explanations can be found in our paper 😊\n\nPhase Space Sampling and Inference from Weighted Events with Autoregressive Flows\nhttps://t.co/Qif5qrlPB7']",https://arxiv.org/abs/2011.13445,"We explore the use of autoregressive flows, a type of generative model with tractable likelihood, as a means of efficient generation of physical particle collider events. The usual maximum likelihood loss function is supplemented by an event weight, allowing for inference from event samples with variable, and even negative event weights. To illustrate the efficacy of the model, we perform experiments with leading-order top pair production events at an electron collider with importance sampling weights, and with next-to-leading-order top pair production events at the LHC that involve negative weights. ","Phase Space Sampling and Inference from Weighted Events with
  Autoregressive Flows"
27,1333297801300561920,345254938,Snehasish Bhattacharjee (Bil),['My new paper in arXiv: <LINK>'],https://arxiv.org/abs/2011.13135,"In this work, we analyzed the effect of different prescriptions of the IR cutoffs, namely the Hubble horizon cutoff, particle horizon cutoff, Granda and Oliveros horizon cut off, and the Ricci horizon cutoff on the growth rate of clustering for the Tsallis holographic dark energy (THDE) model in an FRW universe devoid of any interactions between the dark Universe. Furthermore, we used the concept of configurational entropy to derive constraints (qualitatively) on the model parameters for the THDE model in each IR cutoff prescription from the fact that the rate of change of configurational entropy hits a minimum at a particular scale factor $a_{DE}$ which indicate precisely the epoch of dark energy domination predicted by the relevant cosmological model as a function of the model parameter(s). By using the current observational constraints on the redshift of transition from a decelerated to an accelerated Universe, we derived constraints on the model parameters appearing in each IR cutoff definition and on the non-additivity parameter $\delta$ characterizing the THDE model and report the existence of simple linear dependency between $\delta$ and $a_{DE}$ in each IR cutoff setup. ","Growth Rate and Configurational Entropy in Tsallis Holographic Dark
  Energy"
28,1333230769549045760,955812906892918784,Xiaowe Yue,"['Please find our new paper ""Neural Network Gaussian Process considering Input Uncertainty"": <LINK>. It was just accepted by IEEE/ASME Transactions on Mechatronics (also won IISE DAIS Best Student Paper Award). Thanks!']",https://arxiv.org/abs/2011.10861,"Developing machine learning enabled smart manufacturing is promising for composite structures assembly process. To improve production quality and efficiency of the assembly process, accurate predictive analysis on dimensional deviations and residual stress of the composite structures is required. The novel composite structures assembly involves two challenges: (i) the highly nonlinear and anisotropic properties of composite materials; and (ii) inevitable uncertainty in the assembly process. To overcome those problems, we propose a neural network Gaussian process model considering input uncertainty for composite structures assembly. Deep architecture of our model allows us to approximate a complex process better, and consideration of input uncertainty enables robust modeling with complete incorporation of the process uncertainty. Based on simulation and case study, the NNGPIU can outperform other benchmark methods when the response function is nonsmooth and nonlinear. Although we use composite structure assembly as an example, the proposed methodology can be applicable to other engineering systems with intrinsic uncertainties. ","Neural Network Gaussian Process Considering Input Uncertainty for
  Composite Structures Assembly"
29,1332866659191824385,1073882198456090624,Stephen Hughes,"['Check out our new arXiv paper on nonlinear waveguide QED, comparing matrix product states with a space discretized waveguide model: <LINK>']",https://arxiv.org/abs/2011.12205,"We present two different methods for modelling non-Markovian quantum light-matter interactions in waveguide QED systems, using matrix product states (MPSs) and a space-discretized waveguide (SDW) model. After describing the general theory and implementation of both approaches, we compare and contrast these methods directly on three topical problems of interest in waveguide-QED, including (i) a two-level system (TLS) coupled to an infinite (one-dimensional) waveguide, (ii) a TLS coupled to a terminated waveguide with a time-delayed coherent feedback, and (iii) two spatially separated TLSs coupled within an infinite waveguide. Both approaches are shown to efficiently model multi-photon nonlinear dynamics in highly non-Markovian regimes, and we highlight the advantages and disadvantages of these methods for modelling waveguide QED interactions, including their implementation in Python, computational run times, and ease of conceptual understanding. We explore both vacuum dynamics as well as regimes of strong optical pumping, where a weak excitation approximation cannot be applied. The MPS approach scales better when modelling multi-photon dynamics and long delay times, and explicitly includes non-Markovian memory effects. In contrast, the SDW model accounts for non-Markovian effects through space discretization, and solves Markovian equations of motion, yet rigorously includes the effects of retardation. The SDW model, based on an extension of recent collisional pictures in quantum optics, is solved through quantum trajectory techniques, and can more easily add in additional dissipation processes, including off-chip decay and TLS pure dephasing. The impact of these processes is shown directly on feedback-induced population trapping and TLS entanglement between spatially separated TLSs. ","Modelling quantum light-matter interactions in waveguide-QED with
  retardation and a time-delayed feedback: matrix product states versus a
  space-discretized waveguide model"
30,1332377870480662528,108977933,David Rothschild 🇺🇦,"['Let me emphasize 3 findings of this new working paper <LINK> (with a bunch of co-authors in quoted tweet) (1) News is still a relatively small part of YouTube, but YouTube is huge, news  is growing, and right-wing news in particular is growing very fast. <LINK>', '(2) Conditional on consuming right-wing news on YouTube, people are watching a lot in each setting &amp; coming back regularly (3) Recommendation engines not key driver of right-wing news: people click on links from right-wing online sites, emails, etc. or they are search for it.', ""Facebook gets all the attention, because their leadership keeps going pubic with their support of various right-wing initiatives, but if you worry about people living in extreme right-wing content YouTube's explosive growth in right-wing content &amp; consumption way more concerning."", '@James_Monroe_17 @dukeblue24 Mainstream media has a strong pro-Republican bias driven by falsely equating the validity &amp; quality of the two major parties. Social Media generally reflects the market they have created, which (while generally reflecting MSM) tends towards more extreme content (both sides).', 'Censorship is *never* the answer: We need to shift objectives of mainstream media away from false equivalency to objectivity, regulate social media with push towards transparency (exact opposite of what most people are pushing for), and Democrats should embrace partisan media.', 'Fact Republicans have massive media empires is a asymmetric pull on selection &amp; framing of all news, which Democrats need to counter, or any other changes will be relatively futile.', 'Republican partisan media has no journalistic standards, but Democratic partisan media can be held to the highest standards: just have no need to balance their lineup with Republicans and temper their advice to appear neutral.']",https://arxiv.org/abs/2011.12843,"Although it is under-studied relative to other social media platforms, YouTube is arguably the largest and most engaging online media consumption platform in the world. Recently, YouTube's scale has fueled concerns that YouTube users are being radicalized via a combination of biased recommendations and ostensibly apolitical anti-woke channels, both of which have been claimed to direct attention to radical political content. Here we test this hypothesis using a representative panel of more than 300,000 Americans and their individual-level browsing behavior, on and off YouTube, from January 2016 through December 2019. Using a labeled set of political news channels, we find that news consumption on YouTube is dominated by mainstream and largely centrist sources. Consumers of far-right content, while more engaged than average, represent a small and stable percentage of news consumers. However, consumption of anti-woke content, defined in terms of its opposition to progressive intellectual and political agendas, grew steadily in popularity and is correlated with consumption of far-right content off-platform. We find no evidence that engagement with far-right content is caused by YouTube recommendations systematically, nor do we find clear evidence that anti-woke channels serve as a gateway to the far right. Rather, consumption of political content on YouTube appears to reflect individual preferences that extend across the web as a whole. ",Examining the consumption of radical content on YouTube
31,1331958171292872704,976155561522794497,Juliano César Silva Neves,['My new paper is online. It is a study on the influence of the quantum world on the general relativity world. A quantum field can decrease the black hole shadow! \n<LINK>'],https://arxiv.org/abs/2011.12841,"In this work, we present black hole solutions with a cosmological constant in bumblebee gravity, which provides a mechanism for the Lorentz symmetry violation by assuming a nonzero vacuum expectation value for the bumblebee field. From the gravitational point of view, such solutions are spherically symmetric black holes with an effective cosmological constant and are supported by an anisotropic energy-momentum tensor, conceived of as the manifestation of the bumblebee field in the spacetime geometry. Then we calculate the shadow angular radius for the proposed black hole solution with a positive effective cosmological constant. In particular, our results are the very first relation between the bumblebee field and the shadow angular size. ",Black holes with a cosmological constant in bumblebee gravity
32,1331916661859946498,225401718,Aurel Schneider,"[""New paper out (together with the great @sam_giri and Jordan Mirocha):  <LINK>\n\nIt's about a new, fast approach to predict the 21-cm power spectrum at the cosmic dawn (the epoch right before re-ionisation)"", 'The method is based on the halo model approach, using source radiation profiles (instead of haloes). It provides fast predictions (within seconds) and agrees well with other methods from the literature.', 'Here is one plot, showing a comparison with the established code 21cmFAST (coloured bands): https://t.co/gA0f3L3fY4', 'The remaining differences between the lines and the bands  are of the same order than the expected systematic uncertainties due to differences in the source parametrisation (i.e the mass function, bias, mass accretion rates, etc).', 'Check out the link above if you want to know more :-)']",https://arxiv.org/abs/2011.12308,"Prior to the epoch of reionisation, the 21-cm signal of the cosmic dawn is dominated by the Lyman-$\alpha$ coupling and gas temperature fluctuations caused by the first sources of radiation. While early efforts to model this epoch relied on analytical techniques, the community quickly transitioned to more expensive semi-numerical models. Here, we re-assess the viability of simpler approaches that allow for rapid explorations of the vast astrophysical parameter space. We propose a new analytical method to calculate the 21-cm power spectrum based on the framework of the halo model. Both the Lyman-$\alpha$ coupling and temperature fluctuations are described by overlapping radiation flux profiles that include spectral red-shifting and source attenuation due to look-back (light-cone) effects. The 21-cm halo model is compared to the semi-numerical code 21cmFAST exhibiting generally good agreement, i.e., the power spectra differ by less than a factor of three over a large range of $k$-modes and redshifts. We show that the remaining differences between the two methods are comparable to the expected variations from modelling uncertainties associated with the abundance, bias, and accretion rates of haloes. While these current uncertainties must be reduced in the future, our work suggests that inference at acceptable accuracy will become feasible with very efficient halo models of the cosmic dawn. ",A halo model approach for the 21-cm power spectrum at cosmic dawn
33,1331910824202526721,51700215,Phil Bull,"['New paper! We ran a suite of 53 fully-relativistic N-body sims, with outputs for lensing, GR grav. potential, rel. treatment of massive neutrinos etc. The sims cover a large volume, and can be used to make exact relativistic predictions for observables. <LINK>', ""Each sim covers the full sky out to z≈0.85, and ~1930 sq. deg. out to z≈3.55, so they're relevant for Euclid, LSST, Roman etc. We have two sets -- one with the same fiducial cosmology but different random ICs, and one with the same ICs but different cosmo params."", ""I think these will be very handy for studying relativistic corrections on ultra-large scales in forthcoming surveys. Please get in touch if you're interested in working with the data! And well done to Louis, Carol, and Julian, who've done a great job putting all this together."", 'Obligatory website: https://t.co/Ox6GI2aANd', ""@Chrisclarkson69 @telescoper I guess so, but there may be some limitations in interpreting the results, since the gevolution code we're using relies on a weak-field expansion: https://t.co/BPRwVQ3OY2"", ""@Chrisclarkson69 @telescoper So, if you're thinking of using this to look at any UV corrections/backreaction, there will be some caveats..."", '@malcfairbairn Nah, most corrections from Newtonian are pretty small. Chris is banking on the bispectrum being where the fun stuff happens. So far so good on that score: https://t.co/274AGaT59X', ""@HeXiang125 @malcfairbairn That's alright, we're not trying to get past the Big Bang. Growing modes only!"", '@HeXiang125 No, no, our initial conditions are perfectly reasonable, consistent with observations etc. It sounds like the ones you are talking about are made up.', '@HeXiang125 No, it generates itself.', ""@HeXiang125 That's not true, of course it does! Non-linear gravitational collapse generates vorticity (https://t.co/Gws8rXbBYL). Why would it be absolutely conserved in a perturbed FLRW spacetime that doesn't have a corresponding exact isometry for angular momentum?"", '@HeXiang125 Plus any global rotation is strongly constrained by CMB anisotropy constraints, e.g. https://t.co/YK9ACk2KY7', ""@HeXiang125 I mean it in the usual sense. I linked to one of the papers; there's very little wiggle room. I believe the anisotropies would be generated as a late-time effect (like ISW), so it doesn't matter what the horizon size at last scattering is."", ""@HeXiang125 No it doesn't. Stop making things up."", ""@HeXiang125 No it isn't. Perfectly happy thinking about rotation. The problem is, we've gone out there and looked for it. It's not there."", ""@HeXiang125 No it isn't, it's a perfectly normal use of English in order to state a justified conviction. You are saying a lot of things that do not make sense or are not justified. It would be more productive to work to understand your misunderstandings."", '@HeXiang125 This is a strange comment to make, as you seem to think there is some kind of conspiracy, when in fact you have just made something up and do not have any evidence or sensible logical arguments for it.', '@HeXiang125 Yes, I think you might be stumbling over some technical language here -- I mean that it has been ""constrained to be consistent with zero"", or in other words: we see no evidence for it. See, and now you have just made up some other things as well.', '@HeXiang125 Yes, energy is *not* conserved in an expanding universe because there is no corresponding (time translation) isometry. Angular momentum is more subtle, as there is an isometry in exact FLRW, but not when perturbations are introduced.', '@HeXiang125 I have no such habit -- I answered your queries and then discovered that you were making things up. You have not investigated the very relevant references I sent which explain the problems with your statements. You started by stating that my paper contained misconceptions.', ""@HeXiang125 I think you may be suffering from something called the Dunning-Kruger effect here. Energy most certainly isn't unconditionally conserved; physical conservation laws require symmetries (Noethers theorem); an expanding spacetime doesn't possess the right symmetry to conserve energy"", '@HeXiang125 See, this is another example of you making things up. These are just assertions, which no logical or evidence basis. And on a technical level it does not make sense.', '@HeXiang125 I literally sent you a research paper discussing the observational constraints earlier.', ""@HeXiang125 I didn't make anything up, I even provided references that you can use to independently verify the information. You just keep saying things with no evidence or justification, hence: made up."", ""@HeXiang125 It's not really a debate if you don't support your arguments. I've supported mine."", '@HeXiang125 Seriously, look it up! For example: https://t.co/3WCAGgAuFx', ""@HeXiang125 It supports plenty, and is a reasonable line of evidence that's part of a wider literature -- follow some of the references etc. I understand Noether's theorem just fine. You just don't want to admit that this idea you have made up is, in fact, made up."", '@HeXiang125 Oh dear, oh dear. Where are you getting these ideas about energy conservation, may I ask?', ""@HeXiang125 I sent you some evidence and you didn't take it in. You don't even have your own consistent interpretation of it; it directly contradicts what you are saying."", ""@HeXiang125 I have yet to see any logical statements from you I'm afraid -- just unfounded assertions."", ""@HeXiang125 I suspect that you don't have much luck convincing people of your theory, do you..."", '@HeXiang125 See, you\'re using all these words, like ""brain-damaged"", and it\'s silly -- just another thing you\'re making up. You waded in with no good arguments and have just been uttering nonsense. A shame.', ""@HeXiang125 Ha ha, well have fun with your made up theory! I'm sure there are some other folks on Twitter you can talk nonsense to."", ""@HeXiang125 I don't think I'll remember anything much that you've said to be fair...""]",https://arxiv.org/abs/2011.12936,"The standard cosmological model is inherently relativistic, and yet a wide range of cosmological observations can be predicted accurately from essentially Newtonian theory. This is not the case on `ultra-large' distance scales, around the cosmic horizon size, however, where relativistic effects can no longer be neglected. In this paper, we present a novel suite of 53 fully relativistic simulations generated using the gevolution code, each covering the full sky out to $z \approx$ 0.85, and approximately 1930 square degrees out to $z \approx$ 3.55. These include a relativistic treatment of massive neutrinos, as well as the gravitational potential that can be used to exactly calculate observables on the past light cone. The simulations are divided into two sets, the first being a set of 39 simulations of the same fiducial cosmology (based on the Euclid Flagship 2 cosmology) with different realisations of the initial conditions, and the second which fixes the initial conditions, but varies each of seven cosmological parameters in turn. Taken together, these simulations allow us to perform statistical studies and calculate derivatives of any relativistic observable with respect to cosmological parameters. As an example application, we compute the cross-correlation between the Doppler magnification term in the convergence, $\kappa_v$, and the CDM+baryon density contrast, $\delta_{\rm cb}$, which arises only in a (special) relativistic treatment. We are able to accurately recover this term as predicted by relativistic perturbation theory, and study its sample variance and derivatives with respect to cosmological parameters. ","Observing relativistic features in large-scale structure surveys -- II:
  Doppler magnification in an ensemble of relativistic simulations"
34,1331866878881517569,1273593819485868033,Marc Habermann,['Checkout our new paper on arxiv (<LINK>) about physics-aware monocular human performance capture. \n\nMany thanks to the whole team and especially Yue Li (@liyuereal). \n\n#humanperformancecapture #ComputerVision #CV #CG <LINK>'],https://arxiv.org/abs/2011.12866,"Recent monocular human performance capture approaches have shown compelling dense tracking results of the full body from a single RGB camera. However, existing methods either do not estimate clothing at all or model cloth deformation with simple geometric priors instead of taking into account the underlying physical principles. This leads to noticeable artifacts in their reconstructions, e.g. baked-in wrinkles, implausible deformations that seemingly defy gravity, and intersections between cloth and body. To address these problems, we propose a person-specific, learning-based method that integrates a simulation layer into the training process to provide for the first time physics supervision in the context of weakly supervised deep monocular human performance capture. We show how integrating physics into the training process improves the learned cloth deformations, allows modeling clothing as a separate piece of geometry, and largely reduces cloth-body intersections. Relying only on weak 2D multi-view supervision during training, our approach leads to a significant improvement over current state-of-the-art methods and is thus a clear step towards realistic monocular capture of the entire deforming surface of a clothed human. ","Deep Physics-aware Inference of Cloth Deformation for Monocular Human
  Performance Capture"
35,1331852547238342656,2680275613,Haydee,"['""When was the last time she did Algebra?"" @att. New paper on the arXiv ""Canonical Resolutions over Koszul Algebras"" made with my inimitable co-authors from WICA at @BIRS_Math! #tryingsomethingnew #womendoalgebra  <LINK>']",https://arxiv.org/abs/2011.12409,We generalize Buchsbaum and Eisenbud's resolutions for the powers of the maximal ideal of a polynomial ring to resolve powers of the homogeneous maximal ideal over graded Koszul algebras. Our approach has the advantage of producing resolutions that are both more explicit and minimal compared to those previously discovered by Green and Mart\'{\i}nez-Villa \cite{GreenMartinezVilla} or Mart\'{\i}nez-Villa and Zacharia \cite{MartinezVillaZacharia}. ,Canonical Resolutions over Koszul Algebras
36,1331807935475826688,1125849123494793216,Kosuke Mitarai,"['New paper out! We have developed a technique for efficient benchmark and initialization of variational quantum algos. Our method is based on classical simulatablity of Cliffords, and in the paper we apply it up to 48 qubit systems.\n<LINK>']",http://arxiv.org/abs/2011.09927,"Variational quantum algorithms are considered to be appealing applications of near-term quantum computers. However, it has been unclear whether they can outperform classical algorithms or not. To reveal their limitations, we must seek a technique to benchmark them on large scale problems. Here, we propose a perturbative approach for efficient benchmarking of variational quantum algorithms. The proposed technique performs perturbative expansion of a circuit consisting of Clifford and Pauli rotation gates, which is enabled by exploiting the classical simulatability of Clifford circuits. Our method can be applied to a wide family of parameterized quantum circuits consisting of Clifford gates and single-qubit rotation gates. The approximate optimal parameter obtained by the method can also serve as an initial guess for further optimizations on a quantum device, which can potentially solve the so-called ``barren-plateau'' problem. As the first application of the method, we perform a benchmark of so-called hardware-efficient-type ansatzes when they are applied to the VQE of one-dimensional hydrogen chains up to $\mathrm{H}_{24}$, which corresponds to $48$-qubit system, using a standard workstation. ","Quadratic Clifford expansion for efficient benchmarking and
  initialization of variational quantum algorithms"
37,1331782087351754753,2583516397,Johannes Kopf,"['Our awesome new paper on *free-viewpoint* video from cell phone capture.\n\nSpace-time neural rendering with the powerful @wenqi_xian, @jbhuang0604, and @_ChangilKim!\n\nPaper: <LINK>\nWebsite: <LINK> <LINK>']",https://arxiv.org/abs/2011.12950,"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results. ",Space-time Neural Irradiance Fields for Free-Viewpoint Video
38,1331674010049007622,2780458609,Cristian Joana,"['I am glad to announce that Sebastien Clesse and I have a new paper on the arXiv! 🙂\nWe have used #GRChombo to simulate the very early universe and test the beginning of inflation. \n\n👉 <LINK> <LINK>', ""In short, we show that inflation with Starobinsky-like potentials naturally start inflating after a pre-inflationary era, where the Universe's energy budget is initially in scalar field gradients and/or inhomogeneous field velocities."", 'This work contradicts some fine-tuning claims suggesting that inhomogeneities would block and prevent inflation. Indeed, the best favoured inflationary model by Planck is still robust and safe from the inhomogeneous initial conditions. https://t.co/fVXXfAXdDo', ""Still, the transition to inflation is all but boring, and it's full of wibbly-wobbly, timey-wimey... stuff!  (incl. black holes!) \n \n(thanks @bbcdoctorwho)""]",https://arxiv.org/abs/2011.12190,"We use the 3+1 formalism of numerical relativity to investigate the robustness of Starobinsky and Higgs inflation to inhomogeneous initial conditions, in the form of either field gradient or kinetic energy density. Sub-Hubble and Hubble-sized fluctuations generically lead to inflation after an oscillatory phase between gradient and kinetic energies. Hubble-sized inhomogeneities also produce contracting regions that may end up forming primordial black holes, subsequently diluted by inflation. We analyse the dynamics of the preinflation era and the generation of vector and tensor fluctuations. Our analysis further supports the robustness of inflation to any size of inhomogeneity, in the field, velocity or equation of state. At large field values, the preinflation dynamics only marginally depends on the field potential and it is expected that such behaviour is universal and applies to any inflation potential of plateau-type, favoured by CMB observations after Planck. ","Inhomogeneous initial conditions for inflation: A wibbly-wobbly
  timey-wimey path to salvation"
39,1331653437252120580,923231130383536128,Eduardo Fonseca,"['🔊New paper out! ""Unsupervised Contrastive Learning of Sound Event Representations"". Work done with @Diego_OrtegoH\nWe learn audio representations by contrasting differently augmented views of sound events.\npaper: <LINK>\ncode (soon): <LINK>\n(1/5) <LINK>', 'The different views are computed via random patch sampling, mixing of training examples with unrelated backgrounds (mix-back), and other data augmentations.\nWe show that unsupervised contrastive pre-training helps in scenarios of data scarcity and noisy labels! (2/5)', 'Our results suggest that our method is able to learn useful sound event representations even with more limited resources (data and compute) than typically used in previous works. (3/5)', ""Our work is similar to the recent Google's COLA https://t.co/tlzCHMHp2N  by @aaqib_saeed @neilzegh\nMain differences: we apply augmentations and we use more limited data for train/eval as our compute resources are more modest 😅"", 'Our proposed example mixing strategy (mix-back) bears some similarities to the Mixup-noise just proposed this month here https://t.co/FSbtqM4k6G\n(and 5/5)']",https://arxiv.org/abs/2011.07616,"Self-supervised representation learning can mitigate the limitations in recognition tasks with few manually labeled data but abundant unlabeled data---a common scenario in sound event research. In this work, we explore unsupervised contrastive learning as a way to learn sound event representations. To this end, we propose to use the pretext task of contrasting differently augmented views of sound events. The views are computed primarily via mixing of training examples with unrelated backgrounds, followed by other data augmentations. We analyze the main components of our method via ablation experiments. We evaluate the learned representations using linear evaluation, and in two in-domain downstream sound event classification tasks, namely, using limited manually labeled data, and using noisy labeled data. Our results suggest that unsupervised contrastive pre-training can mitigate the impact of data scarcity and increase robustness against noisy labels, outperforming supervised baselines. ",Unsupervised Contrastive Learning of Sound Event Representations
40,1331614517634224132,322460769,Yoav Artzi,"['New NLP+robotics+vision paper: Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following @ CoRL 2020\n<LINK>\nWork done by Valts Blukis\nThree core contributions make this happen, let’s upack them ... <LINK>', 'First contribution: a general few-shot language-conditioned object segmentation method. We align references to objects in language to their observations in the world via a database of exemplars. Need to reason about new objects? Add exemplars to the database! https://t.co/ZZYmO1dxkK', 'Training requires quite a bit of data to generalize well to unseen objects, *but* the nice part: the whole thing is trained from automatically generated augmented reality data using synthetic ShapeNet objects https://t.co/kDAf7aXzLU', 'Second contribution: given the alignment, we formalize and build a learned object context map. The map masks over object identities and easily handles new objects. It explicitly encodes text context information into positions in the spatial environment. https://t.co/qjlpE7g29g', 'Third contribution: integrating the map into a complete policy (natural language instructions, raw observations) -&gt; continuous velocity control. The policy generates plans on top of our context maps as visitation distributions. https://t.co/I8wb6Pfhin', 'First stage mapping is through a series of projections and using our LingUNet architecture to predict the visitation distributions on top of the map (we first introduced this method in our CoRL 2018 paper) https://t.co/UWYuHvvRJs', 'Agent never saw all these objects during learning, just a few exemplars. Bonus 1: modeling of concepts like objects and trajectories is strong inductive bias —&gt; outperform systems that learn with hundreds of demonstrations with new objects.  Let’s look at the complete video again https://t.co/pBZ8qYakTV', 'Bonus2: interpretability via explicit alignments! Another example ... https://t.co/M1jzdfz3Wz', 'There’s a much longer talk with more details https://t.co/41Isbg4JEt from the recent SpLU workshop and also a brief overview from CoRL https://t.co/fk8Fw7seXN']",https://arxiv.org/abs/2011.07384,"We study the problem of learning a robot policy to follow natural language instructions that can be easily extended to reason about new objects. We introduce a few-shot language-conditioned object grounding method trained from augmented reality data that uses exemplars to identify objects and align them to their mentions in instructions. We present a learned map representation that encodes object locations and their instructed use, and construct it from our few-shot grounding output. We integrate this mapping approach into an instruction-following policy, thereby allowing it to reason about previously unseen objects at test-time by simply adding exemplars. We evaluate on the task of learning to map raw observations and instructions to continuous control of a physical quadcopter. Our approach significantly outperforms the prior state of the art in the presence of new objects, even when the prior approach observes all objects during training. ","Few-shot Object Grounding and Mapping for Natural Language Robot
  Instruction Following"
41,1331607514241069056,1041314886,Keisuke Fujii,['Our new paper on benchmarking and initialization of VQE: <LINK>\nQuadratic Clifford expansion (an extension of GK approach) allows perturbative analysis of VQE with 48 qubits. The perturbative solution can be a good initial parameters to avoid barren plateau.'],https://arxiv.org/abs/2011.09927,"Variational quantum algorithms are considered to be appealing applications of near-term quantum computers. However, it has been unclear whether they can outperform classical algorithms or not. To reveal their limitations, we must seek a technique to benchmark them on large scale problems. Here, we propose a perturbative approach for efficient benchmarking of variational quantum algorithms. The proposed technique performs perturbative expansion of a circuit consisting of Clifford and Pauli rotation gates, which is enabled by exploiting the classical simulatability of Clifford circuits. Our method can be applied to a wide family of parameterized quantum circuits consisting of Clifford gates and single-qubit rotation gates. The approximate optimal parameter obtained by the method can also serve as an initial guess for further optimizations on a quantum device, which can potentially solve the so-called ``barren-plateau'' problem. As the first application of the method, we perform a benchmark of so-called hardware-efficient-type ansatzes when they are applied to the VQE of one-dimensional hydrogen chains up to $\mathrm{H}_{24}$, which corresponds to $48$-qubit system, using a standard workstation. ","Quadratic Clifford expansion for efficient benchmarking and
  initialization of variational quantum algorithms"
42,1331568075712368640,911734434,Jonathan Keeling,"['New paper by @cm_cdt  student Roberta Palacino on atom-only effective models.  With U(1) symmetry, Redfield theory fails to predict the phase transition. Using higher-order density matrix equation of motion restores this transition. <LINK> <LINK>']",https://arxiv.org/abs/2011.12120,"We consider a generalized Dicke model with U(1) symmetry, which can undergo a transition to a superradiant state that spontaneously breaks this symmetry. By exploiting the difference in timescale between atomic and cavity dynamics, one may eliminate the cavity dynamics, providing an atom-only theory. We show that the standard Redfield theory cannot describe the transition to the superradiant state, but including higher-order corrections does recover the transition. Our work reveals how the forms of effective theories must vary for models with continuous symmetry, and provides a template to develop effective theories of more complex models. ",Atom-only theories for U(1) symmetric cavity-QED models
43,1331525904577736706,292981256,Josh Borrow,"[""I've got a new paper out on the arxiv today (<LINK>) about the inconsistencies that you get when coupling Pressure-based SPH to galaxy formation models... Essentially this all comes down to them (when coupled to sub-grid models) breaking the fundamental... <LINK>"", '... rule of ""don\'t smooth over things that vary rapidly compared to your time-step"". Thanks to those at @spheric_ where I got a chance to speak to people about these ideas. I had initially planned to present this at the 2020 workshop, but alas COVID removed that possibility.']",https://arxiv.org/abs/2011.11641,"Smoothed Particle Hydrodynamics (SPH) is a Lagrangian method for solving the fluid equations that is commonplace in astrophysics, prized for its natural adaptivity and stability. The choice of variable to smooth in SPH has been the topic of contention, with smoothed pressure (P-SPH) being introduced to reduce errors at contact discontinuities relative to smoothed density schemes. Smoothed pressure schemes produce excellent results in isolated hydrodynamics tests; in more complex situations however, especially when coupling to the `sub-grid' physics and multiple time-stepping used in many state-of-the-art astrophysics simulations, these schemes produce large force errors that can easily evade detection as they do not manifest as energy non-conservation. Here two scenarios are evaluated: the injection of energy into the fluid (common for stellar feedback) and radiative cooling. In the former scenario, force and energy conservation errors manifest (of the same order as the injected energy), and in the latter large force errors that change rapidly over a few timesteps lead to instability in the fluid (of the same order as the energy lost to cooling). Potential ways to remedy these issues are explored with solutions generally leading to large increases in computational cost. Schemes using a Density-based formulation do not create these instabilities and as such it is recommended that they are preferred over Pressure-based solutions when combined with an energy diffusion term to reduce errors at contact discontinuities. ","Inconsistencies arising from the coupling of galaxy formation sub-grid
  models to Pressure-Smoothed Particle Hydrodynamics"
44,1331505670177259522,973860795246198784,Z.Wei,"['Our new paper is on arXiv! We reconsidered the moving mirror setup, which is often introduced as a toy model of black hole evaporation, in the context of CFT, holography and island. Thank you to my fantastic collaborators!\n<LINK> <LINK>']",https://arxiv.org/abs/2011.12005,"We calculate the time evolution of entanglement entropy in two dimensional conformal field theory with a moving mirror. For a setup modeling Hawking radiation, we obtain a linear growth of entanglement entropy and show that this can be interpreted as the production of entangled pairs. For a setup, which mimics black hole formation and evaporation, we find that the evolution follows the ideal Page curve. We perform these computations by constructing the gravity dual of the moving mirror model via holography. We also argue that our holographic setup provides a concrete model to derive the Page curve for black hole radiation in the strong coupling regime of gravity. ",Entanglement entropy in holographic moving mirror and Page curve
45,1331429947911536640,969190164764372993,Xudong Sun,"['Our new paper with @markcheung on electric current neutralization is out in arXiv: <LINK>. We show that a geometric projection effect may explain the observed non-zero net current in sunspots. 1/n', 'Observationally, vector magnetograms show significant net current in active regions that are flare productive. Most CME models also have flux ropes with such ""non-neutralized"" current. 2/n', 'However, subsurface flux tubes as progenitors of ARs are thought to be ""current neutralized"" because they are embedded in magnetic-free environment. This apparent mismatch requires some explanation. 3/n', 'Past simulations have shown flux emergence (in stratified atmosphere) and shearing (along polarity inversion line) can generate net currents. In the current study, we demonstrate an alternative/complementary mechanism. 4/n', 'We use an analytical, torus flux tube from Fan &amp; Gibson (2003), and kinematically lift it across solar surface to mimic flux emergence. The flux tube has no net axial (toroidal) current, as assumed for subsurface tubes. 5/n https://t.co/iA0KACDC35', 'However, the *poloidal* current, when projected along the vertical direction, is significantly non-neutralized. It is expected to contribute to the observed surface current during the early stages of flux emergence. 6/n', 'The simple model reproduces a few observational signatures. The net poloidal current comes from the rapid decay of the toroidal field away from the tube axis, which should apply to many other flux tube models. 7/n', ""This model, of course, excludes most of the interesting physical processes during flux emergence (which ever your favorite is, it's not in the model!). I learned a lot from talking to @markcheung and Tibor Torok. 8/n"", 'The take home message is, when we are talking about ""neutralized current"", we are talking about two different things when referring to surface observation vs subsurface tubes. Also, deriving equations (like a freshman) is fun! 9/n']",https://arxiv.org/abs/2011.11873,"Active regions (ARs) often possess an observed net electric current in a single magnetic polarity. We show that such ""non-neutralized"" currents can arise from a geometric projection effect when a twisted flux tube obliquely intersects the photosphere. To this end, we emulate surface maps of an emerging AR by sampling horizontal slices of a semi-torus flux tube at various heights. Although the tube has no net toroidal current, its poloidal current, when projected along the vertical direction, amounts to a significant non-neutralized component on the surface. If the tube emerges only partially as in realistic settings, the non-neutralized current will 1) develop as double ribbons near the sheared polarity inversion line, (2) positively correlate with the twist, and 3) reach its maximum before the magnetic flux. The projection effect may be important to the photospheric current distribution, in particular during the early stages of flux emergence. ","Non-Neutralized Electric Current of Active Regions Explained as a
  Projection Effect"
46,1331425321334300673,382961853,Jo Dunkley,"['New paper today led by @NaomiR_astro shows how gravity from cosmic structures in universe distorts  - or lenses - both light from background galaxies and light from the big bang. Using data from KiDS, @ACT_Pol and @Planck, joint signal is now over 7 sigma! <LINK>']",https://arxiv.org/abs/2011.11613,"We measure the cross-correlation between galaxy weak lensing data from the Kilo Degree Survey (KiDS-1000, DR4) and cosmic microwave background (CMB) lensing data from the Atacama Cosmology Telescope (ACT, DR4) and the Planck Legacy survey. We use two samples of source galaxies, selected with photometric redshifts, $(0.1<z_{\rm B}<1.2)$ and $(1.2<z_{\rm B}<2)$, which produce a combined detection significance of the CMB lensing/weak galaxy lensing cross-spectrum of $7.7\sigma$. With the lower redshift galaxy sample, for which the cross-correlation is detected at a significance of $5.3\sigma$, we present joint cosmological constraints on the matter density parameter, $\Omega_{\rm m}$, and the matter fluctuation amplitude parameter, $\sigma_8$, marginalising over three nuisance parameters that model our uncertainty in the redshift and shear calibration, and the intrinsic alignment of galaxies. We find our measurement to be consistent with the best-fitting flat $\Lambda$CDM cosmological models from both Planck and KiDS-1000. We demonstrate the capacity of CMB-weak lensing cross-correlations to set constraints on either the redshift or shear calibration, by analysing a previously unused high-redshift KiDS galaxy sample $(1.2<z_{\rm B}<2)$, with the cross-correlation detected at a significance of $7\sigma$. This analysis provides an independent assessment for the accuracy of redshift measurements in a regime that is challenging to calibrate directly owing to known incompleteness in spectroscopic surveys. ","Strong detection of the CMB lensingxgalaxy weak lensingcross-correlation
  from ACT-DR4,PlanckLegacy and KiDS-1000"
47,1331328329195065348,1012448981207613440,Nisheeth Vishnoi,['A new coreset construction for regression problems on panel data -- size essentially independent of the number of entities and the time! @NeurIPSConf @csi_yale\n\nJoint w/ Lingxiao Huang and @YaleSOM colleague @ksudhir1 \n\nBlog <LINK>\n\nPaper <LINK> <LINK>'],https://arxiv.org/abs/2011.00981,"This paper introduces the problem of coresets for regression problems to panel data settings. We first define coresets for several variants of regression problems with panel data and then present efficient algorithms to construct coresets of size that depend polynomially on 1/$\varepsilon$ (where $\varepsilon$ is the error parameter) and the number of regression parameters - independent of the number of individuals in the panel data or the time units each individual is observed for. Our approach is based on the Feldman-Langberg framework in which a key step is to upper bound the ""total sensitivity"" that is roughly the sum of maximum influences of all individual-time pairs taken over all possible choices of regression parameters. Empirically, we assess our approach with synthetic and real-world datasets; the coreset sizes constructed using our approach are much smaller than the full dataset and coresets indeed accelerate the running time of computing the regression objective. ",Coresets for Regressions with Panel Data
48,1331270116290523136,891766035250122757,Rosie Talbot,"['Very excited to share my first paper which is out on arXiv today! \nI describe a new numerical model for AGN jets that are powered by the spin of the black hole and test it in simulations of the central regions of a Seyfert galaxy. Find it here: <LINK> <LINK>', '@gusbeane I just interpolated the sim output onto a regular grid. If the grid is sufficiently fine this picks out AREPO’s voronoi cells :)']",https://arxiv.org/abs/2011.10580,"Jets launched by active galactic nuclei (AGN) are believed to play a significant role in shaping the properties of galaxies and provide an energetically viable mechanism through which galaxies can become quenched. Here we present a novel AGN feedback model, which we have incorporated into the AREPO code, that evolves the black hole mass and spin as the accretion flow proceeds through a thin $\alpha$-disc which we self-consistently couple to a Blandford-Znajek jet. We apply our model to the central region of a typical radio-loud Seyfert galaxy embedded in a hot circumgalactic medium (CGM). We find that jets launched into high pressure environments thermalise efficiently due to the formation of recollimation shocks and the vigorous instabilities that these shocks excite increase the efficiency of the mixing of CGM and jet material. The beams of more overpressured jets, however, are not as readily disrupted by instabilities so the majority of the momentum flux at the jet base is retained out to the head, where the jet terminates in a reverse shock. All jets entrain a significant amount of cold circumnuclear disc material which, while energetically insignificant, dominates the lobe mass together with the hot, entrained CGM material. The jet power evolves significantly due to effective self-regulation by the black hole, fed by secularly-driven, intermittent mass flows. The direction of jets launched directly into the circumnuclear disc changes considerably due to effective Bardeen-Petterson torquing. Interestingly, these jets obliterate the innermost regions of the disc and drive large-scale, multi-phase, turbulent, bipolar outflows. ","Blandford-Znajek jets in galaxy formation simulations: method and
  implementation"
49,1331266603217002497,990346198908223488,Nikolai Matni,"['New paper with @AntonXue on data-driven versions of System Level Synthesis!  By making connections to behavioral systems theory, we show that all you need are libraries of past system trajectories to optimize over system responses! <LINK>', 'For noise free data we show exact equivalence between traditional and data-driven SLS. When the system is driven by noise, we use robust SLS to quantify the effects of noise and use tools from matrix concentration to show that trajectory averaging can mitigate these effects.', ""@YangZhe46859983 @AntonXue I see no barrier to extending this to the output feedback setting and no need for open loop stability: we're working on this now. We were heavily inspired by @florian_dorfler's DeePC work, which is all output feedback and doesn't require open loop stability.""]",https://arxiv.org/abs/2011.10674,"We establish data-driven versions of the System Level Synthesis (SLS) parameterization of achievable closed-loop system responses for a linear-time-invariant system over a finite-horizon. Inspired by recent work in data-driven control that leverages tools from behavioral theory, we show that optimization problems over system-responses can be posed using only libraries of past system trajectories, without explicitly identifying a system model. We first consider the idealized setting of noise free trajectories, and show an exact equivalence between traditional and data-driven SLS. We then show that in the case of a system driven by process noise, tools from robust SLS can be used to characterize the effects of noise on closed-loop performance, and further draw on tools from matrix concentration to show that a simple trajectory averaging technique can be used to mitigate these effects. We end with numerical experiments showing the soundness of our methods. ",Data-Driven System Level Synthesis
50,1331163565374529538,335941225,Saeed Jahromi,['Check out our new paper with @OrusRoman:\n\nThermodynamics of 3D Kitaev quantum spin liquids via tensor networks\n\n<LINK> <LINK>'],https://arxiv.org/abs/2011.11577,"We study the 3D Kitaev and Kitaev-Heisenberg models respectively on the hyperhoneycomb and hyperoctagon lattices, both at zero and finite-temperature, in the thermodynamic limit. Our analysis relies on advanced tensor network (TN) simulations based on graph Projected Entangled-Pair States (gPEPS). We map out the TN phase diagrams of the models and characterize their underlying gapped and gapless phases both at zero and finite temperature. In particular, we demonstrate how cooling down the hyperhoneycomb system from high-temperature leads to fractionalization of spins to itinerant Majorana fermions and gauge fields that occurs in two separate temperature regimes, leaving their fingerprint on specific heat as a double-peak feature as well as on other quantities such as the thermal entropy, spin-spin correlations and bond entropy. Using the Majorana representation of the Kitaev model, we further show that the low-temperature thermal transition to the Kitaev quantum spin liquid (QSL) phase is associated with the non-trivial Majorana band topology and the presence of Weyl nodes, which manifests itself via non-vanishing Chern number and finite thermal Hall conductivity. Beyond the pure Kitaev limit, we study the 3D Kitaev-Heisenberg (KH) model on the hyperoctagon lattice and extract the full phase diagram for different Heisenberg couplings. We further explore the thermodynamic properties of the magnetically-ordered regions in the KH model and show that, in contrast to the QSL phase, here the thermal phase transition follows the standard Landau symmetry-breaking theory. ",Thermodynamics of 3D Kitaev quantum spin liquids via tensor networks
51,1331102851637022720,4716962310,Li Junnan,"['Excited to introduce CoMatch, our new semi-supervised learning method! CoMatch jointly learns class probability and image representation with graph-based contrastive learning. @CaimingXiong @stevenhoi \nBlog: <LINK>\nPaper: <LINK>', 'CoMatch advances both semi-supervised classification and representation learning. With only 1% of labeled ImageNet training samples, CoMatch achieves a state-of-the-art top1-accuracy of 66%!']",https://arxiv.org/abs/2011.11183,"Semi-supervised learning has been an effective paradigm for leveraging unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a new semi-supervised learning method that unifies dominant approaches and addresses their limitations. CoMatch jointly learns two representations of the training data, their class probabilities and low-dimensional embeddings. The two representations interact with each other to jointly evolve. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels, whereas the pseudo-labels regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves state-of-the-art performance on multiple datasets. It achieves substantial accuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1% labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch by 12.6%. Furthermore, CoMatch achieves better representation learning performance on downstream tasks, outperforming both supervised learning and self-supervised learning. Code and pre-trained models are available at this https URL ",CoMatch: Semi-supervised Learning with Contrastive Graph Regularization
52,1329850091226673154,1128608544431861760,Wei Jin,"['#𝗠𝗮𝗰𝗵𝗶𝗻𝗲𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 Our WSDM\'21 paper ""Node Similarity Preserving Graph Convolutional Networks"" is now available on <LINK>! We propose a new GNN framework to effectively and efficiently preserve node similarity while exploiting graph structure. <LINK>']",https://arxiv.org/abs/2011.09643,"Graph Neural Networks (GNNs) have achieved tremendous success in various real-world applications due to their strong ability in graph representation learning. GNNs explore the graph structure and node features by aggregating and transforming information within node neighborhoods. However, through theoretical and empirical analysis, we reveal that the aggregation process of GNNs tends to destroy node similarity in the original feature space. There are many scenarios where node similarity plays a crucial role. Thus, it has motivated the proposed framework SimP-GCN that can effectively and efficiently preserve node similarity while exploiting graph structure. Specifically, to balance information from graph structure and node features, we propose a feature similarity preserving aggregation which adaptively integrates graph structure and node features. Furthermore, we employ self-supervised learning to explicitly capture the complex feature similarity and dissimilarity relations between nodes. We validate the effectiveness of SimP-GCN on seven benchmark datasets including three assortative and four disassorative graphs. The results demonstrate that SimP-GCN outperforms representative baselines. Further probe shows various advantages of the proposed framework. The implementation of SimP-GCN is available at \url{this https URL}. ",Node Similarity Preserving Graph Convolutional Networks
53,1329841877546192901,342079808,Elisa Mantelli,"[""Hey! Check out Christian Schoof's and mine new paper on ice stream formation! We study the spontaneous birth of ice streams, with new theory and cool simulations.\n\nLink: <LINK> (in review on PRSA)""]",https://arxiv.org/abs/2011.01996,"Ice streams are bands of fast-flowing ice in ice sheets. We investigate their formation as an example of spontaneous pattern formation, based on positive feedbacks between dissipation and basal sliding. Our focus is on temperature-dependent subtemperate sliding, where faster sliding leads to enhanced dissipation and hence warmer temperatures, weakening the bed further, although we also treat a hydromechanical feedback mechanism that operates on fully molten beds. We develop a novel thermomechanical model capturing ice-thickness scale physics in the lateral direction while assuming the the flow is shallow in the main downstream direction. Using that model, we show that formation of a steady-in-time pattern can occur by the amplification in the downstream direction of noisy basal conditions, and often leads to the establishment of a clearly-defined ice stream separated from slowly-flowing, cold-based ice ridges by narrow shear margins, with the ice stream widening in the downstream direction. We are able to show that downward advection of cold ice is the primary stabilizing mechanism, and give an approximate, analytical criterion for pattern formation. ",Ice stream formation
54,1329819466436202498,919538082,samantha scibelli,['NEW paper!! Undergraduate student Hannah Ambrose in our group @azstewobs has published her paper on deuterated methanol (CH2DOH) detections in prestellar cores! Check it out!! #astrochemistry #chemistry #astronomynews <LINK> <LINK>'],https://arxiv.org/abs/2011.08957,"Recent observations indicate that organic molecules are prevalent towards starless and prestellar cores. Deuteration of these molecules has not been well-studied during the starless phase. Published observations of singly-deuterated methanol, CH$_2$DOH, have only been observed in a couple of well-studied, dense and evolved prestellar cores (e.g. L1544, L183). Since the formation of gas-phase methanol during this cold phase is believed to occur via desorption from the icy grain surfaces, observations of CH$_2$DOH may be useful as a probe of the deuterium fraction in the ice mantles of dust grains. We present a systematic survey of CH$_2$DOH towards 12 starless and prestellar cores in the B10 region of the Taurus Molecular Cloud. Nine of the twelve cores are detected with [CH$_2$DOH]/[CH$_3$OH] ranging from $< 0.04$ to $0.23^{+0.12}_{-0.06}$ with a median value of $0.11$. Sources not detected tend to have larger virial parameters and larger methanol linewidths than detected sources. The results of this survey indicate that deuterium fractionation of organic molecules, such as methanol, during the starless phase may be more easily detectable than previously thought. ","A Survey of CH2DOH Towards Starless and Prestellar Cores in the Taurus
  Molecular Cloud"
55,1329742265871265794,973860795246198784,Z.Wei,['Our new paper is on arXiv! A lot of thanks to my collaborators! They are fantastic!\n<LINK> <LINK>'],https://arxiv.org/abs/2011.09648,"Pseudo entropy is an interesting quantity with a simple gravity dual, which generalizes entanglement entropy such that it depends on both an initial and a final state. Here we reveal the basic properties of pseudo entropy in quantum field theories by numerically calculating this quantity for a set of two-dimensional free scalar field theories and the Ising spin chain. We extend the Gaussian method for pseudo entropy in free scalar theories with two parameters: mass $m$ and dynamical exponent $z$. This computation finds two novel properties of Pseudo entropy which we conjecture to be universal in field theories, in addition to an area law behavior. One is a saturation behavior and the other one is non-positivity of the difference between pseudo entropy and averaged entanglement entropy. Moreover, our numerical results for the Ising chain imply that pseudo entropy can play a role as a new quantum order parameter which detects whether two states are in the same quantum phase or not. ",Pseudo Entropy in Free Quantum Field Theories
56,1329725647468572678,526872160,Pablo Moreno-Muñoz,"['A novel application of the probabilistic ML methods that we develop is focused on mental health.\n\nHeterogeneous data, latent variables, change-point detection, all that is put into practice in our new paper at the #ML4MH Workshop @ NeurIPS 2020\n\npaper: <LINK> <LINK>', 'Fantastic collaboration with A. Moreno, L. Romero-Medrano and J. Herrera-López. Also thanks to @eBasedBehavior, @aar_2009 and E. Baca-García for the support.']",http://arxiv.org/abs/2011.09848,"More than one million people commit suicide every year worldwide. The costs of daily cares, social stigma and treatment issues are still hard barriers to overcome in mental health. Most symptoms of mental disorders are related to the behavioral state of a patient, such as the mobility or social activity. Mobile-based technologies allow the passive collection of patients data, which supplements conventional assessments that rely on biased questionnaires and occasional medical appointments. In this work, we present a non-invasive machine learning (ML) model to detect behavioral shifts in psychiatric patients from unobtrusive data collected by a smartphone app. Our clinically validated results shed light on the idea of an early detection mobile tool for the task of suicide attempt prevention. ",Passive detection of behavioral shifts for suicide attempt prevention
57,1329547654293053442,2585907650,Leslie Smith,"['Check out my new paper ""FROST: Faster and more Robust One-shot Semi-supervised Training"" at <LINK>.  FROST is cool and it is the most practical method for one-shot semi-supervised training.']",https://arxiv.org/abs/2011.09471,"Recent advances in one-shot semi-supervised learning have lowered the barrier for deep learning of new applications. However, the state-of-the-art for semi-supervised learning is slow to train and the performance is sensitive to the choices of the labeled data and hyper-parameter values. In this paper, we present a one-shot semi-supervised learning method that trains up to an order of magnitude faster and is more robust than state-of-the-art methods. Specifically, we show that by combining semi-supervised learning with a one-stage, single network version of self-training, our FROST methodology trains faster and is more robust to choices for the labeled samples and changes in hyper-parameters. Our experiments demonstrate FROST's capability to perform well when the composition of the unlabeled data is unknown; that is when the unlabeled data contain unequal numbers of each class and can contain out-of-distribution examples that don't belong to any of the training classes. High performance, speed of training, and insensitivity to hyper-parameters make FROST the most practical method for one-shot semi-supervised training. Our code is available at this https URL ",FROST: Faster and more Robust One-shot Semi-supervised Training
58,1329545666184441856,735269593443274753,Dr. Jennifer Burt,"['New paper out today by me and the extended Lick-Carnegie Exoplanet team!! Three new planets orbiting two nearby stars -- two around HD 216520 &amp; one around HD 190007)  <LINK>', 'These discoveries  build on some early suggestions of Keplerian signals from the HIRES data release in Butler et al. 2017, which we then followed up with the Automated Planet Finder (my favorite robotic RV facility &lt;3) https://t.co/O1L9DFv7kq', 'All three of the new planets are in the super-Earth/sub-Neptune mass regime (10-16 Earth masses), and two of them are on relatively short periods: ~12 days for HD 190007 b, and 35 days for HD 216520 b. HD 216520 c, meanwhile, is further out with an orbital period of 154 days https://t.co/yDN1z6ogAW', ""It's a little surprising that these systems don't seem to contain a similar planets on nearby orbits... https://t.co/WHwegmxKNz"", 'B/c out of the 136 Kepler and K2 sub-Neptune mass exoplanets that have orbital periods less than 100 days, 85% are in multi-planet systems. And HD 190007 b &amp; HD 216520 b sit in a similar mass/period regime https://t.co/cBykgxXSyx', 'But our injection/recovery analysis suggests that there\'s not a whole lot of room left for additional planets to hide - certainly not within the factor of 2 in period/mass that we\'d expect for a ""similar"" planet (which excludes HD 216520 c as well) https://t.co/KGRymLzEls', 'We also provided some updated orbital parameters to GJ 686 b and HD 180617 b using data from the APF &amp; PFS -- all in good agreement with the original discoveries by Affers+2019, Lalitha+2019, and Kaminski+2018 https://t.co/1jbQOM3iWR', ""So all in all, we've got a nice set of new and improved planets, making use of data from both HIRES and the APF, that we're super excited to add to the RV planet landscape! https://t.co/t1aWxG70Do""]",https://arxiv.org/abs/2011.08867,"Analysis of new precision radial velocity (RV) measurements from the Lick Automated Planet Finder (APF) and Keck HIRES have yielded the discovery of three new exoplanet candidates orbiting two nearby K dwarfs not previously reported to have companions (HD 190007 & HD 216520). We also report new velocities from both the APF and the Planet Finder Spectrograph (PFS) for the previously reported planet host stars GJ 686 and HD 180617 and update the corresponding exoplanet orbital models. Of the newly discovered planets, HD 190007 b has a period of 11.72 days, an RV semi-amplitude of K = 5.64$\pm$0.55 m s$^{-1}$, a minimum mass of 16.46$\pm$1.66 $\rm M_{\oplus}$, and orbits the slightly metal-rich, active K4 dwarf star HD 190007 (d = 12.7 pc). HD 216520 b has an orbital period of 35.45 days, an RV semi-amplitude of K = 2.28$\pm$0.20 m s$^{-1}$, and a minimum mass of 10.26$\pm$0.99 $\rm M_{\oplus}$, while HD 216520 c has an orbital period of P = 154.43 days, an RV semi-amplitude of K = 1.29$\pm0.22$ m s$^{-1}$, and a minimum mass of 9.44$\pm$1.63 $\rm M_{\oplus}$. Both of these planets orbit the slightly metal-poor, inactive K0 dwarf star HD 216520 (d = 19.6 pc). We find that our updated best fit models for HD 180617 b and GJ 686 b are in good agreement with the previously published results. For HD 180617 b we obtain an orbital period of 105.91 days, an RV semi-amplitude of K = 2.696$\pm$0.22 m s$^{-1}$, and a minimum mass of 2.214$\pm$1.05 $\rm M_{\oplus}$. For GJ 686 b we find the orbital period to be 15.53 days, the RV semi-amplitude to be K = 3.00$\pm$0.18 m s$^{-1}$, and the minimum mass to be 6.624$\pm$0.432 $\rm M_{\oplus}$. Using an injection-recovery exercise, we find that HD 190007 b and HD 216520 b are unlikely to have additional planets with masses and orbital periods within a factor of two, in marked contrast to $\sim$85\% of planets in this mass and period range found with Kepler. ","A collage of small planets from the Lick Carnegie Exoplanet Survey :
  Exploring the super-Earth and sub-Neptune mass regime"
59,1329481995542552579,1240355312,Yu Su,"['GrailQA: A new large-scale, high-quality dataset for QA on knowledge bases\n\n- 64K questions over 3.7K relations across 86 domains\n- Support evaluation of i.i.d., compositional, and zero-shot generalization\n\npaper: <LINK>\nleaderboard: <LINK>', 'My student @yugu_nlp will discuss the work at the Interactive and Executable Semantic Parsing Workshop@EMNLP (@intexsempar2020) at 23:00 (UTC) / 18:00 (EST).\n\nJoint work with Sue Kase, Michelle Vanni, Brian Sadler, @percyliang, Xifeng Yan', 'We also propose new BERT-based KBQA models and demonstrate, for the first time, the critical role of joint contextualized representation of utterances and KB schema for non-i.i.d. generalization of KBQA models! More details in the paper.']",https://arxiv.org/abs/2011.07743,"Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d may be neither reasonably achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sample training examples from the enormous space would be highly data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d, compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA. ","Beyond I.I.D.: Three Levels of Generalization for Question Answering on
  Knowledge Bases"
60,1329420894083579906,502864087,Alham Fikri Aji,"['Introducing a new informal-formal Indonesian parallel corpus. We collect informal texts from Twitter, which are then annotated into the formal form. We also explore seq2seq techs for informal-formal style transfer.\n\ndata: <LINK>\npaper: <LINK> <LINK>', 'This research was carried out by the @KataDotAI research team, in collaboration with a researcher and interns from UI and Binus. Great work @haryoaw and team.']",https://arxiv.org/abs/2011.03286v1,"In its daily use, the Indonesian language is riddled with informality, that is, deviations from the standard in terms of vocabulary, spelling, and word order. On the other hand, current available Indonesian NLP models are typically developed with the standard Indonesian in mind. In this work, we address a style-transfer from informal to formal Indonesian as a low-resource machine translation problem. We build a new dataset of parallel sentences of informal Indonesian and its formal counterpart. We benchmark several strategies to perform style transfer from informal to formal Indonesian. We also explore augmenting the training set with artificial forward-translated data. Since we are dealing with an extremely low-resource setting, we find that a phrase-based machine translation approach outperforms the Transformer-based approach. Alternatively, a pre-trained GPT-2 fined-tuned to this task performed equally well but costs more computational resource. Our findings show a promising step towards leveraging machine translation models for style transfer. Our code and data are available in this https URL ","] Semi-Supervised Low-Resource Style Transfer of Indonesian Informal to
  Formal Language with Iterative Forward-Translation"
61,1329401288761761792,1134375290581524480,Kai Schmitz,"['Wash-In Leptogenesis [<LINK>] New paper on the arXiv, in collaboration with Valerie Domcke, @KoheiKamadaPhys , Kyohei Mukaida, &amp; Masaki Yamada, in which we generalize standard freeze-out leptogenesis to an arbitrary ""chemical background"" in the early Universe! 🥳 <LINK>']",https://arxiv.org/abs/2011.09347,"We present a leptogenesis mechanism based on the standard type-I seesaw model that successfully operates at right-handed-neutrino masses as low as a few 100 TeV. This mechanism, which we dub ""wash-in leptogenesis"", does not require any CP violation in the neutrino sector and can be implemented even in the regime of strong wash-out. The key idea behind wash-in leptogenesis is to generalize standard freeze-out leptogenesis to a nonminimal cosmological background in which the chemical potentials of all particles not in chemical equilibrium at the temperature of leptogenesis are allowed to take arbitrary values. This sets the stage for building a plethora of new baryogenesis models where chemical potentials generated at high temperatures are reprocessed to generate a nonvanishing B-L asymmetry at low temperatures. As concrete examples, we discuss wash-in leptogenesis after axion inflation and in the context of grand unification. ",Wash-In Leptogenesis
62,1329376942378934274,226900035,Prof B Buchanan OBE,['New paper ... @pavlosatnapier .. <LINK> <LINK>'],https://arxiv.org/abs/2011.09260,"Electronic health record (EHR) management systems require the adoption of effective technologies when health information is being exchanged. Current management approaches often face risks that may expose medical record storage solutions to common security attack vectors. However, healthcare-oriented blockchain solutions can provide a decentralized, anonymous and secure EHR handling approach. This paper presents PREHEALTH, a privacy-preserving EHR management solution that uses distributed ledger technology and an Identity Mixer (Idemix). The paper describes a proof-of-concept implementation that uses the Hyperledger Fabric's permissioned blockchain framework. The proposed solution is able to store patient records effectively whilst providing anonymity and unlinkability. Experimental performance evaluation results demonstrate the scheme's efficiency and feasibility for real-world scale deployment. ",A Privacy-Preserving Healthcare Framework Using Hyperledger Fabric
63,1329360036766904321,720027280051957761,Oliver Newton,"['New paper is out! It feels good to finally get this one out the door.\n\nHere, we build on previous work to constrain the properties of warm dark matter models using the satellite galaxies of the MW. We also end up with a lower bound on the MW halo mass\n\n<LINK>\n1/7', ""In the first part, we compare the predicted abundance of DM subhaloes in MW haloes with the pop. of MW satellites. WDM models that can't produce enough substructure to host the MW dwarf galaxies are ruled out.\n\nFor thermal relic DM, particle masses below 2 keV aren't viable.\n2/7 https://t.co/l4VSXoxqkh"", 'This is a conservative robust lower limit on the thermal relic particle mass that is independent of assumptions about galaxy formation processes.\n\nWe can improve the result by modelling these. In particular, reionization is important as it suppresses dwarf galaxy formation.\n3/7', ""We use Galform (@DarkerMatters ) to model these processes and explore several reionization prescriptions.\n\nIn our fiducial scenario, we rule out thermal relic DM with mass below 4 keV. We also find that even the coldest models can't be ruled out if the MW mass is too low.\n4/7 https://t.co/kDQhrXdSrK"", 'This is true when the WDM is very cold and differs little from CDM, so we interpret this as a lower bound on the MW halo mass in CDM.\n\nReionization that finishes earlier and models preventing subsequent gas cooling into the smallest galaxies produce the strongest constraints.\n5/7', 'We also discuss a few technical challenges with analyses of this type and compare with other constraints in the literature from different techniques.\n\nWe are applying this approach to other WDM models so stay tuned for those results!\n\n6/7', 'Of course, thanks must go to my co-authors and collaborators whose sage advice and insight brought this work to a good conclusion!\n\n7/7']",https://arxiv.org/abs/2011.08865,"The satellite galaxies of the Milky Way (MW) are effective probes of the underlying dark matter (DM) substructure, which is sensitive to the nature of the DM particle. In particular, a class of DM models have a power spectrum cut-off on the mass scale of dwarf galaxies and thus predict only small numbers of substructures below the cut-off mass. This makes the MW satellite system appealing to constrain the DM properties: feasible models must produce enough substructure to host the number of observed Galactic satellites. Here, we compare theoretical predictions of the abundance of DM substructure in thermal relic warm DM (WDM) models with estimates of the total satellite population of the MW. This produces conservative robust lower limits on the allowed mass, $m_\mathrm{th}$, of the thermal relic WDM particle. As the abundance of satellite galaxies depends on the MW halo mass, we marginalize over the corresponding uncertainties and rule out $m_\mathrm{th} \leq 2.02\, \mathrm{keV}$ at 95 per cent confidence independently of assumptions about galaxy formation processes. Modelling some of these - in particular, the effect of reionization, which suppresses the formation of dwarf galaxies - strengthens our constraints on the DM properties and excludes models with $m_\mathrm{th} \leq 3.99\, \mathrm{keV}$ in our fiducial model. We also find that thermal relic models cannot produce enough satellites if the MW halo mass is $M_{200}\leq 0.6\times 10^{12}\, \mathrm{M_\odot}$, which imposes a lower limit on the MW halo mass in CDM. We address several observational and theoretical uncertainties and discuss how improvements in these will strengthen the DM mass constraints. ","Constraints on the properties of warm dark matter using the satellite
  galaxies of the Milky Way"
64,1329332536380698626,1125141282861588480,Matthias Walter,"['Just finished the paper with @MarijeSiemann about her MSc thesis at @UTwente with a first polyhedral study on the traveling tournament problem where we lifted model inequalities and found some new ones. Results are easy to parse, but proofs are technical: <LINK>']",https://arxiv.org/abs/2011.09135,"We consider the unconstrained traveling tournament problem, a sports timetabling problem that minimizes traveling of teams. Since its introduction about 20 years ago, most research was devoted to modeling and reformulation approaches. In this paper we carry out a polyhedral study for the cubic integer programming formulation by establishing the dimension of the integer hull as well as of faces induced by model inequalities. Moreover, we introduce a new class of inequalities and show that they are facet-defining. Finally, we evaluate the impact of these inequalities on the linear programming bounds. ","A Polyhedral Study for the Cubic Formulation of the Unconstrained
  Traveling Tournament Problem"
65,1329322561830793217,84822915,Itamar Caspi,"['🚨*New working paper alert* 🚨\n\n""Predicting Disaggregated CPI Inflation Components via Hierarchical Recurrent Neural Networks""\n\nwith Oren Barkan, Allon Hammer, and Noam Koenigstein\n\n<LINK> <LINK>', 'The code and data are available at: \n\nhttps://t.co/KXqWJeCdHV', '@NirGodin שמח לשמוע!']",https://arxiv.org/abs/2011.07920,"We present a hierarchical architecture based on Recurrent Neural Networks (RNNs) for predicting disaggregated inflation components of the Consumer Price Index (CPI). While the majority of existing research is focused mainly on predicting the inflation headline, many economic and financial entities are more interested in its partial disaggregated components. To this end, we developed the novel Hierarchical Recurrent Neural Network (HRNN) model that utilizes information from higher levels in the CPI hierarchy to improve predictions at the more volatile lower levels. Our evaluations, based on a large data-set from the US CPI-U index, indicate that the HRNN model significantly outperforms a vast array of well-known inflation prediction baselines. ","Forecasting CPI Inflation Components with Hierarchical Recurrent Neural
  Networks"
66,1329250146706604034,1015053310603284480,Stephen Kane,"['Today, I\'m excited to tell you about a new paper from myself, Zhexing Li, @storyofthewolf1, @ColbyOstberg, and @michelle_hill63 entitled ""Eccentricity Driven Climate Effects in the Kepler-1649 System"". You can access it at the below link.\n<LINK>', 'Back in 2017, @IsabelNAngelo published a paper that announced the discovery of a potential Venus analog in the Kepler-1649 system, confirmed by climate simulations I published in 2018. More recently, @amvanderburg found another terrestrial planet in the Habitable Zone (HZ).', 'Our new paper explores 1. Could there be another planet in a stable orbit between the b and c planets, 2. What are the possible eccentricities of the known planets, and 3. How would eccentricity influence the climate of the HZ planet.', 'For question 1, we found that there are indeed stable regions for an additional terrestrial planet between b and c, and they occur predominantly at the location that would produce a 3:2 resonant chain for the three planets.', 'For question 2, the orbits of the two known planets are very sensitive to each others eccentricities, and the period of eccentricity oscillation for the planets can be as low as 1000 years for an eccentricity of 0.3, where the incident flux by changes by +/-30%.', 'For question 3, our climate simulations show that, despite large changes in flux, and a high frequency oscillating eccentricity, the surface of the HZ planet can maintain long-term stable climates for a variety of initial climate configurations.', 'In summary, systems like Kepler-1649 are exciting because they provide potential Venus/Earth analogs in terms of their size and incident flux, but with very different orbit and climate evolutionary histories.', 'The good news is that significant angular momentum exchange between such planets, even with high frequency, need not bring a catastrophic change to a temperate surface environment. Remember that the next time you complain about the winter/summer temperature differential on Earth!', '@amvanderburg @IsabelNAngelo Thanks for the clarification Andrew!']",https://arxiv.org/abs/2011.09074,"The discovery of terrestrial exoplanets is uncovering increasingly diverse architectures. Of particular interest are those systems that contain exoplanets at a variety of star-planet separations, allowing direct comparison of exoplanet evolution (comparative planetology). The Kepler-1649 system contains two terrestrial planets similar both in size and insolation flux to Venus and Earth, although their eccentricities remain largely unconstrained. Here we present results of dynamical studies of the system and the potential effects on climate. The eccentricities of the Kepler-1649 system are poorly constrained, and we show that there are dynamically viable regions for further terrestrial planets in between the two known planets for a limited range of eccentricities. We investigate the effect of eccentricity of the outer planet on the dynamics of both planets and show that this results in high-frequency (1000-3000 year) eccentricity oscillations in long-term stable configurations. We calculate the resulting effect of these eccentricity variations on insolation flux and present the results of 3D climate simulations for the habitable zone planet. Our simulations demonstrate that, despite large eccentricity variations, the planet can maintain stable climates with relatively small temperature variations on the substellar hemisphere for a variety of initial climate configurations. Such systems thus provide key opportunities to explore alternative Venus/Earth climate evolution scenarios. ",Eccentricity Driven Climate Effects in the Kepler-1649 System
67,1329176413677756417,15751681,Irene Celino,['#DBLP added a new #paper <LINK> to my page <LINK>'],https://arxiv.org/abs/2011.06423,"Complying with the EU Regulation on multimodal transportation services requires sharing data on the National Access Points in one of the standards (e.g., NeTEx and SIRI) indicated by the European Commission. These standards are complex and of limited practical adoption. This means that datasets are natively expressed in other formats and require a data translation process for full compliance. This paper describes the solution to turn the authoritative data of three different transport stakeholders from Italy and Spain into a format compliant with EU standards by means of Semantic Web technologies. Our solution addresses the challenge and also contributes to build a multi-modal transport Knowledge Graph of interlinked and interoperable information that enables intelligent querying and exploration, as well as facilitates the design of added-value services. ","Turning Transport Data to Comply with EU Standards while Enabling a
  Multimodal Transport Knowledge Graph"
68,1329108047575347206,1048984881131401217,Cora Dvorkin,['The comment below is about our new paper: <LINK> <LINK>'],https://arxiv.org/abs/2011.08186,"Dark matter (DM) could be a relic of freeze-in through a light mediator, where the DM is produced by extremely feeble, IR-dominated processes in the thermal Standard Model plasma. In the simplest viable models with the DM mass below the MeV scale, the DM has a small effective electric charge and is born with a nonthermal phase-space distribution. This DM candidate would cause observable departures from standard cosmological evolution. In this work, we combine data from the cosmic microwave background (CMB), Lyman-$\alpha$ forest, quasar lensing, stellar streams, and Milky Way satellite abundances to set a lower limit on freeze-in DM masses up to $\sim 20\,$keV, with the exact constraint depending on whether the DM thermalizes in its own sector. We perform forecasts for the CMB-S4 experiment, the Hydrogen Epoch of Reionization Array, and the Vera Rubin Observatory, finding that freeze-in DM masses up to $\sim 80\,$keV can be explored. These cosmological probes are highly complementary with proposed direct-detection efforts to search for this DM candidate. ",The cosmology of sub-MeV dark matter freeze-in
69,1329039662074236930,1556664198,Kyle Cranmer,"['New paper! \nHierarchical clustering in particle physics through reinforcement learning\nwith Johann Brehmer, Sebastian Macaluso and Duccio Pappadopulo (@ducciolvp)\n\nOne of the few papers using reinforcement learning for particle physics.\n<LINK>\n\nthread 👇 <LINK>', 'The most commonly produced object at the Large Hadron Collider is a “jet”. It is a collection of particles that emerged from quark or gluon through a tree-like cascade of radiation and decays called the “parton shower"". https://t.co/6wW7Y3bnb9', 'We only observe the leaves of the tree (the stable particles) and the rest of the tree is unobserved (a latent variable). https://t.co/Jfsza3v3TI', 'A common tasks that we have is to try to invert this process, &amp; estimate the showering history that gave rise to the particles we observe. This is usually called “jet reconstruction”, &amp; the most common approach to this is the an anti-kT algorithm (cited &gt;7000 of times!) https://t.co/5HXaKImut7', ""From a data-science / statistics / machine learning point of view, this can be seen as a hierarchical clustering problem. Furthermore, the anti-kT algorithm (as well as the kT and C/A algorithms for the experts) are all agglomerative clustering algorithms.... cont'd https://t.co/7qORCBm8S4"", '... agglomerative clustering algorithms, which iteratively cluster particles from “the bottom up"". And the choice of which particles to cluster next is based on a physically motivated notion of “distance”… but it is still a bit ad hoc from a statistical point of view.', 'However, we have a generative model for the jets. So in principle we have a likelihood function or probability model for the process. Some of our recent work is to reframe jet physics in statistical terms. https://t.co/K8zpNDSjob', 'A few years ago @kchonyc, @glouppe, C. Becot, and I explored the analogy with jet physics and NLP in this paper \nhttps://t.co/mbk837YgwE\nThere we used the greedy algorithm to estimate the tree (which is analogous to a parse tree) and then used a TreeRNN on top of that structure. https://t.co/eOhG26bans', 'Now our focus has moved to improving our estimate of (or marginalization over) the latent tree-like structure itself Part of what makes this problem so challenging is the combinatorial growth in the number of trees.  For 11 particles there are roughly 600 Million possible trees! https://t.co/ul4th7OJd7', 'A naive, exhaustive search isn’t feasible. Maybe you can see how this feels a little bit like playing chess or Go… the moves are which particles to cluster next, there are an enormous number of possible games, and the reward is the final likelihood associated to the clustering.', 'We reformulated jet reconstruction as a Markov Decision Process and used Monte-Carlo Tree Search (MCTS) and Behavioral Cloning (BC) algorithms. https://t.co/Ef1mZfiuSl', 'We found that Monte-Carlo Tree Search (MCTS) and Behavioral Cloning (BC) algorithms could improve not only over the greedy algorithm, but also over beam search, though at the cost of additional computing time. https://t.co/bvHhOf1DSS', 'The greedy algorithm is fast, and the RL algorithm takes orders of magnitude more compute. However, the RL Algorithm improves on beam search even for the same compute cost. I’ll say that it wasn’t easy to get RL to improve over the domain-specific greedy baseline. https://t.co/ECsLkRbenP', 'You may notice that those plots have an entry that is even better than RL, which is labeled MLE. That’s the exact MLE, which you might think would be practically impossible to find given the enormous latent space / number of trees.', 'But in parallel work we’ve developed a dynamic programming algorithm based on a hierarchical trellis, which allows us to efficiently find the MLE (and compute the exact marginal likelihood p(x|θ). That’s described in this paper:\nhttps://t.co/6cWq5q7qim https://t.co/rj3jkVHfEI', 'You might also ask ""how do you know the likelihood”?\nIn principle our simulation tools (eg. Pythia, Sherpa, Herwig, etc.) that describe the parton shower describe a likelihood. In practice, they aren’t written in a way that exposes that likelihood model conveniently.', 'So to facilitate this research we’ve developed Ginkgo, which is a toy parton shower / generative model for jets with a tractable and convenient likelihood. It’s written in pyro and has been interfaced to OpenAI Gym. https://t.co/d9g3txfeS2', 'We wrote more about this broad set of emerging computational approaches to jet physics in this “letter of intent” for the US particle physics prioritization process called “Snowmass” here:\nhttps://t.co/ZROv1Ji9At', 'Finally, I should point out that previously RL has been used in a jet context with this paper\nJet Grooming through Reinforcement Learning by Stefano Carrazza and Frédéric A. Dreyer\nhttps://t.co/ymz8pXPlUw', 'That work did not have the probabilistic interpretation, but used a reward that aimed to improve mass resolution for boosted objects. Surprisingly, there isn’t much other use of reinforcement learning in particle physics data analysis.', 'I should also say that the focus of this work isn’t advancement in reinforcement learning, but reframing a common physics problem in that language, creating the “environment”, and applying those RL techniques to this problem.', '@unsorsodicorda We only have this in our short paper. I will reply more later, but with out some help the RL algorithms were struggling. That also makes this a potentially interesting benchmark. https://t.co/1JNUvQvxkA']",https://arxiv.org/abs/2011.08191,"Particle physics experiments often require the reconstruction of decay patterns through a hierarchical clustering of the observed final-state particles. We show that this task can be phrased as a Markov Decision Process and adapt reinforcement learning algorithms to solve it. In particular, we show that Monte-Carlo Tree Search guided by a neural policy can construct high-quality hierarchical clusterings and outperform established greedy and beam search baselines. ","Hierarchical clustering in particle physics through reinforcement
  learning"
70,1329008943562158080,875731106338926592,Thomas Seiller,"[""New paper with J. G. Simonsen from @DIKU_Institut: Agafonov's Theorem for finite and infinite alphabets and probability distributions different from equidistribution, <LINK> (@arxiv)."", 'Also available on @hal_fr: https://t.co/lHDlmac6Cu.']",https://arxiv.org/abs/2011.08552,"An infinite sequence over a finite alphabet {\Sigma} of symbols is called normal iff the limiting frequency of every finite string w exists and equals |{\Sigma}|^{|w|}. A celebrated theorem by Agafonov states that a sequence {\alpha} is normal iff every finite-state selector. Normality is generalised to arbitrary probability maps \mu: {\alpha} is is \mu-distributed if, for every finite string w, the limiting frequency of w in {\alpha} exists and equals \mu(w). Unlike normality, \mu-distributedness is not preserved by finite-state selectors for all probability maps \mu. This raises the question of how to characterize the probability maps \mu for which \mu-distributedness is preserved across finite-state selection, or equivalently, by selection by programs using constant space. We prove the following result: for any finite or countably infinite alphabet {\Sigma}, every finite-state selector over {\Sigma} selects a \mu-distributed sequence from every \mu-distributed sequence {\alpha} iff \mu is induced by a Bernoulli distribution on {\Sigma}, that is a probability distribution on the alphabet extended to words by taking the product. The primary -- and remarkable -- consequence of our main result is a complete characterization of the set of probability maps, on finite and infinite alphabets, for which Agafonov-type results hold. The main positive takeaway is that (the appropriate generalization of) Agafonov's Theorem holds for Bernoulli distributions (rather than just equidistributions) on both finite and countably infinite alphabets. As a further consequence, we obtain a result in the area of symbolic dynamical systems: the shift-invariant measures {\nu} on {\Sigma}^{\omega} such that any finite-state selector preserves the property of genericity for {\mu}, are exactly the positive Bernoulli measures. ","Agafonov's Theorem for finite and infinite alphabets and probability
  distributions different from equidistribution"
71,1328993641709395970,1234075207486312448,Muhtasham Oblokulov,"['New SOTA on BCI SSVEP spellers.\n\nOur new DNN achieves impressive information transfer rates (ITR) with only 0.4 seconds of stimulation: 265.23 bits/min on the benchmark and 196.59 bits/min on BETA dataset.\n\nPaper: <LINK>\nCode: <LINK>\n\n#bci #ssvep <LINK>', '@neuralink @elonmusk 👀\n\n#Neuralink #ElonMusk']",https://arxiv.org/abs/2011.08562,"Objective: Target identification in brain-computer interface (BCI) spellers refers to the electroencephalogram (EEG) classification for predicting the target character that the subject intends to spell. When the visual stimulus of each character is tagged with a distinct frequency, the EEG records steady-state visually evoked potentials (SSVEP) whose spectrum is dominated by the harmonics of the target frequency. In this setting, we address the target identification and propose a novel deep neural network (DNN) architecture. Method: The proposed DNN processes the multi-channel SSVEP with convolutions across the sub-bands of harmonics, channels, time, and classifies at the fully connected layer. We test with two publicly available large scale (the benchmark and BETA) datasets consisting of in total 105 subjects with 40 characters. Our first stage training learns a global model by exploiting the statistical commonalities among all subjects, and the second stage fine tunes to each subject separately by exploiting the individualities. Results: Our DNN achieves impressive information transfer rates (ITRs) on both datasets, 265.23 bits/min and 196.59 bits/min, respectively, with only 0.4 seconds of stimulation. The code is available for reproducibility at this https URL Conclusion: The presented DNN strongly outperforms the state-of-the-art techniques as our accuracy and ITR rates are the highest ever reported performance results on these datasets. Significance: Due to its unprecedentedly high speller ITRs and flawless applicability to general SSVEP systems, our technique has great potential in various biomedical engineering settings of BCIs such as communication, rehabilitation and control. ",A Deep Neural Network for SSVEP-based Brain-Computer Interfaces
72,1328967361265856513,1392935011,Ole-Chr. Granmo,"['New paper from @cairuia\'s talented PhD student Bimal Bhattarai!\n""Measuring the Novelty of Natural Language Text Using the Conjunctive Clauses of a #TsetlinMachine Text Classifier""\nBimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao \n<LINK> #MachineLearning <LINK>']",https://arxiv.org/abs/2011.08755,"Most supervised text classification approaches assume a closed world, counting on all classes being present in the data at training time. This assumption can lead to unpredictable behaviour during operation, whenever novel, previously unseen, classes appear. Although deep learning-based methods have recently been used for novelty detection, they are challenging to interpret due to their black-box nature. This paper addresses \emph{interpretable} open-world text classification, where the trained classifier must deal with novel classes during operation. To this end, we extend the recently introduced Tsetlin machine (TM) with a novelty scoring mechanism. The mechanism uses the conjunctive clauses of the TM to measure to what degree a text matches the classes covered by the training data. We demonstrate that the clauses provide a succinct interpretable description of known topics, and that our scoring mechanism makes it possible to discern novel topics from the known ones. Empirically, our TM-based approach outperforms seven other novelty detection schemes on three out of five datasets, and performs second and third best on the remaining, with the added benefit of an interpretable propositional logic-based representation. ","Measuring the Novelty of Natural Language Text Using the Conjunctive
  Clauses of a Tsetlin Machine Text Classifier"
73,1328908309445808128,105984261,Tsendsuren,['New paper on Fast Weight memory:\nLearning Associative Inference Using Fast Weight Memory <LINK>\n\nwith Imanol Schlag and Jürgen Schmidhuber.'],https://arxiv.org/abs/2011.07831v1,"Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling. ",] Learning Associative Inference Using Fast Weight Memory
74,1328784015822876672,1171853767710511104,Hiroaki_Hayashi,"['We release a new challenging *Multi-domain* *Aspect-based* summarization dataset: WikiAsp w/ AlphaSense and @gneubig.\nPaper: <LINK>\nData: <LINK>\nTask: Summarize references of a Wikipedia article into aspect-based summary (i.e. specific section) <LINK>', 'Our dataset includes a variety of domains (20 of them!), each of which exhibits unique characteristics. For example,  *Event* domain requires summaries to be coherent in terms of chronological order.', 'By the way, this is NOT presented at #emnlp2020 😅, but a to-appear TACL paper. We will present at a future *ACL conference.']",https://arxiv.org/abs/2011.07832,"Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp, a large-scale dataset for multi-domain aspect-based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events. ",WikiAsp: A Dataset for Multi-domain Aspect-based Summarization
75,1328768976533532679,1152655010871820288,Kevin Chen,"['New short paper documenting an easy recipe for incorporating machine learning into linear IV to boost instrument strength *essentially for free*: <LINK>\n\nJoint work with Daniel Chen and @econ_greg\n\n#EconTwitter #CausalTwitter', ""We've all learned two-stage least-squares (TSLS). The exclusion restriction in TSLS requires that the instrument W is *uncorrelated* with structural error U. This is a weak assumption and it may be reasonable to strengthen it to a conditional mean-zero assumption (2/n) https://t.co/GhKHIp0zhC"", 'If we do so, we would have a conditional moment restriction. Gary Chamberlain (1987) works out that the optimal instrument is a conditional mean E[Endogenous Treatment | Instrument].\n\nSo it suffices to do TSLS with the optimal instrument (3/n) https://t.co/7wbzH0jutI', 'But to do so we need to estimate the optimal instrument. Luckily, the opt. instrument looks like a prediction problem, something black-box machine learning is quite good at. Amounts to replacing the linear 1st stage with a black box, and using the fitted values as instruments 4/n', 'We might be concerned that the black box is not fun to work with theoretically, but sample-splitting provides a cheat code for proofs. \n\nInterestingly, things work *without* requiring rates of convergence (in semiparametrics, typically happy with o(n^-1/4) requirements) 5/n https://t.co/2DnTKGNNEC', ""Things get slightly dicier with exogenous covariates X, since it's probably not reasonable for nonlinear transforms v(X) to provide identifying variation for the structural coefficient. (v(X) could be valid if we take the model literally, but it's ident'n via funct'l form) (6/n) https://t.co/MIio4OA8WM"", 'Nevertheless, if we shut down the identification channel from v(X), we can still form a *sequential moment restriction*. Chamberlain (1992) works out optimal estimation *in a JBES comment*. The optimal instrument is again a function of conditional means =&gt; ML + sample-split https://t.co/SktYuNW3VF', 'What about weak IV? On each sample-split fold, (estimated optimal instrument, treatment, Y) is a just-identified linear IV problem. So Anderson-Rubin is valid and efficient on each fold, and weak IV robust confidence sets can be constructed by combining folds (8/n)', 'We apply this to a criminal justice setting where instruments are characteristics of a randomly assigned appellate judge. We find that TSLS suffer from a weak instrument problem (AR confidence sets are extremely wide or non-convex), but first-stage with random forest works (9/n) https://t.co/1WeRoGJpj5', 'The reason is that the instrument -&gt; treatment relationship may be complex, and linear predictors fail to capture the relationship very well, while ML can juice every last drop of variation in the instrument that helps predict treatment (10/n)', ""In fact, better treatment prediction translates directly to better estimates of the structural parameter. Sample-splitting provides an honest estimate of the out-of-sample prediction quality. There's little reason to not use the method that predicts well in the first stage! 11/n https://t.co/YoQY8IrQOj"", ""Ofc, in some settings OLS may well deliver high quality predictions (especially true if the instruments are binary or discrete), and in those settings fancier ML won't do much \n\n(this is partly what I think is going on in Machine Labor, https://t.co/fIyTqCgUM9) 12/n https://t.co/VOjflKzA58"", ""So, main pitch:\n- Use ML to juice as much variation from valid instruments as possible \n- OOS prediction quality on 1st stage is a good diagnostic\n- Compatible with Anderson-Rubin\n\nI had a blast working on this @MSFTResearch\nSee us @ NeurIPS'20 Workshop on ML for Econ Policy! n/n"", ""@DavidnLang @MSFTResearch A more comprehensive library is probably future work unfortunately, but I'm happy to share a github gist---it's not difficult to implement with existing tools""]",https://arxiv.org/abs/2011.06158,"We offer straightforward theoretical results that justify incorporating machine learning in the standard linear instrumental variable setting. The key idea is to use machine learning, combined with sample-splitting, to predict the treatment variable from the instrument and any exogenous covariates, and then use this predicted treatment and the covariates as technical instruments to recover the coefficients in the second-stage. This allows the researcher to extract non-linear co-variation between the treatment and instrument that may dramatically improve estimation precision and robustness by boosting instrument strength. Importantly, we constrain the machine-learned predictions to be linear in the exogenous covariates, thus avoiding spurious identification arising from non-linear relationships between the treatment and the covariates. We show that this approach delivers consistent and asymptotically normal estimates under weak conditions and that it may be adapted to be semiparametrically efficient (Chamberlain, 1992). Our method preserves standard intuitions and interpretations of linear instrumental variable methods, including under weak identification, and provides a simple, user-friendly upgrade to the applied economics toolbox. We illustrate our method with an example in law and criminal justice, examining the causal effect of appellate court reversals on district court sentencing decisions. ","Mostly Harmless Machine Learning: Learning Optimal Instruments in Linear
  IV Models"
76,1328669419808190464,85962581,Adenilton Silva,"['In this new paper with @TiagoVerass , Ismael Araujo, and @daniel_kpark . We combine a probabilistic quantum memory with FF-QRAM to prepare an n-qubit quantum state with M nonzero amplitudes with computational cost O(nM).\n\n<LINK>']",https://arxiv.org/abs/2011.07977,"Loading data in a quantum device is required in several quantum computing applications. Without an efficient loading procedure, the cost to initialize the algorithms can dominate the overall computational cost. A circuit-based quantum random access memory named FF-QRAM can load M n-bit patterns with computational cost O(CMn) to load continuous data where C depends on the data distribution. In this work, we propose a strategy to load continuous data without post-selection with computational cost O(Mn). The proposed method is based on the probabilistic quantum memory, a strategy to load binary data in quantum devices, and the FF-QRAM using standard quantum gates, and is suitable for noisy intermediate-scale quantum computers. ","Circuit-based quantum random access memory for classical data with
  continuous amplitudes"
77,1328651968500420608,885067890755600384,Eric Savin,"['New paper: Coherent interferometric imaging in a random flow, with Luc Bonnet, Etienne Gay, Christophe Peyret, and Josselin Garnier.\n<LINK>']",https://arxiv.org/abs/2011.07817,"This paper is concerned with the development of imaging methods to localize sources or reflectors in inhomogeneous moving media with acoustic waves that have travelled through them. A typical example is the localization of broadband acoustic sources in a turbulent jet flow for aeroacoustic applications. The proposed algorithms are extensions of Kirchhoff migration (KM) and coherent interferometry (CINT) which have been considered for smooth and randomly inhomogeneous quiescent media so far. They are constructed starting from the linearized Euler equations for the acoustic perturbations about a stationary ambient flow. A model problem for the propagation of acoustic waves generated by a fixed point source in an ambient flow with constant velocity is addressed. Based on this result imaging functions are proposed to modify the existing KM and CINT functions to account for the ambient flow velocity. They are subsequently tested and compared by numerical simulations in various configurations, including a synthetic turbulent jet representative of the main features encountered in actual jet flows. ",Coherent interferometric imaging in a random flow
78,1328643574867628033,992535707557269504,Dr. Aaron Labdon,"['Our new paper is out today! We present the first J band interferometric observations of a YSO using the MIRC-X instrument @CHARAArray. Observing the infamous outbursting star FU Orionis and the surrounding accretion disk. <LINK>', 'Using our tri-wavelength observations we derive the temperature gradient across the disk. We find evidence of direct boundary layer accretion onto the central star and viscous heating in the inner disk. (A thanks to Laws et al 2020 for the wonderful GPI image) https://t.co/0ah2piDJ6b', 'A big thanks to collaborators from @UoE_Astro, @michiganastro and @CHARAArray for making this work possible.']",https://arxiv.org/abs/2011.07865,"Context. FU Orionis is the archetypal FUor star, a subclass of young stellar object (YSO) that undergo rapid brightening events, often gaining 4-6 magnitudes on timescales of days. This brightening is often associated with a massive increase in accretion; one of the most ubiquitous processes in astrophysics from planets and stars to super-massive black holes. We present multi-band interferometric observations of the FU Ori circumstellar environment, including the first J-band interferometric observations of a YSO. Aims. We investigate the morphology and temperature gradient of the inner-most regions of the accretion disk around FU Orionis. We aim to characterise the heating mechanisms of the disk and comment on potential outburst triggering processes. Methods. Recent upgrades to the MIRC-X instrument at the CHARA array allowed the first dual-band J and H observations of YSOs.Using baselines up to 331 m, we present high angular resolution data of a YSO covering the near-infrared bands J, H, and K. The unprecedented spectral range of the data allows us to apply temperature gradient models to the innermost regions of FU Ori. Results. We spatially resolve the innermost astronomical unit of the disk and determine the exponent of the temperature gradient of the inner disk to $T=r^{-0.74\pm0.02}$. This agrees with theoretical work that predicts $T = r^{-0.75}$ for actively accreting, steady state disks, a value only obtainable through viscous heating within the disk. We find a disk which extends down to the stellar surface at $0.015\pm0.007$ au where the temperature is found to be $5800\pm700$ K indicating boundary layer accretion. We find a disk inclined at $32\pm4^\circ$ with a minor-axis position angle of $34\pm11^\circ$. ","Viscous Heating and Boundary Layer Accretion in the Disk of Outbursting
  Star FU Orionis"
79,1328634835276083200,1200003750162829312,Aarynn Carter,"['My new paper is out now! <LINK>\n\nQuick summary: @NASAWebb will be able to *directly* detect sub-Jupiter mass exoplanets at wide separations across a broad sample of objects, an order of magnitude improvement over current instruments.\n\nKeep reading for more info! <LINK>', ""Before I start, it's super-important to acknowledge the help I had on this work from:\n\nMy unwavering @UoE_Astro PhD advisor Sasha Hinkley &amp; now @ucsc postdoc mentor Andy Skemer\n\nThe statistical prowess of Mariangela Bonavita at @PhysAstroEd\n\nThe modelling maestro @AstroMarkyMark"", ""The coronagraphy masters @djulik @marshallperrin and @LaurentPueyo from @stsci \n\nVLT exoplanet survey expert @ArthurVigan \n\nand last but not least, the young moving group cartographer @jgagneastro \n\nNow let's dig into it! https://t.co/435093FmUS"", 'We know JWST will be great at directly imaging exoplanets as it:\n\n1) Is super-sensitive due to its whopping 6.5m primary mirror \n\n2) Has coronagraphic modes to block out stars and observe faint companion objects\n\n3) Can observe in the infrared, where planets are brighter https://t.co/yiNHHb1AAp', 'But the question this paper aims to answer is: How great is it going to be? \n\nAlthough inspired by the previous similar work of Beichman et al 2010 (https://t.co/FkAacm7tJO), this time we were armed with the latest JWST simulation tools, and planetary evolution/atmosphere models.', 'To tackle this we looked at JWST performance when observing members of the young moving groups Beta Pictoris/TW Hya\n\nThese objects are awesome as they are all within ~80pc so the angular scales we explore correspond to small physical separations where planets are more common https://t.co/fLoGtYZnb7', ""and their ages are ~10Myr and ~24Myr, young enough that planetary formation has largely ended, but old enough that any planets haven't significantly cooled and are therefore more luminous and easier to detect. \n\nAs lower mass exoplanets of a given age are fainter - this is great!"", 'So, we assembled almost 100 of these objects and performed a suite of simulations of what JWST observations would look like. LOADS more detail in the paper, but all you need to know is that we tested four different filters at ~3.6, ~4.4, ~11.4, and ~15.5 microns. https://t.co/qY5O01lVY5', 'Images like those in the flowchart above are cool, but what we want to get an idea of is what our sensitivity is as a function of radial separation for each target. \n\nLet me introduce: ""The Contrast Curve""\n\nObjects above the solid line should be detectable in our images. https://t.co/gQP2QEz7TX', 'Using the target stars magnitude we can convert these relative magnitude contrasts into absolute magnitude sensitivity limits, and because we picked young moving group members with known ages, we can also use evolutionary models to turn these into *mass* sensitivity limits. https://t.co/0CKLAlobzM', 'Almost there! To account for the range of possible orbital inclinations, eccentricities and locations of potential companions, we utilised a population synthesis model to produce ""detection probability maps"" and convert our angular separations to physical ones. https://t.co/peT46Li0iS', 'The contours of these plots tell us what fraction of the time we could detect a companion of a given mass and separation. But what is truly powerful is that we can *average* these maps over subsets of our sample to assess JWST sensitivity across a broad sample of objects', 'When we separate our sample by young moving group, we can see that Beta Pictoris members are more sensitive to closer separations companions (as they are closer to us), whilst TW Hya members are more sensitive to lower mass planets (as they are younger). https://t.co/YrK9Abc9Da', 'We also see that irrespective of moving group, the longest wavelength filters are best for detecting the lowest mass objects.\n\nThis is because at a given age, lower mass exoplanets are cooler and the peak of their spectral energy distributions are further into the mid-infrared.', ""When we separate things instead by spectral type, it's clear that M stars are the best targets irrespective of the filter we're observing in. As these objects are at similar distances, this is because the earlier type stars are brighter and impart more noise into the images. https://t.co/8Whur4pGef"", ""Finally, if we compare our entire sample to an equally sized subsample from a recent real survey using the state-of-the-art ground based VLT SPHERE instrument, we can determine what specific advantage JWST will provide that we don't already have. https://t.co/bTXOYyRRHr"", ""In the bottom panels, which show the absolute differenced detection probabilities of the surveys, it's clear that ground-based instruments will retain their advantage at the closest separations, but further out JWST offers a unique opportunity towards imaging sub-Jupiters."", 'Looking to the known planetary population, this region of parameter space has been largely unexplored. Judging by the contours from the best performing 15.5 micron filter, we can see that JWST could provide important constraints on the occurrence rates at wide separations. https://t.co/RmEkpnQu4q', ""I think I'll leave it there, thanks to whoever read this far, this was my first attempt at a tweet thread so I hope it turned out ok! If you have remaining questions feel free to send them my way. Hopefully some of you can use this paper as ammo in your upcoming JWST proposals!"", '@DeanaTanguay Thanks Deana! 🙂', '@maxwellmb Not particularly, if you look at the plot near the end of the thread however you can see that it does give better sensitivity than MIRI to &gt; 2 Mjup companions below ~40 au. But as we can do this better from the ground, the advantages of NIRCam are primarily in characterisation.', ""@PhilHinz @exoZafar @NASAWebb Thanks! I think it's buried in the text of the paper somewhere, but for the lowest masses it's around 200-250K."", '@AstroThayne @NASAWebb So there\'s a lot of differences in how the contrast is simulated and computed which make them difficult to compare, but in general I\'d say that beyond ~1"" the sensitivity is a lot worse as you start entering the photon noise dominated regime...', ""@AstroThayne @NASAWebb I think it's safe to use the JWST webpage as a quick guide, but I think you need to perform a simulation of your observations to really know what possible for a given target/integration time.""]",https://arxiv.org/abs/2011.07075,"The James Webb Space Telescope (JWST), currently scheduled to launch in 2021, will dramatically advance our understanding of exoplanetary systems with its ability to directly image and characterise planetary-mass companions at wide separations through coronagraphy. Using state-of-the-art simulations of JWST performance, in combination with the latest evolutionary models, we present the most sophisticated simulated mass sensitivity limits of JWST coronagraphy to date. In particular, we focus our efforts towards observations of members within the nearby young moving groups $\beta$ Pictoris and TW Hya. These limits indicate that whilst JWST will provide little improvement towards imaging exoplanets at short separations, at wide separations the increase in sensitivity is dramatic. We predict JWST will be capable of imaging sub-Jupiter mass objects beyond ~30 au, sub-Saturn mass objects beyond ~50 au, and that beyond ~100 au, JWST will be capable of directly imaging companions as small as 0.1 $M_\textrm{J}$ - at least an order of magnitude improvement over the leading ground-based instruments. Probing this unexplored parameter space will be of immediate value to modelling efforts focused on planetary formation and population synthesis. JWST will also serve as an excellent complement to ground based observatories through its unique ability to characterise previously detected companions across the near- to mid-infrared for the first time. ","Direct imaging of sub-Jupiter mass exoplanets with James Webb Space
  Telescope coronagraphy"
80,1328389339663921157,2227168756,Filip,"['Our new paper is out! <LINK> If interested in noisy QAOA, with an extra pinch of error correction, you need to read it!\nBig shout to @StreifMichael #VWDataLab, #QuAIL and @RIACSedu team!']",https://arxiv.org/abs/2011.06873,"Quantum circuits with local particle number conservation (LPNC) restrict the quantum computation to a subspace of the Hilbert space of the qubit register. In a noiseless or fault-tolerant quantum computation, such quantities are preserved. In the presence of noise, however, the evolution's symmetry could be broken and non-valid states could be sampled at the end of the computation. On the other hand, the restriction to a subspace in the ideal case suggest the possibility of more resource efficient error mitigation techniques for circuits preserving symmetries that are not possible for general circuits. Here, we analyze the probability of staying in such symmetry-preserved subspaces under noise, providing an exact formula for local depolarizing noise. We apply our findings to benchmark, under depolarizing noise, the symmetry robustness of XY-QAOA, which has local particle number conserving symmetries, and is a special case of the Quantum Alternating Operator Ansatz. We also analyze the influence of the choice of encoding the problem on the symmetry robustness of the algorithm and discuss a simple adaption of the bit flip code to correct for symmetry-breaking errors with reduced resources. ","Quantum algorithms with local particle number conservation: noise
  effects and error correction"
81,1328303410064596994,39215770,Dimitris Spathis,"['How much information about a person’s behavior and health can you ascertain from a wearable device?\n\nThis is what we study in our new paper at the ML for Mobile Health workshop @ NeurIPS 2020!\n\n✏️ Paper: <LINK>\n📰 Press (@VentureBeat): <LINK> <LINK>', 'We propose a self-supervised neural network in which we set the heart rate responses as the supervisory signal for movement data, leveraging their underlying physiological relationship. Then, thru transfer learning we predict outcomes such as obesity &amp; cardiorespiratory fitness.']",https://arxiv.org/abs/2011.04601,"To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship. We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer & wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring. ","Learning Generalizable Physiological Representations from Large-scale
  Wearable Data"
82,1328297084215300097,3352372504,Dr Hannah Williams,['Ever wanted to recreate works of art with atoms? Well in our new paper on @arxiv we tell you how! #monalisa (Also very useful for simulating many-body physics)\n<LINK> \n@InstitutOptique @MuseeLouvre <LINK>'],https://arxiv.org/abs/2011.06827,"We report on improvements extending the capabilities of the atom-by-atom assembler described in [Barredo et al., Science 354, 1021 (2016)] that we use to create fully-loaded target arrays of more than 100 single atoms in optical tweezers, starting from randomly-loaded, half-filled initial arrays. We describe four variants of the sorting algorithm that (i) allow decrease the number of moves needed for assembly and (ii) enable the assembly of arbitrary, non-regular target arrays. We finally demonstrate experimentally the performance of this enhanced assembler for a variety of target arrays. ",Enhanced atom-by-atom assembly of arbitrary tweezers arrays
83,1328238068961144832,1204724799211220993,Christian Rohrhofer 🇺🇦,['Let me show you our recent paper on the U(1) anomaly at high T: <LINK>\n\nMore statistics and proper control over the chirality of fermions reinforce previous findings that the effects of U(1) anomaly disappear.\n\nBonus: fancy new fits for screening masses!'],http://arxiv.org/abs/2011.01499,"We investigate the axial U(1) anomaly of two-flavor QCD at temperatures 190--330 MeV. In order to preserve precise chiral symmetry on the lattice, we employ the Mobius domain-wall fermion action as well as overlap fermion action implemented with a stochastic reweighting technique. Compared to our previous studies, we reduce the lattice spacing to 0.07 fm, simulate larger multiple volumes to estimate finite size effect, and take more than four quark mass points, including one below physical point to investigate the chiral limit. We measure the topological susceptibility, axial U(1) susceptibility, and examine the degeneracy of U(1) partners in meson and baryon correlators. All the data above the critical temperature indicate that the axial U(1) violation is consistent with zero within statistical errors. The quark mass dependence suggests disappearance of the U(1) anomaly at a rate comparable to that of the SU(2)_L x SU(2)_R symmetry breaking. ","Study of the axial U(1) anomaly at high temperature with lattice chiral
  fermions"
84,1328189001719631872,70614241,Dr Fiona H. Panther,"[""It's been a while, but here is a paper Monday promo for my new research group: SPIIR online coherent pipeline by Qi Chu et al \n<LINK>"", 'SPIIR is one of only 5 algorithms that search for gravitational waves in real time. Our algorithm uses summed parallel infinitie impulse response filters on GPUs to hunt for GWs in the time domain', ""The only frequency domain step in SPIIR's method is our whitening. By performing our filtering in the time domain, we are able to achieve some of the lowest latencies of any of the online search pipelines"", 'SPIIR contributed to the discovery of 36 of the events found during O3.', ""We may be a small group here at @uwanews, but we have some exciting updates planned for the SPIIR pipeline before O4. Check out the paper to see what we've achieved so far and stay tuned for more updates!""]",https://arxiv.org/abs/2011.06787,"This paper presents the SPIIR pipeline used for public alerts during the third advanced LIGO and Virgo observation run (O3 run). The SPIIR pipeline uses infinite impulse response (IIR) filters to perform extremely low-latency matched filtering and this process is further accelerated with graphics processing units (GPUs). It is the first online pipeline to select candidates from multiple detectors using a coherent statistic based on the maximum network likelihood ratio statistic principle. Here we simplify the derivation of this statistic using the singular-value-decomposition (SVD) technique and show that single-detector signal-to-noise ratios from matched filtering can be directly used to construct the statistic for each sky direction. Coherent searches are in general more computationally challenging than coincidence searches due to extra search over sky direction parameters. The search over sky directions follows an embarrassing parallelization paradigm and has been accelerated using GPUs. The detection performance is reported using a segment of public data from LIGO-Virgo's second observation run. We demonstrate that the median latency of the SPIIR pipeline is less than 9 seconds, and present an achievable roadmap to reduce the latency to less than 5 seconds. During the O3 online run, SPIIR registered triggers associated with 38 of the 56 non-retracted public alerts. The extreme low-latency nature makes it a competitive choice for joint time-domain observations, and offers the tantalizing possibility of making public alerts prior to the merger phase of binary coalescence systems involving at least one neutron star. ","The SPIIR online coherent pipeline to search for gravitational waves
  from compact binary coalescences"
85,1327316590615007239,913810182332968961,Dr. Jeanne Clelland,"[""New paper up on the arXiv today! My awesome collaborators and I used ensemble analysis to perform a detailed analysis of Colorado's Congressional districts based on 2018 election data.  Lots of pictures! <LINK>""]",https://arxiv.org/abs/2011.06049,"In this paper, we apply techniques of ensemble analysis to understand the political baseline for Congressional representation in Colorado. We generate a large random sample of reasonable redistricting plans and determine the partisan balance of each district using returns from state-wide elections in 2018, and analyze the 2011/2012 enacted districts in this context. Colorado recently adopted a new framework for redistricting, creating an independent commission to draw district boundaries, prohibiting partisan bias and incumbency considerations, requiring that political boundaries (such as counties) be preserved as much as possible, and also requiring that mapmakers maximize the number of competitive districts. We investigate the relationships between partisan outcomes, number of counties which are split, and number of competitive districts in a plan. This paper also features two novel improvements in methodology--a more rigorous statistical framework for understanding necessary sample size, and a weighted-graph method for generating random plans which split approximately as few counties as acceptable human-drawn maps. ","Colorado in Context: Congressional Redistricting and Competing Fairness
  Criteria in Colorado"
86,1327296066836946944,1054424174,Roland Herzog,"['New preprint ""Optimal Control of Hughes\' Model for Pedestrian Flow via Local Attraction"" with @jfpxtal and Max Winkler available on <LINK>! Numerical results will be in a follow-up paper.']",https://arxiv.org/abs/2011.03580,"We discuss the control of a human crowd whose dynamics is governed by a regularized version of Hughes' model, cf. Hughes: A continuum theory for the flow of pedestrians. Transportation research part B: methodological, 36 (2002). We assume that a finite number of agents act on the crowd and try to optimize their paths in a given time interval. The objective functional can be general and it can correspond, for instance, to the desire for fast evacuation or to maintain a single group of individuals. We provide an existence result for the forward model, study differentiability properties of the control-to-state map, establish the existence of a globally optimal control and formulate optimality conditions. ","Optimal Control of Hughes' Model for Pedestrian Flow via Local
  Attraction"
87,1327184284025823232,2969696397,Ion Nechita,"['New paper out with @hippoquantus and Anna Jenčová on different characterizations of measurement compatibility in general probabilistic theories <LINK> featuring tensor product of cones, tensor norms, as well as a generalization of spectrahedra <LINK>']",https://arxiv.org/abs/2011.06497,"In this work, we investigate measurement incompatibility in general probabilistic theories (GPTs). We show several equivalent characterizations of compatible measurements. The first is in terms of the positivity of associated maps. The second relates compatibility to the inclusion of certain generalized spectrahedra. For this, we extend the theory of free spectrahedra to ordered vector spaces. The third characterization connects the compatibility of dichotomic measurements to the ratio of tensor crossnorms of Banach spaces. We use these characterizations to study the amount of incompatibility present in different GPTs, i.e. their compatibility regions. For centrally symmetric GPTs, we show that the compatibility degree is given as the ratio of the injective and the projective norm of the tensor product of associated Banach spaces. This allows us to completely characterize the compatibility regions of several GPTs, and to obtain optimal universal bounds on the compatibility degree in terms of the 1-summing constants of the associated Banach spaces. Moreover, we find new bounds on the maximal incompatibility present in more than three qubit measurements. ","Incompatibility in general probabilistic theories, generalized
  spectrahedra, and tensor norms"
88,1327075926174142465,90276706,Jaehoon Lee,"['Can we leverage the power of infinite-width limit to help with Neural Architecture Search (NAS)?\n\nIn this new paper (<LINK>), we find that empirical NNGP can provide cheap and effective signals that can be used for NAS! <LINK>', 'From recent developments in the study of infinite-width networks, we know that inductive priors of large NNs are described by the Neural Network Gaussian Processes (NNGP). \n\nWe ask whether this can be used to predict the performance of neural architectures. https://t.co/E8439FLsoz', 'Since the infinite-width analytic NNGP for common architectures is expensive, we explore empirical monte-carlo estimated NNGP kernels.\n\nside: We find empirical NTK provides similar but slightly worse signal, and is more compute-expensive, so we focus on NNGP.', 'We find advantages from NNGP-based metrics on the NAS-bench 101 dataset (423k networks) on CIFAR-10 and the MNAS search space (10k networks) on ImageNet. These metrics can be used to shrink the search space, or in conjunction with shortened training signals. https://t.co/YgUSxHsT1K', 'The NNGP performance can be computed using a simple algorithm without the need to do any gradient based updates. It is thus much more computationally efficient than signals from shortened training. https://t.co/FI0TPSR4B9', 'We have also open-sourced a colab notebook showing the gist of the algorithm:\nhttps://t.co/Zol7lH2q9N', 'Daniel Park co-led the research, and this work was only possible with excellent collaborators at \nGoogle Brain team: Daiyi Peng, Yuan Cao, @jaschasd\n\nWe thank @yasamanbb, Gabriel Bender, Pieter-Jan Kindermans, @quocleix, Esteban Real, @sschoenholz, @tanmingxing for feedback!', 'image credit (infinite-width limit): @ARomanNovak']",http://arxiv.org/abs/2011.06006,"The predictions of wide Bayesian neural networks are described by a Gaussian process, known as the Neural Network Gaussian Process (NNGP). Analytic forms for NNGP kernels are known for many models, but computing the exact kernel for convolutional architectures is prohibitively expensive. One can obtain effective approximations of these kernels through Monte-Carlo estimation using finite networks at initialization. Monte-Carlo NNGP inference is orders-of-magnitude cheaper in FLOPs compared to gradient descent training when the dataset size is small. Since NNGP inference provides a cheap measure of performance of a network architecture, we investigate its potential as a signal for neural architecture search (NAS). We compute the NNGP performance of approximately 423k networks in the NAS-bench 101 dataset on CIFAR-10 and compare its utility against conventional performance measures obtained by shortened gradient-based training. We carry out a similar analysis on 10k randomly sampled networks in the mobile neural architecture search (MNAS) space for ImageNet. We discover comparative advantages of NNGP-based metrics, and discuss potential applications. In particular, we propose that NNGP performance is an inexpensive signal independent of metrics obtained from training that can either be used for reducing big search spaces, or improving training-based performance measures. ",Towards NNGP-guided Neural Architecture Search
89,1327032710318190593,1559281832,Vishvas Pandey,"['Check out new paper today: <LINK> - Flavor universality do break in CEvNS at loop level and it has interesting implications! I learnt a lot of interesting physics and fun stuff working with some awesome people Sasha, @RyanPlestid and @PedroANMachado!', '@psbarbeau @RyanPlestid @PedroANMachado @KateScholberg Thanks Phil, that would be great! https://t.co/MYGpSXFuXk mentions that a detailed CEvNS calculations including radiative corrections were carried out in Ref. [29], would it be possible to share that tech note.', '@psbarbeau @RyanPlestid @PedroANMachado @KateScholberg I guess the codes of that tech note are in the duke github that you shared.']",https://arxiv.org/abs/2011.05960,"We calculate coherent elastic neutrino-nucleus scattering cross sections on spin-0 nuclei (e.g. $^{40}$Ar and $^{28}$Si) at energies below 100 MeV within the Standard Model and account for all effects of permille size. We provide a complete error budget including uncertainties at nuclear, nucleon, hadronic, and quark levels separately as well as perturbative error. Our calculation starts from the four-fermion effective field theory to explicitly separate heavy-particle mediated corrections (which are absorbed by Wilson coefficients) from light-particle contributions. Electrons and muons running in loops introduce a nontrivial dependence on the momentum transfer due to their relatively light masses. These same loops, and those mediated by tau leptons, break the flavor universality because of mass-dependent electromagnetic radiative corrections. Nuclear physics uncertainties significantly cancel in flavor asymmetries resulting in subpercent relative errors. We find that for low neutrino energies, the cross section can be predicted with a relative precision that is competitive with neutrino-electron scattering. We highlight potentially useful applications of such a precise cross section prediction ranging from precision tests of the Standard Model, to searches for new physics and to the monitoring of nuclear reactors. ","Flavor-dependent radiative corrections in coherent elastic
  neutrino-nucleus scattering"
90,1326864444937396224,1614594931,Dr. Rebecca Levy,['I’m so excited to share my new paper! We use 0.5 pc resolution data from ALMA in the starburst galaxy NGC253 to discover and characterize outflows from massive young star clusters in the central starburst nucleus!\n\n<LINK>\n\nThread coming soon! <LINK> <LINK>'],https://arxiv.org/abs/2011.05334,"Young massive clusters play an important role in the evolution of their host galaxies, and feedback from the high-mass stars in these clusters can have profound effects on the surrounding interstellar medium. The nuclear starburst in the nearby galaxy NGC253 at a distance of 3.5 Mpc is a key laboratory in which to study star formation in an extreme environment. Previous high resolution (1.9 pc) dust continuum observations from ALMA discovered 14 compact, massive super star clusters (SSCs) still in formation. We present here ALMA data at 350 GHz with 28 milliarcsecond (0.5 pc) resolution. We detect blueshifted absorption and redshifted emission (P-Cygni profiles) towards three of these SSCs in multiple lines, including CS 7$-$6 and H$^{13}$CN 4$-$3, which represents direct evidence for previously unobserved outflows. The mass contained in these outflows is a significant fraction of the cluster gas masses, which suggests we are witnessing a short but important phase. Further evidence of this is the finding of a molecular shell around the only SSC visible at near-IR wavelengths. We model the P-Cygni line profiles to constrain the outflow geometry, finding that the outflows must be nearly spherical. Through a comparison of the outflow properties with predictions from simulations, we find that none of the available mechanisms completely explains the observations, although dust-reprocessed radiation pressure and O star stellar winds are the most likely candidates. The observed outflows will have a very substantial effect on the clusters' evolution and star formation efficiency. ",Outflows from Super Star Clusters in the Central Starburst of NGC253
91,1326705261113958402,2427184074,Christopher Berry,"['Congratulations to @ChaseBKimball on his new paper <LINK>\nIn this we ask if the black holes we see with @LIGO &amp; @ego_virgo could be made of smaller black holes? The answer is *definitely* maybe <LINK>', 'We know that black holes merge together to form bigger black holes. Could one of these merger remnants get a new partner and merge again? Potentially, if somewhere like a globular cluster or a nuclear star cluster. How could we tell? https://t.co/squreUR1mb', ""Second-generation black holes formed from mergers should be more massive and have distinctive spins. But since we don't know the distribution of first-generation black holes, they can be difficult to spot. We have to fit both generations at the same time https://t.co/gIy8XZ8G7n"", ""Accounting for the possibility of second-generation black holes is especially important if you want to reconstruct the first-generation black hole properties (say to find if there is a maximum mass) otherwise you'll pollute your results and you could come to false conclusions"", 'When we infer the properties of the black hole distribution, we see evidence for some of the heavier black holes, like #GW190521 and GW190519, being merger remnants! *But* this depends upon some important assumptions https://t.co/zBfOBxfe0z', 'Our analysis assumes that all binaries are formed in identical clusters. If we assume a cluster with a low escape velocity, second-generation systems are unlikely, but for a high escape velocity (a few hundred km/s) they are almost certain! https://t.co/otgK3Xqcmc', 'The assumption that all binaries come from identical clusters is a simplification. We really need to consider a distribution and add in non-cluster formation. This is difficult, but we think our results are exciting enough to show it will be worth the work', '@ChaseBKimball will be taking a good nap before starting on the *next* paper though', '@gravitysydney @ChaseBKimball @LIGO @ego_virgo @ColmMTalbot @spacedontwait @EHThrane @chionatan @MattCarney106 @TomD_Santiago @hannahmidd8 @daniel_williams 🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\n🐢\nhttps://t.co/Q3Xf5xYOVT']",https://arxiv.org/abs/2011.05332,"We study the population properties of merging binary black holes in the second LIGO--Virgo Gravitational-Wave Transient Catalog assuming they were all formed dynamically in gravitationally bound clusters. Using a phenomenological population model, we infer the mass and spin distribution of first-generation black holes, while self-consistently accounting for hierarchical mergers. Considering a range of cluster masses, we see compelling evidence for hierarchical mergers in clusters with escape velocities $\gtrsim 100~\mathrm{km\,s^{-1}}$. For our most probable cluster mass, we find that the catalog contains at least one second-generation merger with $99\%$ credibility. We find that the hierarchical model is preferred over an alternative model with no hierarchical mergers (Bayes factor $\mathcal{B} > 1400$) and that GW190521 is favored to contain two second-generation black holes with odds $\mathcal{O}>700$, and GW190519, GW190602, GW190620, and GW190706 are mixed-generation binaries with $\mathcal{O} > 10$. However, our results depend strongly on the cluster escape velocity, with more modest evidence for hierarchical mergers when the escape velocity is $\lesssim 100~\mathrm{km\,s^{-1}}$. Assuming that all binary black holes are formed dynamically in globular clusters with escape velocities on the order of tens of $\mathrm{km\,s^{-1}}$, GW190519 and GW190521 are favored to include a second-generation black hole with odds $\mathcal{O}>1$. In this case, we find that $99\%$ of black holes from the inferred total population have masses that are less than $49\,M_{\odot}$, and that this constraint is robust to our choice of prior on the maximum black hole mass. ","Evidence for hierarchical black hole mergers in the second LIGO--Virgo
  gravitational-wave catalog"
92,1326655303656628226,892997634813710336,Adam Fisch,"['Excited to share our new EMNLP paper, “CapWAP: Captioning with a Purpose”!\n\nWe take a fresh perspective on image captioning by giving it an explicit purpose. In CapWAP, models are trained and evaluated with respect to information that users care about.\n\n<LINK> <LINK>', ""The CapWAP objective is ultimately to provide informative captions that anticipate and satisfy users' potential needs. We use QA pairs as an implicit signal for information need. E.g., in the above image, a good caption should be able to be used answer where the bus is headed."", ""Solving this task is difficult but important. This topic has been studied in the summarization/generation communities for years, but hasn't yet been a part of the work surrounding language and vision. Our approach makes significant progress, but there’s still work to be done."", 'This work was done together with a great set of collaborators: @kentonctlee , @mchang21 , @JonClarkSeattle , @BarzilayRegina.\n\nCode available online: \nhttps://t.co/JOrwkDHxJa.']",http://arxiv.org/abs/2011.04264,"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with a Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs---a natural expression of information need---from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context. ",CapWAP: Captioning with a Purpose
93,1326589713730375680,29089735,Brian Colquhoun,"['New paper day: <LINK>\n\nAnother paper on self-interacting dark matter.', 'Technical: We derived analytic approximations for self-interaction cross sections in the semi-classical regime, because doing it numerically with partial waves can be computationally expensive.', 'We also show results where masses and coupling strengths are consistent with observations on scales of galaxy groups and clusters (a call-back to the last paper that everyone read 😉).']",https://arxiv.org/abs/2011.04679,"Many particle physics models for dark matter self-interactions - motivated to address long-standing challenges to the collisionless cold dark matter paradigm - fall within the semi-classical regime, with interaction potentials that are long-range compared to the de Broglie wavelength for dark matter particles. In this work, we present a quantum mechanical derivation and new analytic formulas for the semi-classical momentum transfer and viscosity cross sections for self-interactions mediated by a Yukawa potential. Our results include the leading quantum corrections beyond the classical limit and allow for both distinguishable and identical dark matter particles. Our formulas supersede the well-known formulas for the momentum transfer cross section obtained from the classical scattering problem, which are often used in phenomenological studies of self-interacting dark matter. Together with previous approximation formulas for the cross section in the quantum regime, our new results allow for nearly complete analytic coverage of the parameter space for self-interactions with a Yukawa potential. We also discuss the phenomenological implications of our results and provide a new velocity-averaging procedure for constraining velocity-dependent self-interactions. Our results have been implemented in the newly released code CLASSICS. ",The Semi-Classical Regime for Dark Matter Self-Interactions
94,1326361594603970560,1192861416535085056,Libby Tolman,"['My latest paper is out on arXiv: <LINK>.  In it, Peter Catto and I develop a new analytic method for calculating alpha particle heat fluxes in a tokamak that has small perturbations in its electric and magnetic fields. <LINK>', 'Our work suggests that alpha particle transport in SPARC (one tokamak being designed) due to the TAE (one type of perturbation to the tokamak fields) might be small.  It will be interesting to see if other methods for predicting such transport agree with this result. https://t.co/KSySypfeFQ', ""The paper's math is heavy, so I'll be explaining it with lots of pictures and diagrams during a Friday #apsdpp talk (session ZI02). https://t.co/DZtFKl48RH""]",https://arxiv.org/abs/2011.04920,"Upcoming tokamak experiments fueled with deuterium and tritium are expected to have large alpha particle populations. Such experiments motivate new attention to the theory of alpha particle confinement and transport. A key topic is the interaction of alphas with perturbations to the tokamak fields, including those from ripple and magnetohydrodynamic modes like Alfv\'{e}n eigenmodes. These perturbations can transport alphas, leading to changed localization of alpha heating, loss of alpha power, and damage to device walls. Alpha interaction with these perturbations is often studied with single particle theory. In contrast, we derive a drift kinetic theory to calculate the alpha heat flux resulting from arbitrary perturbation frequency and periodicity (provided these can be studied drift kinetically). Novel features of the theory include the retention of a large effective collision frequency resulting from the resonant alpha collisional boundary layer, correlated interactions over many poloidal transits, and finite orbit effects. Heat fluxes are considered for the example cases of ripple and the toroidal Alfv\'{e}n eigenmode (TAE). The ripple heat flux is small. The TAE heat flux is significant and scales with the square of the perturbation amplitude, allowing the derivation of constraints on mode amplitude for avoidance of significant alpha depletion. A simple saturation condition suggests that TAEs in one upcoming experiment will not cause significant alpha transport via the mechanisms in this theory. However, saturation above the level suggested by the simple condition, but within numerical and experimental experience, which could be accompanied by the onset of stochasticity, could cause significant transport. ",Drift kinetic theory of alpha transport by tokamak perturbations
95,1326281833282072576,790033937531703296,Yi Tay,"['As a companion to our recent efficient Transformer survey, we designed ""Long Range Arena"" a new challenging benchmark to help understand and analyze trade-offs between recent efficient Transformer models. Check out our paper at <LINK>. @GoogleAI @DeepMind <LINK>', 'Joint work with amazing collaborators: @m__dehghani (co-first), @samiraabnar @Yikang_Shen @dara_bahri @philly_pham @Jeffy_Sailing @liuyang_irnlp @seb_ruder @metzlerd.\n\nCode repository: https://t.co/VWoBQcQkN1']",https://arxiv.org/abs/2011.04006,"Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at this https URL ",Long Range Arena: A Benchmark for Efficient Transformers
96,1326263421042614276,990346198908223488,Nikolai Matni,"['Very excited about our new paper with @AlexRobey23 @HaiminHu and @HanwenZhang35 on learning hybrid control barrier functions from data, to appear @corl_conf!\n\nPaper: <LINK>\nCode: <LINK>\n15min talk: <LINK> <LINK>', 'We define a novel class of hybrid CBFs, and extend our prior work on learning CBFs from data to systems described by smooth flows and discrete jumps, allowing us to learn safe controllers for systems with contact dynamics, such as the compass gait walker shown in the gif below. https://t.co/f1G2jE7gx4']",https://arxiv.org/abs/2011.04112,"Motivated by the lack of systematic tools to obtain safe control laws for hybrid systems, we propose an optimization-based framework for learning certifiably safe control laws from data. In particular, we assume a setting in which the system dynamics are known and in which data exhibiting safe system behavior is available. We propose hybrid control barrier functions for hybrid systems as a means to synthesize safe control inputs. Based on this notion, we present an optimization-based framework to learn such hybrid control barrier functions from data. Importantly, we identify sufficient conditions on the data such that feasibility of the optimization problem ensures correctness of the learned hybrid control barrier functions, and hence the safety of the system. We illustrate our findings in two simulations studies, including a compass gait walker. ",Learning Hybrid Control Barrier Functions from Data
97,1326163783107010560,2527310100,Dr. Ashutosh Modi,"['Check out our new paper #coling2020 “Adapting a Language Model for Controlled Affective Text Generation”. Work by @ishika3singh @ahsan_barkati @TusharGoswamy \nPaper: <LINK>\nVideo: <LINK>\n@cseatiitk', 'We generate affective text by perturbing the hidden representations of a transformer model by incorporating affective and topic losses. Check out the paper for more details.']",http://arxiv.org/abs/2011.04000,"Human use language not just to convey information but also to express their inner feelings and mental states. In this work, we adapt the state-of-the-art language generation models to generate affective (emotional) text. We posit a model capable of generating affect-driven and topic-focused sentences without losing grammatical correctness as the affect intensity increases. We propose to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions fall out on grammatical correctness at extreme intensities, but our model is resilient to this and delivers robust results at all intensities. We conduct automated evaluations and human studies to test the performance of our model and provide a detailed comparison of the results with other models. In all evaluations, our model outperforms existing affective text generation models. ",Adapting a Language Model for Controlled Affective Text Generation
98,1326102438303174656,234888240,konstantin herbst,"['Fantastic news! \n\nOur new paper ""From Starspots to Stellar Coronal Mass Ejections - Revisiting Empirical Stellar Relations"" together with Athanasios Papaioannou, @VladimirAirape1, and @cosmicatri has been accepted. \n\nSneak peek: <LINK>\nin press at ApJ 🥳']",https://arxiv.org/abs/2011.03761,"Upcoming missions, including the James Webb Space Telescope, will soon characterize the atmospheres of terrestrial-type exoplanets in habitable zones around cool K- and M-type stars searching for atmospheric biosignatures. Recent observations suggest that the ionizing radiation and particle environment from active cool planet hosts may be detrimental for exoplanetary habitability. Since no direct information on the radiation field is available, empirical relations between signatures of stellar activity, including the sizes and magnetic fields of starspots, are often used. Here, we revisit the empirical relation between the starspot size and the effective stellar temperature and evaluate its impact on estimates of stellar flare energies, coronal mass ejections, and fluxes of the associated stellar energetic particle events. ","From Starspots to Stellar Coronal Mass Ejections -- Revisiting Empirical
  Stellar Relations"
99,1326035048408121344,96779364,Arnab Bhattacharyya,"['New paper (<LINK>) on learning tree-structured distributions using the Chow-Liu algorithm with Sutanu Gayen, Eric Price and Vinod Vinodchandran!', ""Chow-Liu is a classic algorithm from '68 for learning tree models. Here it is in a single tweet: Given samples from a distribution P on n variables, empirically estimate I(X;Y) between each pair of vars X &amp; Y. With these estimates as weights, output a maximum spanning tree."", 'The guarantee is that if the estimates are exact, Chow-Liu outputs a tree T such that there exists a T-structured distribution Q minimizing KL(P,Q). In other words, Q maximizes the likelihood of generating the data.', 'Our paper takes another look at this analysis. We pin down the # of samples needed from P to recover a tree-structured Q that approximately minimizes KL(P,Q). The headline result is that when P is tree-structured, the sample complexity of Chow-Liu is nearly-linear in n.', ""If you wanted to get a good additive estimate of each weight, you'd need ~n^2 samples. But we show Chow-Liu doesn't require that! The errors in estimating the weights are correlated in such a way that with much fewer samples, the max spanning tree is still a good approximation."", ""The key technical ingredient is a new independence tester with respect to KL divergence. It's still an open question whether it's possible to directly reduce tree-structure estimation to conditional independence testing."", 'Just as we were writing up our work, another paper was posted online about the same problem by Costis Daskalakis and Qinxuan Pan: https://t.co/A17QF9cB1t. Their main result is roughly the same but their analysis is restricted to the Boolean case and seems more complicated.']",https://arxiv.org/abs/2011.04144,"We provide finite sample guarantees for the classical Chow-Liu algorithm (IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model of a distribution. For a distribution $P$ on $\Sigma^n$ and a tree $T$ on $n$ nodes, we say $T$ is an $\varepsilon$-approximate tree for $P$ if there is a $T$-structured distribution $Q$ such that $D(P\;||\;Q)$ is at most $\varepsilon$ more than the best possible tree-structured distribution for $P$. We show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with the plug-in estimator for mutual information with $\widetilde{O}(|\Sigma|^3 n\varepsilon^{-1})$ i.i.d.~samples outputs an $\varepsilon$-approximate tree for $P$ with constant probability. In contrast, for a general $P$ (which may not be tree-structured), $\Omega(n^2\varepsilon^{-2})$ samples are necessary to find an $\varepsilon$-approximate tree. Our upper bound is based on a new conditional independence tester that addresses an open problem posed by Canonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three random variables $X,Y,Z$ each over $\Sigma$, testing if $I(X; Y \mid Z)$ is $0$ or $\geq \varepsilon$ is possible with $\widetilde{O}(|\Sigma|^3/\varepsilon)$ samples. Finally, we show that for a specific tree $T$, with $\widetilde{O} (|\Sigma|^2n\varepsilon^{-1})$ samples from a distribution $P$ over $\Sigma^n$, one can efficiently learn the closest $T$-structured distribution in KL divergence by applying the add-1 estimator at each node. ",Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu
100,1325990127106215936,1162181213475540992,Kaze Wong,"['So here is another paper learning astrophysics the new @LIGO @ego_virgo  catalogue.\nWe figured ~80% of the binary black holes are from globular clusters. Also, the host clusters seem to share similar properties as the cluster we saw in Milky way.\n<LINK>\n#GWTC2 <LINK>', 'Disclaimer: we know the conclusions are model-dependent and we are more careful in terms of what statements to make in the paper. This Twitter post is just my personal opinion which is probably overly dramatic.']",https://arxiv.org/abs/2011.03564,"The recent release of the second Gravitational-Wave Transient Catalog (GWTC-2) has increased significantly the number of known GW events, enabling unprecedented constraints on formation models of compact binaries. One pressing question is to understand the fraction of binaries originating from different formation channels, such as isolated field formation versus dynamical formation in dense stellar clusters. In this paper, we combine the $\texttt{COSMIC}$ binary population synthesis suite and the $\texttt{CMC}$ code for globular cluster evolution to create a mixture model for black hole binary formation under both formation scenarios. For the first time, these code bodies are combined self-consistently, with $\texttt{CMC}$ itself employing $\texttt{COSMIC}$ to track stellar evolution. We then use a deep-learning enhanced hierarchical Bayesian analysis to constrain the mixture fraction $f$ between formation models, while simultaneously constraining the common envelope efficiency $\alpha$ assumed in $\texttt{COSMIC}$ and the initial cluster virial radius $r_v$ assumed in $\texttt{CMC}$. Under specific assumptions about other uncertain aspects of isolated binary and globular cluster evolution, we report the median and $90\%$ confidence interval of three physical parameters $(f,\alpha,r_v)=(0.20^{+0.32}_{-0.18},2.26^{+2.65}_{-1.84},2.71^{+0.83}_{-1.17})$. This simultaneous constraint agrees with observed properties of globular clusters in the Milky Way and is an important first step in the pathway toward learning astrophysics of compact binary formation from GW observations. ","Joint constraints on the field-cluster mixing fraction, common envelope
  efficiency, and globular cluster radii from a population of binary hole
  mergers via deep learning"
101,1325928457726021638,6222842,Nick Feamster,"['How did the Internet/ISPs respond to COVID-19?  See the presentation @jlivingood and I gave to the IAB this morning, as well as our recently posted paper, here: <LINK> (lots of new data on changes in traffic patterns, ISP provisioning upgrades, @FCC data, etc.). <LINK>']",https://arxiv.org/abs/2011.00419,"The COVID-19 pandemic has resulted in dramatic changes to the daily habits of billions of people. Users increasingly have to rely on home broadband Internet access for work, education, and other activities. These changes have resulted in corresponding changes to Internet traffic patterns. This paper aims to characterize the effects of these changes with respect to Internet service providers in the United States. We study three questions: (1)How did traffic demands change in the United States as a result of the COVID-19 pandemic?; (2)What effects have these changes had on Internet performance?; (3)How did service providers respond to these changes? We study these questions using data from a diverse collection of sources. Our analysis of interconnection data for two large ISPs in the United States shows a 30-60% increase in peak traffic rates in the first quarter of 2020. In particular, we observe traffic downstream peak volumes for a major ISP increase of 13-20% while upstream peaks increased by more than 30%. Further, we observe significant variation in performance across ISPs in conjunction with the traffic volume shifts, with evident latency increases after stay-at-home orders were issued, followed by a stabilization of traffic after April. Finally, we observe that in response to changes in usage, ISPs have aggressively augmented capacity at interconnects, at more than twice the rate of normal capacity augmentation. Similarly, video conferencing applications have increased their network footprint, more than doubling their advertised IP address space. ","Characterizing Service Provider Response to the COVID-19 Pandemic in the
  United States"
102,1325811929475129345,2766925212,Andrew Childs,"['New paper with @JinPengLiu__Sky, Kolden, Krovi, Loureiro, and Trivisa gives an efficient quantum for nonlinear differential equations with strong enough dissipation, shows hardness for weak dissipation. [1/2] <LINK>', 'Leyton &amp; Osborne gave a quantum algorithm for nonlinear diff eqs 10+ years ago (https://t.co/gYT0x3aZNT). Their cost is exponential in evolution time. We improve this to polynomial with enough dissipation and rule out improvement for weak dissipation. [2/2]']",http://arxiv.org/abs/2011.03185,"Nonlinear differential equations model diverse phenomena but are notoriously difficult to solve. While there has been extensive previous work on efficient quantum algorithms for linear differential equations, the linearity of quantum mechanics has limited analogous progress for the nonlinear case. Despite this obstacle, we develop a quantum algorithm for dissipative quadratic $n$-dimensional ordinary differential equations. Assuming $R < 1$, where $R$ is a parameter characterizing the ratio of the nonlinearity and forcing to the linear dissipation, this algorithm has complexity $T^2 q~\mathrm{poly}(\log T, \log n, \log 1/\epsilon)/\epsilon$, where $T$ is the evolution time, $\epsilon$ is the allowed error, and $q$ measures decay of the solution. This is an exponential improvement over the best previous quantum algorithms, whose complexity is exponential in $T$. While exponential decay precludes efficiency, driven equations can avoid this issue despite the presence of dissipation. Our algorithm uses the method of Carleman linearization, for which we give a novel convergence theorem. This method maps a system of nonlinear differential equations to an infinite-dimensional system of linear differential equations, which we discretize, truncate, and solve using the forward Euler method and the quantum linear system algorithm. We also provide a lower bound on the worst-case complexity of quantum algorithms for general quadratic differential equations, showing that the problem is intractable for $R \ge \sqrt{2}$. Finally, we discuss potential applications, showing that the $R < 1$ condition can be satisfied in realistic epidemiological models and giving numerical evidence that the method may describe a model of fluid dynamics even for larger values of $R$. ","Efficient quantum algorithm for dissipative nonlinear differential
  equations"
103,1325802040447000577,15612654,Alan Stern,"['#PI_Daily Sometimes data you take yields major collateral discoveries. Check out this cool new paper led by @TodLauer advancing a longstanding problem in extragalactic astronomy, care of @NewHorizons2015 data made for other purposes! #Science #Space #NASA <LINK> <LINK>']",https://arxiv.org/abs/2011.03052?fbclid=IwAR3mUGNUCUu__yhLO3vb8boBoFdcxFMUdMjWMWL6zTV211PQxrViM9D6n-I,"We used existing data from the New Horizons LORRI camera to measure the optical-band ($0.4\lesssim\lambda\lesssim0.9{\rm\mu m}$) sky brightness within seven high galactic latitude fields. The average raw level measured while New Horizons was 42 to 45 AU from the Sun is $33.2\pm0.5{\rm ~nW ~m^{-2} ~sr^{-1}}.$ This is $\sim10\times$ darker than the darkest sky accessible to the {\it Hubble Space Telescope}, highlighting the utility of New Horizons for detecting the cosmic optical background (COB). Isolating the COB contribution to the raw total requires subtracting scattered light from bright stars and galaxies, faint stars below the photometric detection-limit within the fields, and diffuse Milky Way light scattered by infrared cirrus. We remove newly identified residual zodiacal light from the IRIS $100\mu$m all sky maps to generate two different estimates for the diffuse galactic light (DGL). Using these yields a highly significant detection of the COB in the range ${\rm 15.9\pm 4.2\ (1.8~stat., 3.7~sys.) ~nW ~m^{-2} ~sr^{-1}}$ to ${\rm 18.7\pm 3.8\ (1.8~stat., 3.3 ~sys.)~ nW ~m^{-2} ~sr^{-1}}$ at the LORRI pivot wavelength of 0.608 $\mu$m. Subtraction of the integrated light of galaxies (IGL) fainter than the photometric detection-limit from the total COB level leaves a diffuse flux component of unknown origin in the range ${\rm 8.8\pm4.9\ (1.8 ~stat., 4.5 ~sys.) ~nW ~m^{-2} ~sr^{-1}}$ to ${\rm 11.9\pm4.6\ (1.8 ~stat., 4.2 ~sys.) ~nW ~m^{-2} ~sr^{-1}}$. Explaining it with undetected galaxies requires the galaxy-count faint-end slope to steepen markedly at $V>24$ or that existing surveys are missing half the galaxies with $V< 30.$ ",New Horizons Observations of the Cosmic Optical Background
104,1325706065820995585,856802303533305856,Dr Aaron Jones,"['New paper on the arxiv about the #EinsteinTelescope (ET) design, led by Sam Rowlinson (from @UoBIGWaves). <LINK> 1/8 <LINK>', 'The Einstein Telescope (https://t.co/eY4VS6y0PY, ET) is a proposed gravitational wave observatory, with substantial improvements, allowing us to detect new types and more gravitational waves 2/8 https://t.co/NZ9BJ7a1fv', 'For optimal sensitivity the observatory will be a triangle shape (instead of an L) compromising of 6 interferometers! Each side will be 10 km (instead of 3km used by @ego_virgo and @KAGRA_PR and 4km used by @LIGO). 3/8 https://t.co/RhFMQB5SJF', 'Because the arm is very long, the laser beam will be very wide when it exits the ""arm cavity"" (where the gravitational wave interacts with the light). This ""wide"" laser beam must be reduced in size so it can be matched to the laser 4/8 https://t.co/b3CrfMxwpD', 'We design a telescope to reduce the size of this laser beam to an intermediate size as soon as it exits the arm. This allows the use of a smaller beamsplitter (saving money) and has a number of other technical advantages 5/8 https://t.co/a32NlZkZBD', 'These include: decoupling the effects of beam jitter; steering the ""cold"" and ""hot"" beams around each other, allowing good placement of beams in the tunnels. The size is chosen by a tradeoff analysis considering the effects of thermal lensing and thermal noise. 6/8', 'The paper closes with a discussion about which parameters can be ""tuned"" without changing the overall design, based on further studies. 7/8 https://t.co/9LzHGFV2G2', 'Lastly, all of this work was completed with the open-source software Finesse3 (https://t.co/aeqTWD1Kbi)! 8/8']",https://arxiv.org/abs/2011.02983,"The optical design of the Einstein Telescope (ET) is based on a dual-recycled Michelson interferometer with Fabry-Perot cavities in the arms. ET will be constructed in a new infrastructure, allowing us to consider different technical implementations beyond the constraints of the current facilities. In this paper we investigate the feasibility of using beam-expander telescopes in the interferometer arms. We provide an example implementation that matches the optical layout as presented in the ET design update 2020. We further show that the beam-expander telescopes can be tuned to compensate for mode mismatches between the arm cavities and the rest of the interferometer. ","Feasibility study of beam-expanding telescopes in the interferometer
  arms for the Einstein Telescope"
105,1324814457286074368,3100596960,Walter Scheirer,"['Lots of good discussion on what could be better about the machine learning development cycle has unfolded over at @aivillage_dc. @BlancheMinerva and I have summarized some of these ideas in a new paper (to appear at the ICBINB@NeurIPS 2020 workshop): \n\n<LINK>', 'Common problems include everything from ethical lapses to technical mistakes. Much of this, we argue, can be avoided with a better development process.']",https://arxiv.org/abs/2011.02832,"Machine learning has the potential to fuel further advances in data science, but it is greatly hindered by an ad hoc design process, poor data hygiene, and a lack of statistical rigor in model evaluation. Recently, these issues have begun to attract more attention as they have caused public and embarrassing issues in research and development. Drawing from our experience as machine learning researchers, we follow the machine learning process from algorithm design to data collection to model evaluation, drawing attention to common pitfalls and providing practical recommendations for improvements. At each step, case studies are introduced to highlight how these pitfalls occur in practice, and where things could be improved. ",Pitfalls in Machine Learning Research: Reexamining the Development Cycle
106,1324789973682282497,888216099757490176,Maithra Raghu,"['New paper Teaching with Commentaries <LINK>\n\nWe introduce commentaries, metalearned information to help neural net training &amp; give insights on learning process, dataset &amp; model representations\n\nLed by @RaghuAniruddh &amp; w/ @skornblith @DavidDuvenaud @geoffreyhinton <LINK>']",https://arxiv.org/abs/2011.03037,"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training. ",Teaching with Commentaries
107,1324637965272961024,131879500,John Ilee,"['The composition of a planet depends not only on *where* it forms in a protoplanetary disc, but also *when* it forms. \n\n@jamesmiley73 winds the clock back in a great new paper out today... 🪐⏱🥏🧪\n\n<LINK>']",https://arxiv.org/abs/2011.02536,"We investigate the impact of pre-main sequence stellar luminosity evolution on the thermal and chemical properties of disc midplanes. We create template disc models exemplifying initial conditions for giant planet formation for a variety of stellar masses and ages. These models include the 2D physical structure of gas as well as 1D chemical structure in the disc midplane. The disc temperature profiles are calculated using fully physically consistent radiative transfer models for stars between 0.5 and 3 Msun and ages up to 10 Myr. The resulting temperature profiles are used to determine how the chemical conditions in the mid-plane change over time. We therefore obtain gas and ice-phase abundances of the main carbon and oxygen carrier species. While the temperature profiles produced are not markedly different for the stars of different masses at early stages (<1 Myr), they start to diverge significantly beyond 2 Myr. Discs around stars with mass >1.5 Msun become warmer over time as the stellar luminosity increases, whereas low-mass stars decrease in luminosity leading to cooler discs. This has an observable effect on the location of the CO snowline, which is located >200 au in most models for a 3 Msun star, but is always within 80 au for 0.5 Msun star. The chemical compositions calculated show that a well defined stellar mass and age range exists in which high C/O gas giants can form. In the case of the exoplanet HR8799b, our models show it must have formed before the star was 1 Myr old. ","The impact of pre-main sequence stellar evolution on midplane snowline
  locations and C/O in planet forming discs"
108,1324622780030164993,2969696397,Ion Nechita,"['New paper on random quantum channels <LINK> We show that most of the physical characterizations of quantum operations yield equivalent classes of probability measures. On a computer, your best bet is to randomly choose your Kraus operators. <LINK>']",https://arxiv.org/abs/2011.02994,"Several techniques of generating random quantum channels, which act on the set of $d$-dimensional quantum states, are investigated. We present three approaches to the problem of sampling of quantum channels and show under which conditions they become mathematically equivalent, and lead to the uniform, Lebesgue measure on the convex set of quantum operations. We compare their advantages and computational complexity and demonstrate which of them is particularly suitable for numerical investigations. Additional results focus on the spectral gap and other spectral properties of random quantum channels and their invariant states. We compute mean values of several quantities characterizing a given quantum channel, including its unitarity, the average output purity and the $2$-norm coherence of a channel, averaged over the entire set of the quantum channels with respect to the uniform measure. An ensemble of classical stochastic matrices obtained due to super-decoherence of random quantum stochastic maps is analyzed and their spectral properties are studied using the Bloch representation of a classical probability vector. ",Generating random quantum channels
109,1324555770537467911,2162991392,Shichao,['Our new paper about NSBH GW/EM detectability in 2nd/2.5th/3rd GW era: Kilonova Emission From Black Hole-Neutron Star Mergers. II. Luminosity Function and Implications for Target-of-opportunity Observations of Gravitational-wave Triggers and Blind Searches <LINK>'],https://arxiv.org/abs/2011.02717,"We present detailed simulations of black hole-neutron star (BH-NS) mergers kilonova and gamma-ray burst (GRB) afterglow and kilonova luminosity function, and discuss the detectability of electromagnetic (EM) counterpart in connection with gravitational wave (GW) detections, GW-triggered target-of-opportunity observations, and time-domain blind searches. The predicted absolute magnitude of the BH-NS kilonovae at $0.5\,{\rm days}$ after the merger falls in $[-10,-15.5]$. The simulated luminosity function contains the potential viewing-angle distribution information of the anisotropic kilonova emission. We simulate the GW detection rates, detectable distances and signal duration, for the future networks of 2nd/2.5th/3rd-generation GW detectors. BH-NSs tend to produce brighter kilonovae and afterglows if the BH has a higher aligned-spin, and a less massive NS with a stiffer EoS. The detectability of kilonova is especially sensitive to the BH spin. If BHs typically have low spins, the BH-NS EM counterparts are hard to discover. For the 2nd generation GW detector networks, a limiting magnitude of $m_{\rm limit}\sim23-24\,{\rm mag}$ is required to detect the kilonovae even if BH high spin is assumed. Thus, a plausible explanation for the lack of BH-NS associated kilonova detection during LIGO/Virgo O3 is that either there is no EM counterpart (plunging events), or the current follow-ups are too shallow. These observations still have the chance to detect the on-axis jet afterglow associated with an sGRB or an orphan afterglow. Follow-up observations can detect possible associated sGRB afterglows, from which kilonova signatures may be studied. For time-domain observations, a high-cadence search in redder filters is recommended to detect more BH-NS associated kilonovae and afterglows. ","Kilonova Emission From Black Hole-Neutron Star Mergers. II. Luminosity
  Function and Implications for Target-of-opportunity Observations of
  Gravitational-wave Triggers and Blind Searches"
110,1324530224835383296,611167741,Mahdi Namazifar,"['In this new paper we show how sequential transfer learning through question answering could achieve 50% or more performance improvement in few-shot setting for NLU. With @alex_papangelis @gokhan4 @dilekhakkanitur \n\n<LINK> <LINK>', '@mhajiaghayi @alex_papangelis @gokhan4 @dilekhakkanitur Thanks a lot for the kind words Mahdi jaan']",https://arxiv.org/abs/2011.03023,"Different flavors of transfer learning have shown tremendous impact in advancing research and applications of machine learning. In this work we study the use of a specific family of transfer learning, where the target domain is mapped to the source domain. Specifically we map Natural Language Understanding (NLU) problems to QuestionAnswering (QA) problems and we show that in low data regimes this approach offers significant improvements compared to other approaches to NLU. Moreover we show that these gains could be increased through sequential transfer learning across NLU problems from different domains. We show that our approach could reduce the amount of required data for the same performance by up to a factor of 10. ","Language Model is All You Need: Natural Language Understanding as
  Question Answering"
111,1324431731903107076,3374566037,Kate Storey-Fisher,"['my first first-author paper is out on the arXiv! \n\ntldr: we construct a new estimator, the Continuous-Function Estimator, for the 2-point correlation function. it can make cosmological analyses better &amp; cheaper.\n\na quick thread here⬇️, full paper @ <LINK> ! <LINK>', ""the 2-point correlation function (2pcf) is a critical summary statistic for analyzing large-scale structure - i.e., learning about cosmology from the giant maps of galaxies we've made with redshift surveys. the 2pcf measures the strength of clustering as a function of scale."", 'we typically *estimate* the 2pcf by counting pairs of galaxies in bins. but - nature does not bin! we understand the shape of the 2pcf pretty well, and it is nice and smooth. this binning also results in the well-known bias-variance trade-off, trading accuracy for precision.', 'in our paper, we generalize the standard (Landy-Szalay) estimator, inspired by least-squares fitting. we replace binned pair counts with projections onto basis functions - any basis the user wants.\n\nfor a tophat basis, this reduces to the standard - just revealing the harsh bins! https://t.co/g11LkKvdAL', 'now we are free to choose basis functions relevant to the science use case. as a demo, we use a basis based on the standard baryon acoustic oscillation (BAO) fitting function.\n\nvoila, we get a more accurate 2pcf. it is now *continuous* - we can evaluate it at any separation. https://t.co/BDUWuw2DuO', 'even cooler, we did it with only 5 components, whereas the binned method needed 15! this is critical for estimating precise covariance matrices, which usually require thousands of mock catalogs due to the large number of bins.', 'we have lots of fun ideas for more applications, including understanding the luminosity dependence of clustering (people typically bin, once again, along the luminosity axis), and even testing basic cosmological principles by looking at the directional dependence of the 2pcf.', 'comments and feedback welcome! \n\nhuge shoutouts to everyone who has supported this project along the way, especially @davidwhogg &amp; @manodeepsinha 🙏', '@dalcantonJD @adrianprw thanks!!', '@sandyabuadas 🤩 thanks sandy!!', '@EstevesJH @msoares_santos thanks Johnny!! excited to hear, would be interested to chat about your ideas']",http://arxiv.org/abs/2011.01836,"The two-point correlation function (2pcf) is the key statistic in structure formation; it measures the clustering of galaxies or other density field tracers. Estimators of the 2pcf, including the standard Landy-Szalay (LS) estimator, evaluate the 2pcf in hard-edged separation bins, which is scientifically inappropriate and results in a poor trade-off between bias and variance. We present a new 2pcf estimator, the Continuous-Function Estimator, which generalizes LS to a continuous representation and obviates binning in separation or any other pair property. Our estimator, inspired by the mathematics of least-squares fitting, replaces binned pair counts with projections onto basis functions; it outputs the best linear combination of basis functions to describe the 2pcf. The choice of basis can take into account the expected form of the 2pcf, as well as its dependence on pair properties other than separation. We show that the Continuous-Function Estimator with a cubic-spline basis better represents the shape of the 2pcf compared to LS. We also estimate directly the baryon acoustic scale, using a small number of physically-motivated basis functions. Critically, this leads to a reduction in the number of mock catalogs required for covariance estimation, which is currently the limiting step in many 2pcf analyses. We discuss further applications of the Continuous-Function Estimator, including determination of the dependence of clustering on galaxy properties and searches for potential inhomogeneities or anisotropies in large-scale structure. ","Two-point statistics without bins: A continuous-function generalization
  of the correlation function estimator for large-scale structure"
112,1324389744164270081,2377407248,Daniel Whiteson,"['New paper!\n\n<LINK>\n“Learning to Identify Electrons”\n\nLed by J. Collado and J. Howard.\n\nImage networks reveal that there is unused info that can  suppress jet backgrounds. We TRANSLATE that black-box network into simple obs. not typically used for electron ID.', 'This was fun because electrons and jets use TWO images: from electromagnetic (left) and hadronic calorimeters (right). https://t.co/T9duJwZSUn', 'And as we expected, the images together outperform the physicist-designed variables. https://t.co/aaMDUF6OQp', 'And we used our black-box-translation technique ( https://t.co/c0j9Q5LWNO ) to find a simple observable that closes the gap!  This is the “Les Houches Angularity”, designed for quark/gluon separation. https://t.co/JMlNnblM5a', 'So we can use powerful image networks to identify missing information, and to point out an overlooked and powerful variable that captures that power, without needing to use the black box.\n\nThe NN taught us how to identify electrons!', 'So we can use powerful image networks to identify missing information, and to point out an overlooked and powerful variable that captures that power, without needing to use the black box.\n\nSo the NN taught us how to identify electrons!']",https://arxiv.org/abs/2011.01984,"We investigate whether state-of-the-art classification features commonly used to distinguish electrons from jet backgrounds in collider experiments are overlooking valuable information. A deep convolutional neural network analysis of electromagnetic and hadronic calorimeter deposits is compared to the performance of typical features, revealing a $\approx 5\%$ gap which indicates that these lower-level data do contain untapped classification power. To reveal the nature of this unused information, we use a recently developed technique to map the deep network into a space of physically interpretable observables. We identify two simple calorimeter observables which are not typically used for electron identification, but which mimic the decisions of the convolutional network and nearly close the performance gap. ",Learning to Identify Electrons
113,1324321597243740160,430217063,Dúalta Ó Fionnagáin,"['New paper was recently accepted to #MNRAS and is up on #arxiv now. We examined how the wind of Lambda Andromedae is driven (coronal or Alfvén waves) using the batsrus/awsom code. Our results are constrained with radio observations!\n\n<LINK> <LINK>', 'Plots showing the average temperature and density distribution of some of our different models (there are a few more models in the paper) https://t.co/2LWAQ0HWwx']",https://arxiv.org/abs/2011.02406,"We investigate the wind of lambda And, a solar-mass star that has evolved off the main sequence becoming a sub-giant. We present spectropolarimetric observations and use them to reconstruct the surface magnetic field of lambda And. Although much older than our Sun, this star exhibits a stronger (reaching up to 83 G) large-scale magnetic field, which is dominated by the poloidal component. To investigate the wind of lambda And, we use the derived magnetic map to simulate two stellar wind scenarios, namely a polytropic wind (thermally-driven) and an Alfven-wave driven wind with turbulent dissipation. From our 3D magnetohydrodynamics simulations, we calculate the wind thermal emission and compare it to previously published radio observations and more recent VLA observations, which we present here. These observations show a basal sub-mJy quiescent flux level at ~5 GHz and, at epochs, a much larger flux density (>37 mJy), likely due to radio flares. By comparing our model results with the radio observations of lambda And, we can constrain its mass-loss rate Mdot. There are two possible conclusions. 1) Assuming the quiescent radio emission originates from the stellar wind, we conclude that lambda And has Mdot ~ 3e-9 Msun/yr, which agrees with the evolving mass-loss rate trend for evolved solar-mass stars. 2) Alternatively, if the quiescent emission does not originate from the wind, our models can only place an upper limit on mass-loss rates, indicating that Mdot <~ 3e-9 Msun/yr. ",Lambda And: A post-main sequence wind from a solar-mass star
114,1324273234137780225,75249390,Axel Maas,"['We have published a new paper about the fact that the observed (left-handed) electron in the standard model is actually a bound state of the Higgs and the elementary electron in a proof-of-principle lattice calculation: <LINK>', 'The reason is that the left-handed particles in the standard model are weakly interacting, in contrast to the right-handed. Where handedness has something to do with spin.\n\nBecause the weak interactions is a gauge theory, this makes them unobservable on fundamental grounds.', 'This also affects quarks and neutrinos. It was predicted, 40 years ago, that the bound states have the same mass as the elementary ones. And the right-handed ones.\n\nThe simulations need to do a lot of approximations, but they do not affect the mechanism. And we see it work.', 'This has very interesting consequences, some of which we explored already earlier for #ILC in https://t.co/EhNqRI8CIn\n\nAs this also applies to quarks, we looked at this also at the #LHC in https://t.co/m81ISIEgfT\n\nNow we provide that this is indeed necessary and the way to go.', 'Of course, this is a big shift in the way we understand particles, even such common ones as electrons.\n\nThus, we will need experimental confirmation. But this will likely need the next generation of accelerators. But either we discover this, or there needs to be something new.', '@UlrikEgede We did not yet do the calculations, but we are already gearing up. Whatever happens, it will be small.\n\nBut we would expect that that this creates a generation-dependent effect, as the Higgs component couples differently, due to the masses.', '@UlrikEgede We know already that this will not affected s,t,u parameters, or anything else which is only sensitive to custodial-symmetry breaking effects, as this is not altered.\n\nMost of what we have is so far in the gauge/Higgs sector, as this is easier to calculate, but harder to measure.']",https://arxiv.org/abs/2011.02301,"Strict gauge invariance requires that physical left-handed leptons are actually bound states of the elementary left-handed lepton doublet and the Higgs field within the standard model. That they nonetheless behave almost like pure elementary particles is explained by the Fr\""ohlich-Morchio-Strocchi mechanism. Using lattice gauge theory, we test and confirm this mechanism for fermions. Though, due to the current inaccessibility of non-Abelian gauged Weyl fermions on the lattice, a model which contains vectorial leptons but which obeys all other relevant symmetries has been simulated. ",Testing the mechanism of lepton compositness
115,1324175572268797953,326843207,Yuta Notsu,"['Great news! Our new paper is accepted !!\n\n""Statistical Properties of Superflares on Solar-type Stars: Results Using All of the Kepler Primary Mission Data”\n\n<LINK>\nOkamoto, Notsu, Maehara, Namekata, Honda, Ikuta, Nogami, and Shibata, ApJ in press', 'We report the latest statistical analyses of superflares on solar-type stars using ""all"" of the (4-year) Kepler primary mission data, and Gaia-DR2catalog.  \n\nThe sample size of solar-type stars and Sun-like stars are ∼4 and ∼12 times, respectively, compared with Notsu+2019.', 'We found 2341 superflares on 265 solar-type stars, and 26 superflares on 15 Sun-like stars: the former increased from 527 to 2341 and the latter from 3 to 26 events compared with Notsu+2019. This enabled us to have a more well-established view on stat properties of superflares.', 'We updated the flare detection method from our previous studies by using high-pass filter to remove rotational variations caused by starspots. We also examined the sample biases on the frequency of superflares, taking into account gyrochronology and flare detection completeness.', 'The observed upper limit of the flare energy decreases as the rotation period increases in solar-type stars. The frequency of superflares decreases as the stellar rotation period increases. The maximum energy we found on Sun-like stars (P_rot&gt;20 day) is 4×10^34 erg.', 'One of the important conclusion from all Kepler data:\n\nOur analysis of Sun-like stars suggest that the Sun can cause superflares with energies of ∼7×10^33 erg (∼X700-class flares) and ∼1×10^34 erg (∼X1000-class flares) once every ∼3,000 years and ∼6,000 years, respectively.', '[Figure] Comparison between the frequency distribution of superflares on Sun-like stars and solar flares. https://t.co/y9Ockacyfb']",https://arxiv.org/abs/2011.02117,"We report the latest statistical analyses of superflares on solar-type (G-type main-sequence; effective temperature is 5100 - 6000 K) stars using all of the $Kepler$ primary mission data, and $Gaia$-DR2 (Data Release 2) catalog. We updated the flare detection method from our previous studies by using high-pass filter to remove rotational variations caused by starspots. We also examined the sample biases on the frequency of superflares, taking into account gyrochronology and flare detection completeness. The sample size of solar-type stars and Sun-like stars (effective temperature is 5600 - 6000 K and rotation period is over 20 days in solar-type stars) are $\sim$4 and $\sim$12 times, respectively, compared with Notsu et al. (2019, ApJ, 876, 58). As a result, we found 2341 superflares on 265 solar-type stars, and 26 superflares on 15 Sun-like stars: the former increased from 527 to 2341 and the latter from 3 to 26 events compared with our previous study. This enabled us to have a more well-established view on the statistical properties of superflares. The observed upper limit of the flare energy decreases as the rotation period increases in solar-type stars. The frequency of superflares decreases as the stellar rotation period increases. The maximum energy we found on Sun-like stars is $4 \times 10^{34}$ erg. Our analysis of Sun-like stars suggest that the Sun can cause superflares with energies of $\sim 7 \times 10^{33}$ erg ($\sim$X700-class flares) and $\sim 1 \times 10^{34}$ erg ($\sim$X1000-class flares) once every $\sim$3,000 years and $\sim$6,000 years, respectively. ","Statistical Properties of Superflares on Solar-type Stars: Results Using
  All of the Kepler Primary Mission Data"
116,1323915127645757440,714654290426667008,Fredrik Johansson,"['New paper: ""Calcium: computing in exact real and complex fields"". <LINK> Guaranteed to be a less stressful read than the election coverage.']",https://arxiv.org/abs/2011.01728,"Calcium is a C library for real and complex numbers in a form suitable for exact algebraic and symbolic computation. Numbers are represented as elements of fields $\mathbb{Q}(a_1,\ldots,a_n)$ where the extensions numbers $a_k$ may be algebraic or transcendental. The system combines efficient field operations with automatic discovery and certification of algebraic relations, resulting in a practical computational model of $\mathbb{R}$ and $\mathbb{C}$ in which equality is rigorously decidable for a large class of numbers. ",Calcium: computing in exact real and complex fields
117,1323880722604593153,1661323106,Robert Džudžar,"['My new first-author paper is out today on ArXiv (MNRAS accepted): ""Environmental processing of galaxies in HI-rich groups"" <LINK> \n\nKeywords: #galaxies #galaxygroups #ISM #atomichydrogen aka #HI and #choirs', ""Presented 13 HI-rich and late-type dominated groups called 'Choirs' https://t.co/bgYYeFbtaP"", 'In context to other galaxy samples #AMIGA #SING #HIWISE #xGASS #HIdeficient #HIexcess https://t.co/ho8C4Kcdlc', 'Their mid-infrared properties #WISE https://t.co/4ujL4AqLHK', 'It also contains lots of Figures* of actual #galaxies and #HI maps. \n*all made in #matplotlib, for example: https://t.co/DdPPlFTrIo', 'And sooo much more ... \nI will keep the other material for new tweets :)']",https://arxiv.org/abs/2011.01438,"We present and explore the resolved atomic hydrogen (HI) content of 13 HI-rich and late-type dominated groups denoted `Choirs'. We quantify the HI content of the Choir galaxies with respect to the median of the HI-mass fraction ($f_{\textrm{HI}}$) of their grandparent HIPASS sample. We find that the HI mass fraction of the Choir galaxies is dispersed around the HIPASS median in the range $-1.4 \leq \Delta f_{\textrm{HI}}\textrm{[dex]}\leq 0.7$, from HI-excess to HI-deficient galaxy regime. The HI-excess/HI-deficient galaxies contain more/less than 2.5 times their expected HI content with respect to the HIPASS median. We show and discuss that the environmental processing in Choirs occurs via tidal stripping and galaxy mergers. Our analysis suggests that tidal stripping contributes to the loss of the HI, while galaxy mergers contribute to the enhancement of the HI. Exploring the mid-infrared properties of Choir galaxies we find possible environmental processing in only nine Choir galaxies, which indicates that environmental processing is more perceptible in the HI content than the mid-infrared properties. Moreover, we find that environmental processing occurs in Choir groups regardless of their global environment, whether they are in isolation or in proximity to the denser structures, such as cosmic web filaments. We explore possible scenarios of the Choirs evolution, taking into account their HI content, velocity dispersion, crossing time and their global environment. We conclude that the most likely evolution for the majority of Choir groups is that they will become more compact as their members undergo multiple HI-rich mergers. ",Environmental processing of galaxies in HI-rich groups
118,1323851225046110209,27562677,Roland Matsouaka,"['We have a new paper on overlap weights (under review) that you can access on ArXiv under the title: \n\nA framework for causal inference in the presence of extreme inverse probability weights: the role of overlap weights\n\n<LINK>', ""First all, know that we advocate using overlap or matching weights (as opposed to PS trimming or truncation) when positivity is violated. They're more robust and precise under PS model misspecification, with lower variance inflation\nhttps://t.co/CUJOR3cxec"", ""What are the key take-aways:\n1. Overlap, Matching, (Shannon's) entropy, and Beta weights all target the same (sub)population of patients for whom there is (clinical) equipoise.\n2. Their estimands are often similar; only SEs differ, with OW having smaller ones"", ""3. Depending on the prevalence of trt., these estimands can be close to ATE, ATT, or ATC, i.e., they're flexible (or adaptable)\n4. Although, MW have been presented to us as analog to 1-on-1 matching, this is further from the truth. Thus, MW are better."", ""5. Augmentation of OW, MW, EW, and BW are not doubly robust like augmented IPW. However, they're still better when positivity is violated.\n6. We provide sandwich-variance estimation, which can be easily estimated instead of bootstrap"", ""7. Last, but not least, we provided extensively all the (tedious) theoretical proofs for those who want to dig deeper.\n\nAnd if you still have questions, do not hesitate to contact me at roland.matsouaka@duke.edu or @matsouaka, I'll be more than happy to help""]",https://arxiv.org/abs/2011.01388,"In this paper, we consider recent progress in estimating the average treatment effect when extreme inverse probability weights are present and focus on methods that account for a possible violation of the positivity assumption. These methods aim at estimating the treatment effect on the subpopulation of patients for whom there is a clinical equipoise. We propose a systematic approach to determine their related causal estimands and develop new insights into the properties of the weights targeting such a subpopulation. Then, we examine the roles of overlap weights, matching weights, Shannon's entropy weights, and beta weights. This helps us characterize and compare their underlying estimators, analytically and via simulations, in terms of the accuracy, precision, and root mean squared error. Moreover, we study the asymptotic behaviors of their augmented estimators (that mimic doubly robust estimators), which lead to improved estimations when either the propensity or the regression models are correctly specified. Based on the analytical and simulation results, we conclude that overall overlap weights are preferable to matching weights, especially when there is moderate or extreme violations of the positivity assumption. Finally, we illustrate the methods using a real data example marked by extreme inverse probability weights. ","A framework for causal inference in the presence of extreme inverse
  probability weights: the role of overlap weights"
119,1323806350841909248,1162181213475540992,Kaze Wong,['A paper in my series of O3 papers is out!\nWe use a deep learning enhanced population analysis framework to investigate what the @LIGO new catalogue says about primordial black holes. \n<LINK>\n#BlackHole #GravitationalWave #DeepLearning #GWTC2 #Inference <LINK>'],https://arxiv.org/abs/2011.01865,"Primordial black holes (PBHs) might be formed in the early Universe and could comprise at least a fraction of the dark matter. Using the recently released GWTC-2 dataset from the third observing run of the LIGO-Virgo Collaboration, we investigate whether current observations are compatible with the hypothesis that all black hole mergers detected so far are of primordial origin. We constrain PBH formation models within a hierarchical Bayesian inference framework based on deep learning techniques, finding best-fit values for distinctive features of these models, including the PBH initial mass function, the fraction of PBHs in dark matter, and the accretion efficiency. The presence of several spinning binaries in the GWTC-2 dataset favors a scenario in which PBHs accrete and spin up. Our results indicate that PBHs may comprise only a fraction smaller than $0.3 \%$ of the total dark matter, and that the predicted PBH abundance is still compatible with other constraints. ","Constraining the primordial black hole scenario with Bayesian inference
  and machine learning: the GWTC-2 gravitational wave catalog"
120,1323723123871477760,123421220,Yvette Cendes,"['Astronomer here! If you need a break from doomscrolling and/or waiting in line to vote, I have a new paper on the ArXiv today all about radio observations of a black hole that ripped apart a star and want to tell you about it!\n\nTHREAD 🧵\n\n<LINK>', 'Paper is called ""Radio Monitoring of the Tidal Disruption Event Swift J164449.3+573451. IV. Continued Fading and Non-Relativistic Expansion."" If you prefer non-Twitter threads btw, I also wrote up a layman\'s summary on my subreddit. /2\n\nhttps://t.co/pwB0gnxTfD', 'So, to begin, a tidal disruption event (TDE) happens when a star wanders too close to a black hole and gets ripped apart. About half the material falls onto the BH, half goes out in a shockwave/disc, and a TON of energy is released. More than in a supernova for example! /3 https://t.co/ljE9oU5T1k', ""Swift J1644+57 (Sw1644+57 for short) is special bc in 2011 the Swift satellite detected a ton of X-ray/gamma ray emission that didn't turn OFF. Origin was a galaxy ~3.5 billion years away. Reason? A relativistic jet was launched from the black hole when the TDE happened! /4 https://t.co/ezZtiIeJR8"", ""EVEN MORE CRAZY, ~1.5 years after this radio+X-ray observations showed the jet turned OFF. Normally they last thousands/millions of years, so this was super exciting! And no one predicted it'd happen! Or has seen one since. Conclusion: Sw1644+57 is awesome. /5"", ""My group has checked in on Sw1644+57 regularly with the VLA in New Mexico (pictured here) and @chandraxray since 2011. And my paper covers the latest observations for what's going on in this system! No one's observed a TDE this old before! /6 https://t.co/YyLWkpm5eZ"", ""And what did we find? First of all, it's fading in both radio and X-ray (no longer detectable there ☹️). This is consistent w a non-relativistic shockwave spreading out from the BH, interacting w its surroundings. /7"", ""However! The amazing thing about radio is we can calculate out a LOT of things about the shockwave from just the brightness at different frequencies! Radius, energy, magnetic field, even density of material. It's amazing! Pic here of what one of these curves looks like /8 https://t.co/SOmybLTpmM"", 'We updated a ton of these (see the full paper for gory details) but most impressive IMO is the density of gas surrounding the black hole 3.5 billion light years away!!! Similar to, but becoming less dense than that around M87* (black hole we have a picture of) /9 https://t.co/GeKdUue3K1', ""Seriously guys, it's so amazing how much we can learn about a galaxy so distant from studying late times post-TDE. We now know the density profile around this galaxy better than we do for our own Milky Way 🤯 /10"", ""But wait, there's more! The VLA is also doing an all sky survey right now (VLASS), and we discovered TWO new observations of Sw1644+57 in there! It's the first TDE discovered in VLASS data! (Of course, it helps if you know where to look, but still!) /11 https://t.co/SXLaApZCNk"", 'A LOT of people are interested in finding things like TDEs in VLASS, and we now have the longest light curve of how one of these evolves in radio. Conclusion: maybe one more jetted TDE in VLASS, but ~100 off-axis TDEs might still be in VLASS for others to find! /12 https://t.co/XNa3UST30j', ""In conclusion, I feel really happy that I got to work on this project- I've wanted to study Sw1644+57 for YEARS, and writing this paper was *exactly* what I dreamed I'd be doing as an astronomer someday. It's been fantastic and I've learned a lot- onto the next TDE! /13"", 'Also, I\'ll wrap up w the conclusions I *wish* my paper could have finished with if scientific papers allowed it- ""Swift J1644+57 may be fading in the sky, but will never fade in our hearts. ❤️"" /fin https://t.co/ndy2bZUDkc', '@LindaHosler Oh yeah dang it I totally wanted to throw that buzzword in there but forgot! This is alas years after the spaghettification.\n\nNext TDE though! :)']",https://arxiv.org/abs/2011.00074,"We present continued radio and X-ray observations of the previously relativistic tidal disruption event (TDE) Swift J164449.3+573451 (\sw) extending to about 9.4 years post disruption, as part of ongoing campaigns with the Jansky Very Large Array (VLA) and the \textit{Chandra} X-ray observatory. We find that the X-ray emission has faded below detectable levels, with an upper limit of $\lesssim 3.5\times 10^{-15}$ erg cm$^{-2}$ s$^{-1}$ in a 100 ks observation, while the radio emission continues to be detected and steadily fade. Both are consistent with forward shock emission from a non-relativistic outflow, although we find that the radio spectral energy distribution is better fit at these late times with an electron power law index of $p\approx 3$ (as opposed to $p\approx 2.5$ at earlier times). With the revised spectral index we find $\epsilon_B\approx 0.01$ using the radio and X-ray data, and a density of $\approx 0.04$ cm$^{3}$ at a radius of $R\approx 0.65$ pc ($R_{\rm sch}\approx 2\times 10^6$ R$_\odot$) from the black hole. The energy scale of the blastwave is $\approx 10^{52}$ erg. We also report detections of \sw\ at 3 GHz from the first two epochs of the VLA Sky Survey (VLASS), and find that $\sim 10^2$ off-axis \sw-like events to $z\sim 0.5$ may be present in the VLASS data. Finally, we find that \sw\ itself will remain detectable for decades at radio frequencies, although observations at sub-GHz frequencies will become increasingly important to characterize its dynamical evolution. ","Radio Monitoring of the Tidal Disruption Event Swift J164449.3+573451.
  IV. Continued Fading and Non-Relativistic Expansion"
121,1323663411134238720,704533062860681216,Patrick Coles,"['Congrats to our summer students, Angus Lowe and Max Hunter Gordon. Their new paper unifies two popular methods of error mitigation (EM) to make an even more powerful EM method.\n\n<LINK>\n\nw/ Piotr C., @AndrewArrasmit2, @LCincio <LINK>', 'Our method unifies Zero Noise Extrapolation (ZNE) with Clifford Data Regression (CDR). It can be viewed as guiding the extrapolation (in ZNE) with Clifford circuit examples. Rather than extrapolating “into the dark” with ZNE, Clifford circuits help to guide your extrapolation. https://t.co/Qzg0hosAxo', 'In our numerical benchmarks, our unified method outperforms both ZNE and CDR. See paper for additional details. \n\nThis work is part of our efforts with the Quantum Science Center @QuantumSciCtr https://t.co/tyEULlgdHJ']",https://arxiv.org/abs/2011.01157,"Achieving near-term quantum advantage will require effective methods for mitigating hardware noise. Data-driven approaches to error mitigation are promising, with popular examples including zero-noise extrapolation (ZNE) and Clifford data regression (CDR). Here we propose a novel, scalable error mitigation method that conceptually unifies ZNE and CDR. Our approach, called variable-noise Clifford data regression (vnCDR), significantly outperforms these individual methods in numerical benchmarks. vnCDR generates training data first via near-Clifford circuits (which are classically simulable) and second by varying the noise levels in these circuits. We employ a noise model obtained from IBM's Ourense quantum computer to benchmark our method. For the problem of estimating the energy of an 8-qubit Ising model system, vnCDR improves the absolute energy error by a factor of 33 over the unmitigated results and by factors 20 and 1.8 over ZNE and CDR, respectively. For the problem of correcting observables from random quantum circuits with 64 qubits, vnCDR improves the error by factors of 2.7 and 1.5 over ZNE and CDR, respectively. ",Unified approach to data-driven quantum error mitigation
122,1323610934909218818,1021018530689560577,Hang Le,"['Happy to share our recent work on a new architecture called the dual-decoder Transformer for joint speech recognition and multilingual speech translation (oral presentation @coling2020). \n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'This is joint work with my advisors @laurent_besacie @didier_schwab and colleagues at @facebookai @juanmiguelpino @ChanghanWang @thoma_gu. \n\nWe thank @facebookai @MIAI_UGA and @Genci_fr for their support.', ""@Genci_fr @laurent_besacie @didier_schwab @facebookai @juanmiguelpino @ChanghanWang @thoma_gu @MIAI_UGA Big thanks again to the @Genci_fr and #JeanZay team! I am very grateful to the generous resources and the great technical support that I've received.""]",https://arxiv.org/abs/2011.00747,"We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at this https URL ","Dual-decoder Transformer for Joint Automatic Speech Recognition and
  Multilingual Speech Translation"
123,1323596586694434821,1182331711683776517,Jon Ander Campos,"['Check out our new #COLING2020 paper on ""Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning""\n\nWe present a method that enables to improve an initial system after deployment using binary feedback only.\n\n<LINK>', 'Joint work with @kchonyc @antxaotegi @Aitor57 @gazkune and @eagirre', '@sazoo_nlp Thanks Sashank! Hope you are doing well too!']",http://arxiv.org/abs/2011.00615,"The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary (correct, incorrect) feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification (for development) and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. ","Improving Conversational Question Answering Systems after Deployment
  using Feedback-Weighted Learning"
124,1323582136964816899,2944948432,Edoardo Gallo,"['NEW WORKING PAPER\n\nSocial networks, confirmation bias, and shock elections (with Alastair Langtry)\n\n<LINK>\n\n#EconTwitter #networks #learning #confirmationbias #2020Election \n\nA thread <LINK>', '1/11\n\nWhat happens when learning from others is affected by confirmation bias? Learning is slower, society is more polarized, and influential individuals change.\n\nWhat about politics? Shock electoral results are more likely and fringe media adopt a more extreme ideology', '2/11\n\nSet-up. \n\nIndividuals are endowed with a signal about an underlying state of the world. They learn through a network using DeGroot – they revise their belief by taking a weighted average of their belief and their neighbors’ beliefs. Weights are determined by the network.', '3/11\n\nHow do we model confirmation bias?\n\nIndividuals cut connections with others who have beliefs further away from their own than a threshold. They reassign the weight of these severed connections to themselves, and never reinstate them thereafter.', '4/11\n\nResults.\n\nWhen individuals pay enough attention to themselves, confirmation bias slows down learning in any symmetric network. \n\n(symmetric means that if I listen to you then you listen to me, but how much weight we put on each other’s belief may differ)', '5/11\n\nSlower learning is not just a consequence of a sparser network, but it hinges on confirmation bias\n\nExample. (a) and (b) is confirmation bias as in the model. (c) is a different model where you reassign the weight to others – network is sparser but convergence is faster. https://t.co/HQkrbN5Vvc', '6/11\n\nWith confirmation bias, society is more polarized at each point in time (subject to mean-field assumption)\n\nWe characterize a subset of individuals who become more (less) influential with confirmation bias', '7/11\n\nWhat is the optimal network to minimize the adverse effects of conf bias? \n\nAssume a planner does not observe initial and/or level of bias. Given a fixed budget of links, optimal networks are symmetric, have no self-links and their unweighted equivalent is vertex-transitive', '8/11\n\nVoting (set-up)\n\nConsider a 2-candidate voting model, and focus on the interesting case where society votes for the same candidate before learning and after convergence\n\nDefine a shock election if the other candidate wins at any point in time during the learning process', '9/11\n\nVoting (result)\n\nUsing a mean-field assumption, we prove that a society never has shock elections without conf bias, but it can have shock elections with conf bias\n\nFig shows % of shock elections in simulations with (right) and without (left) confirmation bias https://t.co/AnCA4J9uH7', '10/11\n\nMedia\n\nConsider a Hotelling-style media market. Media choose their ideology and care about max their audience. Individuals choose to follow one and only one media org\n\nWe prove that the ideology of fringe media becomes more extreme as the strength of conf bias increases.', ""11/11\n\nPS What's the message to voters on US election day?\n\nGet informed by checking Biden and Trump’s tweets, talk to both your Dem and Rep friends, then go vote! You may still not make the best choice, but if everyone does the same as you do, the best candidate will win""]",http://arxiv.org/abs/2011.00520,"In recent years online social networks have become increasingly prominent in political campaigns and, concurrently, several countries have experienced shock election outcomes. This paper proposes a model that links these two phenomena. In our set-up, the process of learning from others on a network is influenced by confirmation bias, i.e. the tendency to ignore contrary evidence and interpret it as consistent with one's own belief. When agents pay enough attention to themselves, confirmation bias leads to slower learning in any symmetric network, and it increases polarization in society. We identify a subset of agents that become more/less influential with confirmation bias. The socially optimal network structure depends critically on the information available to the social planner. When she cannot observe agents' beliefs, the optimal network is symmetric, vertex-transitive and has no self-loops. We explore the implications of these results for electoral outcomes and media markets. Confirmation bias increases the likelihood of shock elections, and it pushes fringe media to take a more extreme ideology. ","Social networks, confirmation bias and shock elections"
125,1323580360979042304,176191683,Olga Zamora,"['🔴 ⭐️Phosphorus-rich stars update: new paper available today!. A second observation at the NOT telescope reveals an unknown nucleosynthesis pattern. Is this a new site for the s-process production other than AGB stars?.\nCheck our paper here 👇\n<LINK>', 'Our observations should guide stellar nucleosynthesis theoreticians and observers to identify the P-rich star progenitor, which represents anew site for s-process nucleosynthesis, with important implications for the chemical evolution of our Galaxy 😱.', 'And this is me taking the second spectra... I❤️ NOT 😂 https://t.co/o846C37eGy', '@APOGEEsurvey the mystery continues']",https://arxiv.org/abs/2011.00460,"The recently discovered phosphorus-rich stars pose a challenge to stellar evolution and nucleosynthesis theory, as none of the existing models can explain their extremely peculiar chemical abundances pattern. Apart from the large phosphorus enhancement, such stars also show enhancement in other light (O, Mg, Si, Al) and heavy (e.g., Ce) elements. We have obtained high-resolution optical spectra of two optically bright phosphorus-rich stars (including a new P-rich star), for which we have deter-mined a larger number of elemental abundances (from C to Pb). We confirm the unusual light-element abundance pattern with very large enhancements of Mg, Si, Al, and P, and possibly some Cu enhancement, but the spectra of the new P-rich star is the only one to reveal some C(+N) enhancement.When compared to other appropriate metal-poor and neutron-capture enhanced stars, the two P-rich stars show heavy-element overabundances similar to low neutron density s-process nucleosynthesis,with high first- (Sr, Y, Zr) and second-peak (Ba, La, Ce, Nd) element enhancements (even some Pb enhancement in one star) and a negative [Rb/Sr] ratio. However, this s-process is distinct from the one occurring in asymptotic giant branch (AGB) stars. The notable distinctions encompass larger[Ba/La] and lower Eu and Pb than their AGB counterparts. Our observations should guide stellar nucleosynthesis theoreticians and observers to identify the P-rich star progenitor, which represents anew site for s-process nucleosynthesis, with important implications for the chemical evolution of our Galaxy. ",Heavy element abundances in P-rich stars: A new site for the s-process?
126,1323549929097195521,1212029940033409024,Hiroyuki Kurokawa 黒川宏之,"[""Our new paper on a dwarf planet Ceres is now available from a preprint server! [2011.00157] A probabilistic approach to determination of Ceres' average surface composition from Dawn VIR and GRaND data <LINK>""]",https://arxiv.org/abs/2011.00157,"The Visible-Infrared Mapping Spectrometer (VIR) on board the Dawn spacecraft revealed that aqueous secondary minerals -- Mg-phyllosilicates, NH4-bearing phases, and Mg/Ca carbonates -- are ubiquitous on Ceres. Ceres' low reflectance requires dark phases, which were assumed to be amorphous carbon and/or magnetite (~80 wt.%). In contrast, the Gamma Ray and Neutron Detector (GRaND) constrained the abundances of C (8-14 wt.%) and Fe (15-17 wt.%). Here, we reconcile the VIR-derived mineral composition with the GRaND-derived elemental composition. First, we model mineral abundances from VIR data, including either meteorite-derived insoluble organic matter, amorphous carbon, magnetite, or combination as the darkening agent and provide statistically rigorous error bars from a Bayesian algorithm combined with a radiative-transfer model. Elemental abundances of C and Fe are much higher than is suggested by the GRaND observations for all models satisfying VIR data. We then show that radiative transfer modeling predicts higher reflectance from a carbonaceous chondrite of known composition than its measured reflectance. Consequently, our second models use multiple carbonaceous chondrite endmembers, allowing for the possibility that their specific textures or minerals other than carbon or magnetite act as darkening agents, including sulfides and tochilinite. Unmixing models with carbonaceous chondrites eliminate the discrepancy in elemental abundances of C and Fe. Ceres' average reflectance spectrum and elemental abundances are best reproduced by carbonaceous-chondrite-like materials (40-70 wt.%), IOM or amorphous carbon (10 wt.%), magnetite (3-8 wt.%), serpentine (10-25 wt.%), carbonates (4-12 wt.%), and NH4-bearing phyllosilicates (1-11 wt.%). ","A probabilistic approach to determination of Ceres' average surface
  composition from Dawn VIR and GRaND data"
127,1323504640093278208,526115229,Kevin Heng,"['Our new paper on the SEIR model of epidemiology, to be published in Scientific Reports (Springer Nature). It is a collaboration between myself and the epidemiologist Christian Althaus (@C_Althaus). @uniofbern @unibern @opensciencebern [1/n]\n\n<LINK>', 'The idea arose when I was calculating the models for a previous paper on COVID (https://t.co/FwsZUPE1Rh). Even when different incubation and infectious periods were entered into the Python code, I had noticed that the SEIR model curves had very similar shapes. [2/n]', 'This naturally led to the question of whether there was a way to prove this on paper. The beauty of the physical sciences is how seemingly distinct problems are described by the same mathematical equations. [3/n]', 'Tthe equations governing the SEIR epidemiological model are similar to the ones of atmospheric chemistry. When time in the SEIR model is rescaled by a simple factor, all of the curves (regardless of incubation/infectious period) lie roughly on top of one another. [4/n]', 'It builds on a beautiful paper by Harko et al. (2014); interestingly, the first author of that study is a cosmologist. In both this and our paper, the solutions are fully non-linear. In our case, a single approximation is taken, i.e. a perturbation approach is not used. [5/n].', '@joel_c_miller Your 2017 paper, which we cited, was definitely one of the inspirations!', 'It is a great start to the INPUT methods platform on data analysis and numerical modeling that is a collaboration between the CSH and @ISPMBern. Christian has been named the platform leader. We anticipate more synergies between astrophysics and epidemiology. [6/n] @OscarH_Franco', '@joel_c_miller The way it is written seems like a trivial step and approximation (we assume the third derivative of R equals zero). But figuring out the algebraic tricks in between took some hours, as it always does. It is work that is hard to explain and hard for others to appreciate.', 'Another interesting lesson I learnt: unlike in astrophysics, it is hard to get mathematical papers to be even *read* by reviewers in epidemiology. Several journals we submitted to rejected the manuscript at the editorial level, i.e., it never even passed to the reviewers. [7/n]', 'But a silver lining behind this cloud: learning that Scientific Reports *does* accept mathematical papers filled with equations, unlike @nature, Science or @NatureAstronomy! Will definitely submit there again. [8/n].']",https://arxiv.org/abs/2011.00378,"Compartmental transmission models have become an invaluable tool to study the dynamics of infectious diseases. The Susceptible-Infectious-Recovered (SIR) model is known to have an exact semi-analytical solution. In the current study, the approach of Harko et al. (2014) is generalised to obtain an approximate semi-analytical solution of the Susceptible-Exposed-Infectious-Recovered (SEIR) model. The SEIR model curves have nearly the same shapes as the SIR ones, but with a stretch factor applied to them across time that is related to the ratio of the incubation to infectious periods. This finding implies an approximate characteristic timescale, scaled by this stretch factor, that is universal to all SEIR models, which only depends on the basic reproduction number and initial fraction of the population that is infectious. ","The approximately universal shapes of epidemic curves in the
  Susceptible-Exposed-Infectious-Recovered (SEIR) model"
128,1323460693136273408,370013804,Kei Ohta,['Our new paper presents reactive planning for robots at #CoRL2020. The proposed deep reactive planning framework allows robots to develop reflexes to adapt to changes in its environment. Check out the paper  <LINK> and a descriptive video  <LINK>'],https://arxiv.org/abs/2011.00155,"The main novelty of the proposed approach is that it allows a robot to learn an end-to-end policy which can adapt to changes in the environment during execution. While goal conditioning of policies has been studied in the RL literature, such approaches are not easily extended to cases where the robot's goal can change during execution. This is something that humans are naturally able to do. However, it is difficult for robots to learn such reflexes (i.e., to naturally respond to dynamic environments), especially when the goal location is not explicitly provided to the robot, and instead needs to be perceived through a vision sensor. In the current work, we present a method that can achieve such behavior by combining traditional kinematic planning, deep learning, and deep reinforcement learning in a synergistic fashion to generalize to arbitrary environments. We demonstrate the proposed approach for several reaching and pick-and-place tasks in simulation, as well as on a real system of a 6-DoF industrial manipulator. A video describing our work could be found \url{this https URL}. ",Deep Reactive Planning in Dynamic Environments
129,1323455082197897217,2930047588,Andrew Vanderburg,"['Exciting new paper from Zoe de Beurs (@AstroZo2o, undergraduate at UT Austin working with me, Chris Shallue, and the HARPS-N team) on correcting stellar activity in radial velocity observations! <LINK>', ""Some background: the radial velocity method has been used to discover and characterize planets for decades by measuring the Doppler shift of the star's spectral lines as the planet tugs on the star in its orbit. https://t.co/ElQm8uN8KA"", 'Over the years, as our instruments have become more stable and precise, we have steadily gotten better and better at measuring these tiny shifts. But in the last decade or so, we have stopped finding smaller planets with radial velocities (red line in figure). Why? https://t.co/Ite61cZiry', 'The problem is stellar activity. Inhomogeneities on the surface of the star due to things like starspots and faculae introduce changes to the shape of spectral lines that can mimic and mask Doppler shifts due to small planets. https://t.co/0O0MrppaOR', 'There has been a LOT of work trying to solve this problem, and people (especially Raphaëlle Haywood) have found success modeling stellar activities with Gaussian Process regression, but this method requires high-cadence observations, and is difficult to schedule on telescopes.', ""Zoe's work shows another possible solution to this problem: machine learning and neural networks. The idea is: train a machine learning (either linear regression or neural network) model to predict activity signals based on the changing shape of spectral lines. https://t.co/oZyfTiW4Va"", ""Here's what it looks like in practice: these curves in the top panel are changes to the average shapes of spectral lines on the Sun, as observed by the HARPS-N solar telescope, color coded by the apparent shift due to stellar activity. Bottom shows predictions by the network. https://t.co/eJ8d4FIqGl"", 'When Zoe ""corrects"" the HARPS-N solar telescope RVs by subtracting these predictions, she reduces the scatter from 1.5 m/s to 78 cm/s - a factor of two improvement! The spurious signals at the Sun\'s rotation period are totally suppressed! https://t.co/351gmL0I1f', ""Even better, the machine learning model doesn't require any information about WHEN the observation took place, so in principle, there is no need to observe as often as for GP regression. I think this is super promising and we look forward to trying it on stars other than the Sun!"", ""We're also excited to try it out on data from new instruments like @NEID_at_WIYN, @espresso_astro, MaroonX and EXPRES - maybe using a technique like this to mitigate stellar activity can help these new instruments reach their ultimate precision."", ""Finally -  Zoe (@AstroZo2o) is a graduating senior this year and will be applying to graduate school this fall, so keep an eye out for her! Remember her name - I think we're going to be hearing a lot more from her in the future!""]",https://arxiv.org/abs/2011.00003,"Exoplanet detection with precise radial velocity (RV) observations is currently limited by spurious RV signals introduced by stellar activity. We show that machine learning techniques such as linear regression and neural networks can effectively remove the activity signals (due to starspots/faculae) from RV observations. Previous efforts focused on carefully filtering out activity signals in time using modeling techniques like Gaussian Process regression (e.g. Haywood et al. 2014). Instead, we systematically remove activity signals using only changes to the average shape of spectral lines, and no information about when the observations were collected. We trained our machine learning models on both simulated data (generated with the SOAP 2.0 software; Dumusque et al. 2014) and observations of the Sun from the HARPS-N Solar Telescope (Dumusque et al. 2015; Phillips et al. 2016; Collier Cameron et al. 2019). We find that these techniques can predict and remove stellar activity from both simulated data (improving RV scatter from 82 cm/s to 3 cm/s) and from more than 600 real observations taken nearly daily over three years with the HARPS-N Solar Telescope (improving the RV scatter from 1.47 m/s to 0.78 m/s, a factor of ~ 1.9 improvement). In the future, these or similar techniques could remove activity signals from observations of stars outside our solar system and eventually help detect habitable-zone Earth-mass exoplanets around Sun-like stars. ","Identifying Exoplanets with Deep Learning. IV. Removing Stellar Activity
  Signals from Radial Velocity Measurements Using Neural Networks"
130,1333658260696653825,233516803,Tosin Adewumi,"['Thankfully🥳, we present our new paper this month at NeurIPS ML4D on Yorùbá embeddings (<LINK>). You can watch the 1-minute video of it here: <LINK> \n\n#NLP \n#NLProc \n@ML4Dworkshop \n@NeurIPSConf']",https://arxiv.org/abs/2011.07605,"The major contributions of this work include the empirical establishment of a better performance for Yoruba embeddings from undiacritized (normalized) dataset and provision of new analogy sets for evaluation. The Yoruba language, being a tonal language, utilizes diacritics (tonal marks) in written form. We show that this affects embedding performance by creating embeddings from exactly the same Wikipedia dataset but with the second one normalized to be undiacritized. We further compare average intrinsic performance with two other work (using analogy test set & WordSim) and we obtain the best performance in WordSim and corresponding Spearman correlation. ",The Challenge of Diacritics in Yoruba Embeddings
131,1333440719986565121,1845056570,Zan Gojcic,"['Happy to present our new work ""Predator: Registration of 3D Point Clouds with Low Overlap"" which was done together with @ShengyHuang, Mikhail Usvyatsov, Andreas Wieser, and Konrad Schindler. \n\nPaper: <LINK>\nSource code: <LINK>\n#3D #ComputerVision <LINK>', 'The intuition behind Predator is that when registering point clouds with low overlap one should not only aim to sample salient points but rather salient points that lie in the overlap region. We propose an overlap-attention block that enables us to infer the overlap region, 1/3', ', condition the subsequent decoding on the respective other point cloud, and helps us guide the sampling of the interest points. The ability to focus on the points that lie in the overlap region enables us to significantly outperform the current SOTA results on traditional 2/3', 'and newly introduced low-overlapping benchmark datasets. 3/3', '@kysucix @ShengyHuang Point-cloud REgistration with Deep ATtention to the Overlap Region 😆 borderline :)', '@kysucix @AliOsmanUlusoy @ShengyHuang Hah this brilliant title puts a lot of pressure on us 😅']",https://arxiv.org/abs/2011.13005,"We introduce PREDATOR, a model for pairwise point-cloud registration with deep attention to the overlap region. Different from previous work, our model is specifically designed to handle (also) point-cloud pairs with low overlap. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds. The ability to focus on points that are relevant for matching greatly improves performance: PREDATOR raises the rate of successful registrations by more than 20% in the low-overlap scenario, and also sets a new state of the art for the 3DMatch benchmark with 89% registration recall. ",PREDATOR: Registration of 3D Point Clouds with Low Overlap
132,1333354438166253570,1331335884,Andrea Vanzo,['GCNs + syntax = new SOTA on #framenet 1.5! We show how encoding paths of the constituency tree through GCNs improves shallow semantic parsing! Check out our new paper --&gt; <LINK>. Joint work with @emabastiano @oliverlemon #NLProc <LINK>'],https://arxiv.org/abs/2011.13210,"We study the problem of integrating syntactic information from constituency trees into a neural model in Frame-semantic parsing sub-tasks, namely Target Identification (TI), FrameIdentification (FI), and Semantic Role Labeling (SRL). We use a Graph Convolutional Network to learn specific representations of constituents, such that each constituent is profiled as the production grammar rule it corresponds to. We leverage these representations to build syntactic features for each word in a sentence, computed as the sum of all the constituents on the path between a word and a task-specific node in the tree, e.g. the target predicate for SRL. Our approach improves state-of-the-art results on the TI and SRL of ~1%and~3.5% points, respectively (+2.5% additional points are gained with BERT as input), when tested on FrameNet 1.5, while yielding comparable results on the CoNLL05 dataset to other syntax-aware systems. ","Encoding Syntactic Constituency Paths for Frame-Semantic Parsing with
  Graph Convolutional Networks"
133,1332249512505733120,38941661,Daniel Stilck França,"['This week I had a new paper with the great @RaulGarciaPatr1, A game of quantum advantage: linking verification and simulation (<LINK>) and I forgot the most important part: tweeting about it! So here we go: (1/n)', 'we investigate the following question: can you efficiently verify that the output of a quantum circuit supposed to demonstrate a quantum advantage is not a classically ""easy"" distribution? For instances, that it is not the uniform distribution?', 'we formulate this question as a game between Bob wanting to show quantum advantage to a skeptical Alice. The game works as follows: Alice is allowed to propose an ""easy"" mock distribution that is supposed to reproduce the statistics of Bob\'s device up to reasonable doubt.', ""It is then Bob's task to refute this hypothesis. This is done by providing a function whose expectation value Alice's mock distribution cannot reproduce. Alice is then allowed to update her hypothesis based on this new evidence."", ""The game finishes if Alice cannot reproduce Bob's statistics or if Bob cannot distinguish his device from Alice's mock distribution."", ""For instance, if Bob claims he has an advantage for an optimization problem, then it is easy for him to find a strategy: compare the value achieved by his device vs. Alice's. If he indeed can achieve a lower value than Alice, the debate is settled. But what about random circuits?"", ""we show that if Bob can find functions that can be computed efficiently and are able to distinguish his device from Alice's, then Alice can also efficiently approximately simulate the output of Bob's device."", 'That is, the distinguishing functions can be used to learn an ""easy"" distribution  that is close to the output of the random circuit. So it is unlikely that Bob could win such a game, unless efficient approximate simulation of random circuits is possible.', 'But what about just distinguishing from the uniform distribution efficiently, the simplest sanity check? For random circuits this is also unlikely to be possible. We show that this could then be used to efficiently fool the heavy output generation problem (hog).', 'Thus, even the simplest verification procedures for random circuits are likely to require exponential resources. Or efficient approximate classical simulation is possible!', 'All the proofs rely on the fact that the output of random quantum circuits is likely to have essentially maximum Shannon entropy. This allows us to use mirror descent to quickly learn a distribution and rejection sampling to sample from it.', 'As a bonus we also discuss how noise affects this whole discussion! Check the paper out for more details! And thanks again to @RaulGarciaPatr1 for another nice collaboration!']",https://arxiv.org/abs/2011.12173,"We present a formalism that captures the process of proving quantum superiority to skeptics as an interactive game between two agents, supervised by a referee. Bob, is sampling from a classical distribution on a quantum device that is supposed to demonstrate a quantum advantage. The other player, the skeptical Alice, is then allowed to propose mock distributions supposed to reproduce Bob's device's statistics. He then needs to provide witness functions to prove that Alice's proposed mock distributions cannot properly approximate his device. Within this framework, we establish three results. First, for random quantum circuits, Bob being able to efficiently distinguish his distribution from Alice's implies efficient approximate simulation of the distribution. Secondly, finding a polynomial time function to distinguish the output of random circuits from the uniform distribution can also spoof the heavy output generation problem in polynomial time. This pinpoints that exponential resources may be unavoidable for even the most basic verification tasks in the setting of random quantum circuits. Beyond this setting, by employing strong data processing inequalities, our framework allows us to analyse the effect of noise on classical simulability and verification of more general near-term quantum advantage proposals. ",A game of quantum advantage: linking verification and simulation
134,1332005066941882370,475760077,Dr Sarah Casewell,"['New paper alert! A through analysis of a massive, inflated brown dwarf in a close orbit with a hot subdwarf! Long term monitoring with the fabulous ULTRACAM instrument has meant we actually observe the orbit shrinking as the binary synchronises.\n\n<LINK>', '@nickcasewell A giant star swallowed its brown dwarf companion, but the brown dwarf survived, it’s a massive brown dwarf, but physically bigger than it should be. It now orbits a 25000 degree not-quite white dwarf in 2.5 hrs. But, we have observed this over 10 yrs and the orbit is shrinking!', '@nickcasewell Pretty much! You have a while to wait, but in 2.2 billion years the brown dwarf will be close enough for the subdwarf to start pulling mass off it and it will become what’s called a cataclysmic variable. And then it’s basically doomed.', '@nickcasewell Could be a while ...']",https://arxiv.org/abs/2011.10013,"Subdwarf B stars are core-helium burning stars located on the extreme horizontal branch. Extensive mass loss on the red giant branch is necessary to form them. It has been proposed that substellar companions could lead to the required mass-loss when they are engulfed in the envelope of the red giant star. J08205+0008 was the first example of a hot subdwarf star with a close, substellar companion candidate to be found. Here we perform an in-depth re-analysis of this important system with much higher quality data allowing additional analysis methods. From the higher resolution spectra obtained with ESO-VLT/XSHOOTER we derive the chemical abundances of the hot subdwarf as well as its rotational velocity. Using the { it Gaia} parallax and a fit to the spectral energy distribution in the secondary eclipse, tight constraints to the radius of the hot subdwarf are derived. From a long-term photometric campaign we detected a significant period decrease of $-3.2(8)\cdot 10^{-12} \,\rm dd^{-1}$. This can be explained by the non-synchronised hot subdwarf star being spun up by tidal interactions forcing it to become synchronised. From the rate of period decrease we could derive the synchronisation timescale to be 4 Myr, much smaller than the lifetime on EHB. By combining all different methods we could constrain the hot subdwarf to a mass of $0.39-0.50\,\rm M_\odot$ and a radius of $R_{\rm sdB}=0.194\pm0.008\,\rm R_\odot$, and the companion to $0.061-0.071\rm\,M_\odot$ with a radius of $R_{\rm comp}=0.092 \pm 0.005\,\rm R_\odot$, below the hydrogen burning limit. We therefore confirm that the companion is most likely a massive brown dwarf. ","A quantitative in-depth analysis of the prototype sdB+BD system SDSS
  J08205+0008 revisited in the Gaia era"
135,1330186928654782468,2729061,Cory Doctorow,"['""Underspecification Presents Challenges for Credibility in Modern Machine Learning"" is a new ML paper co-authored by 33 (!) Google researchers. It\'s been called a ""wrecking ball"" for our understanding of problems in machine learning.\n\n<LINK>\n\n1/ <LINK>', 'There\'s been a lot of work on the problems of inadequate, low-quality, biased or poorly labeled training date in machine learning classifiers (""garbage in, garbage out""), but that\'s not what these researchers are documenting. \n\n2/', 'They\'re focused on ""underspecification,"" a well-known statistical phenomenon that has not been at the center of machine learning analysis (until now). \n\n3/', ""It's a gnarly concept, and I quickly found myself lost while reading the original paper; thankfully, @strwbilly did a great breakdown for MIT Tech Review.\n\nhttps://t.co/KqDCQjQaUf\n\n4/"", '""Underspecification,"" appears to be the answer to a longstanding problem in ML: why do models that work well in the lab fail in the field? Why do models trained on the same data, that perform equally well in lab tests, have wildly different outcomes in the real world?\n\n5/', 'The answer appears to be minor, random variations: starting values for nodes in the neural net; the means by which training data is considered; the number of training runs. \n\n6/', 'These differences were considered unimportant, but they appear to explain why models that perform the same in the lab are very different in the field. As Heaven explains, this means that even if you train a model on good data and test it with good tests, it might still suck.\n\n7/', ""The paper describes the researchers' experiment to validate this hypothesis: they created 50 variations on a visual classifier, trained on the standard Imagenet data-set, each with random variations in the values of the nodes in the neural net.\n\n8/"", 'They selected models that performed with near-equivalence on data retained from the training set for testing, and then they stress-tested these equally ranked models with Imagenet-C (a distorted subset of Imagenet) and Objectnet (a set of common objects in unusual poses).\n\n9/', ""The models' stress-test outcomes were hugely variant. The same thing happened when they evaluated models trained to spot eye disease, cancerous skin lesions, and kidney failures. \n\n10/"", 'Even more confounding: models that performed well on (say) pixelated images underperformed on (say) low-contrast images - even the ""good"" models were not good at everything.\n\n11/', ""Heaven says that addressing this will involve a huge expense: producing many variant models and testing them against many real-world conditions. It's the kind of thing Google can afford to do, but which may be out of reach of smaller firms.\n\neof/""]",https://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain. ","Underspecification Presents Challenges for Credibility in Modern Machine
  Learning"
136,1329540294744928258,2238437131,Matthew Stib,"['“Underspecification” in training may be another reason machine-learning models fail when applied to real-world data, cites a new paper by #google.\n\nHighlights the need for caution when applying an algorithm, particularly in healthcare. #AI \n\n<LINK>']",https://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain. ","Underspecification Presents Challenges for Credibility in Modern Machine
  Learning"
137,1329374673470099456,1329362804185423874,Javier Moreno-Gordo,"['Our new paper ""Temperature chaos is present in off-equilibrium spin-glass dynamics"" is here <LINK>']",https://arxiv.org/abs/2011.09419,"We find a dynamic effect in the non-equilibrium dynamics of a spin glass that closely parallels equilibrium temperature chaos. This effect, that we name dynamic temperature chaos, is spatially heterogeneous to a large degree. The key controlling quantity is the time-growing spin-glass coherence length. Our detailed characterization of dynamic temperature chaos paves the way for the analysis of recent and forthcoming experiments. This work has been made possible thanks to the most massive simulation to date of non-equilibrium dynamics, carried out on the Janus~II custom-built supercomputer. ",Temperature chaos is present in off-equilibrium spin-glass dynamics
138,1329374025576030213,4866589137,Dr. Nathan Adams,"['Paper day! Today on arxiv the MIGHTEE team show off the HI-emission component of a new 20 square degree MeerKAT survey, featuring some teasers of the science we are already able to do with a fraction of the data. I helped make figure 9 😁\n\n<LINK> <LINK>']",https://arxiv.org/abs/2011.09470,"We present the HI emission project within the MIGHTEE survey, currently being carried out with the newly commissioned MeerKAT radio telescope. This is one of the first deep, blind, medium-wide interferometric surveys for neutral hydrogen (HI) ever undertaken, extending our knowledge of HI emission to z=0.6. The science goals of this medium-deep, medium-wide survey are extensive, including the evolution of the neutral gas content of galaxies over the past 5 billion years. Simulations predict nearly 3000 galaxies over 0<z<0.4 will be detected directly in HI, with statistical detections extending to z=0.6. The survey allows us to explore HI as a function of galaxy environment, with massive groups and galaxy clusters within the survey volume. Additionally, the area is large enough to contain as many as 50 local galaxies with HI mass $<10^8$ Msun, which allows us to study the low-mass galaxy population. The 20 deg$^2$ main survey area is centred on fields with exceptional multi-wavelength ancillary data, with photometry ranging from optical through far-infrared wavelengths, supplemented with multiple spectroscopic campaigns. We describe here the survey design and the key science goals. We also show first results from the Early Science observations, including kinematic modelling of individual sources, along with the redshift, HI, and stellar mass ranges of the sample to date. ",MIGHTEE-HI: The HI emission project of the MeerKAT MIGHTEE survey
139,1329345548575535106,3313806489,Tim Roberts,['📣NEW PAPER KLAXON📣\n\n<LINK>\n\nEnjoy.'],https://arxiv.org/abs/2011.08870,"We report on the discovery of a new, transient ultraluminous X-ray source (ULX) in the galaxy NGC 7090. This new ULX, which we refer to as NGC 7090 ULX3, was discovered via monitoring with $Swift$ during 2019-20, and to date has exhibited a peak luminosity of $L_{\rm{X}} \sim 6 \times 10^{39}$ erg s$^{-1}$. Archival searches show that, prior to its recent transition into the ULX regime, ULX3 appeared to exhibit a fairly stable luminosity of $L_{\rm{X}} \sim 10^{38}$ erg s$^{-1}$. Such strong long-timescale variability may be reminiscent of the small population of known ULX pulsars, although deep follow-up observations with $XMM$-$Newton$ and $NuSTAR$ do not reveal any robust X-ray pulsation signals. Pulsations similar to those seen from known ULX pulsars cannot be completely excluded, however, as the limit on the pulsed fraction of any signal that remains undetected in these data is $\lesssim$20\%. The broadband spectrum from these observations is well modelled with a simple thin disc model, consistent with sub-Eddington accretion, which may instead imply a moderately large black hole accretor ($M_{\rm{BH}} \sim 40 ~ M_{\odot}$). Similarly, though, more complex models consistent with the super-Eddington spectra seen in other ULXs (and the known ULX pulsars) cannot be excluded given the limited signal-to-noise of the available broadband data. The nature of the accretor powering this new ULX therefore remains uncertain. ",A New Transient Ultraluminous X-ray Source in NGC 7090
140,1329160299241381888,956056447502123010,Yuen-Zhou Research Group UCSD,['Could the effects of optical cavities in thermally-activated reactions be due to cavity photon leakage rather than the reshaping of potential energy surfaces? Take a look at the new paper by Matt and @JorgeACamGA <LINK>'],https://arxiv.org/abs/2011.08445,"In vibrational strong coupling (VSC), molecular vibrations strongly interact with the modes of an optical cavity to form hybrid light-matter states known as vibrational polaritons. Experiments show that the kinetics of thermally activated chemical reactions can be modified by VSC. Transition-state theory, which assumes that internal thermalization is fast compared to reactive transitions, has been unable to explain the observed findings. Here, we carry out kinetic simulations to understand how dissipative processes, namely those that VSC introduces to the chemical system, affect reactions where internal thermalization and reactive transitions occur on similar timescales. Using the Marcus-Levich-Jortner type of electron transfer as a model reaction, we show that such dissipation can change reactivity by accelerating internal thermalization, thereby suppressing nonequilibrium effects that occur in the reaction outside the cavity. This phenomenon is attributed mainly to cavity decay (i.e., photon leakage), but a supporting role is played by the relaxation between polaritons and dark states. When nonequilibrium effects are already suppressed in the bare reaction (the reactive species are essentially at internal thermal equilibrium throughout the reaction), we find that reactivity does not change significantly under VSC. Connections are made between our results and experimental observations. ","Nonequilibrium effects of cavity leakage and vibrational dissipation in
  thermally-activated polariton chemistry"
141,1328375255975411712,486852521,Christoforos Mavrogiannis,"['New paper on multiagent trajectory prediction for navigation at uncontrolled intersections appearing at @corl_conf. Joint work with Junha Roh, @rishabhmadan96, Dieter Fox and @siddhss5. \n\npaper: <LINK>\nproject: <LINK>\n\n<LINK>', 'Uncontrolled Intersections are especially challenging for autonomous vehicles. The lack of traffic rules and communication and the complex interaction of multiple agents results in high uncertainty over a huge space of outcomes.', 'We leverage a notion of topological invariance to collapse the space of multivehicle interactions at an intersection into a finite set of modes. Each mode represents a unique pattern of multiagent behavior, encoding discrete and continuous properties of interaction.', 'We train a GNN-based architecture (MTP) to perform multiagent trajectory prediction conditioned on a distribution over modes. Our model achieves Sota performance on a trajectory prediction task.', 'Based on MTP, we build MTPnav, a navigation planner. MTPnav generates multiagent trajectory predictions for likely modes, evaluates them and follows the ego-trajectory from the best mode. MTPnav achieves safe and efficient navigation in challenging intersection scenarios.', '@PeteTrautman @corl_conf @rishabhmadan96 @siddhss5 Thanks, Pete! Haha, we are a big team 😀']",https://arxiv.org/abs/2011.03894,"We focus on decentralized navigation among multiple non-communicating rational agents at \emph{uncontrolled} intersections, i.e., street intersections without traffic signs or signals. Avoiding collisions in such domains relies on the ability of agents to predict each others' intentions reliably, and react quickly. Multiagent trajectory prediction is NP-hard whereas the sample complexity of existing data-driven approaches limits their applicability. Our key insight is that the geometric structure of the intersection and the incentive of agents to move efficiently and avoid collisions (rationality) reduces the space of likely behaviors, effectively relaxing the problem of trajectory prediction. In this paper, we collapse the space of multiagent trajectories at an intersection into a set of modes representing different classes of multiagent behavior, formalized using a notion of topological invariance. Based on this formalism, we design Multiple Topologies Prediction (MTP), a data-driven trajectory-prediction mechanism that reconstructs trajectory representations of high-likelihood modes in multiagent intersection scenes. We show that MTP outperforms a state-of-the-art multimodal trajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24% on a challenging simulated dataset. Finally, we show that MTP enables our optimization-based planner, MTPnav, to achieve collision-free and time-efficient navigation across a variety of challenging intersection scenarios on the CARLA simulator. ","Multimodal Trajectory Prediction via Topological Invariance for
  Navigation at Uncontrolled Intersections"
142,1328148451960434693,1166165104808931328,Zahra Tabrizi,"['New paper: \nAxion-like Particles at Future Neutrino Experiments: Closing the ""Cosmological Triangle""\n\nIt was a great collaboration between theorists and experimentalists DUNE enthusiasts from around the US. So proud of working in this group :).\n\n<LINK>']",https://arxiv.org/abs/2011.07054,"Axion-like particles (ALPs) provide a promising direction in the search for new physics, while a wide range of models incorporate ALPs. We point out that future neutrino experiments, such as DUNE, possess competitive sensitivity to ALP signals. The high-intensity proton beam impinging on a target can not only produce copious amounts of neutrinos, but also cascade photons that are created from charged particle showers stopping in the target. Therefore, ALPs interacting with photons can be produced (often energetically) with high intensity via the Primakoff effect and then leave their signatures at the near detector through the inverse Primakoff scattering or decays to a photon pair. Moreover, the high-capability near detectors allow for discrimination between ALP signals and potential backgrounds, improving the signal sensitivity further. We demonstrate that a DUNE-like detector can explore a wide range of parameter space in ALP-photon coupling $g_{a\gamma}$ vs ALP mass $m_a$, including some regions unconstrained by existing bounds; the ""cosmological triangle"" will be fully explored and the sensitivity limits would reach up to $m_a\sim3-4$ GeV and down to $g_{a\gamma}\sim 10^{-8} {\rm GeV}^{-1}$. ","Axion-like Particles at Future Neutrino Experiments: Closing the
  ""Cosmological Triangle"""
143,1327277490990092288,281711973,Dr. Emily Rickman,"['Check out our new @SPHERE_outreach paper on ""A triple star in disarray. Multi-epoch observations of T Tauri with VLT-SPHERE and LBT-LUCI""\n\n👇👇👇\n\n<LINK> <LINK>']",https://arxiv.org/abs/2011.06345,"T Tauri remains an enigmatic triple star for which neither the evolutionary state of the stars themselves, nor the geometry of the complex outflow system is completely understood. Eight-meter class telescopes equipped with state-of-the-art adaptive optics provide the spatial resolution necessary to trace tangential motion of features over a timescale of a few years, and they help to associate them with the different outflows. We used J-, H-, and K-band high-contrast coronagraphic imaging with VLT-SPHERE recorded between 2016 and 2018 to map reflection nebulosities and obtain high precision near-infrared (NIR) photometry of the triple star. We also present molecular hydrogen emission maps of the 1-0 S(1) line at 2.122 micron obtained with LBT-LUCI during its commissioning period at the end of 2016. The data reveal a number of new features in the system, some of which are seen in reflected light and some are seen in H2 emission; furthermore, they can all be associated with the main outflows. The tangential motion of the features provides compelling evidence that T Tauri Sb drives the southeast-northwest outflow. T Tauri Sb has recently faded probably because of increased extinction as it passes through the southern circumbinary disk. While T Tauri Sb is approaching periastron, T Tauri Sa instead has brightened and is detected in all our J-band imagery for the first time. ","A triple star in disarray -- Multi-epoch observations of T Tauri with
  VLT-SPHERE and LBT-LUCI"
144,1326847093886115841,38941661,Daniel Stilck França,"['New paper out with the great @ChristophHirche and Cambyse Rouzé: On contraction coefficients, partial orders and approximation of capacities for quantum channels (<LINK>). See 👇 for a summary!', 'as indicated by the title, we talk about three intertwined subjects: contraction coefficients, partial orders for channels and capacities. Roughly speaking, contraction coefficients quantify by how much a channel makes states indistinguishable.', 'Partial orders try to capture the notion that one channel is noisier than the other, while capacities quantify how useful a channel is for a task in the limit of many uses. Let us now discuss what we add to the discussion:', 'As usual, tensorization is a huge issue for all these themes. We define some new orders for channels that tensorize and show new relations amongst existing ones. This figure summarizes all relations: https://t.co/lmkWENU5wZ', 'We then put epsilons around all these orderings. This leads to new capacity bounds. Also, using our new orderings, we get simple, transparent proofs of the bounds based on approximate degradability by Sutter et al.', 'Most of these orderings, like less noisy, are based on a bound on the mutual information of the output of the channels. We then proceed to show that one can equivalently formulate them in terms of a strong data processing inequality. This leads us to contraction coefficients.', 'Here we study new ways to obtain strict contractions, with a focus on the relative entropy. We discuss a hypercontractive approach combined with the Petz recovery map.', 'We also get new results for Weyl-covariant channels and compute the contraction coefficients for important classes of Gaussian channels. Finally, we extend our results to f-divergences and discuss how the contraction coefficients for various divergences are related.', 'Now, some personal comments. I was really happy to finally write a paper with @ChristophHirche! We met at a spring school 7 years ago when we were still bachelor students, so it was time for this to happen!', 'And I am also happy to continue my fruitful collaboration with Cambyse, as after this paper he is now my most frequent collaborator!']",https://arxiv.org/abs/2011.05949,"The data processing inequality is the most basic requirement for any meaningful measure of information. It essentially states that distinguishability measures between states decrease if we apply a quantum channel and is the centerpiece of many results in information theory. Moreover, it justifies the operational interpretation of most entropic quantities. In this work, we revisit the notion of contraction coefficients of quantum channels, which provide sharper and specialized versions of the data processing inequality. A concept closely related to data processing is partial orders on quantum channels. First, we discuss several quantum extensions of the well-known less noisy ordering and relate them to contraction coefficients. We further define approximate versions of the partial orders and show how they can give strengthened and conceptually simple proofs of several results on approximating capacities. Moreover, we investigate the relation to other partial orders in the literature and their properties, particularly with regards to tensorization. We then examine the relation between contraction coefficients with other properties of quantum channels such as hypercontractivity. Next, we extend the framework of contraction coefficients to general f-divergences and prove several structural results. Finally, we consider two important classes of quantum channels, namely Weyl-covariant and bosonic Gaussian channels. For those, we determine new contraction coefficients and relations for various partial orders. ","On contraction coefficients, partial orders and approximation of
  capacities for quantum channels"
145,1326611258674614272,75535964,Sheikh Nasrullah,"['check our new paper on interaction prediction in Recommender systems using dynamic embeddings of users and items. \n""Dynamic Embeddings for Interaction Prediction""\n<LINK>', '@shahid_iisc yes in the web conf (WWW) 2021']",https://arxiv.org/abs/2011.05208,"In recommender systems (RSs), predicting the next item that a user interacts with is critical for user retention. While the last decade has seen an explosion of RSs aimed at identifying relevant items that match user preferences, there is still a range of aspects that could be considered to further improve their performance. For example, often RSs are centered around the user, who is modeled using her recent sequence of activities. Recent studies, however, have shown the effectiveness of modeling the mutual interactions between users and items using separate user and item embeddings. Building on the success of these studies, we propose a novel method called DeePRed that addresses some of their limitations. In particular, we avoid recursive and costly interactions between consecutive short-term embeddings by using long-term (stationary) embeddings as a proxy. This enable us to train DeePRed using simple mini-batches without the overhead of specialized mini-batches proposed in previous studies. Moreover, DeePRed's effectiveness comes from the aforementioned design and a multi-way attention mechanism that inspects user-item compatibility. Experiments show that DeePRed outperforms the best state-of-the-art approach by at least 14% on next item prediction task, while gaining more than an order of magnitude speedup over the best performing baselines. Although this study is mainly concerned with temporal interaction networks, we also show the power and flexibility of DeePRed by adapting it to the case of static interaction networks, substituting the short- and long-term aspects with local and global ones. ",Dynamic Embeddings for Interaction Prediction
146,1326430417700970497,802543221943439360,Andrea Caputo,"['<LINK>\n\nNew paper out guys! We assess the sensitivity of future Higgs Factories, such as FCC-ee, CLIC-380, ILC and\nCEPC, to the seesaw portal with GeV sterile neutrinos and generic new physics at higher scale. <LINK>', 'https://t.co/JBa4y8DLPg\nThanks a lot to all my collaborators for the fun. I had this idea the first time 3 years ago, it was a long process and I am grateful is finally out.']",https://arxiv.org/abs/2011.04725,"We consider an extension of the Standard Model with two right-handed singlet fermions with mass at the electroweak scale that induce neutrino masses, plus a generic new physics sector at a higher scale $\Lambda$. We focus on the effective operators of lowest dimension $d=5$, which induce new production and decay modes for the singlet fermions. We assess the sensitivity of future Higgs Factories, such as FCC-ee, CLIC-380, ILC and CEPC, to the coefficients of these operators for various center of mass energies. We show that future lepton colliders can test the cut-off of the theory up to $\Lambda \simeq 500 - 1000\;$TeV, surpassing the reach of future indirect measurements of the Higgs and $Z$ boson widths. We also comment on the possibility of determining the underlying model flavor structure should a New Physics signal be observed, and on the impact of higher dimensional $d=6$ operators on the experimental signatures. ",The see-saw portal at future Higgs Factories
147,1326186853075218434,829415362647187456,Vanessa Graber,"['Time for a brief summary of our new paper <LINK> Really happy that this is finally out, because sooooo much thinking / calculating time went into this. Read on if you want to know more about the magnetic field in the neutron star interior 🤩 ... 1/n', 'Neutron stars are unique in many ways. Most fascinating one for me is that their densities are so high that nucleons become superfluid. We know of similar behaviour from condensates on Earth, but modelling these phases in NSs is challenging. We cannot really peek inside ... 2/n', 'For this paper, we focus on the superconducting protons and ask: What happens when they are coupled to the superfluid neutrons? To answer that we combine techniques used to analyse laboratory superconductors with those used to model nuclear matter at high densities ... 3/n', 'This allows us to study the properties of the protons as a function of density and magnetic field strength and we construct phase diagrams of the small-scale magnetic flux distribution in the NS interior. Special benefit: we can do this for different equations of state ... 4/n', 'Main results: (i) for all equations of states we looked at, the outer NS core does not contain a pure type-II fluxtube lattice as is often assumed. Instead protons are in a type-1.5 state, where domains with bundles of fluxtubes alternate with flux-free Meissner regions ... 5/n', '(ii) At higher densities, towards the NS centre, type-1.5 SC changes into an intermediate type-I state, where normal conducting regions are mixed with flux-free Meissner regions. The location of this transition is controlled by the neutron-proton coupling strength ... 6/n', 'These mixed states do not come as a complete surprise, because some terrestrial superconductors show similar domain-like behaviour. Nonetheless, their existence is typically ignored in NS models and our results suggest that we should revisit this assumption. So stay tuned 😉n/n', '@IanHawke It depends a bit on what you mean by layered. The schematic flux distribution on small scales that comes out of this looks something like this (not to scale though) - orange is the type-1.5 outer core, blue the type-I inner core. https://t.co/qlxA3KyaF4', ""@IanHawke We can calculate the separation between two individual two orange fluxtubes but how many per 'bundle' or how large the separation between bundles is much harder to determine. The size of the domains in the mixed states depends on the 'history' of the system, e.g. temperature etc."", ""@IanHawke Same is true for the sizes of the blue/white domains in the inner core. So on some macroscopic scale the average flux distribution will pretty much look 'standard' as you said. But we cannot find that scale from our micro-scale model alone. More work needed for that 😁"", ""@IanHawke What are a few orders of magnitude ... 😁? I would naively assume that microscopic composition or transport properties  don't change, but need to think a little more about this. I'll join one of your group meetings in a couple of weeks, so maybe we can discuss that then.""]",https://arxiv.org/abs/2011.02873,"We identify the possible ground states for a mixture of two superfluid condensates (one neutral, the other electrically charged) using a phenomenological Ginzburg-Landau model. While this framework is applicable to any interacting condensed-matter mixture of a charged and a neutral component, we focus on nuclear matter in neutron star cores, where proton and neutron condensates are coupled via non-dissipative entrainment. We employ the Skyrme interaction to determine the neutron star's equilibrium composition, and hence obtain realistic coefficients for our Ginzburg-Landau model at each depth within the star's core. We then use the Ginzburg-Landau model to determine the ground state in the presence of a magnetic field. In this way, we obtain superconducting phase diagrams for six representative Skyrme models, revealing the microphysical magnetic flux distribution throughout the neutron star core. The phase diagrams are rather complex and the locations of most of the phase transitions can only be determined through numerical calculations. Nonetheless, we find that for all equations of state considered in this work, much of the outer core exhibits type-1.5 superconductivity, rather than type-II superconductivity as is generally assumed. For local magnetic field strengths $\lesssim 10^{14} \, {\rm G}$, the magnetic flux is distributed inhomogeneously, with bundles of magnetic fluxtubes separated by flux-free Meissner regions. We provide an approximate criterion to determine the transition between this type-1.5 phase and the type-I region in the inner core. ","Superconducting phases in a two-component microscale model of neutron
  star cores"
148,1326148578096656385,1096458315507421186,Gert-Jan,"[""Happy to present my new paper with @RemyKusters :\n\n'Sparsely constrained neural networks for model discovery of PDEs' - <LINK>\n\nWe show how to combine neural-network based model discovery with classical methods!\n\nA not-so-little thread :-) <LINK>"", 'A little background: to discover models directly from data, we usually use sparse regression: select a whole bunch of features you think might be in your model, than fit your data to these features with the condition you want to use as few as possible...', 'This was done most famously (AFAIK) by @eigensteve with SINDY and PDE-find. Discovering PDEs this way is really hard, because your features are derivatives! That means your error gets correlated, you get issues with finite differences and all this gets worse when you add noise...', 'In our first paper (https://t.co/XScwossAsq) we showed that using a NN constrained to your preselected features to recreate a noiseless version of your data massively boosted performance, because your features are much cleaner. However, many questions remained:', ""1) We used simple L1 regularization, but can we use better versions, such as PDE-find?\n2) How should we pick the right value of regularization?\n3) When should we do feature selection?\n \nIn this new paper we're able to resolve many of these issues :-)"", 'Our main idea is that we can **decouple the constraint on the network from the sparsity estimation**. The sparsity estimation simply tells the constraint with which terms it should constrain, and the constraint can do whatever it wants within that subset.', ""The sparsity estimation doesn't need to happen differentiably and can be any sparsity promoting algorithm you can think of! This gives us a nicely modular approach, where you can mix-and-match whatever you want: https://t.co/S1FEpOj8l6"", 'Our approach bridges classical and deep learning based model discovery and is super flexible. Go wild! Want to use a RNN as function approximator with SR3 as sparsity estimator? You can. Prefer SIRENs? why not. https://t.co/CUJq3e2X79', 'Thinking in terms of modularity makes stuff much easier. We came up with a constraint which calculates the coefficients of the constraint using least-squares and see much faster convergence compared to using gradient descent: https://t.co/KoLhtYHcfM', ""We make this super easy because we're releasing all this as a modular framework: https://t.co/repG6AYgRq\n\nWe've been working hard with our new engineer @GVermarien on properly implementing and documenting (https://t.co/cFS5YvRr31) this..."", 'We follow the pytorch and scikit-learn API, so you can use any pytorch model as function approximator and any sk-learn estimator as sparsity estimator. Try it out and let us know what you think!', ""On a personal level I'm very happy with the paper and the code. I started my PhD two years ago completely new to the field and this paper shows a lot of small things we've learned over the last two years. We also have some other cool stuff lined up, so stay tuned :)"", ""Also we're super happy to collaborate or help you out, so shoot me a message if you're interested.\n\n#automateyourphysicistaway""]",https://arxiv.org/abs/2011.04336,"Sparse regression on a library of candidate features has developed as the prime method to discover the partial differential equation underlying a spatio-temporal data-set. These features consist of higher order derivatives, limiting model discovery to densely sampled data-sets with low noise. Neural network-based approaches circumvent this limit by constructing a surrogate model of the data, but have to date ignored advances in sparse regression algorithms. In this paper we present a modular framework that dynamically determines the sparsity pattern of a deep-learning based surrogate using any sparse regression technique. Using our new approach, we introduce a new constraint on the neural network and show how a different network architecture and sparsity estimator improve model discovery accuracy and convergence on several benchmark examples. Our framework is available at \url{this https URL} ",Sparsely constrained neural networks for model discovery of PDEs
149,1325497990333804547,2940404451,Peter Xenopoulos,"['If you’re interested in esports analytics, check out our new paper, Valuing Player Actions in Counter-Strike: Global Offensive (<LINK>), where we extract event and trajectory data from CSGO matches to predict win probability and value players.', 'Here’s a graph of what win probability can look like over the course of a CSGO round. As one might expect, kill and damage events can drastically change the WP. Our model considers factors such as players remaining and their HP, armor, and equipment. https://t.co/Hsj7xHdvLo', 'We then created a metric where each player’s action was valued by how much it changed their team’s chance of winning. If you’re interested in CSGO data, I recommend you check out our demofile parser, which can return detailed event and trajectory data https://t.co/TyPAVPtnDK']",https://arxiv.org/abs/2011.01324,"Esports, despite its expanding interest, lacks fundamental sports analytics resources such as accessible data or proven and reproducible analytical frameworks. Even Counter-Strike: Global Offensive (CSGO), the second most popular esport, suffers from these problems. Thus, quantitative evaluation of CSGO players, a task important to teams, media, bettors and fans, is difficult. To address this, we introduce (1) a data model for CSGO with an open-source implementation; (2) a graph distance measure for defining distances in CSGO; and (3) a context-aware framework to value players' actions based on changes in their team's chances of winning. Using over 70 million in-game CSGO events, we demonstrate our framework's consistency and independence compared to existing valuation frameworks. We also provide use cases demonstrating high-impact play identification and uncertainty estimation. ",Valuing Player Actions in Counter-Strike: Global Offensive
150,1324393907514273792,1138872700376571905,Roozbeh Mottaghi,"['Rearrangement: a new challenge for the Embodied AI community. This report is the result of 13 two-hour discussions over five months.\n\nPaper: <LINK> <LINK>', 'w/ @DhruvBatraDB, Angel Chang, Sonia Chernova, @AjdDavison, @jiadeng, Vladlen Koltun, @svlevine, Jitendtra Malik, @IMordatch, Manolis Savva, and Hao Su']",https://arxiv.org/abs/2011.01975,"We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a specified state. The goal state can be specified by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems. ",Rearrangement: A Challenge for Embodied AI
151,1324354758463918096,76009287,Desh Raj,"['🎉 New paper on ArXiv 🎉\n\nDOVER-Lap: A method for combining overlap-aware diarization outputs\n\nPaper: <LINK>\nCode: <LINK>\n\nThread about the work 👇\n\n1/n', 'Ensembles usually improve over single-best systems. But ensembling diarization (""who spoke when"") systems is hard  for 2 reasons:\n\n1. Output labels are not aligned across systems\n2. Overlap-aware systems may predict multiple labels in a region\n\n2/n', 'We propose a method to achieve this combination in two stages: (i) mapping all system outputs to a common label space, and (ii) rank-weighted voting which considers overlapping speakers.\n\n3/n', 'We formulate (i) as a weighted k-partite graph matching problem, and propose a greedy maximal matching algorithm based on a ""global cost tensor"" containing all pairwise distances between predicted labels.\n\n4/n https://t.co/A48IjoIELM', 'For (ii), we vote among the systems and predict the top N labels, where N is the weighted mean of number of labels rounded to the nearest integer. In simple words, if systems A, B, and C predict 2, 1, and 2 speakers in a region, the combined output contains 2 speakers.\n\n5/n https://t.co/d2acDXCGMT', 'We combined outputs from several new diarization methods (RPN, TS-VAD, etc.) -&gt; consistent improvements across different overlap conditions (see results on LibriCSS in the table):\n\n6/n https://t.co/HuFc7174Fs', 'More experiments in paper: \n- Combining systems on AMI meeting data\n- Using DOVER-Lap for multi-channel fusion (TL;DR -&gt; does better than WPE + beamforming)\n\n7/n', 'No training or GPUs needed! I did all the experiments on my MacBook Pro CPU and they ran in a few seconds.\n\nConsider using DOVER-Lap when you are submitting your diarization systems to future editions of DIHARD or VoxSRC 😀\n\nn/n']",https://arxiv.org/abs/2011.01997,"Several advances have been made recently towards handling overlapping speech for speaker diarization. Since speech and natural language tasks often benefit from ensemble techniques, we propose an algorithm for combining outputs from such diarization systems through majority voting. Our method, DOVER-Lap, is inspired from the recently proposed DOVER algorithm, but is designed to handle overlapping segments in diarization outputs. We also modify the pair-wise incremental label mapping strategy used in DOVER, and propose an approximation algorithm based on weighted k-partite graph matching, which performs this mapping using a global cost tensor. We demonstrate the strength of our method by combining outputs from diverse systems -- clustering-based, region proposal networks, and target-speaker voice activity detection -- on AMI and LibriCSS datasets, where it consistently outperforms the single best system. Additionally, we show that DOVER-Lap can be used for late fusion in multichannel diarization, and compares favorably with early fusion methods like beamforming. ",DOVER-Lap: A Method for Combining Overlap-aware Diarization Outputs
152,1324220255191797761,710917213994102784,Krishna D N,"['Very excited to share our new paper ""Multi-modal transformer for utterance-level code-switching detection""\n\npaper: <LINK>\ncode: <LINK>']",https://arxiv.org/abs/2011.02132,"An utterance that contains speech from multiple languages is known as a code-switched sentence. In this work, we propose a novel technique to predict whether given audio is mono-lingual or code-switched. We propose a multi-modal learning approach by utilising the phoneme information along with audio features for code-switch detection. Our model consists of a Phoneme Network that processes phoneme sequence and Audio Network(AN), which processes the mfcc features. We fuse representation learned from both the Networks to predict if the utterance is code-switched or not. The Audio Network and Phonetic Network consist of initial convolution, Bi-LSTM, and transformer encoder layers. The transformer encoder layer helps in selecting important and relevant features for better classification by using self-attention. We show that utilising the phoneme sequence of the utterance along with the mfcc features improves the performance of code-switch detection significantly. We train and evaluate our model on Microsoft code-switching challenge datasets for Telugu, Tamil, and Gujarati languages. Our experiments show that the multi-modal learning approach significantly improved accuracy over the uni-modal approaches for Telugu-English, Gujarati-English, and Tamil-English datasets. We also study the system performance using different neural layers and show that the transformers help obtain better performance. ",Multi-Modal Transformers Utterance-Level Code-Switching Detection
153,1323556685827493889,597775255,Dr Adam Finley,"['New paper on arxiv: <LINK> We measure the angular momentum carried by the alpha particles in the solar wind in the inner Heliosphere with Parker Solar Probe, in order to determine their significance. @AWESoMeStarsERC @NASASun @UoE_Astro @UKSolarPhysics (1/4) <LINK>', 'We use data from the SPAN-Ion and FIELDS instruments on-board Parker Solar Probe, during its third and fourth perihelion passes. We find the alpha particles carry around a fifth of the mechanical AM flux in the solar wind. (2/4)', 'We also analyse the proton core and beam populations separately. We find the proton beam can contain a similar contribution to the mechanical AM flux as the alpha particles (the beam component has previously been neglected from studies of AM-loss).  (3/4)', 'The fast wind appears to be carrying a negative AM flux (i.e. spinning the Sun up), unlike the slow wind. This fast/slow wind dichotomy has also been observed at 1au, so maybe this is common in the equatorial solar wind? More data from PSP and SolO will shed light on this. (4/4)']",https://arxiv.org/abs/2011.00016,"An accurate assessment of the Sun's angular momentum (AM) loss rate is an independent constraint for models that describe the rotation evolution of Sun-like stars. In-situ measurements of the solar wind taken by Parker Solar Probe (PSP), at radial distances of $\sim 28-55R_{\odot}$, are used to constrain the solar wind AM-loss rate. For the first time with PSP, this includes a measurement of the alpha particle contribution. The mechanical AM flux in the solar wind protons (core and beam), and alpha particles, is determined as well as the transport of AM through stresses in the interplanetary magnetic field. The solar wind AM flux is averaged over three hour increments, so that our findings more accurately represent the bulk flow. During the third and fourth perihelion passes of PSP, the alpha particles contain around a fifth of the mechanical AM flux in the solar wind (the rest is carried by the protons). The proton beam is found to contain $\sim 10-50\%$ of the proton AM flux. The sign of the alpha particle AM flux is observed to correlate with the proton core. The slow wind has a positive AM flux (removing AM from the Sun as expected), and the fast wind has a negative AM flux. As with previous works, the differential velocity between the alpha particles and the proton core tends to be aligned with the interplanetary magnetic field. In future, by utilising the trends in the alpha-proton differential velocity, it may be possible to estimate the alpha particle contribution when only measurements of the proton core are available. Based on the observations from this work, the alpha particles contribute an additional $10-20\%$ to estimates of the solar wind AM-loss rate which consider only the proton and magnetic field contributions. Additionally, the AM flux of the proton beam can be just as significant as the alpha particles, and so should not be neglected in future studies. ","The Contribution of Alpha Particles to the Solar Wind Angular Momentum
  Flux in the Inner Heliosphere"
154,1335722062502113280,19269506,Dr. Matias Valdenegro Toro🐧,"['I am happy to share our paper ""Unsupervised Difficulty Estimation with Action Scores"" with @Octavio_Arriaga , we propose a simple method to find biases in models and datasets. <LINK> <LINK>', 'We propose action scores, which is simply to sum the loss during training or validation at each epoch for each sample separately. This can be kept as a metric, and we find that the action score gives information about sample difficulty. https://t.co/nG8U04e8Xj', 'On Image Classification with CIFAR10, we see that the visually easiest examples have the lowest scores, while visually difficult samples have the highest action scores. We even find some mislabelled images such a frog labelled as a cat. A dog has a very similar pose to a horse. https://t.co/xKCR7nRLjN', 'On Object Detection with PASCAL VOC on SSD, we find similar issues, very small objects, occluded objects, strange points of view, etc. Since we can apply our method to any loss, we also look at the loss components (localization loss, positive loss, etc). https://t.co/3k2V6Zxaod', 'On the localization loss of SSD, we see a different kind of difficulty. Objects in canonical poses are easy to localize, while difficulty increases for small or hard to see objects. https://t.co/oqQf4rwOQU', 'The advantage of our method is that it does not require any changes to the model or dataset. We just use loss information, so it can be used in any task ,image segmentation, NLP, object detection, etc. We hope this method can be a standard part of model development pipelines.', 'Our paper ( https://t.co/54VYVYMUvg ) will be presented tomorrow at the #LatinXInAI research workshop at #NeurIPS2020 as an oral presentation, and we will have our poster at the joint affinity group poster session. Please come if you have questions!']",https://arxiv.org/abs/2011.11461,"Evaluating difficulty and biases in machine learning models has become of extreme importance as current models are now being applied in real-world situations. In this paper we present a simple method for calculating a difficulty score based on the accumulation of losses for each sample during training. We call this the action score. Our proposed method does not require any modification of the model neither any external supervision, as it can be implemented as callback that gathers information from the training process. We test and analyze our approach in two different settings: image classification, and object detection, and we show that in both settings the action score can provide insights about model and dataset biases. ",Unsupervised Difficulty Estimation with Action Scores
155,1334205029226582019,20913824,"Johannes Gasteiger, né Klicpera","['In our latest work at the #ML4molecules workshop at #NeurIPS2020, we propose\n\n- DimeNet++, roughly as fast as SchNet, but 4x more accurate\n- the COLL dataset, which goes far beyond equilibrium mols (such as QM9)\n- early steps in uncertainty for GNNs in MD\n\n<LINK> <LINK>', ""There's never enough room for also attributing the amazing team in a single tweet...\n\nWork with @G2Giri @jtmargraf @guennemann!""]",https://arxiv.org/abs/2011.14115,"Many important tasks in chemistry revolve around molecules during reactions. This requires predictions far from the equilibrium, while most recent work in machine learning for molecules has been focused on equilibrium or near-equilibrium states. In this paper we aim to extend this scope in three ways. First, we propose the DimeNet++ model, which is 8x faster and 10% more accurate than the original DimeNet on the QM9 benchmark of equilibrium molecules. Second, we validate DimeNet++ on highly reactive molecules by developing the challenging COLL dataset, which contains distorted configurations of small molecules during collisions. Finally, we investigate ensembling and mean-variance estimation for uncertainty quantification with the goal of accelerating the exploration of the vast space of non-equilibrium structures. Our DimeNet++ implementation as well as the COLL dataset are available online. ","Fast and Uncertainty-Aware Directional Message Passing for
  Non-Equilibrium Molecules"
156,1333736004759523335,426509606,Yamir Moreno,"['New preprint out today ""Induced Percolation on Networked Systems"" (<LINK>). We propose a new percolation framework that accounts for indirect influence, finding a rich variety of 1st-order, 2nd-order, and hybrid phase transitions. w/ @xiangrongwang et al. <LINK>']",https://arxiv.org/abs/2011.14034,"Percolation theory has been widely used to study phase transitions in complex networked systems. It has also successfully explained several macroscopic phenomena across different fields. Yet, the existent theoretical framework for percolation places the focus on the direct interactions among the system's components, while recent empirical observations have shown that indirect interactions are common in many systems like ecological and social networks, among others. Here, we propose a new percolation framework that accounts for indirect interactions, which allows to generalize the current theoretical body and understand the role of the underlying indirect influence of the components of a networked system on its macroscopic behavior. We report a rich phenomenology in which first-order, second-order or hybrid phase transitions are possible depending on whether the links of the substrate network are directed, undirected or a mix, respectively. We also present an analytical framework to characterize the proposed induced percolation, paving the way to further understand network dynamics with indirect interactions. ",Induced Percolation on Networked Systems
157,1333610638501789697,16079444,Ying-Jer Kao,['Hybrid quantum-classical classifier based on tensor network and variational quantum circuit\n\nWe propose a TN+VQC architecture that allows for an end-to-end training \n\n<LINK>'],https://arxiv.org/abs/2011.14651,"One key step in performing quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices is the dimension reduction of the input data prior to their encoding. Traditional principle component analysis (PCA) and neural networks have been used to perform this task; however, the classical and quantum layers are usually trained separately. A framework that allows for a better integration of the two key components is thus highly desirable. Here we introduce a hybrid model combining the quantum-inspired tensor networks (TN) and the variational quantum circuits (VQC) to perform supervised learning tasks, which allows for an end-to-end training. We show that a matrix product state based TN with low bond dimensions performs better than PCA as a feature extractor to compress data for the input of VQCs in the binary classification of MNIST dataset. The architecture is highly adaptable and can easily incorporate extra quantum resource when available. ","Hybrid quantum-classical classifier based on tensor network and
  variational quantum circuit"
158,1333329063512043521,2603024598,Ricardo Pérez-Marco,"['""Notes on the historical bibliography of the Gamma function""\n\nMany references in the classical literature are erroneous, and numerous results misattributed. We can find some in classical texts from Gauss, Weierstrass, etc\n\nWe share these personal notes.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2011.12140,"Telegraphic notes on the historical bibliography of the Gamma function and Eulerian integrals. Correction to some classical references. Some topics of the interest of the author. We provide some extensive (but not exhaustive) bibliography. Feedback is welcome, notes will be updated and some references need completion. ",Notes on the historical bibliography of the gamma function
159,1332290582778142720,1212040530592333826,Sunny Vagnozzi,"['A few days ago, with Avi Loeb from Harvard (the ""other"" Cambridge!) and Michele Moresco from Bologna, we had another paper on spatial curvature and whether the Universe is flat or not, which you can find on arXiv: <LINK>. Here comes an explanatory thread 1/n <LINK>', 'Where were we? This is somewhat of a follow-up on my earlier https://t.co/NpBKF7vRtI - for a recap and more backstory on why we care about spatial curvature and what the ongoing trouble is, see this thread https://t.co/6DLK3vJ1oW, especially between tweets 2/n and 19/n. 2/n', ""A three-tweets super-condensed backstory is: It's hard to believe the Planck temperature+polarization (TP) result alone that the Universe is spatially closed (Ωk&lt;0). But to get reliable constraints on Ωk we have to break the geometrical degeneracy (GD)... 3/n"", 'How does one break the GD? With external (ext) datasets probing the late-time expansion, i.e. one or more among Ωm, Ωk, and H0. Examples are CMB lensing, BAO, full-shape galaxy power spectrum, local H0 measurements, uncalibrated SNe, ++ (weak lensing, cluster counts...) 4/n', '🐘 in the room: combining Planck TP+ext comes at the cost of strong tensions between Planck and ext when assuming a ΛCDM+Ωk Universe (e.g. https://t.co/4aBsMq30Ka). #1 rule of data analysis: never trust results coming from combinations of data in tension. End of backstory 5/n https://t.co/h6yHseDZJB', ""So for me the starting point is this stalemate: as many cosmologists I have biases and want to believe Ωk~0. But to do this I need to break the GD and combine Planck TP with something else. And I'd like to do this without incurring in tensions. https://t.co/kBjDrYG16f 6/n"", ""Another thing whoever follows me knows I've always been a bit unsatisfied with is that there is some (little yes, but still some) residual model-dependence in BAO measurements (https://t.co/1lKnCjWJr5). So I also want to break the GD in as model-independent a way as possible 7/n"", 'So the question underlying our paper is: can we find a ""golden dataset"" which:\n   •can break the geometrical degeneracy when combined \n    with Planck TP data\n   •can do so without incurring in tensions when assuming \n    Ωk≠0\n   •is as model-independent as possible\n?\n8/n', 'So I turned to an old idea by Raúl Jiménez and Avi Loeb to use relative galaxy ages to constrain the expansion history (https://t.co/57ya7SiVle): turn the time-redshift relation around to measure H(z) from the differential age evolution dt vs dz of some appropriate object 9/n https://t.co/zJ5b28HmVo', 'The hard part is finding this ""cosmic chronometer"", i.e. an unbiased tracer of the evolution of the differential age of the Universe as a function of redshift. Lots of work has gone into this in the past 10 years, especially from groups in Barcelona and Bologna 10/n', 'The ideal cosmic chronometer (CC) turns out to be a population of massive early-type passively evolving galaxies: they formed and assembled their mass at high-z and over a very short period of time, before quickly exhausting their gas reservoir and evolving passively 11/n https://t.co/oIccoCbAri', ""Finding a good CC is hard, even harder is showing that it is reliable and quantifying all the associated systematics. I won't go into the details here, but it is basically what Michele (likely the #1 observational CC expert) has been doing for the past 10 years 12/n"", 'So this is how CC measurements of H(z) look. Direct measurements of H(z) in the range 0&lt;z&lt;2 can very efficiently break the GD. The nice thing is that they make *no. cosmological. model. assumption. whatsoever.* and directly measure H(z) with no calibration required 13/n https://t.co/QZijmWvlkl', 'So what happens when we take Planck+CC (left figure)? #1 we clearly break the geometrical degeneracy #2 constraints on Ωk move towards Ωk~0, H0 and Ωm move to more sensible values (~70 and ~0.3) #3 by eye there is no strong tension with Planck (cf w BAO on the right figure) 14/n https://t.co/FAI561vGzH', 'The actual numbers you find below. Planck+CC tells us that Ωk=-0.0054±0.0055, so consistent with spatial flatness at 1σ. The error bar is only a factor of ~2.5 worse than from Planck+BAO and ~2 worse than Planck+full-shape galaxy power spectrum. So the method is competitive 15/n https://t.co/MbWLzOnxY9', 'The tension between Planck and CC *within a non-flat Universe* by eye is not strong. On a Jeffreys-like scale, we quantified it as being mild (on this scale the Planck-BAO tension would be strong). There is some mild disagreement, the origin you see in the residuals below 16/n https://t.co/4h4Vh93sul', 'This is good! CC satisfy all the characteristics for the ""golden dataset"" we were looking for: we can finally break the geometrical degeneracy in Planck TP data to learn that Ωk~0 at 1σ without incurring in strong tensions, with a virtually model-independent dataset 17/n https://t.co/ojTMMb06N5', 'Then we did a few more tests: what if we free up more parameters such as the sum of the neutrino masses and the dark energy equation of state w? Overall the result is stable, although when we free up w things get a bit funnier, but still Ωk~0 at 1.7σ 18/n https://t.co/hPV4aQLiTR', ""Then we checked the impact of systematics. Michele has worked hard on these for 10 years, and quantified the impact of subdominant young population, SFH, metallicity, SPS, IMF ++ Their effect is shown below, and overall they don't change our conclusions (see Appendix) 19/n https://t.co/S0FOEqt7Tw"", 'So the one-sentence take-away message is: ""Despite hints from Planck alone for Ωk&lt;0 and the difficulty in breaking the geometrical degeneracy without incurring in tensions, we can still find a way to safely conclude that the Universe is flat to Ωk~10^-2: good for inflation!"" 20/n', 'I see this as somewhat of a relief, and I hope it might finally settle what I refer to as ""the trouble with spatial curvature"". Over and out! 21/21', 'I forgot an important clarification as to the title. It is in Italian and follows from Galileo\'s famous ""Eppur si muove!"" (https://t.co/B4xuPHYGHK). It means ""And yet it is flat?"", although in hindsight I realize the question mark is perhaps not the best punctuation choice  22/21', '@CburgesCliff No problems, hope it is accessible enough 🙂', '@threadreaderapp unroll', ""@franco_vazza Yep, but maybe it'll take 4-5 tweets. I'll make it simple. It's basically the fact that for the CMB alone it's hard to disentangle the matter density Ωm, the Hubble constant H0, and the curvature parameter Ωk, and it can't tell between 3+ possibilities (see my slide below) 1/n https://t.co/4oAKsjZFKR"", ""@franco_vazza Imagine you are looking at a friend far away from you, with no references in between (no trees on the road between you etc.). Can you tell whether your friend is close to you but short, or far from you but tall? No you can't! That is the geometrical degeneracy 2/n"", '@franco_vazza To break it you need some ""reference"" in between, e.g. some trees between you, or a car moving between you. You know the typical height of a tree (better if it\'s a standard ruler), or a typical car speed, so you can then gauge by eye the distance between you and your friend 3/n', '@franco_vazza Mathematically speaking, it\'s like you have a system with more unknowns than equations. You need to close it adding equations (the ""references"" between you I mentioned, e.g. BAO, which you can think of as trees of known height) 4/n', ""@franco_vazza @franco_vazza this wasn't super rigorous but it shouldn't be misleading either, hope it helped 🙂 5/5"", ""@franco_vazza Yep but in cosmology we do have BAO which have a known height (actually length), or SNe who have a known luminosity. And cosmic chronometers as discussed in my thread, which you can think of as a car of which you can measure the speed H(z) (you're the cop with the autovelox!)"", '@SeshNadathur Thanks! 🙂 We used the DIC-based metric the 2017 KiDS papers used (Hildebrant+ &amp; Joudaki+) - see below. This indeed requires running CC-only chains, which give Omegak~-0.3±0.2 and H0~70.5±3.5. Most of the mild tension bw Planck and CC comes from H0, as you see from the residuals https://t.co/nXJq7LO7rE', '@NikoSarcevic Haha the magic of the ApJ template 😉', ""@SeshNadathur The keywords is not in *strong* tension. There is mild tension as you see from the residuals plot. I think it's basically impossible to get Omegak~0 from Planck+sth if there isn't at least a little bit of tension between Planck and sth, given how much Planck wants Omegak&lt;0 😉 https://t.co/pJEPBBMziD"", ""@SeshNadathur For me the question has always been how much tension I am willing to tolerate. The level of tension between Planck and CC is by eye (and as we quantified) something I can live with, particularly since it tells me Omegak~0 which is what I want to believe (yes, I'm biased 😆)"", '@NikoSarcevic hello and have a nice weekend! 🙂', '@SeshNadathur Ah another thing @sesh! There is another important difference wrt BAO and full-shape in that both Planck+BAO and Planck+FS, while consistent with Omegak~0, give a central value of Omegak&gt;0, which contributes to the tension. Planck+CC gives a negative central value, which helps', '@SeshNadathur Yes the residuals can be misleading (chi by eye is never a good thing to do), but to my eyesight these two plots show that the Planck vs CC tension is much less than Planck vs BAO. Also as I wrote in my last tweet the central value sign plays some role\nhttps://t.co/lNaG7kfARz', ""@SeshNadathur (of course we didn't use the residuals to work out the tension, but just to understand where that little bit of tension was coming from)"", ""@SeshNadathur The integral for D_A(z*) picks up most of its contributions from H(z) at 0&lt;z&lt;2. It's not very rigorous, but if you want to go from distances at low-z D(z_bao), to D_A(z*), schematically the way I think of this:\nD(z_bao) -&gt; infer H(z) -&gt; D_A(z*)\nhaving H(z) you skip the first step"", ""@SeshNadathur Well the last two points were actually a bit general and not curvature-specific ;-) But I'm curious now, do you have a reference for *strong* constraints on curvature from BAO alone (so uncalibrated, with no BBN prior)? My understanding was you can't get strong constraints?"", '@SeshNadathur Ah OK I think I get this now, it is really measuring BAO over a wide range of redshifts (and different types of BAO as well) which helps you measure OmegaL or equivalently Omegak? You really only need the calibration if you want H0, right?', '@SeshNadathur @sesh Are the eBOSS BAO-only (no BBN calibration) chains for LCDM+Omegak publicly available (or could you send them to me)? I thought they would only be available upon publication?', ""@SeshNadathur @sesh Yeah I'm trying to stay MCMC free for a while 😆😆 should go to sth like anonynous MCMCists. Anyhow if you could send me the chains directly both w and wo BBN that would be great!""]",https://arxiv.org/abs/2011.11645,"The question of whether Cosmic Microwave Background (CMB) temperature and polarization data from Planck favor a spatially closed Universe with curvature parameter $\Omega_K<0$ has been the subject of recent intense discussions. Attempts to break the geometrical degeneracy combining Planck data with external datasets such as Baryon Acoustic Oscillation (BAO) measurements all point towards a spatially flat Universe, at the cost of significant tensions with Planck, which make the resulting dataset combination problematic. Settling this issue requires identifying a dataset which can break the geometrical degeneracy while not incurring in these tensions. We argue that cosmic chronometers (CC), measurements of the expansion rate $H(z)$ from the relative ages of massive early-type passively evolving galaxies, are the dataset we are after. Furthermore, CC come with the additional advantage of being virtually free of cosmological model assumptions. Combining Planck 2018 CMB temperature and polarization data with the latest CC measurements, we break the geometrical degeneracy and find $\Omega_K=-0.0054 \pm 0.0055$, consistent with a spatially flat Universe and competitive with the Planck+BAO constraint. Our results are stable against minimal parameter space extensions and CC systematics, and we find no substantial tension between Planck and CC data within a non-flat Universe, making the resulting combination reliable. Our results allow us to assert with confidence that the Universe is spatially flat to the ${\cal O}(10^{-2})$ level, a finding which might possibly settle the ongoing spatial curvature debate, and lends even more support to the already very successful inflationary paradigm. ","Eppur \`e piatto? The cosmic chronometer take on spatial curvature and
  cosmic concordance"
160,1331922601560731650,1002238325812580353,Seong Hun Lee,"['(1/n) Rotation-Only Bundle Adjustment: We propose a novel method for estimating the camera rotations INDEPENDENTLY OF the translations and the scene structure! Our method provides full immunity to inaccurate translations and structure.\nCheck our paper at: <LINK> <LINK>', '(2/n) Our work is based on the two-view rotation estimation method by Kneip and Lynen: ""Direct Optimization of Frame-to-Frame Rotation"". This is one of my favorite works in 3D vision! We extend their awesome idea to multiple views with general pose.', '(3/n) For optimization, we use the Adam optimizer. This method has been super popular in deep learning, but not so much in multiview geometry problems. We found that it works quite well for our problem.', '(4/n) Our method can be used after rotation averaging to improve the accuracy. Our evaluation demonstrates a consistent and significant improvement when used after the state-of-the-art rotation averaging method.', '(5/5) Also, it can be generalized to both pure and non-pure rotations, planar and non-planar scenes! We believe that our method can be used to provide better initialization for full bundle adjustment in Structure-from-Motion. For more discussions, check our paper! :)', '@jneirap @jcivera Hi! In this work, we assume that the given point matches do not contain outliers. Technically, the residual from each point can be checked after the optimization (although it is not as simple as checking reprojection errors in standard bundle adjustment).', ""@jneirap @jcivera Regarding the singular configurations, we tested a planar scene and it didn't break the method. We haven't tested for collinear points. The proposed method is designed for general pose &amp; point configurations, typically with more than 10 covisible points between pairs of views."", '@jneirap @jcivera In our evaluation on synthetic and real-world datasets, we did not encounter critical problems caused by the point configurations. But it might be problematic in extreme scenarios (e.g., all points being collinear) where the two-view pose estimators would most likely fail.', '@jrpowers @jcivera Thanks!', '@fdellaert @jcivera Thanks! What we mean by that sentence is that the relative rotations are not treated differently based on the number, distribution, noise of the associated image measurements. I will consider rewriting this sentence for clarity. Thanks for pointing it out!', '@5trobl @jcivera Thanks a lot!', '@Rafael_L_Spring @jcivera Thanks for your interest! Regular BA depends on the initial camera positions and the points, whereas ours depends only on the initial rotations. In large-scale problems, initial solutions often contain inaccurate points and camera positions. Our method is unaffected by this.', '@Rafael_L_Spring @jcivera Generally, more accurate rotations will lead to more accrate translations, which in turn will lead to more accurate points. So improving the rotational accuracy is important for the initialization of bundle adjustment, especially in large-scale problems.']",https://arxiv.org/abs/2011.11724,"We propose a novel method for estimating the global rotations of the cameras independently of their positions and the scene structure. When two calibrated cameras observe five or more of the same points, their relative rotation can be recovered independently of the translation. We extend this idea to multiple views, thereby decoupling the rotation estimation from the translation and structure estimation. Our approach provides several benefits such as complete immunity to inaccurate translations and structure, and the accuracy improvement when used with rotation averaging. We perform extensive evaluations on both synthetic and real datasets, demonstrating consistent and significant gains in accuracy when used with the state-of-the-art rotation averaging method. ",Rotation-Only Bundle Adjustment
161,1330935030160314368,101810581,Animesh Garg,"['How do we bridge Sim 2 Real gap? \nWhat is necessary and what is sufficient! 🤖🤯\n\nCheckout: Dynamics Randomization Revisited\nA Case Study for Quadrupedal Locomotion\n\nPaper: <LINK> \nProject: <LINK>\n\nDennis Da, @zhaomingxie @Mvandepanne @BuckBabich <LINK>', 'This is in continuation of our work on quadruped locomotion @NVIDIAAI \n\nLearning a Contact-Adaptive Controller \nhttps://t.co/llHKuEwcUb\n@corl_conf #CoRL2020\n\nhttps://t.co/IWimhuBSoW']",https://arxiv.org/abs/2011.02404,"Understanding the gap between simulation and reality is critical for reinforcement learning with legged robots, which are largely trained in simulation. However, recent work has resulted in sometimes conflicting conclusions with regard to which factors are important for success, including the role of dynamics randomization. In this paper, we aim to provide clarity and understanding on the role of dynamics randomization in learning robust locomotion policies for the Laikago quadruped robot. Surprisingly, in contrast to prior work with the same robot model, we find that direct sim-to-real transfer is possible without dynamics randomization or on-robot adaptation schemes. We conduct extensive ablation studies in a sim-to-sim setting to understand the key issues underlying successful policy transfer, including other design decisions that can impact policy robustness. We further ground our conclusions via sim-to-real experiments with various gaits, speeds, and stepping frequencies. Additional Details: this https URL ",Dynamics Randomization Revisited:A Case Study for Quadrupedal Locomotion
162,1329850091226673154,1128608544431861760,Wei Jin,"['#𝗠𝗮𝗰𝗵𝗶𝗻𝗲𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 Our WSDM\'21 paper ""Node Similarity Preserving Graph Convolutional Networks"" is now available on <LINK>! We propose a new GNN framework to effectively and efficiently preserve node similarity while exploiting graph structure. <LINK>']",https://arxiv.org/abs/2011.09643,"Graph Neural Networks (GNNs) have achieved tremendous success in various real-world applications due to their strong ability in graph representation learning. GNNs explore the graph structure and node features by aggregating and transforming information within node neighborhoods. However, through theoretical and empirical analysis, we reveal that the aggregation process of GNNs tends to destroy node similarity in the original feature space. There are many scenarios where node similarity plays a crucial role. Thus, it has motivated the proposed framework SimP-GCN that can effectively and efficiently preserve node similarity while exploiting graph structure. Specifically, to balance information from graph structure and node features, we propose a feature similarity preserving aggregation which adaptively integrates graph structure and node features. Furthermore, we employ self-supervised learning to explicitly capture the complex feature similarity and dissimilarity relations between nodes. We validate the effectiveness of SimP-GCN on seven benchmark datasets including three assortative and four disassorative graphs. The results demonstrate that SimP-GCN outperforms representative baselines. Further probe shows various advantages of the proposed framework. The implementation of SimP-GCN is available at \url{this https URL}. ",Node Similarity Preserving Graph Convolutional Networks
163,1329841877546192901,342079808,Elisa Mantelli,"[""Hey! Check out Christian Schoof's and mine new paper on ice stream formation! We study the spontaneous birth of ice streams, with new theory and cool simulations.\n\nLink: <LINK> (in review on PRSA)""]",https://arxiv.org/abs/2011.01996,"Ice streams are bands of fast-flowing ice in ice sheets. We investigate their formation as an example of spontaneous pattern formation, based on positive feedbacks between dissipation and basal sliding. Our focus is on temperature-dependent subtemperate sliding, where faster sliding leads to enhanced dissipation and hence warmer temperatures, weakening the bed further, although we also treat a hydromechanical feedback mechanism that operates on fully molten beds. We develop a novel thermomechanical model capturing ice-thickness scale physics in the lateral direction while assuming the the flow is shallow in the main downstream direction. Using that model, we show that formation of a steady-in-time pattern can occur by the amplification in the downstream direction of noisy basal conditions, and often leads to the establishment of a clearly-defined ice stream separated from slowly-flowing, cold-based ice ridges by narrow shear margins, with the ice stream widening in the downstream direction. We are able to show that downward advection of cold ice is the primary stabilizing mechanism, and give an approximate, analytical criterion for pattern formation. ",Ice stream formation
164,1329332536380698626,1125141282861588480,Matthias Walter,"['Just finished the paper with @MarijeSiemann about her MSc thesis at @UTwente with a first polyhedral study on the traveling tournament problem where we lifted model inequalities and found some new ones. Results are easy to parse, but proofs are technical: <LINK>']",https://arxiv.org/abs/2011.09135,"We consider the unconstrained traveling tournament problem, a sports timetabling problem that minimizes traveling of teams. Since its introduction about 20 years ago, most research was devoted to modeling and reformulation approaches. In this paper we carry out a polyhedral study for the cubic integer programming formulation by establishing the dimension of the integer hull as well as of faces induced by model inequalities. Moreover, we introduce a new class of inequalities and show that they are facet-defining. Finally, we evaluate the impact of these inequalities on the linear programming bounds. ","A Polyhedral Study for the Cubic Formulation of the Unconstrained
  Traveling Tournament Problem"
165,1329305378794901504,3275439755,Nicola Branchini,"['1/ :  I have arXived my first preprint, ""Optimized Auxiliary Particle Filters"":  <LINK> . \nThis was done during my MSc at Edi, with Victor Elvira. \nI really enjoyed learning about SMC, a very fundamental topic. In this work, we propose new ways of choosing', '2/ :  the proposal. We consider a mixture proposal: Klaas et al (2005) and also Elvira (2019) argued for a mixture proposal in two different ways:  through a Rao-Blackwellisation of the APF auxiliary variable  (former), or by Multiple Importance Sampling arguments (latter) .', '3/ :  While more expensive than SMC, these methods which marginalize the previous state offer significant benefits in terms of estimators variance. With the MIS perspective it was possible to devise an analytic choice for the mixture weights. We propose a more flexible mixture', '4/ : where the number of kernels is a free parameter, and importantly the weights can be derived as a solution to a convex optimization problem. We prove an unbiased estimate of the marginal likelihood and consistent IS estimators, generalizing the APF estimator (Pitt et al 2012)', '5/ : Our way of optimizing the mixture weights is related to the literature on approximate dynamic programming, cost-to-go PFs , but performed in marginal space. Our method effectively allows for optimizing a proposal with general transition/observation densities, but avoiding', '6/ : black-box methods, non-convex  (e.g. black-box VI) techniques. We show consistently improved performance across several state-space models , comparing to the IAPF (Elvira et al 2018,2019) , BPF and APF.\nI really enjoyed working with Victor .  The first day I met him,', '7/ :  he dedicated &gt;2 hours talking about his work and slowly explaining foundational concepts and ideas to me .  Over the MSc, he always treated me as a peer and let me follow the path I was more interested into , while being very supportive and present.', '@victorelvira']",http://arxiv.org/abs/2011.09317,"Auxiliary particle filters (APFs) are a class of sequential Monte Carlo (SMC) methods for Bayesian inference in state-space models. In their original derivation, APFs operate in an extended state space using an auxiliary variable to improve inference. In this work, we propose optimized auxiliary particle filters, a framework where the traditional APF auxiliary variables are interpreted as weights in an importance sampling mixture proposal. Under this interpretation, we devise a mechanism for proposing the mixture weights that is inspired by recent advances in multiple and adaptive importance sampling. In particular, we propose to select the mixture weights by formulating a convex optimization problem, with the aim of approximating the filtering posterior at each timestep. Further, we propose a weighting scheme that generalizes previous results on the APF (Pitt et al. 2012), proving unbiasedness and consistency of our estimators. Our framework demonstrates significantly improved estimates on a range of metrics compared to state-of-the-art particle filters at similar computational complexity in challenging and widely used dynamical models. ","Optimized Auxiliary Particle Filters: adapting mixture proposals via
  convex optimization"
166,1329252718842687490,722450978990092289,Alphonsus Adu-Bredu,"['How should robots exploit the low uncertainty estimation capabilities of modern perception systems when planning  for domestic tasks? We propose an online planning algorithm to do just that!\n\nA colab with Zhen Z., Neha P. &amp; Chad Jenkins @UMRobotics \nLink: <LINK> <LINK>']",https://arxiv.org/abs/2011.09105,"Recent advances in computational perception have significantly improved the ability of autonomous robots to perform state estimation with low entropy. Such advances motivate a reconsideration of robot decision-making under uncertainty. Current approaches to solving sequential decision-making problems model states as inhabiting the extremes of the perceptual entropy spectrum. As such, these methods are either incapable of overcoming perceptual errors or asymptotically inefficient in solving problems with low perceptual entropy. With low entropy perception in mind, we aim to explore a happier medium that balances computational efficiency with the forms of uncertainty we now observe from modern robot perception. We propose an approach for efficient task planning for goal-directed robot reasoning. Our approach combines belief space representation with the fast, goal-directed features of classical planning to efficiently plan for low entropy goal-directed reasoning tasks. We compare our approach with current classical planning and belief space planning approaches by solving low entropy goal-directed grocery packing tasks in simulation. Our approach outperforms these approaches in planning time, execution time, and task success rate in our simulation experiments. We also demonstrate our approach on a real world grocery packing task with physical robot. ","Elephants Don't Pack Groceries: Robot Task Planning for Low Entropy
  Belief States"
167,1329154484614774785,3127541301,Marco Cerezo,"['Latest work with Summer Student @EnricoFontana19 🔥  <LINK>\n\nWe study symmetries in parameterized quantum circuits, and derive theorems that show how noise affect them. An optimizer is proposed that uses our theoretical results.\n\n@ColesQuantum @AndrewArrasmit2 <LINK>']",http://arxiv.org/abs/2011.08763,"Very little is known about the cost landscape for parametrized Quantum Circuits (PQCs). Nevertheless, PQCs are employed in Quantum Neural Networks and Variational Quantum Algorithms, which may allow for near-term quantum advantage. Such applications require good optimizers to train PQCs. Recent works have focused on quantum-aware optimizers specifically tailored for PQCs. However, ignorance of the cost landscape could hinder progress towards such optimizers. In this work, we analytically prove two results for PQCs: (1) We find an exponentially large symmetry in PQCs, yielding an exponentially large degeneracy of the minima in the cost landscape. (2) We show that noise (specifically non-unital noise) can break these symmetries and lift the degeneracy of minima, making many of them local minima instead of global minima. Based on these results, we introduce an optimization method called Symmetry-based Minima Hopping (SYMH), which exploits the underlying symmetries in PQCs to hop between local minima in the cost landscape. The versatility of SYMH allows it to be combined with local optimizers (e.g., gradient descent) with minimal overhead. Our numerical simulations show that SYMH improves the overall optimizer performance. ","Optimizing parametrized quantum circuits via noise-induced breaking of
  symmetries"
168,1329111048985714691,976521596289671172,Thomas Fai,"['New preprint, and a change of pace from my usual work. We study hard spheres packed on lattices in terms of geometry, i.e. the free volume. We find a ""leaky"" regime in which the geometry changes and the cage of neighbors confining each sphere expands! <LINK>', 'Because of the simplifying lattice geometry, we can calculate the free volumes exactly using known formulas for the volumes of intersection between multiple (e.g. two, three, and four) spheres. We repeat this for different lattices above and below the leaky transition. https://t.co/n2rxI99Stp', 'Related to this work, I also made an outreach lecture on triangulations and tilings of space, a favorite topic of mine that comes up in surprisingly diverse applications such as simulating red blood cells and guarding art galleries. You can find it here: https://t.co/gpGWeLkOE6']",https://arxiv.org/abs/2011.07106,"We study packings of hard spheres on lattices. The partition function, and therefore the pressure, may be written solely in terms of the accessible free volume, i.e. the volume of space that a sphere can explore without touching another sphere. We compute these free volumes using a leaky cell model, in which the accessible space accounts for the possibility that spheres may escape from the local cage of lattice neighbors. We describe how elementary geometry may be used to calculate the free volume exactly for this leaky cell model in two- and three-dimensional lattice packings and compare the results to the well-known Carnahan-Starling and Percus-Yevick liquid models. We provide formulas for the free volumes of various lattices and use the common tangent construction to identify several phase transitions between them in the leaky cell regime, indicating the possibility of coexistence in crystalline materials. ",Leaky Cell Model of Hard Spheres
169,1329019880830611456,925826826651529216,Hatice Gunes,"['Most facial Action Unit detectors rely on static analysis, encoding a snapshot of heightened facial activity. We propose Lifecycle-Aware Capsule Network to selectively focus on spatial / spatio-temporal information depending on the AU lifecycle. Paper: <LINK> <LINK>', 'authors: @NikhilChuramani @kalkansinan']",https://arxiv.org/abs/2011.08819,"Most state-of-the-art approaches for Facial Action Unit (AU) detection rely upon evaluating facial expressions from static frames, encoding a snapshot of heightened facial activity. In real-world interactions, however, facial expressions are usually more subtle and evolve in a temporal manner requiring AU detection models to learn spatial as well as temporal information. In this paper, we focus on both spatial and spatio-temporal features encoding the temporal evolution of facial AU activation. For this purpose, we propose the Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU detection using both frame and sequence-level features. While at the frame-level the capsule layers of AULA-Caps learn spatial feature primitives to determine AU activations, at the sequence-level, it learns temporal dependencies between contiguous frames by focusing on relevant spatio-temporal segments in the sequence. The learnt feature capsules are routed together such that the model learns to selectively focus more on spatial or spatio-temporal information depending upon the AU lifecycle. The proposed model is evaluated on the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art results on both the datasets. ","Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule
  Networks"
170,1328954077540212737,1710697381,Diego F. Torres,"['We release today the second part of the study on the CTA prospects for observing Crab nebula. The focus is variability at short timescales. Find the two papers (published by MNRAS) online, here: <LINK>\n<LINK> <LINK>']",http://arxiv.org/abs/2011.08586,"Since 2009, several rapid and bright flares have been observed at high energies (>100 MeV) from the direction of the Crab Nebula. Several hypotheses have been put forward to explain this phenomenon, but the origin is still unclear. The detection of counterparts at higher energies with the next generation of Cherenkov telescopes will be determinant to constrain the underlying emission mechanisms. We aim at studying the capability of the Cherenkov Telescope Array (CTA) to explore the physics behind the flares, by performing simulations of the Crab Nebula spectral energy distribution, both in flaring and steady state, for different parameters related to the physical conditions in the nebula. In particular, we explore the data recorded by Fermi during two particular flares that occurred in 2011 and 2013. The expected GeV and TeV gamma-ray emission is derived using different radiation models. The resulting emission is convoluted with the CTA response and tested for detection, obtaining an exclusion region for the space of parameters that rule the different flare emission models. Our simulations show different scenarios that may be favourable for achieving the detection of the flares in Crab with CTA, in different regimes of energy. In particular, we find that observations with low sub-100 GeV energy threshold telescopes could provide the most model-constraining results. ","The Crab nebula variability at short timescales with the Cherenkov
  Telescope Array"
171,1328654716906188800,201654004,Sara Seager,"['Venus phosphine update: Newly recalibrated ALMA data is less noisy than the original but we the Greaves et al. team now find a weaker, tentative signal. <LINK>\u200b . Meanwhile independent evidence from Pioneer Venus data finds phosphine: <LINK>']",https://arxiv.org/abs/2011.08176,"We first respond to two points raised by Villanueva et al. We show the JCMT discovery spectrum of PH3 can not be re-attributed to SO2, as the line width is larger than observed for SO2 features, and the required abundance would be an extreme outlier. The JCMT spectrum is also consistent with our simple model, constant PH3-abundance with altitude, with no discrepancy in line profile (within data limits); reconciliation with a full photochemical model is the subject of future work. Section 2 presents initial results from re-processed ALMA data. Villanueva et al. noted an issue with bandpass calibration. They have worked on a partially re-processed subset of the ALMA data, so we note where their conclusions, and those of Greaves et al., are now superseded. To summarise: we recover PH3 in Venus' atmosphere with ALMA (~5{\sigma} confidence). Localised abundance appears to peak at ~5-10 parts-per-billion (ppb), with suggestions of spatial variation. Advanced data-products suggest a planet-averaged PH3 abundance ~1-4 ppb, lower than from the earlier ALMA processing (which indicated 7+ ppb). The ALMA data are reconcilable with the JCMT detection (~20 ppb) if there is order-of-magnitude temporal variation; more advanced processing of the JCMT data is underway to check methods. Independent PH3 measurements suggest possible altitude dependence (under ~5 ppb at 60+ km, up to ~100 ppb at 50+ km; see Section 2: Conclusions.). Given that both ALMA and JCMT were working at the limit of observatory capabilities, new spectra should be obtained. The ALMA data in-hand are no longer limited by calibration, but spectral ripples still exist, probably due to size and brightness of Venus in relation to the primary beam. Further, spatial ripples are present, potentially reducing significance of real narrow spectral features. ",Re-analysis of Phosphine in Venus' Clouds
172,1328303410064596994,39215770,Dimitris Spathis,"['How much information about a person’s behavior and health can you ascertain from a wearable device?\n\nThis is what we study in our new paper at the ML for Mobile Health workshop @ NeurIPS 2020!\n\n✏️ Paper: <LINK>\n📰 Press (@VentureBeat): <LINK> <LINK>', 'We propose a self-supervised neural network in which we set the heart rate responses as the supervisory signal for movement data, leveraging their underlying physiological relationship. Then, thru transfer learning we predict outcomes such as obesity &amp; cardiorespiratory fitness.']",https://arxiv.org/abs/2011.04601,"To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship. We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer & wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring. ","Learning Generalizable Physiological Representations from Large-scale
  Wearable Data"
173,1327078938825224195,1265445645495660545,Max Hopkins,"['Really excited to share a new work joint with Tali Kaufman and Shachar Lovett! <LINK>\n\nWe study the combinatorial and spectral structure of higher order random walks on High Dimensional Expanders (HDX), and explore their connections with unique games. 1/12', ""Let's start with a few basic definitions. \n\nFirst, what's an HDX?\n\nWe focus on a spectral variant of HDX called 'two-sided local spectral expanders.' These objects are hypergraphs (simplicial complexes) where every 'local graph' (1-skeleton of a link) is a spectral expander. 2/12"", ""Second, what's a higher order random walk?\n\nIn a graph, we often consider the walk on vertices defined by moving from vertex, to edge, to vertex.\n\nIn a hypergraph, we can generalize this to higher-dimensional edges, say by walking from triangle to pyramid to triangle. 3/12"", 'Why care about these walks? It turns out that they capture a huge variety of important structures (e.g. sampling matroid bases, independent sets...). \n\nIn this work we focus on the fact that these walks give a broad generalization of the (non-negative) Johnson scheme. 4/12', ""Let's talk results.\n\nWe prove that the spectra of these walks are strongly concentrated in a small number of strips, each (loosely) corresponding to a level in the underlying HDX. Moreover, the eigenvalues decrease exponentially across strips depending on the walk's 'depth.' 5/12"", '(depth measures how far the walk reaches into the HDX)\n\nUsing this machinery, we give a tight characterization of the edge-expansion of sets in higher-order walks via their combinatorial structure. In short, small sets which are locally-pseudorandom expand near-perfectly! 6/12', ""Now the burning question on everyone's mind: what does this have to do with unique games (UG)? \n\nIt turns out this type of structure has strong connections to both hardness and algorithms for unique games. 7/12"", ""First, let's talk algorithms. \n\nUsing Sum-of-Squares methodology developed recently by Bafna, @boazbaraktcs, Khothari, Schramm, and Steurer, we show how our high-dimensional framework implies an efficient algorithm for (affine) UG on the Johnson scheme. 8/12"", 'The runtime of our algorithm depends on the number of spectral strips in the constraint graph with large eigenvalues, a parameter we call High-dimensional Threshold Rank. This generalizes standard threshold rank, long known to be intimately tied to algorithms for UG [BRS11]. 9/12', ""Finally, let's discuss hardness. \n\nA few years ago, Khot, Minzer, and Safra (KMS) completed a breakthrough sequence of works (including many other authors) on the 2-2 Games Conjecture by proving that pseudorandom sets in the Grassmann graph expand near-perfectly. 10/12"", ""While the Grassmann graph isn't a higher order random walk in the sense we've discussed (since we focus on simplicial complexes), it is a higher-order walk on an extension called an 'expanding poset' (eposet) due to Dikstein, Dinur, @YFilmus, and Harsha. 11/12"", 'Our results extend naturally to eposets. While the resulting characterization does not (yet) recover the 2-2 Games Conjecture, we believe the framework may provide a more natural and general way to analyze such structure than the old fourier analytic techniques used by KMS. 12/12', ""Finally I want to give due credit to some previous works I didn't have space to cite. Namely, our spectral/structural results on HDX rely on the machinery of [KO20] and [DDFH18], and bear some similarity to analysis in [AJT19]. If you made it this far, thanks for reading! 13/12"", ""@praveshkkothari Thanks Professor Kothari--reading your paper when it came out a few months ago was definitely eye-opening for me :)\n\nAlso, yikes! I just saw there's a typo in your name in the thread, sorry. That's embarrassing, and explains why I couldn't find your twitter... I'll go fix it."", '*Kothari (In particular, @praveshkkothari). I somehow messed this up even after double checking all the names... 😬', '@YFilmus @praveshkkothari Nice catch, thanks! An advanced spellcheck like that sounds useful--or even just a tool that crosschecks the names in the body of your paper with the references.']",https://arxiv.org/abs/2011.04658,"Higher order random walks (HD-walks) on high dimensional expanders (HDX) have seen an incredible amount of study and application since their introduction by Kaufman and Mass [KM16], yet their broader combinatorial and spectral properties remain poorly understood. We develop a combinatorial characterization of the spectral structure of HD-walks on two-sided local-spectral expanders [DK17], which offer a broad generalization of the well-studied Johnson and Grassmann graphs. Our characterization, which shows that the spectra of HD-walks lie tightly concentrated in a few combinatorially structured strips, leads to novel structural theorems such as a tight $\ell_2$-characterization of edge-expansion, as well as to a new understanding of local-to-global algorithms on HDX. Towards the latter, we introduce a spectral complexity measure called Stripped Threshold Rank, and show how it can replace the (much larger) threshold rank in controlling the performance of algorithms on structured objects. Combined with a sum-of-squares proof of the former $\ell_2$-characterization, we give a concrete application of this framework to algorithms for unique games on HD-walks, in many cases improving the state of the art [RBS11, ABS15] from nearly-exponential to polynomial time (e.g. for sparsifications of Johnson graphs or of slices of the $q$-ary hypercube). Our characterization of expansion also holds an interesting connection to hardness of approximation, where an $\ell_\infty$-variant for the Grassmann graphs was recently used to resolve the 2-2 Games Conjecture [KMS18]. We give a reduction from a related $\ell_\infty$-variant to our $\ell_2$-characterization, but it loses factors in the regime of interest for hardness where the gap between $\ell_2$ and $\ell_\infty$ structure is large. Nevertheless, we open the door for further work on the use of HDX in hardness of approximation and unique games. ","High Dimensional Expanders: Eigenstripping, Pseudorandomness, and Unique
  Games"
174,1327075926174142465,90276706,Jaehoon Lee,"['Can we leverage the power of infinite-width limit to help with Neural Architecture Search (NAS)?\n\nIn this new paper (<LINK>), we find that empirical NNGP can provide cheap and effective signals that can be used for NAS! <LINK>', 'From recent developments in the study of infinite-width networks, we know that inductive priors of large NNs are described by the Neural Network Gaussian Processes (NNGP). \n\nWe ask whether this can be used to predict the performance of neural architectures. https://t.co/E8439FLsoz', 'Since the infinite-width analytic NNGP for common architectures is expensive, we explore empirical monte-carlo estimated NNGP kernels.\n\nside: We find empirical NTK provides similar but slightly worse signal, and is more compute-expensive, so we focus on NNGP.', 'We find advantages from NNGP-based metrics on the NAS-bench 101 dataset (423k networks) on CIFAR-10 and the MNAS search space (10k networks) on ImageNet. These metrics can be used to shrink the search space, or in conjunction with shortened training signals. https://t.co/YgUSxHsT1K', 'The NNGP performance can be computed using a simple algorithm without the need to do any gradient based updates. It is thus much more computationally efficient than signals from shortened training. https://t.co/FI0TPSR4B9', 'We have also open-sourced a colab notebook showing the gist of the algorithm:\nhttps://t.co/Zol7lH2q9N', 'Daniel Park co-led the research, and this work was only possible with excellent collaborators at \nGoogle Brain team: Daiyi Peng, Yuan Cao, @jaschasd\n\nWe thank @yasamanbb, Gabriel Bender, Pieter-Jan Kindermans, @quocleix, Esteban Real, @sschoenholz, @tanmingxing for feedback!', 'image credit (infinite-width limit): @ARomanNovak']",http://arxiv.org/abs/2011.06006,"The predictions of wide Bayesian neural networks are described by a Gaussian process, known as the Neural Network Gaussian Process (NNGP). Analytic forms for NNGP kernels are known for many models, but computing the exact kernel for convolutional architectures is prohibitively expensive. One can obtain effective approximations of these kernels through Monte-Carlo estimation using finite networks at initialization. Monte-Carlo NNGP inference is orders-of-magnitude cheaper in FLOPs compared to gradient descent training when the dataset size is small. Since NNGP inference provides a cheap measure of performance of a network architecture, we investigate its potential as a signal for neural architecture search (NAS). We compute the NNGP performance of approximately 423k networks in the NAS-bench 101 dataset on CIFAR-10 and compare its utility against conventional performance measures obtained by shortened gradient-based training. We carry out a similar analysis on 10k randomly sampled networks in the mobile neural architecture search (MNAS) space for ImageNet. We discover comparative advantages of NNGP-based metrics, and discuss potential applications. In particular, we propose that NNGP performance is an inexpensive signal independent of metrics obtained from training that can either be used for reducing big search spaces, or improving training-based performance measures. ",Towards NNGP-guided Neural Architecture Search
175,1326959189789454340,2883271903,Yuxiang Wu,"['Having scalability issues with your ODQA systems?🆘 Adaptive Computation can help! We find that adaptive computation and global scheduling can reduce computation by 4.3x while retaining 95% of the accuracy on SQuAD-Open!🚀 #EMNLP2020 <LINK> [1/5] <LINK>', 'For ODQA systems, the computational bottleneck often lies in the number of layers that the document reader has to process the passages through. We introduce an adaptive computation and global scheduling strategy for learning to allocate computation across multiple passages! [2/5]', 'The global scheduling strategy is further enhanced with reinforcement learning. Our experiments show that both global scheduling and RL training are essential for improving our adaptive computation method. [3/5] https://t.co/Mf2UDlUHYZ', 'Our further analysis demonstrates that the proposed method can focus its computation on passages that contain the answer, and its scheduling policy learns an exploration-exploitation trade-off. [4/5] https://t.co/hviv4pu6ql', 'With my amazing co-authors @PMinervini, Pontus, @riedelcastro at @ucl_nlp - paper: https://t.co/PlrFrCBdRG, slides https://t.co/Z7jIIa0EKV. Join us at #EMNLP2020, Gather Session 2J (10:00am UTC, Nov 17)!  [5/5]']",https://arxiv.org/abs/2011.05435,"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SkylineBuilder, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model. ","Don't Read Too Much into It: Adaptive Computation for Open-Domain
  Question Answering"
176,1326125319078686721,1667135348,Alexander Terenin,"['Pathwise Conditioning of Gaussian Processes\n\nWe study implications of the pathwise formula (f|y)(.) = f(.) + K_{(.)x} K_{xx}^{-1} (y - f(x)) on Gaussian processes, following our outstanding-paper-honorable-mention-winning ICML paper. Check it out!\n\n<LINK>\n\n@mpd37 <LINK>']",https://arxiv.org/abs/2011.04026,"As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning. ",Pathwise Conditioning of Gaussian Processes
177,1326122952165892101,1662363468,Amélie Saintonge,"['Check out arXiv for a new study by my PhD student Lucy Hogarth with @astroquokka, @timdavisastro et al. about outflows in nearby galaxies! We combine new @almaobs and @SAMI_survey observations of galaxies with large-scale ionised gas outflows <LINK>  (1/2)', 'What’s special about these wind-launching galaxies? They have overall normal amounts of molecular gas and star formation, but all of that is *much more centrally concentrated*. Bars and other dynamical instabilities the likely culprits. (2/2)']",https://arxiv.org/abs/2011.03566,"We perform a joint-analysis of high spatial resolution molecular gas and star-formation rate (SFR) maps in main-sequence star-forming galaxies experiencing galactic-scale outflows of ionised gas. Our aim is to understand the mechanism that determines which galaxies are able to launch these intense winds. We observed CO(1-0) at 1"" resolution with ALMA in 16 edge-on galaxies, which also have 2"" spatial resolution optical integral field observations from the SAMI Galaxy Survey. Half the galaxies in the sample were previously identified as harbouring intense and large-scale outflows of ionised gas (""outflow-types""), the rest serve as control galaxies. The dataset is complemented by integrated CO(1-0) observations from the IRAM 30-m telescope to probe the total molecular gas reservoirs. We find that the galaxies powering outflows do not possess significantly different global gas fractions or star-formation efficiencies when compared with a control sample. However, the ALMA maps reveal that the molecular gas in the outflow-type galaxies is distributed more centrally than in the control galaxies. For our outflow-type objects, molecular gas and star-formation is largely confined within their inner effective radius ($\rm r_{eff}$), whereas in the control sample the distribution is more diffuse, extending far beyond $\rm r_{eff}$. We infer that outflows in normal star-forming galaxies may be caused by dynamical mechanisms that drive molecular gas into their central regions, which can result in locally-enhanced gas surface density and star-formation. ","Centrally concentrated molecular gas driving galactic-scale ionised gas
  outflows in star-forming galaxies"
178,1326104584604020737,422672164,Dr Michael Reidinger,"['Constraints on Decaying Dark Matter with DES-Y1 and external data\n\n""We study a class of decaying dark matter models as a possible resolution to the observed discrepancies between early- and late-time probes of the universe.""\n<LINK>']",https://arxiv.org/abs/2011.04606,"We study a phenomenological class of models where dark matter converts to dark radiation in the low redshift epoch. This class of models, dubbed DMDR, characterizes the evolution of comoving dark matter density with two extra parameters, and may be able to help alleviate the observed discrepancies between early- and late-time probes of the universe. We investigate how the conversion affects key cosmological observables such as the CMB temperature and matter power spectra. Combining 3x2pt data from Year 1 of the Dark Energy Survey, {\it Planck}-2018 CMB temperature and polarization data, supernovae (SN) Type Ia data from Pantheon, and baryon acoustic oscillation (BAO) data from BOSS DR12, MGS and 6dFGS, we place new constraints on the amount of dark matter that has converted to dark radiation and the rate of this conversion. The fraction of the dark matter that has converted since the beginning of the universe in units of the current amount of dark matter, $\zeta$, is constrained at 68\% confidence level to be $<0.32$ for DES-Y1 3x2pt data, $<0.030$ for CMB+SN+BAO data, and $<0.037$ for the combined dataset. The probability that the DES and CMB+SN+BAO datasets are concordant increases from 4\% for the $\Lambda$CDM model to 8\% (less tension) for DMDR. The tension in $S_8 = \sigma_8 \sqrt{\Omega_{\rm m}/0.3}$ between DES-Y1 3x2pt and CMB+SN+BAO is slightly reduced from $2.3\sigma$ to $1.9\sigma$. We find no reduction in the Hubble tension when the combined data is compared to distance-ladder measurements in the DMDR model. The maximum-posterior goodness-of-fit statistics of DMDR and $\Lambda$CDM model are comparable, indicating no preference for the DMDR cosmology over $\Lambda$CDM. ","Constraints on dark matter to dark radiation conversion in the late
  universe with DES-Y1 and external data"
179,1326058826768977921,561167071,Sascha Caron,"['With @fdiblen, @eScienceCenter, @l_hendriks, @bob_stienen we propose (with demos) the scientific diagrams of the future \n\n-&gt; interactive dashboards\n<LINK>\n\n- Publish theory data in full dimensionality\n- Web-based visualization  \n- ML-based generalization of data <LINK>']",https://arxiv.org/abs/2011.03801,"In this manuscript, we propose to expand the use of scientific repositories such as Zenodo and HEP Data, in particular in order to better examine multi-parametric solutions of physical models. The implementation of interactive web-based visualizations enables fast and comfortable re-analysis and comparisons of phenomenological data. In order to illustrate our point of view, we present some examples and demos for dark matter models, supersymmetry exclusions and LHC simulations. ","Interactive web-based visualization of multi-dimensional physical and
  astronomical data"
180,1325748584927662083,1263036867450155008,Johannes Rahlf,['Check our new Preprint 📰 - We show that #forest timber volume predictions from large-scale resource maps 🗺️ based on NFI🌲🌳 and ALS 🛩️ data can directly be used in forest management inventories! \n(case study using 🇳🇴#SR16 and @nfi100 data)\n\n<LINK> <LINK>'],https://arxiv.org/abs/2011.02051,"Large-scale forest resource maps based on national forest inventory (NFI) data and airborne laser scanning may facilitate synergies between NFIs and forest management inventories (FMIs). A comparison of models used in such a NFI-based map and a FMI indicate that NFI-based maps can directly be used in FMIs to estimate timber volume of mature spruce forests. Traditionally, FMIs and NFIs have been separate activities. The increasing availability of detailed NFI-based forest resource maps provides the possibility to eliminate or reduce the need of field sample plot measurements in FMIs if their accuracy is similar. We aim to 1) compare a timber volume model used in a NFI-based map and models used in a FMI, and 2) evaluate utilizing additional local sample plots in the model of the NFI-based map. Accuracies of timber volume estimates using models from an existing NFI-based map and a FMI were compared at plot and stand level. Estimates from the NFI-based map were similar to or more accurate than the FMI. The addition of local plots to the modeling data did not clearly improve the model of the NFI-based map.The comparison indicates that NFI-based maps can directly be used in FMIs for timber volume estimation in mature spruce stands, leading to potentially large cost savings. ","Timber Volume Estimation Based on Airborne Laser Scanning -- Comparing
  the Use of National Forest Inventory and Forest Management Inventory Data"
181,1325717080977645568,1085279811692621825,Luca Matrà,"['*It’s a trap!* In a paper led by @peytonbenac today, we used the Submillimeter Array to find another example of a (resolved) high contrast mm asymmetry in the disk around a Herbig Ae binary, HD34700AaAb - with spiral arms in scattered light observations:\n<LINK> <LINK>', 'A big shout out to my student @peytonbenac for this first paper of hers! Keep an eye out for her in the future, especially as she is applying to grad school this year! 💫']",https://arxiv.org/abs/2011.03489,"Millimeter observations of disks around young stars reveal substructures indicative of gas pressure traps that may aid grain growth and planet formation. We present Submillimeter Array observations of HD 34700- two Herbig Ae stars in a close binary system (Aa/Ab, $\sim$0.25 AU), surrounded by a disk presenting a large cavity and spiral arms seen in scattered light, and two distant, lower mass companions. These observations include 1.3 mm continuum emission and the $^{12}$CO 2-1 line at $\sim0.5$"" (178 AU) resolution. They resolve a prominent azimuthal asymmetry in the continuum, and Keplerian rotation of a circumbinary disk in the $^{12}$CO line. The asymmetry is located at a radius of $155^{+11}_{-7}$ AU, consistent with the edge of the scattered light cavity, being resolved in both radius ($72 ^{+14}_{-15}$ AU) and azimuth (FWHM = $64 ^{\circ +8}_{-7}$). The strong asymmetry in millimeter continuum emission could be evidence for a dust trap, together with the more symmetric morphology of $^{12}$CO emission and small grains. We hypothesize an unseen circumbinary companion, responsible for the cavity in scattered light and creating a vortex at the cavity edge that manifests in dust trapping. The disk mass has limitations imposed by the detection of $^{12}$CO and non-detection of $^{13}$CO. We discuss its consequences for the potential past gravitational instability of this system, likely accounting for the rapid formation of a circumbinary companion. We also report the discovery of resolved continuum emission associated with HD 34700B (projected separation $\sim1850$AU), which we explain through a circumstellar disk. ",A Dust Trap in the Young Multiple System HD 34700
182,1325628451932364801,721084980697239552,Adam R. H. Stevens,"[""It's paper day! I'm pleased to share with you my latest piece of research:\n<LINK>\n\nHere we study how the molecular gas in galaxies is affected by their environment. What's so interesting about molecular gas, you ask? Well, read on, friend. \n\n1/15"", 'Gas is the life blood of a galaxy. The continual acquisition of gas is how galaxies grow and form stars. Without gas, galaxies stagnate. They stop evolving. Understanding the processes that can cause this is one of the biggest ongoing research areas in astrophysics. \n\n2/15', 'When smaller galaxies fall towards larger galaxies, they experience ram pressure – the same force you feel on your face when you stick your head out of the moving-car window. This strips the smaller galaxy of its gas, shutting down its evolution. \n\n3/15', 'We’ve known for decades from radio observations that “satellite” galaxies (small galaxies orbiting and/or on a collision course with a big galaxy) have relatively less atomic gas than “central” galaxies (the ones that satellites orbit). Ram pressure drives this difference. \n\n4/15', 'We also know from optical, UV, and infrared observations that satellite galaxies have relatively less star formation taking place in them than centrals. So, less atomic gas and less star formation for the galaxies in “dense environments” – makes sense! \n\n5/15', 'But here’s the catch: atomic gas isn’t what forms stars. At least, not directly. Rather, stars form in giant clouds of molecular gas. To close the loop on how environment shuts down the evolution of galaxies, we need to measure galaxies’ molecular-gas content. \n\n6/15', 'The problem is that observing emission from molecular gas is much harder than atomic gas, and there’s usually less of it. This can make it hard to test predictions from models and simulations. That is, until now! Enter our research… \n\n7/15', 'Using a state-of-the-art cosmological simulation (TNG100), we predicted what the molecular-gas content of galaxies across the universe should be, and how this changes with their environment. But wait, there’s more! \n\n8/15', 'What makes this research so niche is that we catered our predictions to the specifications of a molecular-gas galaxy survey (xGASS-CO), forward-modelling what the instrument is sensitive to. This figure from the paper shows how much gas would go undetected. \n\n9/15 https://t.co/Zu1fIj0zKA', 'Here’s the best bit: the data for the survey already exist. This means we could immediately test our predictions by comparing to those data. Sure enough, the “mock observations” we conducted with the simulation were crucial to our results. \n\n10/15', 'As shown here, we predict the ratio of molecular-gas mass to stellar mass (y-axis) as a function of stellar mass (x-axis) for satellite and central galaxies from the simulation. With our mock-observation procedure (top panels), the agreement with the real data is excellent. 11/15 https://t.co/nlwWlDTNpk', 'And so arrives the main result. We both predict from the simulation and confirm with an original analysis of the observations for the first time that satellite galaxies have ~40% less molecular gas than central galaxies, when stellar mass is controlled for. \n\n12/15 https://t.co/UAyr6u85hZ', 'Actually, 40% refers to what we correctly predicted the survey would detect. Observational uncertainties make this difference smaller than what it is in reality. We expect the true difference is more like 75%. But new observations are needed to test that. \n\n13/15', 'We then predicted (or “postdicted”) how galaxies’ star formation rates decline in denser environments, again forward-modelling observational uncertainties. With orders of magnitude more data to test against (from SDSS), our theoretical results become even more favourable.\n\n14/15 https://t.co/ENDQh2xWUI', 'Thus closes the loop on how the environment of a galaxy can halt its evolution. Problem identified, problem solved. Yay, science! These results are robust for the nearby Universe. Further studies are needed to test if this holds as simply in the early Universe.\n\n15/15', '@astro_mir By ""dense environment"" I mean on halo scales, not scales within galaxies.  ""Dense"" in this context means ""in a cluster"", i.e. more likely to experience processes like ram-pressure stripping.', '@astro_mir In the Figure 11 plot, the squiggly M is log10(host halo mass [solar masses]).']",https://arxiv.org/abs/2011.03226,"We examine how the post-processed content of molecular hydrogen (H$_2$) in galaxies from the TNG100 cosmological, hydrodynamic simulation changes with environment at $z\!=\!0$, assessing central/satellite status and host halo mass. We make close comparisons with the carbon monoxide (CO) emission survey xCOLD GASS where possible, having mock-observed TNG100 galaxies to match the survey's specifications. For a representative sample of host haloes across $10^{11}\!\lesssim\!M_{\rm 200c}/{\rm M}_{\odot}\!<\!10^{14.6}$, TNG100 predicts that satellites with $m_*\!\geq\!10^9\,{\rm M}_{\odot}$ should have a median deficit in their H$_2$ fractions of $\sim$0.6 dex relative to centrals of the same stellar mass. Once observational and group-finding uncertainties are accounted for, the signature of this deficit decreases to $\sim$0.2 dex. Remarkably, we calculate a deficit in xCOLD GASS satellites' H$_2$ content relative to centrals of 0.2--0.3 dex, in line with our prediction. We further show that TNG100 and SDSS data exhibit continuous declines in the average star formation rates of galaxies at fixed stellar mass in denser environments, in quantitative agreement with each other. By tracking satellites from their moment of infall in TNG100, we directly show that atomic hydrogen (HI) is depleted at fractionally higher rates than H$_2$ on average. Supporting this picture, we find that the H$_2$/HI mass ratios of satellites are elevated relative to centrals in xCOLD GASS. We provide additional predictions for the effect of environment on H$_2$ -- both absolute and relative to HI -- that can be tested with spectral stacking in future CO surveys. ","Molecular hydrogen in IllustrisTNG galaxies: carefully comparing
  signatures of environment with local CO & SFR data"
183,1324787707684352000,1176230196736737281,Aniruddh Raghu,"['Teaching with Commentaries: <LINK>\n\nWe study the use of commentaries, metalearned auxiliary information, to improve neural network training and provide insights.\n\nWith @maithra_raghu, @skornblith, @DavidDuvenaud, @geoffreyhinton\n\nThread⬇️ <LINK>', 'We define commentaries as functions of a task/dataset that are learned by optimizing a network’s validation loss. They can be used to improve the training of new models and to understand aspects of the network training process. https://t.co/4yPwsCGcDu', 'We first learn commentaries that encode a weight for each training example at each training iteration. These example weighting curricula commentaries capture intuitive structure, lead to speedups in network training, and can improve performance on few-shot learning problems. https://t.co/iQFbdWGObV', 'We then investigate commentaries that define a label-dependent data augmentation policy, where images are blended together with a proportion based on their labels. The learned commentaries are interpretable and can improve model performance. https://t.co/zZKkWTYi0H', 'Finally, we explore applying commentaries to identify salient image regions by using them to parameterize image attention masks. On a variety of datasets, the learned masks capture important regions and can improve network robustness to spurious background correlations. https://t.co/o0UHhUwzDF']",https://arxiv.org/abs/2011.03037,"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training. ",Teaching with Commentaries
184,1324775339545882624,1028632965121626114,Jiaxin Pei,"['Excited to share my #emnlp2020 paper with @david__jurgens. We build an NLP model to estimate intimacy in language and study social norms in interpersonal communications\nPaper: <LINK>\nPip: <LINK>\nModel @huggingface: <LINK>\n1/11', 'We build a new model to quantify the intimacy of questions and examine social norms in interpersonal communications in a variety of settings: How do these norms shape the way we communicate with each other? 2/11', 'One famous example from Psychology of a social norm around intimacy is who you can ask an intimate question to. Close friends are fine—but so are strangers! The low social cost of potentially offending a stranger means you can engage in a more intimate discussion. 3/11', 'But acquaintances are in this middle ground; if you get too intimate and they get offended, they know people you know, which could have a real cost. 4/11', 'There are plenty of friends, acquaintances, and strangers online. Do we see the same trend there? Yes! In fact, using a 1.1B edge social network on Twitter, we see people are highly sensitive to social distance in their level of question intimacy. 5/11 https://t.co/2vIhwBQFsK', 'What about gender norms around intimacy communication? Across books, movies, and social media, we consistently find that male-male interactions are the least intimate. 6/11 https://t.co/r48ApCijAF', 'Looking at book author’s gender, surprisingly, we also see these norms be upheld by women authors too, underscoring how ingrained these social expectations are! 7/11 https://t.co/NKRspkC25i', 'Could all of these differences be due to the topic? Actually, no! While topic is moderately correlated with intimacy, questions about a topic often fall along a broad range of the intimacy spectrum. 8/11 https://t.co/tYNeFfNzsy', 'The paper has more fun results on the effects of anonymity on intimacy and different linguistic strategies people use in phrasing their questions. 9/11', 'As a fun teaser, would swearing make a question more or less intimate—and why? Theory and answers in the paper! 10/11', 'The website for the paper has a longer summary of results and more details on how you can get the data and python libraries to do your own studies of intimacy.  https://t.co/CGIjsj41It \n11/11']",https://arxiv.org/abs/2011.03020,"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson's r=0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language. ",Quantifying Intimacy in Language
185,1324403187827105792,209022852,David Yllanes,"['Our latest preprint is up on the arXiv (<LINK>).\n\nMotivated by the problem of transport in the cell, we conduct a scaling study of diffusion in dynamic crowded spaces and find that it is controlled, above and below percolation, by two critical exponents. <LINK>']",https://arxiv.org/abs/2011.02444,"We study Brownian motion in a space with a high density of moving obstacles in 1, 2 and 3 dimensions. Our tracers diffuse anomalously over many decades in time, before reaching a diffusive steady state with an effective diffusion constant $D_\mathrm{eff}$ that depends on the obstacle density and diffusivity. The scaling of $D_\mathrm{eff}$, above and below a critical regime at the percolation point for void space, is characterized by two critical exponents: the conductivity $\mu$, also found in models with frozen obstacles, and $\psi$, which quantifies the effect of obstacle diffusivity. ",Scaling study of diffusion in dynamic crowded spaces
186,1324263696483393538,948995274961309697,Andrea Botteon,"['Paper day! Analysis of the galaxy clusters in LoTSS-DR1, by vanWeeren (+ @HambObs, @IRA_INAF, @roxycas , @AnnalisBonafede, @fabiogasta, @astro_jit, @fradega)\nThis is a massive work where we studied the diffuse radio emission in 40+ clusters with @LOFAR 1/3\n<LINK> <LINK>', 'A new method to improve the image quality of @LOFAR images towards specific targets is reported. This allowed us to reach higher sensitivity and image fidelity compared to the first LoTSS data release 2/3 https://t.co/nzZhwDDCPy', 'The LoTSS-DR1 covers the ""HETDEX region"", an area of ~400 deg^2 released last year by Shimwerll+19. We reported the detection of many new diffuse synchrotron sources in the ICM of clusters in this area, and studied for the first time the P-M and P-Y relations at 150 MHz 3/3 https://t.co/uhoPLW9s0m']",https://arxiv.org/abs/2011.02387,"Diffuse cluster radio sources, in the form of radio halos and relics, reveal the presence of cosmic rays and magnetic fields in the intracluster medium (ICM). These cosmic rays are thought to be (re-)accelerated through ICM turbulence and shock waves generated by cluster merger events. Here we characterize the presence of diffuse radio emission in known galaxy clusters in the HETDEX Spring Field, covering 424 deg$^2$. For this, we developed a method to extract individual targets from LOFAR observations processed with the LoTSS DDF-pipeline. This procedure enables improved calibration and joint imaging and deconvolution of multiple pointings of selected targets. The calibration strategy can also be used for LOFAR Low-Band Antenna (LBA) and international-baseline observations. The fraction of Planck PSZ2 clusters with any diffuse radio emission apparently associated with the ICM is $73\pm17\%$. We detect a total of 10 radio halos and 12 candidate halos in the HETDEX Spring Field. Five clusters host radio relics. The fraction of radio halos in Planck PSZ2 clusters is $31\pm11\%$, and $62\pm15\%$ when including the candidate radio halos. Based on these numbers, we expect that there will be at least $183 \pm 65$ radio halos found in the LoTSS survey in PSZ2 clusters, in agreement with predictions. The integrated flux densities for the radio halos were computed by fitting exponential models to the radio images. From these flux densities, we determine the cluster mass (M$_{500}$) and Compton Y parameter (Y$_{500}$) 150 MHz radio power (P$_{\rm{150 MHz}}$) scaling relations for Planck PSZ2-detected radio halos. We find that the slopes of these relations are steeper than those determined from the 1.4 GHz radio powers. However, considering the uncertainties this is not a statistically significant result. ",LOFAR observations of galaxy clusters in HETDEX
187,1324100469204176897,954294518513307648,Pablo Barros,"['We got our pre-registration study accepted at <LINK> at #Neurips2020!🎉🎉\nWe propose a study on how to add socially-observed #rivalry to traditional #RL algorithms to change their behavior when playing a competitive card game.\npre-print: <LINK> <LINK>', 'Another collaboration with the great people from \n@CONTACT_unit and #ÖzgeNilayYalçın from the British Columbia Uni. We are very excited about continuing this pre-registration study to the experimental phase in the next months. Congrats to the organizers for this great initiative!', 'It is my first time participating in a pre-reg study, and I cannot recommend it enough! We put into practice many techniques we usually learn during scientific training that are usually forgotten when mass-producing papers to pump our cv!']",https://arxiv.org/abs/2011.01337,"Recent advances in reinforcement learning with social agents have allowed us to achieve human-level performance on some interaction tasks. However, most interactive scenarios do not have as end-goal performance alone; instead, the social impact of these agents when interacting with humans is as important and, in most cases, never explored properly. This preregistration study focuses on providing a novel learning mechanism based on a rivalry social impact. Our scenario explored different reinforcement learning-based agents playing a competitive card game against human players. Based on the concept of competitive rivalry, our analysis aims to investigate if we can change the assessment of these agents from a human perspective. ",Incorporating Rivalry in Reinforcement Learning for a Competitive Game
188,1323906108134629377,15989147,Tadashi Okoshi,"['""NationalMood: Large-scale Estimation of People\'s Mood from Web Search Query and Mobile Sensor Data"" on arxiv: \n- We propose a novel way of #mood estimation based on a combinational use of user\'s web search queries and mobile sensor data. <LINK>\n#ubicomp #www <LINK>']",https://arxiv.org/abs/2011.00665,"The ability to estimate current affective statuses of web users has considerable potential towards the realization of user-centric opportune services. However, determining the type of data to be used for such estimation as well as collecting the ground truth of such affective statuses are difficult in the real world situation. We propose a novel way of such estimation based on a combinational use of user's web search queries and mobile sensor data. Our large-scale data analysis with about 11,000,000 users and 100 recent advertisement log revealed (1) the existence of certain class of advertisement to which mood-status-based delivery would be significantly effective, (2) that our ""National Mood Score"" shows the ups and downs of people's moods in COVID-19 pandemic that inversely correlated to the number of patients, as well as the weekly mood rhythm of people. ","NationalMood: Large-scale Estimation of People's Mood from Web Search
  Query and Mobile Sensor Data"
189,1323448743253938177,20041912,Chentao Yang | 杨辰涛,"[""proud to be a co-author of today's arXiv paper: <LINK> -- using the impressive ALMA (@almaobs) data, plus the @ESAHerschel data, we studied the ISM structure of ESO320-G030 and identify a molecular inflow associated with a bar that fueling the nuclear starburst <LINK>""]",https://arxiv.org/abs/2011.00347,"Galaxies with nuclear bars are believed to efficiently drive gas inward, generating a nuclear starburst and possibly an active galactic nucleus (AGN). We confirm this scenario for the isolated, double-barred, luminous infrared galaxy ESO 320-G030 based on an analysis of Herschel and ALMA spectroscopic observations. Herschel/PACS and SPIRE observations of ESO 320-G030 show absorption/emission in 18 lines of H2O, which we combine with the ALMA H2O 423-330 448 GHz line (Eupper~400 K) and continuum images to study the nuclear region. Radiative transfer models indicate that 3 nuclear components are required to account for the H2O and continuum data. An envelope, with R~130-150 pc, T_dust~50 K, and N_H2~2x10^{23} cm^{-2}, surrounds a nuclear disk with R~40 pc and tau_100um~1.5-3 (N_H2~2x10^{24} cm^{-2}) and an extremely compact (R~12 pc), warm (~100 K), and buried (tau_100um>5, N_H2>~5x10^{24} cm^{-2}) core component. The three nuclear components account for 70% of the galaxy L_IR (SFR~16-18 Msun yr^{-1}). The nucleus is fed by a molecular inflow observed in CO 2-1 with ALMA, which is associated with the nuclear bar. With decreasing radius (r=450-225 pc), the mass inflow rate increases up to ~20 Msun yr^{-1}, which is similar to the nuclear SFR, indicating that the starburst is sustained by the inflow. At lower r, the inflow is best probed by the far-infrared OH ground-state doublets, with an estimated inflow rate of ~30 Msun yr^{-1}. The short timescale of ~20 Myr for nuclear gas replenishment indicates quick secular evolution, and indicates that we are witnessing an intermediate stage (<100 Myr) proto-pseudobulge fed by a massive inflow that is driven by a strong nuclear bar. We also apply the H2O model to the Herschel far-infrared spectroscopic observations of H2^{18}O, OH, $^{18}OH, OH+, H2O^+, H3O^+, NH, NH2, NH3, CH, CH^+, ^{13}CH^+, HF, SH, and C3, and estimate their abundances. ","A proto-pseudobulge in ESO 320-G030 fed by a massive molecular inflow
  driven by a nuclear bar"
190,1328593904048988160,2917879835,Claudia Cicone,"['While waiting for the actual @EU_H2020 AtLAST Design study to start, we put together a SPIE paper summarising the main goals and structure of the project <LINK>']",https://arxiv.org/abs/2011.07974,"The coldest and densest structures of gas and dust in the Universe have unique spectral signatures across the (sub-)millimetre bands ($\nu \approx 30-950$~GHz). The current generation of single dish facilities has given a glimpse of the potential for discovery, while sub-mm interferometers have presented a high resolution view into the finer details of known targets or in small-area deep fields. However, significant advances in our understanding of such cold and dense structures are now hampered by the limited sensitivity and angular resolution of our sub-mm view of the Universe at larger scales. In this context, we present the case for a new transformational astronomical facility in the 2030s, the Atacama Large Aperture Submillimetre Telescope (AtLAST). AtLAST is a concept for a 50-m-class single dish telescope, with a high throughput provided by a 2~deg - diameter Field of View, located on a high, dry site in the Atacama with good atmospheric transmission up to $\nu\sim 1$~THz, and fully powered by renewable energy. We envision AtLAST as a facility operated by an international partnership with a suite of instruments to deliver the transformative science that cannot be achieved with current or in-construction observatories. As an 50m-diameter telescope with a full complement of advanced instrumentation, including highly multiplexed high-resolution spectrometers, continuum cameras and integral field units, AtLAST will have mapping speeds hundreds of times greater than current or planned large aperture ($>$ 12m) facilities. By reaching confusion limits below L$_*$ in the distant Universe, resolving low-mass protostellar cores at the distance of the Galactic Centre, and directly mapping both the cold and the hot (the Sunyaev-Zeldovich effect) circumgalactic medium of galaxies, AtLAST will enable a fundamentally new understanding of the sub-mm Universe. ",The Atacama Large Aperture Submillimetre Telescope (AtLAST)
191,1326067640306192384,2401274228,tvayer,['Happy to share the manuscript of my PhD thesis! <LINK>\nContains detailed version of my works past 3yea. New results in Ch4 about Gromov-Wasserstein: we study the problem of finding an optimal Monge map + closed form linear Gromov Monge between Gaussian.'],https://arxiv.org/abs/2011.04447,"Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points. Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization. This thesis proposes to study the complex scenario in which the different data belong to incomparable spaces. In particular we address the following questions: how to define and apply Optimal Transport between graphs, between structured data? How can it be adapted when the data are varied and not embedded in the same metric space? This thesis proposes a set of Optimal Transport tools for these different cases. An important part is notably devoted to the study of the Gromov-Wasserstein distance whose properties allow to define interesting transport problems on incomparable spaces. More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation. ",A contribution to Optimal Transport on incomparable spaces
