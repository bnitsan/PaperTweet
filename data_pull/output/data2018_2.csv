,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,973366224766816258,807855012927995904,Artsiom Sanakoyeu,"['We got a new paper on Unsupervised Learning published at Patter Recognition Journal!  \n\nDeep Unsupervised Learning of Visual Similarities, \nArtsiom Sanakoyeu,  Miguel A. Bautista (@mbautista_ibz ),  Bj√∂rn Ommer  \n\n<LINK>  \n<LINK> <LINK>']",https://arxiv.org/abs/1802.08562,"Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to Computer Vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification. ",Deep Unsupervised Learning of Visual Similarities
1,969663589375184896,882257115863187457,Sanjeev Arora,"[""My new paper (joint with Nadav Cohen and Elad Hazan) on the benefits of overparametrization is up <LINK>.\nI recommend Nadav's nice blog post as a starting point: \n<LINK>""]",https://arxiv.org/abs/1802.06509,"Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization - linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with $\ell_p$ loss, $p>2$, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer. ","On the Optimization of Deep Networks: Implicit Acceleration by
  Overparameterization"
2,969352189314465793,2800204849,Andrew Gordon Wilson,"['Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\nOur new paper: <LINK> <LINK>']",https://arxiv.org/abs/1802.10026,"The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet. ","Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"
3,969247170489589762,825508249,Andrew W. Steiner,"['Utk graduate student Xingfu Du, Jeremy Holt, and I just finished the first paper on our new class of phenomenological finite temperature equations of state: <LINK>']",https://arxiv.org/abs/1802.09710,"We construct a new class of phenomenological equations of state for homogeneous matter for use in simulations of hot and dense matter in local thermodynamic equilibrium. We construct a functional form which respects experimental, observational and theoretical constraints on the nature of matter in various density and temperature regimes. Our equation of state matches (i) the virial coefficients expected from nucleon-nucleon scattering phase shifts, (ii) experimental measurements of nuclear masses and charge radii, (iii) observations of neutron star radii, (iv) theory results on the equation of state of neutron matter near the saturation density, and (v) theory results on the evolution of the EOS at finite temperatures near the saturation density. Our analytical model allows one to compute the variation in the thermodynamic quantities based on the uncertainties in the nature of the nucleon-nucleon interaction. Finally, we perform a correction to ensure the equation of state is causal at all densities, temperatures, and electron fractions. ","Hot and Dense Homogeneous Nucleonic Matter Constrained by Observations,
  Experiment, and Theory"
4,969027596255047680,472628395,Peter Melchior,"[""Here's our new paper on source separation with constrained matrix factorization: each celestial source gets a SED and a non-parametric shape, and we can deblend really crowded scenes. Plenty applications for HSC, @LSST, and @NASAWFIRST <LINK>""]",https://arxiv.org/abs/1802.10157,"We present the source separation framework SCARLET for multi-band images, which is based on a generalization of the Non-negative Matrix Factorization to alternative and several simultaneous constraints. Our approach describes the observed scene as a mixture of components with compact spatial support and uniform spectra over their support. We present the algorithm to perform the matrix factorization and introduce constraints that are useful for optical images of stars and distinct stellar populations in galaxies, in particular symmetry and monotonicity with respect to the source peak position. We also derive the treatment of correlated noise and convolutions with band-dependent point spread functions, rendering our approach applicable to coadded images observed under variable seeing conditions. SCARLET thus yields a PSF-matched photometry measurement with an optimally chosen weight function given by the mean morphology in all available bands. We demonstrate the performance of SCARLET for deblending crowded extragalactic scenes and on an AGN jet -- host galaxy separation problem in deep 5-band imaging from the Hyper Suprime-Cam Stategic Survey Program. Using simulations with prominent crowding we show that SCARLET yields superior results to the HSC-SDSS deblender for the recovery of total fluxes, colors, and morphologies. Due to its non-parametric nature, a conceptual limitation of SCARLET is its sensitivity to undetected sources or multiple stellar population within detected sources, but an iterative strategy that adds components at the location of significant residuals appears promising. The code is implemented in Python with C++ extensions and is available at this https URL ","SCARLET: Source separation in multi-band images by Constrained Matrix
  Factorization"
5,968910018396344320,2235411914,Surya Ganguli,['New #deeplearning paper: The Emergence of Spectral Universality in Deep Networks <LINK> to appear at AISTATS 2018.  Fun collab w/ Jeffery Pennington &amp; Sam Schoenholz @GoogleBrain ; provides analytic info about Jacobian spectra for deep networks w/ random init'],https://arxiv.org/abs/1802.09979,"Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity. ",The Emergence of Spectral Universality in Deep Networks
6,968829477819121664,855945227718230016,Tarraneh Eftekhari,['Our new paper on associating fast radio bursts with persistent radio counterparts out on the arXiv today! <LINK>'],https://arxiv.org/abs/1802.09525,"The discovery of a repeating fast radio burst has led to the first precise localization, an association with a dwarf galaxy, and the identification of a coincident persistent radio source. However, further localizations are required to determine the nature of FRBs, the sources powering them, and the possibility of multiple populations. Here we investigate the use of associated persistent radio sources to establish FRB counterparts, taking into account the localization area and the persistent source flux density. Due to the lower areal number density of radio sources compared to faint optical sources, robust associations can be achieved for less precise localizations as compared to direct optical host galaxy associations. For generally larger localizations which preclude robust associations, the number of candidate hosts can be reduced based on the ratio of radio-to-optical brightness. We find that confident associations with $\sim 0.01-$1 mJy sources, comparable to the luminosity of the persistent source associated with FRB 121102 over the redshift range $z \approx 0.1 - 1$, require FRB localizations of $\lesssim 20''$. In the absence of a robust association, constraints can be placed on the luminosity of an associated radio source as a function of localization and DM. For DM $\approx 1000 \rm \ pc \ cm^{-3}$, an upper limit comparable to the luminosity of the FRB 121102 persistent source can be placed if the localization is $\lesssim 10''$. We apply our analysis to the case of the ASKAP FRB 170107, using optical and radio observations of the localization region. We identify two candidate hosts based on a ratio of radio-to-optical brightness of $\gtrsim 100$. We find that if one of these is associated with FRB 170107, the resulting radio luminosity ($1 \times 10^{29} - 4 \times 10^{30} \ \rm erg \ s^{-1} \ Hz^{-1}$) is comparable to the luminosity of the FRB 121102 persistent source. ","Associating Fast Radio Bursts with Extragalactic Radio Sources: General
  Methodology and a Search for a Counterpart to FRB 170107"
7,968825983854874624,29843511,Nando de Freitas üè≥Ô∏è‚Äçüåà,['Our new paper on learning diverse robot manipulation skills with generative adversarial imitation and reinforcement  <LINK>'],https://arxiv.org/abs/1802.09564,"We propose a model-free deep reinforcement learning method that leverages a small amount of demonstration data to assist a reinforcement learning agent. We apply this approach to robotic manipulation tasks and train end-to-end visuomotor policies that map directly from RGB camera inputs to joint velocities. We demonstrate that our approach can solve a wide variety of visuomotor tasks, for which engineering a scripted controller would be laborious. In experiments, our reinforcement and imitation agent achieves significantly better performances than agents trained with reinforcement learning or imitation learning alone. We also illustrate that these policies, trained with large visual and dynamics variations, can achieve preliminary successes in zero-shot sim2real transfer. A brief visual description of this work can be viewed in this https URL ",Reinforcement and Imitation Learning for Diverse Visuomotor Skills
8,968799053172695040,4249537197,Christian Wolf,"['Our FG2018 paper on Deep Learning of Multi-Touch Gesture Recognition, plus a new Dataset on this application called ""Itekube-7"". @Quentin_Debard, @itekube. Paper: <LINK>, Dataset: <LINK> <LINK>']",https://arxiv.org/abs/1802.09901,"We propose a fully automatic method for learning gestures on big touch devices in a potentially multi-user context. The goal is to learn general models capable of adapting to different gestures, user styles and hardware variations (e.g. device sizes, sampling frequencies and regularities). Based on deep neural networks, our method features a novel dynamic sampling and temporal normalization component, transforming variable length gestures into fixed length representations while preserving finger/surface contact transitions, that is, the topology of the signal. This sequential representation is then processed with a convolutional model capable, unlike recurrent networks, of learning hierarchical representations with different levels of abstraction. To demonstrate the interest of the proposed method, we introduce a new touch gestures dataset with 6591 gestures performed by 27 people, which is, up to our knowledge, the first of its kind: a publicly available multi-touch gesture dataset for interaction. We also tested our method on a standard dataset of symbolic touch gesture recognition, the MMG dataset, outperforming the state of the art and reporting close to perfect performance. ","Learning to recognize touch gestures: recurrent vs. convolutional
  features and dynamic sampling"
9,968787175629381632,2432886163,Oscar Barrag√°n,"['Our new paper on the mass determination of a 3 planet system is out!\n\n""Mass determination of the 1:3:5 near-resonant planets transiting GJ 9827""\n\n<LINK> <LINK>']",https://arxiv.org/abs/1802.09557,"Aims. GJ 9827 (K2-135) has recently been found to host a tightly packed system consisting of three transiting small planets whose orbital periods of 1.2, 3.6, and 6.2 days are near the 1:3:5 ratio. GJ 9827 hosts the nearest planetary system (d = $30.32\pm1.62$ pc) detected by Kepler and K2 . Its brightness (V = 10.35 mag) makes the star an ideal target for detailed studies of the properties of its planets. Results. We find that GJ 9827 b has a mass of $M_\mathrm{b}=3.74^{+0.50}_{-0.48}$ $M_\oplus$ and a radius of $R_\mathrm{b}=1.62^{+0.17}_{-0.16}$ $R_\oplus$, yielding a mean density of $\rho_\mathrm{b} = 4.81^{+1.97}_{-1.33}$ g cm$^{-3}$. GJ 9827 c has a mass of $M_\mathrm{c}=1.47^{+0.59}_{-0.58}$ $M_\oplus$, radius of $R_\mathrm{c}=1.27^{+0.13}_{-0.13}$ $R_\oplus$, and a mean density of $\rho_\mathrm{c}= 3.87^{+2.38}_{-1.71}$ g cm$^{-3}$. For GJ 9827 d we derive $M_\mathrm{d}=2.38^{+0.71}_{-0.69}$ $M_\oplus$, $R_\mathrm{d}=2.09^{+0.22}_{-0.21}$ $R_\oplus$, and $\rho_\mathrm{d}= 1.42^{+0.75}_{-0.52}$ g cm$^{-3}$. Conclusions. GJ 9827 is one of the few known transiting planetary systems for which the masses of all planets have been determined with a precision better than 30%. This system is particularly interesting because all three planets are close to the limit between super-Earths and mini-Neptunes. We also find that the planetary bulk compositions are compatible with a scenario where all three planets formed with similar core/atmosphere compositions, and we speculate that while GJ 9827 b and GJ 9827 c lost their atmospheric envelopes, GJ 9827 d maintained its atmosphere, owing to the much lower stellar irradiation. This makes GJ 9827 one of the very few systems where the dynamical evolution and the atmospheric escape can be studied in detail for all planets, helping us to understand how compact systems form and evolve. ","Mass determination of the 1:3:5 near-resonant planets transiting GJ 9827
  (K2-135)"
10,968697309801472000,1041578714,Benjamin Pope,"['Our paper on the red giant Aldebaran and its planet, led by @farrwill, is on arXiv! We reanalyze two decades of Doppler data, plus new K2 photometry, and detect oscillations in Aldebaran and therefore determine its mass. <LINK>', 'It turns out the ""noise"" in historic RV data is in fact the signal of stellar oscillations - literally sound in the star - and @farrwill and @exoplaneteer figured out how to extract this signal statistically. @hviddie &amp; I confirmed this measurement with @NASAKepler K2 photometry.', 'A neat consequence of this is Aldebaran is somewhat less massive than previously thought - and its new mass means that its planet Aldebaran b, and any moons, received as much sunlight from Aldebaran as we do from the Sun on Earth, when Aldebaran was a main sequence star.', 'Nowadays Aldebaran is a red giant and the planet is blasted to well over a thousand degrees - but billions of years ago, the planet\'s moons, if they exist, were probably ""habitable"", or at least temperate, in some sense of the word.', ""People find habitable zone worlds all the time but we think it's pretty fun to find a dead world."", ""This work is underpinned by @farrwill and @exoplaneteer pioneering ways to apply @brendonbrewer's Gaussian Process models to RV data with much greater computational efficiency, and @hviddie and I working on extracting photometry of bright stars."", ""I believe Aldebaran may be the brightest ever star observed by @NASAKepler leading to science-quality published data - is that right @hviddie? It's slightly brighter than Spica... Anyway, at ~10k times the saturation limit, this was a huge task and @KeplerGO were invaluable help."", 'Planets around red giants have been controversial because of problems related to stellar noise - literally noise, the acoustic oscillations in the star - and this has meant we have only detected very massive planets around giants so far. Our work shows a way past this problem.', ""@brendonbrewer @farrwill @exoplaneteer @hviddie this is quite useful! We have been using @farrwill's https://t.co/0WUvWddAmr plus his own CARMA implementation https://t.co/c9u1PVfYFU"", 'The @NASAKepler K2 data of Aldebaran are wildly saturated and the spacecraft is jittering around like crazy, so we use a machine learning approach to figure out how to calibrate this data and extract a good light curve: our code is open source at https://t.co/AERI9CCfth https://t.co/yzxJZHXEuk', 'As an aside, it\'s fun to work on stars that are so bright they have names and histories. Aldebaran means ""the follower"" in Arabic, as it follows the Pleiades across the sky, so it\'s appropriate that it was the subject of the second paper to do halo photometry after the Pleiades!', ""In science fiction, Aldebaran b is where the aliens come from in Ursula Le Guin's Lathe of Heaven; where Terrans first contact the Taurans in Haldeman's Forever War; and where the dragons live in Peter F Hamilton's Fallen Dragon."", 'All of this was made possible by the bold and successful open data policies of Artie Hatzes, for sharing publicly more than twenty years of RV observations, of @KeplerGO for giving us quite unorthodox observing modes, and the kind SONG Telescope folk for giving us follow up time.', '@CharlesSimmins @Dr_CMingarelli @farrwill I have always wondered if Alderaan was a typo for Aldebaran in the script', '@brendonbrewer @farrwill @exoplaneteer @hviddie keen to play with it!', '@MrAllenBlair go for it!']",https://arxiv.org/abs/1802.09812,"The nearby red giant Aldebaran is known to host a gas giant planetary companion from decades of ground-based spectroscopic radial velocity measurements. Using Gaussian Process-based Continuous Auto-Regressive Moving Average (CARMA) models, we show that these historic data also contain evidence of acoustic oscillations in the star itself, and verify this result with further dedicated ground-based spectroscopy and space-based photometry with the Kepler Space Telescope. From the frequency of these oscillations we determine the mass of Aldebaran to be $1.16 \pm 0.07 \, M_\odot$, and note that this implies its planet will have been subject to insolation comparable to the Earth for some of the star's main sequence lifetime. Our approach to sparse, irregularly sampled time series astronomical observations has the potential to unlock asteroseismic measurements for thousands of stars in archival data, and push to lower-mass planets around red giant stars. ",Aldebaran b's temperate past uncovered in planet search data
11,968659858051317760,23980621,"Brett Morris, PhD","['New paper inspired by the #knowthystar conference: starspots are a source of astrometric jitter, which we can use to study the activity of the nearest stars with @ESAGaia!\n<LINK> <LINK>', 'The above animation shows a digital approximation of hand-recorded sunspot observations from Mount Wilson Observatory near solar maximum in 1957. As the Sun rotates, spots move into and out of view, moving the apparent centroid of the Sun (red X, exaggerated).', 'Among the best targets for activity detection with @ESAGaia (in the TGAS sample) are Gl 825, sigma Dra, and Gl 15 A. As the bright limits of Gaia are revised in future data releases, there may be even better candidates.', '@bmac_astro @johannateske @ESAGaia Yes, but the signal would be very small for Sun-like active latitudes. The biggest signal is the on/off nature of activity min/max.', '@bmac_astro @johannateske @ESAGaia  https://t.co/w6nYXNhJUd', '@lkreidberg  https://t.co/aGR3iL2Mqt']",https://arxiv.org/abs/1802.09943,"Astrometry from Gaia will measure the positions of stellar photometric centroids to unprecedented precision. We show that the precision of Gaia astrometry is sufficient to detect starspot-induced centroid jitter for nearby stars in the Tycho-Gaia Astrometric Solution (TGAS) sample with magnetic activity similar to the young G-star KIC 7174505 or the active M4 dwarf GJ 1243, but is insufficient to measure centroid jitter for stars with Sun-like spot distributions. We simulate Gaia observations of stars with 10 year activity cycles to search for evidence of activity cycles, and find that Gaia astrometry alone likely can not detect activity cycles for stars in the TGAS sample, even if they have spot distributions like KIC 7174505. We review the activity of the nearby low-mass stars in the TGAS sample for which we anticipate significant detections of spot-induced jitter. ",Spotting Stellar Activity Cycles in Gaia Astrometry
12,968548998431825920,817046037487566852,"Alireza Bagheri, PhD","['Check out our new paper: ""Adversarial Training for Probabilistic Spiking Neural Networks"" @arXivStats <LINK> <LINK>']",https://arxiv.org/abs/1802.08567,"Classifiers trained using conventional empirical risk minimization or maximum likelihood methods are known to suffer dramatic performance degradations when tested over examples adversarially selected based on knowledge of the classifier's decision rule. Due to the prominence of Artificial Neural Networks (ANNs) as classifiers, their sensitivity to adversarial examples, as well as robust training schemes, have been recently the subject of intense investigation. In this paper, for the first time, the sensitivity of spiking neural networks (SNNs), or third-generation neural networks, to adversarial examples is studied. The study considers rate and time encoding, as well as rate and first-to-spike decoding. Furthermore, a robust training mechanism is proposed that is demonstrated to enhance the performance of SNNs under white-box attacks. ",Adversarial Training for Probabilistic Spiking Neural Networks
13,968523266997415936,29251447,Tuan Do,"['In our new paper, we found that some stars at the Galactic center have very unusual abundances compared to elsewhere in the Milky Way. This indicates a potentially very different chemical enrichment history or supernova rates. \n\n<LINK>', ""It's also the first paper I've written with @jlu_astro and @QuinnKono together!""]",https://arxiv.org/abs/1802.08270,"We present adaptive-optics assisted near-infrared high-spectral resolution observations of late-type giants in the nuclear star cluster of the Milky Way. The metallicity and elemental abundance measurements of these stars offer us an opportunity to understand the formation and evolution of the nuclear star cluster. In addition, their proximity to the supermassive black hole ($\sim 0.5$ pc) offers a unique probe of the star formation and chemical enrichment in this extreme environment. We observed two stars identified by medium spectral-resolution observations as potentially having very high metallicities. We use spectral-template fitting with the PHOENIX grid and Bayesian inference to simultaneously constrain the overall metallicity, [M/H], alpha-element abundance [$\alpha$/Fe], effective temperature, and surface gravity of these stars. We find that one of the stars has very high metallicity ([M/H] $> 0.6$) and the other is slightly above solar metallicity. Both Galactic center stars have lines from scandium (Sc), vanadium (V), and yttrium (Y) that are much stronger than allowed by the PHOENIX grid. We find, using the spectral synthesis code Spectroscopy Made Easy, that [Sc/Fe] may be an order of magnitude above solar. For comparison, we also observed an empirical calibrator in NGC6791, the highest metallicity cluster known ([M/H] $\sim 0.4$). Most lines are well matched between the calibrator and the Galactic center stars, except for Sc, V, and Y, which confirms that their abundances must be anomalously high in these stars. These unusual abundances, which may be a unique signature of nuclear star clusters, offer an opportunity to test models of chemical enrichment in this region. ","Super-Solar Metallicity Stars in the Galactic Center Nuclear Star
  Cluster: Unusual Sc, V, and Y Abundances"
14,968504204716371968,885528008,William Fedus,"[""Our @NipsConference '17 workshop paper finally on arxiv:  <LINK>!\n\nTLDR:  New intrinsic reward for RL agents to learn independently controllable features of the environment by interacting with it.  The objective is a lower bound on causal directed information."", 'And a cool tribute to the Montreal AI community.  A seemingly improbable collaboration between @UMontreal , @mcgillu, @Facebook, @GoogleBrain, @element_ai, @DeepMindAI']",https://arxiv.org/abs/1802.09484,"It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors, and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal. ","Disentangling the independently controllable factors of variation by
  interacting with the world"
15,968409608699203584,481539448,Richard Alexander,"['New paper, led by @PhysicsUoL postdoc @cassidentprone (w/@ExoKenRice, Giovanni Dipierro, @dh4gan, @tharries &amp; me). We built detailed models of the Elias 2-27 disc, and found that the spirals seen by ALMA are probably *not* due to gravitational instability.\n<LINK> <LINK>', 'The argument is subtle, and required some painstaking work by @cassidentprone to reproduce the ALMA images. But the result can be crudely summarised as ""it\'s very hard to make real self-gravitating discs look like this"".', '@julianonions @cassidentprone Not quite... üòâ\nThe original ALMA image was processed with an unsharp mask, so we did the same with the simulation results.']",https://arxiv.org/abs/1802.09451,"A recent ALMA observation of the Elias 2-27 system revealed a two-armed structure extending out to ~300 au in radius. The protostellar disc surrounding the central star is unusually massive, raising the possibility that the system is gravitationally unstable. Recent work has shown that the observed morphology of the system can be explained by disc self-gravity, so we examine the physical properties of the disc necessary to detect self-gravitating spiral waves. Using three-dimensional Smoothed Particle Hydrodynamics, coupled with radiative transfer and synthetic ALMA imaging, we find that observable spiral structure can only be explained by self-gravity if the disc has a low opacity (and therefore efficient cooling), and is minimally supported by external irradiation. This corresponds to a very narrow region of parameter space, suggesting that, although it is possible for the spiral structure to be due to disc self-gravity, other explanations, such as an external perturbation, may be preferred. ","Is the spiral morphology of the Elias 2-27 circumstellar disc due to
  gravitational instability?"
16,968404410920554496,1562913787,Nathan Moynihan,['New paper out! <LINK>'],https://arxiv.org/abs/1802.09499,"In General Relativity, gravity is universally attractive, a feature embodied by the Raychaudhuri equation which requires that the expansion of a congruence of geodesics is always non-increasing, as long as matter obeys the strong or weak energy conditions. This behavior of geodesics is an important ingredient in general proofs of singularity theorems, which show that many spacetimes are singular in the sense of being geodesically incomplete and suggest that General Relativity is itself incomplete. It is possible that alternative theories of gravity, which reduce to General Relativity in some limit, can resolve these singularities, so it is of interest to consider how the behavior of geodesics is modified in these frameworks. We compute the leading corrections to the Raychaudhuri equation for the expansion due to models in string theory, braneworld gravity, $f(R)$ theories, and Loop Quantum Cosmology, for cosmological and black hole backgrounds, and show that while in most cases geodesic convergence is reinforced, in a few cases terms representing repulsion arise, weakening geodesic convergence and thereby the conclusions of the singularity theorems. ",Towards the Raychaudhuri Equation Beyond General Relativity
17,968318084221231105,180333300,Nicolas Papernot,"['Find out why privacy and accuracy are not necessarily enemies! The new PATE is more accurate, more private and more confident! Final version of the ICLR 2018 paper here: <LINK> <LINK>']",https://arxiv.org/abs/1802.08908,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\varepsilon$ < 1.0). ",Scalable Private Learning with PATE
18,968307087481569281,51590414,Tom Brown,"['Our new paper: ""Is Generator Conditioning Causally Related to GAN Performance?""\n\nTLDR: ""Almost certainly""\n\n<LINK> <LINK>']",https://arxiv.org/abs/1802.08768,"Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the 'quality' of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a 'regularization' technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs. ",Is Generator Conditioning Causally Related to GAN Performance?
19,968220914780426240,968215499543900161,Ben King,['New paper on mode ALP-Compton scattering in #QFT  the @arxiv from @PlymUni researcher @bkingspeaking and collaborators. <LINK>'],https://arxiv.org/abs/1802.07498,"We derive production yields for massive pseudo-scalar and scalar axion-like-particles (ALPs), through non-linear Compton scattering of an electron in the background of low- and high-intensity electromagnetic fields. In particular, we focus on electromagnetic fields from Gaussian plane wave laser pulses. A detailed study of the angular distributions and effects of the scalar and pseudo-scalar masses is presented. It is shown that ultra-relativistic seed electrons can be used to produce scalars and pseudo-scalars with masses up to the order of the electron mass. We briefly discuss future applications of this work towards lab-based searches for light beyond-the-Standard-Model particles. ",ALP production through non-linear Compton scattering in intense fields
20,968188861519093760,1690186220,Erich Elsen,"['Check out my new paper ""Efficient Neural Audio Synthesis.""  Enables Wavenet quality inference on low power CPUs: <LINK>']",https://arxiv.org/abs/1802.08435,"Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency. ",Efficient Neural Audio Synthesis
21,968180229687709697,157014702,Jessie Shelton,"['New paper today! <LINK> #neutronlifetime', 'Briefly, any new particle that can explain the outstanding discrepancy in neutron lifetime measurements has to have incredibly special interactions with itself, or neutron stars go kaput', 'Two other groups reached the same conclusion at exactly the same time -- \nhttps://t.co/00Lowa4PyU\nhttps://t.co/tJlLVUNjdq']",https://arxiv.org/abs/1802.08282,"We demonstrate that the observation of neutron stars with masses greater than one solar mass places severe demands on any exotic neutron decay mode that could explain the discrepancy between beam and bottle measurements of the neutron lifetime. If the neutron can decay to a stable, feebly-interacting dark fermion, the maximum possible mass of a neutron star is 0.7 solar masses, while all well-measured neutron star masses exceed one solar mass. The survival of $2 M_\odot$ neutron stars therefore indicates that any explanation beyond the Standard Model for the neutron lifetime puzzle requires dark matter to be part of a multi-particle dark sector with highly constrained interactions. ",Testing dark decays of baryons in neutron stars
22,967975603038138368,428543680,Enrico Coiera MB BS PhD,['[new paper] Modelling spatiotemporal variation of positive and negative sentiment on Twitter to improve the identification of localised deviations. Work led by the inimitable @adamgdunn  <LINK>'],https://arxiv.org/abs/1802.07859,"Background: Studies examining how sentiment on social media varies depending on timing and location appear to produce inconsistent results, making it hard to design systems that use sentiment to detect localized events for public health applications. Objective: The aim of this study was to measure how common timing and location confounders explain variation in sentiment on Twitter. Methods: Using a dataset of 16.54 million English-language tweets from 100 cities posted between July 13 and November 30, 2017, we estimated the positive and negative sentiment for each of the cities using a dictionary-based sentiment analysis and constructed models to explain the differences in sentiment using time of day, day of week, weather, city, and interaction type (conversations or broadcasting) as factors and found that all factors were independently associated with sentiment. Results: In the full multivariable model of positive (Pearson r in test data 0.236; 95\% CI 0.231-0.241) and negative (Pearson r in test data 0.306; 95\% CI 0.301-0.310) sentiment, the city and time of day explained more of the variance than weather and day of week. Models that account for these confounders produce a different distribution and ranking of important events compared with models that do not account for these confounders. Conclusions: In public health applications that aim to detect localized events by aggregating sentiment across populations of Twitter users, it is worthwhile accounting for baseline differences before looking for unexpected changes. ","Modeling Spatiotemporal Factors Associated With Sentiment on Twitter:
  Synthesis and Suggestions for Improving the Identification of Localized
  Deviations"
23,967648638469136386,816299192364728324,Sandro Wimberger,"['Our new paper, perfect experiment-theory collaboration! <LINK>']",https://arxiv.org/abs/1802.08160,"We present a discrete-time, one-dimensional quantum walk based on the entanglement between the momentum of ultracold rubidium atoms (the walk space) and two internal atomic states (the ""coin"" degree of freedom). Our scheme is highly flexible and can provide a platform for a wide range of applications such as quantum search algorithms, the observation of topological phases, and the realization of walks with higher dimensionality. Along with the investigation of the quantum-to-classical transition, we demonstrate the distinctive features of a quantum walk and contrast them to those of its classical counterpart. Also, by manipulating either the walk or coin operator, we show how the walk dynamics can be steered or even reversed. ",Quantum Walk in Momentum Space with a Bose-Einstein Condensate
24,967111809344856064,19465243,Hannah Earnshaw,['We have a new paper out! In which we present a way of finding candidate pulsar ULXs independent of detecting pulsations. <LINK>'],http://arxiv.org/abs/1802.07753,"We search for transient sources in a sample of ULXs from the 3XMM-DR4 release of the XMM-Newton Serendipitous Source Catalogue in order to find candidate neutron star ULXs alternating between an accreting state and the propeller regime, in which the luminosity drops dramatically. By examining their fluxes and flux upper limits, we identify five ULXs that demonstrate long-term variability of over an order of magnitude. Using Chandra and Swift data to further characterise their light curves, we find that two of these sources are detected only once and could be X-ray binaries in outburst that only briefly reach ULX luminosities. Two others are consistent with being super-Eddington accreting sources with high levels of inter-observation variability. One source, M51 ULX-4, demonstrates apparent bimodal flux behaviour that could indicate the propeller regime. It has a hard X-ray spectrum, but no significant pulsations in its timing data, although with an upper limit of 10% of the signal pulsed at ~1.5 Hz a pulsating ULX cannot be excluded, particularly if the pulsations are transient. By simulating XMM-Newton observations of a population of pulsating ULXs, we predict that there could be approximately 200 other bimodal ULXs that have not been observed sufficiently well by XMM-Newton to be identified as transient. ","Searching for propeller-phase ULXs in the XMM-Newton Serendipitous
  Source Catalogue"
25,967016520500105216,718799493160964096,Ernesto Estrada,"['If you heat your network, IT MELTS! See our new paper wt @_AlexArenas in <LINK> <LINK>']",http://arxiv.org/abs/1802.07809,"Complex networks are the representative graphs of interactions in many complex systems. Usually, these interactions are abstractions of the communication/diffusion channels between the units of the system. Real complex networks, e.g. traffic networks, reveal different operation phases governed by the dynamical stress of the system. Here we show how, communicability, a topological descriptor that reveals the efficiency of the network functionality in terms of these diffusive paths, could be used to reveal the transitions mentioned. By considering a vibrational model of nodes and edges in a graph/network at a given temperature (stress), we show that the communicability function plays the role of the thermal Green's function of a network of harmonic oscillators. After, we prove analytically the existence of a universal phase transition in the communicability structure of every simple graph. This transition resembles the melting process occurring in solids. For instance, regular-like graphs resembling crystals, melts at lower temperatures and display a sharper transition between connected to disconnected structures than the random spatial graphs, which resemble amorphous solids. Finally, we study computationally this graph melting process in some real-world networks and observe that the rate of melting of graphs changes either as an exponential or as a power-law with the inverse temperature. At the local level we discover that the main driver for node melting is the eigenvector centrality of the corresponding node, particularly when the critical value of the inverse temperature approaches zero. These universal results sheds light on many dynamical diffusive-like processes on networks that present transitions as traffic jams, communication lost or failure cascades. ",Communication Melting in Graphs and Complex Networks
26,966904795763109888,742149925,David Meyer,"['My student Alex Meill will talk about our new paper, ""Pairwise concurrence in cyclically symmetric quantum states"" <LINK> tomorrow (Friday) afternoon at #SQuInT2018 in the session on Frontiers of Quantum Information Theory.']",https://arxiv.org/abs/1802.06877,"We provide an initial characterization of pairwise concurrence in quantum states which are invariant under cyclic permutations of party labeling. We prove that maximal entanglement can be entirely described by adjacent pairs, then give explicit descriptions of those states in specific subsets of 4 and 5 qubit states - X states. We also construct a monogamy bound on shared concurrences in the same subsets in 4 and 5 qubits, finding that above non-maximal entanglement thresholds, no other entanglements are possible. ",Pairwise Concurrence in Cyclically Symmetric Quantum States
27,966775122119012353,893109085,Edouard Grave,"['New fastText word vectors for 157 languages, trained on Wikipedia+Common Crawl: <LINK>. Corresponding paper: <LINK>. #nlproc #fasttext']",https://arxiv.org/abs/1802.06893,"Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models. ",Learning Word Vectors for 157 Languages
28,966711562848407552,2319245521,Luisa Zintgraf,"['I\'m excited to share our new paper ""Ordered Preference Elicitation Strategies for Supporting Multi-Objective Decision Making"" (L Zintgraf, D Roijers, S Linders, C Jonker, A Now√©) in which we use Gaussian Processes to model user preferences. #AAMAS2018 <LINK> <LINK>']",https://arxiv.org/abs/1802.07606,"In multi-objective decision planning and learning, much attention is paid to producing optimal solution sets that contain an optimal policy for every possible user preference profile. We argue that the step that follows, i.e, determining which policy to execute by maximising the user's intrinsic utility function over this (possibly infinite) set, is under-studied. This paper aims to fill this gap. We build on previous work on Gaussian processes and pairwise comparisons for preference modelling, extend it to the multi-objective decision support scenario, and propose new ordered preference elicitation strategies based on ranking and clustering. Our main contribution is an in-depth evaluation of these strategies using computer and human-based experiments. We show that our proposed elicitation strategies outperform the currently used pairwise methods, and found that users prefer ranking most. Our experiments further show that utilising monotonicity information in GPs by using a linear prior mean at the start and virtual comparisons to the nadir and ideal points, increases performance. We demonstrate our decision support framework in a real-world study on traffic regulation, conducted with the city of Amsterdam. ","Ordered Preference Elicitation Strategies for Supporting Multi-Objective
  Decision Making"
29,966638717426372609,1436258888,Mustafa Al-Bassam,['New cryptography paper with @alberto_sonnino @thatBano @GDanezis.\n\nCoconut: Threshold Issuance Selective Disclosure Credentials with Applications to Distributed Ledgers\n<LINK>'],https://arxiv.org/abs/1802.07344,"Coconut is a novel selective disclosure credential scheme supporting distributed threshold issuance, public and private attributes, re-randomization, and multiple unlinkable selective attribute revelations. Coconut integrates with blockchains to ensure confidentiality, authenticity and availability even when a subset of credential issuing authorities are malicious or offline. We implement and evaluate a generic Coconut smart contract library for Chainspace and Ethereum; and present three applications related to anonymous payments, electronic petitions, and distribution of proxies for censorship resistance. Coconut uses short and computationally efficient credentials, and our evaluation shows that most Coconut cryptographic primitives take just a few milliseconds on average, with verification taking the longest time (10 milliseconds). ","Coconut: Threshold Issuance Selective Disclosure Credentials with
  Applications to Distributed Ledgers"
30,966527751774789632,133148364,Devendra Chaplot,"['We just posted a new paper on arxiv on global pose estimation using Neural Graph Optimization, making progress towards end-to-end SLAM! \nArxiv: <LINK>\n(with E. Parisotto, J. Zhang, and @rsalakhu) <LINK>']",https://arxiv.org/abs/1802.06857,"The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment. ",Global Pose Estimation with an Attention-based Recurrent Network
31,966518709832638469,19510090,Julian Togelius,"['New paper in @TCIAIG by @holmgard @Bumblebor @SentientDesigns and me: Automated Playtesting with Procedural Personas through MCTS with Evolved  Heuristics\n<LINK>\nProcedural personas are game-playing agents that play games in the style of different human players <LINK>', 'We start from the assumption that there are many things you may want to do in a game. For example, in the Minidungeons 2 game we use as a testbed you can run to the exit of the level, kill monsters, collect treasure, drink potions etc.', 'So you can express a game-playing style as the relative weight you give to these activities, plus how long your plans are. (The player types are somewhat inspired by the work of @Richard_Bartle and many that followed him, and by work on bounded rationality.)', ""Once we've defined a playing style, we search for utility functions that, when used together with Monte Carlo Tree Search, exhibit this playing style. We do this search using a form of genetic programming."", 'The different personas we create can play the same game in very different ways, as seen in these player traces. How well the agents play the game are somewhat decoupled from what style they play in - we can give the same persona more or less computational resources. https://t.co/1egSRL6NfL', 'One of our main aims with this work is to make playtesting easier. In fact, we are working on releasing the Minidungeons 2 game we developed for Android and iOS, complete with levels that are automatically generated and tested using these procedural personas.', 'But this method could also be used to create demonstrations or tutorials for games using game characters with believable behavior, or maybe for adversaries or sidekicks with interesting behavior.', 'The work was done @NYUGameLab at @nyutandon and @InDigitalGames, building on earlier work at @ITUkbh.']",https://arxiv.org/abs/1802.06881,"This paper describes a method for generative player modeling and its application to the automatic testing of game content using archetypal player models called procedural personas. Theoretically grounded in psychological decision theory, procedural personas are implemented using a variation of Monte Carlo Tree Search (MCTS) where the node selection criteria are developed using evolutionary computation, replacing the standard UCB1 criterion of MCTS. Using these personas we demonstrate how generative player models can be applied to a varied corpus of game levels and demonstrate how different play styles can be enacted in each level. In short, we use artificially intelligent personas to construct synthetic playtesters. The proposed approach could be used as a tool for automatic play testing when human feedback is not readily available or when quick visualization of potential interactions is necessary. Possible applications include interactive tools during game development or procedural content generation systems where many evaluations must be conducted within a short time span. ","Automated Playtesting with Procedural Personas through MCTS with Evolved
  Heuristics"
32,966443887274418177,5850692,Aaron Roth,"['A new paper: <LINK> ""Fairness Through Awareness"" when you don\'t know the fairness metric and need to learn it from feedback. The regulator can\'t quantify the fairness metric, but ""knows unfairness when he sees it"".']",https://arxiv.org/abs/1802.06936,"We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability (arXiv:1104.3913), which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who ""knows unfairness when he sees it"" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on $T$, while obtaining an optimal $O(\sqrt{T})$ regret bound to the best fair policy. ",Online Learning with an Unknown Fairness Metric
33,966426274976747520,721931072,Shimon Whiteson,"['Our new paper: using Fourier analysis to derive policy gradients: we recast the integrals as convolutions, which a Fourier transform turns into multiplications.  The resulting analysis unifies existing policy gradient results. <LINK>', '@KyleCranmer Ref?']",https://arxiv.org/abs/1802.06891,"We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results. ",Fourier Policy Gradients
34,966385996496596997,882257115863187457,Sanjeev Arora,"['Our paper on generalization bounds for deep nets (joint with Rong Ge, Behnam Neyshabur, and Yi Zhang) is here <LINK>  Is uses a new approach based upon direct compression. See also my blog post on <LINK>']",https://arxiv.org/abs/1802.05296,"Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified \textquotedblleft noise stability\textquotedblright properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization. ",Stronger generalization bounds for deep nets via a compression approach
35,966367857922662401,829311242,Ben Farr,"['New paper with @farrwill on the arXiv, mapping exoplanet surfaces from brightness variations.  <LINK> <LINK> <LINK>']",https://arxiv.org/abs/1802.06805,"Future space telescopes will directly image extrasolar planets at visible wavelengths. Time-resolved reflected light from an exoplanet encodes information about atmospheric and surface inhomogeneities. Previous research has shown that the light curve of an exoplanet can be inverted to obtain a low-resolution map of the planet, as well as constraints on its spin orientation. Estimating the uncertainty on 2D albedo maps has so far remained elusive. Here we present exocartographer, a flexible open-source Bayesian framework for solving the exo-cartography inverse problem. The map is parameterized with equal-area HEALPix pixels. For a fiducial map resolution of 192 pixels, a four-parameter Gaussian process describing the spatial scale of albedo variations, and two unknown planetary spin parameters, exocartographer explores a 198-dimensional parameter space. To test the code, we produce a light curve for a cloudless Earth in a face-on orbit with a 90$^\circ$ obliquity. We produce synthetic white light observations of the planet: 5 epochs of observations throughout the planet's orbit, each consisting of 24 hourly observations with a photometric uncertainty of $1\%$ (120 data). We retrieve an albedo map and$-$for the first time$-$its uncertainties, along with spin constraints. The albedo map is recognizably of Earth, with typical uncertainty of $30\%$. The retrieved characteristic length scale is 88$\pm 7 ^\circ$, or 9800 km. The obliquity is recovered with a $1-\sigma$ uncertainty of $0.8^\circ$. Despite the uncertainty in the retrieved albedo map, we robustly identify a high albedo region (the Sahara desert) and a large low-albedo region (the Pacific Ocean). ","exocartographer: A Bayesian Framework for Mapping Exoplanets in
  Reflected Light"
36,966349255203086338,718114061293109248,David Craft,['New machine learning paper!: <LINK>'],https://arxiv.org/abs/1802.05688,"Motivation: In a predictive modeling setting, if sufficient details of the system behavior are known, one can build and use a simulation for making predictions. When sufficient system details are not known, one typically turns to machine learning, which builds a black-box model of the system using a large dataset of input sample features and outputs. We consider a setting which is between these two extremes: some details of the system mechanics are known but not enough for creating simulations that can be used to make high quality predictions. In this context we propose using approximate simulations to build a kernel for use in kernelized machine learning methods, such as support vector machines. The results of multiple simulations (under various uncertainty scenarios) are used to compute similarity measures between every pair of samples: sample pairs are given a high similarity score if they behave similarly under a wide range of simulation parameters. These similarity values, rather than the original high dimensional feature data, are used to build the kernel. Results: We demonstrate and explore the simulation based kernel (SimKern) concept using four synthetic complex systems--three biologically inspired models and one network flow optimization model. We show that, when the number of training samples is small compared to the number of features, the SimKern approach dominates over no-prior-knowledge methods. This approach should be applicable in all disciplines where predictive models are sought and informative yet approximate simulations are available. Availability: The Python SimKern software, the demonstration models (in MATLAB, R), and the datasets are available at this https URL ",Simulation assisted machine learning
37,966241396897255424,2337598033,Geraint F. Lewis,['We‚Äôve got a cool new paper on the arxiv - <LINK> <LINK>'],https://arxiv.org/abs/1802.06810,"We present Magellan/IMACS, Anglo-Australian Telescope/AAOmega+2dF, and Very Large Telescope/GIRAFFE+FLAMES spectroscopy of the CarinaII (Car II) & Carina III (Car III) dwarf galaxy candidates, recently discovered in the Magellanic Satellites Survey (MagLiteS). We identify 18 member stars in Car II, including 2 binaries with variable radial velocities and 2 RR Lyrae stars. The other 14 members have a mean heliocentric velocity $v_{\rm hel} = 477.2 \pm 1.2$ km/s and a velocity dispersion of $\sigma_v = 3.4^{+1.2}_{-0.8}$ km/s. Assuming Car II is in dynamical equilibrium, we derive a total mass within the half-light radius of $1.0^{+0.8}_{-0.4} \times 10^{6} M_\odot$, indicating a mass-to-light ratio of $369^{+309}_{-161} M_\odot/L_\odot$. From equivalent width measurements of the calcium triplet lines of 9 RGB stars, we derive a mean metallicity of [Fe/H] = $-2.44 \pm 0.09$ with dispersion $\sigma_{\rm [Fe/H]} = 0.22 ^{+0.10}_{-0.07}$. Considering both the kinematic and chemical properties, we conclude that Car II is a dark-matter-dominated dwarf galaxy. For Car III, we identify 4 member stars, from which we calculate a systemic velocity of $v_{\rm hel} = 284.6^{+3.4}_{-3.1}$ km/s. The brightest RGB member of Car III has a metallicity of [Fe/H] $= -1.97 \pm 0.12$. Due to the small size of the Car III spectroscopic sample, we cannot conclusively determine its nature. Although these two systems have the smallest known physical separation ($\Delta d\sim10~kpc$) among Local Group satellites, the large difference in their systemic velocities, $\sim200$ km/s, indicates that they are unlikely to be a bound pair. One or both systems are likely associated with the Large Magellanic Cloud (LMC), and may remain LMC satellites today. No statistically significant excess of $\gamma$-rays emission is found at the locations of Car II and Car III in eight years of Fermi-LAT data. ","Ships Passing in the Night: Spectroscopic Analysis of Two Ultra-Faint
  Satellites in the Constellation Carina"
38,966209979941302273,1698006024,niki parmar,"['Our new paper ‚ÄúImage Transformer‚Äù, extends self-attention from the original Transformer to much longer sequences on Image Generation and Super-Resolution tasks. It beats previous SOTA autoregressive models like PixelCNN, PixelRNN. <LINK>', 'Our human evaluation result is 3x better than Pixel Super Resolution. We beat ppl by a significant margin on ImageNet and match PixelCNN++ on Cifar10 image generation.']",https://arxiv.org/abs/1802.05751,"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art. ",Image Transformer
39,966131177865318401,5850692,Aaron Roth,"['Our new paper on repeatedly updating a differentially private computation in the local model, so that your privacy cost degrades only with the number of times the distribution changes, not with the number of times you recompute. <LINK>']",https://arxiv.org/abs/1802.07128,"There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the ""local model"" of differential privacy that these systems use. In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation. ",Local Differential Privacy for Evolving Data
40,966127721360355328,1214528593,Miles Brundage,"['arXiv copy of our new paper, ""The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,"" by 26 authors at 14 institutions: <LINK>\n\nü§ñü§îüßê', '@AntonioGrzt I‚Äôm unfortunately not but CCing colleagues in case they are @HaydnBelfield @CSERCambridge', ""@HeidyKhlaaf Thanks! That'd be awesome. I'm in Oxford most of the time but also drop by London occasionally :)""]",https://arxiv.org/abs/1802.07228,"This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders. ","The Malicious Use of Artificial Intelligence: Forecasting, Prevention,
  and Mitigation"
41,966126604756819968,939498802767044608,Stephan,['New #MachineLearning paper on multi-resolution tensor learning for large-scale spatial data with @yuqirose and @yisongyue! How do you learn basketball shot prediction models quickly? Use multi-resolution gradient descent with gradient entropy control! <LINK>'],https://arxiv.org/abs/1802.06825,"High-dimensional tensor models are notoriously computationally expensive to train. We present a meta-learning algorithm, MMT, that can significantly speed up the process for spatial tensor models. MMT leverages the property that spatial data can be viewed at multiple resolutions, which are related by coarsening and finegraining from one resolution to another. Using this property, MMT learns a tensor model by starting from a coarse resolution and iteratively increasing the model complexity. In order to not ""over-train"" on coarse resolution models, we investigate an information-theoretic fine-graining criterion to decide when to transition into higher-resolution models. We provide both theoretical and empirical evidence for the advantages of this approach. When applied to two real-world large-scale spatial datasets for basketball player and animal behavior modeling, our approach demonstrate 3 key benefits: 1) it efficiently captures higher-order interactions (i.e., tensor latent factors), 2) it is orders of magnitude faster than fixed resolution learning and scales to very fine-grained spatial resolutions, and 3) it reliably yields accurate and interpretable models. ",Multi-resolution Tensor Learning for Large-Scale Spatial Data
42,965972708839260162,876274407995527169,David Madras,"['New paper up with Elliot Creager, Toni Pitassi and Rich Zemel! Addressing what we see as one of the core questions of #FairML - ""Learning Adversarially Fair and Transferable Representations"" <LINK>']",https://arxiv.org/abs/1802.06309,"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning. ",Learning Adversarially Fair and Transferable Representations
43,965966800373669888,3209362451,Burton Lab,['New paper from the Burton Lab on Leidenfrost drop oscillations posted to the arxiv:\n<LINK>'],https://arxiv.org/abs/1802.06136,"In the Leidenfrost effect, a thin layer of evaporated vapor forms between a liquid and a hot solid. The complex interactions between the solid, liquid, and vapor phases can lead to rich dynamics even in a single Leidenfrost drop. Here we investigate the self-organized oscillations of Leidenfrost drops that are excited by a constant flow of evaporated vapor beneath the drop. We show that for small Leidenfrost drops, the frequency of a recently reported ""breathing mode"" can be explained by a simple balance of gravitational and surface tension forces. For large Leidenfrost drops, azimuthal star-shaped oscillations are observed. Our previous work showed how the coupling between the rapid evaporated vapor flow and the vapor-liquid interface excites the star oscillations (Ma \textit{et al., Phys. Rev. Fluids}, 2, 2017, 031602). In our experiments, star-shaped oscillation modes of $n=2$ to 13 are observed in different liquids, the number of observed modes depends sensitively on the liquid viscosity. Here we expand on this work by directly comparing the oscillations with theoretical predictions, and show how the oscillations are initiated by a parametric forcing mechanism through pressure oscillations in the vapor layer. The pressure oscillations are driven by the capillary waves of a characteristic wavelength beneath the drop. These capillary waves can be generated by a large shear stress at the liquid-vapor interface due to the rapid flow of evaporated vapor. We also explore potential effects of thermal convection in the liquid. Although the measured Rayleigh number is significantly larger than the critical value, the frequency (wavelength) of the oscillations depends only on the capillary length of the liquid, and is independent of the drop radius and substrate temperature. Thus convection seems to play a minor role in Leidenfrost drop oscillations, which are mostly hydrodynamic in origin. ",Self-organized oscillations of Leidenfrost drops
44,965957765263974402,40299444,Alexey Petrov,"['New paper! We argue that large gluon luminosity at he LHC allows one to probe lepton-flavor violating gluonic operators.\n<LINK> <LINK>', '@PKoppenburg What did I do? :-)', '@PKoppenburg Oh, you mean different colors for different authors? Good point.']",https://arxiv.org/abs/1802.06082,"We examine the charged lepton flavor violating process $gg \rightarrow \mu^\pm \tau^\mp$ at the $\sqrt{s} = 13$ TeV LHC. Operators generating this process can be induced by new physics (NP) at dimension 8. Despite the power suppression associated with dimension 8 operators, we show that the LHC's large gluon luminosity makes it possible to probe this channel. For an integrated luminosity of 100 fb$^{-1}$ at the LHC, we predict a constraint on the NP scale $\Lambda \gtrsim 3$ TeV. In addition, we point out that such operators can be induced through top quark loops in models that generate dimension 6 operators of the form $\overline{t} t \, \mu \tau$. We find that the NP scale of these dimension 6 operators can be constrained to be $\Lambda \gtrsim 3.4-4.1$ TeV with 100 fb$^{-1}$ of data. ",Studies of Lepton Flavor Violation at the LHC
45,965928052432744449,232642215,Jukka Suomela,"['A new manuscript ""Distributed Recoloring"", with Marthe Bonamy, Paul Ouvrard, Mika√´l Rabie, and Jara Uitto: <LINK>\n\nAnd here is a Github repository with the computational proof of one lemma in our paper: <LINK>', '@OmegaPolice Added MIT license.']",https://arxiv.org/abs/1802.06742,"Given two colorings of a graph, we consider the following problem: can we recolor the graph from one coloring to the other through a series of elementary changes, such that the graph is properly colored after each step? We introduce the notion of distributed recoloring: The input graph represents a network of computers that needs to be recolored. Initially, each node is aware of its own input color and target color. The nodes can exchange messages with each other, and eventually each node has to stop and output its own recoloring schedule, indicating when and how the node changes its color. The recoloring schedules have to be globally consistent so that the graph remains properly colored at each point, and we require that adjacent nodes do not change their colors simultaneously. We are interested in the following questions: How many communication rounds are needed (in the LOCAL model of distributed computing) to find a recoloring schedule? What is the length of the recoloring schedule? And how does the picture change if we can use extra colors to make recoloring easier? The main contributions of this work are related to distributed recoloring with one extra color in the following graph classes: trees, $3$-regular graphs, and toroidal grids. ",Distributed Recoloring
46,965781280939036672,3199605543,Afonso S. Bandeira,['New paper on the optimization landscape of Neural Networks: <LINK>'],http://arxiv.org/abs/1802.06384,"Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may explain such success. In this paper, we address this phenomenon by studying a key topological property of the loss: the presence or absence of spurious valleys, defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of two-layer neural networks defined by smooth (but generally non-linear) activation functions, we identify a notion of intrinsic dimension and show that it provides necessary and sufficient conditions for the absence of spurious valleys. More concretely, finite intrinsic dimension guarantees that for sufficiently overparametrised models no spurious valleys exist, independently of the data distribution. Conversely, infinite intrinsic dimension implies that spurious valleys do exist for certain data distributions, independently of model overparametrisation. Besides these positive and negative results, we show that, although spurious valleys may exist in general, they are confined to low risk levels and avoided with high probability on overparametrised models. ",Spurious Valleys in Two-layer Neural Network Optimization Landscapes
47,965769967143735296,718779366134648834,Wei Ji Ma,"['New from the lab: theoretical paper on how to optimally allocate attention given a ""local"" task (such as a Posner cueing task) and a set of probing probabilities. Turns out that allocating attention in proportion to the probing probabilities is suboptimal! <LINK>']",https://arxiv.org/abs/1802.06456,"In natural perception, different items (objects) in a scene are rarely equally relevant to the observer. The brain improves performance by directing attention to the most relevant items, for example the ones most likely to be probed. For a general set of probing probabilities, it is not known how attentional resources should be allocated to maximize performance. Here, we investigate the optimal strategy for allocating a fixed resource budget E among N items when on each trial, only one item gets probed. We develop an efficient algorithm that, for any concave utility function, reduces the N-dimensional problem to a set of N one-dimensional problems that the brain could plausibly solve. We find that the intuitive strategy of allocating resource in proportion to the probing probabilities is in general not optimal. In particular, in some tasks, if resource is low, the optimal strategy involves allocating zero resource to items with a nonzero probability of being probed. Our work opens the door to normatively guided studies of attentional allocation. ","Optimal allocation of attentional resource to multiple items with
  unequal relevance"
48,965672474913443840,2999702157,Anton Ilderton,['New paper on mode truncations in #QFT  the @arxiv from @PlymUni researcher @StrongFieldQED and collaborators! <LINK>'],https://arxiv.org/abs/1802.05933,"Truncating quantum field theories to a dominant mode offers a non-perturbative approach to their solution. We consider here the interaction of charged scalar matter with a single mode of the electromagnetic field. The implied breaking of explicit Lorentz invariance prompts us to compare instant-form quantisation and front-form, with the latter yielding significant simplifications when light-front zero modes are included. Using these field theory results we reassess the validity of existing first-quantised approaches to depletion effects in strong laser fields, and propose an alternative interpretation based on the dressing approach to QED and its infra-red structure. ",Mode truncations and scattering in strong fields
49,964462544671989760,3343151685,Pierre-Alex Mattei,"['Is ML well-posed for deep generative models? Can VAEs be used for missing data imputation? Our take on these questions in our new paper with @jesfrellsen ""Leveraging the Exact Likelihood of Deep Latent Variables Models"" !\n<LINK> <LINK>']",https://arxiv.org/abs/1802.04826,"Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a deep latent variable model. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs. ",Leveraging the Exact Likelihood of Deep Latent Variable Models
50,964406473500446721,797888987675365377,Tom Rainforth,"['Check out our new paper ""Tighter Variational Bounds are not Necessarily Better"" <LINK>.  We show that using IWAE damages learning of the inference network and introduce three new algorithms with superior performance for both the generative and inference networks']",https://arxiv.org/abs/1802.04537,"We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted auto-encoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks. ",Tighter Variational Bounds are Not Necessarily Better
51,964398237640638467,820031736914513925,Fabrizio Leisen,['New paper with @cv60villa and Luca Rossini @UniKentStats <LINK>'],https://arxiv.org/abs/1802.05292,"Two-piece location-scale models are used for modeling data presenting departures from symmetry. In this paper, we propose an objective Bayesian methodology for the tail parameter of two particular distributions of the above family: the skewed exponential power distribution and the skewed generalised logistic distribution. We apply the proposed objective approach to time series models and linear regression models where the error terms follow the distributions object of study. The performance of the proposed approach is illustrated through simulation experiments and real data analysis. The methodology yields improvements in density forecasts, as shown by the analysis we carry out on the electricity prices in Nordpool markets. ","Loss-based approach to two-piece location-scale distributions with
  applications to dependent data"
52,964317448693432325,637108210,Matthew Peters,"['Our paper ""Deep contextualized word representations"" is now on Arxiv. ELMo representations from pre-trained language models set new SOTA for 6 diverse NLP tasks, SQuAD, SNLI, SRL, coref, NER, sentiment. Paper: <LINK> Code: <LINK> <LINK>']",https://arxiv.org/abs/1802.05365,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. ",Deep contextualized word representations
53,964250481165045760,738769492122214400,Johannes Lischner,['Our new paper on plasmonic photocatalysis is on the arxiv now (<LINK>). We study how material properties influence the conversion of light into hot electrons in metallic nanoparticles. #SolarEnergy #nanotechnology <LINK>'],https://arxiv.org/abs/1802.05096,"Harnessing hot electrons and holes resulting from the decay of localized surface plasmons in nanomaterials has recently led to new devices for photovoltaics, photocatalysis and optoelectronics. Properties of hot carriers are highly tunable and in this work we investigate their dependence on the material, size and environment of spherical metallic nanoparticles. In particular, we carry out theoretical calculations of hot carrier generation rates and energy distributions for six different plasmonic materials (Na, K, Al, Cu, Ag and Au). The plasmon decay into hot electron-hole pairs is described via Fermi's Golden Rule using the quasistatic approximation for optical properties and a spherical well potential for the electronic structure. We present results for nanoparticles with diameters up to 40 nm, which are embedded in different dielectric media. We find that small nanoparticles with diameters of 16 nm or less in media with large dielectric constants produce most hot carriers. Among the different materials, Na, K and Au generate most hot carriers. We also investigate hot-carrier induced water splitting and find that simple-metal nanoparticles are useful for initiating the hydrogen evolution reaction, while transition-metal nanoparticles produce dominantly holes for the oxygen evolution reaction. ","Material, size and environment dependence of plasmon-induced hot
  carriers in metallic nanoparticles"
54,964215776134746113,51257255,Natasha Jaques,"['A new, short paper I wrote on the importance of learning from implicit social feedback as an intrinsic motivator for deep learning models: <LINK>', '@jeremymlane Thanks dude!']",https://arxiv.org/abs/1802.04877,"In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, by optimizing the model to produce sketches that it predicts will lead to more positive facial expressions. We show in multiple independent evaluations that the model trained with facial feedback produced sketches that are more highly rated, and induce significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model. ","Learning via social awareness: Improving a deep generative sketching
  model with facial feedback"
55,964192382328610816,3187990776,Tianqi Chen,"['With the emergence of new hardware, what are the challenges for deep learning systems? We try to answer it in our TVM paper <LINK>  to provide reusable optimization stack for DLSys on CPU/GPU, and accelerators. I will give an oral presentation about it at #SysML']",https://arxiv.org/abs/1802.04799,"There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies. ",TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
56,963964560138031104,2956121356,Russ Salakhutdinov,"['New preprint on Characterizing the Capacity of Neural Networks using Algebraic Topology: Using persistent homology to characterize the capacity of neural architectures in their ability to generalize, with @wgussml \nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1802.04443,"The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks. ","On Characterizing the Capacity of Neural Networks using Algebraic
  Topology"
57,963724924132216832,859611673,Gareth Baxter,['We attack the thorny problem of what is the quickest way to destroy interdependent networks in our new paper <LINK>'],https://arxiv.org/abs/1802.03992,"The giant mutually connected component (GMCC) of an interdependent or multiplex network collapses with a discontinuous hybrid transition under random damage to the network. If the nodes to be damaged are selected in a targeted way, the collapse of the GMCC may occur significantly sooner. Finding the minimal damage set which destroys the largest mutually connected component of a given interdependent network is a computationally prohibitive simultaneous optimization problem. We introduce a simple heuristic strategy -- Effective Multiplex Degree -- for targeted attack on interdependent networks that leverages the indirect damage inherent in multiplex networks to achieve a damage set smaller than that found by any other non computationally intensive algorithm. We show that the intuition from single layer networks that decycling (damage of the $2$-core) is the most effective way to destroy the giant component, does not carry over to interdependent networks, and in fact such approaches are worse than simply removing the highest degree nodes. ",Targeted Damage to Interdependent Networks
58,963704037525917696,882303076505456642,Timon Emken,"['Can Earth-based detectors observe strongly interacting #DarkMatter? At what point do the Earth crust and atmosphere shield off DM and leave the underground and surface detectors blind? If these questions keep you up at night, check out our new paper:  <LINK> <LINK>', 'Also check out the recent paper by @DanHooperAstro and Samuel D. McDermott, who used analytic methods to shed light on these questions. Additionally they also focus on DM-cosmic ray interactions: https://t.co/NHcyWQxzSd', 'The #MonteCarlo #simulation #code DaMaSCUS-CRUST used in our paper is available at https://t.co/Zo5H7bXCKz. #OpenScience', '@JonathanHMDavis We called it high altitude experiments. But yes!']",https://arxiv.org/abs/1802.04764,"Above a critical dark matter-nucleus scattering cross section any terrestrial direct detection experiment loses sensitivity to dark matter, since the Earth crust, atmosphere, and potential shielding layers start to block off the dark matter particles. This critical cross section is commonly determined by describing the average energy loss of the dark matter particles analytically. However, this treatment overestimates the stopping power of the Earth crust. Therefore the obtained bounds should be considered as conservative. We perform Monte Carlo simulations to determine the precise value of the critical cross section for various direct detection experiments and compare them to other dark matter constraints in the low mass regime. In this region we find parameter space where typical underground and surface detectors are completely blind to dark matter. This ""hole"" in the parameter space can hardly be closed with an increase in the detector exposure. Dedicated surface or high-altitude experiments may be the only way to directly probe this part of the parameter space. ","How blind are underground and surface detectors to strongly interacting
  Dark Matter?"
59,963588291642654720,22148802,Leo C. Stein ü¶Å,"['New preprint, with Davide Gerosa and Fran√ßois H√©bert! Using numerical relativity data to directly model the kicks that black holes get when they merge. Read the paper at <LINK> <LINK>', ""@astrocrash When it's as easy as 'import surrkick', how could you pass it over?\nBTW, I'm guessing most astronomers just want the final remnant properties, and don't care about the kick profile. In that case, a custom interpolant could be built that would be way faster! On the TODO list.""]",https://arxiv.org/abs/1802.04276,"Binary black holes radiate linear momentum in gravitational waves as they merge. Recoils imparted to the black-hole remnant can reach thousands of km/s, thus ejecting black holes from their host galaxies. We exploit recent advances in gravitational waveform modeling to quickly and reliably extract recoils imparted to generic, precessing, black hole binaries. Our procedure uses a numerical-relativity surrogate model to obtain the gravitational waveform given a set of binary parameters, then from this waveform we directly integrate the gravitational-wave linear momentum flux. This entirely bypasses the need of fitting formulae which are typically used to model black-hole recoils in astrophysical contexts. We provide a thorough exploration of the black-hole kick phenomenology in the parameter space, summarizing and extending previous numerical results on the topic. Our extraction procedure is made publicly available as a module for the Python programming language named SURRKICK. Kick evaluations take ~0.1s on a standard off-the-shelf machine, thus making our code ideal to be ported to large-scale astrophysical studies. ",Black-hole kicks from numerical-relativity surrogate models
60,963445184271998977,131012004,Dr. John Barentine FRAS,"[""Excited to announce that the paper my collaborators and I wrote on our measurements and modeling of skyglow changes due to @cityoftucson's new LED lighting system has been accepted for publication: <LINK> #LightPollution <LINK>"", 'Here are the tl;dr highlights of our work. 1. Modeling was used to predict skyglow changes resulting from an LED conversion in the city where I live, which consisted of the installation of ~20,000 new 3000K white LED roadway lights and a ~37% overall light emission reduction.', '2. Several kinds of skyglow measurements were obtained in/around tucson during epochs approximately bracketing the conversion effort (2014 and 2017, for a conversion performed in 2016-17). We looked at different indicators of visual skyglow and made radiometric measurements.', '3. Results of modeling and observations were compared against satellite ""night lights"" data from the same epochs as a way to gauge changes in both the upward-escaping radiance as well as the night sky luminance characterizing the downward-scattered component.', '4. Some evidence supports the hypothesis that reducing the total light emission of the new street lighting system (even though more light was shifted to shorter wavelengths) resulted in a concomitant decrease in both upward- and downward-directed light after the LED conversion.', '5. A clear need exists for further such studies of city-scale changes to municipal lighting systems, especially with respect to observation campaigns based on permanently installed skyglow monitoring equipment.', 'Have to be super careful about how we interpret these results and not read too much into them, but the broad suggestions of our models and data are encouraging. For what the are, they basically validate the expectation that lowering the light emissions of a city reduces skyglow.', ""Next step: Wait a couple of years for traffic safety &amp; uniform crime data to become available to try to determine whether reduced light levels had any influence on public safety/security. (Hypothesis: Probably not. Public isn't even broadly aware new lighting levels are lower!)"", 'The bottom-line result is that we found no evidence that the switch from a predominantly HPS-based municipal lighting system to white LED resulted in *more* skyglow, contrary to some predictions. The key seems to be reducing total light emission levels.']",https://arxiv.org/abs/1802.03474,"The transition from earlier lighting technologies to white light-emitting diodes (LEDs) is a significant change in the use of artificial light at night. LEDs emit considerably more short-wavelength light into the environment than earlier technologies on a per-lumen basis. Radiative transfer models predict increased skyglow over cities transitioning to LED unless the total lumen output of new lighting systems is reduced. The City of Tucson, Arizona (U.S.), recently converted its municipal street lighting system from a mixture of fully shielded high- and low-pressure sodium (HPS/LPS) luminaires to fully shielded 3000 K white LED luminaires. The lighting design intended to minimize increases to skyglow in order to protect the sites of nearby astronomical observatories without compromising public safety. This involved the migration of over 445 million fully shielded HPS/LPS lumens to roughly 142 million fully shielded 3000 K white LED lumens and an expected concomitant reduction in the amount of visual skyglow over Tucson. SkyGlow Simulator models predict skyglow decreases on the order of 10-20% depending on whether fully shielded or partly shielded lights are in use. We tested this prediction using visual night sky brightness estimates and luminance-calibrated, panchromatic all-sky imagery at 15 locations in and near the city. Data were obtained in 2014, before the LED conversion began, and in mid-2017 after approximately 95% of $\sim$18,000 luminaires was converted. Skyglow differed marginally, and in all cases with valid data changed by $<{\pm}$20%. Over the same period, the city's upward-directed optical radiance detected from Earth orbit decreased by approximately 7%. While these results are not conclusive, they suggest that LED conversions paired with dimming can reduce skyglow over cities. ","Skyglow Changes Over Tucson, Arizona, Resulting From A Municipal LED
  Street Lighting Conversion"
61,963347790335234050,135150782,Michele Ginolfi,"['My new paper ""Extended and broad LyŒ± emission around a BAL quasar at z~5"", accepted by MNRAS, just appeared on the @arxiv! \nOur Lya Nebula (in the pic below), observed with the @ESO VLT-MUSE telescope, shows some interesting properties..find out more at <LINK> <LINK>']",https://arxiv.org/abs/1802.03400,"In this work we report deep MUSE observations of a Broad Absorption Line (BAL) quasar at z ~ 5, revealing a Lyman alpha nebula with a maximum projected linear size of ~ 60 kpc around the quasar (down to our 2-sigma SB limit per layer of ~ 9e-19 erg/s/cm^2/arcsec^2 for a 1 arcsec^2 aperture). After correcting for the cosmological surface brightness dimming, we find that our nebula, at z ~ 5, has an intrinsically less extended Lyman alpha emission than nebulae at lower redshift. However, such a discrepancy is greatly reduced when referring to comoving distances, which take into account the cosmological growth of dark matter (DM) haloes, suggesting a positive correlation between the size of Lyman alpha nebulae and the sizes of DM haloes/structures around quasars. Differently from the typical nebulae around radio-quiet non-BAL quasars, in the inner regions (~ 10 kpc) of the circumgalactic medium (CGM) of our source, the velocity dispersion of the Lyman alpha emission is very high (FWHM > 1000 km/s), suggesting that in our case we may be probing outflowing material associated with the quasar. ",Extended and broad Lyman alpha emission around a BAL quasar at z~5
62,962860120404213760,795877354266456064,KoheiKamadaPhys,"['<LINK>\nMy new paper appeared! I constructed a scenario where the GUT baryogenesis generates chiral asymmetry that produces hypermagnetic fields, which reproduce baryon asymmetry at electroweak symmetry breaking.', 'Usually GUT baryogenesis suffered from the washout by sphalerons, but in this scenario, this problem is avoided. Just asymmetry in the right-handed electrons are needed. No other artificial tricks are needed.']",https://arxiv.org/abs/1802.03055,"It has been considered that baryogenesis models without a generation of $B$-$L$ asymmetry such as the GUT baryogenesis do not work since the asymmetry is washed out by the electroweak sphalerons. Here, we point out that helical hypermagnetic fields can be generated through the chiral magnetic effect with a chiral asymmetry generated in such baryogenesis models. The helical hypermagnetic fields then produce baryon asymmetry mainly at the electroweak symmetry breaking, which remains until today. Therefore, the baryogenesis models without $B$-$L$ asymmetry can still be the origin of the present baryon asymmetry. In particular, if it can produce chiral asymmetry mainly carried by right-handed electrons of order of $10^{-3}$ in terms of the chemical potential to temperature ratio, the resultant present-day baryon asymmetry can be consistent with our Universe, although simple realizations of the GUT baryogenesis are hard to satisfy the condition. We also argue the way to overcome the difficulty in the GUT baryogenesis. The intergalactic magnetic fields with $B_0\sim 10^{-16 \sim 17}$ G and $\lambda_0 \sim 10^{-2\sim3}$ pc are the smoking gun of the baryogenesis scenario as discussed before. ","Return of the grand unified theory baryogenesis: Source of helical
  hypermagnetic fields for the baryon asymmetry of the universe"
63,962421907298627584,333069274,sukanya chakrabarti,['our new paper on the first detection of HI in a strong spiral lens: <LINK>'],https://arxiv.org/abs/1802.01588,"We report HI observations of eight spiral galaxies that are strongly lensing background sources. Our targets were selected from the Sloan WFC (Wide Field Camera) Edge-on Late-type Lens Survey (SWELLS) using the Arecibo, Karl G. Jansky Very Large Array, and Green Bank telescopes. We securely detect J1703+2451 at z=0.063 with a signal-to-noise of 6.7 and W50=79+/-13 km/s, obtaining the first detection of HI emission in a strong spiral lens. We measure a mass of M(HI)= 1.77+/-0.06(+0.35/-0.75) x 10^9 M_(sol) for this source. We find that this lens is a normal spiral, with observable properties that are fairly typical of spiral galaxies. For three other sources we did not secure a detection; however, we are able to place strong constraints on the HI masses of those galaxies. The observations for four of our sources were rendered unusable due to strong radio frequency interference. ","The First Detection of Neutral Hydrogen in Emission in a Strong Spiral
  Lens"
64,961969447195041794,749437811015704577,Ted Mackereth,"['Need orbit parameters? Can‚Äôt wait for orbit integration? Check out my new paper with @jobovy on the arXiv today for a handy new method <LINK>', '@jobovy Fully implemented in galpy, tutorial here: https://t.co/JNDPv7POw6']",https://arxiv.org/abs/1802.02592,"Orbital parameters, such as eccentricity and maximum vertical excursion, of stars in the Milky Way are an important tool for understanding its dynamics and evolution, but calculation of such parameters usually relies on computationally-expensive numerical orbit integration. We present and test a fast method for estimating these parameters using an application of the St\""ackel fudge, used previously for the estimation of action-angle variables. We show that the method is highly accurate, to a level of $<1\%$ in eccentricity, over a large range of relevant orbits and in different Milky Way-like potentials, and demonstrate its validity by estimating the eccentricity distribution of the RAVE-TGAS data set and comparing it to that from orbit integration. Using the method, the orbital characteristics of the $\sim 7$ million $\textit{Gaia}$ DR2 stars with radial velocity measurements are computed with Monte Carlo sampled errors in $\sim 116$ hours of parallelised cpu time, at a speed that we estimate to be $\sim 3$ to $4$ orders of magnitude faster than using numerical orbit integration. We demonstrate using this catalogue that $\textit{Gaia}$ DR2 samples a large range of orbits in the solar vicinity, down to those with $r_\mathrm{peri} \lesssim 2.5$ kpc, and out to $r_\mathrm{ap} \gtrsim 13$ kpc. We also show that many of the features present in orbital parameter space have a low mean $z_\mathrm{max}$, suggesting that they likely result from disk dynamical effects. ",Fast estimation of orbital parameters in Milky-Way-like potentials
65,961798822606274560,2332368062,Sam Scarpino #BLM,"['""These results both characterize key aspects of neonatal care in the US and may apply more broadly to the role of hierarchical forces in organizing complex adaptive systems."" Our new paper w/ the @VTOxfordNetwork, led by @munikShrestha, is online <LINK> <LINK>', '@xl772 @VTOxfordNetwork @munikShrestha That polygon is headed off to Hawaii. We also used data from Alaska and Hawaii, but didn‚Äôt include them in the plot. There were also lots of rare, long-distance transfers (e.g., maybe the mother was traveling) but we can‚Äôt show them for privacy.', '@AnisabelBento @VTOxfordNetwork @munikShrestha Thanks!']",https://arxiv.org/abs/1802.02855,"Very low birth weight (VLBW) infants require specialized care in neonatal intensive care units. In the United States (U.S.), such infants frequently are transferred between hospitals. Although these neonatal transfer networks are important, both economically and for infant morbidity and mortality, the national-level pattern of neonatal transfers is largely unknown. Using data from Vermont Oxford Network on 44,753 births, 2,122 hospitals, and 9,722 inter-hospital infant transfers from 2015, we performed the largest analysis to date on the inter-hospital transfer network for VLBW infants in the U.S. We find that transfers are organized around regional communities, but that despite being largely within state boundaries, most communities often contain at least two hospitals in different states. To classify the structural variation in transfer pattern amongst these communities, we applied a spectral measure for regionalization and found an association between a community's degree of regionalization and their infant transfer rate, which was not utilized in detecting communities. We also demonstrate that the established measures of network centrality and hierarchy, e.g., the community-wide entropy in PageRank or betweenness centrality and number of distinct `layers' within a community, correlate weakly with our regionalization index and were not significantly associated with metrics on infant transfer rate. Our results suggest that the regionalization index captures novel information about the structural properties of VLBW infant transfer networks, have the practical implication of characterizing neonatal care in the U.S., and may apply more broadly to the role of centralizing forces in organizing complex adaptive systems. ","The Interhospital Transfer Network for Very Low Birth Weight Infants in
  the United States"
66,961535238416592896,59413748,Reuben Binns,"['Power in the digital economy is about consumer surveillance; tracking behaviour across multiple sites and apps.\n\nRegulators are worried about the concentration of tracking power, but how can they measure it?\n\nNew paper (w/ @junszhao @emax @Nigel_Shadbolt) <LINK>']",https://arxiv.org/abs/1802.02507,"Third-party networks collect vast amounts of data about users via web sites and mobile applications. Consolidations among tracker companies can significantly increase their individual tracking capabilities, prompting scrutiny by competition regulators. Traditional measures of market share, based on revenue or sales, fail to represent the tracking capability of a tracker, especially if it spans both web and mobile. This paper proposes a new approach to measure the concentration of tracking capability, based on the reach of a tracker on popular websites and apps. Our results reveal that tracker prominence and parent-subsidiary relationships have significant impact on accurately measuring concentration. ",Measuring third party tracker power across web and mobile
67,961518287367688192,3091314052,Pawel Biernacki,"['New paper is out: <LINK>\n\nWe test particle methods (traditional SPH, pressure SPH), hybrid methods (meshless finite mass) and grid (AMR) in an idealised yet realistic cluster setup. <LINK>', 'We inject hot bubble and study how it buoyantly rises in the environment without and with turbulence. If Kelvin-Helmholtz instability is well captured, then it is disrupted as expected. Energy is then deposited as kinetic energy. SPH variants do it v. poorly https://t.co/qSNcsAWcGy', 'As soon as the paper is accepted we intend to make the initial conditions fully public and encourage others to try their favourite code in this setup, which we hope to become a standard, 3D, astrophysical test for hydro codes.', '@franco_vazza It is of order of what is feasible for these masses of clusters. Here RAMSES max res is about 1.5 kpc, Rhapsody-G had about 2 kpc.', '@franco_vazza For full cosmo boxes‚Ä¶ I think only Illustris TNG comes close', '@CosmoCa3sar @franco_vazza Yup. I was thinking more of a spatial resolution (we compared mostly to Illustris TNG)']",https://arxiv.org/abs/1802.02177,"While feedback from Active Galactic Nuclei (AGN) is an important heating source in the centre of galaxy clusters, it is still unclear how the feedback energy is injected into the intracluster medium (ICM) and what role different numerical approaches play. Here, we compare four hydrodynamical schemes in idealized simulations of a rising bubble inflated by AGN feedback in a hot stratified ICM: (traditional) smoothed particle hydrodynamics (TSPH), a pressure flavour of SPH (PSPH), a meshless finite mass (MFM) scheme, as well as an Eulerian code with adaptive mesh refinement. In the absence of magnetic fields, the bubble is Kelvin-Helmholtz unstable on short enough time scales to dissolve it fully in the ICM, which is captured by MFM and RAMSES simulations, while in the TSPH simulation the bubble survives. When the ICM is turbulent, mixing of the bubble with the ICM is accelerated. This occurs if the numerical scheme can capture the instabilities well. The differences in the evolution of the bubble has a surprisingly small influence on the thermal structure of the ICM. However, in the simulations with MFM and RAMSES the bubble disruption leads to turbulent stirring of the ICM which is suppressed in SPH. In the latter the thermal energy remains trapped in the bubble and is transported to large radii. We discuss if the choice of hydrodynamical schemes can lead to systematic differences in the outcomes of cosmological simulations. ","Physical and numerical stability and instability of AGN bubbles in a hot
  intracluster medium"
68,961188824989749248,772809603046334464,Jonathan Mackey,['New paper about 30 Doradus in the Large Magellanic Cloud:\n<LINK>'],https://arxiv.org/abs/1802.01597,"We introduce VLT-MUSE observations of the central 2$'\times2'$ (30$\times$30 pc) of the Tarantula Nebula in the Large Magellanic Cloud. The observations provide an unprecedented spectroscopic census of the massive stars and ionised gas in the vicinity of R136, the young, dense star cluster located in NGC 2070, at the heart of the richest star-forming region in the Local Group. Spectrophotometry and radial-velocity estimates of the nebular gas (superimposed on the stellar spectra) are provided for 2255 point sources extracted from the MUSE datacubes, and we present estimates of stellar radial velocities for 270 early-type stars (finding an average systemic velocity of 271$\pm$41 km/s). We present an extinction map constructed from the nebular Balmer lines, with electron densities and temperatures estimated from intensity ratios of the [SII], [NII], and [SIII] lines. The interstellar medium, as traced by H$\alpha$ and [NII] $\lambda$6583, provides new insights in regions where stars are probably forming. The gas kinematics are complex, but with a clear bi-modal, blue- and red-shifted distribution compared to the systemic velocity of the gas centred on R136. Interesting point-like sources are also seen in the eastern cavity, western shell, and around R136; these might be related to phenomena such as runaway stars, jets, formation of new stars, or the interaction of the gas with the population of Wolf--Rayet stars. Closer inspection of the core reveals red-shifted material surrounding the strongest X-ray sources, although we are unable to investigate the kinematics in detail as the stars are spatially unresolved in the MUSE data. Further papers in this series will discuss the detailed stellar content of NGC 2070 and its integrated stellar and nebular properties. ","Mapping the core of the Tarantula Nebula with VLT-MUSE: I. Spectral and
  nebular content around R136"
69,961061898916544513,101980926,Masahito Yamazaki,"['New paper ""Gauge Theory and Integrability, II"", co-authored with Kevin and Ed\n<LINK>']",http://arxiv.org/abs/1802.01579,"Starting with a four-dimensional gauge theory approach to rational, elliptic, and trigonometric solutions of the Yang-Baxter equation, we determine the corresponding quantum group deformations to all orders in $\hbar$ by deducing their RTT presentations. The arguments we give are a mix of familiar ones with reasoning that is more transparent from the four-dimensional gauge theory point of view. The arguments apply most directly for $\mathfrak{gl}_N$ and can be extended to all simple Lie algebras other than $\mathfrak{e}_8$ by taking into account the self-duality of some representations, the framing anomaly for Wilson operators, and the existence of quantum vertices at which several Wilson operators can end. ","Gauge Theory and Integrability, II"
70,960887584208629762,632665722,Vivek Khetan,"[""My new paper on 'Correlation and Prediction of Evaluation Metrics in Information Retrieval' system is up on arXiv.\n\n\xa0<LINK> <LINK>""]",https://arxiv.org/abs/1802.00323,"Because researchers typically do not have the time or space to present more than a few evaluation metrics in any published study, it can be difficult to assess relative effectiveness of prior methods for unreported metrics when baselining a new method or conducting a systematic meta-review. While sharing of study data would help alleviate this, recent attempts to encourage consistent sharing have been largely unsuccessful. Instead, we propose to enable relative comparisons with prior work across arbitrary metrics by predicting unreported metrics given one or more reported metrics. In addition, we further investigate prediction of high-cost evaluation measures using low-cost measures as a potential strategy for reducing evaluation cost. We begin by assessing the correlation between 23 IR metrics using 8 TREC test collections. Measuring prediction error wrt. R-square and Kendall's tau, we show that accurate prediction of MAP, P@10, and RBP can be achieved using only 2-3 other metrics. With regard to lowering evaluation cost, we show that RBP(p=0.95) can be predicted with high accuracy using measures with only evaluation depth of 30. Taken together, our findings provide a valuable proof-of-concept which we expect to spur follow-on work by others in proposing more sophisticated models for metric prediction. ","Correlation and Prediction of Evaluation Metrics in Information
  Retrieval"
71,960872925560823810,20309837,Michael Veale,"['We talk lots about theoretical algorithmic fairness+accountability, but can we learn from those grappling w/ these issues on the ground? 27 folks in police, justice, child welfare ML shared their experiences in our new #chi2018 paper <LINK> #fatml @emax @RDBinns <LINK>']",https://arxiv.org/abs/1802.01029,"Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions---like taxation, justice, and child protection---are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning---absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications. ","Fairness and Accountability Design Needs for Algorithmic Support in
  High-Stakes Public Sector Decision-Making"
72,959373923082211328,40639812,Colin Cotter,['New paper with @MPECDT student Thomas Gibson (plus @_wence and David Ham) on code-generation for hybridizable FEM problems. <LINK> Now we are applying this to semi-implicit solvers for dynamical cores in weather models.'],https://arxiv.org/abs/1802.00303,"Within the finite element community, discontinuous Galerkin (DG) and mixed finite element methods have become increasingly popular in simulating geophysical flows. However, robust and efficient solvers for the resulting saddle-point and elliptic systems arising from these discretizations continue to be an on-going challenge. One possible approach for addressing this issue is to employ a method known as hybridization, where the discrete equations are transformed such that classic static condensation and local post-processing methods can be employed. However, it is challenging to implement hybridization as performant parallel code within complex models, whilst maintaining separation of concerns between applications scientists and software experts. In this paper, we introduce a domain-specific abstraction within the Firedrake finite element library that permits the rapid execution of these hybridization techniques within a code-generating framework. The resulting framework composes naturally with Firedrake's solver environment, allowing for the implementation of hybridization and static condensation as runtime-configurable preconditioners via the Python interface to PETSc, petsc4py. We provide examples derived from second order elliptic problems and geophysical fluid dynamics. In addition, we demonstrate that hybridization shows great promise for improving the performance of solvers for mixed finite element discretizations of equations related to large-scale geophysical flows. ","Slate: extending Firedrake's domain-specific abstraction to hybridized
  solvers for geoscience and beyond"
73,959319145736167425,216729597,Marcel S. Pawlowski,"['Here\'s the arXiv version of our new paper: ""A whirling plane of satellite galaxies around Centaurus A challenges cold dark matter cosmology""\n<LINK> <LINK>']",https://arxiv.org/abs/1802.00081,"The Milky Way and Andromeda galaxy are each surrounded by a thin plane of satellite galaxies that may be corotating. Cosmological simulations predict that most satellite galaxy systems are close to isotropic with random motions, so those two well-studied systems are often interpreted as rare statistical outliers. We test this assumption using the kinematics of satellite galaxies around the Centaurus A galaxy. Our statistical analysis reveals evidence for corotation in a narrow plane: of the 16 Centaurus A's satellites with kinematic data, 14 follow a coherent velocity pattern aligned with the long axis of their spatial distribution. In standard cosmology simulations, < 0.5% of Centaurus A-like systems show such behavior. Corotating satellite systems may be common in the Universe, challenging small-scale structure formation in the prevailing cosmological paradigm. ","A whirling plane of satellite galaxies around Centaurus A challenges
  cold dark matter cosmology"
74,959234329930010624,2570689932,Amartya Sanyal,"['Here is our new paper on ""Optimizing Non decomposable Measures with Deep Networks""\n<LINK>']",http://arxiv.org/abs/1802.00086,"We present a class of algorithms capable of directly training deep neural networks with respect to large families of task-specific performance measures such as the F-measure and the Kullback-Leibler divergence that are structured and non-decomposable. This presents a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions (that are decomposable) to train neural networks. We demonstrate that directly training with task-specific loss functions yields much faster and more stable convergence across problems and datasets. Our proposed algorithms and implementations have several novel features including (i) convergence to first order stationary points despite optimizing complex objective functions; (ii) use of fewer training samples to achieve a desired level of convergence, (iii) a substantial reduction in training time, and (iv) a seamless integration of our implementation into existing symbolic gradient frameworks. We implement our techniques on a variety of deep architectures including multi-layer perceptrons and recurrent neural networks and show that on a variety of benchmark and real data sets, our algorithms outperform traditional approaches to training deep networks, as well as some recent approaches to task-specific training of neural networks. ",Optimizing Non-decomposable Measures with Deep Networks
75,968926336281710592,64360289,Dimitris Rizopoulos,['Check out the new paper of @ERandrinopoulou that combines shared random effects joint models with latent classes: <LINK>'],https://arxiv.org/abs/1802.10015,"Cystic fibrosis is a chronic lung disease which requires frequent patient monitoring to maintain lung function over time and minimize onset of acute respiratory events known as pulmonary exacerbations. From the clinical point of view it is important to characterize the association between key biomarkers such as $FEV_1$ and time-to first exacerbation. Progression of the disease is heterogeneous, yielding different sub-groups in the population exhibiting distinct longitudinal profiles. It is desirable to categorize these unobserved sub-groups (latent classes) according to their distinctive trajectories. Accounting for these latent classes, in other words heterogeneity, will lead to improved estimates of association arising from the joint longitudinal-survival model. The joint model of longitudinal and survival data constitutes a popular framework to analyze such data arising from heterogeneous cohorts. In particular, two paradigms within this framework are the shared parameter joint models and the joint latent class models. The former paradigm allows one to quantify the strength of the association between the longitudinal and survival outcomes but does not allow for latent sub-populations. The latter paradigm explicitly postulates the existence of sub-populations but does not directly quantify the strength of the association. We propose to integrate latent classes in the shared parameter joint model in a fully Bayesian approach, which allows us to investigate the association between $FEV_1$ and time-to first exacerbation within each latent class. We, furthermore, focus on the selection of the optimal number of latent classes. ","Integrating Latent Classes in the Bayesian Shared Parameter Joint Model
  of Longitudinal and Survival Outcomes"
76,967333932923543554,704799523,francesca dominici,['New paper led by Kwonsang Lee! Super proud of this work Discovering Effect Modification and Randomization Inference in Air Pollution Studies - <LINK> #ScholarAlerts'],https://arxiv.org/abs/1802.06710,"Studies have shown that exposure to air pollution, even at low levels, significantly increases mortality. As regulatory actions are becoming prohibitively expensive, robust evidence to guide the development of targeted interventions to reduce air pollution exposure is needed. In this paper, we introduce a novel statistical method that splits the data into two subsamples: (a) Using the first subsample, we consider a data-driven search for $\textit{de novo}$ discovery of subgroups that could have exposure effects that differ from the population mean; and then (b) using the second subsample, we quantify evidence of effect modification among the subgroups with nonparametric randomization-based tests. We also develop a sensitivity analysis method to assess the robustness of the conclusions to unmeasured confounding bias. Via simulation studies and theoretical arguments, we demonstrate that since we discover the subgroups in the first subsample, hypothesis testing on the second subsample can focus on theses subgroups only, thus substantially increasing the statistical power of the test. We apply our method to the data of 1,612,414 Medicare beneficiaries in New England region in the United States for the period 2000 to 2006. We find that seniors aged between 81-85 with low income and seniors aged above 85 have statistically significant higher causal effects of exposure to PM$_{2.5}$ on 5-year mortality rate compared to the population mean. ","Discovering Effect Modification and Randomization Inference in Air
  Pollution Studies"
77,966961865812332544,989510983,Antonio Fern√°ndez,"['We uploaded a new paper ""Formalizing and Implementing Distributed Ledger Objects"" Constructive comments are welcome! <LINK>']",https://arxiv.org/abs/1802.07817,"Despite the hype about blockchains and distributed ledgers, no formal abstraction of these objects has been proposed. To face this issue, in this paper we provide a proper formulation of a distributed ledger object. In brief, we define a ledger object as a sequence of records, and we provide the operations and the properties that such an object should support. Implementation of a ledger object on top of multiple (possibly geographically dispersed) computing devices gives rise to the distributed ledger object. In contrast to the centralized object, distribution allows operations to be applied concurrently on the ledger, introducing challenges on the consistency of the ledger in each participant. We provide the definitions of three well known consistency guarantees in terms of the operations supported by the ledger object: (1) atomic consistency (linearizability), (2) sequential consistency, and (3) eventual consistency. We then provide implementations of distributed ledgers on asynchronous message passing crash-prone systems using an Atomic Broadcast service, and show that they provide eventual, sequential or atomic consistency semantics. We conclude with a variation of the ledger - the validated ledger - which requires that each record in the ledger satisfies a particular validation rule. ",Formalizing and Implementing Distributed Ledger Objects
78,966508018614517762,46945245,Henry Yuen,"['New paper out! Approximate low-weight check codes and circuit lower bounds for noisy ground states (<LINK>). Or, alternatively: fun with history states.', 'TL;DR: The quantum PCP conjecture is one of the biggest open questions in between quantum complexity theory and condensed matter physics. Our paper examines the relationship between some of the attempts that have been made towards qPCP, and also connections to coding theory.', 'The qPCP conjecture roughly says that approximating the ground energy of a local Hamiltonian, even up to constant fraction accuracy, is QMA-hard. Would be the quantum analogue of the classical PCP theorem. We have no idea if qPCP is true.', 'However, *if* qPCP were true, then it would mean the following: there are local Hamiltonians whose excited states (with energy up to constant fraction of system size) will be highly entangled, as measured by the *circuit complexity* of the state.', ""We don't know of any family of Hamiltonians with that property, so Freedman and Hastings conjectured that such a family exists. This is called the No Low-Energy Trivial States (NLTS) conjecture. Condensed matter theorists find this question fascinating."", 'In very rough terms: are there physical systems where, even at ""room temperature"", exhibit highly complex entanglement?', 'In a recent paper, Eldar and Harrow propose a variant of the NLTS conjecture, called NLETS. Instead of talking about low-energy states, they talk about ""low-error states"": take a ground state of a Hamiltonian and corrupt a constant fraction of the qubits.', 'The show that there are local Hamiltonians whose *low-error states* are highly entangled (i.e. have high circuit depth). It appears to be a reasonable relaxation of the NLTS conjecture, and their proof uses some really cool ideas.', 'We give a different proof of their NLETS theorem, and furthermore show that the Hamiltonian family can be made one dimensional. This immediately shows that NLTS is *very* different from NLETS, because NLTS inherently cannot hold for constant-dimensional systems.', 'On the other hand, our construction can be used to obtain something we call ""approximate low-weight check codes"", which is a local Hamiltonian whose ground space is an (approximate) error correcting code, and has linear rate and linear distance.', 'This, in turn seems very reminiscent of what quantum coding theorists have been hunting for a very long time: quantum low-density parity check (LDPC) codes with linear rate and linear distance. Are there any connections between our qLWC codes and qLDPC codes?']",https://arxiv.org/abs/1802.07419,"The No Low-Energy Trivial States (NLTS) conjecture of Freedman and Hastings (Quantum Information and Computation 2014), which asserts the existence of local Hamiltonians whose low energy states cannot be generated by constant depth quantum circuits, identifies a fundamental obstacle to resolving the quantum PCP conjecture. Progress towards the NLTS conjecture was made by Eldar and Harrow (Foundations of Computer Science 2017), who proved a closely related theorem called No Low-Error Trivial States (NLETS). In this paper, we give a much simpler proof of the NLETS theorem, and use the same technique to establish superpolynomial circuit size lower bounds for noisy ground states of local Hamiltonians (assuming $\mathsf{QCMA} \neq \mathsf{QMA}$), resolving an open question of Eldar and Harrow. We discuss the new light our results cast on the relationship between NLTS and NLETS. Finally, our techniques imply the existence of $\textit{approximate quantum low-weight check (qLWC) codes}$ with linear rate, linear distance, and constant weight checks. These codes are similar to quantum LDPC codes except (1) each particle may participate in a large number of checks, and (2) errors only need to be corrected up to fidelity $1 - 1/\mathsf{poly}(n)$. This stands in contrast to the best-known stabilizer LDPC codes due to Freedman, Meyer, and Luo which achieve a distance of $O(\sqrt{n \log n})$. The principal technique used in our results is to leverage the Feynman-Kitaev clock construction to approximately embed a subspace of states defined by a circuit as the ground space of a local Hamiltonian. ","Approximate low-weight check codes and circuit lower bounds for noisy
  ground states"
79,964214126238142464,933084565895286786,Dan Hooper,"[""The mismatch between local and early time measurements of our universe's expansion rate remains one of the most promising indications of new physics. This interesting new paper discusses ways that this might be clarified or resolved:\n<LINK> \n#cosmology #Hubble""]",https://arxiv.org/abs/1802.03404,"The Hubble constant ($H_0$) estimated from the local Cepheid-supernova (SN) distance ladder is in 3-$\sigma$ tension with the value extrapolated from cosmic microwave background (CMB) data assuming the standard cosmological model. Whether this tension represents new physics or systematic effects is the subject of intense debate. Here, we investigate how new, independent $H_0$ estimates can arbitrate this tension, assessing whether the measurements are consistent with being derived from the same model using the posterior predictive distribution (PPD). We show that, with existing data, the inverse distance ladder formed from BOSS baryon acoustic oscillation measurements and the Pantheon SN sample yields an $H_0$ posterior near-identical to the Planck CMB measurement. The observed local distance ladder value is a very unlikely draw from the resulting PPD. Turning to the future, we find that a sample of $\sim50$ binary neutron star ""standard sirens"" (detectable within the next decade) will be able to adjudicate between the local and CMB estimates. ",Prospects for resolving the Hubble constant tension with standard sirens
80,963983121027878912,19510090,Julian Togelius,"[""Who killed Albert Einstein? That's up to you to find out. @GabbBarros @Bumblebor @SentientDesigns and I proudly present our new paper on generating murder mystery games from open data.\n<LINK> <LINK>"", 'It works like this: you input the name of a person with a Wikipedia profile, and the system creates a whodunnit for you to solve. This involves a lot of crawling @Wikipedia , @WikiCommons and @openstreetmap, evolutionary algorithms and a bunch of other tricks.', 'The paper includes a ""blooper reel"" of sorts, as you would not believe  the kind of weird stuff you can find on Wikipedia and the ways in which  combining innocuous data can lead to troublesome content.', ""@FlorianHollandt @Wikipedia @WikiCommons @openstreetmap Thanks! Not yet, we're working on a new version which we hope will be stable enough to release publicly this spring."", '@samim @GabbBarros @Bumblebor @SentientDesigns Thanks! The current version will not be available, but we are working on a new version which will be public this spring.']",https://arxiv.org/abs/1802.05219,"This paper presents a framework for generating adventure games from open data. Focusing on the murder mystery type of adventure games, the generator is able to transform open data from Wikipedia articles, OpenStreetMap and images from Wikimedia Commons into WikiMysteries. Every WikiMystery game revolves around the murder of a person with a Wikipedia article and populates the game with suspects who must be arrested by the player if guilty of the murder or absolved if innocent. Starting from only one person as the victim, an extensive generative pipeline finds suspects, their alibis, and paths connecting them from open data, transforms open data into cities, buildings, non-player characters, locks and keys and dialog options. The paper describes in detail each generative step, provides a specific playthrough of one WikiMystery where Albert Einstein is murdered, and evaluates the outcomes of games generated for the 100 most influential people of the 20th century. ",Who Killed Albert Einstein? From Open Data to Murder Mystery Games
81,960691946258870274,19510090,Julian Togelius,"['What makes games hard for human and AI players? In our new paper on Deceptive Games, we analyze several types of deceptiveness that contribute to game hardness for algorithms such as MCTS.\n<LINK>\nAccepted to @Evostar2018 with @DamorinSolusar @ChristophSalge <LINK>', 'Also by @matthew_anu whose Twitter handle I forgot :)', ""@jackclarkSF @Evostar2018 @DamorinSolusar @ChristophSalge I didn't know about that particular work - thanks for the pointer (and for the write-up)! Will definitely look into the AI safety angle of this."", ""@jackclarkSF @Evostar2018 @DamorinSolusar @ChristophSalge Currently we're working on getting RL algorithms playing these games, which requires some long-overdue modifications of the GVGAI framework. Hope to be able to put out a paper on this soon."", '@b_gum22 @DamorinSolusar @ChristophSalge No, but we can make them available in the regular framework!']",https://arxiv.org/abs/1802.00048,"Deceptive games are games where the reward structure or other aspects of the game are designed to lead the agent away from a globally optimal policy. While many games are already deceptive to some extent, we designed a series of games in the Video Game Description Language (VGDL) implementing specific types of deception, classified by the cognitive biases they exploit. VGDL games can be run in the General Video Game Artificial Intelligence (GVGAI) Framework, making it possible to test a variety of existing AI agents that have been submitted to the GVGAI Competition on these deceptive games. Our results show that all tested agents are vulnerable to several kinds of deception, but that different agents have different weaknesses. This suggests that we can use deception to understand the capabilities of a game-playing algorithm, and game-playing algorithms to characterize the deception displayed by a game. ",Deceptive Games
82,968659858051317760,23980621,"Brett Morris, PhD","['New paper inspired by the #knowthystar conference: starspots are a source of astrometric jitter, which we can use to study the activity of the nearest stars with @ESAGaia!\n<LINK> <LINK>', 'The above animation shows a digital approximation of hand-recorded sunspot observations from Mount Wilson Observatory near solar maximum in 1957. As the Sun rotates, spots move into and out of view, moving the apparent centroid of the Sun (red X, exaggerated).', 'Among the best targets for activity detection with @ESAGaia (in the TGAS sample) are Gl 825, sigma Dra, and Gl 15 A. As the bright limits of Gaia are revised in future data releases, there may be even better candidates.', '@bmac_astro @johannateske @ESAGaia Yes, but the signal would be very small for Sun-like active latitudes. The biggest signal is the on/off nature of activity min/max.', '@bmac_astro @johannateske @ESAGaia  https://t.co/w6nYXNhJUd', '@lkreidberg  https://t.co/aGR3iL2Mqt']",https://arxiv.org/abs/1802.09943,"Astrometry from Gaia will measure the positions of stellar photometric centroids to unprecedented precision. We show that the precision of Gaia astrometry is sufficient to detect starspot-induced centroid jitter for nearby stars in the Tycho-Gaia Astrometric Solution (TGAS) sample with magnetic activity similar to the young G-star KIC 7174505 or the active M4 dwarf GJ 1243, but is insufficient to measure centroid jitter for stars with Sun-like spot distributions. We simulate Gaia observations of stars with 10 year activity cycles to search for evidence of activity cycles, and find that Gaia astrometry alone likely can not detect activity cycles for stars in the TGAS sample, even if they have spot distributions like KIC 7174505. We review the activity of the nearby low-mass stars in the TGAS sample for which we anticipate significant detections of spot-induced jitter. ",Spotting Stellar Activity Cycles in Gaia Astrometry
83,968299992594112512,3874714693,augustus odena,"[""Preprint on local geometry of GAN generators w/ Jacob Buckman, @catherineols, @nottombrown, @ch402, @colinraffel, @goodfellow_ian: <LINK>\n\nWe find:\n1. Spectrum of G's in/out Jacobian predicts Inception Score.\n2. Intervening to change spectrum affects scores a lot <LINK>"", 'We also find that using the intervention technique from 2. allows us to train Conditional WGAN-GP &gt;2x as fast w/ little degradation in performance as measured by those scores.']",https://arxiv.org/abs/1802.08768,"Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the 'quality' of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a 'regularization' technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs. ",Is Generator Conditioning Causally Related to GAN Performance?
84,967476054150565888,852533458668748804,Silvia Chiappa,"['In our journey into the development of machine learning algorithms, we also need to think about how the issue of bias can be addressed. \n\nIn this paper we propose a simple, general causal approach to deal with this problem <LINK> @tpgillam @DeepMindAI']",https://arxiv.org/abs/1802.08139,"We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a causal approach to disregard effects along unfair pathways that simplifies and generalizes previous literature. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information, and does not require an often intractable computation of the path-specific effect. We leverage recent developments in deep learning and approximate inference to achieve a solution that is widely applicable to complex, non-linear scenarios. ",Path-Specific Counterfactual Fairness
85,964584608976330753,888282134019842048,Chandler Squires,"['First paper is on arXiv: <LINK>\n\nWe show how to find the difference between two causal models, without learning the models themselves. We can get much better sample complexity as long as the difference is small compared to the whole model.']",https://arxiv.org/abs/1802.05631,"We consider the problem of estimating the differences between two causal directed acyclic graph (DAG) models with a shared topological order given i.i.d. samples from each model. This is of interest for example in genomics, where changes in the structure or edge weights of the underlying causal graphs reflect alterations in the gene regulatory networks. We here provide the first provably consistent method for directly estimating the differences in a pair of causal DAGs without separately learning two possibly large and dense DAG models and computing their difference. Our two-step algorithm first uses invariance tests between regression coefficients of the two data sets to estimate the skeleton of the difference graph and then orients some of the edges using invariance tests between regression residual variances. We demonstrate the properties of our method through a simulation study and apply it to the analysis of gene expression data from ovarian cancer and during T-cell activation. ",Direct Estimation of Differences in Causal Graphs
86,964466913257623553,9827202,Lukas Mosser,"['We have released our publication on conditioning three-dimensional pore- and reservoir-scale models created by generative adversarial networks. You can find the preprint here: <LINK>\nCode and pre-trained models on GitHub: <LINK> <LINK>', '@GrahamGanssle @stevejpurves @JesperDramsch Cheers @GrahamGannssle! Great to see interest from the Bureau of Economic Geology on Deep Learning in geoscience.']",https://arxiv.org/abs/1802.05622,"Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore- and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore- and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows. ","Conditioning of three-dimensional generative adversarial networks for
  pore and reservoir-scale models"
87,964250481165045760,738769492122214400,Johannes Lischner,['Our new paper on plasmonic photocatalysis is on the arxiv now (<LINK>). We study how material properties influence the conversion of light into hot electrons in metallic nanoparticles. #SolarEnergy #nanotechnology <LINK>'],https://arxiv.org/abs/1802.05096,"Harnessing hot electrons and holes resulting from the decay of localized surface plasmons in nanomaterials has recently led to new devices for photovoltaics, photocatalysis and optoelectronics. Properties of hot carriers are highly tunable and in this work we investigate their dependence on the material, size and environment of spherical metallic nanoparticles. In particular, we carry out theoretical calculations of hot carrier generation rates and energy distributions for six different plasmonic materials (Na, K, Al, Cu, Ag and Au). The plasmon decay into hot electron-hole pairs is described via Fermi's Golden Rule using the quasistatic approximation for optical properties and a spherical well potential for the electronic structure. We present results for nanoparticles with diameters up to 40 nm, which are embedded in different dielectric media. We find that small nanoparticles with diameters of 16 nm or less in media with large dielectric constants produce most hot carriers. Among the different materials, Na, K and Au generate most hot carriers. We also investigate hot-carrier induced water splitting and find that simple-metal nanoparticles are useful for initiating the hydrogen evolution reaction, while transition-metal nanoparticles produce dominantly holes for the oxygen evolution reaction. ","Material, size and environment dependence of plasmon-induced hot
  carriers in metallic nanoparticles"
88,964141722027614208,26419626,Jim Bagrow,"[""Crowdsourcing is a great way to get training data for machine learning, but the crowd isn't being very creative. What if we asked crowds of non-experts to propose their OWN machine learning problems? ü§î My latest:\n<LINK> <LINK>""]",https://arxiv.org/abs/1802.05101,"Non-experts have long made important contributions to machine learning (ML) by contributing training data, and recent work has shown that non-experts can also help with feature engineering by suggesting novel predictive features. However, non-experts have only contributed features to prediction tasks already posed by experienced ML practitioners. Here we study how non-experts can design prediction tasks themselves, what types of tasks non-experts will design, and whether predictive models can be automatically trained on data sourced for their tasks. We use a crowdsourcing platform where non-experts design predictive tasks that are then categorized and ranked by the crowd. Crowdsourced data are collected for top-ranked tasks and predictive models are then trained and evaluated automatically using those data. We show that individuals without ML experience can collectively construct useful datasets and that predictive models can be learned on these datasets, but challenges remain. The prediction tasks designed by non-experts covered a broad range of domains, from politics and current events to health behavior, demographics, and more. Proper instructions are crucial for non-experts, so we also conducted a randomized trial to understand how different instructions may influence the types of prediction tasks being proposed. In general, understanding better how non-experts can contribute to ML can further leverage advances in Automatic ML and has important implications as ML continues to drive workplace automation. ",Democratizing AI: Non-expert design of prediction tasks
89,961421759957348352,22148802,Leo C. Stein ü¶Å,"[""New preprint on the arXiv tonight, get it while it's hot!\nBaoyi &amp; I applied the technology we developed in the previous article to find how the near-horizon limit of extremal black holes get deformed when you turn on some beyond-GR corrections to gravity.\n<LINK>""]",https://arxiv.org/abs/1802.02159,"Black holes are a powerful setting for studying general relativity and theories beyond GR. However, analytical solutions for rotating black holes in beyond-GR theories are difficult to find because of the complexity of such theories. In this paper, we solve for the deformation to the near-horizon extremal Kerr metric due to two example string-inspired beyond-GR theories: Einstein-dilaton-Gauss-Bonnet, and dynamical Chern-Simons theory. We accomplish this by making use of the enhanced symmetry group of NHEK and the weak-coupling limit of EdGB and dCS. We find that the EdGB metric deformation has a curvature singularity, while the dCS metric is regular. From these solutions we compute orbital frequencies, horizon areas, and entropies. This sets the stage for analytically understanding the microscopic origin of black hole entropy in beyond-GR theories. ",Deformation of extremal black holes from stringy interactions
90,960912151736090624,308587014,Robert Feldt,"[""Our latest results on Psychological Safety (PS) seems to support Google's result that PS is important in SW teams. We also find clarity of team norms to be critical. Not yet peer-reviewed (but if you want to be cutting edge :)): <LINK>"", 'Data is from 200+ individuals, 38 teams and 5 SW companies. All in Sweden. :)', 'Independent vars are (self-assessed) team performance &amp; job satisfaction. Both psych safety &amp; norm clarity are strong predictors especially with job satisfaction. \nNote! Not content of norms but if they are clear/explicit/shared.', '@1danilo Yes, it has been part of social science research on teams for long but we have not found much empirical investigstion of it in SE. If you‚Äôve seen anything please share.']",https://arxiv.org/abs/1802.01378,"In the software engineering industry today, companies primarily conduct their work in teams. To increase organizational productivity, it is thus crucial to know the factors that affect team effectiveness. Two team-related concepts that have gained prominence lately are psychological safety and team norms. Still, few studies exist that explore these in a software engineering context. Therefore, with the aim of extending the knowledge of these concepts, we examined if psychological safety and team norm clarity associate positively with software developers' self-assessed team performance and job satisfaction, two important elements of effectiveness. We collected industry survey data from practitioners (N = 217) in 38 development teams working for five different organizations. The result of multiple linear regression analyses indicates that both psychological safety and team norm clarity predict team members' self-assessed performance and job satisfaction. The findings also suggest that clarity of norms is a stronger (30\% and 71\% stronger, respectively) predictor than psychological safety. This research highlights the need to examine, in more detail, the relationship between social norms and software development. The findings of this study could serve as an empirical baseline for such, future work. ",Psychological Safety and Norm Clarity in Software Engineering Teams
91,964436718240784385,51169895,Gianluca Stringhini,"['In our latest paper we analyze <LINK>, the Twitter alternative that ""safeguards free speech"". We find that it\'s an echo chamber for the alt-right, with users heavily reacting to political events and mainstream news barely referred to <LINK> <LINK>']",https://arxiv.org/abs/1802.05287,"Over the past few years, a number of new ""fringe"" communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web's information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting ""people and free speech first'"", welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan's Politically Incorrect board. ",What is Gab? A Bastion of Free Speech or an Alt-Right Echo Chamber?
92,963316625889005568,2971631394,Jakub Mielczarek,"['With @Elmistrana and @zadadam we give more details about ""A concept of biopharmaceutical nanosatellite‚Äù, see <LINK> We propose a #CubeSat #nanosatellite to conduct biopharmaceutical tests is space. #biotech #cancer #LabOnAChip #microfluidics #Mars #SpaceTurism <LINK>']",https://arxiv.org/abs/1802.04078,"The article is a short overview of a proposal of a CubeSat type nanosatellite designed to conduct biopharmaceutical tests on the low earth orbit. Motivations behind the emerging demand for such solution nowadays and in the close future are emphasized. The possible objectives and challenges to be addressed in the planned biopharmaceutical CubeSat missions are discussed. In particular, it is hard to imagine progress of the space tourism and colonization of Mars without a wide-ranging development of pharmaceutics dedicated to be used in space. Finally, an exemplary layout of a 3U type CubeSat is presented. We stress that, thanks to recent development in both nanosatellite technologies and lab-on-a-chip type biofluidic systems the proposed idea becomes now both feasible and relatively affordable. ",A concept of biopharmaceutical nanosatellite
