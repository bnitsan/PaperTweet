,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1083098445165019136,3199605543,Afonso S. Bandeira,"['New paper, with Tim Kunisky, about the space of feasible solutions for the Degree 4 SOS relaxation of the quadratic program arising, e.g., in Max-Cut: <LINK>']",https://arxiv.org/abs/1812.11583,"One of the most widely studied convex relaxations in combinatorial optimization is the relaxation of the cut polytope $\mathscr C^N$ to the elliptope $\mathscr E^N$, which corresponds to the degree 2 sum-of-squares (SOS) relaxation of optimizing a quadratic form over the hypercube $\{\pm 1\}^N$. We study the extension of this classical idea to degree 4 SOS, which gives an intermediate relaxation we call the degree 4 generalized elliptope $\mathscr E_4^N$. Our main result is a necessary and sufficient condition for the Gram matrix of a collection of vectors to belong to $\mathscr E_4^N$. Consequences include a tight rank inequality between degree 2 and degree 4 pseudomoment matrices, and a guarantee that the only extreme points of $\mathscr E^N$ also in $\mathscr E_4^N$ are the cut matrices; that is, $\mathscr E^N$ and $\mathscr E_4^N$ share no ""spurious"" extreme point. For Gram matrices of equiangular tight frames, we give a simple criterion for membership in $\mathscr{E}_4^N$. This yields new inequalities satisfied in $\mathscr{E}_4^N$ but not $\mathscr{E}^N$ whose structure is related to the Schl\""{a}fli graph and which cannot be obtained as linear combinations of triangle inequalities. We also give a new proof of the restriction to degree 4 of a result of Laurent showing that $\mathscr{E}_4^N$ does not satisfy certain cut polytope inequalities capturing parity constraints. Though limited to this special case, our proof of the positive semidefiniteness of Laurent's pseudomoment matrix is short and elementary. Our techniques also suggest that membership in $\mathscr{E}_4^N$ is closely related to the partial transpose operation on block matrices, which has previously played an important role in the study of quantum entanglement. To illustrate, we present a correspondence between certain entangled bipartite quantum states and the matrices of $\mathscr{E}_4^N\setminus\mathscr{C}^N$. ",A Gramian Description of the Degree 4 Generalized Elliptope
1,1081910714091126784,1007101344459587584,Kaspar WÃ¼thrich,['Want to estimate quantile effects based on instrumental variables? Our new paper proposes tractable and easy to implement estimators for linear instrumental variable quantile regression models. \n\nUngated version: <LINK>\n\nComments are welcome!'],https://arxiv.org/abs/1812.10925,"The instrumental variable quantile regression (IVQR) model (Chernozhukov and Hansen, 2005) is a popular tool for estimating causal quantile effects with endogenous covariates. However, estimation is complicated by the non-smoothness and non-convexity of the IVQR GMM objective function. This paper shows that the IVQR estimation problem can be decomposed into a set of conventional quantile regression sub-problems which are convex and can be solved efficiently. This reformulation leads to new identification results and to fast, easy to implement, and tuning-free estimators that do not require the availability of high-level ""black box"" optimization routines. ","Decentralization Estimators for Instrumental Variable Quantile
  Regression Models"
2,1080098435284566018,252608121,Yaneer Bar-Yam,"['Thoughts for a New Year/Era...""In this paper we are interested in how groups engage in decision making so as to benefit from combining individual capabilities."" Please share comments <LINK> or on arXiv <LINK> #teams #necsi #complexity']",http://arxiv.org/abs/1812.11953,"Emergent collective group processes and capabilities have been studied through analysis of transactive memory, measures of group task performance, and group intelligence, among others. In their approach to collective behaviors, these approaches transcend traditional studies of group decision making that focus on how individual preferences combine through power relationships, social choice by voting, negotiation and game theory. Understanding more generally how individuals contribute to group effectiveness is important to a broad set of social challenges. Here we formalize a dynamic theory of interpersonal communications that classifies individual acts, sequences of actions, group behavioral patterns, and individuals engaged in group decision making. Group decision making occurs through a sequence of communications that convey personal attitudes and preferences among members of the group. The resulting formalism is relevant to psychosocial behavior analysis, rules of order, organizational structures and personality types, as well as formalized systems such as social choice theory. More centrally, it provides a framework for quantifying and even anticipating the structure of informal dialog, allowing specific conversations to be coded and analyzed in relation to a quantitative model of the participating individuals and the parameters that govern their interactions. ",A Mathematical Theory of Interpersonal Interactions and Group Behavior
3,1080057493781389312,251528251,Fotis Savva,"['Happy New Year everybody !!! An extension of our paper ""Explaining Aggregates for Exploratory Analytics"" with extra experiments, metrics and more is now available in arXiv : <LINK>']",https://arxiv.org/abs/1812.11346,"Analysts wishing to explore multivariate data spaces, typically pose queries involving selection operators, i.e., range or radius queries, which define data subspaces of possible interest and then use aggregation functions, the results of which determine their exploratory analytics interests. However, such aggregate query (AQ) results are simple scalars and as such, convey limited information about the queried subspaces for exploratory analysis. We address this shortcoming aiding analysts to explore and understand data subspaces by contributing a novel explanation mechanism coined XAXA: eXplaining Aggregates for eXploratory Analytics. XAXA's novel AQ explanations are represented using functions obtained by a three-fold joint optimization problem. Explanations assume the form of a set of parametric piecewise-linear functions acquired through a statistical learning model. A key feature of the proposed solution is that model training is performed by only monitoring AQs and their answers on-line. In XAXA, explanations for future AQs can be computed without any database (DB) access and can be used to further explore the queried data subspaces, without issuing any more queries to the DB. We evaluate the explanation accuracy and efficiency of XAXA through theoretically grounded metrics over real-world and synthetic datasets and query workloads. ",Explaining Aggregates for Exploratory Analytics
4,1079666627422158848,470078739,David Peer,['Check our our new paper <LINK> \nIn this paper we introduce a new routing algorithm called dynamic deep routing that allows the training of deeper capsule networks. This algorithm is also more robust to white box adversarial attacks...'],https://arxiv.org/abs/1812.09707,In this paper we introduce a new inductive bias for capsule networks and call networks that use this prior $\gamma$-capsule networks. Our inductive bias that is inspired by TE neurons of the inferior temporal cortex increases the adversarial robustness and the explainability of capsule networks. A theoretical framework with formal definitions of $\gamma$-capsule networks and metrics for evaluation are also provided. Under our framework we show that common capsule networks do not necessarily make use of this inductive bias. For this reason we introduce a novel routing algorithm and use a different training algorithm to be able to implement $\gamma$-capsule networks. We then show experimentally that $\gamma$-capsule networks are indeed more transparent and more robust against adversarial attacks than regular capsule networks. ,"Increasing the adversarial robustness and explainability of capsule
  networks with $\gamma$-capsules"
5,1079598515079254018,271156299,Kihara Laboratory,"['The last new paper of 2018 from our lab: Three-Dimensional Krawtchouk Descriptors for Protein Local Surface Shape Comparison, by Atilla Sit and D. Kihara <LINK>']",http://arxiv.org/abs/1812.10841,"Direct comparison of three-dimensional (3D) objects is computationally expensive due to the need for translation, rotation, and scaling of the objects to evaluate their similarity. In applications of 3D object comparison, often identifying specific local regions of objects is of particular interest. We have recently developed a set of 2D moment invariants based on discrete orthogonal Krawtchouk polynomials for comparison of local image patches. In this work, we extend them to 3D and construct 3D Krawtchouk descriptors (3DKD) that are invariant under translation, rotation, and scaling. The new descriptors have the ability to extract local features of a 3D surface from any region-of-interest. This property enables comparison of two arbitrary local surface regions from different 3D objects. We present the new formulation of 3DKD and apply it to the local shape comparison of protein surfaces in order to predict ligand molecules that bind to query proteins. ","Three-Dimensional Krawtchouk Descriptors for Protein Local Surface Shape
  Comparison"
6,1078244617534033920,956539964795301889,Jacopo Bertolotti,"['New paper!\nIntuitive, easy to implement, and powerful way to compute light propagation in complex media (including highly scattering, birifrangent, chiral, bi-isotropic, magnetic media).\nWith a full Python implementation available to download.\n<LINK>', '@TimmovdB I lost a bit track after you completed your PhD. What do you work on now?']",https://arxiv.org/abs/1812.10463,"Understanding the interaction of light with a highly scattering material is essential for optical microscopy of optically thick and heterogeneous biological tissues. Ensemble-averaged analytic solutions cannot provide more than general predictions for relatively simple cases. Yet, biological tissues contain chiral organic molecules and many of the cells' structures are birefringent, a property exploited by polarization microscopy for label-free imaging. Solving Maxwell's equations in such materials is a notoriously hard problem. Here we present an efficient method to determine the propagation of electro-magnetic waves in arbitrary anisotropic materials. We demonstrate how the algorithm enables large scale calculations of the scattered light field in complex birefringent materials, chiral media, and even materials with a negative refractive index. ",Calculating coherent light-wave propagation in large heterogeneous media
7,1078219542638276608,892059194240532480,Mikel Artetxe,"['Check out our new paper ""Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"" (w/ Holger Schwenk). New SOTA on cross-lingual transfer (XNLI, MLDoc) and bitext mining (BUCC) using a shared encoder for 93 languages!\n<LINK> <LINK>']",https://arxiv.org/abs/1812.10464,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI dataset), cross-lingual document classification (MLDoc dataset) and parallel corpus mining (BUCC dataset) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low-resource languages. Our implementation, the pre-trained encoder and the multilingual test set are available at this https URL ","Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual
  Transfer and Beyond"
8,1078111509086121984,812782801602957312,Hanieh Falahati,"['Our new review paper on \nThermodynamically Driven Assemblies and Liquid-liquid Phase Separations in Biology is now on @arxiv. It was really fun working on it with @theIranianAlien! <LINK>', '@yogeshgoyal03 @arxiv @theIranianAlien Thanks ð']",https://arxiv.org/abs/1812.09412,"The sustenance of life depends on the high degree of organization that prevails through different levels of living organisms, from subcellular structures such as biomolecular complexes and organelles to tissues and organs. The physical origin of such organization is not fully understood, and even though it is clear that cells and organisms cannot maintain their integrity without consuming energy, there is growing evidence that individual assembly processes can be thermodynamically driven and occur spontaneously due to changes in thermodynamic variables such as intermolecular interactions and concentration. Understanding the phase separation \emph{in vivo} requires a multidisciplinary approach, integrating the theory and physics of phase separation with experimental and computational techniques. This paper aims at providing a brief overview of the physics of phase separation and its biological implications, with a particular focus on the assembly of membraneless organelles. We discuss the underlying physical principles of phase separation from its thermodynamics to its kinetics. We also overview the wide range of methods utilized for experimental verification and characterization of phase separation of membraneless organelles, as well as the utility of molecular simulations rooted in thermodynamics and statistical physics in understanding the governing principles of thermodynamically driven biological self-assembly processes. ","Thermodynamically Driven Assemblies and Liquid-liquid Phase Separations
  in Biology"
9,1077234804578533376,554869994,Yonatan Belinkov,"['Interested in understanding neural networks for #NLProc ? Looking for reading material for your winter break? Check out our new paper, ""Analysis Methods in Neural Language Processing: A Survey"", to appear in TACL.\nPreprint: <LINK>\nWebsite: <LINK>', 'In the paper, we:\n- Review various methods for analyzing neural networks for NLP\n- Categorize them into several trends (more on this below)\n- Discuss limitations in current work\n- And point out directions for future work', '#1 What linguistic information is captured in neural networks:\n- Methods for finding language in #deeplearning\n- Categorization of linguistic phenomena and network components\n- Limitations of the common ""auxiliary prediction tasks"" aka ""diagnostic classifiers"" aka ""probing tasks""', '#2 Visualization as an analysis tool, including heat maps of RNN activations, attention alignments, saliency measures from #computervision, and online visualization tools. We also call for more evaluations of such methods.', '#3 Challenge sets, aka test, suites as a paradigm for fine-grained evaluation, with inspiration from older #NLProc work. \n- Different tasks (#neuralempty, NLI -- we need more!)\n- Linguistic properties\n- Variety of languages (not enough!) and scale\n- Construction methods', '#4 Adversarial examples as a way to detect weaknesses of neural networks. We discuss the difficulty of discrete text input, and:\n- Black-box vs white-box attacks\n- Targeted vs non-targeted\n- Linguistic unit (characters, words, sentences) \n- The attacked task\n- Issues of coherence', '#5 Explaining predictions by generating explanations or finding input-output associations, and call for more work on this important topic.\n\n#6 And some other analysis work like erasure, behavioral experiments, and learning formal languages.', ""For all the refs we've missed, please contribute to the website: https://t.co/4copevvux6\n\nFinally, if you need inspiration, see our conclusion for some of the gaps we identified in the literature.\n\nAnd then submit to the #BlackboxNLP #acl2019 workshop at https://t.co/aXMdnc5cqZ""]",https://arxiv.org/abs/1812.08951,"The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work. ",Analysis Methods in Neural Language Processing: A Survey
10,1077070721216585729,2432886163,Oscar BarragÃ¡n,"['For this Christmas eve, a new K2 exoplanet: HD 119130 b\n\nCheck the paper here - &gt; <LINK> <LINK>']",https://arxiv.org/abs/1812.09242,"We present the discovery and characterization of a new transiting planet from Campaign 17 of the Kepler extended mission K2. HD 119130 b is a warm sub-Neptune on a 17-d orbit around a bright (V = 9.9 mag) solar-like G3 V star with a mass and radius of $M_\star = 1.00\pm0.03\,\mathrm{M_\odot}$ and $R_\star = 1.09\pm0.03\,\mathrm{R_\odot}$, respectively. We model simultaneously the K2 photometry and CARMENES spectroscopic data and derive a radius of $R_\mathrm{p} = 2.63_{-0.10}^{+0.12}\,\mathrm{R_\oplus}$ and mass of $M_\mathrm{p} = 24.5_{-4.4}^{+4.4}\,\mathrm{M_\oplus}$, yielding a mean density of $\rho_\mathrm{p} = 7.4_{-1.5}^{+1.6}\,\mathrm{g\,cm^{-3}}$, which makes it one of the densest sub-Neptune planets known to date. We also detect a linear trend in radial velocities of HD 119130 ($\dot{\gamma}_{\rm RV}= -0.40^{+0.07}_{-0.07}\,\mathrm{m\,s^{-1}\,d^{-1}}$) that suggests a long-period companion with a minimum mass on the order of $33\,\mathrm{M_\oplus}$. If confirmed, it would support a formation scenario of HD 119130 b by migration caused by Kozai-Lidov oscillations. ","Detection and characterization of an ultra-dense sub-Neptune planet
  orbiting the Sun-like star HD 119130"
11,1076524927883202560,633176250,Dr. Larry R. Nittler ðð¹ðâï¸,"['Our new paper on nucleosynthesis in thermonuclear electron capture supernovae, including possible relevance for #presolargrains \n\n<LINK> <LINK>']",https://arxiv.org/abs/1812.08230,"(Abridged) The explosion mechanism of electron-capture supernovae (ECSNe) remains equivocal. We attempt to constrain the explosion mechanism (neutron-star-forming implosion or thermonuclear explosion) and the frequency of occurrence of ECSNe using nucleosynthesis simulations of the latter scenario, population synthesis, the solar abundance distribution, pre-solar meteoritic oxide grain isotopic ratio measurements and the white dwarf mass-radius relation. Tracer particles from 3d hydrodynamic simulations were post-processed with a large nuclear reaction network in order to determine the complete compositional state of the bound ONeFe remnant and the ejecta, and population synthesis simulations were performed in order to estimate the ECSN rate with respect to the CCSN rate. The 3d deflagration simulations drastically overproduce the neutron-rich isotopes $^{48}$Ca, $^{50}$Ti, $^{54}$Cr, $^{60}$Fe and several of the Zn isotopes relative to their solar abundances. Using the solar abundance distribution as our constraint, we place an upper limit on the frequency of thermonuclear ECSNe as 1$-$3~\% the frequency at which core-collapse supernovae (FeCCSNe) occur. This is on par with or 1~dex lower than the estimates for ECSNe from single stars. The upper limit from the yields is also in relatively good agreement with the predictions from our population synthesis simulations. The $^{54}$Cr/$^{52}$Cr and $^{50}$Ti/$^{48}$Ti isotopic ratios in the ejecta are a near-perfect match with recent measurements of extreme pre-solar meteoritc oxide grains, and $^{53}$Cr/$^{52}$Cr can also be matched if the ejecta condenses before mixing with the interstellar medium. Theoretical mass-radius relations for the bound ONeFe WD remnants of these explosions are apparently consistent with several observational WD candidates. ","Remnants and ejecta of thermonuclear electron-capture supernovae:
  Constraining oxygen-neon deflagrations in high-density white dwarfs"
12,1076171585785028608,14825162,Adam M. Smith (å²äºå½),"[""Here's our new paper laying out the problem, importantly including ~metrics~ for automatic exploration in unknown games and comparing these with human performance given equal in-game time: <LINK>"", ""We'll be presenting this at the Second AAAI Workshop on Knowledge Extraction from Games (https://t.co/OFnwgrFg2O) because the goal is to extract useful knowledge from a game, not to act intelligently within its walls."", 'Starter notebooks for running your own experiments are available here: https://t.co/wIELo9OvCU']",https://arxiv.org/abs/1812.03125,"Machine playtesting tools and game moment search engines require exposure to the diversity of a game's state space if they are to report on or index the most interesting moments of possible play. Meanwhile, mobile app distribution services would like to quickly determine if a freshly-uploaded game is fit to be published. Having access to a semantic map of reachable states in the game would enable efficient inference in these applications. However, human gameplay data is expensive to acquire relative to the coverage of a game that it provides. We show that off-the-shelf automatic exploration strategies can explore with an effectiveness comparable to human gameplay on the same timescale. We contribute generic methods for quantifying exploration quality as a function of time and demonstrate our metric on several elementary techniques and human players on a collection of commercial games sampled from multiple game platforms (from Atari 2600 to Nintendo 64). Emphasizing the diversity of states reached and the semantic map extracted, this work makes productive contrast with the focus on finding a behavior policy or optimizing game score used in most automatic game playing research. ",Taking the Scenic Route: Automatic Exploration for Videogames
13,1076150230624714754,116832314,Andreas Damianou,"['Our paper ""Transferring knowledge across learning processes"" has been accepted as ORAL presentation for #ICLR2019 ! \n\n<LINK>\n\nTL;DR: A new view on meta-learning to scale beyond few shot learning.\n\nw/ @flennerhag @pgmoren @lawrennd']",https://arxiv.org/abs/1812.01054,"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps. ",Transferring Knowledge across Learning Processes
14,1076056804973531136,706175245976248321,Oscar Viyuela,['Our new paper on the arXiv: Scalable quantum error correction with Majorana fermions!\nTogether with Liang Fu (MIT) and Sagar Vijay (Harvard)\n<LINK>'],https://arxiv.org/abs/1812.08477,"We study the error correcting properties of Majorana Surface Codes (MSC), topological quantum codes constructed out of interacting Majorana fermions, which can be used to store quantum information and perform quantum computation. These quantum memories suffer from purely ""fermionic"" errors, such as quasiparticle poisoning (QP), that have no analog in conventional platforms with bosonic qubits. In physical realizations where QP dominates, we show that errors can be corrected provided that the poisoning rate is below a threshold of $\sim11\%$. When QP is highly suppressed and fermionic bilinear (""bosonic"") errors become dominant, we find an error threshold of $\sim16\%$, which is much higher than the threshold for spin-based topological memories like the Surface code or the Color code. In addition, we derive new lattice gauge theories to account for measurement errors. These results, together with the inherent error suppression provided by the superconducting gap in physical realizations of the MSC, makes this a strong candidate for a robust topological quantum memory. ",Scalable Fermionic Error Correction in Majorana Surface Codes
15,1076030309672599552,75249390,Axel Maas,"['We have published a new paper on how gluons behave at finite density at <LINK> - though for the moment in a toy theory, QCD with only two colors #np3']",https://arxiv.org/abs/1812.08517,"2-color QCD is the simplest QCD-like theory which is accessible to lattice simulations at finite density. It therefore plays an important role to test qualitative features and to provide benchmarks to other methods and models, which do not suffer from a sign problem. To this end, we determine the minimal-Landau-gauge propagators and 3-point vertices in this theory over a wide range of densities, the vacuum, and at both finite temperature and density. The results show that there is essentially no modification of the gauge sector in the low-temperature, low-density phase. Even outside this phase only mild modifications appear, mostly in the chromoelectric sector. ",Finite-density gauge correlation functions in QC2D
16,1076023970112114689,14101261,Kenneth Falck,"['This is an interesting paper on problems of current serverless platforms, and several future ideas: executing code closer to data, inventing new high-level cloud application DSLs, creating new ""cloud-native"" versions of classic low-level threads and ports. <LINK>']",https://arxiv.org/abs/1812.03651,"Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. ","Serverless Computing: One Step Forward, Two Steps Back"
17,1075731555060981762,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper (<LINK>) on how to use atomic systems for quantum walks on high dimensional hypercubes by myself @etotheipiequals, Viv Kendon and Ben Dodds, includes a 6D hypercube I drew in inkscape, whats not to like @jqcDurNew @DurhamPhysics @DurhamQlm']",https://arxiv.org/abs/1812.07885,"We present a method to experimentally realize large-scale permutation-symmetric Hamiltonians for continuous-time quantum protocols such as quantum walk and adiabatic quantum computation. In particular, the method can be used to perform an encoded continuous-time quantum search on a hypercube graph with 2^n vertices encoded into 2n qubits. We provide details for a realistically achievable implementation in Rydberg atomic systems. Although the method is perturbative, the realization is always achieved at second order in perturbation theory, regardless of the size of the mapped system. This highly efficient mapping provides a natural set of problems which are tractable both numerically and analytically, thereby providing a powerful tool for benchmarking quantum hardware and experimentally investigating the physics of continuous-time quantum protocols. ","Practical designs for permutation symmetric problem Hamiltonians on
  hypercubes"
18,1075323385192833024,57593088,Rizal Fathony,['Our new paper about how to design surrogate losses for general multiclass classification that enjoy guarantee of Fisher consistency and also perform well in practice. #MachineLearning <LINK>'],https://arxiv.org/abs/1812.07526,"We propose a robust adversarial prediction framework for general multiclass classification. Our method seeks predictive distributions that robustly optimize non-convex and non-continuous multiclass loss metrics against the worst-case conditional label distributions (the adversarial distributions) that (approximately) match the statistics of the training data. Although the optimized loss metrics are non-convex and non-continuous, the dual formulation of the framework is a convex optimization problem that can be recast as a risk minimization model with a prescribed convex surrogate loss we call the adversarial surrogate loss. We show that the adversarial surrogate losses fill an existing gap in surrogate loss construction for general multiclass classification problems, by simultaneously aligning better with the original multiclass loss, guaranteeing Fisher consistency, enabling a way to incorporate rich feature spaces via the kernel trick, and providing competitive performance in practice. ","Consistent Robust Adversarial Prediction for General Multiclass
  Classification"
19,1075317829564542983,175921010,Earl Patrick Bellinger,"[""[1/6] New paper out! Even though the title kind of gives it away, here's a quick rundown of the paper... \n\n<LINK>\n\n#asteroseismology #exoplanets #machinelearning"", '[2/6] Asteroseismology is the best way to determine the properties of stars. This diagram shows the precision with which asteroseismology constrains the ages, masses, radii, and other properties of ~100 solar-type stars that were observed by the NASA Kepler mission. https://t.co/nPnNFsB4Yr', '[3/6] The great precision with which asteroseismology constrains stellar parameters is useful for many purposes, including studying exoplanets. This is because our ability to measure the properties of exoplanets hinges on our ability to measure the properties of their hosts. https://t.co/xPtl9RbqtX', ""[4/6] But what if our measurements of the host stars are wrong? This is what we looked at in this paper. This diagram shows how much (percentage-wise) each stellar property changes if the measured temperature (x-axis) and/or metallicity (y-axis) are wrong. It's not much! https://t.co/EHY5QTb7Cv"", '[5/6] All in all, this result is not too shocking. Here are some of the stars that we studied shown in the ""classical"" (left) and ""asteroseismic"" (right) HR diagrams. If you\'ve never seen the kind of diagram on the right before, it serves to reveal the power of asteroseismology. https://t.co/FD11eOrODL', '[6/6] So for all these results and more --- including comparisons of asteroseismology with Gaia DR2 data, and similar investigations into under-reported uncertainties, as well as a table of all confirmed exoplanets for these stars --- go check out the paper!']",https://arxiv.org/abs/1812.06979,"The search for twins of the Sun and Earth relies on accurate characterization of stellar and exoplanetary parameters: i.e., ages, masses, and radii. In the modern era of asteroseismology, parameters of solar-like stars are derived by fitting theoretical models to observational data, which include measurements of their oscillation frequencies, metallicity [Fe/H], and effective temperature Teff. Combining this information with transit data furthermore yields the corresponding parameters for their exoplanets. While [Fe/H] and Teff are commonly stated to a precision of ~0.1 dex and ~100 K, the impact of errors in their measurement has not been studied in practice within the context of the parameters derived from them. Here we use the Stellar Parameters in an Instant (SPI) pipeline to estimate the parameters of nearly 100 stars observed by Kepler and Gaia, many of which are confirmed planet hosts. We adjust the reported spectroscopic measurements of these stars by introducing faux systematic errors and artificially increasing the reported uncertainties, and quantify the differences in the resulting parameters. We find that a systematic error of 0.1 dex in [Fe/H] translates to differences of only 4%, 2%, and 1% on average in the resulting stellar ages, masses, and radii, which are well within their uncertainties (~11%, 3.5%, 1.4%) as derived by SPI. We also find that increasing the uncertainty of [Fe/H] measurements by 0.1 dex increases the uncertainties by only 0.01 Gyr, 0.02 M_sun, and 0.01 R_sun, which are again well below their reported uncertainties (0.5 Gyr, 0.04 M_sun, 0.02 R_sun). The results for Teff at 100 K are similar. Stellar parameters from SPI are unchanged within uncertainties by errors of up to 0.14 dex or 175 K, and are even more robust to errors in Teff than the seismic scaling relations. Consequently, the parameters for their exoplanets are robust as well. ","Stellar ages, masses and radii from asteroseismic modeling are robust to
  systematic errors in spectroscopy"
20,1075310613222572032,954415596,J. Trayford,"['ð¨ New paper ð¨ - we investigate the spatially resolved main sequence, mass-metallicity relations and their emergence in the EAGLE simulations. These are a neat way to probe the internal structure and population statistics of galaxies simultaneously <LINK>']",https://arxiv.org/abs/1812.06984v1,"We explore scaling relations between the physical properties of spatially resolved regions within the galaxies that emerge in the Evolution and Assembly of GaLaxies and their Environments (EAGLE) hydrodynamical, cosmological simulations. Using 1 kpc-scale spaxels, we compute the relationships between the star formation rate and stellar mass surface densities, i.e. the spatially resolved star-forming main sequence (rSFMS), and between the gas metallicity and the stellar mass surface density, i.e. the spatially resolved mass-metallicity relation (rMZR). We compare to observed relations derived from integral field unit surveys and imaging of galaxies. EAGLE reproduces the slope of the local ($z\approx0.1$) rSFMS well, but with a $\approx-0.15$ dex offset, close to that found for the galaxy-integrated relation. The shape of the rMZR agrees reasonably well with observations, replicating the characteristic turnover at high surface density, which we show is due to AGN feedback. The residuals of the rSFMS and rMZR are negatively (positively) correlated at low (high) surface density. The rSFMS becomes shallower as the simulation evolves from $z=2$ to 0.1, a manifestation of inside-out galaxy formation. The shape of the rMZR also exhibits dramatic evolution, from a convex profile at $z=2$ to the observed concave profile at $z=0.1$, such that the gas in regions of high stellar density is more enriched at higher redshift. The redshift independence of the relationship between the galaxy-wide gas fraction and metallicity in EAGLE galaxies is not preserved on 1 kpc scales, implying that chemical evolution is non-local due to the transport of gas and metals within galaxies. ","] Resolved galaxy scaling relations in the EAGLE simulation: star
  formation, metallicity and stellar mass on kpc scales"
21,1075302231107399681,216185146,James Monk,"[""I've got a paper out today describing a new method for pile-up mitigation at the LHC upgrade using wavelets: <LINK>""]",https://arxiv.org/abs/1812.07412,"Collision experiments at the Large Hadron Collider suffer from the problem of pile-up, which is the read-out of multiple simultaneous background proton-proton collisions per beam-crossing. We introduce a pile-up mitigation technique based on wavelet decomposition. Pile-up is treated as a form of white noise, which can be removed by filtering beam-crossing events in the wavelet domain. The particle-level performance of the method is evaluated using a sample of simulated proton-proton collision events that contain Z bosons decaying to a pair of neutrinos, overlaid with pile-up. In the wavelet representation, the pile-up noise level is found to grow with the square root of the number of background proton-proton collisions. ",A Wavelet Based Pile-Up Mitigation Method for the LHC Upgrade
22,1075301652398252032,820031736914513925,Fabrizio Leisen,"['New paper: ""On a flexible construction of a negative binomial model"" written in collaboration with Ramses Mena, Luca Rossini and Freddy Palma <LINK>']",https://arxiv.org/abs/1812.07271,"This work presents a construction of stationary Markov models with negative-binomial marginal distributions. A simple closed form expression for the corresponding transition probabilities is given, linking the proposal to well-known classes of birth and death processes and thus revealing interesting characterizations. The advantage of having such closed form expressions is tested on simulated and real data. ",On a flexible construction of a negative binomial model
23,1075237010086883328,1339581511,Ben Rackham,"['New paper on WASP-4b from Alex Bixel and the ACCESS team (<LINK>): <LINK>\n\nWe find a featureless visual transmission spectrum for this hot Jupiter orbiting a G7 dwarf, w/o a strong slope, Na, K, or imprints of stellar spectral features.']",http://arxiv.org/abs/1812.07177,"We present an optical transmission spectrum of the atmosphere of WASP-4b obtained through observations of four transits with Magellan/IMACS. Using a Bayesian approach to atmospheric retrieval, we find no evidence for scattering or absorption features in our transit spectrum. Our models include a component to model the transit light source effect (spectral contamination from unocculted spots on the stellar photosphere), which we show can have a marked impact on the observed transmission spectrum for reasonable spot covering fractions (< 5%); this is the first such analysis for WASP-4b. We are also able to fit for the size and temperature contrast of spots observed during the second and third transits, finding evidence for both small, cool and large, warm spot-like features on the photosphere. Finally, we compare our results to those published by Huitson et al. (2017) using Gemini/GMOS and May et al. (2018) using IMACS, and find that our data are in agreement. ","ACCESS: Ground-based Optical Transmission Spectroscopy of the Hot
  Jupiter WASP-4b"
24,1075175442808799243,710610891058716673,Jan Leike,['Multiparty computation is awesome because it lets multiple parties train a model without seeing the weights.\n\nBut there are fundamental limits to making it scalable: &gt;24x overhead!\n\nOur new paper addresses this problem.\n\n<LINK>\n\nw/ @MiljanMartic @iamtrask et al. <LINK>'],https://arxiv.org/abs/1812.05979,"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive. ",Scaling shared model governance via model splitting
25,1075162553318281216,992249542824230912,Dionysis Kalogerias,['Check out our new paper on predictive learning on hidden Ising trees (joint work with Anand Sarwate and Kostas Nikolakakis (ECE Rutgers))! \n\nThe paper develops new results and extends prior work on noiseless tree structure learning (by Bresler &amp; Karzand).\n\n<LINK>'],https://arxiv.org/abs/1812.04700,"We provide high-probability sample complexity guarantees for exact structure recovery and accurate predictive learning using noise-corrupted samples from an acyclic (tree-shaped) graphical model. The hidden variables follow a tree-structured Ising model distribution, whereas the observable variables are generated by a binary symmetric channel taking the hidden variables as its input (flipping each bit independently with some constant probability $q\in [0,1/2)$). In the absence of noise, predictive learning on Ising models was recently studied by Bresler and Karzand (2020); this paper quantifies how noise in the hidden model impacts the tasks of structure recovery and marginal distribution estimation by proving upper and lower bounds on the sample complexity. Our results generalize state-of-the-art bounds reported in prior work, and they exactly recover the noiseless case ($q=0$). In fact, for any tree with $p$ vertices and probability of incorrect recovery $\delta>0$, the sufficient number of samples remains logarithmic as in the noiseless case, i.e., $\mathcal{O}(\log(p/\delta))$, while the dependence on $q$ is $\mathcal{O}\big( 1/(1-2q)^{4} \big)$, for both aforementioned tasks. We also present a new equivalent of Isserlis' Theorem for sign-valued tree-structured distributions, yielding a new low-complexity algorithm for higher-order moment estimation. ",Predictive Learning on Hidden Tree-Structured Ising Models
26,1075138371184283648,2337598033,Geraint F. Lewis,"['New paper on the @arxiv, with some rather lovely equations <LINK> <LINK>']",https://arxiv.org/abs/1812.06922,"Observations of the large-scale structure (LSS) implicitly assume an ideal FLRW observer with the ambient structure having no influence on the observer. However, due to correlations in the LSS, cosmological observables are dependent on the position of an observer. We investigate this influence in full generality for a weakly non-Gaussian random field, for which we derive expressions for angular spectra of large-scale structure observables conditional on a property of the large-scale structure that is typical for the observer's location. As an application, we then apply to the formalism to angular spectra of the weak gravitational lensing effect and provide numerical estimates for the resulting change on the spectra using linear structure formation. For angular weak lensing spectra we find the effect to be of order of a few percent, for instance we estimate for an overdensity of $\delta =0.5$ and multipoles up to $\ell=100$ the change in the weak lensing spectra to be approximately 4 percent. We show that without accounting for correlation between the density at observer's location and the weak gravitational lensing spectra, the values of the parameters $\Omega_m$ and $\sigma_8$ are underestimated by a few percent. Thus, this effect will be important when analysing data from future surveys such as Euclid, which aim at the percent-level precision. The effect is difficult to capture in simulations, as estimates of the number of numerical simulations necessary to quantify the effect are high. ",Influence of the local Universe on weak gravitational lensing surveys
27,1075059407258554369,3491609373,David Zimmerer,['Excited to share our new paper on Unsupervised Anomaly Detection ð @MIC_DKFZ. Able to simultaneously detect and localize anomalies in medical images and achieves new unsupervised SoTA performance on two public segmentation datasets ð. \nCheck it out:   <LINK> <LINK>'],https://arxiv.org/abs/1812.05941,"Unsupervised learning can leverage large-scale data sources without the need for annotations. In this context, deep learning-based auto encoders have shown great potential in detecting anomalies in medical images. However, state-of-the-art anomaly scores are still based on the reconstruction error, which lacks in two essential parts: it ignores the model-internal representation employed for reconstruction, and it lacks formal assertions and comparability between samples. We address these shortcomings by proposing the Context-encoding Variational Autoencoder (ceVAE) which combines reconstruction- with density-based anomaly scoring. This improves the sample- as well as pixel-wise results. In our experiments on the BraTS-2017 and ISLES-2015 segmentation benchmarks, the ceVAE achieves unsupervised ROC-AUCs of 0.95 and 0.89, respectively, thus outperforming state-of-the-art methods by a considerable margin. ","Context-encoding Variational Autoencoder for Unsupervised Anomaly
  Detection"
28,1075031977806327808,1339581511,Ben Rackham,"['Interested in how magnetic active regions (spots and faculae) in the photospheres of FGK dwarfs affect exoplanet transmission spectra?  Then check out my new paper (w/ @danielapai &amp; Mark Giampapa), just accepted to AJ:  <LINK>\n\nSome highlights follow:', 'This work extends our earlier study (https://t.co/FqvucnvVrB) on the transit light source (TLS) effect, or how the heterogeneity of stellar photospheres imprints spectral features in exoplanet transmission spectra, from M dwarfs to more solar-like stars. https://t.co/AUZokNWWzI', 'Compared to M dwarfs, FGK dwarfs show lower rotational variabilities, but these tend to increase for later spectral types. This suggests K dwarfs may have more heterogeneous photospheres than F dwarfs and therefore present more of a complication for transmission spectroscopy. https://t.co/UVFOf7ElnY', 'We looked at what covering fractions of magnetic active regions could account for these variabilities with a suite of models of rotating stellar photospheres. We successively added spots and faculae to the models and recorded their rotational variability in the Kepler bandpass. https://t.co/L2zcPhPRD0', 'In the case that the rotational variability is driven by cool starspots, we find that the observed variabilities of FGK dwarfs point to fairly small spot covering fractions:  ~0.1% for F dwarfs and ~1â2% for K dwarfs. https://t.co/mx14soG6q8', 'In the case that the rotational variability is driven by both spots and faculae, spot covering fractions are similar (reaching about ~2â4% for K dwarfs), and faculae are about 10x higher (in line with what we see on the Sun). https://t.co/T4D6JUZJHF', 'We calculated the effect of these magnetic active regions on transmission spectra from 0.05 to 5.5 Âµm. The largest effects are apparent at the shortest wavelengths. Unocculted spots increase transit depths and unocculted faculae decrease them. https://t.co/W6SGByW3uf', 'In the case of a 1% transit depth, we find that unocculted spots can produce detectable (&gt; 30 ppm) slopes in transmission spectra for typical K dwarfs. On the other hand, models with unocculted spots and faculae produce detectable decreases in transit depth for G and K dwarfs. https://t.co/LtKi08eMeA', 'But when you just look at wavelengths of molecular features in planetary atmospheres (generally in the NIR), the TLS effect is much less prevalent.  Assuming a 1% transit depth, no stellar features at wavelengths of interest for CH4, CO, CO2, H2O, N2O, O2, or O3 are &gt; 30 ppm. https://t.co/IGuEPRREVG', 'The most notable changes are at wavelengths of interest for TiO and VO. These molecules can be in both hot-Jupiter and K-dwarf atmospheres. We find âactiveâ K dwarfs (with Kepler variabilities 1Ï above the median) can impart detectable TiO/VO features in transmission spectra. https://t.co/SqZA0sKRPf', 'Taking the long view, we look at how the TLS effect may impact transmission spectra for an Earth-Sun analog. The TLS signals scale with transit depth (84 ppm, in this case), and so we find that they will only be &lt;20 ppbâan order of magnitude smaller than the planetary features. https://t.co/wLtSqoliCO', 'In total, we find that the TLS effect is less of an issue in FGK-dwarf systems than M-dwarf systems. While it can be more problematic for (1) later spectral types, (2) more active hosts, and (3) observations at shorter wavelengths.', 'While care is warranted when dealing with active host stars, in general our findings bode well for high-precision observations of FGK systems with current and near-future facilities, like @NASAWebb.']",https://arxiv.org/abs/1812.06184,"Transmission spectra probe exoplanetary atmospheres, but they can also be strongly affected by heterogeneities in host star photospheres through the transit light source effect. Here we build upon our recent study of the effects of unocculted spots and faculae on M-dwarf transmission spectra, extending the analysis to FGK dwarfs. Using a suite of rotating model photospheres, we explore spot and facula covering fractions for varying activity levels and the associated stellar contamination spectra. Relative to M dwarfs, we find that the typical variabilities of FGK dwarfs imply lower spot covering fractions, though they generally increase with later spectral types, from $\sim 0.1\%$ for F dwarfs to 2-4$\%$ for late-K dwarfs. While the stellar contamination spectra are considerably weaker than those for typical M dwarfs, we find that typically active G and K dwarfs produce visual slopes that are detectable in high-precision transmission spectra. We examine line offsets at H$\alpha$ and the Na and K doublets and find that unocculted faculae in K dwarfs can appreciably alter transit depths around the Na D doublet. We find that band-averaged transit depth offsets at molecular bands for CH$_{4}$, CO, CO$_{2}$, H$_{2}$O, N$_{2}$O, O$_{2}$, and O$_{3}$ are not detectable for typically active FGK dwarfs, though stellar TiO/VO features are potentially detectable for typically active late-K dwarfs. Generally, this analysis shows that inactive FGK dwarfs do not produce detectable stellar contamination features in transmission spectra, though active FGK host stars can produce such features and care is warranted in interpreting transmission spectra from these systems. ","The Transit Light Source Effect II: The Impact of Stellar Heterogeneity
  on Transmission Spectra of Planets Orbiting Broadly Sun-like Stars"
29,1074921923019067394,901661341122912257,Yihui Quek,"['Excited to announce a new paper in which we (together with Stanislav FoÅt @stanislavfort and Hui Khoon Ng) machine learn quantum states directly from data, speeding up the processing in Bayesian adaptive tomography by a factor of up to a million: <LINK>']",https://arxiv.org/abs/1812.06693,"Quantum State Tomography is the task of determining an unknown quantum state by making measurements on identical copies of the state. Current algorithms are costly both on the experimental front -- requiring vast numbers of measurements -- as well as in terms of the computational time to analyze those measurements. In this paper, we address the problem of analysis speed and flexibility, introducing \textit{Neural Adaptive Quantum State Tomography} (NA-QST), a machine learning based algorithm for quantum state tomography that adapts measurements and provides orders of magnitude faster processing while retaining state-of-the-art reconstruction accuracy. Our algorithm is inspired by particle swarm optimization and Bayesian particle-filter based adaptive methods, which we extend and enhance using neural networks. The resampling step, in which a bank of candidate solutions -- particles -- is refined, is in our case learned directly from data, removing the computational bottleneck of standard methods. We successfully replace the Bayesian calculation that requires computational time of $O(\mathrm{poly}(n))$ with a learned heuristic whose time complexity empirically scales as $O(\log(n))$ with the number of copies measured $n$, while retaining the same reconstruction accuracy. This corresponds to a factor of a million speedup for $10^7$ copies measured. We demonstrate that our algorithm learns to work with basis, symmetric informationally complete (SIC), as well as other types of POVMs. We discuss the value of measurement adaptivity for each POVM type, demonstrating that its effect is significant only for basis POVMs. Our algorithm can be retrained within hours on a single laptop for a two-qubit situation, which suggests a feasible time-cost when extended to larger systems. It can also adapt to a subset of possible states, a choice of the type of measurement, and other experimental details. ",Adaptive Quantum State Tomography with Neural Networks
30,1074887565788893184,14544467,Daniel Apai,['Our new paper led by @benrackham on the transit light source effect: how does stellar heterogeneity contaminate spectra of transiting planets? Now for FGK-type stars. TiO absorption? Spec. Slopes? NaI+KI? Halpha? Biosignature contamination? Check out TLSE2!<LINK>'],http://arxiv.org/abs/1812.06184,"Transmission spectra probe exoplanetary atmospheres, but they can also be strongly affected by heterogeneities in host star photospheres through the transit light source effect. Here we build upon our recent study of the effects of unocculted spots and faculae on M-dwarf transmission spectra, extending the analysis to FGK dwarfs. Using a suite of rotating model photospheres, we explore spot and facula covering fractions for varying activity levels and the associated stellar contamination spectra. Relative to M dwarfs, we find that the typical variabilities of FGK dwarfs imply lower spot covering fractions, though they generally increase with later spectral types, from $\sim 0.1\%$ for F dwarfs to 2-4$\%$ for late-K dwarfs. While the stellar contamination spectra are considerably weaker than those for typical M dwarfs, we find that typically active G and K dwarfs produce visual slopes that are detectable in high-precision transmission spectra. We examine line offsets at H$\alpha$ and the Na and K doublets and find that unocculted faculae in K dwarfs can appreciably alter transit depths around the Na D doublet. We find that band-averaged transit depth offsets at molecular bands for CH$_{4}$, CO, CO$_{2}$, H$_{2}$O, N$_{2}$O, O$_{2}$, and O$_{3}$ are not detectable for typically active FGK dwarfs, though stellar TiO/VO features are potentially detectable for typically active late-K dwarfs. Generally, this analysis shows that inactive FGK dwarfs do not produce detectable stellar contamination features in transmission spectra, though active FGK host stars can produce such features and care is warranted in interpreting transmission spectra from these systems. ","The Transit Light Source Effect II: The Impact of Stellar Heterogeneity
  on Transmission Spectra of Planets Orbiting Broadly Sun-like Stars"
31,1074884655793373184,767659609,Yoshihiko Hasegawa,['Our new paper has appeared in arXiv <LINK>'],https://arxiv.org/abs/1812.06620,"Reliable methods for obtaining time-dependent solutions of Langevin equations are in high demand in the field of non-equilibrium theory. In this paper, we present a new method based on variational superposed Gaussian approximation (VSGA) and Pad\'e approximant. The VSGA obtains time-dependent probability density functions as a superposition of multiple Gaussian distributions. However, a limitation of the VSGA is that the expectation of the drift term with respect to the Gaussian distribution should be calculated analytically, which is typically satisfied when the drift term is a polynomial function. When this condition is not met, the VSGA must rely on the numerical integration of the expectation at each step, resulting in huge computational cost. We propose an augmented VSGA (A-VSGA) method that effectively overcomes the limitation of the VSGA by approximating non-linear functions with the Pad\'e approximant. We apply the A-VSGA to two systems driven by chaotic input signals, a stochastic genetic regulatory system and a soft bistable system, whose drift terms are a rational polynomial function and a hyperbolic tangent function, respectively. The numerical calculations show that the proposed method can provide accurate results with less temporal cost than that required for Monte Carlo simulation. ","Augmented Variational Superposed Gaussian Approximation for Langevin
  Equations with Rational Polynomial Functions"
32,1074843081420218369,40285266,Stanislav Fort at EAGx Prague Â¬(ð¥ðð¥ð),['I am excited to announce our new paper on adaptive #quantum state tomography using neural nets (<LINK>) with Yihui Quek @quekpottheories (equal contributions) and Hui Khoon Ng. We use #MachineLearning to learn to learn about #quantum states directly from data. <LINK>'],https://arxiv.org/abs/1812.06693,"Quantum State Tomography is the task of determining an unknown quantum state by making measurements on identical copies of the state. Current algorithms are costly both on the experimental front -- requiring vast numbers of measurements -- as well as in terms of the computational time to analyze those measurements. In this paper, we address the problem of analysis speed and flexibility, introducing \textit{Neural Adaptive Quantum State Tomography} (NA-QST), a machine learning based algorithm for quantum state tomography that adapts measurements and provides orders of magnitude faster processing while retaining state-of-the-art reconstruction accuracy. Our algorithm is inspired by particle swarm optimization and Bayesian particle-filter based adaptive methods, which we extend and enhance using neural networks. The resampling step, in which a bank of candidate solutions -- particles -- is refined, is in our case learned directly from data, removing the computational bottleneck of standard methods. We successfully replace the Bayesian calculation that requires computational time of $O(\mathrm{poly}(n))$ with a learned heuristic whose time complexity empirically scales as $O(\log(n))$ with the number of copies measured $n$, while retaining the same reconstruction accuracy. This corresponds to a factor of a million speedup for $10^7$ copies measured. We demonstrate that our algorithm learns to work with basis, symmetric informationally complete (SIC), as well as other types of POVMs. We discuss the value of measurement adaptivity for each POVM type, demonstrating that its effect is significant only for basis POVMs. Our algorithm can be retrained within hours on a single laptop for a two-qubit situation, which suggests a feasible time-cost when extended to larger systems. It can also adapt to a subset of possible states, a choice of the type of measurement, and other experimental details. ",Adaptive Quantum State Tomography with Neural Networks
33,1074781053804535809,2439364105,Fei Wang,['Check out our new paper accepted to #AAAI19 on joint medical named entity recognition and normalization <LINK>'],https://arxiv.org/abs/1812.06081,"State-of-the-art studies have demonstrated the superiority of joint modelling over pipeline implementation for medical named entity recognition and normalization due to the mutual benefits between the two processes. To exploit these benefits in a more sophisticated way, we propose a novel deep neural multi-task learning framework with explicit feedback strategies to jointly model recognition and normalization. On one hand, our method benefits from the general representations of both tasks provided by multi-task learning. On the other hand, our method successfully converts hierarchical tasks into a parallel multi-task setting while maintaining the mutual supports between tasks. Both of these aspects improve the model performance. Experimental results demonstrate that our method performs significantly better than state-of-the-art approaches on two publicly available medical literature datasets. ","A Neural Multi-Task Learning Framework to Jointly Model Medical Named
  Entity Recognition and Normalization"
34,1074651961742495744,523241142,Juste Raimbault,"['New paper with @ClementineCttn, M. Le Texier, @flenechet and R. Reuillon: Space Matters: extending sensitivity analysis to initial spatial conditions in geosimulation models\n<LINK>\nusing #HPC with @OpenMOLE @ISCPIF. Open methodology soon included in @OpenMOLE']",https://arxiv.org/abs/1812.06008,"Although simulation models of geographical systems in general and agent-based models in particular represent a fantastic opportunity to explore socio-spatial behaviours and to test a variety of scenarios for public policy, the validity of generative models is uncertain unless their results are proven robust and representative of 'real-world' conditions. Sensitivity analysis usually includes the analysis of the effect of stochasticity on the variability of results, as well as the effects of small parameter changes. However, initial spatial conditions are usually not modified systematically in geographical models, thus leaving unexplored the effect of initial spatial arrangements on the interactions of agents with one another as well as with their environment. In this paper, we present a method to assess the effect of some initial spatial conditions on simulation models, using a systematic spatial configuration generator in order to create density grids with which spatial simulation models are initialised. We show, with the example of two classical agent-based models (Schelling's models of segregation and Sugarscape's model of unequal societies) and a straightforward open-source work-flow using high performance computing, that the effect of initial spatial arrangements is significant on the two models. Furthermore, this effect is sometimes larger than the effect of parameters' value change. ","Space Matters: extending sensitivity analysis to initial spatial
  conditions in geosimulation models"
35,1074488028956123141,2432886163,Oscar BarragÃ¡n,['Check our new paper on the discovery and characterization of another TESS planet: HD 219666 b!\n\n<LINK> <LINK>'],https://arxiv.org/abs/1812.05881,"We report on the confirmation and mass determination of a transiting planet orbiting the old and inactive G7 dwarf star HD219666 (Mstar = 0.92 +/- 0.03 MSun, Rstar = 1.03 +/- 0.03 RSun, tau_star = 10 +/- 2 Gyr). With a mass of Mb = 16.6 +/- 1.3 MEarth, a radius of Rb = 4.71 +/- 0.17 REarth, and an orbital period of P ~ 6 days, HD219666b is a new member of a rare class of exoplanets: the hot-Neptunes. The Transiting Exoplanet Survey Satellite (TESS) observed HD219666 (also known as TOI-118) in its Sector 1 and the light curve shows four transit-like events, equally spaced in time. We confirmed the planetary nature of the candidate by gathering precise radial-velocity measurements with HARPS@ESO3.6m. We used the co-added HARPS spectrum to derive the host star fundamental parameters (Teff = 5527 +/- 65 K, log g = 4.40 +/- 0.11 (cgs), [Fe/H]= 0.04 +/- 0.04 dex, log R'HK = -5.07 +/- 0.03), as well as the abundances of many volatile and refractory elements. The host star brightness (V = 9.9) makes it suitable for further characterisation by means of in-transit spectroscopy. The determination of the planet orbital obliquity, along with the atmospheric metal-to-hydrogen content and thermal structure could provide us with important clues on the formation mechanisms of this class of objects. ",HD219666b: A hot-Neptune from TESS Sector 1
36,1074484745172631552,4438354094,Tom Wong,"['New paper with @Creighton undergrad Mason Rhodes! For search by quantum walk, the regular complete bipartite graph behaves like the complete graph, right? Well, yes for the continuous-time quantum walk, but no for the discrete-time (coined) quantum walk. <LINK> <LINK>']",https://arxiv.org/abs/1812.06079,"The coined quantum walk is a discretization of the Dirac equation of relativistic quantum mechanics, and it is the basis of many quantum algorithms. We investigate how it searches the complete bipartite graph of $N$ vertices for one of $k$ marked vertices with different initial states. We prove intriguing dependence on the number of marked and unmarked vertices in each partite set. For example, when the graph is irregular and the initial state is the typical uniform superposition over the vertices, then the success probability can vary greatly from one timestep to the next, even alternating between 0 and 1, so the precise time at which measurement occurs is crucial. When the initial state is a uniform superposition over the edges, however, the success probability evolves smoothly. As another example, if the complete bipartite graph is regular, then the two initial states are equivalent. Then if two marked vertices are in the same partite set, the success probability reaches 1/2, but if they are in different partite sets, it instead reaches $1$. This differs from the complete graph, which is the quantum walk formulation of Grover's algorithm, where the success probability with two marked vertices is 8/9. This reveals a contrast to the continuous-time quantum walk, whose evolution is governed by Schr\""odinger's equation, which asymptotically searches the regular complete bipartite graph with any arrangement of marked vertices in the same manner as the complete graph. ",Quantum Walk Search on the Complete Bipartite Graph
37,1073643413797519361,820031736914513925,Fabrizio Leisen,"['New paper: ""A Loss-Based Prior for Gaussian Graphical Models"" written in collaboration with Laurentiu Hinoveanu and Cristiano Villa @cv60villa <LINK>']",http://arxiv.org/abs/1812.05531,"Gaussian graphical models play an important role in various areas such as genetics, finance, statistical physics and others. They are a powerful modelling tool which allows one to describe the relationships among the variables of interest. From the Bayesian perspective, there are two sources of randomness: one is related to the multivariate distribution and the quantities that may parametrise the model, the other has to do with the underlying graph, $G$, equivalent to describing the conditional independence structure of the model under consideration. In this paper, we propose a prior on G based on two loss components. One considers the loss in information one would incur in selecting the wrong graph, while the second penalises for large number of edges, favouring sparsity. We illustrate the prior on simulated data and on real datasets, and compare the results with other priors on $G$ used in the literature. Moreover, we present a default choice of the prior as well as discuss how it can be calibrated so as to reflect available prior information. ",A Loss-Based Prior for Gaussian Graphical Models
38,1073345088842018817,24859650,Jan-Willem van de Meent,"['New paper by my student @BabakEsmaeili10 on learning structured representations for reviews: <LINK>. We infer embeddings for users and items and model review text using a structured neural topic model, which infers groups of interpretable, related topics. <LINK>']",https://arxiv.org/abs/1812.05035,"We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that maps bag-of-words vectors for combined reviews associated with each paired user and item onto structured embeddings, which in turn define per-aspect topic weights. We model individual reviews in a structured manner by inferring an aspect assignment for each sentence in a given review, where the per-aspect topic weights obtained by the user-item encoder serve to define a mixture over topics, conditioned on the aspect. The result is an autoencoding neural topic model for reviews, which can be trained in a fully unsupervised manner to learn topics that are structured into aspects. Experimental evaluation on large number of datasets demonstrates that aspects are interpretable, yield higher coherence scores than non-structured autoencoding topic model variants, and can be utilized to perform aspect-based comparison and genre discovery. ",Structured Neural Topic Models for Reviews
39,1073254991601811457,78729443,Joe Hellerstein,"[""New CIDR '19 paper from my team at Berkeley on the promise and disappointments of current serverless computing offerings for programming the cloud. Overview at <LINK>, paper at <LINK>.""]",http://arxiv.org/abs/1812.03651,"Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. ","Serverless Computing: One Step Forward, Two Steps Back"
40,1073166280457375744,2676457430,MAGIC telescopes ð´ðº,"['A  new MAGIC paper has been accepted on MNRAS! The paper is focused on the supernova remnant SNR  G24.7+0.6A: in that complex region a new source, dubbed MAGIC J1835â069, has been discovered by MAGIC!check it out: <LINK> #MAGICpapers #theMAGICcollaboration <LINK>']",http://arxiv.org/abs/1812.04854,"SNR G24.7+0.6 is a 9.5 kyrs radio and $\gamma$-ray supernova remnant evolving in a dense medium. In the GeV regime, SNR G24.7+0.6 (3FHL\,J1834.1--0706e/FGES\,J1834.1--0706) shows a hard spectral index ($\Gamma$$\sim$2) up to $200$\,GeV, which makes it a good candidate to be observed with Cherenkov telescopes such as MAGIC. We observed the field of view of \snr\ with the MAGIC telescopes for a total of 31 hours. We detect very high energy $\gamma$-ray emission from an extended source located 0.34\degr\ away from the center of the radio SNR. The new source, named \mgc\ is detected up to 5\,TeV, and its spectrum is well-represented by a power-law function with spectral index of $2.74 \pm 0.08$. The complexity of the region makes the identification of the origin of the very-high energy emission difficult, however the spectral agreement with the LAT source and overlapping position at less than 1.5$\sigma$ point to a common origin. We analysed 8 years of \fermi-LAT data to extend the spectrum of the source down to 60\,MeV. \fermi-LAT and MAGIC spectra overlap within errors and the global broad band spectrum is described by a power-law with exponential cutoff at $1.9\pm0.5$\,TeV. The detected $\gamma$-ray emission can be interpreted as the results of proton-proton interaction between the supernova and the CO-rich surrounding. ","Discovery of TeV $\gamma$-ray emission from the neighbourhood of the
  supernova remnant G24.7+0.6 by MAGIC"
41,1073157291044757509,3313806489,Tim Roberts,"['***KLAXON***\nWe did a new catalogue paper, including ~ 400 ULXs.  Find it on\n\n<LINK>\n\nIf nothing else, it would make an excellent gift for your loved ones this Christmas(*).\n\n(* Disclaimer: it may not make an excellent gift for your loved ones this Christmas.)']",http://arxiv.org/abs/1812.04684,"We have created a new, clean catalogue of extragalactic non-nuclear X-ray sources by correlating the 3XMM-DR4 data release of the XMM-Newton Serendipitous Source Catalogue with the Third Reference Catalogue of Bright Galaxies and the Catalogue of Neighbouring Galaxies, using an improved version of the method presented in Walton et al. (2011). Our catalogue contains 1,314 sources, of which 384 are candidate ultraluminous X-ray sources (ULXs). The resulting catalogue improves upon previous catalogues in its handling of spurious detections by taking into account XMM-Newton quality flags. We estimate the contamination of ULXs by background sources to be 24 per cent. We define a 'complete' subsample as those ULXs in galaxies for which the sensitivity limit is below $10^{39}$ erg/s and use it to examine the hardness ratio properties between ULX and non-ULX sources, and ULXs in different classes of host galaxy. We find that ULXs have a similar hardness ratio distribution to lower-luminosity sources, consistent with previous studies. We also find that ULXs in spiral and elliptical host galaxies have similar distributions to each other independent of host galaxy morphology, however our results do support previous indications that the population of ULXs is more luminous in star-forming host galaxies than in non-star-forming galaxies. Our catalogue contains further interesting subpopulations for future study, including Eddington Threshold sources and highly variable ULXs. We also examine the highest-luminosity (L$_X$ > $5 \times 10^{40}$ erg/s) ULXs in our catalogue in search of intermediate-mass black hole candidates, and find nine new possible candidates. ","A new, clean catalogue of extragalactic non-nuclear X-ray sources in
  nearby galaxies"
42,1072906391889702912,190058865,Vikram Sreekanti,"['1/ New paper with @joe_hellerstein @alsched @cgwu0530 @jmfaleiro @jssmith @mejoeyg on pitfalls of existing serverless infra and future research directions: <LINK> (Paper will also be at #cidr2019.)', ""2/ Autoscaling &amp; managed function execution seems great at first blush, but existing infrastructure doesn't support fine-grained communication or data movement, which makes it terrible for distributed &amp; data systems."", ""3/ To make serverless suitable for a wider variety of applications (which we want to do) there's a ton of room for research &amp; innovation in systems, programming abstractions, hardware, etc."", ""4/ You can check out what we're working on here: https://t.co/PRqfNfyibQ""]",https://arxiv.org/abs/1812.03651,"Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. ","Serverless Computing: One Step Forward, Two Steps Back"
43,1072725398285377541,823277120944242689,Will Kinney,"['New paper out today. Fun idea: cosmology where the speed of sound is faster than the speed of light.\n\n<LINK> <LINK>', '@RobJLow You would think, but remarkably enough the answer appears to be no, at least classically. Babichev, Mukhanov, and Vikman wrote a wonderful paper exploring this question about ten tears ago:\n\nhttps://t.co/EdDfHkArsk', '@RobJLow The trick is that these are not tachyons, but a superluminal ether. This means that it it not possible, for example, to build a tachyonic antitelephone. (Or, in less technical terms, to go back in time and kill baby Hilter.)\n\nhttps://t.co/qshhUuqzqA', '@RobJLow Yes, exactly! Theories of this type can always be re-written as equivalent bimetric theories.', '@RobJLow One of the very intriguing things about these solutions is that at the moment of the Big Bang singularity, the speed of sound diverges, and the  effective metric becomes purely spacelike. That is, the 3+1 spacetime is reduced to an effective 3+0 spacetime.', '@RobJLow In this (somewhat narrow) sense, the time dimension of the spacetime is an emergent property.', ""@RobJLow I'm actually not completely positive whether or not tachyacoustic theories obey the Dominant Energy Condition in all cases. But then cosmologists violate the DEC three times before breakfast these days anyway. It's the Null Energy Condition that's a killer.""]",https://arxiv.org/abs/1812.04447,"Recent studies show that there is tension between the de Sitter swampland conjectures proposed by Obeid, et al. and inflationary cosmology. In this paper, we consider an alternative to inflation, `tachyacoustic' cosmology, in light of swampland conjectures. In tachyacoustic models, primordial perturbations are generated by a period of superluminal sound speed instead of accelerating expansion. We show that realizations of tachyacoustic Lagrangians can be consistent with the de Sitter swampland conjectures, and therefore can in principle be consistent with a UV-complete theory. We derive a general condition for models with $c_S > 1$ to be consistent with swampland conjectures. ","Consistency of Tachyacoustic Cosmology with de Sitter Swampland
  Conjectures"
44,1072708532233883648,865627769631264768,Jamie Tayar,"['New paper on the arXiv today with Keivan Stassun and @EnricoCors where we predict flicker and jitter for &gt;129,000 stars. <LINK> <LINK>', ""Both flicker and jitter are related to convection, and we knew that they could be predicted quite accurately with asteroseismic information (nu max, mass) and metallicity from @EnricoCors's 2017 paper on open cluster stars in the Kepler field."", 'Here we have two orders of magnitude more giants, and we add in dwarfs and subgiants. We \n1. update the initial asteroseismic relationship, \n2. calibrate a relationship between spectroscopic observables (logg, Teff, [Fe/H]) and flicker', '3. predict a flicker for 129,000 @APOGEEsurvey  stars\n4. used published flicker-jitter relations to predict a jitter for those same 129,000 stars', 'We hope these predicted jitters will be helpful to people looking for small (@NASA_TESS) planets, to estimate how much RV jitter a star will have before you start taking a lot of spectra, and you can either target quieter stars or carefully plan observations of noisier stars.', ""These flicker predictions can be checked with @NASA_TESS data, and hopefully we will motivate someone to publish an updated list of jitter values as a function of stellar type, since the current literature sample is a bit sparse. (yay falsifiability....but hopefully we're close) https://t.co/Dxt7TF2OvW""]",https://arxiv.org/abs/1812.04010,"Surface granulation can be predicted with the mass, metallicity, and frequency of maximum power of a star. Using the orders-of-magnitude larger APOGEE-Kepler sample, we recalibrate the relationship fit by Corsaro et al. (2017) for ""flicker"", an easier-to-compute diagnostic of this granulation. We find that the relationship between the stellar parameters and flicker is significantly different for dwarf and subgiant stars than it is for red giants. We also confirm a dependence of flicker amplitude on metallicity as seen originally by Corsaro et al. (2017), although the dependence found here is somewhat weaker. Using the same APOGEE-Kepler sample, we demonstrate that spectroscopic measurements alone provide sufficient information to estimate the flicker amplitude to 7 percent for giants, and 20 percent for dwarfs and subgiants. We provide a relationship that depends on effective temperature, surface gravity, and metallicity, and calculate predicted flicker values for 129,000 stars with APOGEE spectra. Finally, we use published relationships between flicker and radial velocity jitter to estimate minimum jitter values for these same 129,000 stars, and we identify stars whose total jitter is likely to be even larger than the granulation-driven jitter by virtue of large-amplitude photometric variability. ","Predicting Granulation ""Flicker"" and Radial Velocity ""Jitter"" from
  Spectroscopic Observables"
45,1072674955576393733,1004365363574902784,Kevin J. Kelly,['New paper out tonight with Yu-Dai Tsai! He and I looked into what you could say about minicharged particles if you put a scintillating detector in a neutrino beam. Have a look!\n\n<LINK> <LINK>'],https://arxiv.org/abs/1812.03998,"We propose a low-cost and movable setup to probe minicharged particles (or milli-charged particles) using high-intensity proton fixed-target facilities. This proposal, FerMINI, consists of a milliQan-type detector, requiring multi-coincident (nominally, triple-coincident) scintillation signatures within a small time window, located downstream of the proton target of a neutrino experiment. During the collisions of a large number of protons on the target, intense minicharged particle beams may be produced via meson photo-decays and Drell-Yan production. We take advantage of the high statistics, shielding, and potential neutrino-detector-related background reduction to search for minicharged particles in two potential sites: the MINOS near detector hall and the proposed DUNE near detector hall, both at Fermilab. We also explore several alternative designs, including the modifications of the nominal detector to increase signal yield, and combining this detector technology with existing and planned neutrino detectors to better search for minicharged particles. The CERN SPS beam and associated experimental structure also provide a similar alternative. FerMINI can achieve unprecedented sensitivity for minicharged particles in the MeV to few GeV regime with fractional charge $\varepsilon=Q_{\chi}/e $ between $10^{-4}$ (potentially saturating the detector limitation) and $10^{-1}$. ","Proton Fixed-Target Scintillation Experiment to Search for Minicharged
  Particles"
46,1072514666260299777,1874201814,Brian Cherinka,"['Check out @sdssurveys #dr15 and the new paper about @Marvin_SDSS,  <LINK>.  For more in the world of SDSS in astronomy, <LINK> and <LINK>.']",https://arxiv.org/abs/1812.03833,"The Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey, one of three core programs of the fourth-generation Sloan Digital Sky Survey (SDSS-IV), is producing a massive, high-dimensional integral field spectroscopic data set. However, leveraging the MaNGA data set to address key questions about galaxy formation presents serious data-related challenges due to the combination of its spatially inter-connected measurements and sheer volume. For each galaxy, the MaNGA pipelines produce relatively large data files to preserve the spatial correlations of the spectra and measurements, but this comes at the expense of storing the data set in a coarsely-chunked manner. The coarse chunking and total volume of the data make it time-consuming to download and curate locally-stored data. Thus, accessing, querying, visually exploring, and performing statistical analyses across the whole data set at a fine-grained scale is extremely challenging using just FITS files. To overcome these challenges, we have developed \marvin: a toolkit consisting of a Python package, Application Programming Interface (API), and web application utilizing a remote database. \marvin's robust and sustainable design minimizes maintenance, while facilitating user-contributed extensions such as high level analysis code. Finally, we are in the process of abstracting out \marvin's core functionality into a separate product so that it can serve as a foundation for others to develop \marvin-like systems for new science applications. ","Marvin: A Toolkit for Streamlined Access and Visualization of the
  SDSS-IV MaNGA Data Set"
47,1072513666317209601,932805374,KordingLab ð¦,"['The microprocessor paper with @stochastician highlighted problems in the logic of neuroscience (<LINK>). Our new eC paper with @neuroccino now delineates one of the problems, mistaken causal inference, very precisely (<LINK>).']",https://arxiv.org/abs/1812.03363,"As neuroscientists we want to understand how causal interactions or mechanisms within the brain give rise to perception, cognition, and behavior. It is typical to estimate interaction effects from measured activity using statistical techniques such as functional connectivity, Granger Causality, or information flow, whose outcomes are often falsely treated as revealing mechanistic insight. Since these statistical techniques fit models to low-dimensional measurements from brains, they ignore the fact that brain activity is high-dimensional. Here we focus on the obvious confound of common inputs: the countless unobserved variables likely have more influence than the few observed ones. Any given observed correlation can be explained by an infinite set of causal models that take into account the unobserved variables. Therefore, correlations within massively undersampled measurements tell us little about mechanisms. We argue that these mis-inferences of causality from correlation are augmented by an implicit redefinition of words that suggest mechanisms, such as connectivity, causality, and flow. ","The lure of misleading causal statements in functional connectivity
  research"
48,1072463096323719168,20164453,Mark Pearce,['A new PoGO+ paper on arXiv today! \u2066@SSCspace\u2069 \u2066@RymdstyrelsenSE\u2069 \u2066@KTHuniversity\u2069\n[1812.03244] PoGO+ polarimetric constraint on the synchrotron jet emission of Cygnus X-1 <LINK>'],https://arxiv.org/abs/1812.03244,"We report a polarimetric constraint on the hard X-ray synchrotron jet emission from the Cygnus X-1 black-hole binary system. The observational data were obtained using the PoGO+ hard X-ray polarimeter in July 2016, when Cygnus X-1 was in the hard state. We have previously reported that emission from an extended corona with a low polarization fraction is dominating, and that the polarization angle is perpendicular to the disk surface. In the soft gamma-ray regime, a highly-polarized synchrotron jet is reported with INTEGRAL observations. To constrain the polarization fraction and flux of such a jet component in the hard X-ray regime, we now extend analyses through vector calculations in the Stokes QU plane, where the dominant corona emission and the jet component are considered simultaneously. The presence of another emission component with different polarization angle could partly cancel out the net polarization. The 90% upper limit of the polarization fraction for the additional synchrotron jet component is estimated as <10%, <5%, and <5% for polarization angle perpendicular to the disk surface, parallel to the surface, and aligned with the emission reported by INTEGRAL data, respectively. From the 20-180 keV total flux of 2.6 x 10^-8 erg s^-1 cm^-2, the upper limit of the polarized flux is estimated as <3 x 10^-9 erg s^-1 cm^-2. ","PoGO+ polarimetric constraint on the synchrotron jet emission of Cygnus
  X-1"
49,1072431515164905472,2421923048,Anne-Marie Weijmans,['Lots of new @MaNGASurvey galaxies data and for the first time also @MaStar_library stellar spectra. More details in the @sdssurveys #dr15 overview paper: <LINK> <LINK>'],https://arxiv.org/abs/1812.02759,"Twenty years have passed since first light for the Sloan Digital Sky Survey (SDSS). Here, we release data taken by the fourth phase of SDSS (SDSS-IV) across its first three years of operation (July 2014-July 2017). This is the third data release for SDSS-IV, and the fifteenth from SDSS (Data Release Fifteen; DR15). New data come from MaNGA - we release 4824 datacubes, as well as the first stellar spectra in the MaNGA Stellar Library (MaStar), the first set of survey-supported analysis products (e.g. stellar and gas kinematics, emission line, and other maps) from the MaNGA Data Analysis Pipeline (DAP), and a new data visualisation and access tool we call ""Marvin"". The next data release, DR16, will include new data from both APOGEE-2 and eBOSS; those surveys release no new data here, but we document updates and corrections to their data processing pipelines. The release is cumulative; it also includes the most recent reductions and calibrations of all data taken by SDSS since first light. In this paper we describe the location and format of the data and tools and cite technical references describing how it was obtained and processed. The SDSS website (www.sdss.org) has also been updated, providing links to data downloads, tutorials and examples of data use. While SDSS-IV will continue to collect astronomical data until 2020, and will be followed by SDSS-V (2020-2025), we end this paper by describing plans to ensure the sustainability of the SDSS data archive for many years beyond the collection of data. ","The Fifteenth Data Release of the Sloan Digital Sky Surveys: First
  Release of MaNGA Derived Quantities, Data Visualization Tools and Stellar
  Library"
50,1072174184313249792,60893773,James Bullock,"[""Be it therefore resolved - @coralrosew 's new paper presenting cosmological sims of dwarf galaxies w 30 Msun (!) resolution. <LINK>\n\nOne cool result: Mstar~1.e5 is a divide between star-forming dwarfs (blue) and ancient/quenched ultra-faint galaxies (green) <LINK>""]",https://arxiv.org/abs/1812.02749,"We study a suite of extremely high-resolution cosmological FIRE simulations of dwarf galaxies ($M_{\rm halo} \lesssim 10^{10}$$M_{\odot}$), run to $z=0$ with $30 M_{\odot}$ resolution, sufficient (for the first time) to resolve the internal structure of individual supernovae remnants within the cooling radius. Every halo with $M_{\rm halo} \gtrsim 10^{8.6} M_{\odot}$ is populated by a resolved {\em stellar} galaxy, suggesting very low-mass dwarfs may be ubiquitous in the field. Our ultra-faint dwarfs (UFDs; $M_{\ast}<10^{5}\,M_{\odot}$) have their star formation truncated early ($z\gtrsim2$), likely by reionization, while classical dwarfs ($M_{\ast}>10^{5} M_{\odot}$) continue forming stars to $z<0.5$. The systems have bursty star formation (SF) histories, forming most of their stars in periods of elevated SF strongly clustered in both space and time. This allows our dwarf with $M_{\ast}/M_{\rm halo} > 10^{-4}$ to form a dark matter core $>200$pc, while lower-mass UFDs exhibit cusps down to $\lesssim100$pc, as expected from energetic arguments. Our dwarfs with $M_{\ast}>10^{4}\,M_{\odot}$ have half-mass radii ($R_{\rm 1/2}$) in agreement with Local Group (LG) dwarfs; dynamical mass vs. $R_{1/2}$ and the degree of rotational support also resemble observations. The lowest-mass UFDs are below surface brightness limits of current surveys but are potentially visible in next-generation surveys (e.g. LSST). The stellar metallicities are lower than in LG dwarfs; this may reflect pre-enrichment of the LG by the massive hosts or Pop-III stars. Consistency with lower resolution studies implies that our simulations are numerically robust (for a given physical model). ","Be it therefore resolved: Cosmological Simulations of Dwarf Galaxies
  with Extreme Resolution"
51,1072093607958638592,3263851731,PPhDechant,"[""Two things out on the @arxiv today: @PPhDechant s paper on ADE correspondences <LINK> and Yang-Hui He's new book The Calabi-Yau Landscape: from Geometry, to Physics, to #MachineLearning <LINK> #STEMatYSJ <LINK>""]",https://arxiv.org/abs/1812.02804,"In this paper we present novel $ADE$ correspondences by combining an earlier induction theorem of ours with one of Arnold's observations concerning Trinities, and the McKay correspondence. We first extend Arnold's indirect link between the Trinity of symmetries of the Platonic solids $(A_3, B_3, H_3)$ and the Trinity of exceptional 4D root systems $(D_4, F_4, H_4)$ to an explicit Clifford algebraic construction linking the two ADE sets of root systems $(I_2(n), A_1\times I_2(n), A_3, B_3, H_3)$ and $(I_2(n), I_2(n)\times I_2(n), D_4, F_4, H_4)$. The latter are connected through the McKay correspondence with the ADE Lie algebras $(A_n, D_n, E_6, E_7, E_8)$. We show that there are also novel indirect as well as direct connections between these ADE root systems and the new ADE set of root systems $(I_2(n), A_1\times I_2(n), A_3, B_3, H_3)$, resulting in a web of three-way ADE correspondences between three ADE sets of root systems. ","From the Trinity $(A_3, B_3, H_3)$ to an ADE correspondence"
52,1072004338078253056,50939814,Coral Wheeler,"['Tired of simulated ultra-faint âgalaxiesâ that are just a few star particles? Check out my new paper w @PFHopkins_Astro, Andrew Pace, @SheaGKosmo, @MBKplus, @AndrewWetzel, @jbprime &amp; FIRE team where we resolve UFDs (M* &lt; 10^5 Msun) w/ &gt;100 star particles!\n\n<LINK> <LINK>', 'These are the highest res cosmological hydro sims ever run to z=0 (mbar = 30Msun; gsoft &lt; 0.4pc). For the 1st time, we resolve the internal structure of individual SNe remnants within cooling rad. At this res, predictions for SN fdbk become independent of numerical implementation', ""We confirm many previous results at lower res (which is always nice):\nWe form *well-resolved* (&gt;100 star particles) glx in all halos w/ Vmax &gt; 13 km/s, suggesting they're ubiquitous in the field. But if there are many 100s of additional UFDs near the MW, why haven't we seen them?"", ""These obj have very low SB (32 mag/arcsec^2) May only be visible w next-gen surveys/LSST. Interestingly, if you restrict higher mass sim'd UFDs to central region w SB &gt; 30 mag/arcsec^2, you reproduce DES dwfs in M*-rhalf plane. Are we missing low SB stellar halos around DES dwfs? https://t.co/AY6gvuJk72"", ""We also confirm prev results that dwarf glx don't rotate  (v/sigma &lt; 0.5 in most cases). Our FIRE-II dwarfs have marginally more rot than FIRE-I, but still don't reproduce the faster rotators like Aquar/Leo A. However, we need more sims at &gt; mass to know if there is a discrepancy https://t.co/6gjKMvLWMn"", 'We also look at new things!\n- Massive dwarf forms a core (~300pc) \n- Although no UFDs form large cores, some have ~100pc cores. This is important! When we say that UFDs ""don\'t form cores"" in sims we mean ""not large cores"" - discovery of small cores in UFDs not a problem for LCDM! https://t.co/luRuHCt4g9', ""For the 1st time, we investigate the extremely low-mass end of the MZR in well-resolved simulated glx. We find a huge discrepancy in [Fe/H] between @evannkirby's work and FIRE sims at M* &lt;10^5 (less but still significant at 10^5&lt;M*/Msun&lt;10^7). https://t.co/nKh9gh89fK"", ""The difference is likely some combination of: a) the presence of a massive host near the obs'd UFDs (20% of gas around massive MW enriched to observed levels by z~2), b) lack of Pop-III enrichment, or c) the need for more detailed progenitor mass and metallicity dependent\nyields."", 'In a future paper we will push down the strict particle # requirement and investigate the low-mass limit of galaxy formation. For now, feel free to let me know if there was anything you ever wanted to know about UFDs that required sims with more than a few particles per glx!']",https://arxiv.org/abs/1812.02749,"We study a suite of extremely high-resolution cosmological FIRE simulations of dwarf galaxies ($M_{\rm halo} \lesssim 10^{10}$$M_{\odot}$), run to $z=0$ with $30 M_{\odot}$ resolution, sufficient (for the first time) to resolve the internal structure of individual supernovae remnants within the cooling radius. Every halo with $M_{\rm halo} \gtrsim 10^{8.6} M_{\odot}$ is populated by a resolved {\em stellar} galaxy, suggesting very low-mass dwarfs may be ubiquitous in the field. Our ultra-faint dwarfs (UFDs; $M_{\ast}<10^{5}\,M_{\odot}$) have their star formation truncated early ($z\gtrsim2$), likely by reionization, while classical dwarfs ($M_{\ast}>10^{5} M_{\odot}$) continue forming stars to $z<0.5$. The systems have bursty star formation (SF) histories, forming most of their stars in periods of elevated SF strongly clustered in both space and time. This allows our dwarf with $M_{\ast}/M_{\rm halo} > 10^{-4}$ to form a dark matter core $>200$pc, while lower-mass UFDs exhibit cusps down to $\lesssim100$pc, as expected from energetic arguments. Our dwarfs with $M_{\ast}>10^{4}\,M_{\odot}$ have half-mass radii ($R_{\rm 1/2}$) in agreement with Local Group (LG) dwarfs; dynamical mass vs. $R_{1/2}$ and the degree of rotational support also resemble observations. The lowest-mass UFDs are below surface brightness limits of current surveys but are potentially visible in next-generation surveys (e.g. LSST). The stellar metallicities are lower than in LG dwarfs; this may reflect pre-enrichment of the LG by the massive hosts or Pop-III stars. Consistency with lower resolution studies implies that our simulations are numerically robust (for a given physical model). ","Be it therefore resolved: Cosmological Simulations of Dwarf Galaxies
  with Extreme Resolution"
53,1071519307123843072,116832314,Andreas Damianou,"['Sebastian at #NeurIPS2018 meta-learning workshop, presenting the work we did during his internship @amazon. A new view  on meta-learning to scale beyond few-shot learning. Full paper: <LINK> with @flennerhag , P. Moreno, @lawrennd <LINK>']",https://arxiv.org/abs/1812.01054,"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps. ",Transferring Knowledge across Learning Processes
54,1071138587914133504,90520434,Mike Stay,['Our paper about our new consensus algorithm #Casanova just hit the arXiv: <LINK>. Now to build a proof-of-stake blockchain that uses it. With Kyle Butt and Derek Sorensen. @pyrofex'],https://arxiv.org/abs/1812.02232,"We introduce Casanova, a leaderless optimistic consensus protocol designed for a permissioned blockchain. Casanova produces blocks in a DAG rather than a chain, and combines voting rounds with block production by singling out individual conflicting transactions. ",Casanova
55,1071106229446025216,44873859,Thomas Unterthiner,"['Our paper on evaluating generative models for video is out! A cool new metric and new datasets to drive progress in this area: <LINK>\n\nWith @vansteenkiste_s, @karol_kurach, @RaphaelMarinier, @sylvain_gelly and other wonderful people at Google Brain Zurich.']",https://arxiv.org/abs/1812.01717,"Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV. ",Towards Accurate Generative Models of Video: A New Metric & Challenges
56,1071035142888718336,1601296094,David Bowler,"['This new paper, led by experimental colleagues in Geneva, features a set of exquisite experiments to study of the surface diffusion potential for Mn on Si(001) measured by AFM and compared to DFT <LINK>']",https://arxiv.org/abs/1812.02512,"The surface diffusion potential landscape plays an essential role in a number of physical and chemical processes such as self-assembly and catalysis. Diffusion energy barriers can be calculated theoretically for simple systems, but there is currently no experimental technique to systematically measure them on the relevant atomic length scale. Here, we introduce an atomic force microscopy based method to semiquantitatively map the surface diffusion potential on an atomic length scale. In this proof of concept experiment, we show that the atomic force microscope damping signal at constant frequency-shift can be linked to nonconservative processes associated with the lowering of energy barriers and compared with calculated single-atom diffusion energy barriers. ",Towards surface diffusion potential mapping on atomic length scale
57,1071006863263719424,187743747,Andrea Lattuada,['Weâve put the current version of our new differential dataflow paper âK-Pg: Shared State in Differential Dataflowsâ on arXiv. I feel privileged to have worked on this with @frankmcsherry and @ms705!\n\n<LINK>'],http://arxiv.org/abs/1812.02639,"Current systems for data-parallel, incremental processing and view maintenance over high-rate streams isolate the execution of independent queries. This creates unwanted redundancy and overhead in the presence of concurrent incrementally maintained queries: each query must independently maintain the same indexed state over the same input streams, and new queries must build this state from scratch before they can begin to emit their first results. This paper introduces shared arrangements: indexed views of maintained state that allow concurrent queries to reuse the same in-memory state without compromising data-parallel performance and scaling. We implement shared arrangements in a modern stream processor and show order-of-magnitude improvements in query response time and resource consumption for interactive queries against high-throughput streams, while also significantly improving performance in other domains including business analytics, graph processing, and program analysis. ","Shared Arrangements: practical inter-query sharing for streaming
  dataflows"
58,1070702916393156610,50343115,Thomas Kipf,"['CompILE discovers composable segments &amp; encodings of behavior from sequential data (unsupervised and differentiable). Codes can be recomposed to facilitate generalization and exploration in RL.\n\nNew work with collaborators from @DeepMindAI\nPaper: <LINK> <LINK>', ""If you're around at #NeurIPS2018, consider stopping by at our talk at the Learning By Instruction (LBI) Workshop, this Saturday at 10:00 -- https://t.co/T90GsZDz3q, or ping me if you want to have a chat!""]",https://arxiv.org/abs/1812.01483,"We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate CompILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsupervised manner. Latent codes and associated behavior policies discovered by CompILE can be used by a hierarchical agent, where the high-level policy selects actions in the latent code space, and the low-level, task-specific policies are simply the learned decoders. We found that our CompILE-based agent could learn given only sparse rewards, where agents without task-specific policies struggle. ",CompILE: Compositional Imitation Learning and Execution
59,1070676203655303171,31413346,Leonardo N. Ferreira,"['Our new paper accepted for the #ACMSAC19 ""From spatio-temporal data to chronological networks: An application to wildfire analysis"" <LINK>  @didiervega @frankmoshe <LINK>']",https://arxiv.org/abs/1812.01646,"Network theory has established itself as an appropriate tool for complex systems analysis and pattern recognition. In the context of spatiotemporal data analysis, correlation networks are used in the vast majority of works. However, the Pearson correlation coefficient captures only linear relationships and does not correctly capture recurrent events. This missed information is essential for temporal pattern recognition. In this work, we propose a chronological network construction process that is capable of capturing various events. Similar to the previous methods, we divide the area of study into grid cells and represent them by nodes. In our approach, links are established if two consecutive events occur in two different nodes. Our method is computationally efficient, adaptable to different time windows and can be applied to any spatiotemporal data set. As a proof-of-concept, we evaluated the proposed approach by constructing chronological networks from the MODIS dataset for fire events in the Amazon basin. We explore two data analytic approaches: one static and another temporal. The results show some activity patterns on the fire events and a displacement phenomenon over the year. The validity of the analyses in this application indicates that our data modeling approach is very promising for spatio-temporal data mining. ","From spatio-temporal data to chronological networks: An application to
  wildfire analysis"
60,1070565176930295808,142358412,Jaiyam Sharma,['Our paper on improved approximate nearest neighbors (ANN) retrieval inspired by fruit flies is now on Arxiv. We propose a new ANN algorithm which works much better than its biological counterpart and offer extensive theoretical insights #NeurIPS2018 <LINK>'],https://arxiv.org/abs/1812.01844,"We propose a new class of data-independent locality-sensitive hashing (LSH) algorithms based on the fruit fly olfactory circuit. The fundamental difference of this approach is that, instead of assigning hashes as dense points in a low dimensional space, hashes are assigned in a high dimensional space, which enhances their separability. We show theoretically and empirically that this new family of hash functions is locality-sensitive and preserves rank similarity for inputs in any `p space. We then analyze different variations on this strategy and show empirically that they outperform existing LSH methods for nearest-neighbors search on six benchmark datasets. Finally, we propose a multi-probe version of our algorithm that achieves higher performance for the same query time, or conversely, that maintains performance of prior approaches while taking significantly less indexing time and memory. Overall, our approach leverages the advantages of separability provided by high-dimensional spaces, while still remaining computationally efficient ","Improving Similarity Search with High-dimensional Locality-sensitive
  Hashing"
61,1070314638309392385,973860795246198784,Z.Wei,"[""Our new paper is on the arXiv (with TS &amp; TT)\n<LINK>\nIt's about three local quenches (local operator quench, splitting/joining quench) in CFT, analysis using entanglement density, geometry of their gravity duals and a proposal of circuits made from s/j quenches.""]",https://arxiv.org/abs/1812.01176,"We study three different types of local quenches (local operator, splitting and joining) in both the free fermion and holographic CFTs in two dimensions. We show that the computation of a quantity called entanglement density, provides a systematic method to capture essential properties of local quenches. This allows us to clearly understand the differences between the free and holographic CFTs as well as the distinctions between three local quenches. We also analyze holographic geometries of splitting/joining local quenches using the AdS/BCFT prescription. We show that they are essentially described by time evolutions of boundary surfaces in the bulk AdS. We find that the logarithmic time evolution of entanglement entropy arises from the region behind the Poincare horizon as well as the evolutions of boundary surfaces. In the CFT side, our analysis of entanglement density suggests such a logarithmic growth is due to initial non-local quantum entanglement just after the quench. Finally, by combining our results, we propose a new class of gravity duals, which are analogous to quantum circuits or tensor networks such as MERA, based on the AdS/BCFT construction. ",Holographic Quantum Circuits from Splitting/Joining Local Quenches
62,1070286204506177536,2601869406,Michael A Osborne,"['Problem: you would like to integrate something that can be evaluated in parallel (e.g. in marginalising hyperparameters by evaluating a batch of likelihoods).\n\nOur solution: batch Bayesian quadrature! Our new arXiv paper spills the beans.\n\n<LINK> <LINK>', 'Footnote: we particularly target non-negative integrands.', ""@davidjayharris @mcmc_stan We have not, but thanks for the excellent suggestion! We'll try and set up some experiments: that is, I'll suggest this to the amazing Ed and Saad :)""]",https://arxiv.org/abs/1812.01553,"Integration over non-negative integrands is a central problem in machine learning (e.g. for model averaging, (hyper-)parameter marginalisation, and computing posterior predictive distributions). Bayesian Quadrature is a probabilistic numerical integration technique that performs promisingly when compared to traditional Markov Chain Monte Carlo methods. However, in contrast to easily-parallelised MCMC methods, Bayesian Quadrature methods have, thus far, been essentially serial in nature, selecting a single point to sample at each step of the algorithm. We deliver methods to select batches of points at each step, based upon those recently presented in the Batch Bayesian Optimisation literature. Such parallelisation significantly reduces computation time, especially when the integrand is expensive to sample. ",Batch Selection for Parallelisation of Bayesian Quadrature
63,1070234908000485376,523241142,Juste Raimbault,['New paper: Evolving accessibility landscapes: mutations of transportation networks in China <LINK> [featuring qualitative fieldwork]'],https://arxiv.org/abs/1812.01473,"Recent years have witnessed an exceptional extension of public transportation networks in People's Republic of China, both at a national scale with the construction of the first HSR railway network of the world, and at local scales with numerous cities developing high coverage subway networks often from scratch. This chapter studies these mutations, both from a quantitative perspective with the study of the evolution of population accessibility landscapes, at a national level and for several cities, and from a qualitative perspective with fieldwork observations. We confirm that rebalancing planning objectives are well achieved in terms of accessibility at both scales, when all planned lines will be achieved but already in a significant manner. We finally hypothesize possible paths for the coupled network-territory systems, given the extent without precedent of such mutations. ","Evolving accessibility landscapes: mutations of transportation networks
  in China"
64,1070214196141400064,242844365,Kevin Schawinski,"['New paper! @Dennis_Turp, @DS3Lab and I use generative models to test hypotheses in galaxy evolution. <LINK> <LINK>']",https://arxiv.org/abs/1812.01114,"Context. Generative models open up the possibility to interrogate scientific data in a more data-driven way. Aims: We propose a method that uses generative models to explore hypotheses in astrophysics and other areas. We use a neural network to show how we can independently manipulate physical attributes by encoding objects in latent space. Methods: By learning a latent space representation of the data, we can use this network to forward model and explore hypotheses in a data-driven way. We train a neural network to generate artificial data to test hypotheses for the underlying physical processes. Results: We demonstrate this process using a well-studied process in astrophysics, the quenching of star formation in galaxies as they move from low-to high-density environments. This approach can help explore astrophysical and other phenomena in a way that is different from current methods based on simulations and observations. ",Exploring galaxy evolution with generative models
65,1070144193362870272,954465907539152897,Dr. Doctor,"['New paper from our team analyzing our Dark Energy Camera data from optical follow-up of GW170814! Check it out!\n<LINK>', ""Spoiler: We didn't find any associated candidates, but we have done the most comprehensive search for optical emission from binary-black-hole mergers in terms of gravitational-wave sky-map coverage!""]",http://arxiv.org/abs/1812.01579,"Binary black hole (BBH) mergers found by the LIGO and Virgo detectors are of immense scientific interest to the astrophysics community, but are considered unlikely to be sources of electromagnetic emission. To test whether they have rapidly fading optical counterparts, we used the Dark Energy Camera to perform an $i$-band search for the BBH merger GW170814, the first gravitational wave detected by three interferometers. The 87-deg$^2$ localization region (at 90\% confidence) centered in the Dark Energy Survey (DES) footprint enabled us to image 86\% of the probable sky area to a depth of $i\sim 23$ mag and provide the most comprehensive dataset to search for EM emission from BBH mergers. To identify candidates, we perform difference imaging with our search images and with templates from pre-existing DES images. The analysis strategy and selection requirements were designed to remove supernovae and to identify transients that decline in the first two epochs. We find two candidates, each of which is spatially coincident with a star or a high-redshift galaxy in the DES catalogs, and they are thus unlikely to be associated with GW170814. Our search finds no candidates associated with GW170814, disfavoring rapidly declining optical emission from BBH mergers brighter than $i\sim 23$ mag ($L_{\rm optical} \sim 5\times10^{41}$ erg/s) 1-2 days after coalescence. In terms of GW sky map coverage, this is the most complete search for optical counterparts to BBH mergers to date ","A Search for Optical Emission from Binary-Black-Hole Merger GW170814
  with the Dark Energy Camera"
66,1070133813320200194,508659617,Adrien Gaidon,"['And here is our new paper: Learning to Fuse Things and Stuff <LINK> Actually an improved version from the ECCV challenge, called TASCNet where we propose a Things and Stuff Consistency loss for end-to-end panoptic segmentation with a single model! <LINK>']",https://arxiv.org/abs/1812.01192,"We propose an end-to-end learning approach for panoptic segmentation, a novel task unifying instance (things) and semantic (stuff) segmentation. Our model, TASCNet, uses feature maps from a shared backbone network to predict in a single feed-forward pass both things and stuff segmentations. We explicitly constrain these two output distributions through a global things and stuff binary mask to enforce cross-task consistency. Our proposed unified network is competitive with the state of the art on several benchmarks for panoptic segmentation as well as on the individual semantic and instance segmentation tasks. ",Learning to Fuse Things and Stuff
67,1069985100127862786,20957355,Steven Tremblay,"['A paper describing a new #pulsar modelling tool that works from the geometry up, PSRGEOM. Software is linked below as well. This work is led by one of my outstanding students at @CurtinUni , Sammy McSweeney.\n<LINK>\n<LINK>\n<LINK>']",https://arxiv.org/abs/1812.00526,"The phenomenon of subpulse drifting offers unique insights into the emission geometry of pulsars, and is commonly interpreted in terms of a rotating carousel of ""spark"" events near the stellar surface. We develop a detailed geometric model for the emission columns above a carousel of sparks that is entirely calculated in the observer's inertial frame, and which is consistent with the well-understood rotational effects of aberration and retardation. We explore the observational consequences of the model, including (1) the appearance of the reconstructed beam pattern via the cartographic transform and (2) the morphology of drift bands and how they might evolve as a function of frequency. The model, which is implemented in the software package PSRGEOM, is applicable to a wide range of viewing geometries, and we illustrate its implications using PSRs B0809+74 and B2034+19 as examples. Some specific predictions are made with respect to the difference between subpulse evolution and microstructure evolution, which provides a way to further test our model. ","On the Geometry of Curvature Radiation and Implications for Subpulse
  Drifting"
68,1069801265930989569,2425754287,Roman Orus,['New paper out: Topological order on the Bloch sphere <LINK>'],https://arxiv.org/abs/1812.00671,"A Bloch sphere is the geometrical representation of an arbitrary two-dimensional Hilbert space. Possible classes of entanglement and separability for the pure and mixed states on the Bloch sphere were suggested by [M. Boyer, R. Liss, T. Mor, PRA 95, 032308 (2017)]. Here we construct a Bloch sphere for the Hilbert space spanned by one of the ground states of Kitaev's toric code model and one of its closest product states. We prove that this sphere contains only one separable state, thus belonging to the fourth class suggested by the said paper. We furthermore study the topological order of the pure states on its surface and conclude that, according to conventional definitions, only one state (the toric code ground state) seems to present non-trivial topological order. We conjecture that most of the states on this Bloch sphere are neither ``trivial'' states (namely, they cannot be generated from a product state using a trivial circuit) nor topologically ordered. In addition, we show that the whole setting can be understood in terms of Grover rotations with gauge symmetry, akin to the quantum search algorithm. ",Topological order on the Bloch sphere
69,1073063838814154753,321794593,JosÃ© G. FernÃ¡ndez-Trincado,"['Check out our new APOGEE paper published today on ArXiv: ""APOGEE [C/N] Abundances Across the Galaxy ...""<LINK> -- Great Sten!', '""However, there have been discoveries of chemically anomalous N-enhanced field stars in the APOGEE data (see, e.g., FernÃ¡ndez-Trincado et al. 2016, 2017; Schiavon et al. 2017), which might show up in our sample as low-[C/N] stars...', '""These stars are few in number (11 found in the disk), and generally exhibit [Fe/H] &lt; â0.5, so their contribution to the gradient and migration analysis is likely insignificant.""']",https://arxiv.org/abs/1812.05092,"We present [C/N]-[Fe/H] abundance trends from the SDSS-IV Apache Point Observatory Galactic Evolution Experiment (APOGEE) survey, Data Release 14 (DR14), for red giant branch stars across the Milky Way Galaxy (MW, 3 kpc $<$ R $<$ 15 kpc). The carbon-to-nitrogen ratio (often expressed as [C/N]) can indicate the mass of a red giant star, from which an age can be inferred. Using masses and ages derived by Martig et al., we demonstrate that we are able to interpret the DR14 [C/N]-[Fe/H] abundance distributions as trends in age-[Fe/H] space. Our results show that an anti-correlation between age and metallicity, which is predicted by simple chemical evolution models, is not present at any Galactic zone. Stars far from the plane ($|$Z$|$ $>$ 1 kpc) exhibit a radial gradient in [C/N] ($\sim$ $-$0.04 dex/kpc). The [C/N] dispersion increases toward the plane ($\sigma_{[C/N]}$ = 0.13 at $|$Z$|$ $>$ 1 kpc to $\sigma_{[C/N]}$ = 0.18 dex at $|$Z$|$ $<$ 0.5 kpc). We measure a disk metallicity gradient for the youngest stars (age $<$ 2.5 Gyr) of $-$0.060 dex/kpc from 6 kpc to 12 kpc, which is in agreement with the gradient found using young CoRoGEE stars by Anders et al. Older stars exhibit a flatter gradient ($-$0.016 dex/kpc), which is predicted by simulations in which stars migrate from their birth radii. We also find that radial migration is a plausible explanation for the observed upturn of the [C/N]-[Fe/H] abundance trends in the outer Galaxy, where the metal-rich stars are relatively enhanced in [C/N]. ","APOGEE [C/N] Abundances Across the Galaxy: Migration and Infall from Red
  Giant Ages"
70,1079931577990172673,1030366876453371904,Rowan McAllister,"['How should robots react to strange new observations x? We find projecting out-of-distribution x to uncertain in-distribution x using generative models can improve already-trained models, e.g. collision predictors. With Gregory Kahn, @jeffclune, @svlevine: <LINK> <LINK>']",https://arxiv.org/abs/1812.10687,"Deep learning provides a powerful tool for machine perception when the observations resemble the training data. However, real-world robotic systems must react intelligently to their observations even in unexpected circumstances. This requires a system to reason about its own uncertainty given unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but can struggle with out-of-distribution observations. Generative models can in principle detect out-of-distribution observations as those with a low estimated density. However, the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty to cope with uncertainty stemming from out-of-distribution states. Our method estimates an uncertainty measure about the model's prediction, taking into account an explicit (generative) model of the observation distribution to handle out-of-distribution inputs. This is accomplished by probabilistically projecting observations onto the training distribution, such that out-of-distribution inputs map to uncertain in-distribution observations, which in turn produce uncertain task-related predictions, but only if task-relevant parts of the image change. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our method of projecting out-of-distribution observations improves the performance of four standard Bayesian and non-Bayesian neural network approaches, offering more favorable trade-offs between the proportion of time a robot can remain autonomous and the proportion of impending crashes successfully avoided. ","Robustness to Out-of-Distribution Inputs via Task-Aware Generative
  Uncertainty"
71,1078325428308377601,3132910891,Delia Milliron,['How strongly absorbing are semiconductor infrared plasmonic NCs and how can we find intrinsic properties from ensemble spectra? Now on the arXiv: <LINK>'],https://arxiv.org/abs/1812.10142,"The optical extinction spectra arising from localized surface plasmon resonance in doped semiconductor nanocrystals (NCs) have intensities and lineshapes determined by free charge carrier concentrations and the various mechanisms for damping the oscillation of those free carriers. However, these intrinsic properties are convoluted by heterogeneous broadening when measuring spectra of ensembles. We reveal that the traditional Drude approximation is not equipped to fit spectra from a heterogeneous ensemble of doped semiconductor NCs and produces fit results that violate Mie scattering theory. The heterogeneous ensemble Drude approximation (HEDA) model rectifies this issue by accounting for ensemble heterogeneity and near-surface depletion. The HEDA model is applied to tin-doped indium oxide NCs for a range of sizes and doping levels but we expect it can be employed for any isotropic plasmonic particles in the quasistatic regime. It captures individual NC optical properties and their contributions to the ensemble spectra thereby enabling the analysis of intrinsic NC properties from an ensemble measurement. Quality factors for the average NC in each ensemble are quantified and found to be notably higher than those of the ensemble. Carrier mobility and conductivity derived from HEDA fits matches that measured in the bulk thin film literature. ","Intrinsic Optical and Electronic Properties from Quantitative Analysis
  of Plasmonic Semiconductor Nanocrystal Ensemble Optical Extinction"
72,1077124805994917888,261865146,Dr Sofia Qvarfort,['Another thing to add to the Christmas celebrations this year: a paper fresh on the arXiv! <LINK> We study nonlinearities in optomechanical systems. Merry Christmas everyone! ð²'],https://arxiv.org/abs/1812.08874,"We study the non-Gaussian character of quantum optomechanical systems evolving under the fully nonlinear optomechanical Hamiltonian. By using a measure of non-Gaussianity based on the relative entropy of an initially Gaussian state, we quantify the amount of non-Gaussianity induced by both a constant and time-dependent cubic light-matter coupling and study its general and asymptotic behaviour. We find analytical approximate expressions for the measure of non-Gaussianity and show that initial thermal phonon occupation of the mechanical element does not significantly impact the non-Gaussianity. More importantly, we also show that it is possible to continuously increase the amount of non-Gaussianity of the state by driving the light-matter coupling at the frequency of mechanical resonance, suggesting a viable mechanism for increasing the non-Gaussianity of optomechanical systems even in the presence of noise. ","Enhanced continuous generation of non-Gaussianity through optomechanical
  modulation"
73,1075984468337483777,847986180,Fred Jendrzejewski,['And a little christmas present as we propose a quantized refrigerator based on ultracold quantum mixtures. #quantumthermo \n<LINK>'],https://arxiv.org/abs/1812.08474,"We propose to implement a quantized thermal machine based on a mixture of two atomic species. One atomic species implements the working medium and the other implements two (cold and hot) baths. We show that such a setup can be employed for the refrigeration of a large bosonic cloud starting above and ending below the condensation threshold. We analyze its operation in a regime conforming to the quantized Otto cycle and discuss the prospects for continuous-cycle operation, addressing the experimental as well as theoretical limitations. Beyond its applicative significance, this setup has a potential for the study of fundamental questions of quantum thermodynamics. ",Quantized refrigerator for an atomic cloud
74,1075603767112851457,2896282038,Vijay Varma,"['Looking for some Christmas reading? We built a new surrogate model for aligned-spin binary black hole waveforms. Head over to <LINK> to find out why it is the greatest model in all the land! Learn more abt surrogates at <LINK>, see teaser below. <LINK>', ""P.S. This work was done in collaboration with several members of the @SXSProject.\nP.P.S. I'm clearly biased, there are other great models as well ð, some of which are mentioned in the paper."", '@mattkenworthy Glad you liked it! ð']",http://arxiv.org/abs/1812.07865,"Numerical relativity (NR) simulations provide the most accurate binary black hole gravitational waveforms, but are prohibitively expensive for applications such as parameter estimation. Surrogate models of NR waveforms have been shown to be both fast and accurate. However, NR-based surrogate models are limited by the training waveforms' length, which is typically about 20 orbits before merger. We remedy this by hybridizing the NR waveforms using both post-Newtonian and effective one body waveforms for the early inspiral. We present NRHybSur3dq8, a surrogate model for hybridized nonprecessing numerical relativity waveforms, that is valid for the entire LIGO band (starting at $20~\text{Hz}$) for stellar mass binaries with total masses as low as $2.25\,M_{\odot}$. We include the $\ell \leq 4$ and $(5,5)$ spin-weighted spherical harmonic modes but not the $(4,1)$ or $(4,0)$ modes. This model has been trained against hybridized waveforms based on 104 NR waveforms with mass ratios $q\leq8$, and $|\chi_{1z}|,|\chi_{2z}| \leq 0.8$, where $\chi_{1z}$ ($\chi_{2z}$) is the spin of the heavier (lighter) BH in the direction of orbital angular momentum. The surrogate reproduces the hybrid waveforms accurately, with mismatches $\lesssim 3\times10^{-4}$ over the mass range $2.25M_{\odot} \leq M \leq 300 M_{\odot}$. At high masses ($M\gtrsim40M_{\odot}$), where the merger and ringdown are more prominent, we show roughly two orders of magnitude improvement over existing waveform models. We also show that the surrogate works well even when extrapolated outside its training parameter space range, including at spins as large as 0.998. Finally, we show that this model accurately reproduces the spheroidal-spherical mode mixing present in the NR ringdown signal. ","Surrogate model of hybridized numerical relativity binary black hole
  waveforms"
75,1075237010086883328,1339581511,Ben Rackham,"['New paper on WASP-4b from Alex Bixel and the ACCESS team (<LINK>): <LINK>\n\nWe find a featureless visual transmission spectrum for this hot Jupiter orbiting a G7 dwarf, w/o a strong slope, Na, K, or imprints of stellar spectral features.']",http://arxiv.org/abs/1812.07177,"We present an optical transmission spectrum of the atmosphere of WASP-4b obtained through observations of four transits with Magellan/IMACS. Using a Bayesian approach to atmospheric retrieval, we find no evidence for scattering or absorption features in our transit spectrum. Our models include a component to model the transit light source effect (spectral contamination from unocculted spots on the stellar photosphere), which we show can have a marked impact on the observed transmission spectrum for reasonable spot covering fractions (< 5%); this is the first such analysis for WASP-4b. We are also able to fit for the size and temperature contrast of spots observed during the second and third transits, finding evidence for both small, cool and large, warm spot-like features on the photosphere. Finally, we compare our results to those published by Huitson et al. (2017) using Gemini/GMOS and May et al. (2018) using IMACS, and find that our data are in agreement. ","ACCESS: Ground-based Optical Transmission Spectroscopy of the Hot
  Jupiter WASP-4b"
76,1074881464511156224,1023681782363901952,Michal P. Heller,"['We are very happy to share this recent study of #holographic Schwinger-Keldysh effective #fieldtheories, achieved by a collaboration involving our very own Michal P. Heller! Check it out here: [<LINK>] #physics #GQFI #quantum <LINK>']",https://arxiv.org/abs/1812.06093,"We construct a holographic dual of the Schwinger-Keldysh effective action for the dissipative low-energy dynamics of relativistic charged matter at strong coupling in a fixed thermal background. To do so, we use a mixed signature bulk spacetime whereby an eternal asymptotically anti-de Sitter black hole is glued to its Euclidean counterpart along an initial time slice in a way to match the desired double-time contour of the dual field theory. Our results are consistent with existing literature and can be regarded as a fully-ab initio derivation of a Schwinger-Keldysh effective action. In addition, we provide a simple infrared effective action for the near horizon region that drives all the dissipation and can be viewed as an alternative to the membrane paradigm approximation. ",Holographic Schwinger-Keldysh effective field theories
77,1074847023696736256,23104038,Dr Katie Grasha,"[""The single hardest thing I've ever done my entire life is finally done with.\n\nWe combine the GMC catalog from the PAWS survey with the LEGUS star cluster catalogs in the Whirlpool galaxy M51. \n\nWe find a few things:\n\n<LINK>"", '(1) We find that star clusters remain associated with their birth GMCs for about 4-6 Myr. This is a longer timescale than in NGC7793 (Grasha+18, MNRAS, 481, 1016). A result of the higher surface density in M51, which constrains the winds (feedback), thus increasing the timescale.', '(2) The correlation function of the GMCs are significantly flatter than that of the star clusters. When we tune the star formation efficiency of the GMCs until the correlation function resembles that of the star clusters, we find that the GMCs have a SFE of just a few percent.', 'Thus, only the most massive GMCs are capable of forming the young, massive star clusters we observe. \n\nhttps://t.co/Tg1G1xTe8k', 'Nothing can describe the work that went into this paper\n\nThe final, published paper is dramatically different than what I had originally submitted. I encourage people to look up chapter 5 of my dissertation, the paper in its original submitted form. \n\nhttps://t.co/ox6HCU1chG']",https://arxiv.org/abs/1812.06109,"We present a study correlating the spatial locations of young star clusters with those of molecular clouds in NGC~5194, in order to investigate the timescale over which clusters separate from their birth clouds. The star cluster catalogues are from the Legacy ExtraGalactic UV Survey (LEGUS) and the molecular clouds from the Plateau de Bure Interefrometer Arcsecond Whirpool Survey (PAWS). We find that younger star clusters are spatially closer to molecular clouds than older star clusters. The median ages for clusters associated with clouds is 4~Myr whereas it is 50~Myr for clusters that are sufficiently separated from a molecular cloud to be considered unassociated. After $\sim$6~Myr, the majority of the star clusters lose association with their molecular gas. Younger star clusters are also preferentially located in stellar spiral arms where they are hierarchically distributed in kpc-size regions for 50-100~Myr before dispersing. The youngest star clusters are more strongly clustered, yielding a two-point correlation function with $\alpha=-0.28\pm0.04$, than the GMCs ($\alpha=-0.09\pm0.03$) within the same PAWS field. However, the clustering strength of the most massive GMCs, supposedly the progenitors of the young clusters for a star formation efficiency of a few percent, is comparable ($\alpha=-0.35\pm0.05$) to that of the clusters. We find a galactocentric-dependence for the coherence of star formation, in which clusters located in the inner region of the galaxy reside in smaller star-forming complexes and display more homogeneous distributions than clusters further from the centre. This result suggests a correlation between the survival of a cluster complex and its environment. ","The Spatial Relation between Young Star Clusters and Molecular Clouds in
  M 51 with LEGUS"
78,1073172667602272257,84822240,Luca Bertinetto ð®ð¹ ðªðº ð,"['Check out our new work! We propose a simple method to tackle both tasks of semi-supervised video object segmentation *and* visual object tracking, online and at 35 fps.\n \n<LINK>\n\n<LINK>']",https://arxiv.org/abs/1812.05050,"In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is this http URL ",Fast Online Object Tracking and Segmentation: A Unifying Approach
79,1073157291044757509,3313806489,Tim Roberts,"['***KLAXON***\nWe did a new catalogue paper, including ~ 400 ULXs.  Find it on\n\n<LINK>\n\nIf nothing else, it would make an excellent gift for your loved ones this Christmas(*).\n\n(* Disclaimer: it may not make an excellent gift for your loved ones this Christmas.)']",http://arxiv.org/abs/1812.04684,"We have created a new, clean catalogue of extragalactic non-nuclear X-ray sources by correlating the 3XMM-DR4 data release of the XMM-Newton Serendipitous Source Catalogue with the Third Reference Catalogue of Bright Galaxies and the Catalogue of Neighbouring Galaxies, using an improved version of the method presented in Walton et al. (2011). Our catalogue contains 1,314 sources, of which 384 are candidate ultraluminous X-ray sources (ULXs). The resulting catalogue improves upon previous catalogues in its handling of spurious detections by taking into account XMM-Newton quality flags. We estimate the contamination of ULXs by background sources to be 24 per cent. We define a 'complete' subsample as those ULXs in galaxies for which the sensitivity limit is below $10^{39}$ erg/s and use it to examine the hardness ratio properties between ULX and non-ULX sources, and ULXs in different classes of host galaxy. We find that ULXs have a similar hardness ratio distribution to lower-luminosity sources, consistent with previous studies. We also find that ULXs in spiral and elliptical host galaxies have similar distributions to each other independent of host galaxy morphology, however our results do support previous indications that the population of ULXs is more luminous in star-forming host galaxies than in non-star-forming galaxies. Our catalogue contains further interesting subpopulations for future study, including Eddington Threshold sources and highly variable ULXs. We also examine the highest-luminosity (L$_X$ > $5 \times 10^{40}$ erg/s) ULXs in our catalogue in search of intermediate-mass black hole candidates, and find nine new possible candidates. ","A new, clean catalogue of extragalactic non-nuclear X-ray sources in
  nearby galaxies"
80,1070565176930295808,142358412,Jaiyam Sharma,['Our paper on improved approximate nearest neighbors (ANN) retrieval inspired by fruit flies is now on Arxiv. We propose a new ANN algorithm which works much better than its biological counterpart and offer extensive theoretical insights #NeurIPS2018 <LINK>'],https://arxiv.org/abs/1812.01844,"We propose a new class of data-independent locality-sensitive hashing (LSH) algorithms based on the fruit fly olfactory circuit. The fundamental difference of this approach is that, instead of assigning hashes as dense points in a low dimensional space, hashes are assigned in a high dimensional space, which enhances their separability. We show theoretically and empirically that this new family of hash functions is locality-sensitive and preserves rank similarity for inputs in any `p space. We then analyze different variations on this strategy and show empirically that they outperform existing LSH methods for nearest-neighbors search on six benchmark datasets. Finally, we propose a multi-probe version of our algorithm that achieves higher performance for the same query time, or conversely, that maintains performance of prior approaches while taking significantly less indexing time and memory. Overall, our approach leverages the advantages of separability provided by high-dimensional spaces, while still remaining computationally efficient ","Improving Similarity Search with High-dimensional Locality-sensitive
  Hashing"
81,1070133813320200194,508659617,Adrien Gaidon,"['And here is our new paper: Learning to Fuse Things and Stuff <LINK> Actually an improved version from the ECCV challenge, called TASCNet where we propose a Things and Stuff Consistency loss for end-to-end panoptic segmentation with a single model! <LINK>']",https://arxiv.org/abs/1812.01192,"We propose an end-to-end learning approach for panoptic segmentation, a novel task unifying instance (things) and semantic (stuff) segmentation. Our model, TASCNet, uses feature maps from a shared backbone network to predict in a single feed-forward pass both things and stuff segmentations. We explicitly constrain these two output distributions through a global things and stuff binary mask to enforce cross-task consistency. Our proposed unified network is competitive with the state of the art on several benchmarks for panoptic segmentation as well as on the individual semantic and instance segmentation tasks. ",Learning to Fuse Things and Stuff
82,1080505518450446337,57571700,Taha Yasseri,"['New preprint: \n""We apply the classifier to a dataset of 109,488 tweets produced by far right Twitter accounts during 2017 and found that weak Islamophobia is more prevalent (36,963 tweets) than strong (14,895 tweets).""\n<LINK> <LINK>']",https://arxiv.org/abs/1812.10400,"Islamophobic hate speech on social media inflicts considerable harm on both targeted individuals and wider society, and also risks reputational damage for the host platforms. Accordingly, there is a pressing need for robust tools to detect and classify Islamophobic hate speech at scale. Previous research has largely approached the detection of Islamophobic hate speech on social media as a binary task. However, the varied nature of Islamophobia means that this is often inappropriate for both theoretically-informed social science and effectively monitoring social media. Drawing on in-depth conceptual work we build a multi-class classifier which distinguishes between non-Islamophobic, weak Islamophobic and strong Islamophobic content. Accuracy is 77.6% and balanced accuracy is 83%. We apply the classifier to a dataset of 109,488 tweets produced by far right Twitter accounts during 2017. Whilst most tweets are not Islamophobic, weak Islamophobia is considerably more prevalent (36,963 tweets) than strong (14,895 tweets). Our main input feature is a gloVe word embeddings model trained on a newly collected corpus of 140 million tweets. It outperforms a generic word embeddings model by 5.9 percentage points, demonstrating the importan4ce of context. Unexpectedly, we also find that a one-against-one multi class SVM outperforms a deep learning algorithm. ",Detecting weak and strong Islamophobic hate speech on social media
83,1074015648492216327,4916956920,Prajwel Joseph,"['A bar that serves no alcohol, but gas\n\nWe looked at some old pictures of the bar of M95 and found that it was moving huge amounts of gas to the galactic centre.\n\nOur letter: <LINK>\nImage: Spitzer 3.6 micron, HII (yellow), HI (grey) <LINK>']",https://arxiv.org/abs/1812.04178,"The physical processes related to the effect of bar in the quenching of star formation in the region between the nuclear/central sub-kpc region and the ends of the bar (bar-region) of spiral galaxies is not fully understood. It is hypothesized that the bar can either stabilize the gas against collapse, inhibiting star formation or efficiently consume all the available gas, with no fuel for further star formation. We present a multi-wavelength study using the archival data of an early-type barred spiral galaxy, Messier 95, which shows signatures of suppressed star formation in the bar-region. Using the optical, ultraviolet, infrared, CO and HI imaging data we study the pattern of star formation progression, stellar/gas distribution and try to provide insights on the process responsible for the observed pattern. The FUV$-$NUV pixel colour map reveals a cavity devoid of UV flux in the bar-region that interestingly matches with the length of the bar ($\sim$ 4.2kpc). The central nuclear region of the galaxy is showing a blue color clump and along the major-axis of the stellar bar the colour progressively becomes redder. Based on a comparison to single stellar population models, we show that the region of galaxy along the major-axis of the bar (unlike the region outside the bar) is comprised of stellar populations with ages $\geq$ 350 Myr, with a star-forming clump in the center of younger ages ($\sim$ 150Myr). Interestingly the bar-region is also devoid of neutral and molecular hydrogen but with an abundant molecular hydrogen present at the nuclear region of the galaxy. Our results are consistent with a picture in which the stellar bar in Messier 95 is redistributing the gas by funneling gas inflows to nuclear region, thus making the bar-region devoid of fuel for star formation. ","Insights on bar quenching from a multi-wavelength analysis: The case of
  Messier 95"
