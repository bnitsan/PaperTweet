,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,994296594425106433,500950174,Laure Soulier,"['Check out our new #sigir2018 long paper on cross-modal retrieval in the cooking context. <LINK> \nWith @micaelccarvalho, @RemiCadene, D. Picard, @thomenicolas1, @quobbe <LINK>']",http://arxiv.org/abs/1804.11146,"Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases. ","Cross-Modal Retrieval in the Cooking Context: Learning Semantic
  Text-Image Embeddings"
1,991757596746043392,123421220,Yvette Cendes,"['Just read this new paper that found several stars in hyper-velocity mode, going at over 1,000 km/s!  These are thought to be companions flung out by giant supernova explosions. Really now, how could you NOT like astronomy after hearing this stuff?! :D <LINK>']",https://arxiv.org/abs/1804.11163,"Double detonations in double white dwarf (WD) binaries undergoing unstable mass transfer have emerged in recent years as one of the most promising Type Ia supernova (SN Ia) progenitor scenarios. One potential outcome of this ""dynamically driven double-degenerate double-detonation"" (D^6) scenario is that the companion WD survives the explosion and is flung away with a velocity equal to its > 1000 km/s pre-SN orbital velocity. We perform a search for these hypervelocity runaway WDs using Gaia's second data release. In this paper, we discuss seven candidates followed up with ground-based instruments. Three sources are likely to be some of the fastest known stars in the Milky Way, with total Galactocentric velocities between 1000 and 3000 km/s, and are consistent with having previously been companion WDs in pre-SN Ia systems. However, although the radial velocity of one of the stars is > 1000 km/s, the radial velocities of the other two stars are puzzlingly consistent with 0. The combined five-parameter astrometric solutions from Gaia and radial velocities from follow-up spectra yield tentative 6D confirmation of the D^6 scenario. The past position of one of these stars places it within a faint, old SN remnant, further strengthening the interpretation of these candidates as hypervelocity runaways from binary systems that underwent SNe Ia. ","Three Hypervelocity White Dwarfs in Gaia DR2: Evidence for Dynamically
  Driven Double-Degenerate Double-Detonation Type Ia Supernovae"
2,991636581067968512,449236360,Antoine Cully,['Our new paper ‚ÄúHierarchical Behavioral Repertoires with Unsupervised Descriptors‚Äù is now on ArXiv: <LINK> and Youtube: <LINK> It shows how a robot learns to draw digits in an unsupervised manner and how to transfer this knowledge to another robot <LINK>'],https://arxiv.org/abs/1804.07127,"Enabling artificial agents to automatically learn complex, versatile and high-performing behaviors is a long-lasting challenge. This paper presents a step in this direction with hierarchical behavioral repertoires that stack several behavioral repertoires to generate sophisticated behaviors. Each repertoire of this architecture uses the lower repertoires to create complex behaviors as sequences of simpler ones, while only the lowest repertoire directly controls the agent's movements. This paper also introduces a novel approach to automatically define behavioral descriptors thanks to an unsupervised neural network that organizes the produced high-level behaviors. The experiments show that the proposed architecture enables a robot to learn how to draw digits in an unsupervised manner after having learned to draw lines and arcs. Compared to traditional behavioral repertoires, the proposed architecture reduces the dimensionality of the optimization problems by orders of magnitude and provides behaviors with a twice better fitness. More importantly, it enables the transfer of knowledge between robots: a hierarchical repertoire evolved for a robotic arm to draw digits can be transferred to a humanoid robot by simply changing the lowest layer of the hierarchy. This enables the humanoid to draw digits although it has never been trained for this task. ",Hierarchical Behavioral Repertoires with Unsupervised Descriptors
3,991348190245982208,991338306481909760,Artem Sevastopolsky üá∫üá¶,"[""Stack-U-Net: Refinement Network for Image Segmentation on the Example of Optic Disc and Cup\nOur new paper that shows that stacking mÃ∂oÃ∂rÃ∂eÃ∂ Ã∂lÃ∂aÃ∂yÃ∂eÃ∂rÃ∂sÃ∂ U-Net's in the refining manner can be very beneficial for segmentation, even with small datasets. \n<LINK> <LINK>""]",https://arxiv.org/abs/1804.11294,"In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature. ","Stack-U-Net: Refinement Network for Image Segmentation on the Example of
  Optic Disc and Cup"
4,991162315541135360,33113669,Tim Baldwin,"['New paper with Fei Liu and @trevorcohn on memory-augmented model for targeted aspect-based sentiment analysis, to better handle long-distance dependencies between entities and their aspects:  <LINK> #nlproc #naacl2018']",https://arxiv.org/abs/1804.11019,"While neural networks have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) --- extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects --- remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external ""memory chains"" with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases. ","Recurrent Entity Networks with Delayed Memory Update for Targeted
  Aspect-based Sentiment Analysis"
5,991130042418708480,1030693296,Nicolas Martin,"['.@kmalhan07 led our first #GaiaDR2 paper, with R. Ibata. Search for streams in the Milky Way halo. And there they are! This shows stars likely in a stream between 5-15 kpc. Some spurious ones, some known ones (GD-1!) but some very obvious new streams too! <LINK> <LINK>']",https://arxiv.org/abs/1804.11339,"We present a panoramic map of the stellar streams of the Milky Way based upon astrometric and photometric measurements from the Gaia DR2 catalogue. In this first contribution, we concentrate on the halo at heliocentric distances beyond 5 kpc, and at Galactic latitudes $|b|>30\deg$, using the STREAMFINDER algorithm to detect structures along plausible orbits that are consistent with the Gaia proper motion measurements. We find a rich network of criss-crossing streams in the halo. Some of these structures were previously-known, several are new discoveries, but others are potentially artefacts of the Gaia scanning law and will require confirmation. With these initial discoveries, we are starting to unravel the complex formation of the halo of our Galaxy. ","Ghostly Tributaries to the Milky Way: Charting the Halo's Stellar
  Streams with the Gaia DR2 catalogue"
6,990943681602453504,4438354094,Tom Wong,"[""Here's how our new paper <LINK> fits with my existing research. We use degenerate perturbation theory to analyze continuous-time quantum walks searching Kronecker graphs. The numbers correspond to the papers on my website <LINK>. <LINK>""]",https://arxiv.org/abs/1804.10560,"Kronecker graphs, obtained by repeatedly performing the Kronecker product of the adjacency matrix of an ""initiator"" graph with itself, have risen in popularity in network science due to their ability to generate complex networks with real-world properties. In this paper, we explore spatial search by continuous-time quantum walk on Kronecker graphs. Specifically, we give analytical proofs for quantum search on first-, second-, and third-order Kronecker graphs with the complete graph as the initiator, showing that search takes Grover's $O(\sqrt{N})$ time. Numerical simulations indicate that higher-order Kronecker graphs with the complete initiator also support optimal quantum search. ",Quantum Walk Search on Kronecker Graphs
7,990939758795911168,4438354094,Tom Wong,"['New paper, and a new collaboration, with W√ºnscher, @JoshLockhart, and Severini @UCLQuantum. The first in a series on quantum walks and Kronecker graphs. <LINK> <LINK>']",https://arxiv.org/abs/1804.10560,"Kronecker graphs, obtained by repeatedly performing the Kronecker product of the adjacency matrix of an ""initiator"" graph with itself, have risen in popularity in network science due to their ability to generate complex networks with real-world properties. In this paper, we explore spatial search by continuous-time quantum walk on Kronecker graphs. Specifically, we give analytical proofs for quantum search on first-, second-, and third-order Kronecker graphs with the complete graph as the initiator, showing that search takes Grover's $O(\sqrt{N})$ time. Numerical simulations indicate that higher-order Kronecker graphs with the complete initiator also support optimal quantum search. ",Quantum Walk Search on Kronecker Graphs
8,990934517526523904,1524323209,Lenz Lab,"[""Spinning makes active fluids more stable! Ananyo's new paper now on arXiv - <LINK>""]",http://arxiv.org/abs/1804.09994,"Active hydrodynamic theories are a powerful tool to study the emergent ordered phases of internally driven particles such as bird flocks, bacterial suspension and their artificial analogues. While theories of orientationally ordered phases are by now well established, the effect of chirality on these phases is much less studied. In this paper, we present the first complete dynamical theory of orientationally ordered chiral particles in two-dimensional incompressible systems. We show that phase-coherent states of rotating chiral particles are remarkably stable in both momentum-conserved and non-conserved systems in contrast to their non-rotating counterparts. Furthermore, defect separation -- which drives chaotic flows in non-rotating active fluids -- is suppressed by intrinsic rotation of chiral active particles. We thus establish chirality as a source of dramatic stabilization in active systems, which could be key in interpreting the collective behaviours of some biological tissues, cytoskeletal systems and collections of bacteria. ",Spontaneous rotation can stabilise ordered chiral active fluids
9,990793583883042817,930224996785332224,"Jacob White, PhD",['Check out my new paper on measuring the radio emission of A-type stars! This work has important ramifications for characterizing debris disks (think exo-asteroid belts) so that we can more accurately study the amount and distribution of debris around stars\n<LINK>'],https://arxiv.org/abs/1804.10206,"In the early stages of planet formation, small dust grains grow to become mm sized particles in debris disks around stars. These disks can in principle be characterized by their emission at submillimeter and millimeter wavelengths. Determining both the occurrence and abundance of debris in unresolved circumstellar disks of A-type main-sequence stars requires that the stellar photospheric emission be accurately modeled. To better constrain the photospheric emission for such systems, we present observations of Sirius A, an A-type star with no known debris, from the JCMT, SMA, and VLA at 0.45, 0.85, 0.88, 1.3, 6.7, and 9.0 mm. We use these observations to inform a PHOENIX model of Sirius A's atmosphere. We find the model provides a good match to these data and can be used as a template for the submm/mm emission of other early A-type stars where unresolved debris may be present. The observations are part of an ongoing observational campaign entitled Measuring the Emission of Stellar Atmospheres at Submm/mm wavelengths (MESAS) ","MESAS: Measuring the Emission of Stellar Atmospheres at Submm/mm
  wavelengths"
10,989666732724965376,2800204849,Andrew Gordon Wilson,"['Our new paper, Hierarchical Density Order Embeddings, is appearing at #ICLR 2018 (with code)! <LINK>\nWe learn hierarchical representations of concepts using encapsulation of probability densities. <LINK>']",https://arxiv.org/abs/1804.09843,"By representing words with probability densities rather than point vectors, probabilistic word embeddings can capture rich and interpretable semantic information and uncertainty. The uncertainty information can be particularly meaningful in capturing entailment relationships -- whereby general words such as ""entity"" correspond to broad distributions that encompass more specific words such as ""animal"" or ""instrument"". We introduce density order embeddings, which learn hierarchical representations through encapsulation of probability densities. In particular, we propose simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical density representations. Our approach provides state-of-the-art performance on the WordNet hypernym relationship prediction task and the challenging HyperLex lexical entailment dataset -- while retaining a rich and interpretable density representation. ",Hierarchical Density Order Embeddings
11,989663234683584514,930224996785332224,"Jacob White, PhD",['Check out my new paper that uses @ALMA data to characterize the HD 141569 circumstellar disk! The analysis shows that the system may be one of the youngest debris disks meaning that significant grain growth and planet formation can happen in ~5 Myr. :) \n<LINK>'],https://arxiv.org/abs/1804.09724,"We present archival ALMA observations of the HD 141569 circumstellar disk at 345, 230, and 100 GHz. These data detect extended millimeter emission that is exterior to the inner disk. We find through simultaneous visibility modeling of all three data sets that the system's morphology is described well by a two-component disk model. The inner disk ranges from approximately 16 to 45 au with a spectral index of 1.81 (q = 2.95) and the outer disk ranges from 95 to 300 au with a spectral index of 2.28 (q = 3.21). Azimuthally averaged radial emission profiles derived from the continuum images at each frequency show potential emission that is consistent with the visibility modeling. The analysis presented here shows that at ~5 Myr HD 141569's grain size distribution is steeper, and therefore more evolved, in the outer disk than in the inner disk. ","Extended Millimeter Emission in the HD 141569 Circumstellar Disk
  Detected with ALMA"
12,989547884512194560,2956121356,Russ Salakhutdinov,"['New #acl2018 paper on Style Transfer Through Back-Translation with\n@shrimai_, Yulia Tsvetkov, and Alan W Black.\n<LINK> <LINK>']",https://arxiv.org/abs/1804.09000,"Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency. ",Style Transfer Through Back-Translation
13,989512462738313216,335479530,Mattia Walschaers,"['It has taken a while, but finally ""the one with the cool figures"" is out there! Check our new paper ""Tailoring Non-Gaussian Continuous-Variable Graph States"":\n<LINK>']",https://arxiv.org/abs/1804.09444,"Graph states are the backbone of measurement-based continuous-variable quantum computation. However, experimental realisations of these states induce Gaussian measurement statistics for the field quadratures, which poses a barrier to obtain a genuine quantum advantage. In this letter, we propose mode-selective photon addition and subtraction as viable and experimentally feasible pathways to introduce non-Gaussian features in such continuous-variable graph states. In particular, we investigate how the non-Gaussian properties spread among the vertices of the graph, which allows us to show the degree of control that is achievable in this approach. ",Tailoring Non-Gaussian Continuous-Variable Graph States
14,989407013959557120,523241142,Juste Raimbault,['New paper : Modeling the co-evolution of cities and networks\n<LINK>'],https://arxiv.org/abs/1804.09430,"The complexity of interactions between networks and territories has been widely acknowledged empirically, in particular through the existence of circular causal relations in their co-development, that can be understood as a co-evolution. This contribution aims at investigating models that endogenize this co-evolution, in the particular case of cities and transportation networks. We introduce a family of models of co-evolution for systems of cities at the macroscopic scale. Interactions between cities are the main driver of population growth rates, capturing a network effect at the first order (direct interactions). Network growth follows a demand-induced thresholded growth scheme, that can occur at the global level or locally. The exploration of the model on synthetic systems of cities shows the ability of the model to capture co-evolutive patterns. We apply the model on the French system of cities, with population data spanning 1831-1999 and a dynamical railway network (1850-2000). The model is calibrated on successive time-windows, assuming local temporal stationarity. We extract therein indirect knowledge on underlying processes and find that the prediction for city populations are in some cases improved in comparison to a static model. ",Modeling the co-evolution of cities and networks
15,989406753845596160,523241142,Juste Raimbault,['New paper : Indirect Evidence of Network Effects in a System of Cities <LINK>\nForthcoming in Environment and Planning B !'],https://arxiv.org/abs/1804.09416,"We describe a simple spatial model of urban growth for systems of cities at the macroscopic scale, which combines direct interaction between cities and an indirect effect of physical network flows as population growth drivers. The model is parametrized on population data for the French system of cities between 1831 and 1999, which strong non-stationarity in correlation patterns suggest to apply the model on local time windows. The corresponding calibration of the model using genetic algorithms provide the evolution of interaction processes and network effects in time. Furthermore, the fit improvement when adding network module appears effective when controlling for additional parameters, what confirms the ability of the model to unveil network effects in the system of cities. ",Indirect Evidence of Network Effects in a System of Cities
16,989158695983570945,30420963,Eivind Eriksen,['A new paper is out: Iterated Extensions and Uniserial Length Categories <LINK> <LINK>'],https://arxiv.org/abs/1804.03405,"In this paper, we study length categories using iterated extensions. We consider the problem of classifying all indecomposable objects in a length category, and the problem of characterizing those length categories that are uniserial. We solve the last problem, and obtain a necessary and sufficient criterion for uniseriality under weak assumptions. This criterion turns out to be known by Amdal and Ringdal already in 1968; we give a new proof that is both elementary and constructive. The first problem is the most fundamental one, and its general solution is ""the main and perhaps hopeless purpose of representation theory"" according to Gabriel. We solve the problem in the case when the length category is uniserial, using our constructive methods. As an application, we classify all graded holonomic $D$-modules on a monomial curve over the complex numbers, obtaining the most explicit results over the affine line, when $D$ is the first Weyl algebra. Finally, we show that the iterated extensions are completely determined by the noncommutative deformations of its simple factors. This tells us precisely what we can learn about a length category by studying its species; it gives the tangent space of the noncommutative deformation functor, or the infinitesimal deformations, but not the obstructions for lifting these deformations. ",Iterated Extensions and Uniserial Length Categories
17,988944599862337536,186017372,Yuichi Tanaka,"['Our new paper on graph wavelets is out!\nSakiyama, Watanabe, Tanaka, Ortega: Two-channel critically-sampled graph wavelets with spectral domain sampling\n<LINK>']",https://arxiv.org/abs/1804.08811,"We propose two-channel critically-sampled filter banks for signals on undirected graphs that utilize spectral domain sampling. Unlike conventional approaches based on vertex domain sampling, our transforms have the following desirable properties: 1) perfect reconstruction regardless of the characteristics of the underlying graphs and graph variation operators and 2) a symmetric structure; i.e., both analysis and synthesis filter banks are built using similar building blocks. Along with the structure of the filter banks, this paper also proves the general criterion for perfect reconstruction and theoretically shows that the vertex and spectral domain sampling coincide for a special case. The effectiveness of our approach is evaluated by comparing its performance in nonlinear approximation and denoising with various conventional graph transforms. ","Two-Channel Critically-Sampled Graph Filter Banks With Spectral Domain
  Sampling"
18,988813634007547905,171674815,Mark Marley,"['This paper <LINK> has it all, new data, models, clouds, disequilibrium chemistry, analogies to Jupiter, and mysteries unsolved.', 'John Lewis was writing about this in 1969: https://t.co/rNFCQaqgIN', 'I took planetary chemistry from John in grad school, so it‚Äôs cool to be applying his lines of thinking to the most Jupiter-like brown dwarf now.']",https://arxiv.org/abs/1804.07771,"The coldest brown dwarf, WISE 0855, is the closest known planetary-mass, free-floating object and has a temperature nearly as cold as the solar system gas giants. Like Jupiter, it is predicted to have an atmosphere rich in methane, water, and ammonia, with clouds of volatile ices. WISE 0855 is faint at near-infrared wavelengths and emits almost all its energy in the mid-infrared. Skemer et al. 2016 presented a spectrum of WISE 0855 from 4.5-5.1 micron (M band), revealing water vapor features. Here, we present a spectrum of WISE 0855 in L band, from 3.4-4.14 micron. We present a set of atmosphere models that include a range of compositions (metallicities and C/O ratios) and water ice clouds. Methane absorption is clearly present in the spectrum. The mid-infrared color can be better matched with a methane abundance that is depleted relative to solar abundance. We find that there is evidence for water ice clouds in the M band spectrum, and we find a lack of phosphine spectral features in both the L and M band spectra. We suggest that a deep continuum opacity source may be obscuring the near-infrared flux, possibly a deep phosphorous-bearing cloud, ammonium dihyrogen phosphate. Observations of WISE 0855 provide critical constraints for cold planetary atmospheres, bridging the temperature range between the long-studied solar system planets and accessible exoplanets. JWST will soon revolutionize our understanding of cold brown dwarfs with high-precision spectroscopy across the infrared, allowing us to study their compositions and cloud properties, and to infer their atmospheric dynamics and formation processes. ",An L Band Spectrum of the Coldest Brown Dwarf
19,988799694095699968,14556945,Yves-A. de Montjoye,"[""New research: Exploiting @aircloak's Diffix anonymization mechanism through a noise-exploitation attack. Blogpost: <LINK> Paper: <LINK> (#infosec during #EuroSP18) <LINK>"", '@emilianoucl @aircloak Yes, we did!']",https://arxiv.org/abs/1804.06752,"Anonymized data is highly valuable to both businesses and researchers. A large body of research has however shown the strong limits of the de-identification release-and-forget model, where data is anonymized and shared. This has led to the development of privacy-preserving query-based systems. Based on the idea of ""sticky noise"", Diffix has been recently proposed as a novel query-based mechanism satisfying alone the EU Article~29 Working Party's definition of anonymization. According to its authors, Diffix adds less noise to answers than solutions based on differential privacy while allowing for an unlimited number of queries. This paper presents a new class of noise-exploitation attacks, exploiting the noise added by the system to infer private information about individuals in the dataset. Our first differential attack uses samples extracted from Diffix in a likelihood ratio test to discriminate between two probability distributions. We show that using this attack against a synthetic best-case dataset allows us to infer private information with 89.4% accuracy using only 5 attributes. Our second cloning attack uses dummy conditions that conditionally strongly affect the output of the query depending on the value of the private attribute. Using this attack on four real-world datasets, we show that we can infer private attributes of at least 93% of the users in the dataset with accuracy between 93.3% and 97.1%, issuing a median of 304 queries per user. We show how to optimize this attack, targeting 55.4% of the users and achieving 91.7% accuracy, using a maximum of only 32 queries per user. Our attacks demonstrate that adding data-dependent noise, as done by Diffix, is not sufficient to prevent inference of private attributes. We furthermore argue that Diffix alone fails to satisfy Art. 29 WP's definition of anonymization. [...] ",When the signal is in the noise: Exploiting Diffix's Sticky Noise
20,988704747099230208,1132031455,James O' Neill,['New paper on weighted ensemble transfer of GRU networks that mitigates negative transfer and improves performance on the target task with only few examples <LINK> #NLP #MachineLearning'],https://arxiv.org/abs/1804.08501,"Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks. However, transferring all knowledge, some of which irrelevant for a target task, can lead to sub-optimal results due to \textit{negative} transfer. Hence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method. \newline The primary contribution of this paper is the combination of both \textit{Dropout} and \textit{Bagging} for improved transferability in neural networks, referred to as \textit{Dropping} herein. We present a straightforward yet novel approach for incorporating source \textit{Dropping} Networks to a target task for few-shot learning that mitigates \textit{negative} transfer. This is achieved by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training. We compare the proposed approach against hard parameter sharing and soft parameter sharing transfer methods in the few-shot learning case. We also compare against models that are fully trained on the target task in the standard supervised learning setup. The aforementioned adjustment leads to improved transfer learning performance and comparable results to the current state of the art only using a fraction of the data from the target task. ",Dropping Networks for Transfer Learning
21,988693017006657536,4189511729,Marius Millea,"[""New constraints on the epoch of reionization from @Planck, implications for the EDGES 21cm signal, and for @DOE_Stage_4_CMB.\n\nRead in our new paper, out today: <LINK>\n\nAlso, why I've been staring at this weird looking thing for longer than I'd like to admit... <LINK>"", 'And in a companion paper, a basic but quite useful #statistics fact that someone else *must* have figured out already, right? \n\nMostly publishing so some statistician can tell us the right reference that we missed (only 50% kidding...)\n\nRead here: https://t.co/dxYGv9Zpt1', '@ajvengelen @Planck @DOE_Stage_4_CMB Thanks!']",https://arxiv.org/abs/1804.08476,"Non-parametric reconstruction or marginalization over the history of reionization using cosmic microwave background data necessarily assumes a prior over possible histories. We show that different but reasonable choices of priors can shift current and future constraints on the reionization optical depth, $\tau$, or correlated parameters such as the neutrino mass sum, $\Sigma m_\nu$, at the level of 0.3-0.4$\sigma$, i.e., that this analysis is somewhat prior dependent. We point out some prior-related problems with the commonly used principal component reconstruction, concluding that the significance of some recent hints of early reionization in Planck 2015 data has been overestimated. We also present the first non-parametric reconstruction applied to newer Planck intermediate (2016) data and find that the hints of early reionization disappear entirely in this more precise dataset. These results limit possible explanations of the EDGES 21cm signal which would have also significantly reionized the universe at $z\,{>}\,15$. Our findings about the dependence on priors motivate the pursuit of improved data or searches for physical reionization models which can reduce the prior volume. The discussion here of priors is of general applicability to other non-parametric reconstructions, for example of the primordial power spectrum, of the recombination history, or of the expansion rate. ","Cosmic Microwave Background Constraints in Light of Priors Over
  Reionization Histories"
22,988586763328221184,33113669,Tim Baldwin,"['New paper on joint network + text modelling for user geolocation with @ashrayme and @trevorcohn (to appear at @acl2018), with focus on minimal supervision and the use of unlabelled (networked) data -- <LINK> #nlproc']",https://arxiv.org/abs/1804.08049,"Social media user geolocation is vital to many applications such as event detection. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the state- of-the-art over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN. ",Semi-supervised User Geolocation via Graph Convolutional Networks
23,988585800588709888,369569444,Takahiro TERADA (ÂØ∫Áî∞ ÈöÜÂ∫É),"['Our new paper is available now! This is relevant in scenarios with enhanced curvature perturbations or matter dominance, e.g. PBH scenarios and some of curvaton scenarios. <LINK>   By the way, my first [gr-qc] paper (actually changed from hep-ph by arXiv admin). <LINK>']",https://arxiv.org/abs/1804.08577,"Whether or not the primordial gravitational wave (GW) produced during inflation is sufficiently strong to be observable, GWs are necessarily produced from the primordial curvature perturbations in the second order of perturbation. The induced GWs can be enhanced by curvature perturbations enhanced at small scales or by the presence of matter-dominated stages of the cosmological history. We analytically calculate the integral in the expression of the power spectrum of the induced GWs which is a universal part independent of the spectrum of the primordial curvature perturbations. This makes the subsequent numerical integrals significantly easy. In simple cases, we derive fully analytic formulas for the induced GW spectrum. ","Semianalytic Calculation of Gravitational Wave Spectrum Nonlinearly
  Induced from Primordial Curvature Perturbations"
24,988545226510757888,865254426101067778,Mohit Iyyer,"['new #NAACL2018 paper out on controllable paraphrasing w/ John Wieting, @kevingimpel @LukeZettlemoyer <LINK> our model generates paraphrases with user-specified syntactic forms. we use it to generate (and make models more robust to) adversarial examples! #NLProc <LINK>']",https://arxiv.org/abs/1804.06059,"We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) ""fool"" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data. ","Adversarial Example Generation with Syntactically Controlled Paraphrase
  Networks"
25,988222757774155777,11778512,Mason Porter,"['New paper: ""Topological data analysis of continuum percolation with disks"": <LINK>\n\nSavor the fact that I actually wrote a short paper for once. \n\n(I hope you realize how unusual that is.)\n\nCoauthors: @leo_speidel, @haharrington, &amp; Jon Chapman']",https://arxiv.org/abs/1804.07733,"We study continuum percolation with disks, a variant of continuum percolation in two-dimensional Euclidean space, by applying tools from topological data analysis. We interpret each realization of continuum percolation with disks as a topological subspace of $[0,1]^2$ and investigate its topological features across many realizations. We apply persistent homology to investigate topological changes as we vary the number and radius of disks. We observe evidence that the longest persisting invariant is born at or near the percolation transition. ",Topological data analysis of continuum percolation with disks
26,988211748867072000,806058672619212800,Guillaume Lample,"['New paper on unsupervised MT! <LINK> We propose two models (neural and phrase based) that both improve the state of the art by more than 11 BLEU. By combining them we reach up to 27 BLEU points on WMT14, without using a single parallel sentence.', '@universeinanegg @AaronJaech I think the important things are: 1) having data in the same domain than the data you want to translate (i.e. we evaluate on newstest so we use News Crawl). 2) having monolingual corpora in the two languages that allow you to generate a good dictionary in an unsupervised way.', '@universeinanegg @AaronJaech Clearly, the monolingual corpora you use to train your embeddings will have a significant impact on the quality of your embeddings, i.e. Table 2 in https://t.co/u6AdELYbsR where you can get more than 20% word translation accuracy based on what you train your embeddings on.', '@universeinanegg @AaronJaech That being said, we obtain the same word translation accuracy of ~80% P@1 if we train the embeddings on the parallel data of WMT en-fr, or on Wikipedia, or on News Crawl, or on Common Crawl, and clearly the parallel data of WMT en-fr is a much more similar domain than News Crawl.']",https://arxiv.org/abs/1804.07755,"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available. ",Phrase-Based & Neural Unsupervised Machine Translation
27,987253115509460992,864555701783474179,julesh,"[""New preprint: Backward induction for repeated games\nThis is really work I did in 2014, and I *still* can't understand how it works, so I wrote an 'experimental' paper about it. Heavily exploiting features of Haskell to do computable game theory.\n<LINK>"", ""@TheMichaelBurge Good question.... I think yes, and I think one of the interpretations of repeated games is they are really finite-but-unknow-length.Thing is, formalising games where the player doesn't know the rules is a can of worms."", ""@TheMichaelBurge Perhaps though if you have a finite-but-unknown-length game you get 'false horizons' where players have a false belief about when the game ends. Kinda like real life apocalypse sects really...""]",https://arxiv.org/abs/1804.07074,"We present a method of backward induction for computing approximate subgame perfect Nash equilibria of infinitely repeated games with discounted payoffs. This uses the selection monad transformer, combined with the searchable set monad viewed as a notion of 'topologically compact' nondeterminism, and a simple model of computable real numbers. This is the first application of Escard\'o and Oliva's theory of higher-order sequential games to games of imperfect information, in which (as well as its mathematical elegance) lazy evaluation does nontrivial work for us compared with a traditional game-theoretic analysis. Since a full theoretical understanding of this method is lacking (and appears to be very hard), we consider this an 'experimental' paper heavily inspired by theoretical ideas. We use the famous Iterated Prisoner's Dilemma as a worked example. ",Backward Induction for Repeated Games
28,987227161592463362,776765039726460929,Carlo Felice Manara,"['In the middle of the ALMA madness, our new paper on dynamical masses from CO emission in protoplanetary disks came out: <LINK>', 'btw, this is the version after the first referee report']",https://arxiv.org/abs/1804.06272,"In recent ALMA surveys, the gas distributions and velocity structures of most of the protoplanetary disks can still not be imaged at high S/N due to the short integration time. In this work, we re-analyzed the ALMA 13CO (3-2) and C18O (3-2) data of 88 young stellar objects in Lupus with the velocity-aligned stacking method to enhance S/N and to study the kinematics and disk properties traced by molecular lines. This method aligns spectra at different positions in a disk based on the projected Keplerian velocities at their positions and then stacks them. This method enhances the S/N ratios of molecular-line data and allows us to obtain better detections and to constrain dynamical stellar masses and disk orientations. We obtain 13CO detections in 41 disks and C18O detections in 18 disks with 11 new detections in 13CO and 9 new detections in C18O after applying the method. We estimate the disk orientations and the dynamical stellar masses from the 13CO data. Our estimated dynamical stellar masses correlate with the spectroscopic stellar masses, and in a subsample of 16 sources, where the inclination angles are better constrained, the two masses are in a good agreement within the uncertainties and with a mean difference of 0.15 Msun. With more detections of fainter disks, our results show that high gas masses derived from the 13CO and C18O lines tend to be associated with high dust masses estimated from the continuum emission. Nevertheless, the scatter is large (0.9 dex), implying large uncertainties in deriving the disk gas mass from the line fluxes. We find that with such large uncertainties it is expected that there is no correlation between the disk gas mass and the mass accretion rate with the current data. Deeper observations to detect disks with gas masses <1E-5 Msun in molecular lines are needed to investigate the correlation between the disk gas mass and the mass accretion rate. ","Stellar masses and disk properties of Lupus young stellar objects traced
  by velocity-aligned stacked ALMA 13CO and C18O spectra"
29,987090179625312258,2956121356,Russ Salakhutdinov,"['New paper on Neural Models for Reasoning over Multiple Mentions using Coreference with Bhuwan Dhingra, Qiao Jin, Zhilin Yang, and @professorwcohen \n<LINK> <LINK>']",https://arxiv.org/abs/1804.05922,"Many problems in NLP require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets -- Wikihop, LAMBADA and the bAbi AI tasks -- with large gains when training data is scarce. ",Neural Models for Reasoning over Multiple Mentions using Coreference
30,987044488009932800,185910194,Graham Neubig,"['Our new #NAACL2018 paper examines ""When and Why are Pre-trained Embeddings Useful for NMT?"" <LINK>\nSome conclusions intuitive (embeddings help most when systems are bad, but not too bad), and some surprising (explicit bilingual training of embeddings unnecessary) <LINK>']",https://arxiv.org/abs/1804.06323,"The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting. ","When and Why are Pre-trained Word Embeddings Useful for Neural Machine
  Translation?"
31,986836746238230528,28535459,Dougal Mackey,"[""The map revealed a new ultra-faint dwarf, Hydrus I, projected on the Magellanic Brdige. This paper also on today's arXiv, led by Sergey Koposov and including myself, @astrowizicist and @deniserkal - <LINK>"", 'Complete with typo.  Hooray!']",https://arxiv.org/abs/1804.06430,"We report the discovery of a nearby dwarf galaxy in the constellation of Hydrus, between the Large and the Small Magellanic Clouds. Hydrus 1 is a mildy elliptical ultra-faint system with luminosity $M_V\sim$ -4.7 and size $\sim$ 50 pc, located 28 kpc from the Sun and 24 kpc from the LMC. From spectroscopy of $\sim$ 30 member stars, we measure a velocity dispersion of 2.7 km/s and find tentative evidence for a radial velocity gradient consistent with 3 km/s rotation. Hydrus 1's velocity dispersion indicates that the system is dark matter dominated, but its dynamical mass-to-light ratio M/L $\sim$ 66 is significantly smaller than typical for ultra-faint dwarfs at similar luminosity. The kinematics and spatial position of Hydrus~1 make it a very plausible member of the family of satellites brought into the Milky Way by the Magellanic Clouds. While Hydrus 1's proximity and well-measured kinematics make it a promising target for dark matter annihilation searches, we find no evidence for significant gamma-ray emission from Hydrus 1. The new dwarf is a metal-poor galaxy with a mean metallicity [Fe/H]=-2.5 and [Fe/H] spread of 0.4 dex, similar to other systems of similar luminosity. Alpha-abundances of Hyi 1 members indicate that star-formation was extended, lasting between 0.1 and 1 Gyr, with self-enrichment dominated by SN Ia. The dwarf also hosts a highly carbon-enhanced extremely metal-poor star with [Fe/H] $\sim$ -3.2 and [C/Fe] $\sim$ +3.0. ",Snake in the Clouds: A new nearby dwarf galaxy in the Magellanic bridge
32,986544797681946625,114563183,Olga Zagovora,"['Find out our new paper at #websci2018 with @clauwa ,@fariba_k and @t_sennikova on ""Collective Attention towards Scientists and Research Topics""\n<LINK>']",https://arxiv.org/abs/1804.06288,"Emergent patterns of collective attention towards scientists and their research may function as a proxy for scientific impact which traditionally is assessed via committees that award prizes to scientists. Therefore it is crucial to understand the relationships between scientific impact and online demand and supply for information about scientists and their work. In this paper, we compare the temporal pattern of information supply (article creations) and information demand (article views) on Wikipedia for two groups of scientists: scientists who received one of the most prestigious awards in their field and influential scientists from the same field who did not receive an award. Our research highlights that awards function as external shocks which increase supply and demand for information about scientists, but hardly affect information supply and demand for their research topics. Further, we find interesting differences in the temporal ordering of information supply between the two groups: (i) award-winners have a higher probability that interest in them precedes interest in their work; (ii) for award winners interest in articles about them and their work is temporally more clustered than for non-awarded scientists. ",Collective Attention towards Scientists and Research Topics
33,986180542050766849,22473658,Chris Norval,"['New Paper - Decision Provenance: Capturing data flow for accountable systems\nby Jatinder Singh, @jennifercobbe, and I. \n\n<LINK>']",https://arxiv.org/abs/1804.05741,"Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken (at design and run-time) throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary (tech-legal) exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics. ",Decision Provenance: Harnessing data flow for accountable systems
34,986177836535963648,826540638812434432,Jennifer Cobbe,"['New paper - ""Decision Provenance"". From me, Jat Singh, and @cnorval\n\nUsing data provenance techniques to track data flow through interconnected automated systems could increase transparency and accountability and facilitate end-user agency\n\n<LINK>']",https://arxiv.org/abs/1804.05741,"Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken (at design and run-time) throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary (tech-legal) exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics. ",Decision Provenance: Harnessing data flow for accountable systems
35,986165061805854721,3021399517,Jean-Baptiste Mouret,"['Introducing the concept of ""Elite Hypervolume"" (or ""Good Species have almost always common features""). Our new #GECCO2018 paper: <LINK> [ Discovering the Elite Hypervolume by Leveraging Interspecies Correlation, with V. Vassiliades]. #evolution #map_elites <LINK>']",https://arxiv.org/abs/1804.03906,"Evolution has produced an astonishing diversity of species, each filling a different niche. Algorithms like MAP-Elites mimic this divergent evolutionary process to find a set of behaviorally diverse but high-performing solutions, called the elites. Our key insight is that species in nature often share a surprisingly large part of their genome, in spite of occupying very different niches; similarly, the elites are likely to be concentrated in a specific ""elite hypervolume"" whose shape is defined by their common features. In this paper, we first introduce the elite hypervolume concept and propose two metrics to characterize it: the genotypic spread and the genotypic similarity. We then introduce a new variation operator, called ""directional variation"", that exploits interspecies (or inter-elites) correlations to accelerate the MAP-Elites algorithm. We demonstrate the effectiveness of this operator in three problems (a toy function, a redundant robotic arm, and a hexapod robot). ",Discovering the Elite Hypervolume by Leveraging Interspecies Correlation
36,986161343974793216,3021399517,Jean-Baptiste Mouret,"['How to use a surrogate model when when the structure of your inputs is not fixed? See our new #GECCO18 paper! Data-efficient Neuroevolution with Kernel-Based Surrogate Models -- <LINK> [with A. Gaier &amp; A. Asteroth] #GP #evolution #ResiBots @GECCO2018 <LINK>', '@csanhuezalobos @GECCO2018 Thanks!']",https://arxiv.org/abs/1804.05364,"Surrogate-assistance approaches have long been used in computationally expensive domains to improve the data-efficiency of optimization algorithms. Neuroevolution, however, has so far resisted the application of these techniques because it requires the surrogate model to make fitness predictions based on variable topologies, instead of a vector of parameters. Our main insight is that we can sidestep this problem by using kernel-based surrogate models, which require only the definition of a distance measure between individuals. Our second insight is that the well-established Neuroevolution of Augmenting Topologies (NEAT) algorithm provides a computationally efficient distance measure between dissimilar networks in the form of ""compatibility distance"", initially designed to maintain topological diversity. Combining these two ideas, we introduce a surrogate-assisted neuroevolution algorithm that combines NEAT and a surrogate model built using a compatibility distance kernel. We demonstrate the data-efficiency of this new algorithm on the low dimensional cart-pole swing-up problem, as well as the higher dimensional half-cheetah running task. In both tasks the surrogate-assisted variant achieves the same or better results with several times fewer function evaluations as the original NEAT. ",Data-efficient Neuroevolution with Kernel-Based Surrogate Models
37,984842946242461696,157110062,Shirin Nilizadeh,"['In our new @icwsm paper, we found hate instigators target more popular and high profile Twitter users, and that participating in hate speech is associated with greater online visibility. <LINK> with @mai_elsherief, Dana Nguyen, @gio, and Elizabeth Belding', 'The personality analysis showed both groups have eccentric personality facets that differ from the general Twitter population. https://t.co/tZZSzjfkSA with @mai_elsherief, Dana Nguyen, @gio and Elizabeth Belding']",http://arxiv.org/abs/1804.04649,"While social media has become an empowering agent to individual voices and freedom of expression, it also facilitates anti-social behaviors including online harassment, cyberbullying, and hate speech. In this paper, we present the first comparative study of hate speech instigators and target users on Twitter. Through a multi-step classification process, we curate a comprehensive hate speech dataset capturing various types of hate. We study the distinctive characteristics of hate instigators and targets in terms of their profile self-presentation, activities, and online visibility. We find that hate instigators target more popular and high profile Twitter users, and that participating in hate speech can result in greater online visibility. We conduct a personality analysis of hate instigators and targets and show that both groups have eccentric personality facets that differ from the general Twitter population. Our results advance the state of the art of understanding online hate speech engagement. ",Peer to Peer Hate: Hate Speech Instigators and Their Targets
38,984728269474926593,2420562302,Ulrik Lyngs,"[""Our new paper 'Third Party Tracking in the Mobile Ecosystem' is out! Measures presence of 3rd party trackers in almost 1 million free apps from the Google Play store. We're presenting it at #WebSci18 next month! Preprint: <LINK> <LINK>""]",https://arxiv.org/abs/1804.03603,"Third party tracking allows companies to identify users and track their behaviour across multiple digital services. This paper presents an empirical study of the prevalence of third-party trackers on 959,000 apps from the US and UK Google Play stores. We find that most apps contain third party tracking, and the distribution of trackers is long-tailed with several highly dominant trackers accounting for a large portion of the coverage. The extent of tracking also differs between categories of apps; in particular, news apps and apps targeted at children appear to be amongst the worst in terms of the number of third party trackers associated with them. Third party tracking is also revealed to be a highly trans-national phenomenon, with many trackers operating in jurisdictions outside the EU. Based on these findings, we draw out some significant legal compliance challenges facing the tracking industry. ",Third Party Tracking in the Mobile Ecosystem
39,984715238422364161,234398193,Hal Tasaki,"['Hal Tasaki\n""Topological phase transition and Z2 index for S=1 quantum spin chains""\nMy new paper on #QuantumSpin chains, which contains the first rigorous proof that the AKLT model is in a nontrivial SPT phase.  The argument, which I like, is quite pretty.\n<LINK>']",https://arxiv.org/abs/1804.04337,"We study $S=1$ quantum spin systems on the infinite chain with short ranged Hamiltonians which have certain rotational and discrete symmetry. We define a $\mathbb{Z}_2$ index for any gapped unique ground state, and prove that it is invariant under smooth deformation. By using the index, we provide the first rigorous proof of the existence of a ""topological"" phase transition, which cannot be characterized by any conventional order parameters, between the AKLT ground state and trivial ground states. This rigorously establishes that the AKLT model is in a nontrivial symmetry protected topological phase. ","Topological phase transition and $\mathbb{Z}_2$ index for $S=1$ quantum
  spin chains"
40,984703802589708288,75249390,Axel Maas,"['We have published a new paper on #Higgs physics beyond the standard model at <LINK>\n\nRed are our predictions from <LINK> and blue our numerical results in this paper (dashed are upper limits, due to too noisy signal).\n\nWe are quite satisfied :) <LINK>', 'If one would use the methods recommended by textbooks for this type of theory, they would be nonsensical in the plot, much less quantitatively right. Why this is so is explained in my review on #BroutEnglertHiggs physics https://t.co/jVo2JEcpFO', 'You can find a popular science explanation of what is going on in my blog https://t.co/TU2SdD1Z8P the newest entries on this topic are https://t.co/vbqJWmrO8m and https://t.co/UfGAedrBYM #np3', '@ArnoGorgels The one we worked on in the paper - theories with a Brout-Englert-Higgs effect, i.e. (non-)Abelian gauge theories with charged scalars.']",https://arxiv.org/abs/1804.04453,"In gauge theories, the physical, experimentally observable spectrum consists only of gauge-invariant states. This spectrum can be different from the elementary spectrum even at weak coupling and in the presence of the Brout-Englert-Higgs effect. We demonstrate this for an SU(3) gauge theory with a single fundamental Higgs, a toy theory for grand-unified theories. The manifestly gauge-invariant approach of lattice gauge theory is used to determine the spectrum in four different channels. It is found to be qualitatively different from the elementary one, and especially from the one predicted by standard perturbation theory. The result can be understood in terms of the Froehlich-Morchio-Strocchi mechanism. In fact, we find that analytic methods based on this mechanism, a gauge-invariant extension of perturbation theory, correctly determines the spectrum, and gives already at leading order a reasonably good quantitative description. Together with previous results this supports that this approach is the analytic method of choice for theories with a Brout-Englert-Higgs effect. ",The spectrum of an SU(3) gauge theory with a fundamental Higgs field
41,984699828671340545,487990723,Gianfranco Bertone,"['New paper today on the arXiv: ""Probing the nature of dark matter\nparticles with stellar streams"" with @D_ITP postdoc Nil Banik, @jobovy and Nassim Bozorgnia <LINK> <LINK>', 'A key prediction of the standard ""Lambda-Cold-Dark-Matter"" cosmological model is the existence of a large number of dark matter substructures on sub-galactic scales', 'This can be tested by studying the perturbations induced by dark matter substructures on cold stellar streams. We studied the prospects for discriminating cold from warm dark matter with upcoming astronomical surveys such as the Large Synoptic Survey Telescope @LSST', 'We argue that this method will set stringent constraints on the mass of dark matter particles, and possibly to yield an actual measurement if it is in the O(1) keV range']",https://arxiv.org/abs/1804.04384,"A key prediction of the standard cosmological model -- which relies on the assumption that dark matter is cold, i.e. non-relativistic at the epoch of structure formation -- is the existence of a large number of dark matter substructures on sub-galactic scales. This assumption can be tested by studying the perturbations induced by dark matter substructures on cold stellar streams. Here, we study the prospects for discriminating cold from warm dark matter by generating mock data for upcoming astronomical surveys such as the Large Synoptic Survey Telescope (LSST), and reconstructing the properties of the dark matter particle from the perturbations induced on the stellar density profile of a stream. We discuss the statistical and systematic uncertainties, and show that the method should allow to set stringent constraints on the mass of thermal dark matter relics, and possibly to yield an actual measurement of the dark matter particle mass if it is in the $\mathcal{O}(1)$ keV range. ",Probing the nature of dark matter particles with stellar streams
42,984339682887700480,290833514,Jim Barrett,['New paper hits the arxiv! I was mostly involved with section 6  <LINK>'],https://arxiv.org/abs/1804.03765,"We report a framework for spectroscopic follow-up design for optimizing supernova photometric classification. The strategy accounts for the unavoidable mismatch between spectroscopic and photometric samples, and can be used even in the beginning of a new survey -- without any initial training set. The framework falls under the umbrella of active learning (AL), a class of algorithms that aims to minimize labelling costs by identifying a few, carefully chosen, objects which have high potential in improving the classifier predictions. As a proof of concept, we use the simulated data released after the Supernova Photometric Classification Challenge (SNPCC) and a random forest classifier. Our results show that, using only 12\% the number of training objects in the SNPCC spectroscopic sample, this approach is able to double purity results. Moreover, in order to take into account multiple spectroscopic observations in the same night, we propose a semi-supervised batch-mode AL algorithm which selects a set of $N=5$ most informative objects at each night. In comparison with the initial state using the traditional approach, our method achieves 2.3 times higher purity and comparable figure of merit results after only 180 days of observation, or 800 queries (73% of the SNPCC spectroscopic sample size). Such results were obtained using the same amount of spectroscopic time necessary to observe the original SNPCC spectroscopic sample, showing that this type of strategy is feasible with current available spectroscopic resources. The code used in this work is available in the COINtoolbox: this https URL . ","Optimizing spectroscopic follow-up strategies for supernova photometric
  classification with active learning"
43,984338468309520384,59413748,Reuben Binns,"['New paper: ""Third Party Tracking in the Mobile Ecosystem"" from myself, @ulyngs @emax @junszhao Timothy Libert @Nigel_Shadbolt \nMeasures 3rd party trackers on nearly 1m apps. Accepted at #WebSci18\nPreprint: <LINK> <LINK>', '@sicrossley @ulyngs @emax @junszhao @Nigel_Shadbolt Good questions... to be explored in future research!']",https://arxiv.org/abs/1804.03603,"Third party tracking allows companies to identify users and track their behaviour across multiple digital services. This paper presents an empirical study of the prevalence of third-party trackers on 959,000 apps from the US and UK Google Play stores. We find that most apps contain third party tracking, and the distribution of trackers is long-tailed with several highly dominant trackers accounting for a large portion of the coverage. The extent of tracking also differs between categories of apps; in particular, news apps and apps targeted at children appear to be amongst the worst in terms of the number of third party trackers associated with them. Third party tracking is also revealed to be a highly trans-national phenomenon, with many trackers operating in jurisdictions outside the EU. Based on these findings, we draw out some significant legal compliance challenges facing the tracking industry. ",Third Party Tracking in the Mobile Ecosystem
44,984256546396712960,807894948,Vivek Kulkarni,"[""Wouldn't it be fantabulous for your conversational assistant to speak slang on your staycation? Our new paper in #NAACL2018 with @WilliamWangNLP is a first step in that direction. Paper <LINK> #nlproc <LINK>""]",https://arxiv.org/abs/1804.02596,"We propose generative models for three types of extra-grammatical word formation phenomena abounding in English slang: Blends, Clippings, and Reduplicatives. Adopting a data-driven approach coupled with linguistic knowledge, we propose simple models with state of the art performance on human annotated gold standard datasets. Overall, our models reveal insights into the generative processes of word formation in slang -- insights which are increasingly relevant in the context of the rising prevalence of slang and non-standard varieties on the Internet. ",Simple Models for Word Formation in English Slang
45,984248270615703553,326843207,Yuta Notsu,"['""Time resolved spectroscopic observations of an M-dwarf flare star EV Lac during a flare"" (Honda, Notsu, Namekata et al.)  <LINK>  Our new paper reporting possible blue enhancement (blue asymmetry) of the Halpha line is now on today\'s arXiv.', '@rrpaudel18 Thank you !']",https://arxiv.org/abs/1804.03771,"We have performed 5 night spectroscopic observation of the Halpha line of EV Lac with a medium wavelength resolution (R~ 10,000) using the 2m Nayuta telescope at the Nishi-Harima Astronomical Observatory. EV Lac always possesses the Halpha emission line; however, its intensity was stronger on August 15, 2015 than during other four-night periods. On this night, we observed a rapid rise (~ 20min) and a subsequent slow decrease (~ 1.5h) of the emission-line intensity of Halpha, which was probably caused by a flare. We also found an asymmetrical change in the Halpha line on the same night. The enhancement has been observed in the blue wing of the Ha line during each phase of this flare (from the flare start to the flare end), and absorption components were present in its red wing during the early and later phases of the flare. Such blue enhancement (blue asymmetry) of the Halpha line is sometimes seen during solar flares, but only during the early phases. Even for solar flares, little is known about the origin of the blue asymmetry. Compared with solar-flare models, the presented results can lead to the understanding of the dynamics of stellar flares. ","Time resolved spectroscopic observations of an M-dwarf flare star EV
  Lacertae during a flare"
46,984022401582321664,20703003,Peter B Denton,"['New neutrino paper today with two great masters students. We calculated the diffuse supernova neutrino background flux for @HyperKamiokande, @DUNEScience, and JUNO and show that together they can measure some SN parameters. <LINK>']",https://arxiv.org/abs/1804.03157,"The detection of the diffuse supernova neutrino background (DSNB) will preciously contribute to gauge the properties of the core-collapse supernova population. We estimate the DSNB event rate in the next-generation neutrino detectors, Hyper-Kamiokande enriched with Gadolinium, JUNO, and DUNE. The determination of the supernova unknowns through the DSNB will be heavily driven by Hyper-Kamiokande, given its higher expected event rate, and complemented by DUNE that will help in reducing the parameters uncertainties. Meanwhile, JUNO will be sensitive to the DSNB signal over the largest energy range. A joint statistical analysis of the expected rates in 20 years of data taking from the above detectors suggests that we will be sensitive to the local supernova rate at most at a 20-33% level. A non-zero fraction of supernovae forming black holes will be confirmed at a 90% CL, if the true value of that fraction is larger than 20%. On the other hand, the DSNB events show extremely poor statistical sensitivity to the nuclear equation of state and mass accretion rate of the progenitors forming black holes. ","Measuring the supernova unknowns at the next-generation neutrino
  telescopes through the diffuse neutrino background"
47,984015347123662848,829415362647187456,Vanessa Graber,['New paper on the arXiv: we studied rapid crust coupling due to Kelvin wave excitations and the corresponding impact on glitch rises in superfluid neutron stars <LINK>'],https://arxiv.org/abs/1804.02706,"Pulsar glitches provide a unique way to study neutron star microphysics because short post-glitch dynamics are directly linked to strong frictional processes on small scales. To illustrate this connection between macroscopic observables and microphysics, we review calculations of vortex interactions focusing on Kelvin wave excitations and determine the corresponding mutual friction strength for realistic microscopic parameters in the inner crust. These density-dependent crustal coupling profiles are combined with a simplified treatment of the core coupling and implemented in a three-component neutron star model to construct a predictive framework for glitch rises. As a result of the density-dependent dynamics, we find the superfluid to transfer angular momentum to different parts of the crust and the core on different timescales. This can cause the spin frequency change to become non-monotonic in time, allowing for a maximum value much larger than the measured glitch size, as well as a delay in the recovery. The exact shape of the calculated glitch rise is strongly dependent on the relative strength between the crust and core mutual friction, providing the means to probe not only the crustal superfluid but also the deeper neutron star interior. To demonstrate the potential of this approach, we compare our predictive model with the first pulse-to-pulse observations recorded during the December 2016 glitch of the Vela pulsar. Our analysis suggests that the glitch rise behavior is relatively insensitive to the crustal mutual friction strength as long as $\mathcal{B} \gtrsim 10^{-3}$, while being strongly dependent on the core coupling strength, which we find to be in the range $3 \times 10^{-5} \lesssim \mathcal{B}_{\rm core} \lesssim 10^{-4}$. ",Glitch rises as a test for rapid superfluid coupling in neutron stars
48,983990267521970176,814077886374232064,Enrico Gavagnin,['Great news! Our new paper about pair correlation function in discrete domains has been accepted in @PhysRevE \n@JenniferOwen123 @Kit_Yates_Maths \n<LINK> <LINK>'],https://arxiv.org/abs/1804.03452,"Identifying and quantifying spatial correlation are important aspects of studying the collective behaviour of multi-agent systems. Pair correlation functions (PCFs) are powerful statistical tools which can provide qualitative and quantitative information about correlation between pairs of agents. Despite the numerous PCFs defined for off-lattice domains, only a few recent studies have considered a PCF for discrete domains. Our work extends the study of spatial correlation in discrete domains by defining a new set of PCFs using two natural and intuitive definitions of distance for a square lattice: the taxicab and uniform metric. We show how these PCFs improve upon previous attempts and compare between the quantitative data acquired. We also extend our definitions of the PCF to other types of regular tessellation which have not been studied before, including hexagonal, triangular and cuboidal. Finally, we provide a comprehensive PCF for any tessellation and metric allowing investigation of spatial correlation in irregular lattices for which recognising correlation is less intuitive. ","Pair correlation functions for identifying spatial correlation in
  discrete domains"
49,983927083800784896,899136914531393536,Tuomas Haarnoja,"['New paper on how hierarchies emerge naturally from maximum entropy policies with a latent space. These policies achieve state-of-the-art performance on standard benchmark tasks and can solve spare reward tasks. w/ @kristianhartika, @pabbeel &amp; S. Levine. <LINK> <LINK>']",http://arxiv.org/abs/1804.02808,"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives. ",Latent Space Policies for Hierarchical Reinforcement Learning
50,983900696599605253,781309796741947396,EEMS Group @ MIT (PI: Vivienne Sze),"['New paper (in collaboration with the Mobile Vision Team at Google) on ""NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications"" <LINK> #deeplearning']",https://arxiv.org/abs/1804.03230,"This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7$\times$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2). ","NetAdapt: Platform-Aware Neural Network Adaptation for Mobile
  Applications"
51,983800975663292418,2864832233,Sam Kriegman,"['Our new paper (<LINK>) in a nutshell: for the first time, we show robots that evolve to stiffen and soften in response to environmental signals as they move. <LINK>']",https://arxiv.org/abs/1804.02257,"Typically, AI researchers and roboticists try to realize intelligent behavior in machines by tuning parameters of a predefined structure (body plan and/or neural network architecture) using evolutionary or learning algorithms. Another but not unrelated longstanding property of these systems is their brittleness to slight aberrations, as highlighted by the growing deep learning literature on adversarial examples. Here we show robustness can be achieved by evolving the geometry of soft robots, their control systems, and how their material properties develop in response to one particular interoceptive stimulus (engineering stress) during their lifetimes. By doing so we realized robots that were equally fit but more robust to extreme material defects (such as might occur during fabrication or by damage thereafter) than robots that did not develop during their lifetimes, or developed in response to a different interoceptive stimulus (pressure). This suggests that the interplay between changes in the containing systems of agents (body plan and/or neural architecture) at different temporal scales (evolutionary and developmental) along different modalities (geometry, material properties, synaptic weights) and in response to different signals (interoceptive and external perception) all dictate those agents' abilities to evolve or learn capable and robust strategies. ","Interoceptive robustness through environment-mediated morphological
  development"
52,983798599988535298,185910194,Graham Neubig,"['New #NAACL2018 paper on ""Guiding NMT w/ Retrieved Translation Pieces"" <LINK>\nSimple method to retrieve similar sentences in the training corpus and add a bonus for n-grams appearing in the target to help NMT memorize phrases. Up to +6 BLEU on limited-domain data! <LINK>', 'Very proud of Jingyi, the first author: this is the last paper of her very nice PhD Thesis on Neural-Symbolic Machine Translation!', '@sjmielke It will be available here at some point: https://t.co/3Ub9eMeb24\nIn the mean time, you can google her papers.']",https://arxiv.org/abs/1804.02559,"One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect $n$-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call ""translation pieces"". We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation. ",Guiding Neural Machine Translation with Retrieved Translation Pieces
53,983703758868811776,775002133041152001,Panagiotis Tsiotras,['Our new paper on combining covariance steering with chance constraints. More cool applications on this idea coming up soon! \n\n<LINK>'],https://arxiv.org/abs/1804.02829,"This work addresses the optimal covariance control problem for stochastic discrete-time linear time-varying systems subject to chance constraints. Covariance steering is a stochastic control problem to steer the system state Gaussian distribution to another Gaussian distribution while minimizing a cost function. To the best of our knowledge, covariance steering problems have never been discussed with probabilistic chance constraints although it is a natural extension. In this work, first we show that, unlike the case with no chance constraints, the covariance steering with chance constraints problem cannot decouple the mean and covariance steering sub-problems. Then we propose an approach to solve the covariance steering with chance constraints problem by converting it to a semidefinite programming problem. The proposed algorithm is verified using two simple numerical simulations. ","Optimal Covariance Control for Stochastic Systems Under Chance
  Constraints"
54,983585523787882496,817369124519153664,Lucas Bechberger,"[""Just submitted a revised and extended version of last year's @BCS_SGAI paper to a special issue of Expert Systems: <LINK> Including new definitions for conceptual similarity and conceptual betweenness""]",https://arxiv.org/abs/1804.02393,"The highly influential framework of conceptual spaces provides a geometric way of representing knowledge. Instances are represented by points in a high-dimensional space and concepts are represented by regions in this space. In this article, we extend our recent mathematical formalization of this framework by providing quantitative mathematical definitions for measuring relations between concepts: We develop formal ways for computing concept size, subsethood, implication, similarity, and betweenness. This considerably increases the representational capabilities of our formalization and makes it the most thorough and comprehensive formalization of conceptual spaces developed so far. ","Formal Ways for Measuring Relations between Concepts in Conceptual
  Spaces"
55,983351855089668096,3199605543,Afonso S. Bandeira,['New paper on Deterministic guarantees for Burer-Monteiro factorizations of smooth semidefinite programs <LINK>'],https://arxiv.org/abs/1804.02008,"We consider semidefinite programs (SDPs) with equality constraints. The variable to be optimized is a positive semidefinite matrix $X$ of size $n$. Following the Burer--Monteiro approach, we optimize a factor $Y$ of size $n \times p$ instead, such that $X = YY^T$. This ensures positive semidefiniteness at no cost and can reduce the dimension of the problem if $p$ is small, but results in a non-convex optimization problem with a quadratic cost function and quadratic equality constraints in $Y$. In this paper, we show that if the set of constraints on $Y$ regularly defines a smooth manifold, then, despite non-convexity, first- and second-order necessary optimality conditions are also sufficient, provided $p$ is large enough. For smaller values of $p$, we show a similar result holds for almost all (linear) cost functions. Under those conditions, a global optimum $Y$ maps to a global optimum $X = YY^T$ of the SDP. We deduce old and new consequences for SDP relaxations of the generalized eigenvector problem, the trust-region subproblem and quadratic optimization over several spheres, as well as for the Max-Cut and Orthogonal-Cut SDPs which are common relaxations in stochastic block modeling and synchronization of rotations. ","Deterministic guarantees for Burer-Monteiro factorizations of smooth
  semidefinite programs"
56,983287291773968384,153063263,Jake Blackmore,"['New paper on the ArXiv: <LINK> joint work from @jqcDurNew, @imperialcollege centre for cold matter and @OxfordPhysics as part of the @QSUMproject. Demonstrating high coherence Ramsey measurements for quantum simulators built on CaF and RbCs!']",https://arxiv.org/abs/1804.02372,"We explore the uses of ultracold molecules as a platform for future experiments in the field of quantum simulation, focusing on two molecular species, $^{40}$Ca$^{19}$F and $^{87}$Rb$^{133}$Cs. We report the development of coherent quantum state control using microwave fields in both molecular species; this is a crucial ingredient for many quantum simulation applications. We demonstrate proof-of-principle Ramsey interferometry measurements with fringe spacings of $\sim 1~\rm kHz$ and investigate the dephasing time of a superposition of $N=0$ and $N=1$ rotational states when the molecules are confined. For both molecules, we show that a judicious choice of molecular hyperfine states minimises the impact of spatially varying transition-frequency shifts across the trap. For magnetically trapped $^{40}$Ca$^{19}$F we use a magnetically insensitive transition and observe a coherence time of 0.61(3) ms. For optically trapped $^{87}$Rb$^{133}$Cs we exploit an avoided crossing in the AC Stark shift and observe a maximum coherence time of 0.75(6) ms. ","Ultracold molecules for quantum simulation: rotational coherences in CaF
  and RbCs"
57,982956416410505218,131879500,John Ilee,"['CO overtone emission is a really useful tracer of small-scale discs around massive YSOs, but why do we only see it in ~25% of spectra? \n\nManfred Mann said it best - we‚Äôre just blinded by the light. \n\nNew paper out now: <LINK>\n\n#Astronomy #Arxiv #Astroph üòé <LINK>']",https://arxiv.org/abs/1804.01934,"To date, there is no explanation as to why disc-tracing CO first overtone (or `bandhead') emission is not a ubiquitous feature in low- to medium-resolution spectra of massive young stellar objects, but instead is only detected toward approximately 25 per cent of their spectra. In this paper, we investigate the hypothesis that only certain mass accretion rates result in detectable bandhead emission in the near infrared spectra of MYSOs. Using an analytic disc model combined with an LTE model of the CO emission, we find that high accretion rates ($\gtrsim 10^{-4}\,{\rm M}_{\odot}{\mathrm{yr}}^{-1}$) result in large dust sublimation radii, a larger contribution to the $K$-band continuum from hot dust at the dust sublimation radius, and therefore correspondingly lower CO emission with respect to the continuum. On the other hand, low accretion rates ($\lesssim10^{-6}\,{\rm M}_{\odot}{\mathrm{yr}}^{-1}$) result in smaller dust sublimation radii, a correspondingly smaller emitting area of CO, and thus also lower CO emission with respect to the continuum. In general, moderate accretion rates produce the most prominent, and therefore detectable, CO first overtone emission. We compare our findings to a recent near-infrared spectroscopic survey of MYSOs, finding results consistent with our hypothesis. We conclude that the detection rate of CO bandhead emission in the spectra of MYSOs could be the result of MYSOs exhibiting a range of mass accretion rates, perhaps due to the variable accretion suggested by recent multi-epoch observations of these objects. ","Blinded by the light: on the relationship between CO first overtone
  emission and mass accretion rate in massive young stellar objects"
58,982407265830490112,106843613,Jacob Haqq Misra,"['To transmit or not to transmit? Check out my new #METI paper ""Policy options for the radio detectability of Earth"" @METIintl\n<LINK>']",https://arxiv.org/abs/1804.01885,"The METI risk problem refers to the uncertain outcome of sending transmissions into space with the intention of messaging to extraterrestrial intelligence (METI). Here, I demonstrate that this uncertainty is undecidable by proving that that the METI risk problem reduces to the halting problem. This implies that any proposed moratorium on METI activities cannot be based solely on the requirement for new information. I discuss three policy resolutions to deal with this risk ambiguity. Precautionary malevolence assumes that contact with ETI is likely to cause net harm to humanity, which remains consistent with the call for a METI moratorium, while assumed benevolence states that METI is likely to yield net benefits to humanity. I also propose a policy of preliminary neutrality, which suggests that humanity should engage in both SETI (searching for extraterrestrial intelligence) and METI until either one achieves its first success. ",Policy options for the radio detectability of Earth
59,982399966155960320,2295554268,Justin Johnson,"['My new paper on generating images from scene graphs using graph convolution and GANs is up on arXiv! To appear at CVPR2018, with @agrimgupta92 and @drfeifei <LINK> <LINK>', '@skywalkeryxc @agrimgupta92 @drfeifei Yes - stay tuned!']",https://arxiv.org/abs/1804.01622,"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects. ",Image Generation from Scene Graphs
60,982310783257231360,720772140,Anvita Gupta,"['Check out our new paper! ""Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions"" <LINK>\n\nGAN architecture to produce genes &amp;optimize for secondary structure &amp; function of their proteins. Some GAN-produced alpha helices: <LINK>', '@CThurstonERAU HAHA right back at you Courtney!! Incredible job on Goldwater :)', ""@pfau @Miles_Brundage In the past VAEs empirically haven't performed well on high-noise genomic data (https://t.co/yahuBwCezr) and the latent code is often ignored (making interpolation hard). That being said, a full comparison of VAEs to our approach would be interesting.""]",https://arxiv.org/abs/1804.01694,"Generative Adversarial Networks (GANs) represent an attractive and novel approach to generate realistic data, such as genes, proteins, or drugs, in synthetic biology. Here, we apply GANs to generate synthetic DNA sequences encoding for proteins of variable length. We propose a novel feedback-loop architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene sequences for desired properties using an external function analyzer. The proposed architecture also has the advantage that the analyzer need not be differentiable. We apply the feedback-loop mechanism to two examples: 1) generating synthetic genes coding for antimicrobial peptides, and 2) optimizing synthetic genes for the secondary structure of their resulting peptides. A suite of metrics demonstrate that the GAN generated proteins have desirable biophysical properties. The FBGAN architecture can also be used to optimize GAN-generated datapoints for useful properties in domains beyond genomics. ","Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for
  Optimizing Protein Functions"
61,982251981577207808,766684686,Rodrigo D√≠az,"['<LINK>\n@NASAKepler @OsuPytheas Our paper on Kepler-419 is on arXiv. We revisit this intriguing exoplanetary system with new SOPHIE radial velocities, and using the powerful photodynamical modelling.', 'We strengthen the constraints on the mutual inclination of the planetary orbits and explore different scenarios to explain the origin of  Kepler-419.']",https://arxiv.org/abs/1804.01869,"Kepler-419 is a planetary system discovered by the Kepler photometry which is known to harbour two massive giant planets: [...] Here we present new radial velocity (RV) measurements secured over more than two years with the SOPHIE spectrograph, where both planets are clearly detected. The RV data is modelled together with the Kepler photometry using a photodynamical model. The inclusion of velocity information breaks the $MR^{-3}$ degeneracy inherent in timing data alone, allowing us to measure the absolute stellar and planetary radii and masses. With uncertainties of 12% and 13% for the stellar and inner planet radii, and 35%, 24%, and 35% for the masses of the star, planet b, and planet c respectively, these measurements are the most precise to date for a single host star system using this technique. The transiting planet mass is determined at better precision than the star mass. This shows that modelling the radial velocities and the light curve together in systems of dynamically interacting planets provides a way of characterising both the star and the planets without being limited by knowledge of the star. On the other hand, the period ratio and eccentricities place the Kepler-419 system in a sweet spot; had around twice as many transits been observed, the mass of the transiting planet could have been measured using its own TTVs. Finally, the origin of the Kepler-419 system is discussed. We show that the system is near a coplanar high-eccentricity secular fixed point, related to the alignment of the orbits, which has prevented the inner orbit from circularising. For most other relative apsidal orientations, planet b's orbit would be circular with a semi-major axis of 0.03 au. This suggests a mechanism for forming hot Jupiters in multiplanetary systems without the need of high mutual inclinations. ","SOPHIE velocimetry of Kepler transit candidates XVIII. Radial velocity
  confirmation, absolute masses and radii, and origin of the Kepler-419
  multiplanetary system"
62,982099219984015360,128070661,Ernesto Galv√£o,"[""New paper out! (With @Danjost and @FabioSciarrino's group.) How to witness genuine n-photon interference, as opposed to interference of &lt; n indistinguishable photons. Theory bonus: result on model using set intersections to represent 2-photon overlaps: <LINK>""]",https://arxiv.org/abs/1804.01334,"Bosonic interference is a fundamental physical phenomenon, and it is believed to lie at the heart of quantum computational advantage. It is thus necessary to develop practical tools to witness its presence, both for a reliable assessment of a quantum source and for fundamental investigations. Here we describe how linear interferometers can be used to unambiguously witness genuine n-boson indistinguishability. The amount of violation of the proposed witnesses bounds the degree of multi-boson indistinguishability, for which we also provide a novel intuitive model using set theory. We experimentally implement this test to bound the degree of 3-photon indistinguishability in states we prepare using parametric down-conversion. Our approach results in a convenient tool for practical photonic applications, and may inspire further fundamental advances based on the operational framework we adopt. ",Witnessing genuine multi-photon indistinguishability
63,981571359347163136,2840725487,Ravi Gupta,['Working on education and public outreach within @theDESurvey was a truly fun and rewarding experience. Read about our online outreach projects in a new paper here: <LINK> and check out the EDUCATION tab at <LINK>!'],https://arxiv.org/abs/1804.00591,"As large-scale international collaborations become the standard for astronomy research, a wealth of opportunities have emerged to create innovative education and public outreach (EPO) programming. In the past two decades, large collaborations have focused EPO strategies around published data products. Newer collaborations have begun to explore other avenues of public engagement before and after data are made available. We present a case study of the online EPO program of The Dark Energy Survey, currently one of the largest international astronomy collaborations actively taking data. DES EPO is unique at this scale in astronomy, as far as we are aware, as it evolved organically from scientists' passion for EPO and is entirely organized and implemented by the volunteer efforts of collaboration scientists. We summarize the strategy and implementation of eight EPO initiatives. For content distributed via social media, we present reach and user statistics over the 2016 calendar year. DES EPO online products reached ~2,500 users per post, and 94% of these users indicate a predisposition to science-related interests. We find no obvious correlation between post type and post reach, with the most popular posts featuring the intersections of science and art and/or popular culture. We conclude that one key issue of the online DES EPO program was designing material which would inspire new interest in science. The greatest difficulty of the online DES EPO program was sustaining scientist participation and collaboration support; the most successful programs are those which capitalized on the hobbies of participating scientists. We present statistics and recommendations, along with observations from individual experience, as a potentially instructive resource for scientists or EPO professionals interested in organizing EPO programs and partnerships for large science collaborations or organizations. ","New Science, New Media: An Assessment of the Online Education and Public
  Outreach Initiatives of The Dark Energy Survey"
64,981538107106394113,3323459854,Ronnie Clark,['Our new paper on deep dense visual #SLAM which will be presented as an oral @CVPR. Key idea: using a depth autoencoder conditioned on RGB allows us to optimize efficiently the depth (via the code) and pose. #ML #AR #CV\nPaper: <LINK>\nVideo: <LINK> <LINK>'],https://arxiv.org/abs/1804.00874,"The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only. We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM. ","CodeSLAM - Learning a Compact, Optimisable Representation for Dense
  Visual SLAM"
65,981531656854102016,264403483,Jared Sylvester,"['Happy to announce that my latest paper with @EdwardRaffML and Charles Nicholas, ""Engineering a Simplified 0-Bit Consistent Weighted Sampling"" is up on arXiv: <LINK>. As part of my new tradition, here is my haiku abstract:', ""Approx. Jaccard dist.:\nDon't sample temps you don't need\nYields one flop min-hash"", 'The abbreviations may seem like cheating, but the point of the paper is using shortened representations that are close enough to convey the right results, so in fact I am not cheating, I am just *doing poetry*.']",https://arxiv.org/abs/1804.00069,"The Min-Hashing approach to sketching has become an important tool in data analysis, information retrial, and classification. To apply it to real-valued datasets, the ICWS algorithm has become a seminal approach that is widely used, and provides state-of-the-art performance for this problem space. However, ICWS suffers a computational burden as the sketch size K increases. We develop a new Simplified approach to the ICWS algorithm, that enables us to obtain over 20x speedups compared to the standard algorithm. The veracity of our approach is demonstrated empirically on multiple datasets and scenarios, showing that our new Simplified CWS obtains the same quality of results while being an order of magnitude faster. ",Engineering a Simplified 0-Bit Consistent Weighted Sampling
66,981513410033651712,3124305238,Anna Scaife,['NGC891 as seen by @LOFAR in our new paper lead by @ddmulcahy @jodrellbank <LINK> <LINK>'],https://arxiv.org/abs/1804.00752,"Low-frequency radio continuum observations of edge-on galaxies are ideal to study cosmic-ray electrons (CREs) in halos via radio synchrotron emission and to measure magnetic field strengths. We obtained new observations of the edge-on spiral galaxy NGC 891 at 129-163 MHz with the LOw Frequency ARray (LOFAR) and at 13-18 GHz with the Arcminute Microkelvin Imager (AMI) and combine them with recent high-resolution Very Large Array (VLA) observations at 1-2 GHz, enabling us to study the radio continuum emission over two orders of magnitude in frequency. The spectrum of the integrated nonthermal flux density can be fitted by a power law with a spectral steepening towards higher frequencies or by a curved polynomial. Spectral flattening at low frequencies due to free-free absorption is detected in star-forming regions of the disk. The mean magnetic field strength in the halo is 7 +- 2 $\mu$G. The scale heights of the nonthermal halo emission at 146 MHz are larger than those at 1.5 GHz everywhere, with a mean ratio of 1.7 +- 0.3, indicating that spectral ageing of CREs is important and that diffusive propagation dominates. The halo scale heights at 146 MHz decrease with increasing magnetic field strengths which is a signature of dominating synchrotron losses of CREs. On the other hand, the spectral index between 146 MHz and 1.5 GHz linearly steepens from the disk to the halo, indicating that advection rather than diffusion is the dominating CRE transport process. This issue calls for refined modelling of CRE propagation. ","Investigation of the cosmic ray population and magnetic field strength
  in the halo of NGC 891"
67,981443382399643653,50343115,Thomas Kipf,"['New paper on learning hyperspherical latent spaces: Hyperspherical Variational Auto-Encoders (with @im_td, L. Falorsi, @nicola_decao, @jmtomczak). Useful trick for learning node embeddings in graphs and for semi-supervised learning. <LINK> <LINK>', '@riceasphait @im_td @nicola_decao @jmtomczak Thanks! All credit goes to @im_td, Luca Falorsi &amp; @nicola_decao - and to @jmtomczak for co-supervising. The paper is currently under review at UAI.', '@riceasphait @im_td @nicola_decao @jmtomczak Thanks! All the best for your submission as well :-)']",https://arxiv.org/abs/1804.00891,"The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or $\mathcal{S}$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, $\mathcal{N}$-VAE, in low dimensions on other data types. ",Hyperspherical Variational Auto-Encoders
68,981197622399655937,15035863,Noah Snavely,"['And for all y\'all intrinsic image fanatics out there -- Zhengqi also has a cool new paper on learning intrinsic images supervised with time-lapse data: ""Learning Intrinsic Image Decomposition by Watching the World"".\n\nWeb: <LINK>, arXiv: <LINK> <LINK>']",https://arxiv.org/abs/1804.00582,"Single-view intrinsic image decomposition is a highly ill-posed problem, and so a promising approach is to learn from large amounts of data. However, it is difficult to collect ground truth training data at scale for intrinsic images. In this paper, we explore a different approach to learning intrinsic images: observing image sequences over time depicting the same scene under changing illumination, and learning single-view decompositions that are consistent with these changes. This approach allows us to learn without ground truth decompositions, and to instead exploit information available from multiple images when training. Our trained model can then be applied at test time to single views. We describe a new learning framework based on this idea, including new loss functions that can be efficiently evaluated over entire sequences. While prior learning-based methods achieve good performance on specific benchmarks, we show that our approach generalizes well to several diverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild and Shading Annotations in the Wild. ",Learning Intrinsic Image Decomposition from Watching the World
69,981196012944547840,185910194,Graham Neubig,"['New #NAACL2018 paper on the ""Attentive Interaction Model"", a neural model that determines if arguments are convincing or not, and why: <LINK>\nIt identifies ""weaknesses"" in opinions using attention, and determines arguments that change the opinion holder\'s view. <LINK>']",https://arxiv.org/abs/1804.00065,"We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder's (OH's) reasoning and a challenger's argument, with the goal of predicting if the argument successfully changes the OH's view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH's reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH's reasoning and that of the challenger's argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH's change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones. ",Attentive Interaction Model: Modeling Changes in View in Argumentation
70,981161462575042560,706175245976248321,Oscar Viyuela,['Our new paper on the arXiv! More on Topological Superconductors.\n<LINK>'],https://arxiv.org/abs/1804.00007,"We compute the topological phase diagram of 2D tetragonal superconductors for the only possible nodeless pairing channels compatible with that crystal symmetry. Subject to a Zeeman field and spin-orbit coupling, we demonstrate that these superconductors show surprising topological features: non-trivial high Chern numbers, massive edge states, and zero-energy modes out of high symmetry points, even though the edge states remain topologically protected. Interestingly, one of these pairing symmetries, $d+id$, has been proposed to describe materials such as water-intercalated sodium cobaltates, bilayer silicene or highly doped monolayer graphene, which opens the way for further applications of our results. ",Topological Phases in Nodeless Tetragonal Superconductors
71,980996410198843392,2577596593,Chelsea Finn,"['New paper on integrating planning and representation learning! Optimize for representations that lead to effective goal-based planning for visual tasks; learned representation transfers surprisingly well. w/ @aravind7694, @ajabri, @pabbeel, &amp; Sergey Levine\n<LINK> <LINK>', 'Another example of the agent using the learned visual representation to reach a goal. https://t.co/dANdIxwSy4']",https://arxiv.org/abs/1804.00645,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities. ",Universal Planning Networks
72,991457010422923264,4666231375,Konstantin Batygin,"['The new #PlanetNine paper, led by the one and only Tali Khain from @UMich is up on arXiv. Tali worked with me and @plutokiller last summer to study the dependence of P9-induced dynamics on initial conditions and found lots of cool stuff. Check it out here: <LINK>']",https://arxiv.org/abs/1804.11281,"The observation that the orbits of long-period Kuiper Belt objects are anomalously clustered in physical space has recently prompted the Planet Nine hypothesis - the proposed existence of a distant and eccentric planetary member of our solar system. Within the framework of this model, a Neptune-like perturber sculpts the orbital distribution of distant Kuiper Belt objects through a complex interplay of resonant and secular effects, such that in addition to perihelion-circulating objects, the surviving orbits get organized into apsidally aligned and anti-aligned configurations with respect to Planet Nine's orbit. In this work, we investigate the role of Kuiper Belt initial conditions on the evolution of the outer solar system using numerical simulations. Intriguingly, we find that the final perihelion distance distribution depends strongly on the primordial state of the system, and demonstrate that a bimodal structure corresponding to the existence of both aligned and anti-aligned clusters is only reproduced if the initial perihelion distribution is assumed to extend well beyond $\sim 36$ AU. The bimodality in the final perihelion distance distribution is due to the existence of permanently stable objects, with the lower perihelion peak corresponding to the anti-aligned orbits and the higher perihelion peak corresponding to the aligned orbits. We identify the mechanisms which enable the persistent stability of these objects and locate the regions of phase space in which they reside. The obtained results contextualize the Planet Nine hypothesis within the broader narrative of solar system formation, and offer further insight into the observational search for Planet Nine. ","The Generation of the Distant Kuiper Belt by Planet Nine from an
  Initially Broad Perihelion Distribution"
73,991290195524882432,907232486735958018,Jaki Noronha-Hostler,"['We just posted a new paper on @nucl_th_  that has been 2 years in the making (find the right observables was the tricky part)!  We approach the question of the thermalization of charm quarks within the Quark Gluon Plasma from a completely new angle. <LINK>', 'We use the Equation of State with/without thermalized charm quarks and look at vn(pT) of all charged particles to find a strong preference for thermalized charm quarks in peripheral PbPb systems. We make predictions for XeXe, factorization breaking and event plane correlations.', 'Why is the thermalization of charm quarks interesting? For starters their mass (1.29 GeV) is much higher than the temperatures reached in PbPb collisions at the LHC (T~0.6 GeV) so it was thought that they would need much longer time scales to thermalize than lighter quarks.', ""Additionally, in the Early Universe the temperatures were much higher than heavy-ion collisions so we expect charm quarks to be thermalized (see https://t.co/MD05qhl2HK). Evidence of the thermalization of charm quarks shows we're probing the same Equation of State (T&lt;0.6 GeV).""]",https://arxiv.org/abs/1804.10661,"A long standing question in the field of heavy-ion collisions is whether charm quarks are thermalized within the Quark Gluon Plasma. In recent years, progress in lattice QCD simulations has led to reliable results for the equation of state of a system of 2+1 flavors (up, down, and strange) and 2+1+1 flavors (up, down, strange, and charm). We find that the equation of state strongly affects differential flow harmonics and a preference is seen for thermalized charm quarks at the LHC. Predictions are also made for the event-plane correlations at RHIC AuAu $\sqrt{s_{NN}}=200$ GeV collisions, and the scaling of differential flow observables and factorization breaking for all charged particles at LHC PbPb $\sqrt{s_{NN}}=5.02$ TeV collisions compared to LHC XeXe $\sqrt{s_{NN}}=5.44$ TeV collisions, which could be useful in answering the question: are charm quarks thermalized? ",Signatures of thermalized charm quarks in all charged flow observables
74,991266940374192133,776074951262699521,Ivan Titov,['Uploaded our new #ACL2018 paper: Inducing latent relations between entity mentions (no supervision / no coref system) helps to link them to a knowledge base (with @lephongxyz): <LINK>  #NLProc <LINK>'],https://arxiv.org/abs/1804.10637,"Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data. ",Improving Entity Linking by Modeling Latent Relations between Mentions
75,987272269612318720,218250514,Heiko Hamann,"['our new #GECCO2018 paper\nA Robot to Shape your Natural Plant\ncomplex #ML approach, #LSTM network as plant model, evolutionary computation to train a controller, a real plant does collision avoidance\n\n<LINK>\n<LINK>\n#robotics #evolution #plants #ai <LINK>']",https://arxiv.org/abs/1804.06682,"Bio-hybrid systems---close couplings of natural organisms with technology---are high potential and still underexplored. In existing work, robots have mostly influenced group behaviors of animals. We explore the possibilities of mixing robots with natural plants, merging useful attributes. Significant synergies arise by combining the plants' ability to efficiently produce shaped material and the robots' ability to extend sensing and decision-making behaviors. However, programming robots to control plant motion and shape requires good knowledge of complex plant behaviors. Therefore, we use machine learning to create a holistic plant model and evolve robot controllers. As a benchmark task we choose obstacle avoidance. We use computer vision to construct a model of plant stem stiffening and motion dynamics by training an LSTM network. The LSTM network acts as a forward model predicting change in the plant, driving the evolution of neural network robot controllers. The evolved controllers augment the plants' natural light-finding and tissue-stiffening behaviors to avoid obstacles and grow desired shapes. We successfully verify the robot controllers and bio-hybrid behavior in reality, with a physical setup and actual plants. ","A Robot to Shape your Natural Plant: The Machine Learning Approach to
  Model and Control Bio-Hybrid Systems"
76,986580608737570816,338526004,Sam Bowman,"['Niche-but-exciting new workshop paper with Nikita Nangia (@meloncholist): Current latent tree learning methods have trouble even on artificial data for which discovering the correct trees yields huge gains. <LINK>', ""@jrking0 @meloncholist Aagh. Three or four people spent a total of 8+ hours figuring out how to draw them (same code as in a previous paper), and they're still not quite what we wanted. It's modified 'forest'. Download the source from arXiv as needed."", ""@jrking0 @meloncholist For more background, this paper is the group's first project on the topic, and had a much longer page limit: https://t.co/kzf7zQHBdl"", '@yoavgo @meloncholist Good catch‚Äîthanks!', '@jekbradbury @meloncholist Yep‚Äîquite related (should have cited :/), though no equivalent to our tuning the dataset for the RNN/TreeRNN performance gap.']",https://arxiv.org/abs/1804.06028,"Latent tree learning models learn to parse a sentence without syntactic supervision, and use that parse to build the sentence representation. Existing work on such models has shown that, while they perform well on tasks like sentence classification, they do not learn grammars that conform to any plausible semantic or syntactic formalism (Williams et al., 2018a). Studying the parsing ability of such models in natural language can be challenging due to the inherent complexities of natural language, like having several valid parses for a single sentence. In this paper we introduce ListOps, a toy dataset created to study the parsing ability of latent tree models. ListOps sequences are in the style of prefix arithmetic. The dataset is designed to have a single correct parsing strategy that a system needs to learn to succeed at the task. We show that the current leading latent tree models are unable to learn to parse and succeed at ListOps. These models achieve accuracies worse than purely sequential RNNs. ",ListOps: A Diagnostic Dataset for Latent Tree Learning
77,985858608071143424,1022307097,PanagiotisBarkoutsos,['Our new paper on Entanglement generation in superconducting qubits using holonomic operations is on Arxiv <LINK> :)'],https://arxiv.org/abs/1804.04900,"We investigate a non-adiabatic holonomic operation that enables us to entangle two fixed-frequency superconducting transmon qubits attached to a common bus resonator. Two coherent microwave tones are applied simultaneously to the two qubits and drive transitions between the first excited resonator state and the second excited state of each qubit. The cyclic evolution within this effective 3-level $\Lambda$-system gives rise to a holonomic operation entangling the two qubits. Two-qubit states with 95\% fidelity, limited mainly by charge-noise of the current device, are created within $213~\rm{ns}$. This scheme is a step toward implementing a SWAP-type gate directly in an all-microwave controlled hardware platform. By extending the available set of two-qubit operations in the fixed-frequency qubit architecture, the proposed scheme may find applications in near-term quantum applications using variational algorithms to efficiently create problem-specific trial states. ","Entanglement generation in superconducting qubits using holonomic
  operations"
78,985090726932426752,2328493777,Gareth Tyson,['Preprint of our new @icwsm paper looking at online video piracy through the lens of content indexing sites &amp; streaming cyberlockers! <LINK> @gianluca_string @SteveUhlig'],https://arxiv.org/abs/1804.02679,"Online video piracy (OVP) is a contentious topic, with strong proponents on both sides of the argument. Recently, a number of illegal websites, called streaming cyberlockers, have begun to dominate OVP. These websites specialise in distributing pirated content, underpinned by third party indexing services offering easy-to-access directories of content. This paper performs the first exploration of this new ecosystem. It characterises the content, as well the streaming cyberlockers' individual attributes. We find a remarkably centralised system with just a few networks, countries and cyberlockers underpinning most provisioning. We also investigate the actions of copyright enforcers. We find they tend to target small subsets of the ecosystem, although they appear quite successful. 84% of copyright notices see content removed. ",Movie Pirates of the Caribbean: Exploring Illegal Streaming Cyberlockers
79,984128755063971840,503452360,William Wang,['Our new #NAACL2018 paper on hate speech detection is now available: <LINK> joint representation learning with user history and randomized algorithm based inter-user similarity augmentation. #NLProc <LINK>'],https://arxiv.org/abs/1804.03124,"Hate speech detection is a critical, yet challenging problem in Natural Language Processing (NLP). Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor. The central problem is that social media posts are short and noisy, and most existing hate speech detection solutions take each post as an isolated input instance, which is likely to yield high false positive and negative rates. In this paper, we radically improve automated hate speech detection by presenting a novel model that leverages intra-user and inter-user representation learning for robust hate speech detection on Twitter. In addition to the target Tweet, we collect and analyze the user's historical posts to model intra-user Tweet representations. To suppress the noise in a single Tweet, we also model the similar Tweets posted by all other users with reinforced inter-user representation learning techniques. Experimentally, we show that leveraging these two representations can significantly improve the f-score of a strong bidirectional LSTM baseline model by 10.1%. ","Leveraging Intra-User and Inter-User Representation Learning for
  Automated Hate Speech Detection"
80,983994363880591360,3350628106,Christoffer Nellaker,"[""Super excited!  Our new paper on deep learning for computational phenotyping of placenta histology. Pleasure working with @C_Glastonbury @michaelferlaino @ceclindgren @bdi_oxford and coauthors in the Nuffield Department of Women's &amp; Reproductive Health.   <LINK>""]",https://arxiv.org/abs/1804.03270,"The placenta is a complex organ, playing multiple roles during fetal development. Very little is known about the association between placental morphological abnormalities and fetal physiology. In this work, we present an open sourced, computationally tractable deep learning pipeline to analyse placenta histology at the level of the cell. By utilising two deep Convolutional Neural Network architectures and transfer learning, we can robustly localise and classify placental cells within five classes with an accuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic knowledge that is capable of both stratifying five distinct cell populations and learn intraclass phenotypic variance. We envisage that the automation of this pipeline to population scale studies of placenta histology has the potential to improve our understanding of basic cellular placental biology and its variations, particularly its role in predicting adverse birth outcomes. ",Towards Deep Cellular Phenotyping in Placental Histology
81,981394126481559552,2545244784,Joss Bland-Hawthorn,['Check out new FIRE paper on how the Milky Way today (right) has evolved since z~5 (left). What is a progenitor? Where are the oldest stars today? Can we ever reconstruct? Surprises in store...\n\n<LINK> <LINK>'],https://arxiv.org/abs/1804.00659,"The oldest stars in the Milky Way (MW) bear imprints of the Galaxy's early assembly history. We use FIRE cosmological zoom-in simulations of three MW-mass disk galaxies to study the spatial distribution, chemistry, and kinematics of the oldest surviving stars ($z_{\rm form} \gtrsim 5$) in MW-like galaxies. We predict the oldest stars to be less centrally concentrated at $z=0$ than stars formed at later times as a result of two processes. First, the majority of the oldest stars are not formed $\textit{in situ}$ but are accreted during hierarchical assembly. These $\textit{ex situ}$ stars are deposited on dispersion-supported, halo-like orbits but dominate over old stars formed $\textit{in situ}$ in the solar neighborhood, and in some simulations, even in the galactic center. Secondly, old stars formed $\textit{in situ}$ are driven outwards by bursty star formation and energetic feedback processes that create a time-varying gravitational potential at $z\gtrsim 2$, similar to the process that creates dark matter cores and expands stellar orbits in bursty dwarf galaxies. The total fraction of stars that are ancient is more than an order of magnitude higher for sight lines $\textit{away}$ from the bulge and inner halo than for inward-looking sight lines. Although the task of identifying specific stars as ancient remains challenging, we anticipate that million-star spectral surveys and photometric surveys targeting metal-poor stars already include hundreds of stars formed before $z=5$. We predict most of these targets to have higher metallicity ($-3 < \rm [Fe/H] < -2$) than the most extreme metal-poor stars. ",Where are the most ancient stars in the Milky Way?
82,993144530571493378,3433516535,Miguel Hern√°n,"['Data scientists define their work as ‚Äúgaining insights‚Äù or ‚Äúextracting meaning‚Äù from data. That is way too vague. \n\nWe propose that the contributions of #datascience can be organized into 3 classes of tasks:\n1. description\n2. prediction\n3. causal inference\n<LINK> <LINK>', '@chrisbboyer Thanks, Christopher.\n\nMullainathan &amp; Speiss do a terrific job at explaining how machine learning works in simple terms. They also explain the distinction between prediction and causal inference, though that is not the main objective of their paper. https://t.co/8o6qboakq4', '@kerinalthoff Several epidemiologists, biostatisticians (like Vittinghoff et al.), econometricians... have proposed a similar classification. We are looking for papers or textbooks that explicitly define these 3 categories. Suggestions welcome. The older the references, the better.']",https://arxiv.org/abs/1804.10846,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists. ","Data science is science's second chance to get causal inference right: A
  classification of data science tasks"
83,990793583883042817,930224996785332224,"Jacob White, PhD",['Check out my new paper on measuring the radio emission of A-type stars! This work has important ramifications for characterizing debris disks (think exo-asteroid belts) so that we can more accurately study the amount and distribution of debris around stars\n<LINK>'],https://arxiv.org/abs/1804.10206,"In the early stages of planet formation, small dust grains grow to become mm sized particles in debris disks around stars. These disks can in principle be characterized by their emission at submillimeter and millimeter wavelengths. Determining both the occurrence and abundance of debris in unresolved circumstellar disks of A-type main-sequence stars requires that the stellar photospheric emission be accurately modeled. To better constrain the photospheric emission for such systems, we present observations of Sirius A, an A-type star with no known debris, from the JCMT, SMA, and VLA at 0.45, 0.85, 0.88, 1.3, 6.7, and 9.0 mm. We use these observations to inform a PHOENIX model of Sirius A's atmosphere. We find the model provides a good match to these data and can be used as a template for the submm/mm emission of other early A-type stars where unresolved debris may be present. The observations are part of an ongoing observational campaign entitled Measuring the Emission of Stellar Atmospheres at Submm/mm wavelengths (MESAS) ","MESAS: Measuring the Emission of Stellar Atmospheres at Submm/mm
  wavelengths"
84,989389248624254976,948995274961309697,Andrea Botteon,"['My latest paper has been accepted! We studied the double cluster A1758 with @LOFAR, GMRT, @TheNRAO VLA and @chandraxray data and discovered the second double radio halo system known so far!\nYes, Italy joined @LOFAR! #irapapers @IRA_INAF @HambObs @ASTRON_NL\n<LINK> <LINK>', 'We also reported the presence of a candidate radio relic and of a possible bridge of radio emission connecting the two clusters. A transversal shock generated in the initial stage of the merger between A1758N and A1758S could be responsible of the formation of the radio bridge. https://t.co/w9gchThsTw']",https://arxiv.org/abs/1804.09187,"Radio halos and radio relics are diffuse synchrotron sources that extend over Mpc-scales and are found in a number of merger galaxy clusters. They are believed to form as a consequence of the energy that is dissipated by turbulence and shocks in the intra-cluster medium (ICM). However, the precise physical processes that generate these steep synchrotron spectrum sources are still poorly constrained. We present a new LOFAR observation of the double galaxy cluster Abell 1758. This system is composed of A1758N, a massive cluster hosting a known giant radio halo, and A1758S, which is a less massive cluster whose diffuse radio emission is confirmed here for the first time. Our observations have revealed a radio halo and a candidate radio relic in A1758S, and a suggestion of emission along the bridge connecting the two systems which deserves confirmation. We combined the LOFAR data with archival VLA and GMRT observations to constrain the spectral properties of the diffuse emission. We also analyzed a deep archival Chandra observation and used this to provide evidence that A1758N and A1758S are in a pre-merger phase. The ICM temperature across the bridge that connects the two systems shows a jump which might indicate the presence of a transversal shock generated in the initial stage of the merger. ","LOFAR discovery of a double radio halo system in Abell 1758 and
  radio/X-ray study of the cluster pair"
85,988941428012105728,503452360,William Wang,"['Our #ACL2018 visual storytelling paper suggested that many automated generation metrics could be easily gamed! We propose an adversarial reward learning method to understand the incentives of good stories, achieving best results in human evaluation #NLProc <LINK> <LINK>']",https://arxiv.org/abs/1804.09160,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems. ","No Metrics Are Perfect: Adversarial Reward Learning for Visual
  Storytelling"
86,988211748867072000,806058672619212800,Guillaume Lample,"['New paper on unsupervised MT! <LINK> We propose two models (neural and phrase based) that both improve the state of the art by more than 11 BLEU. By combining them we reach up to 27 BLEU points on WMT14, without using a single parallel sentence.', '@universeinanegg @AaronJaech I think the important things are: 1) having data in the same domain than the data you want to translate (i.e. we evaluate on newstest so we use News Crawl). 2) having monolingual corpora in the two languages that allow you to generate a good dictionary in an unsupervised way.', '@universeinanegg @AaronJaech Clearly, the monolingual corpora you use to train your embeddings will have a significant impact on the quality of your embeddings, i.e. Table 2 in https://t.co/u6AdELYbsR where you can get more than 20% word translation accuracy based on what you train your embeddings on.', '@universeinanegg @AaronJaech That being said, we obtain the same word translation accuracy of ~80% P@1 if we train the embeddings on the parallel data of WMT en-fr, or on Wikipedia, or on News Crawl, or on Common Crawl, and clearly the parallel data of WMT en-fr is a much more similar domain than News Crawl.']",https://arxiv.org/abs/1804.07755,"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available. ",Phrase-Based & Neural Unsupervised Machine Translation
87,986765345708011525,4639078397,John Wise,"[""Do black holes from the first stars grow early on (z&gt;8)? We find that it's rare to have any strong accretion events.\n\nWhy? These BHs wander around the galaxy, rarely encountering clumps that are quickly eroded by stars. Led by @aBrittonSmith <LINK>"", 'The green shows the ""accretability"" of the gas in the halo with the most (77) black holes. Halo mass = 1.6e9 Msun. Co-authors: @jaregan, @TurloughDownes, @bwoshea, and M. Norman https://t.co/pCeN97I0Ww']",http://arxiv.org/abs/1804.06477,"The formation of stellar mass black holes from the remnants of Population III stars provides a source of initial black hole seeds with the potential to grow into intermediate or, in rare cases, possibly supermassive black holes. We use the Renaissance simulation suite to follow the growth of over 15,000 black holes born into mini-haloes in the early Universe. We compute the evolution of the black holes by post-processing individual remnant Population III star particles in the Renaissance simulation snapshots. The black holes populate haloes from 10$^{6}$ M$_{\odot}$ up to 10$^{9}$ M$_{\odot}$. We find that all of the black holes display very inefficient growth. On average the black holes increase their initial mass by a factor 10$^{-5}$, with the most active black holes increasing their mass by approximately 10%. Only a single black hole experiences any period of super-Eddington accretion, but the duration is very short and not repeated. Furthermore, we find no correlation of black hole accretion with halo mass in the mass range sampled. Within most haloes, we identify clumps of cool, dense gas for which accretion rates would be high, but instances of black holes encountering these clumps are rare and short-lived. Star formation competes with black hole growth by consuming available gas and driving down accretion rates through feedback. We conclude that the black holes born from Population III remnants do not form a significant population of intermediate mass black holes in the early Universe and will need to wait until later times to undergo significant accretion, if at all. ","The Growth of Black Holes from Population III Remnants in the
  Renaissance Simulations"
88,986703840383610880,2205401466,Matthias Steinmetz,"[""Jennifer Wojno's most recent paper on ArXiv. With TGAS priors we can study kinematics as function of population age - can't wait for Gaia DR2 #WaitingforGaia <LINK>""]",https://arxiv.org/abs/1804.06379,"We explore the connections between stellar age, chemistry, and kinematics across a Galactocentric distance of $7.5 < R\,(\mathrm{kpc}) < 9.0$, using a sample of $\sim 12\,000$ intermediate-mass (FGK) turnoff stars observed with the RAdial Velocity Experiment (RAVE) survey. The kinematics of this sample are determined using radial velocity measurements from RAVE, and parallax and proper motion measurements from the Tycho-Gaia Astrometric Solution (TGAS). In addition, ages for RAVE stars are determined using a Bayesian method, taking TGAS parallaxes as a prior. We divide our sample into young ($0 < \tau < 3$ Gyr) and old ($8 < \tau < 13$ Gyr) populations, and then consider different metallicity bins for each of these age groups. We find significant differences in kinematic trends of young and old, metal-poor and metal-rich, stellar populations. In particular, we find a strong metallicity dependence in the mean Galactocentric radial velocity as a function of radius ($\partial {V_{\rm R}}/\partial R$) for young stars, with metal-rich stars having a much steeper gradient than metal-poor stars. For $\partial {V_{\phi}}/\partial R$, young, metal-rich stars significantly lag the LSR with a slightly positive gradient, while metal-poor stars show a negative gradient above the LSR. We interpret these findings as correlations between metallicity and the relative contributions of the non-axisymmetries in the Galactic gravitational potential (the spiral arms and the bar) to perturb stellar orbits. ","Correlations between age, kinematics, and chemistry as seen by the RAVE
  survey"
89,986687187726069761,277401500,David W Hogg,"['In which we test 5e8 models on each of 1e5 stars and find 4e3 binary companions. Congratulations, @adrianprw\n<LINK>']",https://arxiv.org/abs/1804.04662,"Multi-epoch radial velocity measurements of stars can be used to identify stellar, sub-stellar, and planetary-mass companions. Even a small number of observation epochs can be informative about companions, though there can be multiple qualitatively different orbital solutions that fit the data. We have custom-built a Monte Carlo sampler (The Joker) that delivers reliable (and often highly multi-modal) posterior samplings for companion orbital parameters given sparse radial-velocity data. Here we use The Joker to perform a search for companions to 96,231 red-giant stars observed in the APOGEE survey (DR14) with $\geq 3$ spectroscopic epochs. We select stars with probable companions by making a cut on our posterior belief about the amplitude of the stellar radial-velocity variation induced by the orbit. We provide (1) a catalog of 320 companions for which the stellar companion properties can be confidently determined, (2) a catalog of 4,898 stars that likely have companions, but would require more observations to uniquely determine the orbital properties, and (3) posterior samplings for the full orbital parameters for all stars in the parent sample. We show the characteristics of systems with confidently determined companion properties and highlight interesting systems with candidate compact object companions. ","Binary companions of evolved stars in APOGEE DR14: Search method and
  catalog of ~5,000 companions"
90,986426430660730880,17033890,"Justin Sybrandt, Ph.D.","['Are Abstracts Enough for for Hypothesis Generation?\nWe did a study on our HG system and looked at how different datasets effect our results.\nTL;TR: Full-text helps, but takes 45x longer.\nLonger abstracts are a great middle ground.\n#BigData #MachineLearning <LINK>']",https://arxiv.org/abs/1804.05942,"The potential for automatic hypothesis generation (HG) systems to improve research productivity keeps pace with the growing set of publicly available scientific information. But as data becomes easier to acquire, we must understand the effect different textual data sources have on our resulting hypotheses. Are abstracts enough for HG, or does it need full-text papers? How many papers does an HG system need to make valuable predictions? How sensitive is a general-purpose HG system to hyperparameter values or input quality? What effect does corpus size and document length have on HG results? To answer these questions we train multiple versions of knowledge network-based HG system, Moliere, on varying corpora in order to compare challenges and trade offs in terms of result quality and computational requirements. Moliere generalizes main principles of similar knowledge network-based HG systems and reinforces them with topic modeling components. The corpora include the abstract and full-text versions of PubMed Central, as well as iterative halves of MEDLINE, which allows us to compare the effect document length and count has on the results. We find that, quantitatively, corpora with a higher median document length result in marginally higher quality results, yet require substantially longer to process. However, qualitatively, full-length papers introduce a significant number of intruder terms to the resulting topics, which decreases human interpretability. Additionally, we find that the effect of document length is greater than that of document count, even if both sets contain only paper abstracts. Reproducibility: Our code and data are available at github.com/jsybran/moliere, and bit.ly/2GxghpM respectively. ",Are Abstracts Enough for Hypothesis Generation?
91,986203496390197248,977906884886827008,Marcos Mari√±o,['What is quantum geometry? How does it become classical? We study a toy model with mathematical applications in <LINK> <LINK>'],https://arxiv.org/abs/1804.05574,"Topological strings on toric Calabi--Yau threefolds can be defined non-perturbatively in terms of a free Fermi gas of N particles. Using this approach, we propose a definition of quantum mirror curves as quantum distributions on phase space. The quantum distribution is obtained as the Wigner transform of the reduced density matrix of the Fermi gas. We show that the classical mirror geometry emerges in the strongly coupled, large N limit in which hbar ~ N. In this limit, the Fermi gas has effectively zero temperature, and the Wigner distribution becomes sharply supported on the interior of the classical mirror curve. The quantum fluctuations around the classical limit turn out to be captured by an improved version of the universal scaling form of Balazs and Zipfel. ",Quantum curves as quantum distributions
92,984818834417659905,525932400,Ashton Anderson,['How do constraints affect content? We studied how tweets changed just before and after Twitter switched from 140 to 280 characters. New work in ICWSM with @krisgligoric and @cervisiarius <LINK>'],https://arxiv.org/abs/1804.02318,"It is often said that constraints affect creative production, both in terms of form and quality. Online social media platforms frequently impose constraints on the content that users can produce, limiting the range of possible contributions. Do these restrictions tend to push creators towards producing more or less successful content? How do creators adapt their contributions to fit the limits imposed by social media platforms? To answer these questions, we conduct an observational study of a recent event: on November 7, 2017, Twitter changed the maximum allowable length of a tweet from 140 to 280 characters, thereby significantly altering its signature constraint. In the first study of this switch, we compare tweets with nearly or exactly 140 characters before the change to tweets of the same length posted after the change. This setup enables us to characterize how users alter their tweets to fit the constraint and how this affects their tweets' success. We find that in response to a length constraint, users write more tersely, use more abbreviations and contracted forms, and use fewer definite articles. Also, although in general tweet success increases with length, we find initial evidence that tweets made to fit the 140-character constraint tend to be more successful than similar-length tweets written when the constraint was removed, suggesting that the length constraint improved tweet quality. ","How Constraints Affect Content: The Case of Twitter's Switch from 140 to
  280 Characters"
93,984518582783574016,2383638403,Savvas Zannettou,"['Ever wondered what are the different types of mis/disinformation, as well as the various actors and their motives? \nFind out in our latest survey paper: <LINK>\nAlso, we report useful research directions for various lines of work.\n@msirivia @jhblackb @kourtellis']",https://arxiv.org/abs/1804.03461,"A new era of Information Warfare has arrived. Various actors, including state-sponsored ones, are weaponizing information on Online Social Networks to run false information campaigns with targeted manipulation of public opinion on specific topics. These false information campaigns can have dire consequences to the public: mutating their opinions and actions, especially with respect to critical world events like major elections. Evidently, the problem of false information on the Web is a crucial one, and needs increased public awareness, as well as immediate attention from law enforcement agencies, public institutions, and in particular, the research community. In this paper, we make a step in this direction by providing a typology of the Web's false information ecosystem, comprising various types of false information, actors, and their motives. We report a comprehensive overview of existing research on the false information ecosystem by identifying several lines of work: 1) how the public perceives false information; 2) understanding the propagation of false information; 3) detecting and containing false information on the Web; and 4) false information on the political stage. In this work, we pay particular attention to political false information as: 1) it can have dire consequences to the community (e.g., when election results are mutated) and 2) previous work show that this type of false information propagates faster and further when compared to other types of false information. Finally, for each of these lines of work, we report several future research directions that can help us better understand and mitigate the emerging problem of false information dissemination on the Web. ","The Web of False Information: Rumors, Fake News, Hoaxes, Clickbait, and
  Various Other Shenanigans"
94,984015347123662848,829415362647187456,Vanessa Graber,['New paper on the arXiv: we studied rapid crust coupling due to Kelvin wave excitations and the corresponding impact on glitch rises in superfluid neutron stars <LINK>'],https://arxiv.org/abs/1804.02706,"Pulsar glitches provide a unique way to study neutron star microphysics because short post-glitch dynamics are directly linked to strong frictional processes on small scales. To illustrate this connection between macroscopic observables and microphysics, we review calculations of vortex interactions focusing on Kelvin wave excitations and determine the corresponding mutual friction strength for realistic microscopic parameters in the inner crust. These density-dependent crustal coupling profiles are combined with a simplified treatment of the core coupling and implemented in a three-component neutron star model to construct a predictive framework for glitch rises. As a result of the density-dependent dynamics, we find the superfluid to transfer angular momentum to different parts of the crust and the core on different timescales. This can cause the spin frequency change to become non-monotonic in time, allowing for a maximum value much larger than the measured glitch size, as well as a delay in the recovery. The exact shape of the calculated glitch rise is strongly dependent on the relative strength between the crust and core mutual friction, providing the means to probe not only the crustal superfluid but also the deeper neutron star interior. To demonstrate the potential of this approach, we compare our predictive model with the first pulse-to-pulse observations recorded during the December 2016 glitch of the Vela pulsar. Our analysis suggests that the glitch rise behavior is relatively insensitive to the crustal mutual friction strength as long as $\mathcal{B} \gtrsim 10^{-3}$, while being strongly dependent on the core coupling strength, which we find to be in the range $3 \times 10^{-5} \lesssim \mathcal{B}_{\rm core} \lesssim 10^{-4}$. ",Glitch rises as a test for rapid superfluid coupling in neutron stars
95,983613082810318848,304669884,J Fern√°ndez Rossier,"['Our latest preprint in arXiv,  Van der Waals spin valves.  \n<LINK>\nWe propose that spin proximity effect, ,can drive metal-insulator transitions in graphene bilayers, making awesome spin valves.  It should work CrI3/graphene-BL/CrI3 <LINK>']",https://arxiv.org/abs/1804.03021,"We propose spin valves where a 2D non-magnetic conductor is intercalated between two ferromagnetic insulating layers. In this setup, the relative orientation of the magnetizations of the insulating layers can have a strong impact on the in-plane conductivity of the 2D conductor. We first show this for a graphene bilayer, described with a tight-binding model, placed between two ferromagnetic insulators. In the anti-parallel configuration, a band gap opens at the Dirac point, whereas in the parallel configuration, the graphene bilayer remains conducting. We then compute the electronic structure of graphene bilayer placed between two monolayers of the ferromagnetic insulator CrI$_3$, using density functional theory. Consistent with the model, we find that a gap opens at the Dirac point only in the antiparallel configuration. ",Van der Waals spin valves
96,983543750176550912,19149703,Karina Voggel ‚ú®üî≠üèÉüèº‚Äç‚ôÄÔ∏è,"['Today is double ""Are there black hole in UCDs"" day:\nFirst one is student in our Group here at the @UUtah, Chris Ahn, that looked at M59-UCD3, the current record holder for the most massive UCD. And we find a BH in it! <LINK>', 'And secondly we also looked at UCD3 int he Fornax Cluster and found that it has probably a 3.5 million solar mass BH in its center! https://t.co/sc7bSL1QAA', 'So what does that now mean for UCDs? The main discussion about them was whether they are the stripped nuclei of dwarf galaxies. Those with SMBH seem to be exactly that!', 'So currently we now have a grand total of 5 UCDs above 10^7M_solar that host a SMBH and are thus stripped nuclei. Whereas the two low mass ones I worked on do not host a BH.', 'Main takeaway: It appears thus that for the highest mass UCDs the stripped nuclei fraction is extremely high and most of them are stripped nuclei!!', 'And if most massive UCDs host a SMBH this could have important consequences for the number density of SMBHs in the local Universe! So stay tuned, there is more to come!', 'And on top of it all they make some pretty pictures. On the left the Velocity&amp;dispersion map of M59-UCD3 and on the right the radial dispersion profile of Fornax-UCD3! https://t.co/0Sj8TRoUG3']",https://arxiv.org/abs/1804.02399,"We examine the internal properties of the most massive ultracompact dwarf galaxy (UCD), M59-UCD3, by combining adaptive optics assisted near-IR integral field spectroscopy from Gemini/NIFS, and Hubble Space Telescope (HST) imaging. We use the multi-band HST imaging to create a mass model that suggests and accounts for the presence of multiple stellar populations and structural components. We combine these mass models with kinematics measurements from Gemini/NIFS to find a best-fit stellar mass-to-light ratio ($M/L$) and black hole (BH) mass using Jeans Anisotropic Models (JAM), axisymmetric Schwarzschild models, and triaxial Schwarzschild models. The best fit parameters in the JAM and axisymmetric Schwarzschild models have black holes between 2.5 and 5.9 million solar masses. The triaxial Schwarzschild models point toward a similar BH mass, but show a minimum $\chi^2$ at a BH mass of $\sim 0$. Models with a BH in all three techniques provide better fits to the central $V_{rms}$ profiles, and thus we estimate the BH mass to be $4.2^{+2.1}_{-1.7} \times 10^{6}$ M$_\odot$ (estimated 1$\sigma$ uncertainties). We also present deep radio imaging of M59-UCD3 and two other UCDs in Virgo with dynamical BH mass measurements, and compare these to X-ray measurements to check for consistency with the fundamental plane of BH accretion. We detect faint radio emission in M59cO, but find only upper limits for M60-UCD1 and M59-UCD3 despite X-ray detections in both these sources. The BH mass and nuclear light profile of M59-UCD3 suggests it is the tidally stripped remnant of a $\sim$10$^{9-10}$ M$_\odot$ galaxy. ",The Black Hole in the Most Massive Ultracompact Dwarf Galaxy M59-UCD3
97,983179506562236418,12306892,Neal Weiner,"['My first foray into time-domain astrometry! You can find dark objects by looking at their lens-induced motions. We find that you can find MACHOs, dense halos, and, if it‚Äôs there, Planet 9 <LINK>']",https://arxiv.org/abs/1804.01991,"Halometry---mapping out the spectrum, location, and kinematics of nonluminous structures inside the Galactic halo---can be realized via variable weak gravitational lensing of the apparent motions of stars and other luminous background sources. Modern astrometric surveys provide unprecedented positional precision along with a leap in the number of cataloged objects. Astrometry thus offers a new and sensitive probe of collapsed dark matter structures over a wide mass range, from one millionth to several million solar masses. It opens up a window into the spectrum of primordial curvature fluctuations with comoving wavenumbers between $5~\text{Mpc}^{-1}$ and $10^5~\text{Mpc}^{-1}$, scales hitherto poorly constrained. We outline detection strategies based on three classes of observables---multi-blips, templates, and correlations---that take advantage of correlated effects in the motion of many background light sources that are produced through time-domain gravitational lensing. While existing techniques based on single-source observables such as outliers and mono-blips are best suited for point-like lens targets, our methods offer parametric improvements for extended lens targets such as dark matter subhalos. Multi-blip lensing events may also unveil the existence, location, and mass of planets in the outer reaches of the Solar System, where they would likely have escaped detection by direct imaging. ",Halometry from Astrometry
98,981526934088704001,952949678533849088,Kareem El-Badry,"[""On the ArXiv: where are the ancient stars in the MW, and how did they get there? We propose that the high-z MW progenitor's shallow potential felt the same feedback-driven fluctuations that create cores in dwarfs, and that these drove old stars outward. <LINK> <LINK>"", 'Another takeaway, which has been shown before but perhaps is not fully appreciated, is that in CDM, a dominant fraction of the oldest stars in MW-like galaxies today formed ex situ. This is true even if you select stars in the inner Galaxy (not just the halo!) https://t.co/buBb3fpEGk', 'also featuring @JossBlandHawtho @AndrewWetzel @bigticketdw @MBKplus @SheaGKosmo ++']",https://arxiv.org/abs/1804.00659,"The oldest stars in the Milky Way (MW) bear imprints of the Galaxy's early assembly history. We use FIRE cosmological zoom-in simulations of three MW-mass disk galaxies to study the spatial distribution, chemistry, and kinematics of the oldest surviving stars ($z_{\rm form} \gtrsim 5$) in MW-like galaxies. We predict the oldest stars to be less centrally concentrated at $z=0$ than stars formed at later times as a result of two processes. First, the majority of the oldest stars are not formed $\textit{in situ}$ but are accreted during hierarchical assembly. These $\textit{ex situ}$ stars are deposited on dispersion-supported, halo-like orbits but dominate over old stars formed $\textit{in situ}$ in the solar neighborhood, and in some simulations, even in the galactic center. Secondly, old stars formed $\textit{in situ}$ are driven outwards by bursty star formation and energetic feedback processes that create a time-varying gravitational potential at $z\gtrsim 2$, similar to the process that creates dark matter cores and expands stellar orbits in bursty dwarf galaxies. The total fraction of stars that are ancient is more than an order of magnitude higher for sight lines $\textit{away}$ from the bulge and inner halo than for inward-looking sight lines. Although the task of identifying specific stars as ancient remains challenging, we anticipate that million-star spectral surveys and photometric surveys targeting metal-poor stars already include hundreds of stars formed before $z=5$. We predict most of these targets to have higher metallicity ($-3 < \rm [Fe/H] < -2$) than the most extreme metal-poor stars. ",Where are the most ancient stars in the Milky Way?
99,991290195524882432,907232486735958018,Jaki Noronha-Hostler,"['We just posted a new paper on @nucl_th_  that has been 2 years in the making (find the right observables was the tricky part)!  We approach the question of the thermalization of charm quarks within the Quark Gluon Plasma from a completely new angle. <LINK>', 'We use the Equation of State with/without thermalized charm quarks and look at vn(pT) of all charged particles to find a strong preference for thermalized charm quarks in peripheral PbPb systems. We make predictions for XeXe, factorization breaking and event plane correlations.', 'Why is the thermalization of charm quarks interesting? For starters their mass (1.29 GeV) is much higher than the temperatures reached in PbPb collisions at the LHC (T~0.6 GeV) so it was thought that they would need much longer time scales to thermalize than lighter quarks.', ""Additionally, in the Early Universe the temperatures were much higher than heavy-ion collisions so we expect charm quarks to be thermalized (see https://t.co/MD05qhl2HK). Evidence of the thermalization of charm quarks shows we're probing the same Equation of State (T&lt;0.6 GeV).""]",https://arxiv.org/abs/1804.10661,"A long standing question in the field of heavy-ion collisions is whether charm quarks are thermalized within the Quark Gluon Plasma. In recent years, progress in lattice QCD simulations has led to reliable results for the equation of state of a system of 2+1 flavors (up, down, and strange) and 2+1+1 flavors (up, down, strange, and charm). We find that the equation of state strongly affects differential flow harmonics and a preference is seen for thermalized charm quarks at the LHC. Predictions are also made for the event-plane correlations at RHIC AuAu $\sqrt{s_{NN}}=200$ GeV collisions, and the scaling of differential flow observables and factorization breaking for all charged particles at LHC PbPb $\sqrt{s_{NN}}=5.02$ TeV collisions compared to LHC XeXe $\sqrt{s_{NN}}=5.44$ TeV collisions, which could be useful in answering the question: are charm quarks thermalized? ",Signatures of thermalized charm quarks in all charged flow observables
100,984222187715428358,953282872550584321,Patrick Tamburo üå≤,"['Incredibly excited to share my first first-authored paper, which was accepted yesterday for publication in The Astronomical Journal!  We found that 55 Cancri e, a super-Earth exoplanet, has a variable eclipse depth that *might* indicate volcanic activity. <LINK>']",https://arxiv.org/abs/1804.03735,"We present a reanalysis of five transit and eight eclipse observations of the ultra-short period super-Earth 55 Cancri e observed using the Spitzer Space Telescope during 2011-2013. We use pixel-level decorrelation to derive accurate transit and eclipse depths from the Spitzer data, and we perform an extensive error analysis. We focus on determining possible variability in the eclipse data, as was reported in Demory et al. 2016a. From the transit data, we determine updated orbital parameters, yielding T0 = 2455733.0037 $\pm$ 0.0002, P = 0.7365454 $\pm$ 0.0000003 days, i = 83.5 $\pm$ 1.3 degrees, and $R_p$ = 1.89 $\pm$ 0.05 $R_\oplus$. Our transit results are consistent with a constant depth, and we conclude that they are not variable. We find a significant amount of variability between the eight eclipse observations, and confirm agreement with Demory et al. 2016a through a correlation analysis. We convert the eclipse measurements to brightness temperatures, and generate and discuss several heuristic models that explain the evolution of the planet's eclipse depth versus time. The eclipses are best modeled by a year-to-year variability model, but variability on shorter timescales cannot be ruled out. The derived range of brightness temperatures can be achieved by a dark planet with inefficient heat redistribution intermittently covered over a large fraction of the sub-stellar hemisphere by reflective grains, possibly indicating volcanic activity or cloud variability. This time-variable system should be observable with future space missions, both planned (JWST) and proposed (i.e. ARIEL). ","Confirming Variability in the Secondary Eclipse Depth of the Super-Earth
  55 Cancri e"
