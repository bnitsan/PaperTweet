,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1139557624351866881,2575510129,Naman Shukla,"[""I'm excited to present our new paper (w/ my great coauthor @arinbjornkol) on Adaptive Model Selection Framework (<LINK>) at the #ICML2019  Real-World Sequencial Decision Making workshop on Friday 2.30 Seaside Ballroom <LINK> <LINK>""]",https://arxiv.org/abs/1905.08874,"Multiple machine learning and prediction models are often used for the same prediction or recommendation task. In our recent work, where we develop and deploy airline ancillary pricing models in an online setting, we found that among multiple pricing models developed, no one model clearly dominates other models for all incoming customer requests. Thus, as algorithm designers, we face an exploration - exploitation dilemma. In this work, we introduce an adaptive meta-decision framework that uses Thompson sampling, a popular multi-armed bandit solution method, to route customer requests to various pricing models based on their online performance. We show that this adaptive approach outperform a uniformly random selection policy by improving the expected revenue per offer by 43% and conversion score by 58% in an offline simulation. ",Adaptive Model Selection Framework: An Application to Airline Pricing
1,1139373084438220802,368330699,Christian Gagné,"['New paper presented tomorrow at #ICML2019 URDL workshop: Unsupervised Temperature Scaling: An Unsupervised Post-Processing Calibration Method of Deep Networks  <LINK> -- with A. Mozafari, H. Gomes and W. Leão']",https://arxiv.org/abs/1905.00174,"The great performances of deep learning are undeniable, with impressive results over a wide range of tasks. However, the output confidence of these models is usually not well-calibrated, which can be an issue for applications where confidence on the decisions is central to providing trust and reliability (e.g., autonomous driving or medical diagnosis). For models using softmax at the last layer, Temperature Scaling (TS) is a state-of-the-art calibration method, with low time and memory complexity as well as demonstrated effectiveness. TS relies on a T parameter to rescale and calibrate values of the softmax layer, whose parameter value is computed from a labelled dataset. We are proposing an Unsupervised Temperature Scaling (UTS) approach, which does not depend on labelled samples to calibrate the model, which allows, for example, the use of a part of a test samples to calibrate the pre-trained model before going into inference mode. We provide theoretical justifications for UTS and assess its effectiveness on a wide range of deep models and datasets. We also demonstrate calibration results of UTS on skin lesion detection, a problem where a well-calibrated output can play an important role for accurate decision-making. ","Unsupervised Temperature Scaling: An Unsupervised Post-Processing
  Calibration Method of Deep Networks"
2,1138018104561340416,899968956253044737,Yanai Elazar,"['Our (w/ @yoavgo) new task and dataset on Missing Elements, and more specifically Numeric Fused-Head are now on #nlpprogress, created by @seb_ruder.\nHelp us create better models!\n<LINK>\n\nThe paper: <LINK>\nand demo: <LINK>']",https://arxiv.org/abs/1905.10886,"We provide the first computational treatment of fused-heads constructions (FH), focusing on the numeric fused-heads (NFH). FHs constructions are noun phrases (NPs) in which the head noun is missing and is said to be `fused' with its dependent modifier. This missing information is implicit and is important for sentence understanding. The missing references are easily filled in by humans but pose a challenge for computational models. We formulate the handling of FH as a two stages process: identification of the FH construction and resolution of the missing head. We explore the NFH phenomena in large corpora of English text and create (1) a dataset and a highly accurate method for NFH identification; (2) a 10k examples (1M tokens) crowd-sourced dataset of NFH resolution; and (3) a neural baseline for the NFH resolution task. We release our code and dataset, in hope to foster further research into this challenging problem. ","Where's My Head? Definition, Dataset and Models for Numeric Fused-Heads
  Identification and Resolution"
3,1136654835384815616,2770204252,Michael Gygli,"['Did you know that when annotating bounding boxes, you can also annotate the class of the object, at zero extra cost? 🙂Check our new paper on ""Efficient Object Annotation via Speaking and Pointing"": <LINK>']",https://arxiv.org/abs/1905.10576,"Deep neural networks deliver state-of-the-art visual recognition, but they rely on large datasets, which are time-consuming to annotate. These datasets are typically annotated in two stages: (1) determining the presence of object classes at the image level and (2) marking the spatial extent for all objects of these classes. In this work we use speech, together with mouse inputs, to speed up this process. We first improve stage one, by letting annotators indicate object class presence via speech. We then combine the two stages: annotators draw an object bounding box via the mouse and simultaneously provide its class label via speech. Using speech has distinct advantages over relying on mouse inputs alone. First, it is fast and allows for direct access to the class name, by simply saying it. Second, annotators can simultaneously speak and mark an object location. Finally, speech-based interfaces can be kept extremely simple, hence using them requires less mouse movement compared to existing approaches. Through extensive experiments on the COCO and ILSVRC datasets we show that our approach yields high-quality annotations at significant speed gains. Stage one takes 2.3x - 14.9x less annotation time than existing methods based on a hierarchical organization of the classes to be annotated. Moreover, when combining the two stages, we find that object class labels come for free: annotating them at the same time as bounding boxes has zero additional cost. On COCO, this makes the overall process 1.9x faster than the two-stage approach. ",Efficient Object Annotation via Speaking and Pointing
4,1136268329998532608,206818334,Ivan Oseledets,['<LINK> - Check out our new paper that lays foundations for the universality of generative models.'],https://arxiv.org/abs/1905.11520,"Despite the fact that generative models are extremely successful in practice, the theory underlying this phenomenon is only starting to catch up with practice. In this work we address the question of the universality of generative models: is it true that neural networks can approximate any data manifold arbitrarily well? We provide a positive answer to this question and show that under mild assumptions on the activation function one can always find a feedforward neural network that maps the latent space onto a set located within the specified Hausdorff distance from the desired data manifold. We also prove similar theorems for the case of multiclass generative models and cycle generative models, trained to map samples from one manifold to another and vice versa. ",Universality Theorems for Generative Models
5,1136255869111406592,119837224,Jason Baldridge,"['New ACL paper on the Room-to-Room (R2R) dataset by @vihaniaj, @gabriel_ilharco, @alex_y_ku, @ashVaswani, Eugene Ie and me, ""Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation."" The tag line is ""putting the L back in VLN"". (thread)\n<LINK> <LINK>', 'We started exploring the R2R dataset and were excited by the environment and the VLN problem it supports. See https://t.co/56JXuWLPz5 for an overview of R2R. However, we were unsure how much the existing paths and metrics stressed the modeling of language in the problem.', 'In particular, the core metric was success rate, which judges only whether an agent reached the goal location and not *how* they reached it. Many agents implementations actually wandered quite a bit (or used beam search) and figured out where to stop effectively.', 'An example instruction is ""Walk down the stairs, at the landing enter the second doorway on the left. Wait near the bookshelf."" An agent that attends ignores the instruction except the last few tokens could move all over, detect a bookshelf and stop to get high success.', 'To address this, @panderson_me et al (2018) introduced the SPL metric, which normalizes success by the max of the agent path length or the direct path length. https://t.co/vvxWbIecb9', ""SPL is a strict metric:  it requires success to get any credit. However, it is loose in that it doesn't distinguish between different paths and instead only uses their lengths. We wanted something that rewards a path that is close to the gold path more than one diverges a lot."", ""The Touchdown paper from @yoavartzi's group proposes a path edit distance measure, SED, that does compare the step-by-step choices of the agent to the gold path.  https://t.co/EMpCkgcuWh"", ""However, SED is binary on the success measure, so it fail to distinguish between paths that succeed by diverge a bit from those that succeed but diverge a lot. Here's an example illustrating why that matters. We want the orange path here to score more highly than the red one. https://t.co/j1vAyIdog0"", 'In the paper, we define five desiderata for metrics, cover how the existing metrics stack up on these, and develop a new metric, Coverage weighted by Length Score (CLS), that fulfills all desiderata. CLS reflects a fine-grained step-by-step comparison including distance. https://t.co/8rk0C2VycW', 'It was tricky to get CLS right, and many kudos go to @gabriel_ilharco for carefully considering many alternatives, coming up with the final formulation, and doing the detailed metric comparisons. (And for great ways to visualize paths!)', 'Unfortunately, the R2R paths are all direct paths from starting point to goal location. This means that the SPL metric is actually a pretty good judge of whether an agent stuck to an R2R path (because it normalizes by length).  But this is a property of the data, not the metric.', 'To address this, we composed R2R paths and their instructions to create twistier paths which nearly always diverge from the shortest path. We call this variant of the dataset Room-for-Room (R4R) and release code for others to reconstruct it from R2R: https://t.co/IM1EkJSFon https://t.co/SGseYem5mC', 'The step, path and instruction lengths exhibit much greater variance in R4R compared to R2R, which our experiments show provide an effective new instrument for characterizing agent behavior. (Props to @alex_y_ku for the idea of composing paths and creating R4R!) https://t.co/zMcuEYG20P', 'We reimplemented the Speaker Follower model of @dan_fried et al (2018) and the RCM agent of Wang et al (2019). Our implementations matched the reported performance on validation unseen, and their CLS scores were consistent with SPL.\nhttps://t.co/zg6KufRINi\nhttps://t.co/yVc7EKYUUe', 'The base RCM model receives a sparse reward based on success, which we call the goal-oriented RCM model. We also added CLS to the sparse reward to create a fidelity-oriented RCM model, and this model got small improvements on R2R for SPL and CLS on validation unseen. https://t.co/p055hsLyav', ""However, it's with R4R where the fidelity-oriented agent really shines: it gets a CLS measure of 34.6 compared to 20.4 for the goal-oriented agent! It also gets lower navigation error. Interestingly, the SF model gets a CLS of 29.6, better than goal-oriented RCM. https://t.co/U2Pn12Qg4T"", 'We also ablated the language to see how that impacts performance. When seeing only the last five tokens of the instruction, the fidelity-oriented agent drops CLS from 34.6 to 25.3! The goal-oriented agent has no change at all, staying at 20.4. Note that SPL proves useless w/ R4R.', 'Of course, a CLS measure of 34.6 leaves considerable headroom. R2R has driven a lot of innovation for this problem space. We hope that our R4R variant of the data provides a new resource for driving better modeling of language for rich VLN problems like this!', 'I also want to give a shout out for the paper ""Shifting the Baseline: Single Modality Performance on Visual Navigation &amp; QA"" by @_jessethomason_, Daniel Gordan, and @ybisk, which significantly influenced our thinking about the data and ablations.\n\nhttps://t.co/gZw3dHuGln', ""@yoavartzi True for latter, yes. But SED gives 0 for these because the execution paths have no (s_i,a_i) in common, thus lev(reference_execution,predicted_execution)/max is 1, so success doesn't matter in these cases. Look right to you?"", '@delliott @vihaniaj @gabriel_ilharco @alex_y_ku @ashVaswani @gspandana Thanks for the pointers! Looking forward to reading them.']",https://arxiv.org/abs/1905.12255,"Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language understanding plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion. ",Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation
6,1136020549245816833,872072000906424321,Martin Mundt,"['Super excited to share that our #PyTorch code for our new paper ""Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition"" is now available on GitHub.\n\npaper: <LINK>\ncode: <LINK> <LINK>']",https://arxiv.org/abs/1905.12019,"Modern deep neural networks are well known to be brittle in the face of unknown data instances and recognition of the latter remains a challenge. Although it is inevitable for continual-learning systems to encounter such unseen concepts, the corresponding literature appears to nonetheless focus primarily on alleviating catastrophic interference with learned representations. In this work, we introduce a probabilistic approach that connects these perspectives based on variational inference in a single deep autoencoder model. Specifically, we propose to bound the approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are shown to serve a dual purpose: unseen unknown out-of-distribution data can be distinguished from already trained known tasks towards robust application. Simultaneously, to retain already acquired knowledge, a generative replay process can be narrowed to strictly in-distribution samples, in order to significantly alleviate catastrophic interference. ","Unified Probabilistic Deep Continual Learning through Generative Replay
  and Open Set Recognition"
7,1135716329703575563,344205032,Larry Snyder,"['Our new paper ""Don\'t Forget Your Teacher: A Corrective Reinforcement Learning Framework"" (w/@mrza_nazari, @MajidJahani5, and Martin Takac) is on arXiv: <LINK>']",https://arxiv.org/abs/1905.13562,"Although reinforcement learning (RL) can provide reliable solutions in many settings, practitioners are often wary of the discrepancies between the RL solution and their status quo procedures. Therefore, they may be reluctant to adapt to the novel way of executing tasks proposed by RL. On the other hand, many real-world problems require relatively small adjustments from the status quo policies to achieve improved performance. Therefore, we propose a student-teacher RL mechanism in which the RL (the ""student"") learns to maximize its reward, subject to a constraint that bounds the difference between the RL policy and the ""teacher"" policy. The teacher can be another RL policy (e.g., trained under a slightly different setting), the status quo policy, or any other exogenous policy. We formulate this problem using a stochastic optimization model and solve it using a primal-dual policy gradient algorithm. We prove that the policy is asymptotically optimal. However, a naive implementation suffers from high variance and convergence to a stochastic optimal policy. With a few practical adjustments to address these issues, our numerical experiments confirm the effectiveness of our proposed method in multiple GridWorld scenarios. ",Don't Forget Your Teacher: A Corrective Reinforcement Learning Framework
8,1135590386406027265,2260933722,Dan Schroeder,"['A fitting topic, I think, for my first stand-alone tweet:\n\nKevin Randles, Bruce Thomas, and I have a new paper up on arXiv, and submitted to @JournalPhysics: Quantum matrix diagonalization visualized.\n<LINK>', ""Kevin is a WSU student and Bruce was one of my college professors years ago, so this is a three-generation collaboration!\n\nHere's a direct link to the cool web app that Kevin wrote for this project: https://t.co/u74VVtm3XL"", ""If you haven't studied quantum mechanics and linear algebra the topic may seem esoteric, but you can still run the web app and, I think, gain some mathematical intuition from it.""]",https://arxiv.org/abs/1905.13269,"We show how to visualize the process of diagonalizing the Hamiltonian matrix to find the energy eigenvalues and eigenvectors of a generic one-dimensional quantum system. Starting in the familiar sine-wave basis of an embedding infinite square well, we display the Hamiltonian matrix graphically with the basis functions alongside. Each step in the diagonalization process consists of selecting a nonzero off-diagonal matrix element, then rotating the two corresponding basis vectors in their own subspace until this element is zero. We provide Mathematica code to display the effects of these rotations on both the matrix and the basis functions. As an electronic supplement we also provide a JavaScript web app to interactively carry out this process. ",Quantum matrix diagonalization visualized
9,1135556723064868865,66971944,Alon Talmor,"['We are happy to announce our new ACL19 paper ""MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"" <LINK> a joint work with @JonathanBerant. The repository with a unified format set of datasets used + models coming soon! <LINK>', '@DanielKhashabi @nlpmattg @JonathanBerant In the code and unified format datasets we will definitely try to include more datasets!']",https://arxiv.org/abs/1905.13453,"A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community. ","MultiQA: An Empirical Investigation of Generalization and Transfer in
  Reading Comprehension"
10,1135554292226711552,907232486735958018,Jaki Noronha-Hostler,"['New paper on the arxiv: <LINK> we compare the initial condition+hydrodynamic vs. color glass condensate scenarios in small, deformed ion collisions.  We also explore the effect of substructure thanks to the new TRENTO 2.0.', 'Essentially we find that initial conditions+hydro has v2 scales inversely with multiplicity whereas for CGC v2 scales directly with multiplicity.  We also find that substructure enhances the hydro effect and suppresses the CGC effect.', 'We propose a number of new systems, but some of this data would be on tape already (e.g. dAu) where one could just reanalyze the results in 0-10% centrality to check the scaling.', 'This paper was mostly focused on RHIC results since its been done already in STAR with the ZDC but it could be extrapolated to LHC, we would just need to look into the centrality binning for each individual experiment carefully.', ""Also, I'm tagging @ronbelmont  since I'm assuming you'll have some thoughts on it, can you do this in PHENIX?""]",https://arxiv.org/abs/1905.13323,"In this paper, we study a range of collision systems involving deformed ions and compare the elliptic and triangular flow harmonics produced in a hydrodynamics scenario versus a color glass condensate (CGC) scenario. For the hydrodynamics scenario, we generate initial conditions using TRENTO and work within a linear response approximation to obtain the final flow harmonics. For the CGC scenario, we use the explicit calculation of two-gluon correlations taken in the high-$p_T$ ``(semi)dilute-(semi)dilute'' regime to express the flow harmonics in terms of the density profile of the collision. We consider ultracentral collisions of deformed ions as a testbed for these comparisons because the difference between tip-on-tip and side-on-side collisions modifies the multiplicity dependence in both scenarios, even at zero impact parameter. We find significant qualitative differences in the multiplicity dependence obtained in the initial conditions+hydrodynamics scenario and the CGC scenario, allowing these collisions of deformed ions to be used as a powerful discriminator between models. We also find that sub-nucleonic fluctuations have a systematic effect on the elliptic and triangular flow harmonics which are most discriminating in $0-1\%$ ultracentral symmetric collisions of small deformed ions and in $0-10\%$ $\mathrm{d} {}^{197}\mathrm{Au}$ collisions. The collision systems we consider are ${}^{238}\mathrm{U} {}^{238}\mathrm{U}$, $\mathrm{d} {}^{197}\mathrm{Au}$, ${}^{9}\mathrm{Be} {}^{197}\mathrm{Au}$, ${}^{9}\mathrm{Be} {}^{9}\mathrm{Be}$, ${}^{3}\mathrm{He} {}^{3}\mathrm{He}$, and ${}^{3}\mathrm{He} {}^{197}\mathrm{Au}$. ","Ultracentral Collisions of Small and Deformed Systems at RHIC: ${U}
  {U}$, ${d} {Au}$, ${}^{9}{Be} {Au}$, ${}^{9}{Be} {}^{9}{Be}$, ${}^{3}{He}
  {}^{3}{He}$, and ${}^{3}{He} {Au}$ Collisions"
11,1135549745206898688,612943605,Adam Bielski 🇵🇱🇺🇦💙💛,['My new paper is on arxiv! Emergence of Object Segmentation in Perturbed Generative Models\n<LINK> <LINK>'],https://arxiv.org/abs/1905.12663,"We introduce a novel framework to build a model that can learn how to segment objects from a collection of images without any human annotation. Our method builds on the observation that the location of object segments can be perturbed locally relative to a given background without affecting the realism of a scene. Our approach is to first train a generative model of a layered scene. The layered representation consists of a background image, a foreground image and the mask of the foreground. A composite image is then obtained by overlaying the masked foreground image onto the background. The generative model is trained in an adversarial fashion against a discriminator, which forces the generative model to produce realistic composite images. To force the generator to learn a representation where the foreground layer corresponds to an object, we perturb the output of the generative model by introducing a random shift of both the foreground image and mask relative to the background. Because the generator is unaware of the shift before computing its output, it must produce layered representations that are realistic for any such random perturbation. Finally, we learn to segment an image by defining an autoencoder consisting of an encoder, which we train, and the pre-trained generator as the decoder, which we freeze. The encoder maps an image to a feature vector, which is fed as input to the generator to give a composite image matching the original input image. Because the generator outputs an explicit layered representation of the scene, the encoder learns to detect and segment objects. We demonstrate this framework on real images of several object categories. ",Emergence of Object Segmentation in Perturbed Generative Models
12,1135531311354077185,17373048,Rodrigo Nemmen,"['New paper out! @Resident_Ivo_ and myself simulate accretion onto supermassive black holes in ""normal"" galaxies which host low-luminosity AGNs. Our black hole winds are reasonably powerful and slow. Model results broadly agree with observations <LINK> <LINK>', 'Here are some snapshots of the gas density near the black hole in code units. The last frame on the right show gas blowing out from the initial accretion disk in a thermally-driven coronal wind. https://t.co/A8MPPkHdvl', 'And here are the ion temperatures in Kelvin in the disk and wind. Pretty hot, right? https://t.co/mjs15w7WiS', 'We have tried to keep model assumptions and initial conditions as simple as possible (because we know very little about the ICs). The model is purely hydrodynamic, shear stress based on MRI physics, large torus (we wanted to understand global dynamics), Schwarzschild black hole']",https://arxiv.org/abs/1905.13708v1,"Outflows produced by a supermassive black hole (SMBH) can have important feedback effects in its host galaxy. An unresolved question is the nature and properties of winds from SMBHs accreting at low rates in low-luminosity active galactic nuclei (LLAGNs). We performed two-dimensional numerical, hydrodynamical simulations of radiatively inefficient accretion flows onto non spinning black holes. We explored a diversity of initial conditions in terms of rotation curves and viscous shear stress prescriptions, and evolved our models for very long durations of up to $8 \times 10^5 GM/c^3$. Our models resulted in powerful subrelativistic, thermally-driven winds originated from the corona of the accretion flow at distances $10-100 GM/c^2$ from the SMBH. The winds reached velocities of up to $0.01 c$ with kinetic powers corresponding to 0.1-1% of the rest-mass energy associated with inflowing gas at large distances, in good agreement with models of the ``radio mode'' of AGN feedback. The properties of our simulated outflows are in broad agreement with observations of winds in quiescent galaxies that host LLAGNs, which are capable of heating ambient gas and suppressing star formation. ","] Winds and feedback from supermassive black holes accreting at low rates:
  Hydrodynamical treatment"
13,1135513814491959301,1259188770,Olivier Bachem,"['Interested in how disentanglement might help with ML fairness? Check out our new paper “On the Fairness of Disentangled Representations”  (<LINK>)! With @FrancescoLocat8, G. Abbati @tom_rainforth, S. Bauer &amp; @bschoelkopf. @GoogleAI @MPI_IS @ETH_en @OxfordStats <LINK>']",https://arxiv.org/abs/1905.13662,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an \emph{unobserved} sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing the representations of more than \num{12600} trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed. ",On the Fairness of Disentangled Representations
14,1135495249768394755,967387429262028801,Guenter Wallner,"[""Our new paper 'Tweeting your Destiny...' with @andersdrachen @SKriglstein to be presented @cog2019ieee offers a characterization of tweets surrounding #Destiny #DestinyTheGame using #TopicModelling. Preprint available on #arXiv <LINK> #gamesUR #GUR #gameAnalytics <LINK>""]",http://arxiv.org/abs/1905.12694,"Social media has become a major communication channel for communities centered around video games. Consequently, social media offers a rich data source to study online communities and the discussions evolving around games. Towards this end, we explore a large-scale dataset consisting of over 1 million tweets related to the online multiplayer shooter Destiny and spanning a time period of about 14 months using unsupervised clustering and topic modelling. Furthermore, we correlate Twitter activity of over 3,000 players with their playtime. Our results contribute to the understanding of online player communities by identifying distinct player groups with respect to their Twitter characteristics, describing subgroups within the Destiny community, and uncovering broad topics of community interest. ","Tweeting your Destiny: Profiling Users in the Twitter Landscape around
  an Online Game"
15,1135359647504748544,97707247,Gautam Kamath,"['New paper on @arxiv: Private Hypothesis Selection (<LINK>), with Mark Bun (@markmbun), Thomas Steinke, and Steven Wu (@zstevenwu). 1/n', 'In this work, we adapt a classical tool from the statistical toolkit to the private setting. Imagine you have a set of m hypotheses, and you have to pick which one best fits an unknown distribution, from samples. 2/n', 'In the non-private setting, it is well known that O(log m) samples suffice, due to ""pairwise comparison"" based methods developed by some of my heroes, Luc Devroye and Gabor Lugosi. But privately? 3/n', 'Not immediately clear, since these methods ""reuse samples"" via Chernoff and union bound, which can be bad news for privacy. Nonetheless, we show that O(log m) samples suffice! We can do better in well structured classes, or by relaxing to approximate DP. 4/n', 'We apply this to give new algorithms for learning many classes of distributions under pure DP, including Gaussians and product distributions. Sample complexity is O(d), compared to previous best of O(d^3/2). 5/5']",https://arxiv.org/abs/1905.13229,"We provide a differentially private algorithm for hypothesis selection. Given samples from an unknown probability distribution $P$ and a set of $m$ probability distributions $\mathcal{H}$, the goal is to output, in a $\varepsilon$-differentially private manner, a distribution from $\mathcal{H}$ whose total variation distance to $P$ is comparable to that of the best such distribution (which we denote by $\alpha$). The sample complexity of our basic algorithm is $O\left(\frac{\log m}{\alpha^2} + \frac{\log m}{\alpha \varepsilon}\right)$, representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes $\mathcal{H}$ by relaxing to $(\varepsilon,\delta)$-differential privacy. We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes. Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class. As the covering and packing numbers are often closely related, for constant $\alpha$, our algorithms achieve the optimal sample complexity for many classes of interest. Finally, we describe an application to private distribution-free PAC learning. ",Private Hypothesis Selection
16,1135264993996365829,3226232932,Gianluca Detommaso,['Check out my new paper!\nHINT: Hierarchical Invertible Neural Transport for General and Sequential Bayesian inference\n<LINK>'],https://arxiv.org/abs/1905.10687,"Many recent invertible neural architectures are based on coupling block designs where variables are divided in two subsets which serve as inputs of an easily invertible (usually affine) triangular transformation. While such a transformation is invertible, its Jacobian is very sparse and thus may lack expressiveness. This work presents a simple remedy by noting that subdivision and (affine) coupling can be repeated recursively within the resulting subsets, leading to an efficiently invertible block with dense, triangular Jacobian. By formulating our recursive coupling scheme via a hierarchical architecture, HINT allows sampling from a joint distribution p(y,x) and the corresponding posterior p(x|y) using a single invertible network. We evaluate our method on some standard data sets and benchmark its full power for density estimation and Bayesian inference on a novel data set of 2D shapes in Fourier parameterization, which enables consistent visualization of samples for different dimensionalities. ","HINT: Hierarchical Invertible Neural Transport for Density Estimation
  and Bayesian Inference"
17,1135105523991416833,339868631,Georgios Leontidis,['Check out our latest paper on fundamental research on machine/deep learning.\n <LINK>. New routing algorithm for Capsule Networks via Variational Bayes. Caps. Nets have the potential to perform well under conds where CNNs perform badly.Kudos to my PhD stud. Fabio'],https://arxiv.org/abs/1905.11455,"Capsule networks are a recently proposed type of neural network shown to outperform alternatives in challenging shape recognition tasks. In capsule networks, scalar neurons are replaced with capsule vectors or matrices, whose entries represent different properties of objects. The relationships between objects and their parts are learned via trainable viewpoint-invariant transformation matrices, and the presence of a given object is decided by the level of agreement among votes from its parts. This interaction occurs between capsule layers and is a process called routing-by-agreement. In this paper, we propose a new capsule routing algorithm derived from Variational Bayes for fitting a mixture of transforming gaussians, and show it is possible transform our capsule network into a Capsule-VAE. Our Bayesian approach addresses some of the inherent weaknesses of MLE based models such as the variance-collapse by modelling uncertainty over capsule pose parameters. We outperform the state-of-the-art on smallNORB using 50% fewer capsules than previously reported, achieve competitive performances on CIFAR-10, Fashion-MNIST, SVHN, and demonstrate significant improvement in MNIST to affNIST generalisation over previous works. ",Capsule Routing via Variational Bayes
18,1134630118930997249,24141244,Karl Krauth,"['New paper with @stephenltu and @beenwrekt on analyzing the sample complexity of policy iteration on the linear quadratic regulator. <LINK>', ""This paper spans ideas from control theory, reinforcement learning, and nonasymptotic statistics. It might be quite hard to parse if you haven't encountered some of those ideas before. In case you're having a hard time understanding it here's a tl;dr instead."", ""There's recently been a lot of interest in analyzing how many samples RL algorithms need to perform well. Unfortunately we don't have the mathematical tools to analyze these algorithms in full generality. So instead we study specialized problems."", 'One such problem is called the linear quadratic regulator (LQR) which is a classic continuous controls problem with a known optimal solution. The continuous part is very important since continuous RL has seen much less analysis than its discrete counterpart.', ""In our paper we study policy iteration (PI) when applied to LQR. Lucky for us specializing PI to LQR just involves taking this paper: https://t.co/AU0jCMmF0s and making a few variable substitutions. Most of the work is in proving bounds on the algorithm's sample complexity."", ""That's where ideas from statistics come into play. Thanks to recently published results we're able to provide a non-asymptotic analysis that assumes very little of our problem."", ""So what's the result of our analysis? Well first of all we show that the number of samples we need for PI has a quadratic (up to logarithmic factors) dependence on how close we want to get to the optimal policy."", ""Second we show that when we run PI and compare it against the optimal policy we won't be too worse off in terms of cost. In the language of multi-armed bandits we say that our algorithm exhibits sublinear regret."", 'In particular we show that the regret grows at a rate of at most T^(2/3) where T is the number of iterations we run for. This is in contrast to model-based methods that are able to achieve a regret of T^(1/2).', ""However our work doesn't definitively say that PI can't achieve a rate of T^(1/2). We only provide an upper bound on regret. It'd be very exciting if someone derived a corresponding lower bound to answer this question!""]",https://arxiv.org/abs/1905.12842,"We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within $\varepsilon$ of the optimal LQR controller, each step of policy evaluation requires at most $(n+d)^3/\varepsilon^2$ samples, where $n$ is the dimension of the state vector and $d$ is the dimension of the input vector. On the other hand, only $\log(1/\varepsilon)$ policy improvement steps suffice, resulting in an overall sample complexity of $(n+d)^3 \varepsilon^{-2} \log(1/\varepsilon)$. We furthermore build on our analysis and construct a simple adaptive procedure based on $\varepsilon$-greedy exploration which relies on approximate PI as a sub-routine and obtains $T^{2/3}$ regret, improving upon a recent result of Abbasi-Yadkori et al. ","Finite-time Analysis of Approximate Policy Iteration for the Linear
  Quadratic Regulator"
19,1134457794269028353,1376287471,Tharindu Jayasinghe,"['New paper day! Check out our latest work on the @milkywayproj where we aggregate ~3 million classifications made by over 31,000 citizen science volunteers on @the_zooniverse. HUGE KUDOS to all our wonderful volunteers who made this work possible! You rock!\n<LINK> <LINK>']",https://arxiv.org/abs/1905.12625,"Citizen science has helped astronomers comb through large data sets to identify patterns and objects that are not easily found through automated processes. The Milky Way Project (MWP), a citizen science initiative on the Zooniverse platform, presents internet users with infrared (IR) images from Spitzer Space Telescope Galactic plane surveys. MWP volunteers make classification drawings on the images to identify targeted classes of astronomical objects. We present the MWP second data release (DR2) and an updated data reduction pipeline written in Python. We aggregate ${\sim}3$ million classifications made by MWP volunteers during the years 2012-2017 to produce the DR2 catalogue, which contains 2600 IR bubbles and 599 candidate bow-shock driving stars. The reliability of bubble identifications, as assessed by comparison to visual identifications by trained experts and scoring by a machine-learning algorithm, is found to be a significant improvement over DR1. We assess the reliability of IR bow shocks via comparison to expert identifications and the colours of candidate bow-shock driving stars in the 2MASS point-source catalogue. We hence identify highly-reliable subsets of 1394 DR2 bubbles and 453 bow-shock driving stars. Uncertainties on object coordinates and bubble size/shape parameters are included in the DR2 catalog. Compared with DR1, the DR2 bubbles catalogue provides more accurate shapes and sizes. The DR2 catalogue identifies 311 new bow shock driving star candidates, including three associated with the giant HII regions NGC 3603 and RCW 49. ",The Milky Way Project Second Data Release: Bubbles and Bow Shocks
20,1134453131931267073,185910194,Graham Neubig,"['Cross-lingual transfer is a powerful tool for low-resource NLP. But when you build a system for a new language (say Bengali), what language do you transfer from? Our #ACL2019 paper ""Choosing Transfer Languages for Cross-lingual Learning"" asks this: <LINK> 1/7 <LINK>', 'We first ran exhaustive experiments on machine translation, entity linking, POS tagging, and dependency parsing, transferring from many languages to create grids of accuracies for each transfer/task language pair. Raw data here: https://t.co/lwEFC1lqeS 2/7 https://t.co/GIUOphnZMP', 'Then, we train a ranker to try to guess, for any task language, what language we should be transferring from. As features to the ranker, we use various measures of linguistic similarity of the languages, and also the size of the available data to learn from (important!). 3/7 https://t.co/roopJUg812', ""Then we use the ranker to predict the best transfer languages on a held out language, with pretty good accuracy. For machine translation from Bengali to English. The best transfer languages are Hungarian and Turkish. I would've never guessed this, but our model got it right! 4/7 https://t.co/ZnJKb9CLVK"", ""We've released software, LangRank, that implements the algorithm. If you want to build MT, EL, POS tagging, or dependency parsing models for new languages you can just download and run, and it will make suggestions for you: https://t.co/yfUneGCQAv\nTry it out! 5/7 https://t.co/q1m7n5QDTk"", ""Finally we tried to do analysis that would still be useful even if you don't run our software. For example, we find that geographic distance, syntactic distance, data size, and word overlap are the most important features that indicate good transfer languages. 6/7 https://t.co/mMUIgRp6UQ"", 'This was a large collaborative project with 13 contributors. Thanks to everyone involved, and especially thanks to the first 5 authors, Yu-Hsiang, Chian-Yu, Jean, Zirui, and Yuyan who were the main drivers of the project! 7/7']",https://arxiv.org/abs/1905.12688,"Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method. Code, data, and pre-trained models are available at this https URL ",Choosing Transfer Languages for Cross-Lingual Learning
21,1134402279656828928,872072000906424321,Martin Mundt,"['Maybe my favorite figure from our new continual learning paper: <LINK> \n\nI find it cool how our approach can decide if samples from the prior will generate ambiguous or class interpolated images that lead to confusion, before even decoding. Helps quite a bit. <LINK>']",https://arxiv.org/abs/1905.12019,"Modern deep neural networks are well known to be brittle in the face of unknown data instances and recognition of the latter remains a challenge. Although it is inevitable for continual-learning systems to encounter such unseen concepts, the corresponding literature appears to nonetheless focus primarily on alleviating catastrophic interference with learned representations. In this work, we introduce a probabilistic approach that connects these perspectives based on variational inference in a single deep autoencoder model. Specifically, we propose to bound the approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are shown to serve a dual purpose: unseen unknown out-of-distribution data can be distinguished from already trained known tasks towards robust application. Simultaneously, to retain already acquired knowledge, a generative replay process can be narrowed to strictly in-distribution samples, in order to significantly alleviate catastrophic interference. ","Unified Probabilistic Deep Continual Learning through Generative Replay
  and Open Set Recognition"
22,1134342275474046979,883039700,Lenka Zdeborova,"['Generative models are the new sparsity ... or even better actually as shown in our last paper: <LINK> <LINK>', '@carlonicolini84 You are perfectly right, the prior is based on the whole database, it does not know which particular picture was chosen to be the spike.', '@DanFrederiksen2 It is not denoising, but we do want to reconstruct the images as in denoising. The point is that the shirt is there better in the lower line than in the upper line which is the standard methods. The noisy data are not shown as they do not come in the form of picture.']",https://arxiv.org/abs/1905.12385,"Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets. ",The spiked matrix model with generative priors
23,1134278135053266944,913981622193664000,Simon Shaolei Du,"['1/2 I am very excited to announce our new paper: “Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels” <LINK>', '2/2 We use recent development on the connection between kernel methods and over-parameterized neural networks to derive a class of new graph kernels: GNTKs.\nOn some benchmark graph classification datatsets, GNTKs can beat graph neural networks!', 'Joint work with @kangchenghou, Barnabas Poczos, @rsalakhu, Ruosong Wang, Keyulu Xu']",http://arxiv.org/abs/1905.13192,"While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance. ","Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph
  Kernels"
24,1134262871536197633,1075649842955866114,Luca Cortese,"['New @SAMI_survey @ARC_ASTRO3D paper led by @Dilyar_B out today, looking at gas/stellar kinematic scaling relations and the prospects for a unified dynamical scaling relation for galaxies of all types. <LINK> <LINK>']",https://arxiv.org/abs/1905.12637,"We use data from the Sydney-AAO Multi-object Integral-field spectroscopy (SAMI) Galaxy Survey to study the dynamical scaling relation between galaxy stellar mass $M_*$ and the general kinematic parameter $S_K = \sqrt{K V_{rot}^2 + \sigma^2}$ that combines rotation velocity $V_{rot}$ and velocity dispersion $\sigma$. We show that the $\log M_* - \log S_K$ relation: (1)~is linear above limits set by properties of the samples and observations; (2)~has slightly different slope when derived from stellar or gas kinematic measurements; (3)~applies to both early-type and late-type galaxies and has smaller scatter than either the Tully-Fisher relation ($\log M_* - \log V_{rot}$) for late types or the Faber-Jackson relation ($\log M_* - \log\sigma$) for early types; and (4)~has scatter that is only weakly sensitive to the value of $K$, with minimum scatter for $K$ in the range 0.4 and 0.7. We compare $S_K$ to the aperture second moment (the `aperture velocity dispersion') measured from the integrated spectrum within a 3-arcsecond radius aperture ($\sigma_{3^{\prime\prime}}$). We find that while $S_{K}$ and $\sigma_{3^{\prime\prime}}$ are in general tightly correlated, the $\log M_* - \log S_K$ relation has less scatter than the $\log M_* - \log \sigma_{3^{\prime\prime}}$ relation. ",The SAMI Galaxy Survey: mass-kinematics scaling relations
25,1134164733546979330,737878789636694016,Kshitij Jain,"['New paper on @arxiv: Relational Representation Learning for Dynamic (Knowledge) Graphs: A Survey <LINK> with @kazemi_sm, Pascal Poupart and other colleagues at @BorealisAI.']",https://arxiv.org/abs/1905.11485,"Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research. ",Representation Learning for Dynamic Graphs: A Survey
26,1134140437252304896,18850305,Zachary Lipton,"['Adversarial spelling mistakes are a ***real problem in the wild*** (e.g. spam filter evasion). Our new #ACL2019 paper shows that editing just 1-2 characters can cripple SoA NLP classifier (BERT). Work w CMU PhDs *Danish Pruthi* &amp; *Bhuwan Dhingra*) <LINK> (1/4)', 'You might think that character level models would be better able to handle spelling mistakes (vs word-level models which simply get UNK-d). However, character-level and word-piece (e.g. BERT) models expose a larger attack surface, and thus are even more vulnerable. (2/4)', 'Our proposed solution stacks a word recognition model between the raw (possibly manipulated) inputs and the downstream classifier. Salvages performance 90% (original) -&gt; 45.3% (attack) -&gt; 75% (defense)—outperforming data aug. &amp; adversarial training\n https://t.co/ZE2eOP0FpV (3/4)', ""While this paper went through the cycle twice before getting in, I'm glad for the *CL peer review process. This version is considerably stronger than our earlier  attempts thanks to critical feedback from NAACL reviewers (thanks R2!). Less glad for the arXiv embargo but meh (4/4)"", ""...oh and I finally found Danish's Twitter handle which could not possibly have been harder to locate while still existing @danish037 (5/4)"", '@Qdatalab Thanks for sharing! Looking forward to reading.']",https://arxiv.org/abs/1905.11268,"To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity. ",Combating Adversarial Misspellings with Robust Word Recognition
27,1134122253417943044,991498823494184961,Adam Oberman,['Excited about our new paper: Adversarial robustness which scales to ImageNet Scaleable input gradient regularization for adversarial robustness <LINK> <LINK>'],https://arxiv.org/abs/1905.11468,"In this work we revisit gradient regularization for adversarial robustness with some new ingredients. First, we derive new per-image theoretical robustness bounds based on local gradient information. These bounds strongly motivate input gradient regularization. Second, we implement a scaleable version of input gradient regularization which avoids double backpropagation: adversarially robust ImageNet models are trained in 33 hours on four consumer grade GPUs. Finally, we show experimentally and through theoretical certification that input gradient regularization is competitive with adversarial training. Moreover we demonstrate that gradient regularization does not lead to gradient obfuscation or gradient masking. ",Scaleable input gradient regularization for adversarial robustness
28,1134106143259684869,28840722,Phil Long,"['New paper with @HanieSedghi called ""Size-free generalization bounds for convolutional neural networks"": <LINK>.']",https://arxiv.org/abs/1905.12600,"We prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number of parameters, the Lipschitz constant of the loss and the distance from the weights to the initial weights. They are independent of the number of pixels in the input, and the height and width of hidden feature maps. We present experiments using CIFAR-10 with varying hyperparameters of a deep convolutional network, comparing our bounds with practical generalization gaps. ",Generalization bounds for deep convolutional neural networks
29,1134082317629231106,785284514947932160,Thomas Davidson,"['New paper ""Racial Bias in Hate Speech and Abusive Language Detection Datasets"" w @CornellCIS undergrad Debasmita Bhattacharya and @ingmarweber has been accepted for @AbusiveLangWS at ACL 2019 <LINK> 1/', 'We show that models trained on these datasets to detect hate speech and abuse may likely to discriminate against African-Americans 2/', 'We use five different datasets to train classifiers to predict different types of abusive language.\n\nUsing data from https://t.co/nsJWwFE2gw we then compare how the classifiers perform on tweets written in language used by African-Americans and whites 3/', 'We find substantial and systematic bias, with AA tweets more frequently classified as negative classes, e.g. hate speech and sexism.\nThis is something of a mea culpa, as I helped to build one of these datasets. Bias can enter into training data despite good intentions 4/', 'We expect bias is the result of a combination of factors including overrepresentation of language used by AAs in data, (implicit) biases of annotators, and imbalanced training data 5/', 'A lot more work is necessary to ensure that systems designed to detect one type of bias do not perpetuate another /end', 'Of course there is a typo but I\'m not redoing the thread. Second tweet should read: ""We show that models trained on datasets to detect hate speech and abuse may be likely to discriminate against African-Americans""']",https://arxiv.org/abs/1905.12516,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect. ",Racial Bias in Hate Speech and Abusive Language Detection Datasets
30,1134050713879941120,1259188770,Olivier Bachem,"['Does disentanglement make your model smarter? Check out our new paper ""Are Disentangled Representations Helpful for Abstract Visual Reasoning?"" (<LINK>) Joint work with @vansteenkiste_s, @FrancescoLocat8 and Jürgen Schmidhuber. <LINK>']",https://arxiv.org/abs/1905.12506,"A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples. ",Are Disentangled Representations Helpful for Abstract Visual Reasoning?
31,1134015910128340993,1324428524,Rikard Enberg,"[""A fundamental physics theory shouldn't have any fundamental constants. If that's the case you would expect to have new exotic particles around. We have a new paper investigating how that might look at the LHC. \n<LINK>""]",https://arxiv.org/abs/1905.11314,"In this paper we investigate a natural extension of the Standard Model that involves varying coupling constants. This is a general expectation in any fundamental theory such as string theory, and there are good reasons for why new physics could appear at reachable energy scales. We investigate the collider phenomenology of models with varying gauge couplings where the variations are associated with real singlet scalar fields. We introduce three different heavy scalar fields that are responsible for the variations of the three gauge couplings of the Standard Model. This gives rise to many interesting collider signatures that we explore, resulting in exclusion limits based on the most recent LHC data, and predictions of the future discovery potential at the high-luminosity LHC. ",Varying gauge couplings and collider phenomenology
32,1133998736919715841,46190032,Ayman Boustati,['New paper on multi-task learning in Deep Gaussian processes with @21stCenturySci. My first open contribution to the ML community! Happy to share it with everyone on: <LINK>'],http://arxiv.org/abs/1905.12407,"We present a multi-task learning formulation for Deep Gaussian processes (DGPs), through non-linear mixtures of latent processes. The latent space is composed of private processes that capture within-task information and shared processes that capture across-task dependencies. We propose two different methods for segmenting the latent space: through hard coding shared and task-specific processes or through soft sharing with Automatic Relevance Determination kernels. We show that our formulation is able to improve the learning performance and transfer information between the tasks, outperforming other probabilistic multi-task learning models across real-world and benchmarking settings. ",Non-linear Multitask Learning with Deep Gaussian Processes
33,1133805094556831744,97707247,Gautam Kamath,"['New paper on @arxiv: Private Identity Testing for High-Dimensional Distributions (<LINK>), with Clement Canonne (@ccanonne_), Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou (@zakynthinou). 1/n', ""There's recently been a good deal of work on differentially private hypothesis testing. But most of it focuses on testing in univariate/multinomial settings. We study hypothesis testing in two natural multivariate settings: product distributions and Gaussians. 2/n"", 'Let\'s focus on testing uniformity of a product distribution. For a good private statistic, we need a good non-private statistic. We use the optimal test from (https://t.co/a9WE8rqK4t), which essentially checks if the sum of squared biases of coordinates looks ""too large"". 3/n', 'The problem is that such ""squared"" statistics are generally quite sensitive, resulting in a high cost of privacy. A naive approach would result in the complexity increasing from sqrt(d) to d. 4/n', 'However, this statistic is sensitive due to ""unusual"" points. For instance, we would realize this large sensitivity if we added the string 1^d, which is *very* unlikely to arise under a uniform (or close to uniform) product distribution. 5/n', 'Solution: Lipschitz extensions! If a statistic is insensitive on a subset of the domain (i.e., datasets which are likely to arise from a product distribution), there exists an extension to the entire domain, which matches on the subset. 6/n', ""This allows us to get a much better bound on the sample complexity. If we're interested in an efficient algorithm, we have a simple filtering method which tries to simulate the Lipschitz extension, at the cost of some loss in parameters. 7/n"", 'We also have some tools which I think are neat even without privacy -- reductions between univariate and multivariate settings, between Gaussians and product distributions, and between balanced and uniform product distributions. See the paper for full details! 8/8']",https://arxiv.org/abs/1905.11947,"In this work we present novel differentially private identity (goodness-of-fit) testers for natural and widely studied classes of multivariate product distributions: Gaussians in $\mathbb{R}^d$ with known covariance and product distributions over $\{\pm 1\}^{d}$. Our testers have improved sample complexity compared to those derived from previous techniques, and are the first testers whose sample complexity matches the order-optimal minimax sample complexity of $O(d^{1/2}/\alpha^2)$ in many parameter regimes. We construct two types of testers, exhibiting tradeoffs between sample complexity and computational complexity. Finally, we provide a two-way reduction between testing a subclass of multivariate product distributions and testing univariate distributions, and thereby obtain upper and lower bounds for testing this subclass of product distributions. ",Private Identity Testing for High-Dimensional Distributions
34,1133800310584086533,41422332,Damien Anderson,"['New Paper on Ensemble Decision Systems and General Video Game Playing. \n\n<LINK>\n\nWe look at combining the output from multiple algorithms into a single action, improving the generality of the agent.\n\nWith @diego_pliebana @kisenshi\nAccepted at @cog2019ieee']",https://arxiv.org/abs/1905.10792,"Ensemble Decision Systems offer a unique form of decision making that allows a collection of algorithms to reason together about a problem. Each individual algorithm has its own inherent strengths and weaknesses, and often it is difficult to overcome the weaknesses while retaining the strengths. Instead of altering the properties of the algorithm, the Ensemble Decision System augments the performance with other algorithms that have complementing strengths. This work outlines different options for building an Ensemble Decision System as well as providing analysis on its performance compared to the individual components of the system with interesting results, showing an increase in the generality of the algorithms without significantly impeding performance. ",Ensemble Decision Systems for General Video Game Playing
35,1133728950935609345,2971631394,Jakub Mielczarek,"['In our new article, written together with my collaborators from @penn_state University, we have applied an old fashioned pencil and paper method to explain some unclear features seen in the simulations of Causal Dynamical Triangulations. <LINK>  #QuantumGravity <LINK>']",https://arxiv.org/abs/1905.11843,"Detailed applications of minisuperspace methods are presented and compared with results obtained in recent years by means of causal dynamical triangulations (CDTs), mainly in the form of effective actions. The analysis sheds light on conceptual questions such as the treatment of time or the role and scaling behavior of statistical and quantum fluctuations. In the case of fluctuations, several analytical and numerical results show agreement between the two approaches and offer possible explanations of effects that have been seen in causal dynamical triangulations but whose origin remained unclear. The new approach followed here suggests `CDT experiments' in the form of new simulations or evaluations motivated by theoretical predictions, testing CDTs as well as the minisuperspace approximation. ",Minisuperspace results for causal dynamical triangulations
36,1133712745713549312,786109620284719104,Ethan fetaya,"['New paper with Dan Levi, Liran Gispan and Niv Giladi on calibration for regression tasks <LINK>. We show major flaws in a recently proposed definition and method, where even random uncertainty can be perfectly calibrated and propose a simple alternative.']",https://arxiv.org/abs/1905.11659,"Predicting not only the target but also an accurate measure of uncertainty is important for many machine learning applications and in particular safety-critical ones. In this work we study the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. We show that the existing definition for calibration of a regression uncertainty [Kuleshov et al. 2018] has severe limitations in distinguishing informative from non-informative uncertainty predictions. We propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach. Our method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. We also propose a simple, scaling-based calibration method that preforms as well as much more complex ones. We show results on both a synthetic, controlled problem and on the object detection bounding-box regression task using the COCO and KITTI datasets. ",Evaluating and Calibrating Uncertainty Prediction in Regression Tasks
37,1133646746612056064,813985040526962688,Matan,"['Surprisingly, a direct control over level sets of neural networks can significantly improve networks’ robustness to adversarial examples AND facilitate reconstruction of high fidelity surfaces from 3D point cloud data.\n\nCheck out our new paper:\n<LINK>', 'Joint work with @HaimNiv, @YarivLior, @iz_ofer, @HaggaiMaron, @lipmanya']",https://arxiv.org/abs/1905.11911,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods. ",Controlling Neural Level Sets
38,1133537573366747136,863828060600139776,Dr. Deep Anand,"['check out our new paper on the motions of galaxies in the local Universe! \n\n(<LINK>) <LINK>', 'mini-summary: The Milky Way lies in a thin plane (the Local Sheet) right near a large void (the Local Void). Here we test the idea that voids expand by measuring the peculiar velocities of galaxies in the direction opposite the void.', 'We received near-infrared Hubble observations for four dwarf galaxies (the binary galaxy HIZSS-003 pictured here) to measure their tip of the red giant branch distances and reconstruct their motions. https://t.co/8pW4kmymVA', 'We would expect these galaxies to have a negative peculiar velocity (towards us, blue in the plot), which would be an imprint of our downward motion towards these galaxies and away from the Local Void. \nThis is consistent with what we find! (galaxies labelled 1-4). https://t.co/dXmgUBR173', 'However, we did find one galaxy (GALFA-DW4) in the antivoid direction that may have a positive peculiar velocity, where the expectation is the exact opposite. Unfortunately the present Hubble data is not deep enough to nail down a robust distance, so this matter is still TBD. https://t.co/3THPg74KPx', ""We insert these galaxies into our group's numerical action methods model (Shaya et al. 2017) to reconstruct their full 3D motions. Here we see that most galaxies in the Local Volume participate in \na few key flow patterns https://t.co/DDWh8M8Aax"", '- i.e. 1) Evacuation from the Local Void (at +SGZ), 2) a flow towards Virgo and the Great Attractor (+SGY, -SGX) , 3) only modest peculiar velocities for members of the Local Sheet']",https://arxiv.org/abs/1905.11416,"The Milky Way lies in a thin plane, the Local Sheet, a part of a wall bounding the Local Void lying toward the north supergalactic pole. Galaxies with accurate distances both above and below this supergalactic equatorial plane have systematically negative peculiar velocities. The interpretation of this situation is that the Local Void is expanding, giving members of the Local Sheet deviant velocities toward the south supergalactic pole. The few galaxies within the void are evacuating the void. Galaxies in a filament in the supergalactic south are not feeling the expansion so their apparent motion toward us is mainly a reflex of our motion. The model of the local velocity field was uncertain because the apex of our motion away from the Local Void lies in obscurity near the Galactic plane. Here, results of Hubble Space Telescope infrared observations are reported that find tip of the red giant branch distances to four obscured galaxies. All the distances are $\sim7$ Mpc, revealing that these galaxies are part of a continuous filamentary structure passing between the north and south Galactic hemispheres and sharing the same kinematic signature of peculiar velocities toward us. A fifth galaxy nearby in projection, GALFA-DW4, has an ambiguous distance. If nearby at $\sim 3$ Mpc, this galaxy has an anomalous velocity away from us of +348 km/s. Alternatively, perhaps the resolved stars are on the asymptotic giant branch and the galaxy is beyond 6 Mpc whence the observed velocity would not be unusual. ",Peculiar Velocities of Galaxies Just Beyond the Local Group
39,1133432786721882112,1073201991701139456,Pablo Fernández de Salas,['New paper on the thermalization of sterile neutrinos in the early Universe! With full mixing matrix!\n<LINK>'],https://arxiv.org/abs/1905.11290,"In the framework of a 3+1 scheme with an additional inert state, we consider the thermalisation of sterile neutrinos in the early Universe taking into account the full $4\times4$ mixing matrix. The evolution of the neutrino energy distributions is found solving the momentum-dependent kinetic equations with full diagonal collision terms, as in previous analyses of flavour neutrino decoupling in the standard case. The degree of thermalisation of the sterile state is shown in terms of the effective number of neutrinos, $N_{\rm eff}$, and its dependence on the three additional mixing angles ($\theta_{14}$, $\theta_{24}$, $\theta_{34}$) and on the squared mass difference $\Delta m^2_{41}$ is discussed. Our results are relevant for fixing the contribution of a fourth light neutrino species to the cosmological energy density, whose value is very well constrained by the final Planck analysis. For the preferred region of active-sterile mixing parameters from short-baseline neutrino experiments, we find that the fourth state is fully thermalised ($N_{\rm eff}\simeq 4$). ","Thermalisation of sterile neutrinos in the early Universe in the 3+1
  scheme with full mixing matrix"
40,1133370060070952962,252867237,Juan Miguel Arrazola,"['New paper out! Quantum-inspired algorithms in practice: <LINK>\nSee also accompanying blog post: <LINK> <LINK>', 'Remember when Ewin Tang rocked the quantum world by dequantizing quantum algorithms for linear algebra? We explain, analyze, implement, and test the algorithms to understand how well they perform.', 'In practice, we find that the performance of quantum-inspired algorithms is better than their scary complexity bounds would suggest. Good news for people aiming to improve these theoretical bounds.', 'The algorithms perform reasonably well compared to existing classical techniques, but only provided that stringent conditions are met: low rank, low condition number, and extremely large dimension of the input matrix. These conditions almost never occur simultaneously in practice', 'By contrast, there exist many practical datasets for linear algebra problems that are sparse and high-rank, precisely the type that can be handled by quantum algorithms.', 'Whether quantum algorithms for linear algebra will ever be impactful in practice remains to be seen, but if they fail to do so, it almost certainly won’t be because they are outperformed by these quantum-inspired algorithms.', ""If you want to try out teh algorithms yourself, we've made them publicly available on @XanaduAI's github: https://t.co/qwlNJGoozP""]",https://arxiv.org/abs/1905.10415,"We study the practical performance of quantum-inspired algorithms for recommendation systems and linear systems of equations. These algorithms were shown to have an exponential asymptotic speedup compared to previously known classical methods for problems involving low-rank matrices, but with complexity bounds that exhibit a hefty polynomial overhead compared to quantum algorithms. This raised the question of whether these methods were actually useful in practice. We conduct a theoretical analysis aimed at identifying their computational bottlenecks, then implement and benchmark the algorithms on a variety of problems, including applications to portfolio optimization and movie recommendations. On the one hand, our analysis reveals that the performance of these algorithms is better than the theoretical complexity bounds would suggest. On the other hand, their performance as seen in our implementation degrades noticeably as the rank and condition number of the input matrix are increased. Overall, our results indicate that quantum-inspired algorithms can perform well in practice provided that stringent conditions are met: low rank, low condition number, and very large dimension of the input matrix. By contrast, practical datasets are often sparse and high-rank, precisely the type that can be handled by quantum algorithms. ",Quantum-inspired algorithms in practice
41,1133363986680860672,1068545181576773632,Kenneth Brown,['New paper on the arXiv today with @kalandsman and non-tweeting collaborators on two-qubit gates in long ion chains. <LINK> .'],https://arxiv.org/abs/1905.10421,"Ion trap systems are a leading platform for large scale quantum computers. Trapped ion qubit crystals are fully-connected and reconfigurable, owing to their long range Coulomb interaction that can be modulated with external optical forces. However, the spectral crowding of collective motional modes could pose a challenge to the control of such interactions for large numbers of qubits. Here, we show that high-fidelity quantum gate operations are still possible with very large trapped ion crystals, simplifying the scaling of ion trap quantum computers. To this end, we present analytical work that determines how parallel entangling gates produce a crosstalk error that falls off as the inverse cube of the distance between the pairs. We also show experimental work demonstrating entangling gates on a fully-connected chain of seventeen $^{171}{\rm{Yb}}^{+}$ ions with fidelities as high as $97(1)\%$. ","Two-qubit entangling gates within arbitrarily long chains of trapped
  ions"
42,1133305806680657920,930002720,Fabio A.,"['Really proud of the 1st paper of Adrien’s PhD proposing a method to fully exploit the 3D nature of @chandraxray, @ESA_XMM data with application to CasA SNR. New methods are needed to exploit the future rich datasets of @AthenaXIFU and @lynxobservatory. \n<LINK> <LINK>']",https://arxiv.org/abs/1905.10175,"In high-energy astronomy, spectro-imaging instruments such as X-ray detectors allow investigation of the spatial and spectral properties of extended sources including galaxy clusters, galaxies, diffuse interstellar medium, supernova remnants and pulsar wind nebulae. In these sources, each physical component possesses a different spatial and spectral signature, but the components are entangled. Extracting the intrinsic spatial and spectral information of the individual components from this data is a challenging task. Current analysis methods do not fully exploit the 2D-1D (x,y,E) nature of the data, as the spatial and spectral information are considered separately. Here we investigate the application of a Blind Source Separation algorithm that jointly exploits the spectral and spatial signatures of each component in order to disentangle them. We explore the capabilities of a new BSS method (General Morphological Component Analysis; GMCA), initially developed to extract an image of the Cosmic Microwave Background from Planck data, in an X-ray context. The performance of GMCA on X-ray data is tested using Monte-Carlo simulations of supernova remnant toy models, designed to represent typical science cases. We find that GMCA is able to separate highly entangled components in X-ray data even in high contrast scenarios, and can extract with high accuracy the spectrum and map of each physical component. A modification is proposed to improve the spectral fidelity in the case of strongly overlapping spatial components, and we investigate a resampling method to derive realistic uncertainties associated to the results of the algorithm. Applying the modified algorithm to the deep Chandra observations of Cassiopeia A, we are able to produce detailed maps of the synchrotron emission at low energies (0.6-2.2 keV), and of the red/blue shifted distributions of a number of elements including Si and Fe K. ","A novel method for component separation of extended sources in X-ray
  astronomy"
43,1133299381040558080,841031248839618560,Relja Arandjelović,"['My new paper is on arXiv, Object Discovery with a Copy-Pasting GAN <LINK> <LINK>']",https://arxiv.org/abs/1905.11369,"We tackle the problem of object discovery, where objects are segmented for a given input image, and the system is trained without using any direct supervision whatsoever. A novel copy-pasting GAN framework is proposed, where the generator learns to discover an object in one image by compositing it into another image such that the discriminator cannot tell that the resulting image is fake. After carefully addressing subtle issues, such as preventing the generator from `cheating', this game results in the generator learning to select objects, as copy-pasting objects is most likely to fool the discriminator. The system is shown to work well on four very different datasets, including large object appearance variations in challenging cluttered backgrounds. ",Object Discovery with a Copy-Pasting GAN
44,1133284490409893888,631114017,Knud Jahnke,['New paper by @brandherd81 et al. (incl. myself) on large-scale jet-driven outflows in quasar 3C273. A lot of extremes taking place in this host galaxy - shown by the amazing diagnostic capabilities provided by @ESO VLT-MUSE and @almaobs 3d spectroscopy!\n\n<LINK>'],https://arxiv.org/abs/1905.10387,We present an unprecedented view on the morphology and kinematics of the extended narrow-line region (ENLR) and molecular gas around the prototypical hyper-luminous quasar 3C273 ($L\sim10^{47}$ erg/s at z=0.158) based on VLT-MUSE optical 3D spectroscopy and ALMA observations. We find that: 1) The ENLR size of 12.1$\pm$0.2kpc implies a smooth continuation of the size-luminosity relation out to large radii or a much larger break radius as previously proposed. 2) The kinematically disturbed ionized gas with line splits reaching 1000km/s out to 6.1$\pm$1.5kpc is aligned along the jet axis. 3) The extreme line broadening on kpc scales is caused by spatial and spectral blending of many distinct gas clouds separated on sub-arcsecond scales with different line-of-sight velocities. The ENLR velocity field combined with the known jet orientation rule out a simple scenario of a radiatively-driven radial expansion of the outflow. Instead we propose that a pressurized expanding hot gas cocoon created by the radio jet is impacting on an inclined gas disk leading to transverse and/or backflow motion with respect to our line-of-sight. The molecular gas morphology may either be explained by a density wave at the front of the outflow expanding along the jet direction as predicted by positive feedback scenario or the cold gas may be trapped in a stellar over-density caused by a recent merger event. Using 3C273 as a template for observations of high-redshift hyper-luminous AGN reveals that large-scale ENLRs and kpc scale outflows may often be missed due to the brightness of the nuclei and the limited sensitivity of current near-IR instrumentation. ,Jet-driven galaxy-scale gas outflows in the hyper-luminous quasar 3C273
45,1133273346051907584,2889619139,liubov 🇺🇦 🏴‍☠️🤍💙❤,"['Our new paper accept. to @SciReports is now on @arxiv_org <LINK> \nMorphological organization of transport in complex networks \nwith @criparis INSEP, Sloan MIT, @Polytechnique <LINK>']",https://arxiv.org/abs/1905.10333,"We investigate the structural organization of the point-to-point electric, diffusive or hydraulic transport in complex scale-free networks. The random choice of two nodes, a source and a drain, to which a potential difference is applied, selects two tree-like structures, one emerging from the source and the other converging to the drain. These trees merge into a large cluster of the remaining nodes that is found to be quasi-equipotential and thus presents almost no resistance to transport. Such a global ""tree-cluster-tree"" structure is universal and leads to a power law decay of the currents distribution. Its exponent, $-2$, is determined by the multiplicative decrease of currents at successive branching points of a tree and is found to be independent of the network connectivity degree and resistance distribution. ","Morphological organization of point-to-point transport in complex
  networks"
46,1133184124837957632,950132983083671553,Joey Bose,"['How can we build predictive models on graph-structured data that are fair with respect to sensitive attributes like age, gender, and occupation? Check out our new ICML paper titled ""Compositional Fairness Constraints for Graph Embeddings"": <LINK> @williamleif', 'We take an adversarial approach to learn composable filters that are trained with random sets of permutations of sensitive attributes. By leveraging compositionality, our approach can be invariant to different sensitive attributes while minimally sacrificing main task perf.']",https://arxiv.org/abs/1905.10674,"Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is compositional---meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework. ",Compositional Fairness Constraints for Graph Embeddings
47,1133172895579086848,3301643341,Roger Grosse,"[""New paper with @Guodzh and James Martens analyzing theoretical convergence rates of natural gradient for wide networks. Under certain conditions, it behaves like gradient descent in output space, where everything's nice, smooth, and convex.\n\n<LINK> <LINK>"", 'If you use the Euclidean metric on output space, and the network is overparameterized and has full-rank Jacobian, then the instantaneous velocity of the predictions is the output space gradient descent update.', ""So it all comes down to showing that for wide networks, the Jacobian is stable enough that the output space trajectory doesn't start to bend. This all checks out for a particular distribution of networks used in lots of recent theory work.""]",https://arxiv.org/abs/1905.10961,"Natural gradient descent has proven effective at mitigating the effects of pathological curvature in neural network optimization, but little is known theoretically about its convergence properties, especially for \emph{nonlinear} networks. In this work, we analyze for the first time the speed of convergence of natural gradient descent on nonlinear neural networks with squared-error loss. We identify two conditions which guarantee efficient convergence from random initializations: (1) the Jacobian matrix (of network's output for all training cases with respect to the parameters) has full row rank, and (2) the Jacobian matrix is stable for small perturbations around the initialization. For two-layer ReLU neural networks, we prove that these two conditions do in fact hold throughout the training, under the assumptions of nondegenerate inputs and overparameterization. We further extend our analysis to more general loss functions. Lastly, we show that K-FAC, an approximate natural gradient descent method, also converges to global minima under the same assumptions, and we give a bound on the rate of this convergence. ","Fast Convergence of Natural Gradient Descent for Overparameterized
  Neural Networks"
48,1132876237142147072,73083721,Michał Wróbel,['Postprint of our new paper on impact of gender diversity on mood in\xa0IT teams (<LINK>) is available: <LINK>'],https://arxiv.org/abs/1905.10171,"Recent studies show that gender diversity in IT teams has a positive impact on the software development process. However, there is still a great gender inequality. The aim of our study was to examine how the working atmosphere depends on the gender differentiation of IT teams. The analysis of the results of the interviews and questionnaires showed that the atmosphere in gender-differentiated teams is more pleasant compared to purely male ones. The paper also discusses the problem of gender discrimination, which, according to the results of the study, unfortunately still exists and affects the working atmosphere. Finally, we looked at ways to reduce the gender inequity, where it turned out that soft approaches such as dedicated training, workshops to show the human face of the IT industry are preferred. ","Perceptions of Gender Diversity's impact on mood in software development
  teams"
49,1132865008499474432,556151596,Lawrence M. Krauss,['First physics paper while at the \u2066@OriginsProject\u2069! A new way of detecting cosmic axion dark matter?  [1905.10014] Axions and Atomic Clocks <LINK>'],https://arxiv.org/abs/1905.10014,"The equations of electrodynamics are altered in the presence of a classical coherent axion dark matter background field, changing the dispersion relation for electromagnetic waves. Careful measurements of the frequency stability in sensitive atomic clocks could in principle provide evidence for such a background for $f_a \ge 10^7$ GeV. Turning on a background magnetic field might enhance these effects in a controllable way, and interferometric measurements might also be useful for probing the time-varying photon dispersion relation that results from a coherent cosmic axion background. ",Axions and Atomic Clocks
50,1132839063910674433,82376129,Satoshi Nishida,['Our new paper is now available on arXiv: <LINK>\nWe developed a new method to improve the pattern-recognition performance of convolutional neural networks by integrating brain representations into them.'],https://arxiv.org/abs/1905.10037,"The human brain can effectively learn a new task from a small number of samples, which indicate that the brain can transfer its prior knowledge to solve tasks in different domains. This function is analogous to transfer learning (TL) in the field of machine learning. TL uses a well-trained feature space in a specific task domain to improve performance in new tasks with insufficient training data. TL with rich feature representations, such as features of convolutional neural networks (CNNs), shows high generalization ability across different task domains. However, such TL is still insufficient in making machine learning attain generalization ability comparable to that of the human brain. To examine if the internal representation of the brain could be used to achieve more efficient TL, we introduce a method for TL mediated by human brains. Our method transforms feature representations of audiovisual inputs in CNNs into those in activation patterns of individual brains via their association learned ahead using measured brain responses. Then, to estimate labels reflecting human cognition and behavior induced by the audiovisual inputs, the transformed representations are used for TL. We demonstrate that our brain-mediated TL (BTL) shows higher performance in the label estimation than the standard TL. In addition, we illustrate that the estimations mediated by different brains vary from brain to brain, and the variability reflects the individual variability in perception. Thus, our BTL provides a framework to improve the generalization ability of machine-learning feature representations and enable machine learning to estimate human-like cognition and behavior, including individual variability. ",Brain-mediated Transfer Learning of Convolutional Neural Networks
51,1132698849636737025,977906884886827008,Marcos Mariño,"['The Lieb-Liniger model has been an all-time favorite of mine. Last week Tomás Reis and I finally finished a paper about it, solving an old problem and raising some new questions.\n<LINK>']",https://arxiv.org/abs/1905.09575,"We present a systematic procedure to extract the perturbative series for the ground state energy density in the Lieb-Liniger and Gaudin-Yang models, starting from the Bethe ansatz solution. This makes it possible to calculate explicitly the coefficients of these series and to study their large order behavior. We find that both series diverge factorially and are not Borel summable. In the case of the Gaudin-Yang model, the first Borel singularity is determined by the non-perturbative energy gap. This provides a new perspective on the Cooper instability. ",Exact perturbative results for the Lieb-Liniger and Gaudin-Yang models
52,1132636739170971649,1392935011,Ole-Chr. Granmo,"['New CAIR paper by my skillful PhD student Jivitesh Sharma on arXiv, dealing with fire evacuation from complex buildings! Also thanks to @PerArneAndersen  and @mortengoodwin for their contributions. <LINK> #DeepReinforcementLearning <LINK>']",https://arxiv.org/abs/1905.09673,"We focus on the important problem of emergency evacuation, which clearly could benefit from reinforcement learning that has been largely unaddressed. Emergency evacuation is a complex task which is difficult to solve with reinforcement learning, since an emergency situation is highly dynamic, with a lot of changing variables and complex constraints that makes it difficult to train on. In this paper, we propose the first fire evacuation environment to train reinforcement learning agents for evacuation planning. The environment is modelled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty and bottlenecks. We have implemented the environment in the OpenAI gym format, to facilitate future research. We also propose a new reinforcement learning approach that entails pretraining the network weights of a DQN based agents to incorporate information on the shortest path to the exit. We achieved this by using tabular Q-learning to learn the shortest path on the building model's graph. This information is transferred to the network by deliberately overfitting it on the Q-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions. We perform comparisons of the proposed approach with state-of-the-art reinforcement learning algorithms like PPO, VPG, SARSA, A2C and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN based models. Finally, we test our model on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. We use an attention based mechanism to deal with large action spaces. Our model achieves near optimal performance on the real world emergency environment. ","Deep Q-Learning with Q-Matrix Transfer Learning for Novel Fire
  Evacuation Environment"
53,1132300571833188353,504359553,"Nikola Milosevic, PhD","['Pre-print of our new paper on #MachineLearnig #Summarization of #SocialInnovation, with proposed new metric for summarization is available on #Arxiv. See paper here:\n<LINK>\n\n@arxiv #NeuralNetworks #TextSummarization #NLProc #artificialintelligence #datascience']",https://arxiv.org/abs/1905.09086,"In the past decade, social innovation projects have gained the attention of policy makers, as they address important social issues in an innovative manner. A database of social innovation is an important source of information that can expand collaboration between social innovators, drive policy and serve as an important resource for research. Such a database needs to have projects described and summarized. In this paper, we propose and compare several methods (e.g. SVM-based, recurrent neural network based, ensambled) for describing projects based on the text that is available on project websites. We also address and propose a new metric for automated evaluation of summaries based on topic modelling. ","From web crawled text to project descriptions: automatic summarizing of
  social innovation projects"
54,1132012349072715776,778702136339144704,Victoria Strait,"[""We have a new paper out today @UcdAstro! It's a short read about the stellar properties of z~8 galaxies in the Reionization Lensing Cluster Survey (RELICS) sample: <LINK>"", 'Using HST+Spitzer SED fitting, we find a galaxy showing evidence of an evolved stellar population at z~8! Specifically, with a best-fit age of 500 Myr at z=8.4, when the universe was only 600 Myr old… https://t.co/YgU71Jcf8h', 'We’ve already seen evidence of first star formation at earlier times than we thought (e.g., https://t.co/lMWBcfVrE8), but these data seem to prefer a very early onset of star formation (&lt;100 Myr after the Big Bang)!', 'That’s not to overshadow the other six sources in the paper, including a resolved z~10 galaxy first discovered by Salmon+18 (https://t.co/0zc9hJ53eB). Deeper Spitzer data strengthens the z~10 solution, and the z~8 solutions of five others. https://t.co/1nMkU90H7S', 'All of our galaxies are going to be great candidates for follow-up with @NASAWebb, which will be able to detect rest-frame optical emission lines at these redshifts. Very exciting!']",https://arxiv.org/abs/1905.09295,"Measurements of stellar properties of galaxies when the universe was less than one billion years old yield some of the only observational constraints of the onset of star formation. We present here the inclusion of \textit{Spitzer}/IRAC imaging in the spectral energy distribution fitting of the seven highest-redshift galaxy candidates selected from the \emph{Hubble Space Telescope} imaging of the Reionization Lensing Cluster Survey (RELICS). We find that for 6/8 \textit{HST}-selected $z\gtrsim8$ sources, the $z\gtrsim8$ solutions are still strongly preferred over $z\sim$1-2 solutions after the inclusion of \textit{Spitzer} fluxes, and two prefer a $z\sim 7$ solution, which we defer to a later analysis. We find a wide range of intrinsic stellar masses ($5\times10^6 M_{\odot}$ -- $4\times10^9$ $M_{\odot}$), star formation rates (0.2-14 $M_{\odot}\rm yr^{-1}$), and ages (30-600 Myr) among our sample. Of particular interest is Abell1763-1434, which shows evidence of an evolved stellar population at $z\sim8$, implying its first generation of star formation occurred just $< 100$ Myr after the Big Bang. SPT0615-JD, a spatially resolved $z\sim10$ candidate, remains at its high redshift, supported by deep \textit{Spitzer}/IRAC data, and also shows some evidence for an evolved stellar population. Even with the lensed, bright apparent magnitudes of these $z \gtrsim 8$ candidates (H = 26.1-27.8 AB mag), only the \textit{James Webb Space Telescope} will be able further confirm the presence of evolved stellar populations early in the universe. ","Stellar Properties of z ~ 8 Galaxies in the Reionization Lensing Cluster
  Survey"
55,1131916108792045570,890132360397803520,Christoph Salge,"['New paper out with @OccupyMath. Automatic Generation of Level Maps with the Do What’s Possible Representation. \n\n<LINK>\n\nThe DWP representation can make arbitrary large artifacts, with different characteristics, and it can also adapt to the environment. <LINK>', ""In our paper, we look a dungeon generation, but the representation is very flexible and can be used for a range of things. Prior work, and some studies in this paper, also suggest high evolvability. \n\nWe will talk about this at this year's Conference on Games @cog2019ieee https://t.co/TLw8rrVp47""]",https://arxiv.org/abs/1905.09618,"Automatic generation of level maps is a popular form of automatic content generation. In this study, a recently developed technique employing the {\em do what's possible} representation is used to create open-ended level maps. Generation of the map can continue indefinitely, yielding a highly scalable representation. A parameter study is performed to find good parameters for the evolutionary algorithm used to locate high-quality map generators. Variations on the technique are presented, demonstrating its versatility, and an algorithmic variant is given that both improves performance and changes the character of maps located. The ability of the map to adapt to different regions where the map is permitted to occupy space are also tested. ","Automatic Generation of Level Maps with the Do What's Possible
  Representation"
56,1131839110182838272,2191109263,Zamin Iqbal,"['New paper, led by Timo Bingmann, on a high performance C++ follow-up to BIGSI, called COBS:   <LINK>\nBIGSI worked surprisingly well for a Python implem., but had the limitation that it used the same bloom filter size for all samples. COBS solves this. 1/n <LINK>', 'Timo has done a lot of benchmarking comparing many tools on 100k microbial datasets: 2/n https://t.co/R5nArSGGu2', 'Mantis, which we had problems benchmarking in the BIGSI paper, does really well too, and is very fast - was a great incentive to us to improve. However, as we scale up to huge datasets, the lower RAM use of COBS allows it to scale further 3/n https://t.co/uOaZfKDVss', 'This is a fast-moving field, and i expect many improvements to come! The different types of text corpora (eg human transcriptomes versus microbes) have v different levels of diversity, and so will be have to take advantage of different properties for compression etc n/n', 'ah yes, one thing: COBS is v fast, but is also designed to take advantage of SSDs \n(n+1)/n', ""@nomad421 Would very much welcome your feedback. In particular,  we've done our best to do good and fair benchmarking, but this is hard, so happy to talk !""]",https://arxiv.org/abs/1905.09624,"We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over between an inverted index and Bloom filters. Our target application is to index $k$-mers of DNA samples or $q$-grams from text documents and process approximate pattern matching queries on the corpus with a user-chosen coverage threshold. Query results may contain a number of false positives which decreases exponentially with the query length. We compare COBS to seven other index software packages on 100000 microbial DNA samples. COBS' compact but simple data structure outperforms the other indexes in construction time and query performance with Mantis by Pandey et al. in second place. However, unlike Mantis and other previous work, COBS does not need the complete index in RAM and is thus designed to scale to larger document sets. ",COBS: a Compact Bit-Sliced Signature Index
57,1131832606968897537,948044683673923584,Yiping Lu 💙💛,"[""Our YOPO has a new arxiv version! <LINK> 5 time faster than original adversarial training and only need 2/3 GPU time compares with a concurrent paper! **Neural ODE**'s idea is the core!\nstruggling for arxiv version of other neurips submissions... <LINK>""]",https://arxiv.org/abs/1905.00877,"Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximal Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the projected gradient descent (PGD) algorithm. Our codes are available at https://this https URL ","You Only Propagate Once: Accelerating Adversarial Training via Maximal
  Principle"
58,1131685612942221312,2864832233,Sam Kriegman,"['Our new paper (<LINK>) in a nutshell: \n\nFor the first time, we show a robot that automatically recovers from unexpected damage through shape change.\n\nWith Stephanie Walker, Dylan Shah, @drmichaellevin, Rebecca Kramer-Bottiglio, and @DoctorJosh. <LINK>', 'https://t.co/lmadnbaVg6', 'https://t.co/D5zpehKWcG', 'https://t.co/iFzEi8VfDf', 'https://t.co/e1iKnkPUfO', 'https://t.co/dwXLYvchah', '@ZestGohd @uvmcomplexity @drmichaellevin @DoctorJosh Actuated “voxels” in the silicone robot were sticky, symmetrically distributed about the body, and hooked into a single air line; all of which impede forward movement but are easily modified. We hope to get the real robot “walking” before presenting @RoboticsSciSys, June 22-26.']",https://arxiv.org/abs/1905.09264,"A robot's mechanical parts routinely wear out from normal functioning and can be lost to injury. For autonomous robots operating in isolated or hostile environments, repair from a human operator is often not possible. Thus, much work has sought to automate damage recovery in robots. However, every case reported in the literature to date has accepted the damaged mechanical structure as fixed, and focused on learning new ways to control it. Here we show for the first time a robot that automatically recovers from unexpected damage by deforming its resting mechanical structure without changing its control policy. We found that, especially in the case of ""deep insult"", such as removal of all four of the robot's legs, the damaged machine evolves shape changes that not only recover the original level of function (locomotion) as before, but can in fact surpass the original level of performance (speed). This suggests that shape change, instead of control readaptation, may be a better method to recover function after damage in some cases. ",Automated shapeshifting for function recovery in damaged robots
59,1131479265948655616,1126199827547791362,Antonino Furnari,"['Our new paper on Egocentric Action Anticipation is now on arXiv!\n\nCheck the paper here: <LINK>, the project page here: <LINK> and a demo video here: <LINK>. <LINK>']",https://arxiv.org/abs/1905.09035,"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see our web pages for code and examples: this http URL - this https URL ","What Would You Expect? Anticipating Egocentric Actions with
  Rolling-Unrolling LSTMs and Modality Attention"
60,1131473734030778369,2843653680,Anna Ferre-Mateu,['it is goodie thursday! check out our latest accepted paper using the shinning new instrument KCWI to unravel the two phases of formation of the massive galaxy NGC1407! \n<LINK>'],https://arxiv.org/abs/1905.08818,"Using the newly commissioned KCWI instrument on the Keck-II telescope, we analyse the stellar kinematics and stellar populations of the well-studied massive early-type galaxy (ETG) NGC 1407. We obtained high signal-to-noise integral-field-spectra for a central and an outer (around one effective radius towards the south-east direction) pointing with integration times of just 600s and 2400s, respectively. We confirm the presence of a kinematically distinct core also revealed by VLT/MUSE data of the central regions. While NGC 1407 was previously found to have stellar populations characteristic of massive ETGs (with radially constant old ages and high alpha-enhancements), it was claimed to show peculiar super-solar metallicity peaks at large radius that deviated from an otherwise strong negative metallicity gradient, which is hard to reconcile within a `two-phase' formation scenario. Our outer pointing confirms the near-uniform old ages and the presence of a steep metallicity gradient, but with no evidence for anomalously high metallicity values at large galactocentric radii. We find a rising outer velocity dispersion profile and high values of the 4th-order kinematic moment -- an indicator of possible anisotropy. This coincides with the reported transition from a bottom-heavy to a Salpeter initial mass function, which may indicate that we are probing the transition region from the `in-situ' to the accreted phase. With short exposures, we have been able to derive robust stellar kinematics and stellar populations in NGC 1407 to about 1 effective radius. This experiment shows that future work with KCWI will enable 2D kinematics and stellar populations to be probed within the low surface brightness regions of galaxy halos in an effective way. ","Spatially-resolved stellar populations and kinematics with KCWI: probing
  the assembly history of the massive early-type galaxy NGC 1407"
61,1131258097949925377,1439446945,Lav Varshney,"['Our paper on computational creativity for sustainable building materials (e.g. better formulations for concrete) now on arXiv <LINK>; new formulations can significantly reduce carbon emissions, etc. from concrete production @ECEILLINOIS @CSL_Illinois @IBMResearch']",https://arxiv.org/abs/1905.08222,"Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements. Recent advances in artificial intelligence have enabled machines to generate highly plausible artifacts, such as images of realistic looking faces. Semi-supervised generative models allow generation of artifacts with specific, desired characteristics. In this work, we use Conditional Variational Autoencoders (CVAE), a type of semi-supervised generative model, to discover concrete formulas with desired properties. Our model is trained using open data from the UCI Machine Learning Repository joined with environmental impact data computed using a web-based tool. We demonstrate CVAEs can design concrete formulas with lower emissions and natural resource usage while meeting design requirements. To ensure fair comparison between extant and generated formulas, we also train regression models to predict the environmental impacts and strength of discovered formulas. With these results, a construction engineer may create a formula that meets structural needs and best addresses local environmental concerns. ",Accelerated Discovery of Sustainable Building Materials
62,1131181226075729922,973860795246198784,Z.Wei,"['Our new paper is on the arXiv. \n<LINK>\nIn AdS/CFT, a local excitation corresponds to a heavy object in AdS.\nThere is gravitational force between two heavy objects.\nWe analyzed two local excitations in CFT at the same time, and discussed the results wrt gravity.']",https://arxiv.org/abs/1905.08265,"In this work we extensively study the dynamics of excited states created by instantaneous local quenches at two different points, i.e., double local quenches. We focus on setups in two dimensional holographic and free Dirac fermion CFTs. We calculate the energy stress tensor and entanglement entropy for double joining and splitting local quenches. In the splitting local quenches we find an interesting oscillating behaviors. Finally, we study the energy stress tensor in double operator local quenches. In all these examples, we find that, in general, there are non-trivial interactions between the two local quenches. Especially, in holographic CFTs, the differences of the above quantities between the double local quench and the simple sum of two local quenches tend to be negative. We interpret this behavior as merely due to gravitational force in their gravity duals. ",Double Local Quenches in 2D CFTs and Gravitational Force
63,1131053089849577472,93411059,Bob Stienen,"[""New paper out on arxiv today 😃 It's on constraining your favourite parameter space with the help of active learning. Have a look at <LINK>. Written together with @SaschaCaron, Tom Heskes and @SydneyTweeting""]",https://arxiv.org/abs/1905.08628,"Constraining the parameters of physical models with $>5-10$ parameters is a widespread problem in fields like particle physics and astronomy. The generation of data to explore this parameter space often requires large amounts of computational resources. The commonly used solution of reducing the number of relevant physical parameters hampers the generality of the results. In this paper we show that this problem can be alleviated by the use of active learning. We illustrate this with examples from high energy physics, a field where simulations are often expensive and parameter spaces are high-dimensional. We show that the active learning techniques query-by-committee and query-by-dropout-committee allow for the identification of model points in interesting regions of high-dimensional parameter spaces (e.g. around decision boundaries). This makes it possible to constrain model parameters more efficiently than is currently done with the most common sampling algorithms and to train better performing machine learning models on the same amount of data. Code implementing the experiments in this paper can be found on GitHub. ","Constraining the Parameters of High-Dimensional Models with Active
  Learning"
64,1131019312422981635,1406107736,Kameron Decker Harris,"['New paper alert! Time-varying Autoregression with Low Rank Tensors \n<LINK> <LINK>', 'In a nutshell: linear dynamical systems fitting to data that is 1) variable over time 2) low rank and 3) smoothly-varying, using a scalable tensor formulation that seems natural for the problem\n\nLet me know what you think!', 'work with Sasha Aravkin, Raj Rao, and @bingbrunton', 'we can haz teh code https://t.co/1kowVLNUEE', '@tweetsatpreet @ItsNeuronal Thanks @tweetsatpreet would also love to hear what @scott_linderman and @yuqirose think']",https://arxiv.org/abs/1905.08389,"We present a windowed technique to learn parsimonious time-varying autoregressive models from multivariate timeseries. This unsupervised method uncovers interpretable spatiotemporal structure in data via non-smooth and non-convex optimization. In each time window, we assume the data follow a linear model parameterized by a system matrix, and we model this stack of potentially different system matrices as a low rank tensor. Because of its structure, the model is scalable to high-dimensional data and can easily incorporate priors such as smoothness over time. We find the components of the tensor using alternating minimization and prove that any stationary point of this algorithm is a local minimum. We demonstrate on a synthetic example that our method identifies the true rank of a switching linear system in the presence of noise. We illustrate our model's utility and superior scalability over extant methods when applied to several synthetic and real-world example: two types of time-varying linear systems, worm behavior, sea surface temperature, and monkey brain datasets. ",Time-varying Autoregression with Low Rank Tensors
65,1130900739155304448,17933536,Kevin Hardegree-Ullman,"['My new paper came out last week (<LINK>), so here is the TL;DR version in GIFs answering the (age old) question, how common are planets around the smallest stars? <LINK>', 'Previous studies using Kepler data (e.g. Howard+ 2012, Dressing &amp; Charbonneau 2013+2015, Mulders+ 2015, Gaidos+ 2016) have shown that as host star size decreases, the number of planets orbiting them increases. Those studies have mostly stopped at early-type M dwarfs (M0-M2). https://t.co/ShGHaIyqBg', 'We know that planet formation occurs at least through the main sequence (see TRAPPIST-1), but what happens in between for mid-type M dwarfs? https://t.co/NtqSaeNNkS', 'To address the question of planet occurrence rates for Kepler mid-type M dwarfs, we need to assemble a few ingredients: a stellar population, planets orbiting stars in that population, and a measure of signal-to-noise for those stars in the Kepler light curves. https://t.co/bBoNbn4xNl', 'To isolate our stellar population we used a combination of color cuts and estimated stellar temperatures from photometry. This gave us an input sample of ~560 probable mid-Ms. https://t.co/crRPrAbXbd', 'Among those stars, there are 13 confirmed transiting planets orbiting 7 different stars. A few things pop out about these planets: they are all smaller than 2.5 Earth radii, they all orbit their host star in less than 10 days, and 3 of the stars are hosts to 3 planets. https://t.co/hf0RcNPqFb', 'Once we have our first two ingredients, the third ingredient is relatively easy to calculate given a parameter called the Combined Differential Photometric Precision (CDPP), which is so graciously provided by the Kepler team. https://t.co/67CcNrzDua', 'Cool, now we have our ingredients, so why did it take 4ish years to make this paper? Good question. https://t.co/5vuep0Dmas', 'In the rush to discover new exoplanets, scientists sometimes forget that stars are at least equally, if not more, important! https://t.co/SXB0gYKOeb', 'First, we want to make sure that the stars we isolated using photometry are actually mid-type M dwarfs and not something else, like a K dwarf, or worse, an M giant. This requires a spectrum, which can take a lot of time without a dedicated telescope. https://t.co/AH073Qoyv4', 'Due to the wonderful NASA/NSF Exoplanet Observational Research (NN-EXPLORE) program, we managed to get ~300 spectra using the Hydra multi-object spectrograph at the WIYN telescope on Kitt Peak in Arizona. https://t.co/tOdCx5KMZI', 'We also observed ~50 stars using the DeVeny spectrograph on the Discovery Channel Telescope, ~90 stars using SpeX on the NASA Infrared Telescope Facility, and we retrieved ~20 spectra from the LAMOST survey catalog. https://t.co/qBVskXTmBd', ""In total we observed 337 out of 560 stars, which gave us precise spectral types. Since we didn't want this project taking another 4 years, we used machine learning to spectral type the other stars. https://t.co/KqIsIQaoUX"", 'The spectra helped us weed out 4 giant stars, and we identified 4 stars that are potential M dwarf-white dwarf binaries! https://t.co/qsMukO86XH', 'When deriving planet radii via the transit method, our precision is limited by how well we know the radius of the host star. https://t.co/DwogVGc4aq', 'For planet occurrence rates, we need to know not only the host star radius, but also the radii of all stars within the stellar population. This is how we can determine whether or not a known planet could have been found in the Kepler data if it were in a transiting geometry. https://t.co/RBEm3Lv5Cc', 'Gaia DR2 gave us precise distances to over 90% of our stars. With distances we computed absolute K-band magnitudes, and we used magnitude versus radius relationships to derive precise stellar radii to ~3%. https://t.co/32y2rsrjUY', 'This is a 3 to 10 times improvement in stellar radii from previous measurements! Our radii are systematically ~0.1 solar radii larger than previous measurements. https://t.co/jCjdWHqerU', 'With our new stellar radius measurements, we revised the radii of planets around Kepler mid-Ms, and then turned the crank on the planet occurrence rate calculation. https://t.co/lNAPAnJRwT', 'For mid-type M dwarfs we find a planet occurrence rate of 1.19 planets per star for planets smaller than 2.5 Earth radii and orbital periods shorter than 10 days. Additionally, every mid-M is likely to host an Earth-sized planet (0.5-1.5 Earth radii). https://t.co/oXHggdcWvP', ""Since we have individual spectral types for all the stars in our population, we computed planet occurrence rate per spectral sub-type (which hasn't been done before!), and found 0.86, 1.36, and 3.07 planets per star for M3 V, M4 V, and M5 V, respectively. https://t.co/jgzpDwmSml"", 'Due to small number statistics, the uncertainties on these measurements are fairly large, but the numbers suggest there is evidence for an increase in planet occurrence toward later spectral sub-types. https://t.co/XqbOy9XATf', 'We hope to apply this work to K2, which surveyed 5 times more M dwarfs than Kepler!\n\nThere are a lot more details I skipped, so go read my paper (https://t.co/KKGbC5ctnn) and send me questions! https://t.co/X3Xc9IgYX6']",https://arxiv.org/abs/1905.05900,"Previous studies of planet occurrence rates largely relied on photometric stellar characterizations. In this paper, we present planet occurrence rates for mid-type M dwarfs using spectroscopy, parallaxes, and photometry to determine stellar characteristics. Our spectroscopic observations have allowed us to constrain spectral type, temperatures, and in some cases metallicities for 337 out of 561 probable mid-type M dwarfs in the primary Kepler field. We use a random forest classifier to assign a spectral type to the remaining 224 stars. Combining our data with Gaia parallaxes, we compute precise ($\sim$3%) stellar radii and masses, which we use to update planet parameters and planet occurrence rates for Kepler mid-type M dwarfs. Within the Kepler field, there are seven M3 V to M5 V stars which host 13 confirmed planets between 0.5 and 2.5 Earth radii and at orbital periods between 0.5 and 10 days. For this population, we compute a planet occurrence rate of $1.19^{+0.70}_{-0.49}$ planets per star. For M3 V, M4 V, and M5 V, we compute planet occurrence rates of $0.86^{+1.32}_{-0.68}$, $1.36^{+2.30}_{-1.02}$, and $3.07^{+5.49}_{-2.49}$ planets per star, respectively. ","Kepler Planet Occurrence Rates for Mid-Type M Dwarfs as a Function of
  Spectral Type"
66,1130879676824674309,50036150,Jacob Gildenblat,['Our new paper on Self-Supervised Similarity Learning for Digital Pathology: \n<LINK>'],https://arxiv.org/abs/1905.08139,"Using features extracted from networks pretrained on ImageNet is a common practice in applications of deep learning for digital pathology. However it presents the downside of missing domain specific image information. In digital pathology, supervised training data is expensive and difficult to collect. We propose a self-supervised method for feature extraction by similarity learning on whole slide images (WSI) that is simple to implement and allows creation of robust and compact image descriptors. We train a siamese network, exploiting image spatial continuity and assuming spatially adjacent tiles in the image are more similar to each other than distant tiles. Our network outputs feature vectors of length 128, which allows dramatically lower memory storage and faster processing than networks pretrained on ImageNet. We apply the method on digital pathology WSIs from the Camelyon16 train set and assess and compare our method by measuring image retrieval of tumor tiles and descriptor pair distance ratio for distant/near tiles in the Camelyon16 test set. We show that our method yields better retrieval task results than existing ImageNet based and generic self-supervised feature extraction methods. To the best of our knowledge, this is also the first published method for self-supervised learning tailored for digital pathology. ",Self-Supervised Similarity Learning for Digital Pathology
67,1130727097079619584,1020088099,Umberto Picchini,"['New paper with @bayesian_stats: we accelerate pseudomarginal ABC-MCMC sampling for expensive models via data resampling. We reduce bias due to resampling using stratified Monte Carlo. This also allows using a large ABC threshold.  <LINK> <LINK>', 'thanks to a larger threshold, when using appropriate strata we experience lower integrated autocorrelation times (IAT), which means the MCMC procedure is more efficient.']",http://arxiv.org/abs/1905.07976,"Approximate Bayesian computation (ABC) is computationally intensive for complex model simulators. To exploit expensive simulations, data-resampling via bootstrapping can be employed to obtain many artificial datasets at little cost. However, when using this approach within ABC, the posterior variance is inflated, thus resulting in biased posterior inference. Here we use stratified Monte Carlo to considerably reduce the bias induced by data resampling. We also show empirically that it is possible to obtain reliable inference using a larger than usual ABC threshold. Finally, we show that with stratified Monte Carlo we obtain a less variable ABC likelihood. Ultimately we show how our approach improves the computational efficiency of the ABC samplers. We construct several ABC samplers employing our methodology, such as rejection and importance ABC samplers, and ABC-MCMC samplers. We consider simulation studies for static (Gaussian, g-and-k distribution, Ising model, astronomical model) and dynamic models (Lotka-Volterra). We compare against state-of-art sequential Monte Carlo ABC samplers, synthetic likelihoods, and likelihood-free Bayesian optimization. For a computationally expensive Lotka-Volterra case study, we found that our strategy leads to a more than 10-fold computational saving, compared to a sampler that does not use our novel approach. ","Stratified sampling and bootstrapping for approximate Bayesian
  computation"
68,1130721831177080832,16434310,chrislintott,"['New paper on arXiv this morning from @mike_w_ai and the @galaxyzoo team - Bayesian machine learning for galaxy classification. <LINK> (Less technical explanation coming shortly!)', ""@anais_moller @mike_w_ai @galaxyzoo I've just found your paper - I'll have a look!""]",https://arxiv.org/abs/1905.07424,"We use Bayesian convolutional neural networks and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Our posteriors are well-calibrated (e.g. for predicting bars, we achieve coverage errors of 11.8% within a vote fraction deviation of 0.2) and hence are reliable for practical use. Further, using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. We show that training our Bayesian CNNs using active learning requires up to 35-60% fewer labelled galaxies, depending on the morphological feature being classified. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution. ","Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active
  Learning"
69,1130628581581762561,813503217158017024,Brian L Trippe,"['I\'m excited to share a new paper with @jhhhuggins, Raj Agrawal and @ta_broderick coming out @icmlconf!  We speed up Bayesian inference in high dimensional generalized linear models using low rank approximations of data, with a method we call ""LR-GLM"".\n<LINK> 1/5', 'With LR-GLM, we make inference with Laplace approximations and MCMC faster by up to full factor of the dimensionality. The rank of the approximation defines a trade-off between the computational demands and accuracy of the approximation. 2/5', ""Unlike variational Bayes, this approximation is conservative; LR-GLM doesn't underestimate uncertainty.  We also show how increasing computational budget increases the information extracted from data about the model. 3/5"", 'Additionally, we provide theoretical guarantees on approximation quality, with non-asymptotic bounds on approximation error of posterior means and uncertainties. 4/5', 'Also check out our other ICML paper (led by Raj Agrawal) on ""The Kernel Interaction Trick"", which allows us to use Bayesian inference to efficiently identify pairwise interactions between covariates in regression models!\nhttps://t.co/dxM470TzW4 5/5']",https://arxiv.org/abs/1905.07499,"Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis. In these high-dimensional problems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty; a Bayesian approach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension, and so are limited to settings with at most tens of thousand parameters. We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM. When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation and admits running times reduced by a full factor of the parameter dimension. We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable computational-statistical trade-off. Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets. ","LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data
  Approximations"
70,1130486853709250561,2800204849,Andrew Gordon Wilson,"['Simple Black-box Adversarial Attacks! In our new #ICML2019 paper, we introduce SimBA, an extraordinarily simple and query-efficient baseline for black-box attacks (with PyTorch code): <LINK> <LINK>']",https://arxiv.org/abs/1905.07121,"We propose an intriguingly simple method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an open problem to date. With only the mild assumption of continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle: we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image. Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks -- resulting in previously unprecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is extremely fast and its implementation requires less than 20 lines of PyTorch code. ",Simple Black-box Adversarial Attacks
71,1130287006054064130,106843613,Jacob Haqq Misra,"['Does complex life depend upon the spectral type of the host star? Check out my new (perhaps provocative?) paper, forthcoming in Astrobiology. \n\nI speculate that M-dwarfs (less than 0.7 solar masses) are too young to have any complex life today.\n<LINK>']",https://arxiv.org/abs/1905.07343,"This paper presents the proportional evolutionary time hypothesis, which posits that the mean time required for the evolution of complex life is a function of stellar mass. The ""biological available window"" is defined as the region of a stellar spectrum between 200 to 1200 nm that generates free energy for life. Over the $\sim$4 Gyr history of Earth, the total energy incident at the top of the atmosphere and within the biological available window is $\sim$10$^{34}$ J. The hypothesis assumes that the rate of evolution from the origin of life to complex life is proportional to this total energy, which would suggest that planets orbiting other stars should not show signs of complex life if the total energy incident on the planet is below this energy threshold. The proportional evolutionary time hypothesis predicts that late K- and M-dwarf stars (M < 0.7 M$_{\odot}$) are too young to host any complex life at the present age of the universe. F-, G-, and early K-dwarf stars (M > 0.7 M$_{\odot}$) represent the best targets for the next generation of space telescopes to search for spectroscopic biosignatures indicative of complex life. ","Does the evolution of complex life depend on the stellar spectral energy
  distribution?"
72,1130284766446510080,1078963404356767745,Prof. Jared Cole,['Our paper proposing a new method of probing charge carrier dynamics in thin films is on the arXiv. Comments and suggestions welcome. @excitonscience @ResearchRMIT \n<LINK>'],https://arxiv.org/abs/1905.07115,"Understanding the movement of charge within organic semiconducting films is crucial for applications in photo-voltaics and flexible electronics. We study the sensitivity of the electrical conductance of a silicon nanowire to changes of charge states within an organic semiconductor physisorbed on the surface of the nanowire. Elastic scattering caused by motion of charge carriers near the nanowire modifies the mean-free path for backscattering of electrons propagating within it, which we have mathematically expressed in terms of the causal Green's functions. The scattering potential has been computed using a combination of the polarizable continuum model and density functional theory with the range-separated exchange-correlation functional for organic molecules and the semi-empirical tight-binding model for silicon. As an example, the sensitivity to charge state changes in tetracene is computed as a function of operating temperature and geometrical parameters of a nanowire. For a single molecule, ultra-thin silicon nanowires with characteristic sizes of the cross-section below 2 nm produce a detectable conductance change at room temperature. For larger nanowires the sensitivity is reduced, however the conductance change grows with the number of charged molecules: with sub-4 nm nanowires being sensitive enough to detect several tens of charge carriers. We propose using noise spectroscopy to access the temporal evolution of the charge states. Information regarding the spatial distribution of charge carries in organic thin films can be obtained using a grid of nanowire resistors and electric impedance tomography. ","Probing charge carrier movement in organic semiconductor thin films via
  nanowire conductance spectroscopy"
73,1130276465583329280,795877354266456064,KoheiKamadaPhys,"['<LINK>\nSubmitted a new paper! I am very happy in a sense that the knowledge at my very first paper is useful, the understanding on my paper when I was first year as a postdoc gets deeper, and it has a tight connection to my recent research, magnetic fields.', 'It is on the Affleck-Dine mechanism, a famous baryogenesis mechanism. We found that in some cases a magnetogenesis mechanism has already been implemented in that mechanism, which should have many applications and impacts on cosmology.']",https://arxiv.org/abs/1905.06966,"The chiral magnetic effect (CME) is a phenomenon in which an electric current is induced parallel to an external magnetic field in the presence of chiral asymmetry in a fermionic system. In this paper, we show that the electric current induced by the dynamics of a pseudo-scalar field which anomalously couples to electromagnetic fields can be interpreted as closely analogous to the CME. In particular, the velocity of the pseudo-scalar field, which is the phase of a complex scalar, indicates that the system carries a global U(1) number asymmetry as the source of the induced current. We demonstrate that an initial kick to the phase-field velocity and an anomalous coupling between the phase-field and gauge fields are naturally provided, in a set-up such as the Affleck-Dine mechanism. The resulting asymmetry carried by the Affleck-Dine field can give rise to instability in the (electro)magnetic field. Cosmological consequences of this mechanism are also investigated. ","Magnetogenesis from a rotating scalar: \`a la scalar chiral magnetic
  effect"
74,1129502870041235456,66169171,Sherjil Ozair ☀️,"['Our ICML\'19 paper is out on ArXiv! ""On Variational Bounds of Mutual Information"". Link: <LINK>\n\nWe unify various existing and new variational bounds of mutual information in a single framework, and analyzed the tradeoffs between the various bounds. <LINK>', 'Mutual information has been crucial for recent progress in unsupervised representation learning (CPC, Deep Infomax) and unsupervised reinforcement learning (Empowerment). I hope our work will lead to more progress in these exciting applications!', 'Thanks to my fantastic co-authors @poolio, @avdnoord, Alex Alemi, @georgejtucker. I learned a lot collaborating with them!']",https://arxiv.org/abs/1905.06922,"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning. ",On Variational Bounds of Mutual Information
75,1129319285090865152,3351977373,Alex Clark,['New arXiv paper up from us on coupling molecules to a hybrid plasmonic waveguide! <LINK> @ImperialPhysics @QSUMproject @LondonLight_ #quantum #photonics #molecules'],http://arxiv.org/abs/1905.06321,"We demonstrate the emission of photons from a single molecule into a hybrid gap plasmon waveguide (HGPW). Crystals of anthracene, doped with dibenzoterrylene (DBT), are grown on top of the waveguides. We investigate a single DBT molecule coupled to the plasmonic region of one of the guides, and determine its in-plane orientation, excited state lifetime and saturation intensity. The molecule emits light into the guide, which is remotely out-coupled by a grating. The second-order auto-correlation and cross-correlation functions show that the emitter is a single molecule and that the light emerging from the grating comes from that molecule. The coupling efficiency is found to be $\beta_{WG}=11.6(1.5)\%$. This type of structure is promising for building new functionality into quantum-photonic circuits, where localised regions of strong emitter-guide coupling can be interconnected by low-loss dielectric guides. ",Hybrid plasmonic waveguide coupling of photons from a single molecule
76,1129275019102756869,882303076505456642,Timon Emken,"['Our new paper about the direct detection of low-mass #DarkMatter interacting strongly with ordinary matter hit the @arxiv today. Short summary in these tweets.  \n\n<LINK> <LINK>', ""A detection experiment on Earth cannot detect DM above some critical interaction strength due to scatterings in Earth's crust/atmosphere. We used #MonteCarlo analytic methods to estimate these critical cross sections for various DM-electron scattering experiments and models. https://t.co/guBckHRrTz"", 'We find an open window in parameter space for a strongly interacting sub-dominant component of DM (&lt;1%) for ultralight (but not massless) dark photon mediators. https://t.co/aosCZZu1PH', 'A small-scale detector at high altitudes, e.g. on a balloon or a satellite could probe such strong DM interactions. This kind of experiment would be sensitive to a strong, orbital signal modulation due to the Earth\'s ""dark matter shadow"", as seen in the video.', 'The new version of the DaMaSCUS-CRUST code (Dark Matter Simulation Code for Underground Scatterings) is publicly available on @GitHub and @ZENODO_ORG. \n\nhttps://t.co/Zo5H7bXCKz \n\nhttps://t.co/QH5mFwp2mw']",https://arxiv.org/abs/1905.06348,"We consider direct-detection searches for sub-GeV dark matter via electron scatterings in the presence of large interactions between dark and ordinary matter. Scatterings both on electrons and nuclei in the Earth's crust, atmosphere, and shielding material attenuate the expected local dark matter flux at a terrestrial detector, so that such experiments lose sensitivity to dark matter above some critical cross section. We study various models, including dark matter interacting with a heavy and ultralight dark photon, through an electric dipole moment, and exclusively with electrons. For a dark-photon mediator and an electric dipole interaction, the dark matter-electron scattering cross-section is directly linked to the dark matter-nucleus cross section, and nuclear interactions typically dominate the attenuation process. We determine the exclusion bands for the different dark-matter models from several experiments - SENSEI, CDMS-HVeV, XENON10, XENON100, and DarkSide-50 - using a combination of Monte Carlo simulations and analytic estimates. We also derive projected sensitivities for a detector located at different depths and for a range of exposures, and calculate the projected sensitivity for SENSEI at SNOLAB and DAMIC-M at Modane. Finally, we discuss the reach to high cross sections and the modulation signature of a small balloon- and satellite-borne detector sensitive to electron recoils, such as a Skipper-CCD. Such a detector could potentially probe unconstrained parameter space at high cross sections for a sub-dominant component of dark matter interacting with a massive, but ultralight, dark photon. ","Direct Detection of Strongly Interacting Sub-GeV Dark Matter via
  Electron Recoils"
77,1129269590389907462,85788359,Karthik A Sankararaman 🇮🇳🇺🇸,['New paper on Robust identifiability in Linear Structural Equation Model of #causality. This paper will appear in #UAI2019.  We characterize when generic identifiability is robust to adversarial noise. Link below.\n\n<LINK>\n\n@umdcs #ML'],https://arxiv.org/abs/1905.06836v1,"We consider the numerical stability of the parameter recovery problem in Linear Structural Equation Model ($\LSEM$) of causal inference. A long line of work starting from Wright (1920) has focused on understanding which sub-classes of $\LSEM$ allow for efficient parameter recovery. Despite decades of study, this question is not yet fully resolved. The goal of this paper is complementary to this line of work; we want to understand the stability of the recovery problem in the cases when efficient recovery is possible. Numerical stability of Pearl's notion of causality was first studied in Schulman and Srivastava (2016) using the concept of condition number where they provide ill-conditioned examples. In this work, we provide a condition number analysis for the $\LSEM$. First we prove that under a sufficient condition, for a certain sub-class of $\LSEM$ that are \emph{bow-free} (Brito and Pearl (2002)), the parameter recovery is stable. We further prove that \emph{randomly} chosen input parameters for this family satisfy the condition with a substantial probability. Hence for this family, on a large subset of parameter space, recovery is numerically stable. Next we construct an example of $\LSEM$ on four vertices with \emph{unbounded} condition number. We then corroborate our theoretical findings via simulations as well as real-world experiments for a sociology application. Finally, we provide a general heuristic for estimating the condition number of any $\LSEM$ instance. ",] Stability of Linear Structural Equation Models of Causal Inference
78,1129186137203855360,1558538456,Rodrigo Fernández,"['New paper on mass ejection from accretion disks formed in WD-NS and WD-BH mergers. Nuclear fusion is important, which makes for some colorful displays. 1/\n\nIn collaboration with Ben Margalit &amp; @bluekilonova \n\n<LINK> <LINK>', 'The quasi-onion-shell structure was envisioned by @bluekilonova back in 2012, disks are radiatively-inefficient so they eject a lot of mass, and are expected to produce EM transients  2/\n\nhttps://t.co/RQTTXMKoov https://t.co/VSWtuBmcp4', 'Highlights from our new results (CO &amp; ONe WDs):\n- outflow speeds 100 -10,000 km/s\n- composition dominated by WD, burn products mixed in at ~10-30% level\n- make up to ~0.01Msun of Ni56\n- should produce transients with rise time ~ few days\n- potentially also X-ray transients    3/', 'Luminosity from Ni56-&gt;Co56-&gt;Fe56 alone is ~1E+40-1E+41 erg/s (on the low side), possible to enhance it with accretion power or circumstellar interaction  4/']",https://arxiv.org/abs/1905.06343,"We study mass ejection from accretion disks formed in the merger of a white dwarf with a neutron star or black hole. These disks are mostly radiatively-inefficient and support nuclear fusion reactions, with ensuing outflows and electromagnetic transients. Here we perform time-dependent, axisymmetric hydrodynamic simulations of these disks including a physical equation of state, viscous angular momentum transport, a coupled $19$-isotope nuclear network, and self-gravity. We find no detonations in any of the configurations studied. Our global models extend from the central object to radii much larger than the disk. We evolve these global models for several orbits, as well as alternate versions with an excised inner boundary to much longer times. We obtain robust outflows, with a broad velocity distribution in the range $10^2-10^4$ km s$^{-1}$. The outflow composition is mostly that of the initial white dwarf, with burning products mixed in at the $\lesssim 10-30\%$ level by mass, including up to $\sim 10^{-2}M_\odot$ of ${}^{56}$Ni. These heavier elements (plus ${}^{4}$He) are ejected within $\lesssim 40^\circ$ of the rotation axis, and should have higher average velocities than the lighter elements that make up the white dwarf. These results are in broad agreement with previous one- and two-dimensional studies, and point to these systems as progenitors of rapidly-rising ($\sim $ few day) transients. If accretion onto the central BH/NS powers a relativistic jet, these events could be accompanied by high energy transients with peak luminosities $\sim 10^{47}-10^{50}$ erg s$^{-1}$ and peak durations of up to several minutes, possibly accounting for events like CDF-S XT2. ","Nuclear Dominated Accretion Flows in Two Dimensions. II. Ejecta dynamics
  and nucleosynthesis for CO and ONe white dwarfs"
79,1129179259728158721,14755464,Yves Raimond,['New @NetflixResearch paper alert! Beta Survival Models: <LINK>'],https://arxiv.org/abs/1905.03818,"This article analyzes the problem of estimating the time until an event occurs, also known as survival modeling. We observe through substantial experiments on large real-world datasets and use-cases that populations are largely heterogeneous. Sub-populations have different mean and variance in their survival rates requiring flexible models that capture heterogeneity. We leverage a classical extension of the logistic function into the survival setting to characterize unobserved heterogeneity using the beta distribution. This yields insights into the geometry of the problem as well as efficient estimation methods for linear, tree and neural network models that adjust the beta distribution based on observed covariates. We also show that the additional information captured by the beta distribution leads to interesting ranking implications as we determine who is most-at-risk. We show theoretically that the ranking is variable as we forecast forward in time and prove that pairwise comparisons of survival remain transitive. Empirical results using large-scale datasets across two use-cases (online conversions and retention modeling), demonstrate the competitiveness of the method. The simplicity of the method and its ability to capture skew in the data makes it a viable alternative to standard techniques particularly when we are interested in the time to event and when the underlying probabilities are heterogeneous. ",Beta Survival Models
80,1129148265184862208,45724845,Swarat Chaudhuri,"['New #ICML2019 paper: ""Control Regularization for Reduced Variance Reinforcement Learning"". Moral: by regularizing DeepRL with a symbolic ""control prior"", you can: 1) learn more efficiently with lower-variance gradients; 2) get provably stable policies. <LINK>']",https://arxiv.org/abs/1905.05380,"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL. In particular, we regularize the behavior of the deep policy to be similar to a policy prior, i.e., we regularize in function space. We show that functional regularization yields a bias-variance trade-off, and propose an adaptive tuning strategy to optimize this trade-off. When the policy prior has control-theoretic stability guarantees, we further show that this regularization approximately preserves those stability guarantees throughout learning. We validate our approach empirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone. ",Control Regularization for Reduced Variance Reinforcement Learning
81,1129066964071178240,2439364105,Fei Wang,['Learning to learn with Electronic Health Records. Check out our new #kdd2019 paper <LINK>'],https://arxiv.org/abs/1905.03218,"In recent years, increasingly augmentation of health data, such as patient Electronic Health Records (EHR), are becoming readily available. This provides an unprecedented opportunity for knowledge discovery and data mining algorithms to dig insights from them, which can, later on, be helpful to the improvement of the quality of care delivery. Predictive modeling of clinical risk, including in-hospital mortality, hospital readmission, chronic disease onset, condition exacerbation, etc., from patient EHR, is one of the health data analytic problems that attract most of the interests. The reason is not only because the problem is important in clinical settings, but also there are challenges working with EHR such as sparsity, irregularity, temporality, etc. Different from applications in other domains such as computer vision and natural language processing, the labeled data samples in medicine (patients) are relatively limited, which creates lots of troubles for effective predictive model learning, especially for complicated models such as deep learning. In this paper, we propose MetaPred, a meta-learning for clinical risk prediction from longitudinal patient EHRs. In particular, in order to predict the target risk where there are limited data samples, we train a meta-learner from a set of related risk prediction tasks which learns how a good predictor is learned. The meta-learned can then be directly used in target risk prediction, and the limited available samples can be used for further fine-tuning the model performance. The effectiveness of MetaPred is tested on a real patient EHR repository from Oregon Health & Science University. We are able to demonstrate that with CNN and RNN as base predictors, MetaPred can achieve much better performance for predicting target risk with low resources comparing with the predictor trained on the limited samples available for this risk. ","MetaPred: Meta-Learning for Clinical Risk Prediction with Limited
  Patient Electronic Health Records"
82,1129057208711426054,5654412,Konstantin Berlin,"['I am super excited about our new paper ""SMART: Semantic Malware Attribute Relevance Tagging"" (with @fel_d @EthanMRudd @AlexMasonLong, and Tad) <LINK>. Feedback and comments welcome.']",https://arxiv.org/abs/1905.06262,"With the rapid proliferation and increased sophistication of malicious software (malware), detection methods no longer rely only on manually generated signatures but have also incorporated more general approaches like machine learning detection. Although powerful for conviction of malicious artifacts, these methods do not produce any further information about the type of threat that has been detected neither allows for identifying relationships between malware samples. In this work, we address the information gap between machine learning and signature-based detection methods by learning a representation space for malware samples in which files with similar malicious behaviors appear close to each other. We do so by introducing a deep learning based tagging model trained to generate human-interpretable semantic descriptions of malicious software, which, at the same time provides potentially more useful and flexible information than malware family names. We show that the malware descriptions generated with the proposed approach correctly identify more than 95% of eleven possible tag descriptions for a given sample, at a deployable false positive rate of 1% per tag. Furthermore, we use the learned representation space to introduce a similarity index between malware files, and empirically demonstrate using dynamic traces from files' execution, that is not only more effective at identifying samples from the same families, but also 32 times smaller than those based on raw feature vectors. ","Automatic Malware Description via Attribute Tagging and Similarity
  Embedding"
83,1129008425793409026,2423945684,Vinny Davies,"['New paper now available on @arxiv <LINK> The paper looks at inferring left ventricle heart model parameters from MRI data, using emulation to speed up the inference and make it suitable for clinical use', 'Work was done with Umberto Noè, @LazarusAl, @sharpgao, @akohneko, @ColinBerryMD, Xiaoyu Luo, Dirk Husmeier as part of the @SofTMech project']",https://arxiv.org/abs/1905.06310,"A central problem in biomechanical studies of personalised human left ventricular (LV) modelling is estimating the material properties and biophysical parameters from in-vivo clinical measurements in a time frame suitable for use within a clinic. Understanding these properties can provide insight into heart function or dysfunction and help inform personalised medicine. However, finding a solution to the differential equations which mathematically describe the kinematics and dynamics of the myocardium through numerical integration can be computationally expensive. To circumvent this issue, we use the concept of emulation to infer the myocardial properties of a healthy volunteer in a viable clinical time frame using in-vivo magnetic resonance image (MRI) data. Emulation methods avoid computationally expensive simulations from the LV model by replacing the biomechanical model, which is defined in terms of explicit partial differential equations, with a surrogate model inferred from simulations generated before the arrival of a patient, vastly improving computational efficiency at the clinic. We compare and contrast two emulation strategies: (i) emulation of the computational model outputs and (ii) emulation of the loss between the observed patient data and the computational model outputs. These strategies are tested with two different interpolation methods, as well as two different loss functions... ","Fast Parameter Inference in a Biomechanical Model of the Left Ventricle
  using Statistical Emulation"
84,1128940222224330752,3051959252,Hien Nguyen,"['Hey look! @JulyanArbel, Hongliang Lu, Florence Forbes &amp; I wrote a new paper on Approximate Bayesian Computation using Energy Statistics. We prove some useful large sample results and plot some cool graphs! Let us know what you think 😄. <LINK>']",https://arxiv.org/abs/1905.05884,"Approximate Bayesian computation (ABC) has become an essential part of the Bayesian toolbox for addressing problems in which the likelihood is prohibitively expensive or entirely unknown, making it intractable. ABC defines a pseudo-posterior by comparing observed data with simulated data, traditionally based on some summary statistics, the elicitation of which is regarded as a key difficulty. Recently, using data discrepancy measures has been proposed in order to bypass the construction of summary statistics. Here we propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the so-called two-sample energy statistic. We establish a new asymptotic result for the case where both the observed sample size and the simulated data sample size increase to infinity, which highlights to what extent the data discrepancy measure impacts the asymptotic pseudo-posterior. The result holds in the broad setting of IS-ABC methodologies, thus generalizing previous results that have been established only for rejection ABC algorithms. Furthermore, we propose a consistent V-statistic estimator of the energy statistic, under which we show that the large sample result holds, and prove that the rejection ABC algorithm, based on the energy statistic, generates pseudo-posterior distributions that achieves convergence to the correct limits, when implemented with rejection thresholds that converge to zero, in the finite sample setting. Our proposed energy statistic based ABC algorithm is demonstrated on a variety of models, including a Gaussian mixture, a moving-average model of order two, a bivariate beta and a multivariate $g$-and-$k$ distribution. We find that our proposed method compares well with alternative discrepancy measures. ",Approximate Bayesian computation via the energy statistic
85,1128919192630104064,1056170700099276801,Mattia Mantovani,"['Check out our new paper on Cooper-pair tunneling: <LINK> \nCollaboration between Konstanz @QtUkon, Nottingham, and Chalmers University.']",https://arxiv.org/abs/1905.06194,"Photon emission by tunneling electrons can be encouraged by locating a resonator close to the tunnel junction and applying an appropriate voltage-bias. However, studies of normal metals show that the resonator also affects how the charges flow, facilitating processes in which correlated tunneling of two charges produces one photon. We develop a theory to analyze this kind of behavior in Josephson junctions by deriving an effective Hamiltonian describing processes where two Cooper-pairs generate a single photon. We determine the conditions under which the transport is dominated by incoherent tunneling of two Cooper-pairs, whilst also uncovering a regime of coherent double Cooper-pair tunneling. We show that the system can also display an unusual form of photon-blockade and hence could serve as a single-photon source. ","Theory of double Cooper-pair tunneling and light emission mediated by a
  resonator"
86,1128863639140749312,4438354094,Tom Wong,"[""New paper with @Creighton undergraduate student Mason Rhodes! Open access preprint at <LINK>. It's paper #31 on my website <LINK>. <LINK>""]",https://arxiv.org/abs/1905.05887,"The lackadaisical quantum walk, which is a quantum walk with a weighted self-loop at each vertex, has been shown to speed up dispersion on the line and improve spatial search on the complete graph and periodic square lattice. In these investigations, each self-loop had the same weight, owing to each graph's vertex-transitivity. In this paper, we propose lackadaisical quantum walks where the self-loops have different weights. We investigate spatial search on the complete bipartite graph, which can be irregular with $N_1$ and $N_2$ vertices in each partite set, and this naturally leads to self-loops in each partite set having different weights $l_1$ and $l_2$, respectively. We analytically prove that for large $N_1$ and $N_2$, if the $k$ marked vertices are confined to, say, the first partite set, then with the typical initial uniform state over the vertices, the success probability is improved from its non-lackadaisical value when $l_1 = kN_2/2N_1$ and $N_2 > (3 - 2\sqrt{2}) N_1$, regardless of $l_2$. When the initial state is stationary under the quantum walk, however, then the success probability is improved when $l_1 = kN_2/2N_1$, now without a constraint on the ratio of $N_1$ and $N_2$, and again independent of $l_2$. Next, when marked vertices lie in both partite sets, then for either initial state, there are many configurations for which the self-loops yield no improvement in quantum search, no matter what weights they take. ",Search by Lackadaisical Quantum Walk with Nonhomogeneous Weights
87,1128860741048643589,4903660834,Joseph Vandehey,['New paper on the arXiv. This one a collaboration with Olivier Carton.\n<LINK>'],http://arxiv.org/abs/1905.05801,"We give two different proofs of the fact that non-oblivious selection via regular group sets preserves normality. Non-oblivious here means that whether or not a symbol is selected can depend on the symbol itself. One proof relies on the incompressibility of normal sequences, the other on the use of augmented dynamical systems. ",Preservation of normality by non-oblivious group selection
88,1128832892921090050,734677275216470016,Guodong Zhang,"['Excited to share a new paper with Chaoqi, @RogerGrosse and @FidlerSanja: EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis, appearing at @icmlconf.\n\nPaper: <LINK>\nCode: <LINK>', 'In the paper, we interpret traditional Hessian-based pruning algorithms (i.e., OBD and OBS) from Bayesian perspective, showing that they each approximate full-covariance Gaussian posterior with factorized Gaussians, but minimizing different objectives (forward and reverse KL).', 'So instead of doing pruning in parameter coordinates (which implicitly assume the posterior is factorial), we proposed a novel network reparameterization where different weights are approximately independent and then conduct pruning in this new space.', 'Here is the Figure 1. https://t.co/mJuwYYQQBU']",https://arxiv.org/abs/1905.05934,"Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10$\times$ reduction in model size and a 8$\times$ reduction in FLOPs on wide ResNet32. ",EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis
89,1128744239779553280,32807399,🇺🇦 Kristina Lerman 🇺🇦,['Twitter memes may not be as popular as they appear. Our new paper on the Friendship Paradox and perception bias. \n<LINK>'],https://arxiv.org/abs/1905.05286,"How popular a topic or an opinion appears to be in a network can be very different from its actual popularity. For example, in an online network of a social media platform, the number of people who mention a topic in their posts---i.e., its global popularity---can be dramatically different from how people see it in their social feeds---i.e., its perceived popularity---where the feeds aggregate their friends' posts. We trace the origin of this discrepancy to the friendship paradox in directed networks, which states that people are less popular than their friends (or followers) are, on average. We identify conditions on network structure that give rise to this perception bias, and validate the findings empirically using data from Twitter. Within messages posted by Twitter users in our sample, we identify topics that appear more frequently within the users' social feeds, than they do globally, i.e., among all posts. In addition, we present a polling algorithm that leverages the friendship paradox to obtain a statistically efficient estimate of a topic's global prevalence from biased perceptions of individuals. We characterize the bias of the polling estimate, provide an upper bound for its variance, and validate the algorithm's efficiency through synthetic polling experiments on our Twitter data. Our paper elucidates the non-intuitive ways in which the structure of directed networks can distort social perceptions and resulting behaviors. ",Friendship Paradox Biases Perceptions in Directed Networks
90,1128664809329831936,1614594931,Dr. Rebecca Levy,"['So excited to share my new paper on extraplanar diffuse ionized gas in nearby galaxies, which is up on the arXiv today! <LINK>\n It’s a monster paper, so here are the highlights! (1/n)', 'What is extraplanar diffuse ionized gas (eDIG)? It’s ionized gas (traced primarily by Halpha) that’s much more diffuse and extended than HII regions. Extraplanar means that it’s not confined to the main thin disk of the galaxy. (2/n) https://t.co/yRxCfwqGht', 'Why should you care about eDIG? Because it’s diffuse, it’s much harder to detect and study. However, it accounts for ~50% of the Halpha luminosity of a normal star-forming disk galaxy and isn’t a negligible component. (3/n)', 'In my first paper, we stumbled across the effects of eDIG in galaxies at intermediate inclinations. We found that the ionized gas rotates more slowly than the molecular (cold) gas in nearby, normally star-forming galaxies (Levy et al. 2018). (4/n)', 'We attributed this to eDIG which rotates more slowly than the midplane. Averaging thru the eDIG along the line of sight results in a net lower rotation velocity. But b/c the galaxies weren’t edge-on, we couldn’t directly prove this. (5/n)', 'Enter this paper. We constructed a sample of edge-on galaxies from the CALIFA survey to investigate the prevalence and kinematics of eDIG in a statistical sample of nearby galaxies. (6/n) https://t.co/GRtw6elKMS', 'We directly measured that the ionized gas rotation velocity decreases with height above the midplane for 60% of our sample. This is called a “lag”. We also measure vertically extended Halpha emission for 90% of the galaxies. (7/n) https://t.co/0oy2a7tvWE', 'Combined with other eDIG studies, this shows that eDIG is commonplace in nearby star-forming galaxies. Ionized gas rotation velocity must be corrected for this effect before being used to derive a dynamical (baryons+dark matter) mass! (8/n)', 'But how did the gas become extraplanar in the first place? It either could have been accreted from the IGM or it could have been ejected via galactic fountains from feedback from star formation (or both). (9/n) https://t.co/DrGnEkXymD', 'Some studies have claimed that you can tell the difference by looking at how the lag changes with radius. We show that it’s not this easy because the potential of the galaxy affects the lag as well. (10/n)', 'We also find that the eDIG remains ionized primarily by photons leaking out of HII regions in the midplane. Many other processes have been invoked to explain the eDIG’s ionization, but they only contribute at the ~10% level. (11/n)', 'In summary, the eDIG matters a lot for galaxy studies and relates to a galaxy’s formation history. eDIG is detectable both photometrically and kinematically and is an important component of galaxies than cannot be disregarded. (12/12)']",https://arxiv.org/abs/1905.05196,"We investigate the prevalence, properties, and kinematics of extraplanar diffuse ionized gas (eDIG) in a sample of 25 edge-on galaxies selected from the CALIFA survey. We measure ionized gas scale heights from ${\rm H\alpha}$ and find that 90% have measurable scale heights with a median of $0.8^{+0.7}_{-0.4}$ kpc. From the ${\rm H\alpha}$ kinematics, we find that 60% of galaxies show a decrease in the rotation velocity as a function of height above the midplane. This lag is characteristic of eDIG, and we measure a median lag of 21 km s$^{-1}$ kpc$^{-1}$ which is comparable to lags measured in the literature. We also investigate variations in the lag with radius. $\rm H{\small I}$ lags have been reported to systematically decrease with galactocentric radius. We find both increasing and decreasing ionized gas lags with radius, as well as a large number of galaxies consistent with no radial lag variation, and investigate these results in the context of internal and external origins for the lagging ionized gas. We confirm that the ${\rm [S{\small II}]}$/${\rm H\alpha}$ and ${\rm [N{\small II}]}$/${\rm H\alpha}$ line ratios increase with height above the midplane as is characteristic of eDIG. The ionization of the eDIG is dominated by star-forming complexes (leaky ${\rm H{\small II}}$ regions). We conclude that the lagging ionized gas is turbulent ejected gas likely resulting from star formation activity in the disk as opposed to gas in the stellar thick disk or bulge. This is further evidence for the eDIG being a product of stellar feedback and for the pervasiveness of this WIM-like phase in many local star-forming galaxies. ","The EDGE-CALIFA Survey: Evidence for Pervasive Extraplanar Diffuse
  Ionized Gas in Nearby Edge-On Galaxies"
91,1128586944991191040,869154646694264832,Hugh Salimbeni,"['Delighted to share a new paper, Deep Gaussian Processes with Importance-Weighted Variational Inference <LINK> appearing at @icmlconf. With @vdutor @jameshensman @mpd37. Code: <LINK>', 'This paper does two things: 1) we revisit the latent variable approach of @adamianou  @lawrennd, but introduces the variables as extra covariates, rather than process noise.', '2) We use Importance Weighed variational inference for the latent variables, and show how it interacts with variational inference for the functions. The end result is simple: an reduce_sum gets replaced with a reduce_logsumexp, though there are some subtleties in the details.', 'Crucially, we can apply the importance weighting ideas without throwing away any of the nice analytic results we already were using for DGP inference.', 'Also, we apply the approach to more than 40 datasets (all available at https://t.co/jP1R9fpgSK) and drill down into what is going on with complexity of marginals and mappings.']",https://arxiv.org/abs/1905.05435,"Deep Gaussian processes (DGPs) can model complex marginal densities as well as complex mappings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incorporating uncorrelated variables to the model. Previous work on DGP models has introduced noise additively and used variational inference with a combination of sparse Gaussian processes and mean-field Gaussians for the approximate posterior. Additive noise attenuates the signal, and the Gaussian form of variational distribution may lead to an inaccurate posterior. We instead incorporate noisy variables as latent covariates, and propose a novel importance-weighted objective, which leverages analytic results and provides a mechanism to trade off computation for improved accuracy. Our results demonstrate that the importance-weighted objective works well in practice and consistently outperforms classical variational inference, especially for deeper models. ",Deep Gaussian Processes with Importance-Weighted Variational Inference
92,1128573331958112257,15719460,didier_schwab,"['Our paper Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation with @_Loic_Vial, Benjamin Lecouteux has been accepted at the 10th Global WordNet Conference - GWC 2019 - new state of the art for WSD <LINK>', '@laurent_besacie @_Loic_Vial On peut pas gagner à tous les coups...']",https://arxiv.org/abs/1905.05677,"In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduces the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our method, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperform the state of the art on all WSD evaluation tasks. ","Sense Vocabulary Compression through the Semantic Knowledge of WordNet
  for Neural Word Sense Disambiguation"
93,1128372404613332992,4891021393,Tobias Gerstenberg,"[""New paper in which we explain adults' judgments of how difficult it is to build different block towers by simulating the physical effort and risk it would take to make them. Joint work with Yildirim, Saeed, Bennett-Pierre, Tenenbaum &amp; Gweon <LINK> <LINK>""]",https://arxiv.org/abs/1905.04445,"The ability to estimate task difficulty is critical for many real-world decisions such as setting appropriate goals for ourselves or appreciating others' accomplishments. Here we give a computational account of how humans judge the difficulty of a range of physical construction tasks (e.g., moving 10 loose blocks from their initial configuration to their target configuration, such as a vertical tower) by quantifying two key factors that influence construction difficulty: physical effort and physical risk. Physical effort captures the minimal work needed to transport all objects to their final positions, and is computed using a hybrid task-and-motion planner. Physical risk corresponds to stability of the structure, and is computed using noisy physics simulations to capture the costs for precision (e.g., attention, coordination, fine motor movements) required for success. We show that the full effort-risk model captures human estimates of difficulty and construction time better than either component alone. ","Explaining intuitive difficulty judgments by modeling physical effort
  and risk"
94,1128341115504144384,1117934792807538688,Tuckerman_group,['New #MachineLearning paper on collective variables for phase transformation on the arXiv:  <LINK>'],https://arxiv.org/abs/1905.01536,We propose a rigorous construction of a 1D path collective variable to sample structural phase transformations in condensed matter. The path collective variable is defined in a space spanned by global collective variables that serve as classifiers derived from local structural units. A reliable identification of local structural environments is achieved by employing a neural network based classification. The 1D path collective variable is subsequently used together with enhanced sampling techniques to explore the complex migration of a phase boundary during a solid-solid phase transformation in molybdenum. ,"Neural network based path collective variables for enhanced sampling of
  phase transformations"
95,1128203983401754625,780011537738174464,Dennis Soemers,"['We have a new paper up on arXiv on the new general game system Ludii: <LINK>. By Éric Piette, me, @matthew_stephe , Chiara Sironi, Mark Winands, and @cambolbro (@UM_DKE). Part of Digital Ludeme Project (<LINK>), funded by @ERC_Research <LINK>']",https://arxiv.org/abs/1905.05013,"While current General Game Playing (GGP) systems facilitate useful research in Artificial Intelligence (AI) for game-playing, they are often somewhat specialised and computationally inefficient. In this paper, we describe the ""ludemic"" general game system Ludii, which has the potential to provide an efficient tool for AI researchers as well as game designers, historians, educators and practitioners in related fields. Ludii defines games as structures of ludemes -- high-level, easily understandable game concepts -- which allows for concise and human-understandable game descriptions. We formally describe Ludii and outline its main benefits: generality, extensibility, understandability and efficiency. Experimentally, Ludii outperforms one of the most efficient Game Description Language (GDL) reasoners, based on a propositional network, in all games available in the Tiltyard GGP repository. Moreover, Ludii is also competitive in terms of performance with the more recently proposed Regular Boardgames (RBG) system, and has various advantages in qualitative aspects such as generality. ",Ludii -- The Ludemic General Game System
96,1128201826283794432,1192577568,Daniel Worrall,"['**NEW WORK** My masters student @diacon995 and I learn how to (group) convolve in our new paper ""Learning to Convolve"": <LINK> accepted to @icmlconf. Simple idea with good results. TAKE HOME: Learned group equivariant weight basis &gt; standard pixel basis <LINK>']",https://arxiv.org/abs/1905.04663,"Recent work (Cohen & Welling, 2016) has shown that generalizations of convolutions, based on group theory, provide powerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flipped, etc. However, coming up with exact models of how to rotate a 3 x 3 filter on a square pixel-grid is difficult. In this paper, we learn how to transform filters for use in the group convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can produce feature maps with low sensitivity to input rotations, while achieving high performance on MNIST and CIFAR-10. ",Learning to Convolve: A Generalized Weight-Tying Approach
97,1128154289812267008,1561095932,SanghyukChun,"['Our new paper CutMix is now available in arXiv <LINK>\nwe achieved 21.60 top-1 error with CutMix augmented ResNet where ResNet-152 baseline is 21.69. Moreover, CutMix trained model enhances the performance of Detectors and Image Captioning! <LINK>']",https://arxiv.org/abs/1905.04899,"Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at this https URL . ","CutMix: Regularization Strategy to Train Strong Classifiers with
  Localizable Features"
98,1128099481587601408,2875482557,Dr Laura McKemmish,"['New @Exomol TiO line list, Toto, is now available on exomol website <LINK> with paper on arxiv <LINK> <LINK>', 'My favourite image from the paper... we get the high resolution spectra for TiO correct in the same region as the 2015 paper from @HoeijmakersJens found errors in previous line lists. https://t.co/R01ivrtZOb', 'So many electronic states in TiO... https://t.co/Qjncxi0pwU', 'Getting the high accuracy data correct really relied on our previous MARVEL analysis (#openaccess) https://t.co/93Eoza7ITz now also available on the new MARVEL website here https://t.co/KkwQEPYfED', '#compchem ists: open problem of high importance to Exoplanet astronomers -- get TiO electronic surfaces, transition dipole moments and spin orbit couplings quantiatively and qualitatively correct for high electronic states!', 'Story of how Toto go its name: @TomRivlin sang ""Old MacDonald had a farm: Ti -- Ti -- O"" one too many times. TiTiO --&gt; Tito. I thought this was dog in Wizards of Oz, but this was Toto and I\'m an Aussie, so... @exomol https://t.co/H5r4r4Y8bj']",https://arxiv.org/abs/1905.04587,"Accurate line lists are crucial for correctly modelling a variety of astrophysical phenomena, including stellar photospheres and the atmospheres of extra-solar planets. This paper presents a new line database Toto for the main isotopologues of titanium oxide (TiO): $^{46}$Ti$^{16}$O, $^{47}$Ti$^{16}$O, $^{48}$Ti$^{16}$O, $^{49}$Ti$^{16}$O and $^{50}$Ti$^{16}$O. The TiO line list contains transitions with wave-numbers up to 30,000 cm$^{-1}$ ie long-wards of 0.33 $\mu$m. The Toto line list includes all dipole-allowed transitions between 13 low-lying electronic states (X $^3\Delta$, a $^1\Delta$, d $^1\Sigma^+$, E $^3\Pi$, A $^3\Phi$ B $^3\Pi$, C $^3\Delta$, b $^1\Pi$, c $^1\Phi$, f $^1\Delta$, e $^1\Sigma^+$). Ab initio potential energy curves (PECs) are computed at the icMRCI level and combined with spin-orbit and other coupling curves. These PECs and couplings are iteratively refined to match known empirical energy levels. Accurate line intensities are generated using ab initio dipole moment curves. The Toto line lists are appropriate for temperatures below 5000 K and contain 30 million transitions for TiO; it is made available in electronic form via the CDS data centre and via www.exomol.com. Tests of the line lists show greatly improved agreement with observed spectra for objects such as M-dwarfs GJ876 and GL581. ",ExoMol Molecular linelists -- XXXIII. The spectrum of Titanium Oxide
99,1127853439285956610,3108542843,François-Xavier Briol,"['Very happy about our new extension of the ""Stein Point"" algorithms to  approximate complex posterior distributions:  <LINK>. This new paper greatly extends the  scalability of the Stein Points algorithms by using local Markov chain  for efficient exploration. <LINK>', 'To appear @icmlconf - and thanks to @turinginst for support! @MarkGirolami', ""@HSalimbeni @icmlconf @turinginst @MarkGirolami Might be because I'm tweeting pictures of 'posteriors'!""]",https://arxiv.org/abs/1905.03673,"An important task in machine learning and statistics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially minimising a Stein discrepancy between the empirical measure and the target and, hence, require the solution of a non-convex optimisation problem to obtain each new point. This paper removes the need to solve this optimisation problem by, instead, selecting each new point based on a Markov chain sample path. This significantly reduces the computational cost of Stein Points and leads to a suite of algorithms that are straightforward to implement. The new algorithms are illustrated on a set of challenging Bayesian inference problems, and rigorous theoretical guarantees of consistency are established. ",Stein Point Markov Chain Monte Carlo
100,1127644850915106817,957733543408537602,Alexander Rakhlin 🐩,['Glad to announce my new paper on #breastcancer Cellularity Assessment using #deeplearning! ArXiv preprint <LINK> Joint work with @tiulpin @alxndrkalinin @shvetsiya @viglovikov @snikolenko \n#breastcancer #deep_learning #AI #machine_learning <LINK>'],https://arxiv.org/abs/1905.01743,"Breast cancer is one of the main causes of death worldwide. Histopathological cellularity assessment of residual tumors in post-surgical tissues is used to analyze a tumor's response to a therapy. Correct cellularity assessment increases the chances of getting an appropriate treatment and facilitates the patient's survival. In current clinical practice, tumor cellularity is manually estimated by pathologists; this process is tedious and prone to errors or low agreement rates between assessors. In this work, we evaluated three strong novel Deep Learning-based approaches for automatic assessment of tumor cellularity from post-treated breast surgical specimens stained with hematoxylin and eosin. We validated the proposed methods on the BreastPathQ SPIE challenge dataset that consisted of 2395 image patches selected from whole slide images acquired from 64 patients. Compared to expert pathologist scoring, our best performing method yielded the Cohen's kappa coefficient of 0.70 (vs. 0.42 previously known in literature) and the intra-class correlation coefficient of 0.89 (vs. 0.83). Our results suggest that Deep Learning-based methods have a significant potential to alleviate the burden on pathologists, enhance the diagnostic workflow, and, thereby, facilitate better clinical outcomes in breast cancer treatment. ",Breast Tumor Cellularity Assessment using Deep Neural Networks
101,1127299092445421568,348637346,Diana Powell,"['Can we measure total protoplanetary disk mass without assuming a tracer-to-H2 ratio? Check out our new paper on the locations of planet formation where we derive total gaseous surface densities for 7 protoplanetary disks using dust lines! <LINK>', 'The disks in our sample have newly derived masses that are 9-27% of their host stellar mass, substantially larger than the minimum mass solar nebula! All are stable to gravitational collapse except for one which approaches the limit of Toomre-Q stability.', 'These masses are determined independent of an assumed dust opacity!', 'Check out how the new total surface densities compared to previously derived values and the minimum mass solar nebula! These are massive disks! https://t.co/d4PhLzwPsA', 'Our mass estimates are 2-15 times larger than estimates from integrated optically thin dust emission.  In these models, the disks formed with an initial dust mass that is a factor of ∼10 greater than is presently observed. More dust mass at early times!', 'Of the three disks in our sample with resolved CO line emission, the masses of HD 163296, AS 209, and TW Hya are roughly 3, 115, and 40 times more massive than estimates from CO respectively. More evidence that the CO story in disks is complicated!', 'Our method of determining surface density using dust lines is robust even if particles form as aggregates and is useful even in the presence of dust substructure caused by pressure traps. https://t.co/gmVGS7NNPx', 'The low Toomre-Q values observed in this sample indicate that at least some disks do not accrete efficiently. https://t.co/P9Vf2aj4jX']",https://arxiv.org/abs/1905.03252,"We present new determinations of disk surface density, independent of an assumed dust opacity, for a sample of 7 bright, diverse protoplanetary disks using measurements of disk dust lines. We develop a robust method for determining the location of dust lines by modeling disk interferometric visibilities at multiple wavelengths. The disks in our sample have newly derived masses that are 9-27% of their host stellar mass, substantially larger than the minimum mass solar nebula. All are stable to gravitational collapse except for one which approaches the limit of Toomre-Q stability. Our mass estimates are 2-15 times larger than estimates from integrated optically thin dust emission. We derive depleted dust-to-gas ratios with typical values of ~$10^{-3}$ in the outer disk. Using coagulation models we derive dust surface density profiles that are consistent with millimeter dust observations. In these models, the disks formed with an initial dust mass that is a factor of ~10 greater than is presently observed. Of the three disks in our sample with resolved CO line emission, the masses of HD 163296, AS 209, and TW Hya are roughly 3, 115, and 40 times more massive than estimates from CO respectively. This range indicates that CO depletion is not uniform across different disks and that dust is a more robust tracer of total disk mass. Our method of determining surface density using dust lines is robust even if particles form as aggregates and is useful even in the presence of dust substructure caused by pressure traps. The low Toomre-Q values observed in this sample indicate that at least some disks do not accrete efficiently. ","New Constraints From Dust Lines On The Surface Densities Of
  Protoplanetary Disks"
102,1126858105986248704,890132360397803520,Christoph Salge,"['New paper out on Arxiv: Intrinsically Motivated Autonomy in Human-Robot Interaction: Human Perception of Predictive Information in Robots\n\n<LINK>\n\n#robot #intrinsicmotivation #autonomy\n /w @mmscheunemann and Kerstin Dautenhahn at @UniofHerts <LINK>', ""This is the first paper in a series of studies. Our hope is that having meaningful, self-owned goals will increase the perception of robots as agent's, and affect their perceived warmth, competence and animacy, ultimately leading to more interesting and sustainable interactions. https://t.co/ln53wlbKdZ"", 'While the results in this paper are not all we hoped for, we learned a lot, and think that this approach might be one option for a good first step towards #social #intrinsic #motivation and more #believable #AI.', ""The intrinsic motivation used on our little BB8 robot is based on @GMartius Georg Martius' Predictive Information work - and if you want to see some other cool robot behavior you can check out Playful Machines:\n\nhttps://t.co/AWdMgrwvWF"", 'The robot used is a reprogrammed #BB8 from Sphero: \n\n@Sphero https://t.co/a7isvXDAE9 https://t.co/tKTHHL0cqa', 'This work will be presented at the 2019 TAROS in London: \nTowards Autonomous Robotic Systems Conference\n\nhttps://t.co/4oV9w6ndms']",https://arxiv.org/abs/1905.01734,"In this paper we present a fully autonomous and intrinsically motivated robot usable for HRI experiments. We argue that an intrinsically motivated approach based on the Predictive Information formalism, like the one presented here, could provide us with a pathway towards autonomous robot behaviour generation, that is capable of producing behaviour interesting enough for sustaining the interaction with humans and without the need for a human operator in the loop. We present a possible reactive baseline behaviour for comparison for future research. Participants perceive the baseline and the adaptive, intrinsically motivated behaviour differently. In our exploratory study we see evidence that participants perceive an intrinsically motivated robot as less intelligent than the reactive baseline behaviour. We argue that is mostly due to the high adaptation rate chosen and the design of the environment. However, we also see that the adaptive robot is perceived as more warm, a factor which carries more weight in interpersonal interaction than competence. ","Intrinsically Motivated Autonomy in Human-Robot Interaction: Human
  Perception of Predictive Information in Robots"
103,1126709412393971712,47059480,Daniela Huppenkothen,"['How can you select cohorts, like e.g. participants for a workshop, fairly and transparently? Can #datascience help? \n\nIn our new paper on #Entrofy, we suggest possible solutions to those questions. Now on an arxiv near you: <LINK>', 'This was a super fun collaboration with my (former) office mate @functiontelechy  and (former) colleague at @nyucds @digitalFlaneuse, and also an @AstroHackWeek project!', 'During the organization of @AstroHackWeek 2015, I was lamenting that I didn’t know how to do a good job of selecting participants in a fair and equitable way, without my own cognitive biases getting in the way.', 'Cue @functiontelechy, who started muttering about submodular functions and promptly wrote down some maths on the board. And thus #Entrofy was born!\n\nhttps://t.co/ZCrsJtx6ww', 'As an astronomer who studies black holes, I know approximately nothing about, you know, people, so thankfully @digitalflaneuse agreed to work with us and provided some much-needed sociology background.', 'This was possibly the most difficult paper I ever wrote, and took nearly four years to finish. \n\nBut it was also super interesting, and I am honoured that I got to work with such talented and insightful researchers. I may be formally first author, but they deserve equal credit!', 'I’ve worked on this and thought about this for so long, it’s both exciting and a bit scary to see it out in the world! \n\nI very much acknowledge that this is a difficult and complex topic, which is to say: comments, suggestions and especially criticism are welcome!', 'Finally, I also want to acknowledge @NYUDataScience, \n@uwescience and @uwdirac, who have given me an intellectual home for the past five years. \n\nThank you, @MooreFound and @SloanFoundation for supporting us in building spaces where interdisciplinary work can flourish.', '@astrocrash Oooh, cool! I think @functiontelechy has been thinking about using it for some data as well!', '@ohunt That depends on whether you have criteria for selection tied to some kind of merit (e.g. rating abstracts for a conf). If you do, humans have to do that first, ideally based on well-defined rubrics.', '@ohunt If you have more meritorious candidates than available slots, you can use Entrofy to break ties, subject to other constraints (e.g. because you want a well-balanced conference programme). There is a *little* bit of randomness in the algorithm, but not very much.']",https://arxiv.org/abs/1905.03314,"Selecting a cohort from a set of candidates is a common task within and beyond academia. Admitting students, awarding grants, choosing speakers for a conference are situations where human biases may affect the make-up of the final cohort. We propose a new algorithm, Entrofy, designed to be part of a larger decision making strategy aimed at making cohort selection as just, quantitative, transparent, and accountable as possible. We suggest this algorithm be embedded in a two-step selection procedure. First, all application materials are stripped of markers of identity that could induce conscious or sub-conscious bias. During blind review, the committee selects all applicants, submissions, or other entities that meet their merit-based criteria. This often yields a cohort larger than the admissible number. In the second stage, the target cohort can be chosen from this meritorious pool via a new algorithm and software tool. Entrofy optimizes differences across an assignable set of categories selected by the human committee. Criteria could include gender, academic discipline, experience with certain technologies, or other quantifiable characteristics. The Entrofy algorithm yields the computational maximization of diversity by solving the tie-breaking problem with provable performance guarantees. We show how Entrofy selects cohorts according to pre-determined characteristics in simulated sets of applications and demonstrate its use in a case study. This cohort selection process allows human judgment to prevail when assessing merit, but assigns the assessment of diversity to a computational process less likely to be beset by human bias. Importantly, the stage at which diversity assessments occur is fully transparent and auditable with Entrofy. Splitting merit and diversity considerations into their own assessment stages makes it easier to explain why a given candidate was selected or rejected. ",Entrofy Your Cohort: A Data Science Approach to Candidate Selection
104,1126498500869525504,1004365363574902784,Kevin J. Kelly,"[""Full disclosure: I'm not a cosmologist! New paper (<LINK>) today with Nikita Blinov, @GordanKrnjaic, and Sam McDermott about strong neutrino self-interactions and the Hubble tension."", 'Some recent work (https://t.co/FKtwEk6QQR, among others) has found that, if neutrinos interact among themselves relatively strongly, the discrepancy among measurements of the Hubble parameter can be alleviated!', 'Interestingly, the interaction strength required is WAY stronger (a factor of 10,000,000,000) than the Standard Model interactions among neutrinos. Also, these interactions point to a new mediator with a mass between roughly a keV and 100 MeV.', 'Nikita, Gordan, Sam, and I wanted to see whether different cosmological and laboratory constraints on mediators and interactions could constrain this solution to the Hubble tension. The combination of Big Bang Nucleosynthesis (BBN) and rare decay constraints are very powerful.', ""If you assume that a new particle (we call it phi) couples to all neutrinos universally, you can't have strong interactions OR moderate interactions (SInu/MInu, respectively). Green/blue bands are those preferred by https://t.co/FKtwEk6QQR, and the grey/red constraints are ours. https://t.co/NTmChRVWiX"", 'One possible way out is if you allow the new mediator to couple ONLY to tau-flavor neutrinos. In that case, there is a small region of parameter space that is viable. Constructing a complete theory that predicts this type of interaction is hard, but could this be the solution? https://t.co/gHBTXEdMyS', 'Thanks again to Nikita, @GordanKrnjaic, and Sam for a great project!']",https://arxiv.org/abs/1905.02727,"Large, non-standard neutrino self-interactions have been shown to resolve the $\sim 4\sigma$ tension in Hubble constant measurements and a milder tension in the amplitude of matter fluctuations. We demonstrate that interactions of the necessary size imply the existence of a force-carrier with a large neutrino coupling ($> 10^{-4}$) and mass in the keV -- 100 MeV range. This mediator is subject to stringent cosmological and laboratory bounds, and we find that nearly all realizations of such a particle are excluded by existing data unless it carries spin 0 and couples almost exclusively to $\tau$-flavored neutrinos. Furthermore, we find that the light neutrinos must be Majorana, and that a UV-complete model requires a non-minimal mechanism to simultaneously generate neutrino masses and appreciable self-interactions. ","Constraining the Self-Interacting Neutrino Interpretation of the Hubble
  Tension"
105,1126423698804805633,24443979,Dan Stowell,"['New preprint from us! This one\'s an algorithmic graph-theory paper: ""Efficient On-line Computation of Visibility Graphs"" <LINK> led by Delia Fano Yela']",https://arxiv.org/abs/1905.03204,"A visibility algorithm maps time series into complex networks following a simple criterion. The resulting visibility graph has recently proven to be a powerful tool for time series analysis. However its straightforward computation is time-consuming and rigid, motivating the development of more efficient algorithms. Here we present a highly efficient method to compute visibility graphs with the further benefit of flexibility: on-line computation. We propose an encoder/decoder approach, with an on-line adjustable binary search tree codec for time series as well as its corresponding decoder for visibility graphs. The empirical evidence suggests the proposed method for computation of visibility graphs offers an on-line computation solution at no additional computation time cost. The source code is available online. ",Efficient On-line Computation of Visibility Graphs
106,1126322804629000192,1338201043,Koichi Hamaguchi,"['New paper!  論文出ました。\nDark Matter Heating vs. Rotochemical Heating in Old Neutron Stars\nKoichi Hamaguchi, Natsumi Nagata, Keisuke Yanagi\n<LINK>\n\n暗黒物質と中性子星に関する論文。\n主要貢献者は永田くんと柳くん@yana_phys です。セミナー依頼歓迎！', 'あらすじ: \n暗黒物質の密度と典型的速度を考えると、中性子星に結構当たっていることが分かる。\n-&gt; WIMP暗黒物質なら中性子星に当たったり中で対消滅したりして表面温度を暖めるので、古い中性子星の表面温度を見れば暗黒物質の兆候が見えるのでは？(ここまで先行研究)', '-&gt; でも実は中性子星には自らを暖める機構が内在していて、それを支持する古くて暖かい中性子星の観測もあるぞ (ここ素粒子サイドにはほぼ知られてなかった)。\n-&gt; 自己加熱機構まで加味して考えたとき、中性子星の温度観測で暗黒物質の兆候は見えるのか？見えないのか？見えるための条件は？を調べた。', 'これで中性子星関連は3つ目。共同研究者の皆さんがすごい人達でモリモリ進んでます。まだまだ続く予定！']",https://arxiv.org/abs/1905.02991,"Dark matter (DM) particles in the Universe accumulate in neutron stars (NSs) through their interactions with ordinary matter. It has been known that their annihilation inside the NS core causes late-time heating, with which the surface temperature becomes a constant value of $T_s \simeq (2-3) \times 10^3$ K for the NS age $t \gtrsim 10^{6-7}$ years. This conclusion is, however, drawn based on the assumption that the beta equilibrium is maintained in NSs throughout their life, which turns out to be invalid for rotating pulsars. The slowdown in the pulsar rotation drives the NS matter out of beta equilibrium, and the resultant imbalance in chemical potentials induces late-time heating, dubbed as rotochemical heating. This effect can heat a NS up to $T_s \simeq 10^6$ K for $t \simeq 10^{6-7}$ years. In fact, recent observations found several old NSs whose surface temperature is much higher than the prediction of the standard cooling scenario and is consistent with the rotochemical heating. Motivated by these observations, in this letter, we reevaluate the significance of the DM heating in NSs, including the effect of the rotochemical heating. We then show that the signature of DM heating can still be detected in old ordinary pulsars, while it is concealed by the rotochemical heating for old millisecond pulsars. To confirm the evidence for the DM heating, however, it is necessary to improve our knowledge on nucleon pairing gaps as well as to evaluate the initial period of the pulsars accurately. In any cases, a discovery of a very cold NS can give a robust constraint on the DM heating, and thus on DM models. To demonstrate this, as an example, we also discuss the case that the DM is the neutral component of an electroweak multiplet, and show that an observation of a NS with $T_s \lesssim 10^3$ K imposes a stringent constraint on such a DM candidate. ",Dark Matter Heating vs. Rotochemical Heating in Old Neutron Stars
107,1126130018101284867,5850692,Aaron Roth,"[""We wrote a paper proposing a new relaxation of differential privacy that has lots of nice properties: <LINK> It's 85 pages long, so here is the TL;DR. Suppose S is a dataset with your data, and S' is the dataset with your data removed. 1/"", ""Differential privacy can be viewed as promising that any hypothesis test aiming to distinguish whether I used S vs. S' in my computation that has false positive rate alpha must have a true positive rate of at most e^eps*alpha+delta. Cool - its easy to interpret this guarantee! 2/"", 'This characterization is exact: its equivalent to the usual definition of DP. But DP is mis-parameterized in that when we compose mechanisms, there is no way to describe the resulting tradeoff between Type I and type II errors with any parameters eps,delta anymore. 3/', 'This is fundamentally the reason why composition theorems for DP are not tight. Even ""optimal"" composition (which is #P hard) is only providing a bound on the correct tradeoff between Type I and Type II error. (The tightest bound parameterized by eps,delta, but still loose)  4/', 'So: We propose describing privacy guarantees with a function f instead of 2 parameters. The function describes the optimal tradeoff between Type I and Type II error. This is expressive enough to exactly capture composition, and admits a simple calculus for composition. 5/', 'It turns out this way of describing a privacy guarantee is ""dual"" to describing it with an infinite collection of (eps,delta)-DP guarantees. You can switch back and forth via the convex conjugate of the function f. This is useful for a couple of reasons. Most notably, 6/', 'It provides a way to import known results from the DP literature into this new framework. This is how we get ""privacy amplification by sub-sampling"" for this new family of definitions --- something that has proven challenging for other proposed relaxations of DP. 7/', 'Isn\'t keeping track of functions complex? Yes, but there is a ""central limit theorem"". In the limit under composition, no matter what your functions f looked like, the privacy guarantee converges to the tradeoff function for testing two standard Gaussians  with shifted means. 8/', 'This family of functions has only one parameter (the gap in the means), and has a simple additive composition rule. We call it ""Gaussian Differential Privacy"". The CLT means that it is the -only- hypothesis testing based definition of privacy tightly closed under composition. 9/', 'It also provides an analytic tool. Too hard to reason about the composition of many functions? Compute the Gaussian-DP parameter using our central limit theorem instead. Convergence is fast: after 10 compositions, its hard to distinguish the CLT bound from the true bound. 10/', ""I think its pretty neat. I can say that, because all credit for this work goes to Jinshuo Dong. If you are at the Simons workshop today, come and ask him questions about it. He's speaking at 11:30. 11/11""]",https://arxiv.org/abs/1905.02383,"Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade. This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling. Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term `$f$-differential privacy' ($f$-DP). This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations. First, $f$-DP preserves the hypothesis testing interpretation. In addition, $f$-DP allows for lossless reasoning about composition in an algebraic fashion. Moreover, we provide a powerful technique to import existing results proven for original DP to $f$-DP and, as an application, obtain a simple subsampling theorem for $f$-DP. In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the $f$-DP class that is referred to as `Gaussian differential privacy' (GDP), defined based on testing two shifted Gaussians. GDP is focal among the $f$-DP class because of a central limit theorem we prove. More precisely, the privacy guarantees of \emph{any} hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition. The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms. Taken together, this collection of attractive properties render $f$-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis. Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent. ",Gaussian Differential Privacy
108,1126003421629128704,972018966490005504,Stefan Oslowski,"[""Here's a nice new paper by the UTMOST team, led by Wael: <LINK> \nThe highlights are an indication of a turn-over in rates somewhere around 1 GHz and  triple structure for FRB 181017. <LINK>""]",https://arxiv.org/abs/1905.02293,"We detail a new fast radio burst (FRB) survey with the Molonglo Radio Telescope, in which six FRBs were detected between June 2017 and December 2018. By using a real-time FRB detection system, we captured raw voltages for five of the six events, which allowed for coherent dedispersion and very high time resolution (10.24 $\mu$s) studies of the bursts. Five of the FRBs show temporal broadening consistent with interstellar and/or intergalactic scattering, with scattering timescales ranging from 0.16 to 29.1 ms. One burst, FRB181017, shows remarkable temporal structure, with 3 peaks each separated by 1 ms. We searched for phase-coherence between the leading and trailing peaks and found none, ruling out lensing scenarios. Based on this survey, we calculate an all-sky rate at 843 MHz of $98^{+59}_{-39}$ events sky$^{-1}$ day$^{-1}$ to a fluence limit of 8 Jy-ms: a factor of 7 below the rates estimated from the Parkes and ASKAP telescopes at 1.4 GHz assuming the ASKAP-derived spectral index $\alpha=-1.6$ ($F_{\nu}\propto\nu^{\alpha}$). Our results suggest that FRB spectra may turn over below 1 GHz. Optical, radio and X-ray followup has been made for most of the reported bursts, with no associated transients found. No repeat bursts were found in the survey. ",Five new real-time detections of Fast Radio Bursts with UTMOST
109,1125996671664451584,147951210,David Berthelot,"['New paper: MixMatch: A Holistic Approach to Semi-Supervised Learning <LINK>\nReduces error rate by up to 4x on CIFAR10.\n\nWith Nicholas Carlini @goodfellow_ian @NicolasPapernot @avitaloliver @colinraffel <LINK>', '@goodfellow_ian @NicolasPapernot @avitaloliver @colinraffel General idea: Guess labels for unlabeled data and use MixUp with labeled data and guessed labels for unlabeled data. https://t.co/TSZVmjC0Pk', '@goodfellow_ian @NicolasPapernot @avitaloliver @colinraffel Labeling data is generally expensive, we compared the effect of adding labeled data vs unlabeled data for SVHN. https://t.co/amZ8xjYlgI']",http://arxiv.org/abs/1905.02249,"Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success. ",MixMatch: A Holistic Approach to Semi-Supervised Learning
110,1125976435456335872,38385087,Adrian Lucy,"['New paper out, submitted to MNRAS today! Regulation of accretion by its outflow in a symbiotic star: the 2016 outflow fast state of MWC 560 <LINK>']",https://arxiv.org/abs/1905.02399,"How are accretion discs affected by their outflows? To address this question for white dwarfs accreting from cool giants, we performed optical, radio, X-ray, and ultraviolet observations of the outflow-driving symbiotic star MWC 560 (=V694 Mon) during its 2016 optical high state. We tracked multi-wavelength changes that signalled an abrupt increase in outflow power at the initiation of a months-long outflow fast state, just as the optical flux peaked: (1) an abrupt doubling of Balmer absorption velocities; (2) the onset of a $20$ $\mu$Jy/month increase in radio flux; and (3) an order-of-magnitude increase in soft X-ray flux. Juxtaposing to prior X-ray observations and their coeval optical spectra, we infer that both high-velocity and low-velocity optical outflow components must be simultaneously present to yield a large soft X-ray flux, which may originate in shocks where these fast and slow absorbers collide. Our optical and ultraviolet spectra indicate that the broad absorption-line gas was fast, stable, and dense ($\gtrsim10^{6.5}$ cm$^{-3}$) throughout the 2016 outflow fast state, steadily feeding a lower-density ($\lesssim10^{5.5}$ cm$^{-3}$) region of radio-emitting gas. Persistent optical and ultraviolet flickering indicate that the accretion disc remained intact. The stability of these properties in 2016 contrasts to their instability during MWC 560's 1990 outburst, even though the disc reached a similar accretion rate. We propose that the self-regulatory effect of a steady fast outflow from the disc in 2016 prevented a catastrophic ejection of the inner disc. This behaviour in a symbiotic binary resembles disc/outflow relationships governing accretion state changes in X-ray binaries. ","Regulation of accretion by its outflow in a symbiotic star: the 2016
  outflow fast state of MWC 560"
111,1125925969628241923,1053730346930384897,Jeff Carlin,"[""New MADCASH (Magellanic Analogs Dwarf Companions and Stellar Halos) paper by awesome U. Arizona grad student @Ragadeepika on extended stellar populations around IC 1613 from exquisite Subaru/HSC data. I'll let her summarize it, but check it out: <LINK>"", '@rareflwr41 @adrianprw @Ragadeepika Yes - keep in touch!']",https://arxiv.org/abs/1905.02210,"Stellar halos offer fossil evidence for hierarchical structure formation. Since halo assembly is predicted to be scale-free, stellar halos around low-mass galaxies constrain properties such as star formation in the accreted subhalos and the formation of dwarf galaxies. However, few observational searches for stellar halos in dwarfs exist. Here we present gi photometry of resolved stars in isolated Local Group dwarf irregular galaxy IC 1613 ($M_{\star} \sim 10^8 M_{\odot})$. These Subaru/Hyper Suprime-Cam observations are the widest and deepest of IC 1613 to date. We measure surface density profiles of young main-sequence, intermediate to old red giant branch, and ancient horizontal branch stars outside of 12' ($\sim 2.6$ kpc; 2.5 half-light radii) from the IC 1613 center. All of the populations extend to ~24' ($\sim 5.2$ kpc; 5 half-light radii), with the older populations best fit by a broken exponential in these outer regions. Comparison with earlier studies sensitive to IC 1613's inner regions shows that the density of old stellar populations steepens substantially with distance from the center; we trace the $g$-band effective surface brightness to an extremely faint limit of $\sim 33.7$ mag arcsec$^{-2}$. Conversely, the distribution of younger stars follows a single, shallow exponential profile in the outer regions, demonstrating different formation channels for the younger and older components of IC 1613. The outermost, intermediate-age and old stars have properties consistent with those expected for accreted stellar halos, though future observational and theoretical work is needed to definitively distinguish this scenario from other possibilities. ","Hyper Wide Field Imaging of the Local Group Dwarf Irregular Galaxy IC
  1613: An Extended Component of Metal-poor Stars"
112,1125923859813761024,6222842,Nick Feamster,"[""New paper draft (in submission) with @jlivingood discussing the history and future of speed testing. <LINK> Here's hoping that future decisions and policy are based on more accurate measures of Internet speed that we are currently witnessing.""]",https://arxiv.org/abs/1905.02334,"Government organizations, regulators, consumers, Internet service providers, and application providers alike all have an interest in measuring user Internet ""speed"". Access speeds have increased by an order of magnitude in past years, with gigabit speeds available to tens of millions of homes. Approaches must evolve to accurately reflect the changing user experience and network speeds. This paper offers historical and technical background on current speed testing methods, highlights their limitations as access network speeds continue to increase, and offers recommendations for the next generation of Internet ""speed"" measurement. ","Internet Speed Measurement: Current Challenges and Future
  Recommendations"
113,1125800750699823104,875456843279081476,Dileep George,"['Thread about our new paper on learning the structure of variable order sequences. Imposing a biologically-inspired sparsity structure alleviates the credit diffusion problem in HMMs! Also relevant for neuroscience...  (1) @vicariousai <LINK> <LINK>', ""The core idea goes back to a compression algo called Dynamic Markov Coding. Instead of forming bigram or trigram states for higher-order Markov chains, represent higher-order info sparsely by 'splitting' or  'cloning' some states. (2) https://t.co/5JyDCMh2zq https://t.co/Tr3Tw173IT"", 'Splitting states is also proposed as the mechanism for sequence learning in song birds. https://t.co/Jcfxf1WW8J. Also, suggested in my earlier work on seq learning. https://t.co/cEM4hMhfs8 (3) https://t.co/w3LXucfKF9', '@Numenta uses this cloning idea in their sequence learning models as well. https://t.co/7qgGsLD7Vr (4)', 'Interestingly, this idea was also recently re-discovered in network science, ""Representing higher-order dependencies networks"" without connecting the dots to earlier works in sequence representation...\nhttps://t.co/K6mxTEme4W  (5) https://t.co/i3PdlwPiWO', 'Rather than splitting/cloning a first order model gradually, we can impose the cloning structure on the emission matrix of an HMM, and let the learning algorithm figure out the transition between clones. This is the idea we pursue in this paper.  (6) https://t.co/4SF9VulaDr', 'It is known that overcomplete HMMs with many more hidden states than emission states can represent higher-order sequences, but learning is challenging without a sparsity structure because of credit diffusion. https://t.co/6WScZ8eq0o    (7) https://t.co/z1csVft3py', ""We found that imposing the 'cloning' sparisty on the emission matrix alleviates this credit diffusion problem, and the cloned HMM (CHMM) can now learn higher-order sequences with an online EM algorithm. You can even overfit with larger models (more clones) https://t.co/Sa6ctSKmX5"", 'CHMMs were able to beat ngrams, sequence memoizers, and even LSTMs on some character-level language modeling tasks. (9) https://t.co/1POrdqciPP', 'One benefit of CHMMs compared to RNNs is that CHMMs discover a directed graph underlying the sequence generation process. If you partition the graph of a char-CHMM, you find words as subgraphs. (10) https://t.co/DM3UEI7RZK', 'Another advantage is that CHMM is a generative model, and can handle uncertainty. Decoding uncertain inputs is a standard MAP query. (11) https://t.co/FJG5LtWUuw', 'These advantages may make CHMMs applicable in many sequence modeling tasks. Of course, many caveats apply. (12) https://t.co/TVCzX8pBjn', 'We think CHMMs are also applicable to sequence learning in other parts of the brain (eg, hippocampus). We hope to have more to say on this in the future. (Fin).']",https://arxiv.org/abs/1905.00507,"Variable order sequence modeling is an important problem in artificial and natural intelligence. While overcomplete Hidden Markov Models (HMMs), in theory, have the capacity to represent long-term temporal structure, they often fail to learn and converge to local minima. We show that by constraining HMMs with a simple sparsity structure inspired by biology, we can make it learn variable order sequences efficiently. We call this model cloned HMM (CHMM) because the sparsity structure enforces that many hidden states map deterministically to the same emission state. CHMMs with over 1 billion parameters can be efficiently trained on GPUs without being severely affected by the credit diffusion problem of standard HMMs. Unlike n-grams and sequence memoizers, CHMMs can model temporal dependencies at arbitrarily long distances and recognize contexts with 'holes' in them. Compared to Recurrent Neural Networks and their Long Short-Term Memory extensions (LSTMs), CHMMs are generative models that can natively deal with uncertainty. Moreover, CHMMs return a higher-order graph that represents the temporal structure of the data which can be useful for community detection, and for building hierarchical models. Our experiments show that CHMMs can beat n-grams, sequence memoizers, and LSTMs on character-level language modeling tasks. CHMMs can be a viable alternative to these methods in some tasks that require variable order sequence modeling and the handling of uncertainty. ",Learning higher-order sequential structure with cloned HMMs
114,1125736988668305409,17373048,Rodrigo Nemmen,"[""New paper today on @arxiv: The Spin of M87*. Now that we have seen the image of the supermassive black hole in M87 (that's M87*) with @ehtelescope and measured its mass, its time to estimate how fast it rotates <LINK> <LINK>"", 'The spin is the second fundamental number in the Kerr spacetime. Here I report observational constraints on the spin of M87* and find that it has a moderate spin at minimum: |a*|&gt;0.4. In other words, the equator of the BH is rotating at 40% of the speed of light at the very least https://t.co/Gq3zzkR98z', 'I also constrain how strong the magnetic flux is near the event horizon, and its not small. This result disfavours many accretion flow models that have been under consideration (""SANE"" models)', 'To make these observational estimates, I considered the amount of energy flowing into the BH vs the energy flowing out of it in the relativistic jet. If you know these 2 numbers, the physics of general relativity (Kerr metric) tells you the BH rotational energy: the spin a*', 'The full story is in this plot, which relates the magnetic flux and black hole spin for two cases: BH and disk spinning in the same (prograde) or opposite (retrograde) senses. https://t.co/WdAQqLVpAe']",https://arxiv.org/abs/1905.02143,"Now that the mass of the central black hole in the galaxy M87 has been measured with great precision using different methods, the remaining parameter of the Kerr metric that needs to be estimated is the spin a*. We have modeled measurements of the average power of the relativistic jet and an upper limit to the mass accretion rate onto the black hole with general relativistic magnetohydrodynamic models of jet formation. This allows us to derive constraints on a* and the black hole magnetic flux phi. We find a lower limit on M87*'s spin and magnetic flux of a* > 0.4 and phi > 6 in the prograde case, and a* > 0.5 and phi > 10 in the retrograde case, otherwise the black hole is not able to provide enough energy to power the observed jet. These results indicate that M87* has a moderate spin at minimum and disfavor a variety of models typified by low values of phi known as ``SANE'', indicating that M87* prefers the magnetically arrested disk state. We discuss how different estimates of the jet power and accretion rate can impact a* and phi. ",The Spin of M87*
115,1125712406980759552,91758004,José Ignacio Latorre,"['A new paper on quantum algorithms. By training output coincidence, a Schmidt decomposition emerges. It follows that entropies can be estimated. <LINK>']",https://arxiv.org/abs/1905.01353,"We present a variational quantum circuit that produces the Singular Value Decomposition of a bipartite pure state. The proposed circuit, that we name Quantum Singular Value Decomposer or QSVD, is made of two unitaries respectively acting on each part of the system. The key idea of the algorithm is to train this circuit so that the final state displays exact output coincidence from both subsystems for every measurement in the computational basis. Such circuit preserves entanglement between the parties and acts as a diagonalizer that delivers the eigenvalues of the Schmidt decomposition. Our algorithm only requires measurements in one single setting, in striking contrast to the $3^n$ settings required by state tomography. Furthermore, the adjoints of the unitaries making the circuit are used to create the eigenvectors of the decomposition up to a global phase. Some further applications of QSVD are readily obtained. The proposed QSVD circuit allows to construct a SWAP between the two parties of the system without the need of any quantum gate communicating them. We also show that a circuit made with QSVD and CNOTs acts as an encoder of information of the original state onto one of its parties. This idea can be reversed and used to create random states with a precise entanglement structure. ",Quantum Singular Value Decomposer
116,1125657425456041985,526115229,Kevin Heng,"['Very proud of my postdoc Jens Hoeijmakers, who followed up his Nature paper on discovering iron and titanium in KELT-9b with this new A&amp;A study. First discovery of chromium, scandium and yttrium in an exoplanetary atmosphere at high spectral resolution. <LINK>', 'It is necessary to point out massive contributions from two of my other (senior) postdocs: Simon Grimm basically converted the entire Kurucz database of atomic line lists into opacities; Daniel Kitzmann converted these opacities into transmission spectra (with variable gravity).']",https://arxiv.org/abs/1905.02096,"Context: KELT-9 b exemplifies a newly emerging class of short-period gaseous exoplanets that tend to orbit hot, early type stars - termed ultra-hot Jupiters. The severe stellar irradiation heats their atmospheres to temperatures of $\sim 4,000$ K, similar to the photospheres of dwarf stars. Due to the absence of aerosols and complex molecular chemistry at such temperatures, these planets offer the potential of detailed chemical characterisation through transit and day-side spectroscopy. Studies of their chemical inventories may provide crucial constraints on their formation process and evolution history. Aims: To search the optical transmission spectrum of KELT-9 b for absorption lines by metals using the cross-correlation technique. Methods: We analyse 2 transits observed with the HARPS-N spectrograph. We use an isothermal equilibrium chemistry model to predict the transmission spectrum for each of the neutral and singly-ionized atoms with atomic numbers between 3 and 78. Of these, we identify the elements that are expected to have spectral lines in the visible wavelength range and use those as cross-correlation templates. Results: We detect absorption of Na I, Cr II, Sc II and Y II, and confirm previous detections of Mg I, Fe I, Fe II and Ti II. In addition, we find evidence of Ca I, Cr I, Co I, and Sr II that will require further observations to verify. The detected absorption lines are significantly deeper than model predictions, suggesting that material is transported to higher altitudes where the density is enhanced compared to a hydrostatic profile. There appears to be no significant blue-shift of the absorption spectrum due to a net day-to-night side wind. In particular, the strong Fe II feature is shifted by $0.18 \pm 0.27$ km~s$^{-1}$, consistent with zero. Using the orbital velocity of the planet we revise the steller and planetary masses and radii. ","A spectral survey of an ultra-hot Jupiter: Detection of metals in the
  transmission spectrum of KELT-9 b"
117,1125464652291485697,1498729908,Nick Rhinehart,"['New work with @rowantmc, @kkitani, &amp; @svlevine on deep conditional forecasting with multiple interacting agents: when you control one of them, you can use its goals to better predict what nearby agents will do\n\nPaper: <LINK>\nVideos &amp; Data: <LINK> <LINK>']",https://arxiv.org/abs/1905.01296,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions. ",PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings
118,1125420970083328000,933084565895286786,Dan Hooper,"[""(1/5) I'm excited about a new paper that I've written with @GordanKrnjaic and Sam McDermott. It combines some of my favorite things: black holes, dark matter, and the Hubble tension.\n<LINK>\n#blackholes #cosmology #darkmatter #Hubble"", '(2/5) If even a small number of black holes were created around the time of  inflation, their relative abundance would grow as the universe expands, making it quite plausible that the energy density of the early universe was once dominated by black holes.', '(3/5) If these black holes were small, they would quickly Hawking evaporate. Unlike most other particle production mechanisms, this generates all kinds of particles, no matter how feebly interacting -- a promising way to generate dark matter and dark radiation.', '(4/5) If the early universe included a black hole dominated era, we find that Hawking radiation will produce dark radiation at a level ΔN_eff∼0.03−0.2 for each light and decoupled species. This range coulld relax the tension between late and early-time Hubble determinations.', '(5/5) The dark matter could also originate as Hawking radiation in a black hole dominated early universe, although such dark matter candidates must be very heavy (&gt;10^11 GeV).']",https://arxiv.org/abs/1905.01301,"If even a relatively small number of black holes were created in the early universe, they will constitute an increasingly large fraction of the total energy density as space expands. It is thus well-motivated to consider scenarios in which the early universe included an era in which primordial black holes dominated the total energy density. Within this context, we consider Hawking radiation as a mechanism to produce both dark radiation and dark matter. If the early universe included a black hole dominated era, we find that Hawking radiation will produce dark radiation at a level $\Delta N_{\rm eff} \sim 0.03-0.2$ for each light and decoupled species of spin 0, 1/2, or 1. This range is well suited to relax the tension between late and early-time Hubble determinations, and is within the reach of upcoming CMB experiments. The dark matter could also originate as Hawking radiation in a black hole dominated early universe, although such dark matter candidates must be very heavy ($m_{\rm DM} >10^{11}$ GeV) if they are to avoid exceeding the measured abundance. ",Dark Radiation and Superheavy Dark Matter from Black Hole Domination
119,1125329668670926848,822867138,Bradley Kavanagh,"['New paper out today with Adam Coogan, @gfbertone, @DanieleGaggero &amp; @C_Weniger exploring the connection between #BlackHoles formed in the early Universe and the #DarkMatter which collects around them: <LINK>\n\nWhat kills white walkers? <LINK>', 'Dragon glass.\n\nWhat kills vampires? https://t.co/DBZJo18Dms', 'Stake through the heart.\n\nWhat kills werewolves? https://t.co/WffKnA9Yam', 'Silver bullets.\n\nWhat kills almost all theories of New Physics which predict new weakly-interacting massive particles? https://t.co/vywwvEINVf', ""The existence of 'primordial' black holes, formed in the very early Universe.\n\nIf we could detect these black holes in gravitational waves or radio waves, we would know that they contribute some fraction (f_PBH) to the density of dark matter in the Universe. https://t.co/dtlXpMNrNM"", ""But we also expect these 'PBHs' to be surrounded by a dense halo of weakly-interacting massive particles (WIMPs, χ). These particles would annihilate and produce lots of gamma-rays. If we see PBHs but not gamma-rays, we could place strong constraints on the WIMP properties! https://t.co/cThc0ePQBv"", ""@PitifulRed Thanks! I'm glad you enjoyed it!""]",https://arxiv.org/abs/1905.01238,"Observational constraints on gamma rays produced by the annihilation of weakly interacting massive particles around primordial black holes (PBHs) imply that these two classes of Dark Matter candidates cannot coexist. We show here that the successful detection of one or more PBHs by radio searches (with the Square Kilometer Array) and gravitational waves searches (with LIGO/Virgo and the upcoming Einstein Telescope) would set extraordinarily stringent constraints on virtually all weak-scale extensions of the Standard Model with stable relics, including those predicting a WIMP abundance much smaller than that of Dark Matter. Upcoming PBHs searches have in particular the potential to rule out almost the entire parameter space of popular theories such as the minimal supersymmetric standard model and scalar singlet Dark Matter. ","Primordial Black Holes as Silver Bullets for New Physics at the Weak
  Scale"
120,1125303748367126528,487990723,Gianfranco Bertone,"['New paper on the arXiv today! Key result: discovering primordial #BlackHoles with @LIGO, @ego_virgo, Einstein Telescope, or @SKA_telescope, would rule out almost completely #supersymmetry and other theories predicting stable particles at the weak scale <LINK> <LINK>', 'Work done in collaboration with terrific @GRAPPAInstitute team: Adam Coogan, @DanieleGaggero, @BradleyKavanagh and @C_Weniger', 'Note that discovering primordial black holes would rule out weak-scale extensions of the standard model even in the case where the neutralino (or any other stable relic) contribute a negligible fraction of the #darkmatter in the universe', 'Adam, @DanieleGaggero and @BradleyKavanagh made analysis code available on @github and @ZENODO_ORG. Click on links in captions to get python code to generate them #OpenScience']",https://arxiv.org/abs/1905.01238,"Observational constraints on gamma rays produced by the annihilation of weakly interacting massive particles around primordial black holes (PBHs) imply that these two classes of Dark Matter candidates cannot coexist. We show here that the successful detection of one or more PBHs by radio searches (with the Square Kilometer Array) and gravitational waves searches (with LIGO/Virgo and the upcoming Einstein Telescope) would set extraordinarily stringent constraints on virtually all weak-scale extensions of the Standard Model with stable relics, including those predicting a WIMP abundance much smaller than that of Dark Matter. Upcoming PBHs searches have in particular the potential to rule out almost the entire parameter space of popular theories such as the minimal supersymmetric standard model and scalar singlet Dark Matter. ","Primordial Black Holes as Silver Bullets for New Physics at the Weak
  Scale"
121,1125287005611597825,4902145390,Gordan Krnjaic,['Cheers to my awesome collaborators @DanHooperAstro and Sam McDermott for our new paper on primordial black holes and their consequences for dark radiation &amp; dark matter <LINK>'],https://arxiv.org/abs/1905.01301,"If even a relatively small number of black holes were created in the early universe, they will constitute an increasingly large fraction of the total energy density as space expands. It is thus well-motivated to consider scenarios in which the early universe included an era in which primordial black holes dominated the total energy density. Within this context, we consider Hawking radiation as a mechanism to produce both dark radiation and dark matter. If the early universe included a black hole dominated era, we find that Hawking radiation will produce dark radiation at a level $\Delta N_{\rm eff} \sim 0.03-0.2$ for each light and decoupled species of spin 0, 1/2, or 1. This range is well suited to relax the tension between late and early-time Hubble determinations, and is within the reach of upcoming CMB experiments. The dark matter could also originate as Hawking radiation in a black hole dominated early universe, although such dark matter candidates must be very heavy ($m_{\rm DM} >10^{11}$ GeV) if they are to avoid exceeding the measured abundance. ",Dark Radiation and Superheavy Dark Matter from Black Hole Domination
122,1124197603284004864,947891260555169792,Antônio Horta Ribeiro,['New preprint now available on arXiv: <LINK>. The novel results presented in this paper shed light on non-convex optimization problems arising when estimating parameters of nonlinear models. #systemIdentification #machineLearning #optimization'],http://arxiv.org/abs/1905.00820,"We shed new light on the \textit{smoothness} of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and $\beta$-smoothness of the objective function might blow up exponentially with the simulation length, making it hard to numerically find minima within those regions or, even, to escape from them. In addition to providing theoretical understanding of this problem, this paper also proposes the use of multiple shooting as a viable solution. The proposed method minimizes the error between a prediction model and the observed values. Rather than running the prediction model over the entire dataset, multiple shooting splits the data into smaller subsets and runs the prediction model over each subset, making the simulation length a design parameter and making it possible to solve problems that would be infeasible using a standard approach. The equivalence to the original problem is obtained by including constraints in the optimization. The new method is illustrated by estimating the parameters of nonlinear systems with chaotic or unstable behavior, as well as neural networks. We also present a comparative analysis of the proposed method with multi-step-ahead prediction error minimization. ",On the smoothness of nonlinear system identification
123,1124133641733509120,616196664,Steven Touzard,"['Check out our new paper from @Yale_QI on how to implement gates that preserve the noise bias of stabilized Schrodinger cat states\n<LINK>', ""@matt_reagor @Yale_QI Yeah that's a great paper ! To be fair, these gates are science fiction for now, but very doable !\nThese engineered dissipation are really fun to play with, much less fun to write in my thesis manuscript right now.""]",https://arxiv.org/abs/1905.00450,"The code capacity threshold for error correction using qubits which exhibit asymmetric or biased noise channels is known to be much higher than with qubits without such structured noise. However, it is unclear how much this improvement persists when realistic circuit level noise is taken into account. This is because implementations of gates which do not commute with the dominant error un-bias the noise channel. In particular, a native bias-preserving controlled-NOT (CX) gate, which is an essential ingredient of stabilizer codes, is not possible in strictly two-level systems. Here we overcome the challenge of implementing a bias-preserving CX gate by using stabilized cat qubits in driven nonlinear oscillators. The physical noise channel of this qubit is biased towards phase-flips, which increase linearly with the size of the cat, while bit-flips are exponentially suppressed with cat size. Remarkably, the error channel of this native CX gate between two such cat qubits is also dominated by phase-flips, while bit-flips remain exponentially suppressed. This CX gate relies on the topological phase that arises from the rotation of the cat qubit in phase space. The availability of bias-preserving CX gates opens a path towards fault-tolerant codes tailored to biased-noise cat qubits with high threshold and low overhead. As an example, we analyze a scheme for concatenated error correction using cat qubits. We find that the availability of CX gates with moderately sized cat qubits, having mean photon number <10, improves a rigorous lower bound on the fault-tolerance threshold by a factor of two and decreases the overhead in logical Clifford operations by a factor of 5. We expect these estimates to improve significantly with further optimization and with direct use of other codes such as topological codes tailored to biased noise. ",Bias-preserving gates with stabilized cat qubits
124,1123888067234992129,1118781177450647552,Thomas Hird,"[""New paper on our latest quantum memories research (and very excitingly the first paper I'm on!) <LINK> <LINK>"", '@OptoLia Thanks Lia 😁  Hopefully you enjoy it?🤞😁']",https://arxiv.org/abs/1905.00042,"Quantum memories are essential for large-scale quantum information networks. Along with high efficiency, storage lifetime and optical bandwidth, it is critical that the memory add negligible noise to the recalled signal. A common source of noise in optical quantum memories is spontaneous four-wave mixing. We develop and implement a technically simple scheme to suppress this noise mechanism by means of quantum interference. Using this scheme with a Raman memory in warm atomic vapour we demonstrate over an order of magnitude improvement in noise performance. Furthermore we demonstrate a method to quantify the remaining noise contributions and present a route to enable further noise suppression. Our scheme opens the way to quantum demonstrations using a broadband memory, significantly advancing the search for scalable quantum photonic networks. ",Raman Quantum Memory with Built-In Suppression of Four-wave Mixing Noise
125,1123836321963888643,3368978577,James Kuszlewicz,"[""If you're interested in asteroseismology of red-giants, eclipsing binaries or heartbeats, take a look at our new paper out today on arXiv!! (@Thomas_S_North @astrokeat) <LINK>"", '@asteronomer @Thomas_S_North @astrokeat Cheers! Much appreciated! 😁']",https://arxiv.org/abs/1905.00040,"KOI-3890 is a highly eccentric, 153-day period eclipsing, single-lined spectroscopic binary system containing a red-giant star showing solar-like oscillations alongside tidal interactions. The combination of transit photometry, radial velocity observations, and asteroseismology have enabled the detailed characterisation of both the red-giant primary and the M-dwarf companion, along with the tidal interaction and the geometry of the system. The stellar parameters of the red-giant primary are determined through the use of asteroseismology and grid-based modelling to give a mass and radius of $M_{\star}=1.04\pm0.06\;\textrm{M}_{\odot}$ and $R_{\star}=5.8\pm0.2\;\textrm{R}_{\odot}$ respectively. When combined with transit photometry the M-dwarf companion is found to have a mass and radius of $M_{\mathrm{c}}=0.23\pm0.01\;\textrm{M}_{\odot}$ and $R_{\mathrm{c}}=0.256\pm0.007\;\textrm{R}_{\odot}$. Moreover, through asteroseismology we constrain the age of the system through the red-giant primary to be $9.1^{+2.4}_{-1.7}\;\mathrm{Gyr}$. This provides a constraint on the age of the M-dwarf secondary, which is difficult to do for other M-dwarf binary systems. In addition, the asteroseismic analysis yields an estimate of the inclination angle of the rotation axis of the red-giant star of $i=87.6^{+2.4}_{-1.2}$ degrees. The obliquity of the system\textemdash the angle between the stellar rotation axis and the angle normal to the orbital plane\textemdash is also derived to give $\psi=4.2^{+2.1}_{-4.2}$ degrees showing that the system is consistent with alignment. We observe no radius inflation in the M-dwarf companion when compared to current low-mass stellar models. ","KOI-3890: A high mass-ratio asteroseismic red-giant$+$M-dwarf eclipsing
  binary undergoing heartbeat tidal interactions"
126,1123754750292439041,3269585132,limmerlab,['New paper with Addison Schile on getting your quantum master equations to play nice with conical intersections:\n<LINK> <LINK>'],https://arxiv.org/abs/1905.00029,"We present a framework for simulating relaxation dynamics through a conical intersection of an open quantum system that combines methods to approximate the motion of degrees of freedom with disparate time and energy scales. In the vicinity of a conical intersection, a few degrees of freedom render the nuclear dynamics nonadiabatic with respect to the electronic degrees of freedom. We treat these strongly coupled modes by evolving their wavepacket dynamics in the absence of additional coupling exactly. The remaining weakly coupled nuclear degrees of freedom are partitioned into modes that are fast relative to the nonadiabatic coupling and those that are slow. The fast degrees of freedom can be traced out and treated with second-order perturbation theory in the form of the time-convolutionless master equation. The slow degrees of freedom are assumed to be frozen over the ultrafast relaxation, and treated as sources of static disorder. In this way, we adopt the recently developed frozen-mode extension to second-order quantum master equations. We benchmark this approach to numerically exact results in models of pyrazine internal conversion and rhodopsin photoisomerization. We use this framework to study the dependence of the quantum yield on the reorganization energy and the characteristic timescale of the bath, in a two-mode model of photoisomerization. We find that the yield is monotonically increasing with reorganization energy for a Markovian bath, but monotonically decreasing with reorganization energy for a non-Markovian bath. This reflects the subtle interplay between dissipation and decoherence in conical intersection dynamics in the condensed phase. ","Simulating conical intersection dynamics in the condensed phase with
  hybrid quantum master equations"
127,1137070470392496128,503452360,William Wang,"['The main issue with Knowledge Graph is coverage. @xwhan_ has a new paper ""Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader"", combining text reading comprehension and KG reading to improve QA. #NLProc #acl2019nlp #acl2019italy <LINK> <LINK>']",https://arxiv.org/abs/1905.07098,"We propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets. Under the assumptions that the structured KB is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge of entities from a question-related KB subgraph; then reformulates the question in the latent space and reads the texts with the accumulated entity knowledge at hand. The evidence from KB and texts are finally aggregated to predict answers. On the widely-used KBQA benchmark WebQSP, our model achieves consistent improvements across settings with different extents of KB incompleteness. ","Improving Question Answering over Incomplete KBs with Knowledge-Aware
  Reader"
128,1134428479263518720,804374251931402241,Tiancheng Zhao (Tony),['Looking for a goal for your chatbot? Checkout out our ACL paper on Target-guided Open Domain Conversation. A new task and dataset to create chatbots that guide open domain chatting towards specific target words. <LINK>  #acl2019italy #NLProc'],https://arxiv.org/abs/1905.11553,"Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches. ",Target-Guided Open-Domain Conversation
129,1134355441180241925,18850305,Zachary Lipton,"['Economic models of statistical discrimination usually treat *the signal* as atomic &amp; given. In a new theory paper w Lee Cohen &amp; Yishay Mansour, we extend to the setting where employers incrementally extract signal (by administering tests or interviews)\n<LINK> (1)', 'Focusing on a Bernoulli model, our work considers a new perspective on fairness—the balance between the number of tests administered, the decision criteria applied to the tests, &amp; various outcomes of interest. A natural impossibility theorem emerges\nhttps://t.co/OjgPra0UT9 (2)', 'We analyze optimal policies w fixed numbers of tests &amp; dynamically-allocated tests, wrt competing desiderata of minimizing false positives (hiring unskilled workers) and minimizing the cost of testing (number of tests administered per hired candidate) https://t.co/OjgPra0UT9 (3)']",https://arxiv.org/abs/1905.11361,"When recruiting job candidates, employers rarely observe their underlying skill level directly. Instead, they must administer a series of interviews and/or collate other noisy signals in order to estimate the worker's skill. Traditional economics papers address screening models where employers access worker skill via a single noisy signal. In this paper, we extend this theoretical analysis to a multi-test setting, considering both Bernoulli and Gaussian models. We analyze the optimal employer policy both when the employer sets a fixed number of tests per candidate and when the employer can set a dynamic policy, assigning further tests adaptively based on results from the previous tests. To start, we characterize the optimal policy when employees constitute a single group, demonstrating some interesting trade-offs. Subsequently, we address the multi-group setting, demonstrating that when the noise levels vary across groups, a fundamental impossibility emerges whereby we cannot administer the same number of tests, subject candidates to the same decision rule, and yet realize the same outcomes in both groups. ","Efficient candidate screening under multiple tests and implications for
  fairness"
130,1134141334414139392,449236360,Antoine Cully,"['Glad to announce that I will be presenting my new (and first single-authored) paper on “Autonomous skill discovery with Quality-Diversity and Unsupervised Descriptors” @GECCO2019.\nThe preprint is available here: <LINK> @ImperialAI @ICComputing <LINK>', 'It introduces AURORA: AUtonomous RObots that Realize their Abilities.\nIt uses auto-encoders to define descriptors in QD algorithms, while using QD algorithms to generate training datasets for the AE. Combined together this allows robots to discover their range of skills.']",https://arxiv.org/abs/1905.11874,"Quality-Diversity optimization is a new family of optimization algorithms that, instead of searching for a single optimal solution to solving a task, searches for a large collection of solutions that all solve the task in a different way. This approach is particularly promising for learning behavioral repertoires in robotics, as such a diversity of behaviors enables robots to be more versatile and resilient. However, these algorithms require the user to manually define behavioral descriptors, which is used to determine whether two solutions are different or similar. The choice of a behavioral descriptor is crucial, as it completely changes the solution types that the algorithm derives. In this paper, we introduce a new method to automatically define this descriptor by combining Quality-Diversity algorithms with unsupervised dimensionality reduction algorithms. This approach enables robots to autonomously discover the range of their capabilities while interacting with their environment. The results from two experimental scenarios demonstrate that robot can autonomously discover a large range of possible behaviors, without any prior knowledge about their morphology and environment. Furthermore, these behaviors are deemed to be similar to handcrafted solutions that uses domain knowledge and significantly more diverse than when using existing unsupervised methods. ","Autonomous skill discovery with Quality-Diversity and Unsupervised
  Descriptors"
131,1134081376934932480,4860077517,Ionut-Teodor Sorodoc,"['I will be at NAACL in Minneapolis next week, presenting our new @amoreupf  paper: ""What do Entity-Centric Models Learn ?...."" <LINK> 👨\u200d🎓']",https://arxiv.org/abs/1905.06649,"Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed. ","What do Entity-Centric Models Learn? Insights from Entity Linking in
  Multi-Party Dialogue"
132,1134059216128397312,837380189512425472,Christopher Rozell,"['ML (and maybe neuro) folks, we\'re excited about a new ICML paper: ""Active Embedding Search via Noisy Paired Comparisons"" by Canal (@GregHCanal), Massimino, Davenport and Rozell. What\'s the summary? Glad you asked.\n\nPreprint: <LINK>\nCode: <LINK>', 'The problem we\'re considering is ""preference search"", which is the task of finding the best ""item"" for a given user. An ""item"" is just a vector in a vector space, so this is a very general formulation that could have many applications.', 'Items could be material things (e.g., a product, an image in a dataset), or it could be something abstract like the preferred settings of a medical device or model parameters. Especially interested in suggestions of neuro applications.', 'We assume a database of example items to query the user with, and that dataset has a low dimensional embedding where distance corresponds to perceived dissimilarity. Another manuscript coming soon on efficiently learning this type of embedding. https://t.co/TmuIqi6yZo', 'We do NOT assume that there are known features that correspond to perceived similarity, or that there is a natural notion of a gradient that can be used to descend in search of a minimizer. The case of interest relies only on subjective user responses.', 'We also do not assume that the preferred point is necessarily in the database. The user might prefer an ""item"" we don\'t currently have available (e.g., item doesn\'t exist, parameters haven\'t been tried). The preferred point implies a ranking of existing items.', 'Instead of absolute judgements which are often very noisy, we use relational queries to search the space. So, the user is asked comparisons of the form ""do you prefer item p or q?"". https://t.co/IybS0p16Xq', 'We propose an active learning method called InfoGain that selects new queries aiming to maximize mutual information between the preference point and the next query given previous responses. We account explicitly for unreliable responses (through a noise models on the user).', 'We provide new theoretical insights into the benefits and challenges of greedy information maximization, and develop two novel tractable strategies (labeled MCMV and EPMV) approximating InfoGain that maximize lower bounds on information gain for each query. https://t.co/LmzevIfCKp', ""Our approach shows superior preference estimation over state-of-the-art selection methods as well as random queries, both in terms of MSE to the continuous preference point and Kendall's tau (rank correlation) on the implied rank ordering of preferred existing points. https://t.co/X8ISYFfAFD"", 'Happy for feedback. Especially excited to hear ideas for collaborations on use cases. Any ideas in computational neuroscience, psychometrics, etc.?']",https://arxiv.org/abs/1905.04363,"Suppose that we wish to estimate a user's preference vector $w$ from paired comparisons of the form ""does user $w$ prefer item $p$ or item $q$?,"" where both the user and items are embedded in a low-dimensional Euclidean space with distances that reflect user and item similarities. Such observations arise in numerous settings, including psychometrics and psychology experiments, search tasks, advertising, and recommender systems. In such tasks, queries can be extremely costly and subject to varying levels of response noise; thus, we aim to actively choose pairs that are most informative given the results of previous comparisons. We provide new theoretical insights into the benefits and challenges of greedy information maximization in this setting, and develop two novel strategies that maximize lower bounds on information gain and are simpler to analyze and compute respectively. We use simulated responses from a real-world dataset to validate our strategies through their similar performance to greedy information maximization, and their superior preference estimation over state-of-the-art selection methods as well as random queries. ",Active embedding search via noisy paired comparisons
133,1133561399546961921,321794593,José G. Fernández-Trincado,"['Check-out our new paper accepted on MNRAS by Tali et al. (2019), about “Analysis of the physical nature of 22 New VVV Survey Globular Cluster candidates in the Milky Way Bulge”: <LINK>']",https://arxiv.org/abs/1905.11835,"In order to characterize 22 new globular cluster (GC) candidates in the Galactic bulge, we present their colour-magnitude diagrams (CMDs) and Ks-band luminosity functions (LFs) using the near-infrared VVV database as well as Gaia-DR2 proper motion dataset. CMDs were obtained, on one hand, after properly decontaminating the observed diagrams from background/foreground disc stars and other sources. On the other hand, CMDs were also obtained based upon star selection in proper motion diagrams. Taking into account our deep CMDs and LFs analyses, we find that 17 out of 22 new GC candidates may be real and should therefore be followed-up, while 5 candidates were discarded from the original sample. We also search for RR Lyrae and Mira variable stars in the fields of these new GC candidates. In particular, we confirm that Minni 40 may be a real cluster. If confirmed by further follow-up analysis, it would be the closest GC to the Galactic centre in projected angular distance, located only 0.5 deg away from it. We consider that it is very difficult to confirm the physical reality of these small, poorly-populated bulge GCs so in many cases alternative techniques are needed to corroborate our findings. ","Analysis of the physical nature of 22 New VVV Survey Globular Cluster
  candidates in the Milky Way Bulge"
134,1133557685570289664,803652792187400192,Mark Sellke,"['Follow up! <LINK>\n\nPrevious paper had competitive ratio 2^{O(d)} in dimension d. New upper bound is d for any norm, which is tight for L^{\\infty}. Nearly optimal sqrt(d log N) for N requests in L^2. Method generalizes the Steiner point we used for nested chasing. <LINK>']",https://arxiv.org/abs/1905.11968,"In the chasing convex bodies problem, an online player receives a request sequence of $N$ convex sets $K_1,\dots, K_N$ contained in a normed space $\mathbb R^d$. The player starts at $x_0\in \mathbb R^d$, and after observing each $K_n$ picks a new point $x_n\in K_n$. At each step the player pays a movement cost of $||x_n-x_{n-1}||$. The player aims to maintain a constant competitive ratio against the minimum cost possible in hindsight, i.e. knowing all requests in advance. The existence of a finite competitive ratio for convex body chasing was first conjectured in 1991 by Friedman and Linial. This conjecture was recently resolved with an exponential $2^{O(d)}$ upper bound on the competitive ratio. We give an improved algorithm achieving competitive ratio $d$ in any normed space, which is exactly tight for $\ell^{\infty}$. In Euclidean space, our algorithm also achieves competitive ratio $O(\sqrt{d\log N})$, nearly matching a $\sqrt{d}$ lower bound when $N$ is subexponential in $d$. The approach extends our prior work for nested convex bodies, which is based on the classical Steiner point of a convex body. We define the functional Steiner point of a convex function and apply it to the associated work function. ",Chasing Convex Bodies Optimally
135,1133537727075430400,3877821072,Shuai Tang,"['A new paper on post-processing methods for word vectors is available now. (I) paper: <LINK> (II) code(hope it works): <LINK>', '1/ we started from the view of shrinkage estimation of the gram matrix of words. The shrinkage of the gram matrix is conducted in semi-Riemannian geometry but the word vectors themselves are still in Euclidean space.', '2/ the similarity of the resulting matrix after shrinkage and the oracle gram matrix of words is measured by Centralised Kernel Alignment, which has been shown to be invariant to any rotations and isotropic scaling.', '3/ the post-processing method is derived from the lower bound of the actual objective, and it seems to be very effective on boosting performance of pretrained word vectors on down-stream tasks, including word similarity, translation and sentence similarity']",https://arxiv.org/abs/1905.10971,"Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks. ",An Empirical Study on Post-processing Methods for Word Embeddings
136,1131241124742324224,19465243,Hannah Earnshaw,"['I have a new paper accepted for publication in ApJ! ""A broadband look at the old and new ULXs of NGC 6946"", including one very strange new transient ULX... <LINK>']",https://arxiv.org/abs/1905.03383,"Two recent observations of the nearby galaxy NGC 6946 with NuSTAR, one simultaneous with an XMM-Newton observation, provide an opportunity to examine its population of bright accreting sources from a broadband perspective. We study the three known ultraluminous X-ray sources (ULXs) in the galaxy, and find that ULX-1 and ULX-2 have very steep power-law spectra with $\Gamma=3.6^{+0.4}_{-0.3}$ in both cases. Their properties are consistent with being super-Eddington accreting sources with the majority of their hard emission obscured and down-scattered. ULX-3 (NGC 6946 X-1) is significantly detected by both XMM-Newton and NuSTAR at $L_{\rm X}=(6.5\pm0.1)\times10^{39}$ erg s$^{-1}$, and has a power-law spectrum with $\Gamma=2.51\pm0.05$. We are unable to identify a high-energy break in its spectrum like that found in other ULXs, but the soft spectrum likely hinders our ability to detect one. We also characterise the new source, ULX-4, which is only detected in the joint XMM-Newton and NuSTAR observation, at $L_{\rm X}=(2.27\pm0.07)\times10^{39}$ erg s$^{-1}$, and is absent in a Chandra observation ten days later. It has a very hard cut-off power-law spectrum with $\Gamma=0.7\pm0.1$ and $E_{\rm cut}=11^{+9}_{-4}$ keV. We do not detect pulsations from ULX-4, but its transient nature can be explained either as a neutron star ULX briefly leaving the propeller regime or as a micro-tidal disruption event induced by a stellar-mass compact object. ",A broadband look at the old and new ULXs of NGC 6946
137,1128345651509637120,1001323493336662016,Noga Zaslavsky,"['Our new #CogSci2019 paper on the Information Bottleneck principle and semantic categories is now on arXiv: <LINK>\nwith Terry Regier, @NaftaliTishby, and Charles Kemp <LINK>']",https://arxiv.org/abs/1905.04562,"It has been argued that semantic categories across languages reflect pressure for efficient communication. Recently, this idea has been cast in terms of a general information-theoretic principle of efficiency, the Information Bottleneck (IB) principle, and it has been shown that this principle accounts for the emergence and evolution of named color categories across languages, including soft structure and patterns of inconsistent naming. However, it is not yet clear to what extent this account generalizes to semantic domains other than color. Here we show that it generalizes to two qualitatively different semantic domains: names for containers, and for animals. First, we show that container naming in Dutch and French is near-optimal in the IB sense, and that IB broadly accounts for soft categories and inconsistent naming patterns in both languages. Second, we show that a hierarchy of animal categories derived from IB captures cross-linguistic tendencies in the growth of animal taxonomies. Taken together, these findings suggest that fundamental information-theoretic principles of efficient coding may shape semantic categories across languages and across domains. ",Semantic categories of artifacts and animals reflect efficient coding
138,1128073835109015552,92182169,Ian Manchester,['Our new paper argues that Riemannian geometry (via Control Contraction Metrics) is the missing link that makes LPV gain scheduling really work:\n<LINK>'],https://arxiv.org/abs/1905.01811,"Gain-scheduled control based on linear parameter-varying (LPV) models derived from local linearizations is a widespread nonlinear technique for tracking time-varying setpoints. Recently, a nonlinear control scheme based on Control Contraction Metrics (CCMs) has been developed to track arbitrary admissible trajectories. This paper presents a comparison study of these two approaches. We show that the CCM based approach is an extended gain-scheduled control scheme which achieves global reference-independent stability and performance through an exact control realization which integrates a series of local LPV controllers on a particular path between the current and reference states. ","A Comparison of LPV Gain Scheduling and Control Contraction Metrics for
  Nonlinear Control"
139,1126394485469011968,50624939,Arne Smeets,"['New paper on the arXiv today - my longest one so far (45 pages):\n<LINK>\nBlood, sweat and tears... :-)']",https://arxiv.org/abs/1905.02795,"Almost one decade ago, Poonen constructed the first examples of algebraic varieties over global fields for which Skorobogatov's etale Brauer-Manin obstruction does not explain the failure of the Hasse principle. By now, several constructions are known, but they all share common geometric features such as large fundamental groups. In this paper, we construct simply connected fourfolds over global fields of positive characteristic for which the Brauer-Manin machinery fails. Contrary to earlier work in this direction, our construction does not rely on major conjectures. Instead, we establish a new diophantine result of independent interest: a Mordell-type theorem for Campana's ""geometric orbifolds"" over function fields of positive characteristic. Along the way, we also construct the first example of simply connected surface of general type over a global field with a non-empty, but non-Zariski dense set of rational points. ","Failure of the Brauer-Manin principle for a simply connected fourfold
  over a global function field, via orbifold Mordell"
140,1126332235215294464,4475055297,Ming-Yu Liu,"['Check out our new #GAN work on translating images to unseen domains in the test time with few example images.\nLive demo <LINK>\nProject page <LINK>\nPaper <LINK>\nVideo <LINK>\n#NVIDIA <LINK>', 'Brought to you by @xunhuang1995  @arunmallya #TeroKarras, #TimoAila of #StyleGAN, @jaakkolehtinen, and @jankautz @NvidiaAI', 'The web demo might be buggy. I know nothing about Javascript until last week. So please read the instruction carefully for run the demo. It currently works only on Chrome and Firefox and you have to click  ""Load unsafe scripts"" or  ""Disable protection for now"" buttons.', 'Check out our paper for more results including translating all kinds of foods to Chowmein. https://t.co/iYwv3wh5Ts', 'PetSwap demo video\nhttps://t.co/CRAoLundVy\n\nlive demo available at https://t.co/KeYHIDpgcx', 'It works for non standard pet too. https://t.co/GcYOPA3Oix', 'The PetSwap model is trained using carnivorous animals. It might be funny when you input images of other kinds of animals. https://t.co/0Lq1BVuJUa', '@chris_j_beckham Man. I am just having fun. :)']",https://arxiv.org/abs/1905.01723,"Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at this https URL . ",Few-Shot Unsupervised Image-to-Image Translation
141,1125869873064706048,738769492122214400,Johannes Lischner,"['Our new paper on ""Twist-angle dependence of electron correlations in moire graphene bilayers"" shows that the magic angle is not so special after all from the viewpoint of *interacting* electrons. Read here: <LINK> #graphene <LINK>']",https://arxiv.org/abs/1905.01887,"Motivated by the recent observation of correlated insulator states and unconventional superconductivity in twisted bilayer graphene, we study the dependence of electron correlations on the twist angle and reveal the existence of strong correlations over a narrow range of twist-angles near the magic angle. Specifically, we determine the on-site and extended Hubbard parameters of the low-energy Wannier states using an atomistic quantum-mechanical approach. The ratio of the on-site Hubbard parameter and the width of the flat bands, which is an indicator of the strength of electron correlations, depends sensitively on the screening by the semiconducting substrate and the metallic gates. Including the effect of long-ranged Coulomb interactions significantly reduces electron correlations and explains the experimentally observed sensitivity of strong correlation phenomena on twist angle. ","Twist-angle dependence of electron correlations in moir\'e graphene
  bilayers"
142,1125573365073367040,1002014071,Frank Wilczek,"['New paper ""Color Blind Detectors Enable Chromatic Interferometry"" <LINK>. Using quantum tricks + strategic ignorance to access fresh information.  It was great fun to work with J. Cotler + powerhouse Chinese experimental group in this project. More to come! <LINK>']",https://arxiv.org/abs/1905.01823,"By engineering and manipulating quantum entanglement between incoming photons and experimental apparatus, we construct single-photon detectors which cannot distinguish between photons of very different wavelengths. These color erasure detectors enable a new kind of intensity interferometry, with potential applications in microscopy and astronomy. We demonstrate chromatic interferometry experimentally, observing robust interference using both coherent and incoherent photon sources. ",Color Erasure Detectors Enable Chromatic Interferometry
143,1136644989017100288,454838126,Jos de Bruijne,"['""First stellar occultation by the Galilean moon Europa and upcoming events between 2019 and 2021"" <LINK> ""We observed a stellar occultation by Europa and propose a [#GaiaDR2-enabled] campaign for observing stellar occultations for all Galilean moons"" #GaiaMission <LINK>']",https://arxiv.org/abs/1905.12520,"Context. Bright stellar positions are now known with an uncertainty below 1 mas thanks to Gaia DR2. Between 2019-2020, the Galactic plane will be the background of Jupiter. The dense stellar background will lead to an increase in the number of occultations, while the Gaia DR2 catalogue will reduce the prediction uncertainties for the shadow path. Aims. We observed a stellar occultation by the Galilean moon Europa (J2) and propose a campaign for observing stellar occultations for all Galilean moons. Methods. During a predicted period of time, we measured the light flux of the occulted star and the object to determine the time when the flux dropped with respect to one or more reference stars, and the time that it rose again for each observational station. The chords obtained from these observations allowed us to determine apparent sizes, oblatness, and positions with kilometre accuracy. Results. We present results obtained from the first stellar occultation by the Galilean moon Europa observed on 2017 March 31. The apparent fitted ellipse presents an equivalent radius of 1561.2 $\pm$ 3.6 km and oblatenesses 0.0010 $\pm$ 0.0028. A very precise Europa position was determined with an uncertainty of 0.8 mas. We also present prospects for a campaign to observe the future events that will occur between 2019 and 2021 for all Galilean moons. Conclusions. Stellar occultation is a suitable technique for obtaining physical parameters and highly accurate positions of bright satellites close to their primary. A number of successful events can render the 3D shapes of the Galilean moons with high accuracy. We encourage the observational community (amateurs included) to observe the future predicted events. ","First stellar occultation by the Galilean moon Europa and upcoming
  events between 2019 and 2021"
144,1135571699968200704,2607674898,Yasith Jayawardana,"['At @WebSciDL, we (@yasithmilinda , @OpenMaze) propose an approach to simplify dataset management, discovery and utilization. Check out our blog at <LINK> and the extended paper on arXiv (<LINK>) for more information.']",https://arxiv.org/abs/1905.13363,"Many research questions can be answered quickly and efficiently using data already collected for previous research. This practice is called secondary data analysis (SDA), and has gained popularity due to lower costs and improved research efficiency. In this paper we propose DFS, a file system to standardize the metadata representation of datasets, and DDU, a scalable architecture based on DFS for semi-automated metadata generation and data recommendation on the cloud. We discuss how DFS and DDU lays groundwork for automatic dataset aggregation, how it integrates with existing data wrangling and machine learning tools, and explores their implications on datasets stored in digital libraries. ",DFS: A Dataset File System for Data Discovering Users
145,1135453406439575552,2444302555,Ludovic Denoyer,"['New Preprint with Diane Bouchacourt: ""EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction"" where we propose a new way to learn interpretable deep NN. Experiments on text and images. <LINK> <LINK>']",https://arxiv.org/abs/1905.11852,"Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model's prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically defined by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classification and multi-sentiment analysis tasks. ","EDUCE: Explaining model Decisions through Unsupervised Concepts
  Extraction"
146,1134558345795461122,810455926733934593,Simon Kohl,"['Medical images can exhibit ambiguities on multiple scales &amp; locations often varying independently. We propose a hierarchical generative model to capture such variations in segmentations &amp; show much improved sample fidelity and fit with the GT distribution: <LINK>', ""I was very fortunate to work on this paper, the `Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities', with @ber24, @maierhein, @DeepSpiker, @arkitus, @pushmeet, Andrew Zisserman &amp; @ORonneberger! Below find a qualitative TL;DR of the paper:"", 'Aiming to learn distributions over segmentations of ambiguous images with improved sample fidelity, i.e. independent and plausible details, and fit to the ground-truth, we propose a hierarchical conditional VAE that is interleaved with a U-Net, see below: https://t.co/sODtz8uBm9', 'Ambiguities are particularly common in medical images and lead to sets of plausible segmentation hypotheses. The LIDC lung CT dataset e.g. was graded by 4 different experts whose annotations can differ significantly, see below: https://t.co/PFDwvJ6GEQ', 'Compared to prior art, the Hierarchical Probabilistic U-Net induces a much more faithful distribution (w. self-consistent samples) and learns to separate ambiguities, i.e. variations, of different scales. See samples from the full and from different levels of the hierarchy below: https://t.co/rZ3cEFooAZ', ""Observe in the standard deviations how the local latents alter fine details, mostly at the boundaries, while the global latents can flick the presence of coarser abnormality segmentations on and off (the absence of a segmentation corresponds to the `normal' assessment)."", ""We show that a hierarchical latent space decomposition makes for a very flexible conditional generative model that significantly improves the Probabilistic U-Net's reconstruction fidelity and its ability to fit the GT distribution. A comparison for 2 lung CT scans is shown below: https://t.co/aTmlygtP5b"", 'Why are we interested in self-consistent segmentation samples in the first place? It may benefit or be required by the down-stream task, e.g. in a potential clinical application a clinician with access to additional non-imaging data can select the correct segmentation, or ..', '.. a number of segmentation hypotheses can be presented to a subsequent classification network to assign a diagnosis to each possible interpretation of the medical scan (see e.g. https://t.co/K2BFRVW4tC).', 'The flexibility of this generative models allows to model local independent variations (like multiple lesions), which we demonstrate on the task of segmenting individual instances of neurons on a electron-microscopy dataset.. https://t.co/VsHcjy5Zr6', '..and on the task of segmenting car instances on Cityscapes. Here the model also picks up on natural ambiguity like truck vs. bus. Note, we restrict the max number of instances at training time, which is why we cluster several samples for a final instance segmentation prediction. https://t.co/zbGB1vwutN', 'Lastly we train the model to extrapolate instance segmentations on the EM dataset (on masked regions). Here the model needs to infer plausible segmentations from the unmasked image regions. This ability may be useful in spatio-temporal prediction tasks like tumor therapy response https://t.co/8k0Heg5dkY']",https://arxiv.org/abs/1905.13077,"Medical imaging only indirectly measures the molecular identity of the tissue within each voxel, which often produces only ambiguous image evidence for target measures of interest, like semantic segmentation. This diversity and the variations of plausible interpretations are often specific to given image regions and may thus manifest on various scales, spanning all the way from the pixel to the image level. In order to learn a flexible distribution that can account for multiple scales of variations, we propose the Hierarchical Probabilistic U-Net, a segmentation network with a conditional variational auto-encoder (cVAE) that uses a hierarchical latent space decomposition. We show that this model formulation enables sampling and reconstruction of segmenations with high fidelity, i.e. with finely resolved detail, while providing the flexibility to learn complex structured distributions across scales. We demonstrate these abilities on the task of segmenting ambiguous medical scans as well as on instance segmentation of neurobiological and natural images. Our model automatically separates independent factors across scales, an inductive bias that we deem beneficial in structured output prediction tasks beyond segmentation. ",A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities
147,1134453686216876032,61127825,Margaret-Anne Storey,"['Our paper ""Methodology Matters: How We Study Socio-Technical Aspects in Software Engineering"" is now online (we welcome feedback!) at <LINK> with @CourtneyELW @neilernst @alexeyzagalsky @irina_kAl #icse19', '@neilernst @drfeldt @CourtneyELW @alexeyzagalsky @irina_kAl Yes we will try to get that updated with the latest analysis which in particular looked more deeply at the ""data only"" papers.  We posted the paper now to perhaps some initial feedback -- we have not submitted yet to any particular venue.', '@drfeldt @neilernst @CourtneyELW @alexeyzagalsky @irina_kAl No I love it -- we have to work together right :)']",https://arxiv.org/abs/1905.12841,"Software engineering is a socio-technical endeavor, and while many of our contributions focus on technical aspects, human stakeholders such as software developers are directly affected by and can benefit from our research and tool innovations. In this paper, we question how much of our research addresses human and social issues, and explore how much we study human and social aspects in our research designs. To answer these questions, we developed a socio-technical research framework to capture the main beneficiary of a research study (the who), the main type of research contribution produced (the what), and the research strategies used in the study (how we methodologically approach delivering relevant results given the who and what of our studies). We used this Who-What-How framework to analyze 151 papers from two well-cited publishing venues---the main technical track at the International Conference on Software Engineering, and the Empirical Software Engineering Journal by Springer---to assess how much this published research explicitly considers human aspects. We find that although a majority of these papers claim the contained research should benefit human stakeholders, most focus on technical contributions without engaging humans in their studies. Although our analysis is scoped to two venues, our results suggest a need for more diversification and triangulation of research strategies. In particular, there is a need for strategies that aim at a deeper understanding of human and social aspects of software development practice to balance the design and evaluation of technical innovations. We recommend that the framework should be used in the design of future studies in order to nudge software engineering research towards explicitly including human and social concerns in their designs, and to improve the relevance of our research for human stakeholders. ","The Who, What, How of Software Engineering Research: A Socio-Technical
  Framework"
148,1134124150812041216,1575689167,Jonathan Vacher,['Check out our new preprint with @CoenCagli_Lab ! We propose a regularization of mixture models (MM). We combine MM for image segmentation and show that MM can achieve SOTA scores on boundaries detection. Fitted MM can be used to perform image synthesis ! <LINK>'],https://arxiv.org/abs/1905.10629,"Probabilistic finite mixture models are widely used for unsupervised clustering. These models can often be improved by adapting them to the topology of the data. For instance, in order to classify spatially adjacent data points similarly, it is common to introduce a Laplacian constraint on the posterior probability that each data point belongs to a class. Alternatively, the mixing probabilities can be treated as free parameters, while assuming Gauss-Markov or more complex priors to regularize those mixing probabilities. However, these approaches are constrained by the shape of the prior and often lead to complicated or intractable inference. Here, we propose a new parametrization of the Dirichlet distribution to flexibly regularize the mixing probabilities of over-parametrized mixture distributions. Using the Expectation-Maximization algorithm, we show that our approach allows us to define any linear update rule for the mixing probabilities, including spatial smoothing regularization as a special case. We then show that this flexible design can be extended to share class information between multiple mixture models. We apply our algorithm to artificial and natural image segmentation tasks, and we provide quantitative and qualitative comparison of the performance of Gaussian and Student-t mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to propagate class information across the layers of deep convolutional neural networks in a probabilistically optimal way, suggesting a new interpretation for feedback signals in biological visual systems. Our flexible approach can be easily generalized to adapt probabilistic mixture models to arbitrary data topologies. ","Flexibly Regularized Mixture Models and Application to Image
  Segmentation"
149,1134108679085551617,2592291038,Alexander C. Nwala,"['.@WebSciDL, #webarchiving folks,\nwe (@acnwala @weiglemc @mln) studied/characterized/compared seed collections generated from SERPs &amp; Micro-collections - MC (e.g., twitter threads) for our #JCDL2019 paper. \n\nTech Report: <LINK>\n\ncc @moffattchristie @justin_littman <LINK>', '@machawk1 @WebSciDL @weiglemc @MLN @moffattchristie @justin_littman @phonedude_mln oops! I forgot this is Twitter not email.']",https://arxiv.org/abs/1905.12220,"In a Web plagued by disappearing resources, Web archive collections provide a valuable means of preserving Web resources important to the study of past events ranging from elections to disease outbreaks. These archived collections start with seed URIs (Uniform Resource Identifiers) hand-selected by curators. Curators produce high quality seeds by removing non-relevant URIs and adding URIs from credible and authoritative sources, but it is time consuming to collect these seeds. Two main strategies adopted by curators for discovering seeds include scraping Web (e.g., Google) Search Engine Result Pages (SERPs) and social media (e.g., Twitter) SERPs. In this work, we studied three social media platforms in order to provide insight on the characteristics of seeds generated from different sources. First, we developed a simple vocabulary for describing social media posts across different platforms. Second, we introduced a novel source for generating seeds from URIs in the threaded conversations of social media posts created by single or multiple users. Users on social media sites routinely create and share posts about news events consisting of hand-selected URIs of news stories, tweets, videos, etc. In this work, we call these posts micro-collections, and we consider them as an important source for seeds because the effort taken to create micro-collections is an indication of editorial activity, and a demonstration of domain expertise. Third, we generated 23,112 seed collections with text and hashtag queries from 449,347 social media posts from Reddit, Twitter, and Scoop.it. We collected in total 120,444 URIs from the conventional scraped SERP posts and micro-collections. We characterized the resultant seed collections across multiple dimensions including the distribution of URIs, precision, ages, diversity of webpages, etc... ","Using Micro-collections in Social Media to Generate Seeds for Web
  Archive Collections"
150,1133712745713549312,786109620284719104,Ethan fetaya,"['New paper with Dan Levi, Liran Gispan and Niv Giladi on calibration for regression tasks <LINK>. We show major flaws in a recently proposed definition and method, where even random uncertainty can be perfectly calibrated and propose a simple alternative.']",https://arxiv.org/abs/1905.11659,"Predicting not only the target but also an accurate measure of uncertainty is important for many machine learning applications and in particular safety-critical ones. In this work we study the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. We show that the existing definition for calibration of a regression uncertainty [Kuleshov et al. 2018] has severe limitations in distinguishing informative from non-informative uncertainty predictions. We propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach. Our method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. We also propose a simple, scaling-based calibration method that preforms as well as much more complex ones. We show results on both a synthetic, controlled problem and on the object detection bounding-box regression task using the COCO and KITTI datasets. ",Evaluating and Calibrating Uncertainty Prediction in Regression Tasks
151,1133403449951956995,3918111614,Oriol Vinyals,"[""Evaluating generative models is hard! We propose Classification Accuracy Score from classifiers trained on generated data:\n\n-Accuracy of 43% when trained purely on BigGAN samples (vs 73%)\n-Naive data augmentation doesn't work (yet!)\n\nPaper: <LINK>\ncc @SumanRavuri <LINK>""]",https://arxiv.org/abs/1905.10887,"Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes---variational autoencoders, autoregressive models, and generative adversarial networks (GANs)---to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9\% and 41.6\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric. ",Classification Accuracy Score for Conditional Generative Models
152,1133354187935440898,2850683124,Paul Michel,"['Happy to share our pre-print ""Are Sixteen Heads Really Better Than One?"" (with @omerlevy_ and @gneubig). We find that a majority of the attention heads in a transformer model can be individually removed without hurting performance significantly. <LINK> <LINK>', 'This also appears to be true across domains to an extent. In particular removing some heads causes a large drop in performance in both domains we tested on, both in an MT transformer and BERT. https://t.co/YIGXT2gMF0', 'Even better that, in some cases you can get away with only 1 head! https://t.co/4m2zTn39d7', 'We also show that you can devise a proxy importance score and use it to prune heads joinlty in a greedy fashion. For example this lets us prune up to 40% heads in BERT without hurting performance. https://t.co/MUK7z73Yls', ""Here's where it gets interesting: in our MT model we find that the encoder-decoder attention is much more sensitive to pruning than either self-attention mechanism! https://t.co/gAF2kDRYS1"", 'And finally another surprising fact: this discrepancy between ""useful"" and ""useless"" heads is decided early during training, we find that in early epochs, performance decreases linearly with the pruning percentage. Check out the paper if you want to read more! https://t.co/f4BsEfBIf5', 'Finally I should mention that there is a recent paper by Elena Voita, David Talbot, Fedor Moissev, @RicoSennrich and @iatitov accepted at ACL that investigates the same phenomenon: https://t.co/cInYBJPNg9 for MT transformer encoders more specifically.', 'Although the two papers share the same basic premise, they are very much complementary and I really recommend giving theirs a read as well. In particular I really liked their analysis of the role of each head and the relationship with pruning. https://t.co/u1xS3OQAy6', '@_lpag @omerlevy_ @gneubig Great question! We had done a few experiments related to that (mainly retraining the sub-network w/ and w/o the same init) but couldn\'t find anything to corroborate the LTH. That being said there was a follow-up paper about ""late resetting"" which we haven\'t looked into yet.']",https://arxiv.org/abs/1905.10650,"Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention ""head"" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention. ",Are Sixteen Heads Really Better than One?
153,1133276644033159168,955082118341840896,Simon Stähler,"['Microseismic noise elsewhere?\nA small study we did with @MarkPanning and others explores whether a seismometer could hear the waves on the lakes of the Saturn moon Titan. TLDR: If it is a really big storm\n<LINK> <LINK>', 'Possibly related topic: @JHUAPL has some really neat visualizations of #DragonflyTitan in flight over Titan: https://t.co/8b3XwqIVR5\n#SEISonTitan']",http://arxiv.org/abs/1905.11251,"Seismology is the main tool for inferring the deep interior structures of Earth and potentially also of other planetary bodies in the solar system. Terrestrial seismology is influenced by the presence of the ocean-generated microseismic signal, which sets a lower limit on the earthquake detection capabilities but also provides a strong energy source to infer the interior structure on scales from local to continental. Titan is the only other place in the solar system with permanent surface liquids and future lander missions there might carry a seismic package. Therefore, the presence of microseisms would be of great benefit for interior studies, but also for detecting storm-generated waves on the lakes remotely. We estimated the strength of microseismic signals on Titan, based on wind speeds predicted from modeled global circulation models interior structure. We find that storms of more than 2 m/s wind speed, would create a signal that is globally observable with a high-quality broadband sensor and observable to a thousand kilometer distance with a space-ready seismometer, such as the InSight instruments currently operating on the surface of Mars. ",Seismic Signals from Waves on Titan's Seas
154,1133260821415583744,2466345433,Lutz Bornmann,"['We developed a @Stata command and R package for simulating h index and halpha index evaluations. In a first study, we used the commands for investigating the Matthew effect in science <LINK> <LINK>']",https://arxiv.org/abs/1905.11052,"Recently, Hirsch (2019a) proposed a new variant of the h index called the $h_\alpha$ index. He formulated as follows: ""we define the $h_\alpha$ index of a scientist as the number of papers in the h-core of the scientist (i.e. the set of papers that contribute to the h-index of the scientist) where this scientist is the $\alpha$-author"" (p. 673). The $h_\alpha$ index was criticized by Leydesdorff, Bornmann, and Opthof (2019). One of their most important points is that the index reinforces the Matthew effect in science. We address this point in the current study using a recently developed Stata command (h_index) and R package (hindex), which can be used to simulate h index and $h_\alpha$index applications in research evaluation. The user can investigate under which conditions $h_\alpha$ reinforces the Matthew effect. The results of our study confirm what Leydesdorff et al. (2019) expected: the $h_\alpha$ index reinforces the Matthew effect. This effect can be intensified if strategic behavior of the publishing scientists and cumulative advantage effects are additionally considered in the simulation. ","Does the $h_\alpha$ index reinforce the Matthew effect in science?
  Agent-based simulations using Stata and R"
155,1133192597709582336,950132983083671553,Joey Bose,"['New Preprint with @andrecianflone &amp; @williamleif: <LINK>. We propose a new framework for adversarial attacks which can be deployed in multiple domains unifying great work from multiple domains into 1 threat model.', 'Our attack strategy also uses a generative model which allows us to produce a distribution of adversarial samples which can generalize to unseen instances WITHOUT any further optimization!']",https://arxiv.org/abs/1905.10864,"Adversarial attacks on deep neural networks traditionally rely on a constrained optimization paradigm, where an optimization procedure is used to obtain a single adversarial perturbation for a given input example. In this work we frame the problem as learning a distribution of adversarial perturbations, enabling us to generate diverse adversarial distributions given an unperturbed input. We show that this framework is domain-agnostic in that the same framework can be employed to attack different input domains with minimal modification. Across three diverse domains---images, text, and graphs---our approach generates whitebox attacks with success rates that are competitive with or superior to existing approaches, with a new state-of-the-art achieved in the graph domain. Finally, we demonstrate that our framework can efficiently generate a diverse set of attacks for a single given input, and is even capable of attacking \textit{unseen} test instances in a zero-shot manner, exhibiting attack generalization. ","Generalizable Adversarial Attacks with Latent Variable Perturbation
  Modelling"
156,1133075097890304000,2533212240,Marco Fiorucci,['How can we separate #structural information from noise in large #graphs? Give a look to our last paper <LINK>\nHere <LINK> you find a a Python 3.6 implementation'],https://arxiv.org/abs/1905.06917,"How can we separate structural information from noise in large graphs? To address this fundamental question, we propose a graph summarization approach based on Szemer\'edi's Regularity Lemma, a well-known result in graph theory, which roughly states that every graph can be approximated by the union of a small number of random-like bipartite graphs called `regular pairs'. Hence, the Regularity Lemma provides us with a principled way to describe the essential structure of large graphs using a small amount of data. Our paper has several contributions: (i) We present our summarization algorithm which is able to reveal the main structural patterns in large graphs. (ii) We discuss how to use our summarization framework to efficiently retrieve from a database the top-k graphs that are most similar to a query graph. (iii) Finally, we evaluate the noise robustness of our approach in terms of the reconstruction error and the usefulness of the summaries in addressing the graph search task. ","Separating Structure from Noise in Large Graphs Using the Regularity
  Lemma"
157,1131721233207308288,393372877,Riccardo Di Sipio 🇨🇦🇮🇹🇪🇺,"['What happens if you run the parton shower algorithm 100,000 times over the same boosted top quark or Higgs boson? Should we take the stochastic uncertainty associated with the PS into account in our measurements? Find out more: <LINK> <LINK> <LINK>', ""@KyleCranmer Ah! I called it both stochastic (in its own nature) and systematic, meaning it is associated with a theoretical model that has tunable parameters. Very good point. I have to think more about the likelihood-free argument, I've never seen it like that before. Thanks!""]",https://arxiv.org/abs/1905.09657,The Parton-Shower algorithm implement in the Pythia generator is applied multiple times to the same parton-level configuration to estimate the systematic uncertainty affecting large-radius jet substructure variables associated with the stochastic nature of the algorithm. Results are presented in the case of boosted $h\rightarrow b\bar{b}$ and $t\rightarrow bqq$. The code is publicly available on the repository this https URL ,"Shower it again, Pythia"
158,1131035045626490882,15132384,Kyosuke Nishida,"['Our #ACL2019_Italy paper about explainable multi-hop QA is out on arXiv!  <LINK> We propose a query-based extractive summarization model, QFE, and the multi-task learning of QA and evidence extraction. Our model achieves SOTA in evidence extraction on HotpotQA! <LINK>']",https://arxiv.org/abs/1905.08511,"Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database. ","Answering while Summarizing: Multi-task Learning for Multi-hop QA with
  Evidence Extraction"
159,1130877179854770178,2968495918,Xavier Trepat,"['Here\'s a preprint of our new review ""Physical Models of Collective Cell Migration"" with Ricard Alert. We propose a classification of mechanical interactions according to their impact on cell position and orientation. 100% theory. To appear @AnnualReviews\n<LINK> <LINK>']",http://arxiv.org/abs/1905.07675,"Collective cell migration is a key driver of embryonic development, wound healing, and some types of cancer invasion. Here we provide a physical perspective of the mechanisms underlying collective cell migration. We begin with a catalogue of the cell-cell and cell-substrate interactions that govern cell migration, which we classify into positional and orientational interactions. We then review the physical models that have been developed to explain how these interactions give rise to collective cellular movement. These models span the sub-cellular to the supracellular scales, and they include lattice models, phase fields models, active network models, particle models, and continuum models. For each type of model, we discuss its formulation, its limitations, and the main emergent phenomena that it has successfully explained. These phenomena include flocking and fluid-solid transitions, as well as wetting, fingering, and mechanical waves in spreading epithelial monolayers. We close by outlining remaining challenges and future directions in the physics of collective cell migration. ",Physical Models of Collective Cell Migration
160,1129185596361060352,750727628294848512,Malena Rice,"['Want to know how we can combine New Horizons, Gaia, LSST, &amp; high-frequency trading to find #PlanetNine, supplement the @NASALucy2Trojan and @NASAPsyche missions, and so much more? Check out our (maybe not so?) crazy idea on arxiv tonight! @greg_laughlin_ <LINK>', '@j_tharindu @NASALucy2Trojan @NASAPsyche @greg_laughlin_ Thank you so much, Tharindu!! 🤗']",https://arxiv.org/abs/1905.06354,"We discuss the feasibility of and present initial designs and approximate cost estimates for a large ($N\sim2000$) network of small photometric telescopes that is purpose-built to monitor $V \lesssim 15$ Gaia Mission program stars for occultations by minor solar system bodies. The implementation of this network would permit measurement of the solar system's tidal gravity field to high precision, thereby revealing the existence of distant trans-Neptunian objects such as the proposed ""Planet Nine."" As a detailed example of the network capabilities, we investigate how occultations by Jovian Trojans can be monitored to track the accumulation of gravitational perturbations, thereby constraining the presence of undetected massive solar system bodies. We also show that the tidal influence of Planet Nine can be discerned from that of smaller, nearer objects in the Kuiper belt. Moreover, ephemerides for all small solar system bodies observed in occultation could be significantly improved using this network, thereby improving spacecraft navigation and refining Solar System modeling. Finally, occultation monitoring would generate direct measurements of size distributions for asteroid populations, permitting a better understanding of their origins. ",The Case for a Large-Scale Occultation Network
161,1127797570099503104,901266828655284225,Brian Metzger,"['<LINK> The last neutron star merger in our Galaxy likely happened 10-100 thousand yrs ago.  As they would appear similar to SN remnants, so how can we find these relic mergers?  Best chance may be &lt;~ MeV gamma-ray decay lines from the r-process isotope Sn126. <LINK>', '@rahulkashyap411 ""Tin""']",https://arxiv.org/abs/1905.03793,"The discovery of a binary neutron star merger (NSM) through both its gravitational wave and electromagnetic emission has revealed these events to be key sites of r-process nucleosynthesis. Here, we evaluate the prospects of finding the remnants of Galactic NSMs by detecting the gamma-ray decay lines from their radioactive r-process ejecta. We find that $^{126}$Sn, which has several lines in the energy range 415-695 keV and resides close to the second r-process peak, is the most promising isotope, because of its half-life $t_{1/2}=2.30(14)\times 10^{5}$ yr being comparable to the ages of recent NSMs. Using a Monte Carlo procedure, we predict that multiple remnants are detectable as individual sources by next-generation gamma-ray telescopes which achieve sub-MeV line sensitivities of $\sim 10^{-8}$-$10^{-6}$ $\gamma$ cm$^{-2}$ s$^{-1}$. However, given the unknown locations of the remnants, the most promising search strategy is a systematic survey of the Galactic plane and bulge extending to high Galactic latitudes. Individual known supernova remnants which may be mis-classified NSM remnants could also be targeted, especially those located outside the Galactic plane. Detection of a moderate sample of Galactic NSM remnants would provide important clues to unresolved issues such as the production of actinides in NSMs, properties of merging NS binaries, and even help distinguish them from rare supernovae as current Galactic r-process sources. We also investigate the diffuse flux from longer-lived nuclei (e.g. $^{182}$Hf) that could in principle trace the Galactic spatial distribution of NSMs over longer timescales, but find that the detection of the diffuse flux appears challenging even with next-generation telescopes. ",Finding the remnants of the Milky Way's last neutron star mergers
162,1126594833701834757,1289767561,Peter Kuipers Munneke,"[""Check <LINK> (my first arXiv preprint 😀). I'm having a cool collaboration with astrophysicists! We propose snow layering as an explanation for anomalous radiowaves originating from cosmic neutrinos interacting with Antarctic ice. With @D_M_Schroeder &amp; @mjsiegert""]",https://arxiv.org/abs/1905.02846,"The ANITA balloon experiment was designed to detect radio signals initiated by neutrinos and cosmic ray air showers. These signals are typically discriminated by the polarization and phase inversions of the radio signal. The reflected signal from cosmic rays suffer phase inversion compared to a direct tau neutrino event. In this paper we study sub-surface reflection, which can occur without phase inversion, in the context of the two anomalous up-going events reported by ANITA. We find that subsurface layers and firn density inversions may plausibly account for the events, while ice fabric layers and wind ablation crusts could also play a role. This hypothesis can be tested with radar surveying of the Antarctic region in the vicinity of the anomalous ANITA events. Future experiments should not use phase inversion as a sole criterion to discriminate between downgoing and upgoing events, unless the subsurface reflection properties are well understood. ","Reflections On the Anomalous ANITA Events: The Antarctic Subsurface as a
  Possible Explanation"
163,1126081164639444992,933826478038544384,Mark Williams,"['LHCb Charm physics paper 6 of 2019. We continue our efforts to find and characterise doubly-charmed baryons, since the Ξcc⁺⁺ discovery in 2017. We here look for its decays into D⁺pK⁻π⁺, but find no evidence of signal <LINK> @LHCbPhysics #LHCbCharmPaper2019 <LINK>']",https://arxiv.org/abs/1905.02421,A search for the $\it{\Xi}^{++}_{cc}$ baryon through the $\it{\Xi}^{++}_{cc} \rightarrow D^{+} p K^{-} \pi^{+}$ decay is performed with a data sample corresponding to an integrated luminosity of 1.7 $\mathrm{fb}^{-1}$ recorded by the LHCb experiment in $pp$ collisions at a centre-of-mass energy of 13 TeV. No significant signal is observed in the mass range from the kinematic threshold of the decay to 3800 $\mathrm{MeV}/c^{2}$. An upper limit is set on the ratio of branching fractions $\mathcal{R} = \frac{\mathcal{B}(\it{\Xi}^{++}_{cc} \rightarrow D^{+} p K^{-} \pi^{+})}{\mathcal{B}(\it{\Xi}^{++}_{cc} \rightarrow \Lambda^{+}_{c} K^{-} \pi^{+}\pi^{+})}$ with $\mathcal{R} < 1.7 \hspace{2pt} (2.1) \times 10^{-2}$ at the 90% (95%) confidence level at the known mass of the $\it{\Xi}^{++}_{cc}$ state. ,"A search for $\it{\Xi}^{++}_{cc} \rightarrow D^{+} p K^{-} \pi^{+}$
  decays"
164,1124544564998447104,3244807924,Simone Langhans,"['Is #ArtificialIntelligence an enabler or inibitor for achieving #SDGs? Both, as we find in our new study: <LINK>\n@BC3Research @Zoology_Otago @MIT_CBMM @LeibnizIGB and more']",https://arxiv.org/abs/1905.00501,"The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors across the society requires an assessment of its effect on sustainable development. Here we analyze published evidence of positive or negative impacts of AI on the achievement of each of the 17 goals and 169 targets of the 2030 Agenda for Sustainable Development. We find that AI can support the achievement of 128 targets across all SDGs, but it may also inhibit 58 targets. Notably, AI enables new technologies that improve efficiency and productivity, but it may also lead to increased inequalities among and within countries, thus hindering the achievement of the 2030 Agenda. The fast development of AI needs to be supported by appropriate policy and regulation. Otherwise, it would lead to gaps in transparency, accountability, safety and ethical standards of AI-based technology, which could be detrimental towards the development and sustainable use of AI. Finally, there is a lack of research assessing the medium- and long-term impacts of AI. It is therefore essential to reinforce the global debate regarding the use of AI and to develop the necessary regulatory insight and oversight for AI-based technologies. ","The role of artificial intelligence in achieving the Sustainable
  Development Goals"
165,1135569335269957633,115426969,Francesco Orabona,"['We (Ashok Cutkosky and yours truly) have found a way to use momentum to do variance-reduction in nonconvex SGD. Paper here: <LINK> \nThis means no double-loops, no large batches, no checkpoints. Also, as usual, we also have adaptive learning rates. Thread: 1/6', 'Recently, variance reduction was used in nonconvex SGD to obtain optimal convergence to stationary points(https://t.co/DSDzvEYhY5 https://t.co/IRYump7Glb). However,these results have the shortcomings of variance reduced methods:double-loops,checkpoints,tons of hyperapameters. 2/6', 'Also, variance-reduction methods are great on paper, however in practice they require a lot of fiddling with hyperparameters and it is unclear how much advantage they give in nonconvex optimization (https://t.co/MlRmgcZEmT). 3/6', 'Instead, we show that the same variance-reduction effect can be achieved with the use of a modified momentum and no need for reference gradients at all. Also, given our hate for hyperparameters, we also use adaptive learning rates, to achieve automatic adaptation to the noise 4/6', 'How we do it? We use a novel Lyapunov function for the analysis of the algorithm, composed by a time-varying sum of the classic potential function for nonsmooth optimization and the error in the variance reduced update. 5/6', 'While our algorithm is not precisely the same as the SGD with momentum, we feel that it provides strong intuitive evidence that the standard momentum is performing some kind of variance reduction.\nFeeback is welcome! 6/6']",https://arxiv.org/abs/1905.10018,"Variance reduction has emerged in recent years as a strong competitor to stochastic gradient descent in non-convex problems, providing the first algorithms to improve upon the converge rate of stochastic gradient descent for finding first-order critical points. However, variance reduction techniques typically require carefully tuned learning rates and willingness to use excessively large ""mega-batches"" in order to achieve their improved results. We present a new algorithm, STORM, that does not require any batches and makes use of adaptive learning rates, enabling simpler implementation and less hyperparameter tuning. Our technique for removing the batches uses a variant of momentum to achieve variance reduction in non-convex optimization. On smooth losses $F$, STORM finds a point $\boldsymbol{x}$ with $\mathbb{E}[\|\nabla F(\boldsymbol{x})\|]\le O(1/\sqrt{T}+\sigma^{1/3}/T^{1/3})$ in $T$ iterations with $\sigma^2$ variance in the gradients, matching the optimal rate but without requiring knowledge of $\sigma$. ",Momentum-Based Variance Reduction in Non-Convex SGD
166,1134025341452902400,1702174146,Janis Keuper,"['We found a very simple but effective way to stabilize #CNN based #GANs. Preliminary results on  <LINK> #DeepLearning <LINK>', 'code also available on https://t.co/ECuwPDNnjc']",https://arxiv.org/abs/1905.12534,"Motivated by recently published methods using frequency decompositions of convolutions (e.g. Octave Convolutions), we propose a novel convolution scheme to stabilize the training and reduce the likelihood of a mode collapse. The basic idea of our approach is to split convolutional filters into additive high and low frequency parts, while shifting weight updates from low to high during the training. Intuitively, this method forces GANs to learn low frequency coarse image structures before descending into fine (high frequency) details. We also show, that the use of the proposed soft octave convolutions reduces common artifacts in the frequency domain of generated images. Our approach is orthogonal and complementary to existing stabilization methods and can simply be plugged into any CNN based GAN architecture. Experiments on the CelebA dataset show the effectiveness of the proposed method. ",Stabilizing GANs with Soft Octave Convolutions
167,1133841519759093760,1091045790242533376,Mariya Toneva,"['Excited to share joint work with Leila Wehbe on using brain recordings of people reading naturalistic text to interpret long-range context information in the intermediate layers of BERT, Transformer-XL, ELMo, and USE. We were most surprised to find..(1/2) <LINK>', '..that removing the pretrained attention in the shallow BERT layers results in better brain predictions and applying this insight to syntactic NLP tasks improved performance by up to 8%. Very excited for this transfer of insight from language in the brain to language in machines!']",https://arxiv.org/abs/1905.11833,"Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination. ","Interpreting and improving natural-language processing (in machines)
  with natural language-processing (in the brain)"
168,1126123509275607040,504239269,Vu Nguyen,"['""Knowing The What But Not The Where in Bayes Opt""\n\nFor some opt problems, we observe the optimum value y* in advance and the goal is to find the optimum input x*.\n\n@maosbot and I propose a new approach to exploit the knowledge of y* to find x* efficiently. <LINK> <LINK>']",https://arxiv.org/abs/1905.02685,"Bayesian optimization has demonstrated impressive success in finding the optimum input x* and output f* = f(x*) = max f(x) of a black-box function f. In some applications, however, the optimum output f* is known in advance and the goal is to find the corresponding optimum input x*. In this paper, we consider a new setting in BO in which the knowledge of the optimum output f* is available. Our goal is to exploit the knowledge about f* to search for the input x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization. We show that our approaches work intuitively and give quantitatively better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available. ",Knowing The What But Not The Where in Bayesian Optimization
169,1125292111841189888,927629300228116480,Maarja Bussov,['Bivariate J-function is a great tool to study spatial clustering between two types of objects. In our paper we used the function for photometric galaxies and filamentary spines detected from spectroscopic galaxies. The clustering is clearly seen!\n<LINK>'],https://arxiv.org/abs/1905.00912,"Galaxy filaments are the dominant feature in the overall structure of the cosmic web. The study of the filamentary web is an important aspect in understanding galaxy evolution and the evolution of matter in the Universe. A map of the filamentary structure is an adequate probe of the web. We propose that photometric redshift galaxies are significantly positively associated with the filamentary structure detected from the spatial distribution of spectroscopic redshift galaxies. The catalogues of spectroscopic and photometric galaxies are seen as point-process realisations in a sphere, and the catalogue of filamentary spines is proposed to be a realisation of a random set in a sphere. The positive association between these sets was studied using a bivariate $J-$function, which is a summary statistics studying clustering. A quotient $D$ was built to estimate the distance distribution of the filamentary spine to galaxies in comparison to the distance distribution of the filamentary spine to random points in $3-$dimensional Euclidean space. This measure gives a physical distance scale to the distances between filamentary spines and the studied sets of galaxies. The bivariate $J-$function shows a statistically significant clustering effect in between filamentary spines and photometric redshift galaxies. The quotient $D$ confirms the previous result that smaller distances exist with higher probability between the photometric galaxies and filaments. The trend of smaller distances between the objects grows stronger at higher redshift. Additionally, the quotient $D$ for photometric galaxies gives a rough estimate for the filamentary spine width of about $1$~Mpc. Photometric redshift galaxies are positively associated with filamentary spines detected from the spatial distribution of spectroscopic galaxies. ",Photometric redshift galaxies as tracers of the filamentary network
