,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1172233340079861760,1171123275595997187,Naoko Sawada,['We have an accepted short paper for IEEE VIS 2019. We presented a new visualization for a large collection of time series data. You can refer the preprint version of it from arXiv (<LINK>).\n#ieeevis'],https://arxiv.org/abs/1908.05505,"Comparing many long time series is challenging to do by hand. Clustering time series enables data analysts to discover relevance between and anomalies among multiple time series. However, even after reasonable clustering, analysts have to scrutinize correlations between clusters or similarities within a cluster. We developed SAX Navigator, an interactive visualization tool, that allows users to hierarchically explore global patterns as well as individual observations across large collections of time series data. Our visualization provides a unique way to navigate time series that involves a ""vocabulary of patterns"" developed by using a dimensionality reduction technique,Symbolic Aggregate approXimation(SAX). With SAX, the time series data clusters efficiently and is quicker to query at scale. We demonstrate the ability of SAX Navigator to analyze patterns in large time series data based on three case studies for an astronomy data set. We verify the usability of our system through a think-aloud study with an astronomy domain scientist. ",SAX Navigator: Time Series Exploration through Hierarchical Clustering
1,1171430801021390852,921990368207437824,Justin Ball,"[""New paper published: <LINK> It's a bit long, but is readable for #FusionEnergy hobbyists? It points out a new fusion fuel cycle, argues that it possesses the largest energy density of ANY known fuel source and speculates that it may help with #SpaceTravel someday <LINK>""]",https://arxiv.org/abs/1908.00834,"Specific energy (i.e. energy per unit mass) is one of the most fundamental and consequential properties of a fuel source. In this work, a systematic study of measured fusion cross-sections is performed to determine which reactions are potentially feasible and identify the fuel cycle that maximizes specific energy. This reveals that, by using normal hydrogen to breed deuterium via neutron capture, the conventional catalyzed D-D fusion fuel cycle can attain a specific energy greater than anything else. Simply surrounding a catalyzed D-D reactor with water enables deuterium fuel, the dominant stockpile of energy on Earth, to produce as much as 65% more energy. Lastly, the impact on space propulsion is considered, revealing that an effective exhaust velocity exceeding that of deuterium-helium-3 is theoretically possible. ",Maximizing specific energy by breeding deuterium
2,1171083540773179396,221292469,Richard Everitt,"['New paper with @felipe_mdn: ""Revisiting the balance heuristic for estimating normalising constants"" <LINK>']",https://arxiv.org/abs/1908.06514,"Multiple importance sampling estimators are widely used for computing intractable constants due to its reliability and robustness. The celebrated balance heuristic estimator belongs to this class of methods and has proved very successful in computer graphics. The basic ingredients for computing the estimator are: a set of proposal distributions, indexed by some discrete label, and a predetermined number of draws from each of these proposals. However, if the number of available proposals is much larger than the number of permitted importance points, one needs to select, possibly at random, which of these distributions will be used. The focus of this work lies within the previous context, exploring some improvements and variations of the balance heuristic via a novel extended-space representation of the estimator, leading to straightforward annealing schemes for variance reduction purposes. In addition, we also look at the intractable scenario where the proposal density is only available as a joint function with the discrete label, as may be encountered in problems where an ordering is imposed. For this case, we look at combinations of correlated unbiased estimators which also fit into the extended-space representation and, in turn, will provide other interesting solutions. ",Revisiting the balance heuristic for estimating normalising constants
3,1171019292495757312,929390967970508800,Erik B Myklebust,"['We have submitted a new paper about enabling semantic data access in ecological risk assessment @NIVAforskning, preprint available at <LINK>, with @ejimenez_ruiz @ChenJiaoyan1 @derboyausleu']",https://arxiv.org/abs/1908.10128,"Ecological risk assessment requires large amounts of chemical effect data from laboratory experiments. Due to experimental effort and animal welfare concerns it is desired to extrapolate data from existing sources. To cover the required chemical effect data several data sources need to be integrated to enable their interoperability. In this paper we introduce the Toxicological Effect and Risk Assessment (TERA) knowledge graph, which aims at providing such integrated view, and the data preparation and steps followed to construct this knowledge graph. We also present the applications of TERA for chemical effect prediction and the potential applications within the Semantic Web community. ",TERA: the Toxicological Effect and Risk Assessment Knowledge Graph
4,1169737138428162048,72885746,Xia ‚ÄúBen‚Äù Hu,['Please check out our new survey paper if you are interested in Fairness in AI at <LINK> <LINK>'],https://arxiv.org/abs/1908.08843,"Deep learning is increasingly being used in high-stake decision making applications that affect individual lives. However, deep learning models might exhibit algorithmic discrimination behaviors with respect to protected groups, potentially posing negative impacts on individuals and society. Therefore, fairness in deep learning has attracted tremendous attention recently. We provide a review covering recent progresses to tackle algorithmic fairness problems of deep learning from the computational perspective. Specifically, we show that interpretability can serve as a useful ingredient to diagnose the reasons that lead to algorithmic discrimination. We also discuss fairness mitigation approaches categorized according to three stages of deep learning life-cycle, aiming to push forward the area of fairness in deep learning and build genuinely fair and reliable deep learning systems. ",Fairness in Deep Learning: A Computational Perspective
5,1169690389579800581,1071548796,CƒÉtƒÉlina Cangea,"['Really excited to finally share our new paper! w/ @ebelilov, Pietro Li√≤, @AaronCourville\n\nVideoNavQA: Bridging the Gap between Visual and Embodied Question Answering (<LINK>)\n+ A benchmark in an alternate EQA-like setting\n+ Generalized VQA-style models', 'We tackle the Embodied QA task from a different perspective, where navigation paths are provided and the focus shifts towards answering much more complex and varied questions about the environment. https://t.co/C4svLQUTJC', 'Our motivation: initial EQA studies use IL+RL, but results show that the task might be too challenging for these methods.\n\nWe propose a novel way of evaluating EQA feasibility, building the VideoNavQA dataset containing pairs of questions and videos generated in House3D. https://t.co/IXZNOPEYjv', 'We generalize widely adopted VQA-style models including FiLM &amp; MAC to a temporal and rich visual setting.\n\nDataset and code available, full results in the paper! :-) https://t.co/CWqD3MELO3 https://t.co/96X0Hi8CgG', 'Work partially carried out during my research internship at @MILAMontreal!']",http://arxiv.org/abs/1908.04950,"Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm. ","VideoNavQA: Bridging the Gap between Visual and Embodied Question
  Answering"
6,1168870605569048576,119837224,Jason Baldridge,"['New EMNLP paper posted: @yinfeiy, Yuan Zhang, Chris Tar and me: ""PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification."" Extends the PAWS dataset with human translations in six languages! \n\nPaper: <LINK>\nData: <LINK>']",https://arxiv.org/abs/1908.11828,"Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information. ","PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase
  Identification"
7,1168752993996423168,766277095479795712,Mahdi Javanmardi,"[""Our new paper is up on @arXiv, titled ‚ÄúHow far should self-driving cars 'see'? Effect of observation range on vehicle self-localization.‚Äù\n\n<LINK> \n#AutonomousVehicles #selfdriving <LINK>"", '@naeimeh_1985 @arxiv €å⁄©€å ÿßÿ≤ ÿ¥ÿ±⁄©ÿ™ Ÿáÿß€å ÿ™ŸàŸÑ€åÿØ ⁄©ŸÜŸÜÿØŸá:\nhttps://t.co/1kFkQ2rfq8\nÿ∏ÿßŸáÿ±ÿß ÿØÿ± €å⁄©€å ÿßÿ≤ Ÿæÿßÿ±⁄©€åŸÜ⁄ØŸáÿß€å ŸÅÿ±ŸàÿØ⁄ØÿßŸá Saint-Exupery Airport ÿØÿ± ŸÑ€åŸàŸÜ ŸÅÿ±ÿßŸÜÿ≥Ÿá ÿØÿßÿ±Ÿá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€åÿ¥Ÿá', '@naeimeh_1985 @arxiv ŸÜÿ¥ŸÜ€åÿØŸÖ ÿ™ÿß ÿ≠ÿßŸÑÿß ÿ±ÿßÿ≥ÿ™ÿ¥ÿå ⁄ÜŸàŸÜ ÿß€åŸÜÿ¨ÿß Ÿæÿßÿ±⁄©€åŸÜ⁄Ø ÿßÿ™ŸàŸÖÿßÿ≥€åŸàŸÜ ŸÖÿ±ÿ≥ŸàŸÖ Ÿáÿ≥ÿ™ ŸàŸÑ€å ÿ®Ÿá ÿß€åŸÜ ÿ¥⁄©ŸÑ ÿ®ÿπ€åÿØ ŸÖ€åÿØŸàŸÜŸÖ', '@naeimeh_1985 @arxiv ÿßÿ™ŸàŸÖÿßÿ≥€åŸàŸÜ ŸÜŸá ŸáŸÖŸàŸÜ ŸÖ⁄©ÿßŸÜ€åÿ≤Ÿá üòÖ']",http://arxiv.org/abs/1908.06588v1,"Accuracy and time efficiency are two essential requirements for the self-localization of autonomous vehicles. While the observation range considered for simultaneous localization and mapping (SLAM) has a significant effect on both accuracy and computation time, its effect is not well investigated in the literature. In this paper, we will answer the question: How far should a driverless car observe during self-localization? We introduce a framework to dynamically define the observation range for localization to meet the accuracy requirement for autonomous driving, while keeping the computation time low. To model the effect of scanning range on the localization accuracy for every point on the map, several map factors were employed. The capability of the proposed framework was verified using field data, demonstrating that it is able to improve the average matching time from 142.2 ms to 39.3 ms while keeping the localization accuracy around 8.1 cm. ","] How far should self-driving cars see? Effect of observation range on
  vehicle self-localization"
8,1168455226489282564,574364219,David Garcia,['How emotions drive opinion polarization: An agent-based model. New paper with @ETHZSystDesign \n<LINK>'],https://arxiv.org/abs/1908.11623,"We provide an agent-based model to explain the emergence of collective opinions not based on feedback between different opinions, but based on emotional interactions between agents. The driving variable is the emotional state of agents, characterized by their valence and their arousal. Both determine their emotional expression, from which collective emotional information is generated. This information feeds back on the dynamics of emotional states and of individual opinions in a non-linear manner. We derive the critical conditions for emotional interactions to obtain either consensus or polarization of opinions. Stochastic agent-based simulations and formal analyses of the model explain our results. Possible ways to validate the model are discussed. ",How emotions drive opinion polarization: An agent-based model
9,1168418430875140096,1115880604560691200,NII Yamagishi Lab,"['Our new SSW paper, ""Initial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments,"" is now available on arXiv:  <LINK>']",https://arxiv.org/abs/1908.11535,"End-to-end text-to-speech (TTS) synthesis is a method that directly converts input text to output acoustic features using a single network. A recent advance of end-to-end TTS is due to a key technique called attention mechanisms, and all successful methods proposed so far have been based on soft attention mechanisms. However, although network structures are becoming increasingly complex, end-to-end TTS systems with soft attention mechanisms may still fail to learn and to predict accurate alignment between the input and output. This may be because the soft attention mechanisms are too flexible. Therefore, we propose an approach that has more explicit but natural constraints suitable for speech signals to make alignment learning and prediction of end-to-end TTS systems more robust. The proposed system, with the constrained alignment scheme borrowed from segment-to-segment neural transduction (SSNT), directly calculates the joint probability of acoustic features and alignment given an input text. The alignment is designed to be hard and monotonically increase by considering the speech nature, and it is treated as a latent variable and marginalized during training. During prediction, both the alignment and acoustic features can be generated from the probabilistic distributions. The advantages of our approach are that we can simplify many modules for the soft attention and that we can train the end-to-end TTS model using a single likelihood function. As far as we know, our approach is the first end-to-end TTS without a soft attention mechanism. ","Initial investigation of an encoder-decoder end-to-end TTS framework
  using marginalization of monotonic hard latent alignments"
10,1167460812690206720,742867776092672004,Lucas Bignone,"['Very happy to share our new paper on the optical morphologies of EAGLE galaxies. <LINK>. In collaboration with @su9000, @trayfud, @PatriciaBTisser and Leonardo Pellizza <LINK>']",http://arxiv.org/abs/1908.10936,"We study the optical morphology of galaxies in a large-scale hydrodynamic cosmological simulation, the EAGLE simulation. Galaxy morphologies were characterized using non-parametric statistics (Gini, $M_{20}$, Concentration and Asymmetry) derived from mock images computed using a 3D radiative transfer technique and post-processed to approximate observational surveys. The resulting morphologies were contrasted to observational results from a sample of $\log_{10}(M_{*}/M_\odot) > 10$ galaxies at $z \sim 0.05$ in the GAMA survey. We find that the morphologies of EAGLE galaxies reproduce observations, except for asymmetry values which are larger in the simulated galaxies. Additionally, we study the effect of spatial resolution in the computation of non-parametric morphologies, finding that Gini and Asymmetry values are systematically reduced with decreasing spatial resolution. Gini values for lower mass galaxies are especially affected. Comparing against other large scale simulations, the non-parametric statistics of EAGLE galaxies largely agree with those found in IllustrisTNG. Additionally, EAGLE galaxies mostly reproduce observed trends between morphology and star formation rate and galaxy size. Finally, We also find a significant correlation between optical and kinematic estimators of morphologies, although galaxy classification based on an optical or a kinematic criteria results in different galaxy subsets. The correlation between optical and kinematic morphologies is stronger in central galaxies than in satellites, indicating differences in morphological evolution. ",Non-parametric Morphologies of Galaxies in the EAGLE Simulation
11,1167459127016271872,322636963,Jonathan Berant,"['#emnlp2019 paper by @jonherzig. Originally titled ""Building a semantic parser that works overnight"", but we changed the name... Careful analysis of issues in ""overnight"" data collection method leads to a new and improved procedure! <LINK> <LINK>']",https://arxiv.org/abs/1908.09940,"A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection. ","Don't paraphrase, detect! Rapid and Effective Data Collection for
  Semantic Parsing"
12,1167361191531995136,793871446049185792,Franziska Schmidt,"['New paper on the arxiv today by my colleague @FKirchschlager: ""Dust survival rates in clumps passing through the Cas A reverse shock I: results for a range of clump densities"", accepted by MNRAS (<LINK>)!\nsee this thread for a tl;dr! üòÄ', 'The origin of cosmic dust (especially in the early universe) is still highly uncertain with one potential source being core-collapse supernovae (massive stars that collapse and then explode at the end of their lives, forming beautiful remnants such as Cassiopeia A). [Img: Wiki] https://t.co/A6kOsBx2XB', 'Observations of such remnants have shown that dust is indeed produced in the inner regions of these remnants, however once formed the dust can easily be destroyed again through interactions with the so-called reverse shock', '(a shock formed by interactions between the expanding remnant and the interstellar material around it). Exactly how much of the dust manages to survive under these conditions is currently not well-constraint.', ""To investigate dust destruction processes in supernova remnants, we are using hydrodynamics simulations (such as this one: https://t.co/DfrdTuDLiM) representing the remnant combined with Dr Kirchschlager's dust post-processing code PAPERBOATS."", 'PAPERBOATS comes with a more complete suite of physics than previous studies including drag effects and destruction by either sputtering (grain erosion due to interactions between dust and gas) or collisions (grain growth or destruction due to interactions between grains).', 'We find that within a reasonable parameter ranges up to 30% of the carbon dust can survive the remnants and the results show quite clearly that grain-grain collisions (which are often neglected in similar studies) play a crucial role and should be considered in realistic models. https://t.co/HlrbLqNO7B']",https://arxiv.org/abs/1908.10875,"The reverse shock in the ejecta of core-collapse supernovae is potentially able to destroy newly formed dust material. In order to determine dust survival rates, we have performed a set of hydrodynamic simulations using the grid-based code AstroBEAR in order to model a shock wave interacting with clumpy supernova ejecta. Dust motions and destruction rates were computed using our newly developed external, post-processing code Paperboats, which includes gas drag, grain charging, sputtering and grain-grain collisions. We have determined dust destruction rates for the oxygen-rich supernova remnant Cassiopeia A as a function of initial grain sizes and clump gas density. We found that up to 30 % of the carbon dust mass is able to survive the passage of the reverse shock if the initial grain size distribution is narrow with radii around ~10 - 50 nm for high gas densities, or with radii around ~0.5 - 1.5 ${\mu}$m for low and medium gas densities. Silicate grains with initial radii around 10 - 30 nm show survival rates of up to 40 % for medium and high density contrasts, while silicate material with micron sized distributions is mostly destroyed. For both materials, the surviving dust mass is rearranged into a new size distribution that can be approximated by two components: a power-law distribution of small grains and a log-normal distribution of grains having the same size range as the initial distribution. Our results show that grain-grain collisions and sputtering are synergistic and that grain-grain collisions can play a crucial role in determining the surviving dust budget in supernova remnants. ","Dust survival rates in clumps passing through the Cas A reverse shock I:
  results for a range of clump densities"
13,1167234941756878848,401510411,Jos√© Figueroa,['New paper arXived!  <LINK>'],https://arxiv.org/abs/1908.11278,"We classify $N{=}1$ $d=4$ kinematical and aristotelian Lie superalgebras with spatial isotropy, but not necessarily parity nor time-reversal invariance. Employing a quaternionic formalism which makes rotational covariance manifest and simplifies many of the calculations, we find a list of $43$ isomorphism classes of Lie superalgebras, some with parameters, whose (nontrivial) central extensions are also determined. We then classify their corresponding simply-connected homogeneous $(4|4)$-dimensional superspaces, resulting in a list of $27$ homogeneous superspaces, some with parameters, all of which are reductive. We determine the invariants of low rank and explore how these superspaces are related via geometric limits. ",Kinematical superspaces
14,1167058977953390593,701638582587371520,Alejandro Perdomo-Ortiz,['Fresh out of the oven! :).Check out our new paper comparing #quantum and classical #MachineLearning models when modeling probability distributions constructed from subsets of the stock market. A benchmark readily implementable in ion-trap quantum computers <LINK> <LINK>'],https://arxiv.org/abs/1908.10778,"Although several models have been proposed towards assisting machine learning (ML) tasks with quantum computers, a direct comparison of the expressive power and efficiency of classical versus quantum models for datasets originating from real-world applications is one of the key milestones towards a quantum ready era. Here, we take a first step towards addressing this challenge by performing a comparison of the widely used classical ML models known as restricted Boltzmann machines (RBMs), against a recently proposed quantum model, now known as quantum circuit Born machines (QCBMs). Both models address the same hard tasks in unsupervised generative modeling, with QCBMs exploiting the probabilistic nature of quantum mechanics and a candidate for near-term quantum computers, as experimentally demonstrated in three different quantum hardware architectures to date. To address the question of the performance of the quantum model on real-world classical data sets, we construct scenarios from a probabilistic version out of the well-known portfolio optimization problem in finance, by using time-series pricing data from asset subsets of the S\&P500 stock market index. It is remarkable to find that, under the same number of resources in terms of parameters for both classical and quantum models, the quantum models seem to have superior performance on typical instances when compared with the canonical training of the RBMs. Our simulations are grounded on a hardware efficient realization of the QCBMs on ion-trap quantum computers, by using their native gate sets, and therefore readily implementable in near-term quantum devices. ","Classical versus Quantum Models in Machine Learning: Insights from a
  Finance Application"
15,1166930461022543872,1115880604560691200,NII Yamagishi Lab,"['Our new paper, ""Neural Harmonic-plus-Noise Waveform Model with Trainable Maximum Voice Frequency for Text-to-Speech Synthesis,"" is available on arXiv:  <LINK>']",https://arxiv.org/abs/1908.10256,"Neural source-filter (NSF) models are deep neural networks that produce waveforms given input acoustic features. They use dilated-convolution-based neural filter modules to filter sine-based excitation for waveform generation, which is different from WaveNet and flow-based models. One of the NSF models, called harmonic-plus-noise NSF (h-NSF) model, uses separate pairs of source and neural filters to generate harmonic and noise waveform components. It is close to WaveNet in terms of speech quality while being superior in generation speed. The h-NSF model can be improved even further. While h-NSF merges the harmonic and noise components using pre-defined digital low- and high-pass filters, it is well known that the maximum voice frequency (MVF) that separates the periodic and aperiodic spectral bands are time-variant. Therefore, we propose a new h-NSF model with time-variant and trainable MVF. We parameterize the digital low- and high-pass filters as windowed-sinc filters and predict their cut-off frequency (i.e., MVF) from the input acoustic features. Our experiments demonstrated that the new model can predict a good trajectory of the MVF and produce high-quality speech for a text-to-speech synthesis system. ","Neural Harmonic-plus-Noise Waveform Model with Trainable Maximum Voice
  Frequency for Text-to-Speech Synthesis"
16,1166896891314880512,1120626288807428096,Jonathan Scarlett,"['New paper uploaded: ""Information-Theoretic Lower Bounds for Compressive Sensing with Generative Models"" <LINK>', 'The idea:  Construct ReLU networks that produce ""group sparse"" signals (1 non-zero per block), and apply minimax lower bounds for those.  Hence, there exist (deep/wide/both/neither) generative ReLU networks that produce signals that are hard to estimate.']",https://arxiv.org/abs/1908.10744,"It has recently been shown that for compressive sensing, significantly fewer measurements may be required if the sparsity assumption is replaced by the assumption the unknown vector lies near the range of a suitably-chosen generative model. In particular, in (Bora {\em et al.}, 2017) it was shown roughly $O(k\log L)$ random Gaussian measurements suffice for accurate recovery when the generative model is an $L$-Lipschitz function with bounded $k$-dimensional inputs, and $O(kd \log w)$ measurements suffice when the generative model is a $k$-input ReLU network with depth $d$ and width $w$. In this paper, we establish corresponding algorithm-independent lower bounds on the sample complexity using tools from minimax statistical analysis. In accordance with the above upper bounds, our results are summarized as follows: (i) We construct an $L$-Lipschitz generative model capable of generating group-sparse signals, and show that the resulting necessary number of measurements is $\Omega(k \log L)$; (ii) Using similar ideas, we construct ReLU networks with high depth and/or high depth for which the necessary number of measurements scales as $\Omega\big( kd \frac{\log w}{\log n}\big)$ (with output dimension $n$), and in some cases $\Omega(kd \log w)$. As a result, we establish that the scaling laws derived in (Bora {\em et al.}, 2017) are optimal or near-optimal in the absence of further assumptions. ","Information-Theoretic Lower Bounds for Compressive Sensing with
  Generative Models"
17,1166783080679399424,917889967233179648,Geoff Bacon,"[""New paper evaluating BERT's syntactic competence across 26 languages: <LINK>""]",https://arxiv.org/abs/1908.09892,"Learning representations that accurately model semantics is an important goal of natural language processing research. Many semantic phenomena depend on syntactic structure. Recent work examines the extent to which state-of-the-art models for pre-training representations, such as BERT, capture such structure-dependent phenomena, but is largely restricted to one phenomenon in English: number agreement between subjects and verbs. We evaluate BERT's sensitivity to four types of structure-dependent agreement relations in a new semi-automatically curated dataset across 26 languages. We show that both the single-language and multilingual BERT models capture syntax-sensitive agreement patterns well in general, but we also highlight the specific linguistic contexts in which their performance degrades. ","Does BERT agree? Evaluating knowledge of structure dependence through
  agreement relations"
18,1166676502462771206,540931079,Bahareh Tolooshams,['Our new paper on deep learning with random/compressed measurements of images to appear in IEEE MLSP. We call it RandNet. <LINK>'],https://arxiv.org/abs/1908.09258,"Principal component analysis, dictionary learning, and auto-encoders are all unsupervised methods for learning representations from a large amount of training data. In all these methods, the higher the dimensions of the input data, the longer it takes to learn. We introduce a class of neural networks, termed RandNet, for learning representations using compressed random measurements of data of interest, such as images. RandNet extends the convolutional recurrent sparse auto-encoder architecture to dense networks and, more importantly, to the case when the input data are compressed random measurements of the original data. Compressing the input data makes it possible to fit a larger number of batches in memory during training. Moreover, in the case of sparse measurements,training is more efficient computationally. We demonstrate that, in unsupervised settings, RandNet performs dictionary learning using compressed data. In supervised settings, we show that RandNet can classify MNIST images with minimal loss in accuracy, despite being trained with random projections of the images that result in a 50% reduction in size. Overall, our results provide a general principled framework for training neural networks using compressed data. ",RandNet: deep learning with compressed measurements of images
19,1166657094608728069,1031546969209143296,Nelly Papalampidi,"['Our #EMNLP2019 paper ""Movie Plot Analysis via Turning Point Identification"" w/ @frank_e_keller and @mlapata  is now on Arxiv.\n<LINK>\n\nWe aim at facilitating processing long and complex narratives -&gt; New task: Turning Point (TP) identification in movies <LINK>', 'TPs are key events in narratives that describe the plotline and segment it into thematic units: We aim at identifying these events in full length screenplays and argue that this narrative structure analysis can facilitate tasks such as summarization and QA for narratives.', 'We introduce the TRIPOD dataset and propose an end-to-end model that finds TP events in synopses and project them into screenplays. Our model outperforms baselines based on sota sentence representations and the expected position of these events.']",https://arxiv.org/abs/1908.10328,"According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay: they define the plot structure, determine its progression and segment the screenplay into thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in movies as a means of analyzing their narrative structure. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as screenplays, for summarization and question answering. We introduce a dataset consisting of screenplays and plot synopses annotated with turning points and present an end-to-end neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays. Our model outperforms strong baselines based on state-of-the-art sentence representations and the expected position of turning points. ",Movie Plot Analysis via Turning Point Identification
20,1166653609372766208,1191190070,Naman Goel,['New paper with Cyril van Schreven and Boi Faltings\n\nInfochain: A Decentralized System for Truthful Information Elicitation\n\n<LINK>\n\n#blockchain #ethereum #AI #DataScience #data'],https://arxiv.org/abs/1908.10258,"Blockchain based systems allow various kinds of financial transactions to be executed in a decentralized manner. However, these systems often rely on a trusted third party (oracle) to get correct information about the real-world events, which trigger the financial transactions. In this paper, we identify two biggest challenges in building decentralized, trustless and transparent oracles. The first challenge is acquiring correct information about the real-world events without relying on a trusted information provider. We show how a peer-consistency incentive mechanism can be used to acquire truthful information from an untrusted and self-interested crowd, even when the crowd has outside incentives to provide wrong information. The second is a system design and implementation challenge. For the first time, we show how to implement a trustless and transparent oracle in Ethereum. We discuss various non-trivial issues that arise in implementing peer-consistency mechanisms in Ethereum, suggest several optimizations to reduce gas cost and provide empirical analysis. ","Infochain: A Decentralized, Trustless and Transparent Oracle on
  Blockchain"
21,1166334749729857536,185910194,Graham Neubig,"['#EMNLP2019 paper ""A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource NER"", examines cross-lingual transfer and active learning to create NER systems in new languages: <LINK> <LINK>', 'Interestingly, even a little annotation is enough to get quite good results, only a few 100 or 1000 entities. Differences between simulation and human annotation are also interesting.']",https://arxiv.org/abs/1908.08983,"Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now several proposed approaches involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. This paper poses the question: given this recent progress, and limited human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we find a dual-strategy approach best, starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. ","A Little Annotation does a Lot of Good: A Study in Bootstrapping
  Low-resource Named Entity Recognizers"
22,1166325024371957760,621147651,zpenoyre,"[""New paper out today / we've invented a new kind of space elevator / one that we can build with modern materials / please consider sharing, it's a project worth considering in earnest: <LINK> <LINK>"", ""@hippke @EmSandford Happy to try - this was an independent genesis of an idea which may well have more overlap with the existing literature than I'm aware (though I've tried my best to find out what already exists)"", ""@hippke @EmSandford The basic concept - extending an elevator from the lunar surface, and in doing so being able to construct it relatively cheaply out of materials already available - was new to me, though I have since found some other mentions of lunar-space elevators (there's a wiki page)"", '@hippke @EmSandford What surprised me was how achievable a protect this is - and how much could be gained from doing it - the existing literature seems to focus on future materials and vast translunar transport systems', '@hippke @EmSandford But a minimal cable - primarily used to support a Lagrange point base camp - which could be deployed in years not decades, for billions not trillions - seems to be unexplored as a concept', '@hippke @EmSandford (also this is very much the theorist in me talking - but I think an independent derivation of a set of equations is always worth putting out there)', '@hippke @EmSandford It may be that there are communities who have explored this in detail - and I look forward to hearing from them - the academic web is incomplete and hard to traverse - who knows what lurks out there']",https://arxiv.org/abs/1908.09339,"Perhaps the biggest hurdle to mankind's expansion throughout the Solar System is the prohibitive cost of escaping Earth's gravitational pull. In its many forms, the space-elevator provides a way to circumvent this cost, allowing payloads to traverse along a cable extending from Earth to orbit. However, modern materials are not strong enough to build a cable capable of supporting its own weight. In this work we present an alternative to the classic space elevator, within reach of modern technology: The Spaceline. By extending a line, anchored on the moon, to deep within Earth's gravity well, we can construct a stable, traversable cable allowing free movement from the vicinity of Earth to the Moon's surface. With current materials, it is feasible to build a cable extending to close to the height of geostationary orbit, allowing easy traversal and construction between the Earth and the Moon. ","The Spaceline: a practical space elevator alternative achievable with
  current technology"
23,1166158438990110721,2337598033,Geraint F. Lewis,"['New paper on the arXiv - ‚ÄúIs there a cosmological basis for E=mc^2?‚Äù - I‚Äôll save you the effort - Answer is no. \n\n<LINK> <LINK>', '@robivison There is this classic https://t.co/NksuMMGOno']",https://arxiv.org/abs/1908.09267,"It has recently been claimed that relativity's most famous equation, E = mc^2, has a cosmological basis, representing the gravitational binding energy for a particle to escape from the origin to a gravitational horizon of the universe. In this paper, I examine these claims in detail, concluding that they result from a misinterpretation of motion of particles in the cosmological space-time, and an incorrect application of 4-vectors. Finally, I demonstrate that the origin of E = mc^2 comes from its usual relativistic interpretation, namely that it is the energy of a particle as seen in its own rest-frame. ",Is there a Cosmological Basis for E = mc^2?
24,1165792253224214528,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper appeared on arXiv today. An extremely large number of e-folds is possible in a simple single-field inflation where the potential has a shallow local min around the hilltop. Then, light scalars can reach the Bunch-Davies distribution. <LINK>']",https://arxiv.org/abs/1908.08694,"We propose a class of single-field, slow-roll inflation models in which a typical number of $e$-folds can be extremely large. The key point is to introduce a very shallow local minimum near the top of the potential in a hilltop inflation model. In particular, a typical number of $e$-folds is enhanced if classical behavior dominates around the local minimum such that the inflaton probability distribution is drifted to the local minimum as a whole. After the inflaton escapes from the local minimum due to the stochastic dynamics, the ordinary slow-roll inflation follows and it can generate the primordial density perturbation consistent with observation. Interestingly, our scenario inherits the advantages of the old and new inflation: the typical $e$-folds can be extremely large as in the old inflation, and slow-roll inflation naturally follows after the stochastic regime as in the new inflation. In our numerical example, the typical number of $e$-folds can be as large as $10^{10^{10}}$, which is large enough for various light scalars such the QCD axion to reach the Bunch-Davies distribution. ",Stochastic inflation with an extremely large number of $e$-folds
25,1165335175787683840,322636963,Jonathan Berant,"[""New #emnlp2019 paper by @megamor2 and with @yoavgo  about annotator bias. We check whether models capture properties of the annotators rather than the task when annotators create language utterances at scale. You'll never guess what we found out! :) <LINK>""]",https://arxiv.org/abs/1908.07898,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. ","Are We Modeling the Task or the Annotator? An Investigation of Annotator
  Bias in Natural Language Understanding Datasets"
26,1165105109409263617,1037208112896376832,Zhipeng Cai,"['[Check out our new work for global optimal robust estimation]: ""Consensus maximization tree search revisited"" accepted to ICCV\'19 for oral presentation. Paper link: <LINK>\nCode available at: <LINK>']",https://arxiv.org/abs/1908.02021,"Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach. ",Consensus Maximization Tree Search Revisited
27,1164934950052720640,349215461,Ken Shen,"['1/ New paper submitted: <LINK>!  Thread follows.', '2/ A new class of supernovae was discovered 10 years ago.  They have a few interesting properties, such as their atmospheres showing a lot more calcium than oxygen, which is different from other explosions.  They are also very common: roughly 1/3 the rate of Type Ia supernovae!', '3/ But their most interesting feature is that they happen very far from galaxies.  Other supernovae happen where there are lots of other stars.  But these explosions happen in the distant outskirts of galaxies, where there are very few stars.', ""4/ This is a really strong constraint on what could be causing them.  It has to be a mechanism that doesn't occur inside galaxies and is instead restricted to some population of stars that mostly exists near the outside of galaxies."", '5/ We did our best to come up with populations of stars that only exist way out there.  One answer is very old stars that haven\'t been polluted by other stellar explosions (""metal-poor"" stars).  But we couldn\'t think of a good scenario that was restricted to these kinds of stars.', '6/ Our other guess is globular clusters.  These are extremely compact groups of stars (as much as 10 million times denser than average).  Our best idea is that whatever makes these explosions is formed in an exotic way due to the extreme stellar density inside globular clusters.', ""7/ Average stars like the Sun don't ever interact strongly with other stars unless they're born as binary systems.  But stars inside globular clusters interact all the time because they're so close to each other."", ""8/ So maybe these explosions are caused by interactions between stars that wouldn't normally interact with each other.  Our best guesses all involve compact stars called white dwarfs interacting with other stars."", '9/ During those interactions, helium can undergo runaway thermonuclear fusion.  And those helium bombs in space might be what causes these new explosions.', ""10/10 We don't have the definitive answer yet, but hopefully we are going in the right direction.  Science isn't usually a nice, linear story.  We fumble around, trying various things until something sticks.  And then we keep sticking those things together until we make progress!""]",https://arxiv.org/abs/1908.08056,"A new class of faint, spectroscopically peculiar transients has emerged in the last decade. We term these events ""calcium-strong transients"" (CaSTs) because of their atypically high calcium-to-oxygen nebular line ratios. Previous studies have struggled to deduce the identity of their progenitors due to a combination of their extremely extended radial distributions with respect to their host galaxies and their relatively high rate of occurrence. In this work, we find that the CaST radial distribution is consistent with the radial distribution of two populations of stars: old (ages > 5 Gyr), low-metallicity (Z/Zsol < 0.3) stars and globular clusters. While no obvious progenitor scenario arises from considering old, metal-poor stars, the alternative production site of globular clusters leads us to narrow down the list of possible candidates to three binary scenarios: mergers of helium and oxygen/neon white dwarfs; tidal disruptions of helium white dwarfs by neutron stars; and stable accretion from low-mass helium-burning stars onto white dwarfs. While rare in the field, these binary systems can be formed dynamically at much higher rates in globular clusters. Subsequent binary hardening both increases their interaction rate and ejects them from their parent globular clusters prior to mass transfer contact. Their production in, and ejection from, globular clusters may explain their radial distribution and the absence of globular clusters at their explosion site. This model predicts a currently undiscovered high rate of CaSTs in nuclear star clusters. Alternatively, an undetermined progenitor scenario involving old, low-metallicity stars may instead hold the key to understanding CaSTs. ",The Progenitors of Calcium-Strong Transients
28,1164844446170701824,4794678126,Mahdi Nasiri,['Very excited to announce our new paper:\n\nTime series model selection with a meta-learning approach; evidence from a pool of forecasting algorithms\n\n(w/@avinarhoubare and @MehrdadRst95)\n<LINK>'],http://arxiv.org/abs/1908.08489,"One of the challenging questions in time series forecasting is how to find the best algorithm. In recent years, a recommender system scheme has been developed for time series analysis using a meta-learning approach. This system selects the best forecasting method with consideration of the time series characteristics. In this paper, we propose a novel approach to focusing on some of the unanswered questions resulting from the use of meta-learning in time series forecasting. Therefore, three main gaps in previous works are addressed including, analyzing various subsets of top forecasters as inputs for meta-learners; evaluating the effect of forecasting error measures; and assessing the role of the dimensionality of the feature space on the forecasting errors of meta-learners. All of these objectives are achieved with the help of a diverse state-of-the-art pool of forecasters and meta-learners. For this purpose, first, a pool of forecasting algorithms is implemented on the NN5 competition dataset and ranked based on the two error measures. Then, six machine-learning classifiers known as meta-learners, are trained on the extracted features of the time series in order to assign the most suitable forecasting method for the various subsets of the pool of forecasters. Furthermore, two-dimensionality reduction methods are implemented in order to investigate the role of feature space dimension on the performance of meta-learners. In general, it was found that meta-learners were able to defeat all of the individual benchmark forecasters; this performance was improved even after applying the feature selection method. ","Time series model selection with a meta-learning approach; evidence from
  a pool of forecasting algorithms"
29,1164698872939188224,382008009,Miles Cranmer,"['New paper with @richardgalvez, Lauren Anderson, @DavidSpergel and @cosmo_shirley! Check it out: <LINK>. \n \nWe use a ""normalizing flow"" to model Gaia\'s color-magnitude diagram, and generate a catalog of distances to 640 million nearby stars. <LINK>']",https://arxiv.org/abs/1908.08045,"We demonstrate an algorithm for learning a flexible color-magnitude diagram from noisy parallax and photometry measurements using a normalizing flow, a deep neural network capable of learning an arbitrary multi-dimensional probability distribution. We present a catalog of 640M photometric distance posteriors to nearby stars derived from this data-driven model using Gaia DR2 photometry and parallaxes. Dust estimation and dereddening is done iteratively inside the model and without prior distance information, using the Bayestar map. The signal-to-noise (precision) of distance measurements improves on average by more than 48% over the raw Gaia data, and we also demonstrate how the accuracy of distances have improved over other models, especially in the noisy-parallax regime. Applications are discussed, including significantly improved Milky Way disk separation and substructure detection. We conclude with a discussion of future work, which exploits the normalizing flow architecture to allow us to exactly marginalize over missing photometry, enabling the inclusion of many surveys without losing coverage. ","Modeling the Gaia Color-Magnitude Diagram with Bayesian Neural Flows to
  Constrain Distance Estimates"
30,1164665133890592768,1137569442404036609,Daniel Liu,"['New paper üì∞:\nAdversarial point perturbations on 3D objects\n<LINK>\n\nBlog post: <LINK>\n\n*FOUR* novel #adversarial attacks against #3D #pointcloud #DeepLearning!\n\n#security #MachineLearning\n\n1/10 <LINK>', 'Thanks to Ronald Yu and Hao Su from @UCSanDiego for minimally mentoring me and providing GPUs even though I am a high school student!\n\nThis paper would not be possible without previous work by @goodfellow_ian @aleks_madry @NicolasPapernot @alexey2004 and many others!\n\n2/10', 'Idea üí°: shape-aware attacks with *just* unordered point sets.\n\nTwo categories:\n- Distributional: imperceptible; measures perturbation through Hausdorff metric.\n- Shape: global changes to shape; realistic and robust against point removal defenses.\n\n3/10 https://t.co/EKOSqIN1qO', 'Distributional attack: estimate shape of point set and use projected gradient descent to keep perturbations on the shape.\n\nShape estimation uses triangulation algorithms, and projection is sped up with a metric tree.\n\n4/10 https://t.co/nf3YqF5mkD', 'For reference, directly using gradient descent results in many outlier points floating around in mid-air:\n\n5/10 https://t.co/tcMVqQYSTx', 'Perturbation resampling attack: gradient descent + resampling points onto shape inferred through triangulation to maintain an even sampling of points on the shape.\n\n6/10 https://t.co/V73fsEiUWk', 'Adversarial sticks: perturb a few points and connect them to triangulated shape by resampling points, instead of the harder task of orienting sticks in 3D. This is easier to construct IRL.\n\n7/10 https://t.co/87ZqfUFGnc', 'Adversarial sinks: instead of perturbing single points, move ""black holes"" that attract points based on a radial basis function to change the shape. This is fully differentiable.\n\n8/10 https://t.co/hYzl6eOoou', 'Shape attacks perform much better than naive gradient descent (iter. gradient L_2) even when we remove up to half of the points (using previously proposed algorithms) as a defense.\n\n9/10 https://t.co/Yj5aItwIYV', 'Please read the blog post (https://t.co/FPbzLsT2Op) for a brief history of ideas in the field of point set adversarial attacks.\n\n10/10', '@UCSanDiego @goodfellow_ian @aleks_madry @NicolasPapernot @alexey2004 Also @ChrSzegedy for the first paper on adversarial examples!']",https://arxiv.org/abs/1908.06062,"The importance of training robust neural network grows as 3D data is increasingly utilized in deep learning for vision tasks in robotics, drone control, and autonomous driving. One commonly used 3D data type is 3D point clouds, which describe shape information. We examine the problem of creating robust models from the perspective of the attacker, which is necessary in understanding how 3D neural networks can be exploited. We explore two categories of attacks: distributional attacks that involve imperceptible perturbations to the distribution of points, and shape attacks that involve deforming the shape represented by a point cloud. We explore three possible shape attacks for attacking 3D point cloud classification and show that some of them are able to be effective even against preprocessing steps, like the previously proposed point-removal defenses. ",Adversarial shape perturbations on 3D point clouds
31,1164542112785997824,201811433,Ameet Talwalkar,"['Want to learn more about Federated Learning (i.e., privacy-preserving training in distributed, heterogenous networks)? Check out our new survey paper: <LINK>']",https://arxiv.org/abs/1908.07873,"Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities. ","Federated Learning: Challenges, Methods, and Future Directions"
32,1164528114170499073,3297640734,St√©phane K√©na-Cohen,"[""Ashutosh's paper finally on arXiv after patent submission. A new paradigm for grating design: <LINK>. A bit of ingenuity gives higher efficiency than AI-optimized gratings!""]",https://arxiv.org/abs/1908.07988,"Conventional surface-relief gratings are inefficient at deflecting normally-incident light by large angles. This constrains their use in many applications and limits the overall efficiency of any optical instrument integrating gratings. Here, we demonstrate a simple approach for the design of diffraction gratings that can be highly efficient for large deflection angles, while also offering additional functionality. The gratings are composed of a unit cell comprising a vertically-oriented asymmetric slot-waveguide. The unit cell shows oscillating unidirectional scattering behavior that can be precisely tuned as a function of the waveguide length. This occurs due to interference between multiple modes excited by the incident light. In contrast to metasurface-based gratings with multiple resonant sub-elements, a periodic arrangement of such non-resonant diffracting elements allows for broadband operation and a strong tolerance for variations in angle of incidence. Full-wave simulations show that our grating designs can exhibit diffraction efficiencies ranging from 94% for a deflection angle of 47$^\circ$ to 80% for deflection angle of 80$^\circ$. To demonstrate the multifunctionality of our grating design technique, we have also proposed a flat polarization beamsplitter, which allows for the separation of the two orthogonal polarizations by 80$^\circ$, with an efficiency of 80%. ","Large-Angle, Broadband and Multifunctional Gratings Based on Directively
  Radiating Waveguide Scatterers"
33,1164509296417263617,1049562817240674304,Marius Lindauer,"['Easily combining efficient multi-fidelity hyperparameter optimization (#HPO) and subsequent analysis of the results, now possible with #BOAH. Check out our new open-source #AutoML software paper: <LINK>']",https://arxiv.org/abs/1908.06756,"Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and HyperBand, and a comprehensive analysis of the optimization process and its outcomes. ","BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis
  of Hyperparameters"
34,1164453782975143937,81312314,Stephan Leitner,['In our new paper we present an agent-based version of the hidden-action problem and shift the attention from decision-influencing on decision-facilitating information. #econtwitter #ABM \n\nCheck out the results at #arXiv üëá\n\n<LINK>'],https://arxiv.org/abs/1908.07998,"The hidden-action model captures a fundamental problem of principal-agent theory and provides an optimal sharing rule when only the outcome but not the effort can be observed. However, the hidden-action model builds on various explicit and also implicit assumptions about the information of the contracting parties. This paper relaxes key assumptions regarding the availability of information included in the hidden-action model in order to study whether and, if so, how fast the optimal sharing rule is achieved and how this is affected by the various types of information employed in the principal-agent relation. Our analysis particularly focuses on information about the environment and about feasible actions for the agent. We follow an approach to transfer closed-form mathematical models into agent-based computational models and show that the extent of information about feasible options to carry out a task only has an impact on performance if decision makers are well informed about the environment, and that the decision whether to perform exploration or exploitation when searching for new feasible options only affects performance in specific situations. Having good information about the environment, on the contrary, appears to be crucial in almost all situations. ","Decision-facilitating information in hidden-action setups: An
  agent-based approach"
35,1164223578922979328,2584290836,David Glowacki,"['new arXiv paper by helen deeks, @beccawalters95,  @AdrianMulholla1 &amp; @mikeoconnor0308 showing that interactive MD in VR can generate binding/unbinding pathways that recover crystallographic poses. <LINK> <LINK>', '@share1992 @share1992 apologies for forgetting to include you in this tweet. i totally spaced.', 'and couldnt have done it without the dimensionality reduction analysis by @share1992']",https://arxiv.org/abs/1908.07395,"Simulating drug binding and unbinding is a challenge, as the rugged energy landscapes that separate bound and unbound states require extensive sampling that consumes significant computational resources. Here, we describe the use of interactive molecular dynamics in virtual reality (iMD-VR) as an accurate low-cost strategy for flexible protein-ligand docking. We outline an experimental protocol which enables expert iMD-VR users to guide ligands into and out of the binding pockets of trypsin, neuraminidase, and HIV-1 protease, and recreate their respective crystallographic protein-ligand binding poses within 5 - 10 minutes. Following a brief training phase, our studies shown that iMD-VR novices were able to generate unbinding and rebinding pathways on similar timescales as iMD-VR experts, with the majority able to recover binding poses within 2.15 Angstrom RMSD of the crystallographic binding pose. These results indicate that iMD-VR affords sufficient control for users to carry out the detailed atomic manipulations required to dock flexible ligands into dynamic enzyme active sites and recover crystallographic poses, offering an interesting new approach for simulating drug docking and generating binding hypotheses. ","Interactive molecular dynamics in virtual reality for accurate flexible
  protein-ligand docking"
36,1164172727722418176,2746722060,Tristan Smith,"['Our (me, @VivPoulin, and Mustafa Amin) new paper came out today: <LINK> - we show that a fully dynamical oscillating scalar field can resolve the Hubble tension and fits the Planck CMB measurements as well as LCDM!', 'We also point out that these models generically predict additional isocurvature perturbations in the field and, for some forms of the scalar field potential, can undergo self-resonance, leading to the growth of scalar field perturbations', 'We also find that if a scenario like this explains the Hubble tension then (near) future CMB experiments (like @SimonsObs and CMB-S4) will detect departures from LCDM at very high significance!', 'Its super exciting to work on a theoretical topic with predictions that can be seen with near future measurements!']",http://arxiv.org/abs/1908.06995,"We present a detailed investigation of a sub-dominant oscillating scalar field ('early dark energy', EDE) in the context of resolving the Hubble tension. Consistent with earlier work, but without relying on fluid approximations, we find that a scalar field frozen due to Hubble friction until ${\rm log}_{10}(z_c)\sim3.5$, reaching $\rho_{\rm EDE}(z_c)/\rho_{\rm tot}\sim10$%, and diluting faster than matter afterwards can bring cosmic microwave background (CMB), baryonic acoustic oscillations, supernovae luminosity distances, and the late-time estimate of the Hubble constant from the SH0ES collaboration into agreement. A scalar field potential which scales as $V(\phi) \propto \phi^{2n}$ with $2\lesssim n\lesssim 3.4$ around the minimum is preferred at the 68% confidence level, and the {\em Planck} polarization places additional constraints on the dynamics of perturbations in the scalar field. In particular, the data prefers a potential which flattens at large field displacements. An MCMC analysis of mock data shows that the next-generation CMB observations (i.e., CMB-S4) can unambiguously detect the presence of the EDE at very high significance. This projected sensitivity to the EDE dynamics is mainly driven by improved measurements of the $E$-mode polarization. We also explore new observational signatures of EDE scalar field dynamics: (i) We find that depending on the strength of the tensor-to-scalar ratio, the presence of the EDE might imply the existence of isocurvature perturbations in the CMB. (ii) We show that a strikingly rapid, scale-dependent growth of EDE field perturbations can result from parametric resonance driven by the anharmonic oscillating field for $n\approx 2$. This instability and ensuing potentially nonlinear, spatially inhomogenoues, dynamics may provide unique signatures of this scenario. ","Oscillating scalar fields and the Hubble tension: a resolution with
  novel signatures"
37,1163817153587367942,1131272636321845248,Sohtaro Kanda,['We have posted new article to arXiv. This paper describes a new experimental proposal to search for a time-reversal symmetry breaking process in muon decay and the Majorana neutrinos. <LINK>'],https://arxiv.org/abs/1908.01630,"We propose a new experiment to search for a time-reversal (T) symmetry breaking process in muon decay and the Majoranality of the neutrinos. In the presence of V+A interactions, the Majoranality appears as a T-violating term in the muon decay width as shown by Doi et al, while in the Standard Model such a T-violating term is negligibly small. The presences of V+A interactions and the corresponding heavy right-handed Majorana neutrinos give us an important clue to solve two major issues in particle physics, the deficit of baryon asymmetry in the universe and the Majoranality of neutrinos. In the experiment, the polarization of positrons from muon decays is measured using a polarimeter consisting of a magnetized foil and a segmented calorimeter. According to our result of numerical calculation, a factor of ten improvement in sensitivity to the T-violating process is expected by a year of measurement at J-PARC Materials and Life Science Experimental Facility, compared to the most recent precursor experiment. ",Hunting for T-violation and Majoranality of Neutrinos in Muon Decays
38,1163812978254155777,627909670,Anna M Nierenberg,['Narrow-line lensing lets you see dark matter in twice the number of lenses! See our companion paper tonight for how this lets us put awesome new constraints on WDM\n<LINK>'],https://arxiv.org/abs/1908.06344,"The magnifications of compact-source lenses are extremely sensitive to the presence of low mass dark matter halos along the entire sight line from the source to the observer. Traditionally, the study of dark matter structure in compact-source strong gravitational lenses has been limited to radio-loud systems, as the radio emission is extended and thus unaffected by microlensing which can mimic the signal of dark matter structure. An alternate approach is to measure quasar nuclear-narrow line emission, which is free from microlensing and present in virtually all quasar lenses. In this paper, we double the number of systems which can be used for gravitational lensing analyses by presenting measurements of narrow-line emission from a sample of 8 quadruply imaged quasar lens systems, WGD J0405-3308, HS 0810+2554, RX J0911+0551, SDSS J1330+1810, PS J1606-2333, WFI 2026-4536, WFI 2033-4723 and WGD J2038-4008. We describe our updated grism spectral modelling pipeline, which we use to measure narrow-line fluxes with uncertainties of 2-10\%, presented here. We fit the lensed image positions with smooth mass models and demonstrate that these models fail to produce the observed distribution of image fluxes over the entire sample of lenses. Furthermore, typical deviations are larger than those expected from macromodel uncertainties. This discrepancy indicates the presence of perturbations caused by small-scale dark matter structure. The interpretation of this result in terms of dark matter models is presented in a companion paper. ","Double dark matter vision: twice the number of compact-source lenses
  with narrow-line lensing and the WFC3 grism"
39,1163793700557926400,994489580643549184,Alexander Rothkopf,"['It‚Äôs a pleasure to announce our new paper on #quarkonium #dynamics in the #QGP as quantum Brownian motion\n\n<LINK>\n\n1st derivation of nonlinear Schr√∂dinger eq. for QQbar from microscopic #QCD. With T. Miura, Y. Akamatsu &amp; M. Asakawa from @osaka_univ @UniStavanger <LINK>']",http://arxiv.org/abs/1908.06293,"In this paper we study the real-time evolution of heavy quarkonium in the quark-gluon plasma (QGP) on the basis of the open quantum systems approach. In particular, we shed light on how quantum dissipation affects the dynamics of the relative motion of the quarkonium state over time. To this end we present a novel non-equilibrium master equation for the relative motion of quarkonium in a medium, starting from Lindblad operators derived systematically from quantum field theory. In order to implement the corresponding dynamics, we deploy the well established quantum state diffusion method. In turn we reveal how the full quantum evolution can be cast in the form of a stochastic non-linear Schr\""odinger equation. This for the first time provides a direct link from quantum chromodynamics (QCD) to phenomenological models based on non-linear Schr\""odinger equations. Proof of principle simulations in one-dimension show that dissipative effects indeed allow the relative motion of the constituent quarks in a quarkonium at rest to thermalize. Dissipation turns out to be relevant already at early times well within the QGP lifetime in relativistic heavy ion collisions. ",Quantum Brownian motion of a heavy quark pair in the quark-gluon plasma
40,1163783699206422530,14551614,Jason Weston,"['New EMNLP paper: \n\nMaking dialogue safer by asking humans to attack our models and learning from the experience!\n\nBuild it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack\n@em_dinan, Sam Humeau, B. Chintagunta, @jaseweston\n <LINK>', ""@em_dinan @emilymbender @haldaume3 @AllysonEttinger Thanks to @emilymbender @haldaume3 @AllysonEttinger Harita Kannan Sudha Rao Ephraim Rothschild for introducing Build It, Break It ideas to NLPers (or at least to us) at EMNLP '17""]",https://arxiv.org/abs/1908.06083,"The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gal\'an-Garc\'ia et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it strategy with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods will be made open source and publicly available. ","Build it Break it Fix it for Dialogue Safety: Robustness from
  Adversarial Human Attack"
41,1163760584489390080,4836585957,Shaaban M. Shaaban,['Do you want to learn about whistler heat-flux instability in the Solar wind? Read our new paper <LINK> <LINK>'],https://arxiv.org/abs/1908.06666v1,"In collision-poor plasmas from space, e.g., solar wind or stellar outflows, the heat-flux carried by the strahl or beaming electrons is expected to be regulated by the self-generated instabilities. Recently, simultaneous field and particle observations have indeed revealed enhanced whistler-like fluctuations in the presence of counter-beaming populations of electrons, connecting these fluctuations to the whistler heat-flux instability (WHFI). This instability is predicted only for limited conditions of electron beam-plasmas, and was not captured in numerical simulations yet. In this letter we report the first simulations of WHFI in particle-in-cell (PIC) setups, realistic for the solar wind conditions, and without temperature gradients or anisotropies to trigger the instability in the initiation phase. The velocity distributions have a complex reaction to the enhanced whistler fluctuations conditioning the instability saturation by a decrease of the relative drifts combined with induced (effective) temperature anisotropies (heating the core electrons and pitch-angle and energy scattering the strahl). These results are in good agreement with a recent quasilinear approach, and support therefore a largely accepted belief that WHFI saturates at moderate amplitudes. In anti-sunward direction the strahl becomes skewed with a pitch-angle distribution decreasing in width as electron energy increases, that seems to be characteristic to self-generated whistlers and not to small-scale turbulence. ","] Particle-in-cell simulations of the whistler heat-flux instability in
  the solar wind conditions"
42,1163691784926633984,157973000,Michael Pfarrhofer,['New working paper online on the measurement of international uncertainty shocks and their consequences @SCEUS_Salzburg  #econometrics\n<LINK>'],https://arxiv.org/abs/1908.06325,"This paper investigates the time-varying impacts of international macroeconomic uncertainty shocks. We use a global vector autoregressive specification with drifting coefficients and factor stochastic volatility in the errors to model six economies jointly. The measure of uncertainty is constructed endogenously by estimating a scalar driving the innovation variances of the latent factors, which is also included in the mean of the process. To achieve regularization, we use Bayesian techniques for estimation, and introduce a set of hierarchical global-local priors. The adopted priors center the model on a constant parameter specification with homoscedastic errors, but allow for time-variation if suggested by likelihood information. Moreover, we assume coefficients across economies to be similar, but provide sufficient flexibility via the hierarchical prior for country-specific idiosyncrasies. The results point towards pronounced real and financial effects of uncertainty shocks in all countries, with differences across economies and over time. ","Measuring international uncertainty using global vector autoregressions
  with drifting parameters"
43,1163639392939970562,268919557,Joshua T. Vogelstein (jovo/he/we),"['new paper out on how to detect whether your 2 (multivariate) time-series are independent of one another: \n<LINK>\n@TingsterX @HopkinsKavli @JHUBME @KordingLab @AToliasLab @neurowitz', '@KordingLab @TingsterX @HopkinsKavli @JHUBME @AToliasLab @neurowitz @KordingLab all pairs of ""natural events"" are *statistically* dependent (barring quantum weirdness), so all applications of dependence testing ever are weird? i don\'t think you mean that? can you please clarify?', '@neuropoetic @KordingLab @TingsterX @HopkinsKavli @JHUBME @AToliasLab @neurowitz yes, that is why we also compute effect size, and the optimal lag of dependence.', '@KordingLab @neuropoetic @TingsterX @HopkinsKavli @JHUBME @AToliasLab @neurowitz @KordingLab a ""statistic"" is a magnitude, the null distribution tells us the probability (under those assumptions) of observing a magnitude more extreme than the observed.  so, all tests quantify the amount, by construction.  right?', ""@KordingLab @neuropoetic @TingsterX @HopkinsKavli @JHUBME @AToliasLab @neurowitz i'm not sure which questions you are referring to @KordingLab , but i bet we could design an experiment with high power for it?""]",https://arxiv.org/abs/1908.06486,"Complex data structures such as time series are increasingly present in modern data science problems. A fundamental question is whether two such time-series are statistically dependent. Many current approaches make parametric assumptions on the random processes, only detect linear association, require multiple tests, or forfeit power in high-dimensional, nonlinear settings. Estimating the distribution of any test statistic under the null is non-trivial, as the permutation test is invalid. This work juxtaposes distance correlation (Dcorr) and multiscale graph correlation (MGC) from independence testing literature and block permutation from time series analysis to address these challenges. The proposed nonparametric procedure is valid and consistent, building upon prior work by characterizing the geometry of the relationship, estimating the time lag at which dependence is maximized, avoiding the need for multiple testing, and exhibiting superior power in high-dimensional, low sample size, nonlinear settings. Neural connectivity is analyzed via fMRI data, revealing linear dependence of signals within the visual network and default mode network, and nonlinear relationships in other networks. This work uncovers a first-resort data analysis tool with open-source code available, directly impacting a wide range of scientific disciplines. ",Independence Testing for Multivariate Time Series
44,1163464667903094784,2279967715,Michael Tremmel,"['New paper day! The formation of ultra-diffuse galaxies (UDGs) in galaxy clusters using RomulusC, one of only two simulations in the world able to resolve dwarf galaxies in such a dense environment within a cosmological simulation. #nbodyshopgotchu\n\n<LINK>', ""Shoutout to co-authors  Anna Wright (@anchwr), Alyson Brooks, Ferah Munshi (@fdmtweets), Daisuke Nagai, and Tom Quinn. #nbodyshopexcellence\n\nBelow I'll give a summary of our results. For some background check out my tweet from Friday\n\nhttps://t.co/yK0Wgpafro"", 'We search RomulusC, which includes hundreds of galaxies, for ones that fit a typical definition of UDG laid out by observers: large galaxies w/ low central surface brightness (SB). We find 122 UDGs in our simulation! A few example images (left: SB profiles with Sersic fit [red]) https://t.co/es0FFgQGEL', 'Note: we are not the first to form such low surface brightness, puffy galaxies in sims. We *are* the first to do so within a simulation of a galaxy cluster, a dense collection of many galaxies and the largest gravitationally bound structures in the Universe. (image from RomulusC) https://t.co/yeGmFqvkHg', 'The first ? we ask is: are UDGs special? Well, not really. Given their sizes, stellar mass, total brightness (Mg), and central surface brightness (mu0) they represent a continuation of the overall dwarf galaxy population, rather than a well-defined, separate population', 'These figures plot size vs stellar mass, central surface brightness (mu0), and total galaxy luminosity (Mg). We compare UDGs (orange), non-UDGs (blue), and simulated isolated dwarf galaxies from our other simulation, Romulus25 (black), as well as observations (red line, diamonds) https://t.co/7XSWGgMneb', 'Another important note for the above figures is that we match nicely to the observed UDG population in the Coma cluster (diamonds), which is encouraging that we actually have a realistic population on our hands in the simulation!', 'We also look at the dark matter (DM) content of UDGs. Tracking each galaxy back in time before they interact strongly with the cluster environment (when their dark matter halos reach their max mass) we find UDGs exist in DM halos with masses typical for their stellar mass. https://t.co/OTTr2J85EO', 'Once they interact with the dense environment of the cluster, their DM gets taken away, decreasing their halo mass (compare small to large points above). Contrary to prev work, we find the angular momentum (or ""spin"") of their halos also is also typical for a dwarf galaxy.', 'So, how do UDGs come to be and why do we find them so often in cluster environments? To figure this out we split our population of dwarf galaxies (Mstar &lt; 10^9 Msun) into three mass bins and track the evolution of their relevant properties', 'This plots the effective radius (i.e. size) of the galaxies over time relative to the time at which each galaxy first falls into the cluster. The average tracks are shown in thick lines and the shaded regions the standard deviation. https://t.co/BZvSIWbFZO', ""Concl: \n\n1) Massive dwarfs are *all* large enough to be UDGs (dashed line is the min size) \n\n2) at lower masses UDGs are larger than non-UDGs and they've typically been that way long before falling into the cluster. So, the cluster itself does very little to determine their sizes"", 'While the cluster is not having a direct, dynamical influence on dwarf sizes (no dramatic change post-infall), it *does* affect the evolution in their central surface brightness. This time we plot relative to the time at which each galaxy stops forming stars (t_quench). https://t.co/NgptNlWVxx', ""Before quenching (i.e. no more star formation) all lie well above the dashed line (maximum surface brightness to be a UDG). After quenching, surface brightness drops w/time. The star-shaped points are the final values. The longer it's been since quenching the dimmer they are."", 'This is what makes the cluster important. Dwarf galaxies in isolation are typically still forming stars today, but when they interact with the dense cluster environment their gas gets removed because of ram pressure as the galaxies move through the hot intracluster gas.', 'Plotted is (top) a map of gas density at 4 times for the progenitor to the most massive UDG in our sample just as it falls into the cluster. Ram pressure removes gas and star formation (bottom) decreases to 0 within ~1 billion yrs post in-fall. Dashed lines correspond to 4 images https://t.co/etrW3ZGnWQ', 'Once the galaxy quenches, its stellar population evolves. The once very bright, massive stars evolve and die, causing the surface brightness to decrease over time. Plotted below are SB profiles for the same galaxy as above. Post quenching, SB decreases steadily https://t.co/pJe3pyOCUP', 'This ""passive evolution"" resulting in low SB is also something we see in isolated dwarfs. Here is the relationship between the mass-weighted stellar age within 1 kpc and the central surface brightness for cluster galaxies (orange, blue) and isolated dwarfs (black) from Romulus25. https://t.co/D35jaAlJcG', 'Old stars = lower central SB (mu_0). It is much rarer for dwarfs to be quenched for so long in isolation. The cluster, however, is able to quench dwarfs very easily. This plot of quenched fraction vs stellar mass for cluster and isolated galaxies is from a prev paper of mine https://t.co/xPlpioSlxd', 'There is also observational evidence for this formation channel of UDGs. Here is a plot from Javier Rom√°n\nand Ignacio Trujillo\'s 2017 paper showing example SB profiles of ""true"" UDGs (red) and ""blue"" UDGs, the latter being higher SB than what is considered ""UDG-like"". https://t.co/Mvx6WZGxNO', 'What they show in this plot is that if you take this ""blue"" UDG population and passively evolve it over 6 Gyrs, you end up with surface brightness profiles very similar to the ""true"" or ""red"" UDG population.', 'So, to conclude, UDGs are not super special intrinsically. Their properties are consistent with being simply the low surface brightness continuation of the low mass galaxy population. They form so readily in clusters because they are so often quenched in such dense environments.', ""There are still many caveats and work to be done. These are very high res simulations, but still we need to consider how resolution effects may enhance the UDG population in RomulusC. Additionally, our res isn't high enough to resolve the effects of supernovae outflows on DM."", 'There is also the question of ""weird"" UDG properties that have been observed. Some seem to lack dark matter. Some have very odd, enhanced populations of globular clusters. We don\'t address these in this work, but hope to in the future!', ""Some immediate future work I'm involved in is being led by Anna Wright (@anchwr) looking at the formation of UDGs in other environments using the Romulus25 simulation. Right now Anna is studying the unique origins of UDGs in the field (i.e. relatively isolated). Stay tuned!""]",https://arxiv.org/abs/1908.05684v1,"We study the origins of 122 ultra-diffuse galaxies (UDGs) in the RomulusC zoom-in cosmological simulation of a galaxy cluster (M$_{200} = 1.15 \times 10^{14}$ M$_{\odot}$), one of the only such simulations capable of resolving the evolution and structure of dwarf galaxies (M$_{\star} < 10^9$ M$_{\odot}$). We find broad agreement with observed cluster UDGs and predict that they are not separate from the overall cluster dwarf population. UDGs in cluster environments form primarily due to the quenching of star formation from ram pressure stripping and the subsequent passive evolution of their stellar population which results in very low surface brightness dwarf galaxies. We predict that there is little difference between UDGs and non-UDGs in terms of their dark matter halo masses and spins, their $z = 0$ colors, nor their evolution over time. UDGs are typically larger dwarf galaxies well before in-fall into the cluster and have had their star formation quenched for longer, typically due to entering the cluster earlier. We find that in most respects cluster UDG and non-UDGs alike are similar to isolated dwarf galaxies, although they are typically larger in size. This is due, in part, to the fact that cluster dwarf galaxies grow from higher angular momentum gas compared to isolated dwarf galaxies. ","] The Formation of Ultra-Diffuse Galaxies from Passive Evolution in the
  RomulusC Galaxy Cluster Simulation"
45,1163438346389086208,1138144242533044225,Andrew S Maxwell,"['Check out our new paper, a method for detecting #parity in #atoms and #molecules using #photoelectron #holography. With both experimental and theoretical demonstrations.\n<LINK>\n#photoelectronholography', 'For more information on #photoelectronholography see our review and previous publications:\nhttps://t.co/NfNOREgoR5\nhttps://t.co/HU1zxUpc24\nhttps://t.co/gH4zw9kRyL\nhttps://t.co/OvjG6gdoI3']",https://arxiv.org/abs/1908.03860,"We introduce a novel and concise methodology to detect the parity of atomic and molecular orbitals based on photoelectron holography, which is more general than the existing schemes. It fully accounts for the Coulomb distortions of electron trajectories, does not require sculpted fields to retrieve phase information and, in principle, is applicable to a broad range of electron momenta. By comparatively measuring the differential photoelectron spectra from strong-field ionization of N$_{2}$ molecules and their companion atoms of Ar, some photoelectron holography patterns are found to be dephased for both targets. This is well reproduced by the full-dimensional time-dependent Schr\""{o}dinger equation and the Coulomb quantum-orbit strong-field approximation (CQSFA) simulation. Using the CQSFA, we trace back our observations to different parities of the 3$p$ orbital of Ar and the highest-occupied molecular orbital of N$_{2}$ via interfering Coulomb-distorted quantum orbits carrying different initial phases. This method could in principle be used to extract bound-state phases from any holographic structure, with a wide range of potential applications in recollision physics and spectroscopy. ",Holographic detection of parity in atomic and molecular orbitals
46,1163379258393153537,1140553960006119424,Bo Zhang,"['Excited to present our newest work SCARLET-NAS, which argues the scalability problem for one-shot NAS. The auto-generated models surpass EfficientNet-B0 and mark a new state-of-the-art at the level of 400M FLOPs. Paper: <LINK> Github:<LINK> <LINK>']",https://arxiv.org/abs/1908.06022,"To discover powerful yet compact models is an important goal of neural architecture search. Previous two-stage one-shot approaches are limited by search space with a fixed depth. It seems handy to include an additional skip connection in the search space to make depths variable. However, it creates a large range of perturbation during supernet training and it has difficulty giving a confident ranking for subnetworks. In this paper, we discover that skip connections bring about significant feature inconsistency compared with other operations, which potentially degrades the supernet performance. Based on this observation, we tackle the problem by imposing an equivariant learnable stabilizer to homogenize such disparities. Experiments show that our proposed stabilizer helps to improve the supernet's convergence as well as ranking performance. With an evolutionary search backend that incorporates the stabilized supernet as an evaluator, we derive a family of state-of-the-art architectures, the SCARLET series of several depths, especially SCARLET-A obtains 76.9% top-1 accuracy on ImageNet. Code is available at this https URL ","SCARLET-NAS: Bridging the Gap between Stability and Scalability in
  Weight-sharing Neural Architecture Search"
47,1163302455913795589,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper is out today. <LINK>\nWe showed that the initial position of the QCD axion can be set close to pi, if it has a mixing with another heavy axion which gives a phase shift of pi. It can be the inflaton. So we named it, ""pi-nflation"".', 'The basic idea of giving a phase shift of pi was given by Daido and us in 1702.03284. We were inspired by\nmany interesting talks and lively discussion at CERN-Korea TH workshop on axions (https://t.co/Y87xflY5UP) to revisit this idea and study it in detail.']",https://arxiv.org/abs/1908.06071,"We show that the initial misalignment angle of the QCD axion (or axion-like particles) can be set very close to $\pi$, if the QCD axion has a mixing with another heavy axion which induces the phase shift $\approx \pi$ after inflation. In the simplest case, the heavy axion plays the role of the inflaton, and we call such inflation as ""$\pi$nflation."" The basic idea was first proposed by Daido and the present authors in Ref. [1702.03284] in 2017 and more recently discussed in Ref. [1903.00462]. We show that the QCD axion with a decay constant $f_a \gtrsim 3 \times 10^9\,$ GeV can explain dark matter by the $\pi$nflation mechanism. A large fraction of the parameter region has an overlap with the projected sensitivity of ORGAN, MADMAX, TOORAD and IAXO. We also study implications for the effective neutrino species and isocurvature perturbations. The $\pi$nflation can provide an initial condition for the hilltop inflation in the axion landscape, and in a certain set-up, a chain of the hilltop inflation may take place. ",QCD Axion on Hilltop by a Phase Shift of $\pi$
48,1163252951965523968,2337598033,Geraint F. Lewis,"['A new paper on the arXiv with PD student, Florian List, Nik Iwanus and Pascal Elahi - ‚ÄúA Novel Scheme for Dark Matter Annihilation Feedback in Cosmological Simulations‚Äù <LINK> <LINK>']",https://arxiv.org/abs/1908.05812,"We present a new self-consistent method for incorporating dark matter annihilation feedback (DMAF) in cosmological N-body simulations. The power generated by DMAF is evaluated at each dark matter (DM) particle which allows for flexible energy injection into the surrounding gas based on the specific DM annihilation model under consideration. Adaptive, individual time steps for gas and DM particles are supported and a new time-step limiter, derived from the propagation of a Sedov--Taylor blast wave, is introduced. We compare this donor-based approach with a receiver-based approach used in recent studies and illustrate the differences by means of a toy example. Furthermore, we consider an isolated halo and a cosmological simulation and show that for these realistic cases, both methods agree well with each other. The extension of our implementation to scenarios such as non-local energy injection, velocity-dependent annihilation cross-sections, and DM decay is straightforward. ","A Novel Scheme for Dark Matter Annihilation Feedback in Cosmological
  Simulations"
49,1162773165497626624,2284016282,Dustin Lang,['Our latest CHIME paper reporting 8 new Repeating Fast Radio Bursts got some press!  The paper is <LINK> <LINK>'],https://arxiv.org/abs/1908.03507,"We report on the discovery of eight repeating fast radio burst (FRB) sources found using the Canadian Hydrogen Intensity Mapping Experiment (CHIME) telescope. These sources span a dispersion measure (DM) range of 103.5 to 1281 pc cm$^{-3}$. They display varying degrees of activity: six sources were detected twice, another three times, and one ten times. These eight repeating FRBs likely represent the bright and/or high-rate end of a distribution of infrequently repeating sources. For all sources, we determine sky coordinates with uncertainties of $\sim$10$^\prime$. FRB 180916.J0158+65 has a burst-averaged DM = $349.2 \pm 0.3$ pc cm$^{-3}$ and a low DM excess over the modelled Galactic maximum (as low as $\sim$20 pc cm$^{-3}$); this source also has a Faraday rotation measure (RM) of $-114.6 \pm 0.6$ rad m$^{-2}$, much lower than the RM measured for FRB 121102. FRB 181030.J1054+73 has the lowest DM for a repeater, $103.5 \pm 0.3$ pc cm$^{-3}$, with a DM excess of $\sim$ 70 pc cm$^{-3}$. Both sources are interesting targets for multi-wavelength follow-up due to their apparent proximity. The DM distribution of our repeater sample is statistically indistinguishable from that of the first 12 CHIME/FRB sources that have not repeated. We find, with 4$\sigma$ significance, that repeater bursts are generally wider than those of CHIME/FRB bursts that have not repeated, suggesting different emission mechanisms. Our repeater events show complex morphologies that are reminiscent of the first two discovered repeating FRBs. The repetitive behavior of these sources will enable interferometric localizations and subsequent host galaxy identifications. ",CHIME/FRB Detection of Eight New Repeating Fast Radio Burst Sources
50,1162362712225136640,17373048,Rodrigo Nemmen,['New paper by Raniere: Optical characterization of WISE selected blazar candidates <LINK> <LINK> \nRaniere is a phd student in the group'],https://arxiv.org/abs/1908.05229,"Over the last decade more than five thousand gamma-ray sources were detected by the Large Area Telescope (LAT) on board Fermi Gamma-ray Space Telescope. Given the positional uncertainty of the telescope, nearly 30% of these sources remain without an obvious counterpart in lower energies. This motivated the release of new catalogs of gamma-ray counterpart candidates and several follow up campaigns in the last decade. Recently, two new catalogs of blazar candidates were released, they are the improved and expanded version of the WISE Blazar-Like Radio-Loud Sources (WIBRaLS2) catalog and the Kernel Density Estimation selected candidate BL Lacs (KDEBLLACS) catalog, both selecting blazar-like sources based on their infrared colors from the Wide-field Infrared Survey Explorer (WISE). In this work we characterized these two catalogs, clarifying the true nature of their sources based on their optical spectra from SDSS data release 15, thus testing how efficient they are in selecting true blazars. We first selected all WIBRaLS2 and KDEBLLACS sources with available optical spectra in the footprint of Sloan Digital Sky Survey data release 15. Then we analyzed these spectra to verify the nature of each selected candidate and see which fraction of the catalogs is composed by spectroscopically confirmed blazars. Finally, we evaluated the impact of selection effects, specially those related to optical colors of WIBRaLS2/KDEBLLACS sources and their optical magnitude distributions. We found that at least ~ 30% of each catalog is composed by confirmed blazars, with quasars being the major contaminants in the case of WIBRaLS2 (~ 58%) and normal galaxies in the case of KDEBLLACS (~ 38.2%). The spectral analysis also allowed us to identify the nature of 11 blazar candidates of uncertain type (BCUs) from the Fermi-LAT 4th Point Source Catalog (4FGL) and to find 25 new BL Lac objects. ",Optical characterization of WISE selected blazar candidates
51,1162215111529865216,169937939,Xiongzhi Chen,['New paper on FDR control for sparse configuration of grouped hypotheses: <LINK>'],https://arxiv.org/abs/1908.05319,"False discovery rate (FDR) control in structured hypotheses testing is an important topic in simultaneous inference. Most existing methods that aim to utilize group structure among hypotheses either employ the groupwise mixture model or weight all p-values or hypotheses. Thus, their powers can be improved when the groupwise mixture model is inappropriate or when most groups contain only true null hypotheses. Motivated by this, we propose a grouped, selectively weighted FDR procedure, which we refer to as ""sGBH"". Specifically, without employing the groupwise mixture model, sGBH identifies groups of hypotheses of interest, weights p-values in each such group only, and tests only the selected hypotheses using the weighted p-values. The sGBH subsumes a standard grouped, weighted FDR procedure which we refer to as ""GBH"". We provide simple conditions to ensure the conservativeness of sGBH, together with empirical evidence on its much improved power over GBH. The new procedure is applied to a gene expression study. ","A grouped, selectively weighted false discovery rate procedure"
52,1162000037540519936,77036349,Alice Booth,"['New paper out today! @almaobs @physicsleedsuni \nFirst detection of 13C17O in a Protoplanetary Disk: a Robust Tracer of Disk Mass\n<LINK>\nTo summarise:\n- The mass of a disk is its most fundamental quantity as this determines its planet formation potential', '- Observations of CO are used to determine disk gas mass but they are only a reliable if the line emission is optically thin \n- We have detected the rarest CO isotopologue, 13C17O, in a disk for the first time\n(Image: HD163296 continuum, Andrews+2018, 10au scale bar) https://t.co/CiTBI3xTNX', '- The 13C17O emission is optically thin within the CO snowline therefore it is tracing the midplane gas whereas the C18O is not\n- We have revealed 2-6 times more mass in the disk than previous observations with C18O \n(Images: 13C17O J=3-2 moment 0 and 1 maps) https://t.co/Zvw1Oq7bN2', '- The new disk mass in 0.21 solar masses (at 101.5pc)\n- Targeting the rarer CO isotopologues in gas rich disks is key in locating the missing mass we need to build planets\nThanks @cwalshastrochem @johnilee @SN_star_ppd and others üåü']",https://arxiv.org/abs/1908.05045,"Measurements of the gas mass are necessary to determine the planet formation potential of protoplanetary disks. Observations of rare CO isotopologues are typically used to determine disk gas masses; however, if the line emission is optically thick this will result in an underestimated disk mass. With ALMA we have detected the rarest stable CO isotopologue, 13C17O, in a protoplanetary disk for the first time. We compare our observations with the existing detections of 12CO, 13CO, C18O and C17O in the HD163296 disk. Radiative transfer modelling using a previously benchmarked model, and assuming interstellar isotopic abundances, significantly underestimates the integrated intensity of the 13C17O J=3-2 line. Reconciliation between the observations and the model requires a global increase in CO gas mass by a factor of 3.5. This is a factor of 2-6 larger than previous gas mass estimates using C18O. We find that C18O emission is optically thick within the CO snow line, while the 13C17O emission is optically thin and is thus a robust tracer of the bulk disk CO gas mass. ","The First Detection of 13C17O in a Protoplanetary Disk: a Robust Tracer
  of Disk Gas Mass"
53,1161956394666528770,554306311,Oliver Shorttle,['How do planets receive their inventory of sulfur?  Our new paper led by @kamatahvel  takes a look at this <LINK> @EarthSciCam @cambridge_astro'],https://arxiv.org/abs/1908.05169,"Sulfur is one of the most abundant elements in the Universe, with important roles in astro-, geo-, and biochemistry. Its main reservoirs in planet-forming disks have previously eluded detection: gaseous molecules only account for $<1\,$\% of total elemental sulfur, with the rest likely in either ices or refractory minerals. Mechanisms such as giant planets can filter out dust from gas accreting onto disk-hosting stars. For stars above 1.4 solar masses, this leaves a chemical signature on the stellar photosphere that can be used to determine the fraction of each element that is locked in dust. Here, we present an application of this method to sulfur, zinc, and sodium. We analyse the accretion-contaminated photospheres of a sample of young stars and find $(89\pm8)\,$\% of elemental sulfur is in refractory form in their disks. The main carrier is much more refractory than water ice, consistent with sulfide minerals such as FeS. ",Abundant refractory sulfur in protoplanetary disks
54,1161891365585899522,72030668,Gilles Louppe,"['New paper by my student @WehenkelAntoine: Unconstrained Monotonic Neural Networks! <LINK> In this work, we show how to make use of free-form neural networks to model arbitrary monotonic functions and derive a new autoregressive flow. <LINK>', 'The UMNN architecture parameterizes a monotonic function F by its strictly positive derivative f modeled as a NN, which only constraint is the positiveness of its output.  The evaluation of F is then carried out by integrating f using Clenshaw-Curtis quadrature.', 'This is helpful because monotonicity enables invertibility and we do so without forcing hard constraints on the neural architecture (e.g., positive weights).  We just need the output to be positive!', 'Our main application is use UMNNs in combination with Masked Autogressive Networks to define a new invertible transformation, which we then compose into an autoregressive model. Results are competitive with NAF and BNAF. https://t.co/Sjaq5yNrbC']",https://arxiv.org/abs/1908.05164,"Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference. ",Unconstrained Monotonic Neural Networks
55,1161838617196728321,945820508066590720,Buddhika Nettasinghe,"['Our new paper on Arxiv presents a ""friendship paradox"" based maximum likelihood estimation method for estimating network degree distributions . \n<LINK> <LINK>']",https://arxiv.org/abs/1908.00310,"This paper considers the problem of estimating a power-law degree distribution of an undirected network using sampled data. Although power-law degree distributions are ubiquitous in nature, the widely used parametric methods for estimating them (e.g. linear regression on double-logarithmic axes, maximum likelihood estimation with uniformly sampled nodes) suffer from the large variance introduced by the lack of data-points from the tail portion of the power-law degree distribution. As a solution, we present a novel maximum likelihood estimation approach that exploits the friendship paradox to sample more efficiently from the tail of the degree distribution. We analytically show that the proposed method results in a smaller bias, variance and a Cramer-Rao lower bound compared to the vanilla maximum-likelihood estimate obtained with uniformly sampled nodes (which is the most commonly used method in literature). Detailed numerical and empirical results are presented to illustrate the performance of the proposed method under different conditions and how it compares with alternative methods. We also show that the proposed method and its desirable properties (i.e. smaller bias, variance and Cramer-Rao lower bound compared to vanilla method based on uniform samples) extend to parametric degree distributions other than the power-law such as exponential degree distributions as well. All the numerical and empirical results are reproducible and the code is publicly available on Github. ","Maximum Likelihood Estimation of Power-law Degree Distributions via
  Friendship Paradox based Sampling"
56,1161830953536425984,41848191,Lesandro Ponciano,"['In the new paper ""Characterising Volunteers\' Task Execution Patterns Across Projects on Multi-Project Citizen Science Platforms"", @thiagomanel and I use GQM and SIM to examine how citizen scientists engage across science projects. #citsci #hcomp\n\nPreprint: <LINK> <LINK>']",https://arxiv.org/abs/1908.01344,"Citizen science projects engage people in activities that are part of a scientific research effort. On multi-project citizen science platforms, scientists can create projects consisting of tasks. Volunteers, in turn, participate in executing the project's tasks. Such type of platforms seeks to connect volunteers and scientists' projects, adding value to both. However, little is known about volunteer's cross-project engagement patterns and the benefits of such patterns for scientists and volunteers. This work proposes a Goal, Question, and Metric (GQM) approach to analyse volunteers' cross-project task execution patterns and employs the Semiotic Inspection Method (SIM) to analyse the communicability of the platform's cross-project features. In doing so, it investigates what are the features of platforms to foster volunteers' cross-project engagement, to what extent multi-project platforms facilitate the attraction of volunteers to perform tasks in new projects, and to what extent multi-project participation increases engagement on the platforms. Results from analyses on real platforms show that volunteers tend to explore multiple projects, but they perform tasks regularly in just a few of them; few projects attract much attention from volunteers; volunteers recruited from other projects on the platform tend to get more engaged than those recruited outside the platform. System inspection shows that platforms still lack personalised and explainable recommendations of projects and tasks. The findings are translated into useful claims about how to design and manage multi-project platforms. ","Characterising Volunteers' Task Execution Patterns Across Projects on
  Multi-Project Citizen Science Platforms"
57,1161644206626091015,2347666219,John F Donoghue,['Our new paper on the arrow of causality - including theories with arrows pointing both directions:\n\n<LINK>'],https://arxiv.org/abs/1908.04170,"Causality in quantum field theory is defined by the vanishing of field commutators for space-like separations. However, this does not imply a direction for causal effects. Hidden in our conventions for quantization is a connection to the definition of an arrow of causality, i.e. what is the past and what is the future. If we mix quantization conventions within the same theory, we get a violation of microcausality. In such a theory with mixed conventions the dominant definition of the arrow of causality is determined by the stable states. In some quantum gravity theories, such as quadratic gravity and possibly asymptotic safety, such a mixed causality condition occurs. We discuss some of the implications. ",The arrow of causality and quantum gravity
58,1161609761076498432,621147651,zpenoyre,['New paper out today - <LINK> - can we see and constrain planets by their relativistic beaming of light <LINK>'],https://arxiv.org/abs/1908.04602,"In this paper I show that the concept of relativistic beaming -- the process by which light emitted by a fast moving sources is lensed towards the direction of motion -- can be easily extended to model the signal from both the star and any secondary companions. Most companions will be cooler and less massive than their host star. Their lower mass leads to faster orbital velocities, and thus a potentially larger beaming effect. The lower temperature will mean that most of their light is emitted at longer wavelengths, where the relative photometric dominance of the primary is reduced. Thus for some systems, the secondary companion can be the main contributor to observed relativistic beaming signals at long wavelengths. Furthermore, if the system is observed over a range of wavelengths we can independently constrain the temperature of the companion, and the mass and radius ratio of the binary. To conclude I discuss the current and future observational prospects of this signal, using the properties of known exoplanets to show that such a signal may be observable by upcoming surveys. ","The Beam Balance -- Measuring Binary Systems via Relativistic Beaming
  Signals from Stars and their Companions"
59,1161545688662052865,24443979,Dan Stowell,"['New paper from us: ""Estimating &amp; Mitigating the Impact of Acoustic Environments on Machine-to-Machine Signalling"" <LINK> by @AmoghMatt - he will present it at @eusipco2019! #eusipco2019', '@AmoghMatt @eusipco2019 Oh also - this is the product of a great collaboration with @chirp!']",https://arxiv.org/abs/1908.04672,"The advance of technology for transmitting Data-over-Sound in various IoT and telecommunication applications has led to the concept of machine-to-machine over-the-air acoustic signalling. Reverberation can have a detrimental effect on such machine-to-machine signals while decoding. Various methods have been studied to combat the effects of reverberation in speech and audio signals, but it is not clear how well they generalise to other sound types. We look at extending these models to facilitate machine-to-machine acoustic signalling. This research investigates dereverberation techniques to shortlist a single-channel reverberation suppression method through a pilot test. In order to apply the chosen dereverberation method a novel method of estimating acoustic parameters governing reverberation is proposed. The performance of the final algorithm is evaluated on quality metrics as well as the performance of a real machine-to-machine decoder. We demonstrate a dramatic reduction in error rate for both audible and ultrasonic signals. ","Estimating & Mitigating the Impact of Acoustic Environments on
  Machine-to-Machine Signalling"
60,1161460501991182336,9674682,Kohei Hayashi,"['Happy to share our new paper w/ Yamaguchi, Sugawara &amp; Maeda. We show a lot of light-weight CNN modules are graphically representable as tensor networks. Architecture search in the graphs finds dense Pareto solutions for the accuracy/efficiency tradeoff. \n<LINK> <LINK>']",https://arxiv.org/abs/1908.04471,"Tensor decomposition methods are widely used for model compression and fast inference in convolutional neural networks (CNNs). Although many decompositions are conceivable, only CP decomposition and a few others have been applied in practice, and no extensive comparisons have been made between available methods. Previous studies have not determined how many decompositions are available, nor which of them is optimal. In this study, we first characterize a decomposition class specific to CNNs by adopting a flexible graphical notation. The class includes such well-known CNN modules as depthwise separable convolution layers and bottleneck layers, but also previously unknown modules with nonlinear activations. We also experimentally compare the tradeoff between prediction accuracy and time/space complexity for modules found by enumerating all possible decompositions, or by using a neural architecture search. We find some nonlinear decompositions outperform existing ones. ","Einconv: Exploring Unexplored Tensor Network Decompositions for
  Convolutional Neural Networks"
61,1161373511883132930,61355172,Maria Womack üá∫üá¶,['Our new paper on the famous outbursting 29P/SW1 significantly advances our understanding of Centaurs &amp; JFCs! It also shows that SW1 will start moving out of its current orbit in ~20 years. <LINK> @sir_galahead @kat_volk @steckloff @LWoodney &amp; W.Harris #WeAreSW1 <LINK>'],http://arxiv.org/abs/1908.04185,"Jupiter-family comets (JFCs) are the evolutionary products of trans-Neptunian objects (TNOs) that evolve through the giant planet region as Centaurs and into the inner solar system. Through numerical orbital evolution calculations following a large number of TNO test particles that enter the Centaur population, we have identified a short-lived dynamical Gateway, a temporary low-eccentricity region exterior to Jupiter through which the majority of JFCs pass. We apply an observationally based size distribution function to the known Centaur population and obtain an estimated Gateway region population. We then apply an empirical fading law to the rate of incoming JFCs implied by the the Gateway region residence times. Our derived estimates are consistent with observed population numbers for the JFC and Gateway populations. Currently, the most notable occupant of the Gateway region is 29P/Schwassmann-Wachmann 1 (SW1), a highly active, regularly outbursting Centaur. SW1's present-day, very-low-eccentricity orbit was established after a 1975 Jupiter conjunction and will persist until a 2038 Jupiter conjunction doubles its eccentricity and pushes its semi-major axis out to its current aphelion. Subsequent evolution will likely drive SW1's orbit out of the Gateway region, perhaps becoming one of the largest JFCs in recorded history. The JFC Gateway region coincides with a heliocentric distance range where the activity of observed cometary bodies increases significantly. SW1's activity may be typical of the early evolutionary processing experienced by most JFCs. Thus, the Gateway region, and its most notable occupant SW1, are critical to both the dynamical and physical transition between Centaurs and JFCs. ","29P/Schwassmann-Wachmann 1, A Centaur in the Gateway to the
  Jupiter-Family Comets"
62,1161287200417484800,1954858226,Lloyd Knox,"[""New paper out with @cosmic_mar:   <LINK>, The Hubble Hunter's Guide. We qualitatively explore potential cosmological solutions to the Hubble constant problem and the challenges they face. We also speculate about the origin of some curiosities in the Planck data.""]",https://arxiv.org/abs/1908.03663,"Measurements of the Hubble constant, and more generally measurements of the expansion rate and distances over the interval $0 < z < 1$, appear to be inconsistent with the predictions of the standard cosmological model ($\Lambda$CDM) given observations of cosmic microwave background temperature and polarization anisotropies. Here we consider a variety of types of departures from $\Lambda$CDM that could, in principle, restore concordance among these datasets, and we explain why we find almost all of them unlikely to be successful. We single out the set of solutions that increase the expansion rate in the decade of scale factor expansion just prior to recombination as the least unlikely. These solutions are themselves tightly constrained by their impact on photon diffusion and on the gravitational driving of acoustic oscillations of the modes that begin oscillating during this epoch -- modes that project on to angular scales that are very well measured. We point out that a general feature of such solutions is a residual to fits to $\Lambda$CDM, like the one observed in Planck power spectra. This residual drives the modestly significant inferences of angular-scale dependence to the matter density and anomalously high lensing power, puzzling aspects of a data set that is otherwise extremely well fit by $\Lambda$CDM. ",The Hubble Hunter's Guide
63,1161208612733235200,23000769,Christopher Conselice,"[""If you are interested in machine learning (ML) on large datasets check out my student Sunny Chen's new paper on the arXiv today <LINK>.  We investigate ways of using ML for classifying galaxies, including Convolutional Neural Network (CNN), K-nearest neighbour,"", 'Logistic Regression, Support Vector Machine, and Neural Networks.  We investigate, using these, the best methods and ways for optimizing classifying galaxies with ML. We show that 99.94% accuracy is possible using Deep Learning.  We did this using Dark Energy Survey (DES) data.', 'In this process we have also discovered where @galaxyzoo misclassifies galaxies due to the lower resolution of SDSS imaging.  As a bonus we also rediscovered the S0 galaxy type without intending to. Lots more in the paper itself!']",https://arxiv.org/abs/1908.03610,"There are several supervised machine learning methods used for the application of automated morphological classification of galaxies; however, there has not yet been a clear comparison of these different methods using imaging data, or a investigation for maximising their effectiveness. We carry out a comparison between several common machine learning methods for galaxy classification (Convolutional Neural Network (CNN), K-nearest neighbour, Logistic Regression, Support Vector Machine, Random Forest, and Neural Networks) by using Dark Energy Survey (DES) data combined with visual classifications from the Galaxy Zoo 1 project (GZ1). Our goal is to determine the optimal machine learning methods when using imaging data for galaxy classification. We show that CNN is the most successful method of these ten methods in our study. Using a sample of $\sim$2,800 galaxies with visual classification from GZ1, we reach an accuracy of $\sim$0.99 for the morphological classification of Ellipticals and Spirals. The further investigation of the galaxies that have a different ML and visual classification but with high predicted probabilities in our CNN usually reveals an the incorrect classification provided by GZ1. We further find the galaxies having a low probability of being either spirals or ellipticals are visually Lenticulars (S0), demonstrating that supervised learning is able to rediscover that this class of galaxy is distinct from both Es and Spirals. We confirm that $\sim$2.5\% galaxies are misclassified by GZ1 in our study. After correcting these galaxies' labels, we improve our CNN performance to an average accuracy of over 0.99 (accuracy of 0.994 is our best result). ","Optimising Automatic Morphological Classification of Galaxies with
  Machine Learning and Deep Learning using Dark Energy Survey Imaging"
64,1161087472807399430,2766925212,Andrew Childs,"['New paper with Chakrabarti, @hungshihhan, @tongyang93, Wang, &amp; @xiaodiwu gives quantum speedup for estimating the volume of a convex body <LINK>']",http://arxiv.org/abs/1908.03903,"Estimating the volume of a convex body is a central problem in convex geometry and can be viewed as a continuous version of counting. We present a quantum algorithm that estimates the volume of an $n$-dimensional convex body within multiplicative error $\epsilon$ using $\tilde{O}(n^{3}+n^{2.5}/\epsilon)$ queries to a membership oracle and $\tilde{O}(n^{5}+n^{4.5}/\epsilon)$ additional arithmetic operations. For comparison, the best known classical algorithm uses $\tilde{O}(n^{4}+n^{3}/\epsilon^{2})$ queries and $\tilde{O}(n^{6}+n^{5}/\epsilon^{2})$ additional arithmetic operations. To the best of our knowledge, this is the first quantum speedup for volume estimation. Our algorithm is based on a refined framework for speeding up simulated annealing algorithms that might be of independent interest. This framework applies in the setting of ""Chebyshev cooling"", where the solution is expressed as a telescoping product of ratios, each having bounded variance. We develop several novel techniques when implementing our framework, including a theory of continuous-space quantum walks with rigorous bounds on discretization error. To complement our quantum algorithms, we also prove that volume estimation requires $\Omega(\sqrt n+1/\epsilon)$ quantum membership queries, which rules out the possibility of exponential quantum speedup in $n$ and shows optimality of our algorithm in $1/\epsilon$ up to poly-logarithmic factors. ",Quantum algorithm for estimating volumes of convex bodies
65,1160957663259348994,1216521482,Luiz G. A. Alves,"['Our new paper ""Reconstructing commuters network using machine learning and urban indicators"" will be published in @SciReports tomorrow. The preprint is available on ArXiv:\n<LINK> - with Gabriel Spadon, Andre de Carvalho, and Jose Rodrigues-Jr <LINK>']",https://arxiv.org/abs/1908.03512,"Human mobility has a significant impact on several layers of society, from infrastructural planning and economics to the spread of diseases and crime. Representing the system as a complex network, in which nodes are assigned to regions (e.g., a city) and links indicate the flow of people between two of them, physics-inspired models have been proposed to quantify the number of people migrating from one city to the other. Despite the advances made by these models, our ability to predict the number of commuters and reconstruct mobility networks remains limited. Here, we propose an alternative approach using machine learning and 22 urban indicators to predict the flow of people and reconstruct the intercity commuters network. Our results reveal that predictions based on machine learning algorithms and urban indicators can reconstruct the commuters network with 90.4% of accuracy and describe 77.6% of the variance observed in the flow of people between cities. We also identify essential features to recover the network structure and the urban indicators mostly related to commuting patterns. As previously reported, distance plays a significant role in commuting, but other indicators, such as Gross Domestic Product (GDP) and unemployment rate, are also driven-forces for people to commute. We believe that our results shed new lights on the modeling of migration and reinforce the role of urban indicators on commuting patterns. Also, because link-prediction and network reconstruction are still open challenges in network science, our results have implications in other areas, like economics, social sciences, and biology, where node attributes can give us information about the existence of links connecting entities in the network. ","Reconstructing commuters network using machine learning and urban
  indicators"
66,1160920423875244033,307826617,Kev Abazajian ‚§∑‚è≥üåé,"['Ongoing &amp; future cosmic microwave background experiments have the sensitivity to indicate or rule out Dirac neutrino mass models, beating the LHC. @SPTelescope @SimonsObs &amp; CMB-S4. New paper with #UCIrvine Humboldt fellow Julian Heek <LINK> <LINK>', 'In other words: we may discover a breakthrough in fundamental particles with experiments measuring the relic light from the Big Bang.', 'Julian *Heeck*', '@di_goldene_pave @SPTelescope @SimonsObs It depends on th me number of degrees of freedom between decoupling &amp; the CMB. We discuss in the paper.']",https://arxiv.org/abs/1908.03286,"Planned CMB Stage IV experiments have the potential to measure the effective number of relativistic degrees of freedom in the early Universe, $N_\text{eff}$, with percent-level accuracy. This probes new thermalized light particles and also constrains possible new-physics interactions of Dirac neutrinos. Many Dirac-neutrino models that aim to address the Dirac stability, the smallness of neutrino masses or the matter--anti-matter asymmetry of our Universe endow the right-handed chirality partners $\nu_R$ with additional interactions that can thermalize them. Unless the reheating temperature of our Universe was low, this leads to testable deviations in $N_\text{eff}$. We discuss well-motivated models for $\nu_R$ interactions such as gauged $U(1)_{B-L}$ and the neutrinophilic two-Higgs-doublet model, and compare the sensitivity of SPT-3G, Simons Observatory, and CMB-S4 to other experiments, in particular the LHC. ",Observing Dirac neutrinos in the cosmic microwave background
67,1160788492034572288,1141165869784977409,KeisukeImoto,['We released a new large-scale dataset of miniature machine operating sounds for anomaly detection in sounds;\n\nCode: \n<LINK>\nDataset: \n<LINK>\nPaper: \n<LINK>'],https://arxiv.org/abs/1908.03299,"This paper introduces a new dataset called ""ToyADMOS"" designed for anomaly detection in machine operating sounds (ADMOS). To the best our knowledge, no large-scale datasets are available for ADMOS, although large-scale datasets have contributed to recent advancements in acoustic signal processing. This is because anomalous sound data are difficult to collect. To build a large-scale dataset for ADMOS, we collected anomalous operating sounds of miniature machines (toys) by deliberately damaging them. The released dataset consists of three sub-datasets for machine-condition inspection, fault diagnosis of machines with geometrically fixed tasks, and fault diagnosis of machines with moving tasks. Each sub-dataset includes over 180 hours of normal machine-operating sounds and over 4,000 samples of anomalous sounds collected with four microphones at a 48-kHz sampling rate. The dataset is freely available for download at this https URL ","ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous
  Sound Detection"
68,1160720631496712194,2285825876,Johanna,"['Beautiful new paper led by Ke Zhang, ""Systematic Variations of CO Gas Abundance with Radius in Gas-rich Protoplanetary Disks""   <LINK> Also, could there BE a nicer list of authors?! I imagine this paper was a joy to work on. :)']",https://arxiv.org/abs/1908.03267,"CO is the most widely used gas tracer of protoplanetary disks. Its abundance is usually assumed to be an interstellar ratio throughout the warm molecular layer of the disk. But recent observations of low CO gas abundance in many protoplanetary disks challenge our understanding of physical and chemical evolutions in disks. Here we investigate the CO abundance structures in four well-studied disks and compare their structures with predictions of chemical processing of CO and transport of CO ice-coated dust grains in disks. We use spatially resolved CO isotopologue line observations and detailed thermo-chemical models to derive CO abundance structures. We find that the CO abundance varies with radius by an order of magnitude in these disks. We show that although chemical processes can efficiently reduce the total column of CO gas within 1 Myr under an ISM level of cosmic-ray ionization rate, the depletion mostly occurs at the deep region of a disk. Without sufficient vertical mixing, the surface layer is not depleted enough to reproduce weak CO emissions observed. The radial profiles of CO depletion in three disks are qualitatively consistent with predictions of pebble formation, settling, and drifting in disks. But the dust evolution alone cannot fully explain the high depletion observed in some disks. These results suggest that dust evolution may play a significant role in transporting volatile materials and a coupled chemical-dynamical study is necessary to understand what raw materials are available for planet formation at different distances from the central star. ","Systematic Variations of CO Gas Abundance with Radius in Gas-rich
  Protoplanetary Disks"
69,1160530603609133056,2347666219,John F Donoghue,"['Our new paper showing that a class of theories with unstable ghosts are unitary. This applies to quadratic gravity, as a UV completion for GR.\n\n<LINK>']",https://arxiv.org/abs/1908.02416,"We present a new understanding of the unstable ghost-like resonance which appears in theories such as quadratic gravity and Lee-Wick type theories. Quantum corrections make this resonance unstable, such that it does not appear in the asymptotic spectrum. We prove that these theories are unitary to all orders. Unitarity is satisfied by the inclusion of only cuts from stable states in the unitarity sum. This removes the need to consider this as a ghost state in the unitarity sum. However, we often use a narrow-width approximation where we do include cuts through unstable states, and ignore cuts through the stable decay products. If we do this with the unstable ghost resonance at one loop, we get the correct answer only by using a contour which was originally defined by Lee and Wick. The quantum effects also provide damping in both the Feynman and the retarded propagators, leading to stability under perturbations. ","Unitarity, stability and loops of unstable ghosts"
70,1160286763970990082,708465957690351616,Stuart Hadfield,['New paper from @NASAAmes #QuAIL:\n \n‚ÄúOptimizing quantum heuristics with meta-learning‚Äù\n#ML #QAOA #VQE\n\n<LINK>'],http://arxiv.org/abs/1908.03185,"Variational quantum algorithms, a class of quantum heuristics, are promising candidates for the demonstration of useful quantum computation. Finding the best way to amplify the performance of these methods on hardware is an important task. Here, we evaluate the optimization of quantum heuristics with an existing class of techniques called `meta-learners'. We compare the performance of a meta-learner to Bayesian optimization, evolutionary strategies, L-BFGS-B and Nelder-Mead approaches, for two quantum heuristics (quantum alternating operator ansatz and variational quantum eigensolver), on three problems, in three simulation environments. We show that the meta-learner comes near to the global optima more frequently than all other optimizers we tested in a noisy parameter setting environment. We also find that the meta-learner is generally more resistant to noise, for example seeing a smaller reduction in performance in Noisy and Sampling environments and performs better on average by a `gain' metric than its closest comparable competitor L-BFGS-B. These results are an important indication that meta-learning and associated machine learning methods will be integral to the useful application of noisy near-term quantum computers. ",Optimizing quantum heuristics with meta-learning
71,1159881489338785803,19346226,Tom Robinson,['Our new paper with researchers from MIT/IBM - Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics <LINK>'],https://arxiv.org/abs/1908.02591,"Anti-money laundering (AML) regulations play a critical role in safeguarding financial systems, but bear high costs for institutions and drive financial exclusion for those on the socioeconomic and international margins. The advent of cryptocurrency has introduced an intriguing paradox: pseudonymity allows criminals to hide in plain sight, but open data gives more power to investigators and enables the crowdsourcing of forensic analysis. Meanwhile advances in learning algorithms show great promise for the AML toolkit. In this workshop tutorial, we motivate the opportunity to reconcile the cause of safety with that of financial inclusion. We contribute the Elliptic Data Set, a time series graph of over 200K Bitcoin transactions (nodes), 234K directed payment flows (edges), and 166 node features, including ones based on non-public data; to our knowledge, this is the largest labelled transaction data set publicly available in any cryptocurrency. We share results from a binary classification task predicting illicit transactions using variations of Logistic Regression (LR), Random Forest (RF), Multilayer Perceptrons (MLP), and Graph Convolutional Networks (GCN), with GCN being of special interest as an emergent new method for capturing relational information. The results show the superiority of Random Forest (RF), but also invite algorithmic work to combine the respective powers of RF and graph methods. Lastly, we consider visualization for analysis and explainability, which is difficult given the size and dynamism of real-world transaction graphs, and we offer a simple prototype capable of navigating the graph and observing model performance on illicit activity over time. With this tutorial and data set, we hope to a) invite feedback in support of our ongoing inquiry, and b) inspire others to work on this societally important challenge. ","Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional
  Networks for Financial Forensics"
72,1159871579460780032,203254308,Feng Li,"[""Our new working paper (Que ser√° ser√°? The uncertainty estimation of feature-based time series forecasts) with @XiaoqianWang7 @YanfeiKang &amp; @fotpetr is now available for preview at <LINK>\n\nBut before you read it, let's enjoy this first <LINK>""]",https://arxiv.org/abs/1908.02891,"Forecasting is an indispensable element of operational research (OR) and an important aid to planning. The accurate estimation of the forecast uncertainty facilitates several operations management activities, predominantly in supporting decisions in inventory and supply chain management and effectively setting safety stocks. In this paper, we introduce a feature-based framework, which links the relationship between time series features and the interval forecasting performance into providing reliable interval forecasts. We propose an optimal threshold ratio searching algorithm and a new weight determination mechanism for selecting an appropriate subset of models and assigning combination weights for each time series tailored to the observed features. We evaluate our approach using a large set of time series from the M4 competition. Our experiments show that our approach significantly outperforms a wide range of benchmark models, both in terms of point forecasts as well as prediction intervals. ",The uncertainty estimation of feature-based forecast combinations
73,1159742927872114688,356676252,John Regan,"['Emergence of massive star-less galaxies in the early Universe may be the key to forming super-massive black holes. Check out my new paper with @AstroAhura , @bwoshea &amp; Mike Norman at <LINK> <LINK>']",https://arxiv.org/abs/1908.02823,"Using the Renaissance suite of simulations we examine the emergence of pristine atomic cooling haloes that are both metal-free and star-free in the early Universe. The absence of metals prevents catastrophic cooling, suppresses fragmentation, and may allow for the formation of massive black hole seeds. Here we report on the abundance of pristine atomic cooling haloes found and on the specific physical conditions that allow for the formation of these direct-collapse-black-hole (DCBH) haloes. In total in our simulations we find that 79 DCBH haloes form before a redshift of 11.6. We find that the formation of pristine atomic haloes is driven by the rapid assembly of the atomic cooling haloes with mergers, both minor and/or major, prior to reaching the atomic cooling limit a requirement. However, the ability of assembling haloes to remain free of (external) metal enrichment is equally important and underlines the necessity of following the transport of metals in such simulations. The candidate DCBH hosting haloes we find, have been exposed to mean Lyman-Werner radiation fields of J$_{LW}$ $\sim$ 1 J$_{21}$ and typically lie at least 10 kpc (physical) from the nearest massive galaxy. Growth rates of the haloes reach values of greater than 10$^7$ M$_{\odot}$ per unit redshift, leading to significant dynamical heating and the suppression of efficient cooling until the halo crosses the atomic cooling threshold. Finally, we also find five synchronised halo candidates where pairs of pristine atomic cooling haloes emerge that are both spatially and temporally synchronised. ","The Emergence of the First Star-free Atomic Cooling Haloes in the
  Universe"
74,1159742055461138437,219813917,Phil Uttley,"[""If you'd like to see images of: gas swirling into a black hole, binary supermassive black holes across the universe, the X-ray coronae of other stars (and maybe the shadow of a crossing exoplanet), you may be interested in our new paper: <LINK> (1/10) <LINK>"", ""We (including tweeps @WiseSpaceNL @DidierBarret, @keithgendreau @vicgrinberg @ErinAstro @astro_ec @OgNimaeb @AstroBianchi @WijersRalph) are proposing the development of X-ray interferometry (XRI), as part of @esa's long range plan #Voyage2050 (2/10)"", ""The ultimate limit on the sharpness of an image is set by the ratio of the wavelength of light to the size of aperture. In visible light this 'diffraction' limit is reached by current telescopes, but in X-rays (with tiny wavelengths) we're *5* orders of magnitude short. (3/10)"", 'Making X-ray optics is really hard and the focus has been on making telescopes bigger, not getting sharper images (@chandraxray is still top!). With XRI we can dramatically break through to reach the diffraction limit by combining X-rays collected from different apertures. (4/10)', 'The technique is similar to that used by radio astronomers to obtain the spectacular @ehtelescope images of the black hole in M87, but because of the tiny X-ray wavelengths, comparable resolution can be obtained in a few metres, rather than the diameter of a planet. (5/10)', ""So why do it in X-rays? Well, we can observe many more types of astrophysical process and object (hence the stars and exoplanets) and also measure spectra, to see the composition and dynamics of the hot X-ray emitting gas for each 'pixel' in the image. (6/10)"", 'But also, if we can fly the collecting apertures on different spacecraft (which requires super-precision formation flying!), we can improve the sharpness of the image by orders of magnitude more - the limits are essentially removed. (7/10)', 'So why now? @keithgendreau and Web Cash led NASA studies of XRI in the 2000s, but the X-ray community was still struggling to push through the next gen large area telescopes with awesome spectral resolution (now being realised as @AthenaXobs and possibly @lynxobservatory). (8/10)', ""Also, those earlier concepts focused on multi-spacecraft XRI which is really hard. We want to start with a single spacecraft, made possible by a clever 'telephoto lens' mirror arrangement from Richard Willingale of @PhysicsUoL (also lead-designer of the @AthenaXobs optics) (9/10)"", ""It'll still be really hard, we need extreme precision in the optics, thermal stability +new detector designs, we may not be able to fly this for 30 years, but if we have learned one thing from @ehtelescope and @LIGO it is that you have to get started early. So let's do it! (ends)""]",https://arxiv.org/abs/1908.03144,"We propose the development of X-ray interferometry (XRI), to reveal the universe at high energies with ultra-high spatial resolution. With baselines which can be accommodated on a single spacecraft, XRI can reach 100 $\mu$as resolution at 10 \AA (1.2 keV) and 20 $\mu$as at 2 \AA (6 keV), enabling imaging and imaging-spectroscopy of (for example) X-ray coronae of nearby accreting supermassive black holes (SMBH) and the SMBH `shadow'; SMBH accretion flows and outflows; X-ray binary winds and orbits; stellar coronae within ~100 pc and many exoplanets which transit across them. For sufficiently luminous sources XRI will resolve sub-pc scales across the entire observable universe, revealing accreting binary SMBHs and enabling trigonometric measurements of the Hubble constant with X-ray light echoes from quasars or explosive transients. A multi-spacecraft `constellation' interferometer would resolve well below 1 $\mu$as, enabling SMBH event horizons to be resolved in many active galaxies and the detailed study of the effects of strong field gravity on the dynamics and emission from accreting gas close to the black hole. ","The high energy universe at ultra-high resolution: the power and promise
  of X-ray interferometry"
75,1159727242383114240,2441942726,Aleksas Mazeliauskas,"['I am pleased to announce my new paper with Patrick Hanus and Klaus Reygers on ""Entropy production in pp and Pb-Pb collisions at the LHC""\n\n<LINK>']",https://arxiv.org/abs/1908.02792,"We use experimentally measured identified particle spectra and Hanbury Brown-Twiss radii to determine the entropy per unit rapidity $dS/dy$ produced in $\sqrt{s} = 7$ TeV pp and $\sqrt{s_{\rm NN}} = 2.76$ TeV Pb-Pb collisions. We find that $dS/dy = 11335 \pm 1188$ in 0-10% Pb-Pb, $dS/dy = 135.7 \pm 17.9$ in high-multiplicity pp, and $dS/dy = 37.8 \pm 3.7$ in minimum bias pp collisions and compare the corresponding entropy per charged particle $(dS/dy)/(dN_{\rm ch}/dy)$ to predictions of statistical models. Finally, we use the QCD kinetic theory pre-equilibrium and viscous hydrodynamics to model entropy production in the collision and reconstruct the average temperature profile at $\tau_0 = 1$ fm/$c$ for high multiplicity pp and Pb-Pb collisions. ","Entropy production in pp and Pb-Pb collisions at energies available at
  the CERN Large Hadron Collider"
76,1159685927104438272,296161364,Chris Power,"['New paper on the arXiv led by our @ICRAR @ARC_ASTRO3D  PhD student, Rodrigo Canas, with  @CDPLagos et al.  - ""From Stellar Halos to Intracluster Light: the physics of the Intra-Halo Stellar Component in cosmological hydrodynamical simulations"" - <LINK> <LINK>']",https://arxiv.org/abs/1908.02945,"We study the Intra-Halo Stellar Component (IHSC) of Milky Way-mass systems up to galaxy clusters in the Horizon-AGN cosmological hydrodynamical simulation. We identify the IHSC using an improved phase-space galaxy finder algorithm which provides an adaptive, physically motivated and shape-independent definition of this stellar component, that can be applied to halos of arbitrary masses. We explore the IHSC mass fraction-total halo's stellar mass, $f_{M*,IHSC}-M*$, relation and the physical drivers of its scatter. We find that on average the $f_{M*,IHSC}$ increases with $M_{*,tot}$, with the scatter decreasing strongly with mass from 2 dex at $M_{*,tot}\sim10^{11}M_\odot$ to 0.3 dex at group masses. At high masses, $M_{*,tot}>10^{11.5}M_\odot$, $f_{M*,IHSC}$ increases with the number of substructures, and with the mass ratio between the central galaxy and largest satellite, at fixed $M_{*,tot}$. From mid-size groups and systems below $M_{*,tot}<10^{12}M_\odot$, we find that the central galaxy's stellar rotation-to-dispersion velocity ratio, V/{\sigma}, displays the strongest (anti)-correlation with $f_{M*,IHSC}$ at fixed $M_{*,tot}$ of all the galaxy and halo properties explored, transitioning from $f_{M*,IHSC}$<0.1% for high V/{\sigma}, to $f_{M*,IHSC}\sim5$% for low V/{\sigma} galaxies. By studying the $f_{M*,IHSC}$ temporal evolution, we find that, in the former, mergers not always take place, but if they did, they happened early (z>1), while the high $f_{M*,IHSC}$ population displays a much more active merger history. In the case of massive groups and galaxy clusters, $M_{*,tot}>10^{12}M_\odot$, a fraction $f_{M*,IHSC}\sim$10-20% is reached at $z\sim1$ and then they evolve across lines of constant $f_{M*,IHSC}$ modulo some small perturbations. Because of the limited simulation's volume, the latter is only tentative and requires a larger sample of simulated galaxy clusters to confirm. ","From Stellar Halos to Intracluster Light: the physics of the Intra-Halo
  Stellar Component in cosmological hydrodynamical simulations"
77,1159666014075772928,2388684386,Bradley Meyers,['My new paper on the emission and scintillation properties of an RRAT with the @mwatelescope and @CSIRO_ATNF Parkes is now up on arXiv! Had a lot of fun with this one.\n\n<LINK>'],https://arxiv.org/abs/1908.02911,"Rotating Radio Transients (RRATs) represent a relatively new class of pulsar, primarily characterised by their sporadic bursting emission of single pulses on time scales of minutes to hours. In addition to the difficulty involved in detecting these objects, low-frequency ($<$300 MHz) observations of RRATs are sparse, which makes understanding their broadband emission properties in the context of the normal pulsar population problematic. Here, we present the simultaneous detection of RRAT J2325-0530 using the Murchison Widefield Array (154 MHz) and Parkes radio telescope (1.4 GHz). On a single-pulse basis, we produce the first polarimetric profile of this pulsar, measure the spectral index ($\alpha=-2.2\pm 0.1$), pulse energy distributions, and present the pulse rates in the context of detections in previous epochs. We find that the distribution of time between subsequent pulses is consistent with a Poisson process and find no evidence of clustering over the $\sim$1.5 hr observations. Finally, we are able to quantify the scintillation properties of RRAT J2325-0530 at 1.4 GHz, where the single pulses are modulated substantially across the observing bandwidth, and show that this characterisation is feasible even with irregular time sampling as a consequence of the sporadic emission behaviour. ","The emission and scintillation properties of RRAT J2325-0530 at 154 MHz
  and 1.4 GHz"
78,1159515922371100672,45825798,Dan Roberts,['New (machine learning!) paper with @judyfhoffman and Sho Yaida efficiently improving stability of models against input perturbations (including an appendix that even manages to work in a use of Haar integration...)\n<LINK>'],https://arxiv.org/abs/1908.02729,"Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data. ",Robust Learning with Jacobian Regularization
79,1159157356543922176,14975979,Abhimat Gautam,"['The black hole at the center of our galaxy, Sgr A*, has been having a particularly active (lit‚Ä¶ üî•?) summer! Check it out in our new paper!\n\n[1908.01777] Unprecedented variability of Sgr A* in NIR <LINK> <LINK>', '(Sgr AF, as the kids might say‚Ä¶ üòõ)']",https://arxiv.org/abs/1908.01777,"The electromagnetic counterpart to the Galactic center supermassive black hole, Sgr A*, has been observed in the near-infrared for over 20 years and is known to be highly variable. We report new Keck Telescope observations showing that Sgr A* reached much brighter flux levels in 2019 than ever measured at near-infrared wavelengths. In the K$^\prime$ band, Sgr A* reached flux levels of $\sim6$ mJy, twice the level of the previously observed peak flux from $>13,000$ measurements over 130 nights with the VLT and Keck Telescopes. We also observe a factor of 75 change in flux over a 2-hour time span with no obvious color changes between 1.6 $\mu$m and 2.1 $\mu$m. The distribution of flux variations observed this year is also significantly different than the historical distribution. Using the most comprehensive statistical model published, the probability of a single night exhibiting peak flux levels observed this year, given historical Keck observations, is less than $0.3\%$. The probability to observe the flux levels similar to all 4 nights of data in 2019 is less than $0.05\%$. This increase in brightness and variability may indicate a period of heightened activity from Sgr A* or a change in its accretion state. It may also indicate that the current model is not sufficient to model Sgr A* at high flux levels and should be updated. Potential physical origins of Sgr A*'s unprecedented brightness may be from changes in the accretion-flow as a result of the star S0-2's closest passage to the black hole in 2018 or from a delayed reaction to the approach of the dusty object G2 in 2014. Additional multi-wavelength observations will be necessary to both monitor Sgr A* for potential state changes and to constrain the physical processes responsible for its current variability. ",Unprecedented variability of Sgr A* in NIR
80,1159074916320370689,444852917,Nicholas G. Reich,"['Congrats to ReichLab postdoc @tomcm39 for his first paper with the lab out on arxiv this week! A new Bayesian model for constructing ensemble forecasts that adapt over time. \n\nAlso, sweet ternary plots! #dataviz \n\n<LINK> <LINK>']",https://arxiv.org/abs/1908.01675,"Seasonal influenza infects between 10 and 50 million people in the United States every year, overburdening hospitals during weeks of peak incidence. Named by the CDC as an important tool to fight the damaging effects of these epidemics, accurate forecasts of influenza and influenza-like illness (ILI) forewarn public health officials about when, and where, seasonal influenza outbreaks will hit hardest. Multi-model ensemble forecasts---weighted combinations of component models---have shown positive results in forecasting. Ensemble forecasts of influenza outbreaks have been static, training on all past ILI data at the beginning of a season, generating a set of optimal weights for each model in the ensemble, and keeping the weights constant. We propose an adaptive ensemble forecast that (i) changes model weights week-by-week throughout the influenza season, (ii) only needs the current influenza season's data to make predictions, and (iii) by introducing a prior distribution, shrinks weights toward the reference equal weighting approach and adjusts for observed ILI percentages that are subject to future revisions. We investigate the prior's ability to impact adaptive ensemble performance and, after finding an optimal prior via a cross-validation approach, compare our adaptive ensemble's performance to equal-weighted and static ensembles. Applied to forecasts of short-term ILI incidence at the regional and national level in the US, our adaptive model outperforms a naive equal-weighted ensemble, and has similar or better performance to the static ensemble, which requires multiple years of training data. Adaptive ensembles are able to quickly train and forecast during epidemics, and provide a practical tool to public health officials looking for forecasts that can conform to unique features of a specific season. ","Adaptively stacking ensembles for influenza forecasting with incomplete
  data"
81,1158914982165094400,2337598033,Geraint F. Lewis,['New paper on the arXiv with PhD student Lawrence Dam and @krzysbolejko <LINK> <LINK>'],https://arxiv.org/abs/1908.01953,"The standard model of cosmology is based on two unknown dark components that are uncoupled from each other. In this paper we investigate whether there is evidence for an interaction between these components of cold dark matter (CDM) and dark energy (DE). In particular, we focus on a minimal extension and reconstruct the interaction history at low-redshifts non-parametrically using a variation of the commonly used principal component analysis. Although we focus on the interaction in the dark sector, any significant deviation from the standard model that changes the expansion history of the Universe, should leave imprints detectable by our analysis. Thus, detecting signatures of interaction could also be indicative of other non-standard phenomena even if they are not the results of the interaction. It is thus interesting to note that the results presented in this paper do not provide support for the interaction in the dark sector, although the uncertainty is still quite large. In so far as interaction is present but undetectable using current data, we show from a Fisher forecast that forthcoming LSST and DESI surveys will be able to constrain a DM-DE coupling at $20\%$ precision --- enough to falsify the non-interacting scenario, assuming the presence of a modest amount of interaction. ","Probing the independence within the dark sector in the fluid
  approximation"
82,1158730989448945664,557011202,Nicola Tomassetti,"['Today on the arXiv, paper 1/2: ""New results in Solar modulation modeling in light of recent #CosmicRays data from space""\n#SpaceScience #Astrophysics #SolarScience\n\n <LINK>']",https://arxiv.org/abs/1908.01599,"Thanks to space-borne experiments such as the AMS-02 and PAMELA missions in low-Earth orbit, along with the Voyager spacecrafts in the interstellar space, a large collection of multi-channel and time-resolved Galactic cosmic ray (GCR) data has recently become available. Here we present an improved measured-validated model of the ""solar modulation"" effect, i.e., the temporal evolution of the GCR flux inside the heliosphere caused by the 11-year variability cycle of the Sun's magnetic activity. We present our improved modeling of the structure of the heliosphere, the physical mechanisms of diffusion, drift, and energy losses of GCR particles in the heliosphere. We present our results for the temporal dependence of the key model parameters and their relationship with solar activity proxies. We discuss implications for the GCR transport in magnetic turbulence, and new insights on our understanding of the solar modulation phenomenon. ","New results in solar modulation modeling in light of recent cosmic-ray
  data from space"
83,1158644385447366656,1020951700067241984,Daoudi Mohamed,"['A preprint of our new paper: Fitting, Comparison, and Alignment of Trajectories on Positive\n\xa0Semi-Definite Matrices with Application to Action Recognition\n<LINK>\n\nWork done by @IMTLilleDouai /CRIStAL, University of Florence and UCLouvain. <LINK>']",https://arxiv.org/abs/1908.00646,"In this paper, we tackle the problem of action recognition using body skeletons extracted from video sequences. Our approach lies in the continuity of recent works representing video frames by Gramian matrices that describe a trajectory on the Riemannian manifold of positive-semidefinite matrices of fixed rank. In comparison with previous works, the manifold of fixed-rank positive-semidefinite matrices is here endowed with a different metric, and we resort to different algorithms for the curve fitting and temporal alignment steps. We evaluated our approach on three publicly available datasets (UTKinect-Action3D, KTH-Action and UAV-Gesture). The results of the proposed approach are competitive with respect to state-of-the-art methods, while only involving body skeletons. ","Fitting, Comparison, and Alignment of Trajectories on Positive
  Semi-Definite Matrices with Application to Action Recognition"
84,1158639552569008129,775785554889863169,Shir Gur,"['Our new #iccv2019 paper -  ""Unsupervised Microvascular Image Segmentation Using an Active Contours Mimicking Neural Network""\narxiv: <LINK>\n\n#deeplearning #computervision #microvascular #patternrecognition']",http://arxiv.org/abs/1908.01373,"The task of blood vessel segmentation in microscopy images is crucial for many diagnostic and research applications. However, vessels can look vastly different, depending on the transient imaging conditions, and collecting data for supervised training is laborious. We present a novel deep learning method for unsupervised segmentation of blood vessels. The method is inspired by the field of active contours and we introduce a new loss term, which is based on the morphological Active Contours Without Edges (ACWE) optimization method. The role of the morphological operators is played by novel pooling layers that are incorporated to the network's architecture. We demonstrate the challenges that are faced by previous supervised learning solutions, when the imaging conditions shift. Our unsupervised method is able to outperform such previous methods in both the labeled dataset, and when applied to similar but different datasets. Our code, as well as efficient PyTorch reimplementations of the baseline methods VesselNN and DeepVess is available on GitHub - this https URL ","Unsupervised Microvascular Image Segmentation Using an Active Contours
  Mimicking Neural Network"
85,1158555469499138048,383142451,Shinnosuke Takamichi (È´òÈÅì ÊÖé‰πã‰ªã),"['Our new paper ""V2S attack: building DNN-based voice conversion from automatic speaker verification"" investigates ""can voice conversion steal user\'s individuality from automatic speaker verification?""\n\n<LINK> <LINK>']",https://arxiv.org/abs/1908.01454,"This paper presents a new voice impersonation attack using voice conversion (VC). Enrolling personal voices for automatic speaker verification (ASV) offers natural and flexible biometric authentication systems. Basically, the ASV systems do not include the users' voice data. However, if the ASV system is unexpectedly exposed and hacked by a malicious attacker, there is a risk that the attacker will use VC techniques to reproduce the enrolled user's voices. We name this the ``verification-to-synthesis (V2S) attack'' and propose VC training with the ASV and pre-trained automatic speech recognition (ASR) models and without the targeted speaker's voice data. The VC model reproduces the targeted speaker's individuality by deceiving the ASV model and restores phonetic property of an input voice by matching phonetic posteriorgrams predicted by the ASR model. The experimental evaluation compares converted voices between the proposed method that does not use the targeted speaker's voice data and the standard VC that uses the data. The experimental results demonstrate that the proposed method performs comparably to the existing VC methods that trained using a very small amount of parallel voice data. ","V2S attack: building DNN-based voice conversion from automatic speaker
  verification"
86,1158453021090729988,96253726,Navin Sridhar,"['New paper out!\nThis paper, led by @jaj_garcia is based on the crucial set of observations of GX 339-4 by @NASANuSTAR and @NASASwift, before the source quickly went to quiescence in 2017. \n<LINK>', 'This paper reinforces the idea of the accretion disk approaching very close to the black hole, as the luminosity increases during the hard state (R_in~ISCO at 0.01*L_Edd).', 'Among other things, the paper also discusses the picture of the black hole corona having a certain vertical dimension to it, and is capable of illuminating the outer regions of the accretion disk.']",https://arxiv.org/abs/1908.00965,"We report on the spectroscopic analysis of the black hole binary GX 339-4 during its recent 2017-2018 outburst, observed simultaneously by the Swift and NuSTAR observatories. Although during this particular outburst the source failed to make state transitions, and despite Sun constraints during the peak luminosity, we were able to trigger four different observations sampling the evolution of the source in the hard state. We show that even for the lowest luminosity observations the NuSTAR spectra show clear signatures of X-ray reprocessing (reflection) in an accretion disk. Detailed analysis of the highest signal-to-noise spectra with our family of relativistic reflection models RELXILL indicates the presence of both broad and narrow reflection components. We find that a dual-lamppost model provides a superior fit when compared to the standard single lamppost plus distant neutral reflection. In the dual lamppost model two sources at different heights are placed on the rotational axis of the black hole, suggesting that the narrow component of the Fe K emission is likely to originate in regions far away in the disk, but still significantly affected by its rotational motions. Regardless of the geometry assumed, we find that the inner edge of the accretion disk reaches a few gravitational radii in all our fits, consistent with previous determinations at similar luminosity levels. This confirms a very low degree of disk truncation for this source at luminosities above ~1% Eddington. Our estimates of Rin reinforces the suggested behavior for an inner disk that approaches the inner-most regions as the luminosity increases in the hard state. ","The 2017 Failed Outburst of GX 339-4: Relativistic X-ray Reflection near
  the Black Hole Revealed by NuSTAR and Swift Spectroscopy"
87,1158428804974616576,804515472167288834,Jos√© Ignacio Orlando ‚òÄÔ∏èüëÅÔ∏è,"['New paper online. ""An amplified-target loss approach for photoreceptor layer segmentation in pathological OCT scans"", accepted for presentation at @miccai2019 workshop in ophthalmic image analysis (OMIA 2019). Preprint available in <LINK>']",https://arxiv.org/abs/1908.00764,"Segmenting anatomical structures such as the photoreceptor layer in retinal optical coherence tomography (OCT) scans is challenging in pathological scenarios. Supervised deep learning models trained with standard loss functions are usually able to characterize only the most common disease appeareance from a training set, resulting in suboptimal performance and poor generalization when dealing with unseen lesions. In this paper we propose to overcome this limitation by means of an augmented target loss function framework. We introduce a novel amplified-target loss that explicitly penalizes errors within the central area of the input images, based on the observation that most of the challenging disease appeareance is usually located in this area. We experimentally validated our approach using a data set with OCT scans of patients with macular diseases. We observe increased performance compared to the models that use only the standard losses. Our proposed loss function strongly supports the segmentation model to better distinguish photoreceptors in highly pathological scenarios. ","An amplified-target loss approach for photoreceptor layer segmentation
  in pathological OCT scans"
88,1158340630743080960,261865146,Dr Sofia Qvarfort,"['New paper on the #arXiv today! Solving the time-evolution of nonlinear optomechanical systems is often messy, but here we provide a general solution that includes both mechanical squeezing and displacements. We also got some beautiful plots!\n<LINK>\n@UCLQuantum <LINK>']",https://arxiv.org/abs/1908.00790,"We solve the time evolution of a nonlinear optomechanical Hamiltonian with arbitrary time-dependent mechanical displacement, mechanical single-mode squeezing and a time-dependent optomechanical coupling up to the solution of two second-order differential equations. The solution is based on identifying a minimal and finite Lie algebra that generates the time-evolution of the system. This reduces the problem to considering a finite set of coupled ordinary differential equations of real functions. To demonstrate the applicability of our method, we compute the degree of non-Gaussianity of the time-evolved state of the system by means of a measure based on the relative entropy of the non-Gaussian state and its closest Gaussian reference state. We find that the addition of a constant mechanical squeezing term to the standard optomechanical Hamiltonian generally decreases the overall non-Gaussian character of the state. For sinusoidally modulated squeezing, the two second-order differential equations mentioned above take the form of the Mathieu equation. We derive perturbative solutions for a small squeezing amplitude at parametric resonance and show that they correspond to the rotating-wave approximation at times larger than the scale set by the mechanical frequency. We find that the non-Gaussianity of the state increases with both time and the squeezing parameter in this specific regime. ","Time-evolution of nonlinear optomechanical systems: Interplay of
  mechanical squeezing and non-Gaussianity"
89,1158336724566777856,2746722060,Tristan Smith,['Robert Caldwell and I have a new paper that appeared today on the arXiv- <LINK> - we present an end-to-end calculation of LISA‚Äôs sensitivity to both stochastic and deterministic signals. See <LINK> for our code!'],https://arxiv.org/abs/1908.00546,"We present the steps to forecast the sensitivity of the Laser Interferometer Space Antenna (LISA) to both a stochastic gravitational wave background and deterministic wave sources. We show how to use these expressions to estimate the precision with which LISA can determine parameters associated with these sources. Tools are included to enable easy calculation of the signal-to-noise ratio and draw sensitivity curves. Benchmark values are given for easy comparison and checking of methods in the case of two worked examples. The first benchmark is the threshold stochastic gravitational wave background $\Omega_{GW} h^2$ that LISA can observe. The second is the signal-to-noise ratio that LISA would observe for a binary black hole system identical to GW150914, radiating 4 years before merger. ","LISA for Cosmologists: Calculating the Signal-to-Noise Ratio for
  Stochastic and Deterministic Sources"
90,1158263340948434944,822867138,Bradley Kavanagh,"[""In our new paper out today, we look at whether clumps (or 'sub-halos') of #DarkMatter could affect searches for DM in the Solar System: <LINK>\n\nDirect detection of DM on Earth: could be affected (a little)\nSearches for DM captured in the Sun: probably not <LINK>"", '@GRAPPAInstitute We used a state-of-the-art model for Milky Way substructure (from https://t.co/i35M88DrXC) and discovered some cool facts:\n\n‚Ä¢ About a 10% chance that the Solar System is currently inside a DM sub-halo\n‚Ä¢ Solar System has probably crossed about 100 sub-halos in its lifetime', ""@GRAPPAInstitute There are still a few small improvements I'd like to make to the paper, but overall I'm very pleased. \n\nIt was a cool project (a couple of years slowly in the making) with lots of neat calculations. Congrats to PhD Student Andreas Rappelt for doing a lot of the hard work!""]",https://arxiv.org/abs/1908.00747,"Dark matter substructure can contribute significantly to local dark matter searches and may provide a large uncertainty in the interpretation of those experiments. For direct detection experiments, sub-halos give rise to an additional dark matter component on top of the smooth dark matter distribution of the host halo. In the case of dark matter capture in the Sun, sub-halo encounters temporarily increase the number of captured particles. Even if the encounter happened in the past, the number of dark matter particles captured by the Sun can still be enhanced today compared to expectations from the host halo as those enhancements decay over time. Using results from an analytical model of the sub-halo population of a Milky Way-like galaxy, valid for sub-halo masses between $10^{-5}\,M_\odot$ and $10^{11}\,M_\odot$, we assess the impact of sub-halos on direct dark matter searches in a probabilistic way. We find that the impact on direct detection can be sizable, with a probability of $\sim 10^{-3}$ to find an $\mathcal{O}(1)$ enhancement of the recoil rate. In the case of the capture rate in the Sun, we find that $\mathcal{O}(1)$ enhancements are very unlikely, with probability $\lesssim 10^{-5}$, and are even impossible for some dark matter masses. ",Impact of substructure on local dark matter searches
91,1157278618722197505,4438354094,Tom Wong,"['New paper! Using (loopless) isolated vertices, we can dramatically simplify dynamic graphs on which continuous-time quantum walks implement a universal set of quantum gates. <LINK> <LINK>']",https://arxiv.org/abs/1908.00507,"It was recently shown that continuous-time quantum walks on dynamic graphs, i.e., sequences of static graphs whose edges change at specific times, can implement a universal set of quantum gates. This result treated all isolated vertices as having self-loops, so they all evolved by a phase under the quantum walk. In this paper, we permit isolated vertices to be loopless or looped, and loopless isolated vertices do not evolve at all under the quantum walk. Using this distinction, we construct simpler dynamic graphs that implement the Pauli gates and a set of universal quantum gates consisting of the Hadamard, $T$, and CNOT gates, and these gates are easily extended to multi-qubit systems. For example, the $T$ gate is simplified from a sequence of six graphs to a single graph, and the number of vertices is reduced by a factor of four. We also construct a generalized phase gate, of which $Z$, $S$, and $T$ are specific instances. Finally, we validate our implementations by numerically simulating a quantum circuit consisting of layers of one- and two-qubit gates, similar to those in recent quantum supremacy experiments, using a quantum walk. ",Isolated Vertices in Continuous-Time Quantum Walks on Dynamic Graphs
92,1157146598893187072,865393920,Dr. Renee Ludlam,"['New paper alert! \n<LINK>\n\nWe analyzed NICER observations of the ultra-compact X-ray binary 4U 1543-624 during its 2017 outburst (lightcurve and color diagrams shown below). We were able to divide the data into different intervals to measure changes in the system. <LINK>', 'The NICER spectra show O &amp; Fe emission lines that are broadened by the motion and strong gravity in the inner accretion disk (R_in). These effects are stronger the closer the disk is to the compact object. Hence, the shape of the line enables us to measure the position of R_in. https://t.co/EAPNkUR6H7', 'The O &amp; Fe line profiles in velocity space show a similar broadening width, indicating that they arise from a common radius in the accretion disk. https://t.co/8dxHsprd7u', 'High energy coverage from @ESA_Integral helped rule out spectral models that were unable to describe both the NICER and INTEGRAL energy bands simultaneously. https://t.co/cMJxURhHEM', 'Additionally, we obtained ATCA radio observations that allow us to compare 4U 1543-624‚Äôs X-ray and radio properties to other accreting objects. Needless to say, it is complicated and neutron stars can exhibit a variety of behavior. https://t.co/l7ecwSTPzh']",http://arxiv.org/abs/1908.00539,"We report on X-ray and radio observations of the ultra-compact X-ray binary 4U 1543-624 taken in August 2017 during an enhanced accretion episode. We obtained NICER monitoring of the source over a $\sim10$ day period during which target-of-opportunity observations were also conducted with Swift, INTEGRAL, and ATCA. Emission lines were measured in the NICER X-ray spectrum at $\sim0.64$ keV and $\sim6.4$ keV that correspond to O and Fe, respectively. By modeling these line components, we are able to track changes in the accretion disk throughout this period. The innermost accretion flow appears to move inwards from hundreds of gravitational radii ($R_{g}=GM/c^{2}$) at the beginning of the outburst to $<8.7$ $R_{g}$ at peak intensity. We do not detect the source in radio, but are able to place a $3\sigma$ upper limit on the flux density at $27$ $\mu$Jy beam$^{-1}$. Comparing the radio and X-ray luminosities, we find that the source lies significantly away from the range typical of black holes in the ${L}_{{r}}$-${L}_{{x}}$ plane, suggesting a neutron star (NS) primary. This adds to the evidence that NSs do not follow a single track in the ${L}_{{r}}$-${L}_{{x}}$ plane, limiting its use in distinguishing between different classes of NSs based on radio and X-ray observations alone. ","Observations of the Ultra-compact X-ray Binary 4U 1543-624 in Outburst
  with NICER, INTEGRAL, Swift, and ATCA"
93,1157125878570242049,200742058,Dimitris Koukoulopoulos,['New paper with Kevin Ford and Ben Green on the mysterious Erdos-Hooley Delta function that measures the concentration of divisors of integers <LINK>'],https://arxiv.org/abs/1908.00378,"We study the extent to which divisors of a typical integer $n$ are concentrated. In particular, defining the Erd\H os-Hooley $\Delta$-function by $\Delta(n) := \max_t # \{d | n, \log d \in [t,t+1]\}$, we show that $\Delta(n) \geq (\log \log n)^{0.35332277\dots}$ for almost all $n$, a bound we believe to be sharp. This disproves a conjecture of Maier and Tenenbaum. We also prove analogs for the concentration of divisors of a random permutation and of a random polynomial over a finite field. Most of the paper is devoted to a study of the following much more combinatorial problem of independent interest. Pick a random set $A \subset \mathbb{N}$ by selecting $i$ to lie in $A$ with probability $1/i$. What is the supremum of all exponents $\beta_k$ such that, almost surely as $D \rightarrow \infty$, some integer is the sum of elements of $A \cap [D^{\beta_k}, D]$ in $k$ different ways? We characterise $\beta_k$ as the solution to a certain optimisation problem over measures on the discrete cube $\{0,1\}^k$, and obtain lower bounds for $\beta_k$ which we believe to be asymptotically sharp. ",Equal sums in random sets and the concentration of divisors
94,1167308433382469632,911474423412219904,Julien Tierny,['Need to compare in-situ simulations to an acquired ground-truth? Checkout our new #ldav 2019 application paper using #TopologicalDataAnalysis and #OptimalTransport <LINK> #TopologyToolKit #datascience #visualization #machinelearning @INS2I_CNRS @Kitware <LINK>'],https://arxiv.org/abs/1908.07841,"This application paper presents a novel framework based on topological data analysis for the automatic evaluation and ranking of viscous finger simulation runs in an ensemble with respect to a reference acquisition. Individual fingers in a given time-step are associated with critical point pairs in the distance field to the injection point, forming persistence diagrams. Different metrics, based on optimal transport, for comparing time-varying persistence diagrams in this specific applicative case are introduced. We evaluate the relevance of the rankings obtained with these metrics, both qualitatively thanks to a lightweight web visual interface, and quantitatively by studying the deviation from a reference ranking suggested by experts. Extensive experiments show the quantitative superiority of our approach compared to traditional alternatives. Our web interface allows experts to conveniently explore the produced rankings. We show a complete viscous fingering case study demonstrating the utility of our approach in the context of porous media fluid flow, where our framework can be used to automatically discard physically-irrelevant simulation runs from the ensemble and rank the most plausible ones. We document an in-situ implementation to lighten I/O and performance constraints arising in the context of parametric studies. ","Ranking Viscous Finger Simulations to an Acquired Ground Truth with
  Topology-aware Matchings"
95,1166424051167793152,348889317,Marsden Lab,"[""Just out on the @arxiv  Jongmin's new paper on UQ in coronary models with FSI using clinically-derived data uncertainty.  @simvascular @ICMEStanford <LINK>""]",https://arxiv.org/abs/1908.07522,"Cardiovascular simulations are increasingly used for non-invasive diagnosis of cardiovascular disease, to guide treatment decisions, and in the design of medical devices. Quantitative assessment of the variability of simulation outputs due to input uncertainty is a key step toward further integration of cardiovascular simulations in the clinical workflow. In this study, we present uncertainty quantification in computational models of the coronary circulation to investigate the effect of uncertain parameters, including coronary pressure waveform, intramyocardial pressure, morphometry exponent, and the vascular wall Young's modulus. We employ a left coronary artery model with deformable vessel walls, simulated via an ALE framework for FSI, with a prescribed inlet pressure and open-loop lumped parameter network outlet boundary conditions. Stochastic modeling of the uncertain inputs is determined from intra-coronary catheterization data or gathered from the literature. Uncertainty propagation is performed using several approaches including Monte Carlo, Quasi MC, stochastic collocation, and multiwavelet stochastic expansion. Variabilities in QoI, including branch pressure, flow, wall shear stress, and wall deformation are assessed. We find that uncertainty in inlet pressures and intramyocardial pressures significantly affect all resulting QoIs, while uncertainty in elastic modulus only affects the mechanical response of the vascular wall. Variability in the morphometry exponent has little effect on coronary hemodynamics or wall mechanics. Finally, we compare convergence behaviors of statistics of QoIs using several uncertainty propagation methods. From the simulation results, we conclude that the multi-wavelet stochastic expansion shows superior accuracy and performance against Quasi Monte Carlo and stochastic collocation methods. ","The effects of clinically-derived parametric data uncertainty in
  patient-specific coronary simulations with deformable walls"
96,1164875248283656192,908673384611041281,Dieuwke Hupkes,"['Curious what people may mean when they say a neural network is (not) compositional? And how that relates to linguistics and philosophy literature on compositionality? \n\nCheck our new paper on compositionality in neural networks: <LINK>! <LINK>', 'Whether neural networks are able to learn linguistic compositionally has been the topic of debate ever since neural networks were first used for natural language processing.', 'But what does it actually mean for a neural network to be compositional? What definition of compositionality is used? And how does this relate to the principle of compositionality and the vast amount of literature about this topic?', 'In this paper, we are explicit about different aspects of compositionality that may be considered important by different researchers, starting from linguistic and philosophical research about compositionality.', 'We propose five theoretically grounded tests and use them to diagnose three popular sequence to sequence architectures: a recurrent architecture, a convolution-based architecture and a transformer model. https://t.co/6P1Q0KP4e6', 'We instantiate these tests on a strongly compositional artificial data set. But make sure the distribution of lengths and parse tree depths of the sentences match the distributions observed in a corpus with English sentences (WMT17) https://t.co/Re5Ed706sF', 'We test if models can deal with this compositional data set at all;', 'If models can systematically recombine new pairs of words not seen in the training data;', 'If they can productivey generalise to input sentences that are longer than the ones they are trained on;', 'Under what conditions they infer that words are synonyms, and how they deal with that;', 'If they compute representations in a *localist* fashion, computing the meanings of all smaller parts first, or instead use the global composition operations.', 'And how much evidence they need to learn *exceptions* to rules. Do they *overgeneralise* when they are presented with exceptions during training?', 'Some teasers from our findings:', 'They do overgeneralise!\n\nConvolution based models and Transformers first overgeneralise and then memorise the exception, LSTMs have difficulty accommodating both rules and exceptions.\n\n(For the impact of the exception frequency, check the paper!) https://t.co/FJYLhjcUu4', 'Although models generally have good scores on the task itself, they do not seem to compute the meanings of phrases in a localist fashion.', 'When comparing the consistency of *incorrect* outputs, however, it seems that in the most difficult condition, none of the models really inferred that words were synonyms.', 'The systematicity scores for all models are surprisingly much lower than the scores for the overall data set, which also requires systematic recombinations of input words.', 'They do overgeneralise!\n\nConvolution based models and Transformers first overgeneralise and then memorise the exception, LSTMs have difficulty accomodating both rules and exceptions.\n\n(For the impact of the exception frequency, check the paper!) https://t.co/DKVRrcXTij', 'Although models generally have good scores on the task itself, they do not seem to compute the meanings of phrases in a localist fashion.', ""To take apart models' ability to correctly compute compositional meanings and to infer synonyms, for this test we look at their *consistency* rather than their accuracy. Also, we consider how close synonyms are in the embedding space."", ""Interestingly, even when the synonyms are distributionally dissimilar (but are identical wrt their 'translation'), transformer models put them close together in the embedding space."", 'When comparing the consistency of *incorrect* outputs, however, it seems that in the most difficult condition, none of the models really inferred that words were synonyms.', 'To assess models productivity, we redistribute the training data such that all long sequences are in the test data.', 'This allows us to compare models\' accuracy when they  *do* have evidence for a particular length of sequence and when they do not. We measure this in terms of different definitions of ""length"", such as number of tokens and parse tree depth.', 'Probably unsurprisingly to many, models are not very good at generalising to longer sequences. https://t.co/Mv7dLzZ65e', 'Interestingly, this does *not* seem to stem from early emission of their end-of-sequence symbol (the ""&lt;eos&gt;-problem""): For all architectures, the percentage of times the generated output is contained in the true output is less than 20%.', 'The systematicity scores for all models are surprisingly much lower than the scores for the overall data set, which also requires systematic recombinations of input words.', 'Why? What else? And what does this have to do with compositionality in natural language?\n\nFor more results, motivation and elaborate discussion, have a look: https://t.co/AqyUZUGHNg!', ""@strubell @AmsterdamNLP I looked at your workshop website before! It looks really interesting and I would love to be there but I won't be able to attend NeurIPS this year""]",https://arxiv.org/abs/1908.08351,"Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement. ",Compositionality decomposed: how do neural networks generalise?
97,1164334533908545536,1001049754787368960,Dr. Yu-Dai Tsai,['Our new paper is now out on arXiv: <LINK>'],https://arxiv.org/abs/1908.07525,"We study hidden sector and long-lived particles at past (CHARM and NuCal), present (NA62 and SeaQuest/DarkQuest), and future (LongQuest) experiments that are at the high-energy frontier of the intensity frontier. We focus on exploring the minimal vector portal and variere-lifetime particles (VLP). VLP models have mostly been devised to explain experimental anomalies while avoiding existing constraints, and we demonstrate that proton fixed-target experiments provide one of the most powerful probes for the sub-GeV to few GeV mass range of the VLP models, using inelastic dark matter (iDM) as an example. We consider an iDM model with small mass splitting that yields the observed dark matter (DM) relic abundance, and a scenario with a sizable mass splitting that can also explain the muon $g-2$ anomaly. We set strong limits based on the CHARM and NuCal experiments, which come close to excluding iDM as full-abundance thermal DM candidates in the MeV to GeV mass range, for the mass arrangements and small mass splittings we consider. We also study the future projections based on NA62 and SeaQuest/DarkQuest, and update the constraints of the minimal dark photon parameter space. We found that NuCal sets the only existing constraint in $\epsilon \sim 10^{-8} - 10^{-4}$ regime reaching $\sim$ 800 MeV in dark photon mass due to the resonant enhancement of the proton bremsstrahlung production. Finally, we propose LongQuest, a three-stage thorough retool of the SeaQuest experiment with short ($\lesssim$ 5 m), medium ($\sim$ 5 m), and long baseline ($\gtrsim$ 35 m) tracking stations/detectors, as a multi-purpose machine to explore dark sector particles with a wide range of couplings to the standard model sector. ","The High-Energy Frontier of the Intensity Frontier: Closing the Dark
  Photon, Inelastic Dark Matter, and Muon g-2 Windows"
98,1161443195479371776,280403336,Sean Welleck,"['our new paper:\n\n""Neural Text dÃ∂eÃ∂Generation with Unlikelihood Training""\n\nis now on arxiv! (w/ @uralik1, @stephenroller, Emily Dinan, @kchonyc, @jaseweston) <LINK>\n\nA step towards solving the case of neural text degeneration üîé <LINK>', ""@uralik1 @stephenroller @kchonyc @jaseweston Language models (e.g. GPT-2) tend to produce repetitive and dull ('degenerate') text, especially with greedy or beam search https://t.co/EFLQlZdu3a"", ""We propose 'unlikelihood training', which augments maximum likelihood with penalties on certain candidate tokens https://t.co/kxdQYZzjpH"", 'Using unlikelihood loss at the token and sequence levels  reduces repetitions, leading to improved generation quality https://t.co/EeM93TccUd']",https://arxiv.org/abs/1908.04319,"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques. ",Neural Text Generation with Unlikelihood Training
99,1159778961381089280,1002014071,Frank Wilczek,['New paper with Jordan Cotler on measuring entangled many-body wave functions: <LINK> .   Strategy reminiscent of Mastermind: <LINK> <LINK>'],https://arxiv.org/abs/1908.02754,"It is now experimentally possible to entangle thousands of qubits, and efficiently measure each qubit in parallel in a distinct basis. To fully characterize an unknown entangled state of $n$ qubits, one requires an exponential number of measurements in $n$, which is experimentally unfeasible even for modest system sizes. By leveraging (i) that single-qubit measurements can be made in parallel, and (ii) the theory of perfect hash families, we show that all $k$-qubit reduced density matrices of an $n$ qubit state can be determined with at most $e^{\mathcal{O}(k)} \log^2(n)$ rounds of parallel measurements. We provide concrete measurement protocols which realize this bound. As an example, we argue that with current experiments, the entanglement between every pair of qubits in a system of 1000 qubits could be measured and completely characterized in a few days. This corresponds to completely characterizing entanglement of nearly half a million pairs of qubits. ",Quantum Overlapping Tomography
100,1159696764921733120,751985317742120961,Andrew Zic,"['New paper out on the arXiv: <LINK>\nWe were using ASKAP to target Proxima Cen as a VAST survey test obs, and we noticed a circularly polarised point source with no archival multiwavelength counterparts. Parkes UWL follow-up found a 2.7 ms pulsar! <LINK>', 'this finding is an excellent illustration of the strength of wide-field radio astronomy for finding unexpected things in your data!', 'Awesome work by the whole team to put this together quickly -  D Kaplan, S Dai, @elenchically, @taraxmurphy, @Astro_Dougal, @jfkaczmarek and others from @CSIRO_ATNF \n\nWhat a blast']",https://arxiv.org/abs/1908.03163,"We identified a highly-polarized, steep-spectrum radio source in a deep image with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope at 888 MHz. After considering and rejecting a stellar origin for this source, we discovered a new millisecond pulsar (MSP) using observations from the Parkes radio telescope. This pulsar has period 2.77 ms and dispersion measure 228.27 pc/cm**3. Although this pulsar does not yet appear to be particularly remarkable, the short spin period, wide profile and high dispersion measure do make it relatively hard to discover through traditional blind periodicity searches. Over the course of several weeks we see changes in the barycentric period of this pulsar that are consistent with orbital motion in a binary system, but the properties of any binary need to be confirmed by further observations. While even a deep ASKAP survey may not identify large numbers of new MSPs compared to the existing population, it would be competitive with existing all-sky surveys and could discover interesting new MSPs at high Galactic latitude without the need for computationally-expensive all-sky periodicity searches. ","Serendipitous Discovery of PSR J1431-6328 as a Highly-Polarized Point
  Source with the Australian SKA Pathfinder"
101,1159365444735815680,1086210121,Jan Bolmer,['New paper out: New constraints on the physical conditions in H2-bearing GRB-host damped Lyman-Œ± absorbers\n<LINK>'],https://arxiv.org/abs/1908.02309,"We report the detections of molecular hydrogen (H$_2$), vibrationally-excited H$_2$ (H$^*_2$), and neutral atomic carbon (CI), in two new afterglow spectra of GRBs\,181020A ($z=2.938$) and 190114A ($z=3.376$), observed with X-shooter at the Very Large Telescope (VLT). Both host-galaxy absorption systems are characterized by strong damped Lyman-$\alpha$ absorbers (DLAs) and substantial amounts of molecular hydrogen with $\log N$(HI, H$_2$) = $22.20\pm 0.05,~20.40\pm 0.04$ (GRB\,181020A) and $\log N$(HI, H$_2$) = $22.15\pm 0.05,~19.44\pm 0.04$ (GRB\,190114A). The DLA metallicites, depletion levels and dust extinctions are [Zn/H] = $-1.57\pm 0.06$, [Zn/Fe] = $0.67\pm 0.03$, and $A_V = 0.27\pm 0.02$\,mag (GRB\,181020A) and [Zn/H] = $-1.23\pm 0.07$, [Zn/Fe] = $1.06\pm 0.08$, and $A_V = 0.36\pm 0.02$\,mag (GRB\,190114A). We then examine the molecular gas content of all known H$_2$-bearing GRB-DLAs and explore the physical conditions and characteristics of these systems. We confirm that H$_2$ is detected in all CI- and H$^*_2$-bearing GRB absorption systems, but that these rarer features are not necessarily detected in all GRB H$_2$ absorbers. We find that a large molecular fraction of $f_{\rm H_2} \gtrsim 10^{-3}$ is required for CI to be detected. The defining characteristic for H$^*_2$ to be present is less clear, though a large H$_2$ column density is an essential factor. We then derive the H$_2$ excitation temperatures of the molecular gas and find that they are relatively low with $T_{\rm ex} \approx 100 - 300$\,K, however, there could be evidence of warmer components populating the high-$J$ H$_2$ levels in GRBs\,181020A and 190114A. Finally, we demonstrate that the otherwise successful X-shooter GRB afterglow campaign is hampered by a significant dust bias excluding the most dust-obscured H$_2$ absorbers from identification [Abridged]. ","New constraints on the physical conditions in H$_2$-bearing GRB-host
  damped Lyman-$\alpha$ absorbers"
102,1171721215138045953,4873684235,Ryan McConville,['In this work we propose a new reinforcement learning method that integrates feedback from multiple sources to better tailor policy. We demonstrate it performing feature selection on a #wearable to improve energy efficiency in a #smarthome #IoT setting.\n\n<LINK>'],https://arxiv.org/abs/1908.06134,"Recent advances in both machine learning and Internet-of-Things have attracted attention to automatic Activity Recognition, where users wear a device with sensors and their outputs are mapped to a predefined set of activities. However, few studies have considered the balance between wearable power consumption and activity recognition accuracy. This is particularly important when part of the computational load happens on the wearable device. In this paper, we present a new methodology to perform feature selection on the device based on Reinforcement Learning (RL) to find the optimum balance between power consumption and accuracy. To accelerate the learning speed, we extend the RL algorithm to address multiple sources of feedback, and use them to tailor the policy in conjunction with estimating the feedback accuracy. We evaluated our system on the SPHERE challenge dataset, a publicly available research dataset. The results show that our proposed method achieves a good trade-off between wearable power consumption and activity recognition accuracy. ","Online Feature Selection for Activity Recognition using Reinforcement
  Learning with Multiple Feedback"
103,1169154244496302085,1485111200,Franck Iutzeler,"['""On the convergence of single-call stochastic extra-gradient methods"" <LINK> w/ Y.-G. Hsieh, P. Mertikopoulos, J. Malick is accepted at #NeurIPS2019 \nWe study the rate of Extragradient variants with 1 oracle call/iter. to solve Variational Inequalities']",https://arxiv.org/abs/1908.08465,"Variational inequalities have recently attracted considerable interest in machine learning as a flexible paradigm for models that go beyond ordinary loss function minimization (such as generative adversarial networks and related deep learning systems). In this setting, the optimal $\mathcal{O}(1/t)$ convergence rate for solving smooth monotone variational inequalities is achieved by the Extra-Gradient (EG) algorithm and its variants. Aiming to alleviate the cost of an extra gradient step per iteration (which can become quite substantial in deep learning applications), several algorithms have been proposed as surrogates to Extra-Gradient with a \emph{single} oracle call per iteration. In this paper, we develop a synthetic view of such algorithms, and we complement the existing literature by showing that they retain a $\mathcal{O}(1/t)$ ergodic convergence rate in smooth, deterministic problems. Subsequently, beyond the monotone deterministic case, we also show that the last iterate of single-call, \emph{stochastic} extra-gradient methods still enjoys a $\mathcal{O}(1/t)$ local convergence rate to solutions of \emph{non-monotone} variational inequalities that satisfy a second-order sufficient condition. ",On the convergence of single-call stochastic extra-gradient methods
104,1168406057821048832,1710697381,Diego F. Torres,"['1/n: We release today (MNRAS) our study on the ""Synchro-curvature modelling of the multi-frequency non-thermal emission of pulsars"". <LINK> It includes the analysis of the MW spectrum of 36 pulsars developing further a theoretical model we introduced last year. <LINK>']",https://arxiv.org/abs/1908.11574,"We apply a synchro-curvature spectral emission model based on characterizing the dynamics of magnetospheric particles to fit the phase-average spectra of the most extended database for the non-thermal spectra of pulsars. We consider 36 pulsars with well-determined non-thermal spectra from X-rays to gamma-rays. The sample includes Crab and Crab twin, for which the spectra extends even to the optical/ultraviolet and infrared energies. We find that the model --with just three physical parameters and a global scaling-- can fit the observations well across eight orders of magnitude for 18 of the 36 pulsars studied. Additionally, we find a set of 8 pulsars for which the model still provides arguably good fits and another set of 10 pulsars for which the model fails in reproducing the spectra. We discuss why, propose and provide physical interpretations for a simple model extension (related to the geometry of the accelerating system with regards to the observer) that allows dealing with all such cases, ultimately providing very good fits for all pulsars. The extended model is still austere, adding only two additional parameters to the former set, of the same kind of the ones previously used. We use these fits to discuss issues going from the observed spectral origin, to the extent of the dominance of synchrotron or curvature regimes, the use of a model as predictor for searching new non-thermal pulsars starting from gamma-ray surveys, and how the model offers a setting where phase shifts between X-ray and gamma-ray light curves would naturally arise. ","Synchro-curvature modelling of the multi-frequency non-thermal emission
  of pulsars"
105,1167245270733557761,2600194802,Grumpy D Kelson,['Am I allowed to say that I find this plot to be totally astounding?\n\nAll because we remembered Gauss‚Äôs theorem...\n\n<LINK> <LINK>'],https://arxiv.org/abs/1908.08952v1,"A key obstacle to developing a satisfying theory of galaxy evolution is the difficulty in extending analytic descriptions of early structure formation into full nonlinearity, the regime in which galaxy growth occurs. Extant techniques, though powerful, are based on approximate numerical methods whose Monte Carlo-like nature hinders intuition building. Here, we develop a new solution to this problem and its empirical validation. We first derive closed-form analytic expectations for the evolution of fixed percentiles in the real-space cosmic density distribution, {\it averaged over representative volumes observers can track cross-sectionally\/}. Using the Lagrangian forms of the fluid equations, we show that percentiles in $\delta$---the density relative to the median---should grow as $\delta(t)\propto\delta_{0}^{\alpha}\,t^{\beta}$, where $\alpha\equiv2$ and $\beta\equiv2$ for Newtonian gravity at epochs after the overdensities transitioned to nonlinear growth. We then use 9.5 sq.~deg.~of Carnegie-Spitzer-IMACS Redshift Survey data to map {\it galaxy\/} environmental densities over $0.2<z<1.5$ ($\sim$7~Gyr) and infer $\alpha=1.98\pm0.04$ and $\beta=2.01\pm0.11$---consistent with our analytic prediction. These findings---enabled by swapping the Eulerian domain of most work on density growth for a Lagrangian approach to real-space volumetric averages---provide some of the strongest evidence that a lognormal distribution of early density fluctuations indeed decoupled from cosmic expansion to grow through gravitational accretion. They also comprise the first exact, analytic description of the nonlinear growth of structure extensible to (arbitrarily) low redshift. We hope these results open the door to new modeling of, and insight-building into, the diversity of galaxy growth in cosmological contexts. ","] Gravity and the nonlinear growth of structure in the
  Carnegie-Spitzer-IMACS Redshift Survey"
106,1166279309746757632,96629016,N√©stor Espinoza,"['So, suppose you look through Kepler or TESS data and you find a lightcurve which shows only *one* transit of an exoplanet: what would you do to estimate its period? Well, we got you covered with this great paper led by @EmSandford: <LINK> (MNRAS accepted) [1/n] <LINK>', ""In the paper we use an already noted fact by previous authors that by Kepler's laws, the period can be constrained if you had precise stellar densities. However none of the previous authors consider eccentricity in their fits, nor the usage of Gaia to get stellar densities! [2/n]"", 'So, how do we estimate precise stellar densities? @rafaelbrahm wrote a code to do this in which you use the precise Gaia parallax, (spectroscopic) effective temperature and broadband photometry to get a precise stellar radius. Then, you use isochrones to get a stellar mass. [3/n]', ""We've been called in the past to show how this works in practice, and here we show it for the first time: we got stellar densities with our method for stars that have precise stellar densities with asteroseismology. I'll let you draw the conclusions from the plot: [4/n] https://t.co/AGWVmURtFw"", 'With this at hand, we first validated our method assuming normal transiting planets were single transiters. We fit for all the transit parameters (eccentricity, limb-darkening, etc.) and try to recover the (known) period. We do overall a pretty good job --- no biases! [5/n] https://t.co/UaJVONDUfo', 'We do find, however, that order-of-magnitude calculations on how the precision of the retrieved period scales with the real period are severely underestimated: our method shows that retrieving periods from single transiters is much harder to do (though not impossible)! [6/n] https://t.co/J75kUhsWaV', ""Interestingly, the precision of the retrieved period doesn't scale as much with precision in, eg, stellar density as it does with *limb-darkening*. If we could find ways to improve our knowledge on limb-darkening, perhaps we could get more precise periods! [7/n] https://t.co/KgOMRbBELc"", 'Overall, using our method (stellar densities via Gaia + isochrones) gives about a three-fold improvement in the period precision (15%) with respect to previous works for circular orbits. When we fit for eccentricity, the period error is of the same order as the real period! [8/n]', ""The code to fit for single transiters is in GitHub (nespinoza/single) but I have already implemented the stellar density parametrization within juliet (https://t.co/sWdDTxT00d). \n\nHuge congrats to @EmSandford for leading this effort --- reminder again she's in the job market!"", ""@TheHelenGiles @EmSandford I would say yes based on some experiments we've ran (and some broadband discrepancies I've seen on some bands using Joel Hartman's code), but I would like to summon @rafaelbrahm here :-)."", ""@CristobalSifon @EmSandford You don't ;-). That's part of the game with single transits.""]",https://arxiv.org/abs/1908.08548,"When a planet is only observed to transit once, direct measurement of its period is impossible. It is possible, however, to constrain the periods of single transiters, and this is desirable as they are likely to represent the cold and far extremes of the planet population observed by any particular survey. Improving the accuracy with which the period of single transiters can be constrained is therefore critical to enhance the long-period planet yield of surveys. Here, we combine Gaia parallaxes with stellar models and broad-band photometry to estimate the stellar densities of K2 planet host stars, then use that stellar density information to model individual planet transits and infer the posterior period distribution. We show that the densities we infer are reliable by comparing with densities derived through asteroseismology, and apply our method to 27 validation planets of known (directly measured) period, treating each transit as if it were the only one, as well as to 12 true single transiters. When we treat eccentricity as a free parameter, we achieve a fractional period uncertainty over the true single transits of $94^{+87}_{-58}\%$, and when we fix $e=0$, we achieve fractional period uncertainty $15^{+30}_{-6}\%$, a roughly threefold improvement over typical period uncertainties of previous studies. ",Estimation of singly-transiting K2 planet periods with Gaia parallaxes
107,1164893559402745856,789306537781104640,Scott Fleming,"['Our first paper to study short duration stellar flares with GALEX archival UV data is on arXiv today! Led by @cebrasseur, we found almost 2,000 flares from 1,000 Kepler stars. These flares are way too short to detect in the @NASAKepler data itself. <LINK> <LINK>', ""Oh, and our sample is from stars that had *simultaneous* Kepler+GALEX coverage. It's a nice complement to other Kepler flare studies, since it's the UV and sampled at the seconds level. A future paper will compare the UV and optical properties of some of the flares! Stay tuned."", ""This figure does a good job highlighting the parameter range that can be explored with gPhoton. BUT: as with most things, there's a lot of systematics and calibration issues you need to take into account, it's not just plug+play! https://t.co/xvz4B0uuSJ""]",https://arxiv.org/abs/1908.08377,"We report on a population of short duration near-ultraviolet (NUV) flares in stars observed by the Kepler and GALEX missions. We analyzed NUV light curves of 34,276 stars observed from 2009-2013 by both the GALEX (NUV) and Kepler (optical) space missions with the eventual goal of investigating multi-wavelength flares. From the GALEX data we constructed light curves with a 10 second cadence, and ultimately detected 1,904 short duration flares on 1,021 stars. The vast majority (94.5\%) of these flares have durations less than five minutes, with flare flux enhancements above the quiescent flux level ranging from 1.5 to 1700. The flaring stars are primarily solar-like, with T$_{\rm eff}$ ranging from 3,000-11,000 K and radii between 0.5-15 R$_{\odot}$. This set of flaring stars is almost entirely distinct from that of previous flare surveys of Kepler data and indicates a previously undetected collection of small flares contained within the Kepler sample. The range in flare energies spans 1.8$\times$10$^{32}$-8.9$\times$10$^{37}$ erg, with associated relative errors spanning 2-87\%. The flare frequency distribution by energy follows a power-law with index $\alpha=1.72\pm0.05$, consistent with results of other solar and stellar flare studies at a range of wavelengths. This supports the idea that the NUV flares we observe are governed by the same physical processes present in solar and optical flares. The relationship between flare duration and associated flare energy extends results found for solar and stellar white-light flares, and suggests that these flares originate in regions with magnetic field strengths of several hundred Gauss, and length scales of order 10$^{10}$ cm. ",Short Duration Stellar Flares in GALEX Data
108,1164515219466530816,2180768821,Erik Hoel,"['Networks can have informative macroscales, but how do you find them? In new work with @RossGriebenow and @jkbren we compare methods for searching the space of possible scales: <LINK>. We also show just how many important network properties change at higher scales <LINK>']",https://arxiv.org/abs/1908.07565,"All networks can be analyzed at multiple scales. A higher scale of a network is made up of macro-nodes: subgraphs that have been grouped into individual nodes. Recasting a network at higher scales can have useful effects, such as decreasing the uncertainty in the movement of random walkers across the network while also decreasing the size of the network. However, the task of finding such a macroscale representation is computationally difficult, as the set of all possible scales of a network grows exponentially with the number of nodes. Here we compare various methods for finding the most informative scale of preferential attachment networks, discovering that an approach based on spectral analysis outperforms greedy and gradient descent-based methods. We then use this procedure to show how several structural properties of these networks vary across scales. We describe how meso- and macroscale representations of networks can have significant benefits over their underlying microscale in terms of information transmission, which include properties such as increase in determinism, a decrease in degeneracy, a lower entropy rate of random walkers on the network, an increase in global network efficiency, and higher values for a variety of centrality measures than the microscale. ","Finding the right scale of a network: Efficient identification of causal
  emergence through spectral clustering"
109,1164397164409348102,948995274961309697,Andrea Botteon,"['My latest accepted paper is on the arxiv today! #irapapers\nWe studied the nearby cluster pair RXCJ1825/CIZAJ1824 (or Lyra complex) with @LOFAR and @ESA_XMM pointing out the role of cluster dynamics in the formation of diffuse radio emission in the ICM 1/9\n<LINK> <LINK>', 'We discovered a giant radio halo in RXCJ1825 (ongoing merger cluster). This is the least powerful radio halo known to date and one of the least massive systems to host this kind of diffuse emission. Assuming alpha=1.3, it would fall a factor 2-4 below the P1.4-M500 relation 2/9 https://t.co/gYDVdzLEz9', 'The halo has a low-surface brightness extension towards the SW due to the interaction with a galaxy group. The Southern Galaxy (SG) is likely the remnant of such a group (post-merger). The largest linear extent of the diffuse radio emission is up to ~1.8 Mpc 3/9 https://t.co/sbiD50lPDZ', 'No diffuse radio emission is observed in CIZAJ1824 (relaxed) nor in the region between RXCJ1825 and CIZAJ1824 (which are pre-merger). We placed an upper limit for the diffuse radio emission in CIZAJ1824 a factor of 10 below the extrapolation of the P1.4-M500 relation 4/9', 'Overall, there is a good match between the radio and X-ray emission of the two clusters (eg the extension towards the SW). This indicates a tight connection between the non-thermal and thermal components and possibly a common origin 5/9 https://t.co/5OnO3y7qm8', 'We also found an interesting tailed radio galaxy in the Lyra complex. Its forked morphology might be due to projection effects related to the motion of the host galaxy in the system 7/9 https://t.co/3SZeeKCTiS', 'Our paper shows that @LOFAR 1) is able to detected low-z/low-surface brightness radio halos that were missed by previous instruments/surveys and 2) allows us to investigate the low-mass/low-power end of the P1.4-M500 scaling relation 8/9', 'We also provided quantitative support to the idea that cluster mergers play a crucial role in the generation of non-thermal components in the ICM. Our results fit very well with the findings from the detailed X-ray and optical works mentioned here https://t.co/kvwwOV0kTl 9/9']",https://arxiv.org/abs/1908.07527,"Diffuse radio emission associated with the intra-cluster medium (ICM) is observed in a number of merging galaxy clusters. It is currently believed that in mergers a fraction of the kinetic energy is channeled into non-thermal components, such as turbulence, cosmic rays and magnetic fields, that may lead to the formation of giant synchrotron sources in the ICM. Studying merging galaxy clusters in different evolutionary phases is fundamental to understanding the origin of radio emission in the ICM. We observed the nearby galaxy cluster pair RXC J1825.3+3026 ($z\sim0.065$) and CIZA J1824.1+3029 ($z\sim0.071$) at 120-168 MHz with the LOw Frequency ARray (LOFAR) and made use of a deep (240 ks) XMM-Newton dataset to study the non-thermal and thermal properties of the system. RXC J1825.3+3026 is in a complex dynamical state, with a primary on-going merger in the E-W direction and a secondary later stage merger with a group of galaxies in the SW, while CIZA J1824.1+3029 is dynamically relaxed. These two clusters are in a pre-merger phase. We report the discovery of a Mpc-scale radio halo with a low surface brightness extension in RXC J1825.3+3026 that follows the X-ray emission from the cluster center to the remnant of a galaxy group in the SW. This is among the least massive systems and the faintest giant radio halo known to date. Contrary to this, no diffuse radio emission is observed in CIZA J1824.1+3029 nor in the region between the pre-merger cluster pair. The power spectra of the X-ray surface brightness fluctuations of RXC J1825.3+3026 and CIZA J1824.1+3029 are in agreement with the findings for clusters exhibiting a radio halo and the ones where no radio emission has been detected, respectively. We provide quantitative support to the idea that cluster mergers play a crucial role in the generation of non-thermal components in the ICM. ","Particle acceleration in a nearby galaxy cluster pair: the role of
  cluster dynamics"
110,1163898677854818304,17354555,Emilio Ferrara,"['Great work by @palashiitkgp &amp; team: We propose &amp; developed an open-source library to benchmark any graph embedding methods. \n\nWe hope this will foster standardization and more research in the area of graph embeddings!\n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'With thanks to @aaronclauset &amp; team for providing most real world datasets! Check out their project: https://t.co/aj5qkD5Hbj']",https://arxiv.org/abs/1908.06543,"Graph embedding is the task of representing nodes of a graph in a low-dimensional space and its applications for graph tasks have gained significant traction in academia and industry. The primary difference among the many recently proposed graph embedding methods is the way they preserve the inherent properties of the graphs. However, in practice, comparing these methods is very challenging. The majority of methods report performance boosts on few selected real graphs. Therefore, it is difficult to generalize these performance improvements to other types of graphs. Given a graph, it is currently impossible to quantify the advantages of one approach over another. In this work, we introduce a principled framework to compare graph embedding methods. Our goal is threefold: (i) provide a unifying framework for comparing the performance of various graph embedding methods, (ii) establish a benchmark with real-world graphs that exhibit different structural properties, and (iii) provide users with a tool to identify the best graph embedding method for their data. This paper evaluates 4 of the most influential graph embedding methods and 4 traditional link prediction methods against a corpus of 100 real-world networks with varying properties. We organize the 100 networks in terms of their properties to get a better understanding of the embedding performance of these popular methods. We use the comparisons on our 100 benchmark graphs to define GFS-score, that can be applied to any embedding method to quantify its performance. We rank the state-of-the-art embedding approaches using the GFS-score and show that it can be used to understand and evaluate novel embedding approaches. We envision that the proposed framework (this https URL) will serve the community as a benchmarking platform to test and compare the performance of future graph embedding techniques. ",Benchmarks for Graph Embedding Evaluation
111,1163794283515785217,1104703454,Francesco Silvestri,"['Tensor architectures (like @GoogleAI TPU and @nvidia TC)  have been developed for accelerating Deep Learning applications. But can we exploit their computational power for other problems? We study this question in <LINK>. The heart of tensor architectures is', 'a hardware circuit for multiplying small and dense matrices. We propose a computational model that captures this feature, and then design algorithms accelerating some linear algebra problems by exploiting tensor architectures. Joint work with @Flav1oV.']",https://arxiv.org/abs/1908.06649,"To respond to the need of efficient training and inference of deep neural networks, a plethora of domain-specific hardware architectures have been introduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A common feature of these architectures is a hardware circuit for efficiently computing a dense matrix multiplication of a given small size. In order to broaden the class of algorithms that exploit these systems, we propose a computational model, named the TCU model, that captures the ability to natively multiply small matrices. We then use the TCU model for designing fast algorithms for several problems, including matrix operations (dense and sparse multiplication, Gaussian Elimination), graph algorithms (transitive closure, all pairs shortest distances), Discrete Fourier Transform, stencil computations, integer multiplication, and polynomial evaluation. We finally highlight a relation between the TCU model and the external memory model. ",A Computational Model for Tensor Core Units
112,1162331688111882240,370172421,Peter Bloem,"['New preprint! Based on MSc work by Floris Hermsen and Wolf Vos.\n\n<LINK>\n\nWe study complex multigraphs with potentially thousands of labeled connections between nodes (think financial transaction networks), and demonstrate end-to-end learning. <LINK>']",https://arxiv.org/abs/1908.05365,"We study the problem of end-to-end learning from complex multigraphs with potentially very large numbers of edges between two vertices, each edge labeled with rich information. Examples range from communication networks to flights between airports or financial transaction graphs. We propose Latent-Graph Convolutional Networks (L-GCNs), which propagate information from these complex edges to a latent adjacency tensor, after which further downstream tasks can be performed, such as node classification. We evaluate the performance of several variations of the model on two synthetic datasets simulating fraud in financial transaction networks, ensuring the model must make use of edge labels in order to achieve good classification performance. We find that allowing for nonlinear interactions on a per-neighbor basis boosts performance significantly, while showing promising results in an inductive setting. Finally, we demonstrate the use of L-GCNs on real-world data in the form of an urban transportation network. ","End-to-End Learning from Complex Multigraphs with Latent-Graph
  Convolutional Networks"
113,1161843855517507587,312448486,Dr. Karan Jani,['Our new preprint on #IndependenceDay2019 \n\nSpecial BLACK HOLES that we will find in every gravitational wave detector for the NEXT 20 YEARS - from @LIGOIndia to next avatar of @LIGO to Einstein Telescope to NASA-ESA mission LISA (@LISACommunity). \n\n<LINK> <LINK>'],https://arxiv.org/abs/1908.04985,"The direct measurement of gravitational waves is a powerful tool for surveying the population of black holes across the universe. The first gravitational wave catalog from LIGO has detected black holes as heavy as $\sim50~M_\odot$, colliding when our Universe was about half its current age. However, there is yet no unambiguous evidence of black holes in the intermediate-mass range of $10^{2-5}~M_\odot$. Recent electromagnetic observations have hinted at the existence of IMBHs in the local universe; however, their masses are poorly constrained. The likely formation mechanisms of IMBHs are also not understood. Here we make the case that multiband gravitational wave astronomy --specifically, joint observations by space- and ground-based gravitational wave detectors-- will be able to survey a broad population of IMBHs at cosmological distances. By utilizing general relativistic simulations of merging black holes and state-of-the-art gravitational waveform models, we classify three distinct population of binaries with IMBHs in the multiband era and discuss what can be observed about each. Our studies show that multiband observations involving the upgraded LIGO detector and the proposed space-mission LISA would detect the inspiral, merger and ringdown of IMBH binaries out to redshift ~2. Assuming that next-generation detectors, Einstein Telescope, and Cosmic Explorer, are operational during LISA's mission lifetime, we should have multiband detections of IMBH binaries out to redshift ~5. To facilitate studies on multiband IMBH sources, here we investigate the multiband detectability of IMBH binaries. We provide analytic relations for the maximum redshift of multiband detectability, as a function of black hole mass, for various detector combinations. Our study paves the way for future work on what can be learned from IMBH observations in the era of multiband gravitational wave astronomy. ","Detectability of Intermediate-Mass Black Holes in Multiband
  Gravitational Wave Astronomy"
114,1159789502371651585,4639078397,John Wise,['Paper day! In a study led by @jaregan we report on several dozen more direct collapse black hole candidates in our simulations. These could be the seeds of most central supermassive black holes <LINK>'],https://arxiv.org/abs/1908.02823,"Using the Renaissance suite of simulations we examine the emergence of pristine atomic cooling haloes that are both metal-free and star-free in the early Universe. The absence of metals prevents catastrophic cooling, suppresses fragmentation, and may allow for the formation of massive black hole seeds. Here we report on the abundance of pristine atomic cooling haloes found and on the specific physical conditions that allow for the formation of these direct-collapse-black-hole (DCBH) haloes. In total in our simulations we find that 79 DCBH haloes form before a redshift of 11.6. We find that the formation of pristine atomic haloes is driven by the rapid assembly of the atomic cooling haloes with mergers, both minor and/or major, prior to reaching the atomic cooling limit a requirement. However, the ability of assembling haloes to remain free of (external) metal enrichment is equally important and underlines the necessity of following the transport of metals in such simulations. The candidate DCBH hosting haloes we find, have been exposed to mean Lyman-Werner radiation fields of J$_{LW}$ $\sim$ 1 J$_{21}$ and typically lie at least 10 kpc (physical) from the nearest massive galaxy. Growth rates of the haloes reach values of greater than 10$^7$ M$_{\odot}$ per unit redshift, leading to significant dynamical heating and the suppression of efficient cooling until the halo crosses the atomic cooling threshold. Finally, we also find five synchronised halo candidates where pairs of pristine atomic cooling haloes emerge that are both spatially and temporally synchronised. ","The Emergence of the First Star-free Atomic Cooling Haloes in the
  Universe"
115,1159638936274292736,1134665912227946497,Kaiyu Yang,"['Our paper ""SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition"" (<LINK>) is accepted to #iccv2019. We propose adversarial crowdsourcing to reduce dataset bias and find challenging examples in the long tail. <LINK>']",https://arxiv.org/abs/1908.02660,"Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be ""behind"" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be ""next to"" each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at this https URL ","SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial
  Relation Recognition"
116,1159093871718141952,1157603230693625857,Valerio Sorichetti,"['The preprint of our new article is out!\n\n<LINK>\n\nWe discuss the limitations of conventional ways to measure the mesh size in polymeric systems, and propose a new, precise method based on the concept of pore size distribution.']",https://arxiv.org/abs/1908.01484,"In order to characterize the geometrical mesh size $\xi$, we simulate a solution of coarse-grained polymers with densities ranging from the dilute to the concentrated regime and for different chain lengths. Conventional ways to estimate $\xi$ rely either on scaling assumptions which give $\xi$ only up to an unknown multiplicative factor, or on measurements of the monomer density fluctuation correlation length $\xi_c$. We determine $\xi_c$ from the monomer structure factor and from the radial distribution function, and find that the identification $\xi=\xi_c$ is not justified outside of the semidilute regime. In order to better characterize $\xi$, we compute the pore size distribution (PSD) following two different definitions, one by Torquato et al. (Ref.1) and one by Gubbins et al. (Ref.2). We show that the mean values of the two distributions, $\langle r \rangle_T$ and $\langle r \rangle_G$, both display the behavior predicted for $\xi$ by scaling theory, and argue that $\xi$ can be identified with either one of these quantities. This identification allows to interpret the PSD as the distribution of mesh sizes, a quantity which conventional methods cannot access. Finally, we show that it is possible to map a polymer solution on a system of hard or overlapping spheres, for which Torquato's PSD can be computed analytically and reproduces accurately the PSD of the solution. We give an expression that allows $\langle r \rangle_T$ to be estimated with great accuracy in the semidilute regime by knowing only the radius of gyration and the density of the polymers. ","Characterizing the mesh size of polymer solutions via the pore size
  distribution"
117,1157296970429554688,885237745198747648,"Jennifer Stiso, PhD","['Ever looked at the huge number of neural systems that are involved in BCI performance and wondered how they interact to support learning? We did  <LINK>! We jointly decompose dynamic brain and behavioral data and find an important role for the attention system. <LINK>', 'Work done with @DaniSBassett, and thanks to collaborators @F_DeVicoFallani and @MConstanceCorsi']",https://arxiv.org/abs/1908.00077,"Motor imagery-based brain-computer interfaces (BCIs) use an individuals ability to volitionally modulate localized brain activity as a therapy for motor dysfunction or to probe causal relations between brain activity and behavior. However, many individuals cannot learn to successfully modulate their brain activity, greatly limiting the efficacy of BCI for therapy and for basic scientific inquiry. Previous research suggests that coherent activity across diverse cognitive systems is a hallmark of individuals who can successfully learn to control the BCI. However, little is known about how these distributed networks interact through time to support learning. Here, we address this gap in knowledge by constructing and applying a multimodal network approach to decipher brain-behavior relations in motor imagery-based brain-computer interface learning using MEG. Specifically, we employ a minimally constrained matrix decomposition method (non-negative matrix factorization) to simultaneously identify regularized, covarying subgraphs of functional connectivity, to assess their similarity to task performance, and to detect their time-varying expression. Individuals also displayed marked variation in the spatial properties of subgraphs such as the connectivity between the frontal lobe and the rest of the brain, and in the temporal properties of subgraphs such as the stage of learning at which they reached maximum expression. From these observations, we posit a conceptual model in which certain subgraphs support learning by modulating brain activity in regions important for sustaining attention. To test this model, we use tools that stipulate regional dynamics on a networked system (network control theory), and find that good learners display a single subgraph whose temporal expression tracked performance and whose architecture supports easy modulation of brain regions important for attention. ","Learning in brain-computer interface control evidenced by joint
  decomposition of brain and behavior"
118,1169532059133587456,3352372504,Dr Hannah Williams,['Preprint of my recent work is now on @arxiv - we have found very magnetically insensitive rotational transitions in dipole molecules which means long coherence times taking us one step closer to using molecules for quantum simulation <LINK> #physics #quantum'],https://arxiv.org/abs/1908.11839,"Polar molecules in superpositions of rotational states exhibit long-range dipolar interactions, but maintaining their coherence in a trapped sample is a challenge. We present calculations that show many laser-coolable molecules have convenient rotational transitions that are exceptionally insensitive to magnetic fields. We verify this experimentally for CaF where we find a transition with sensitivity below 5 Hz G$^{-1}$ and use it to demonstrate a rotational coherence time of 6.4(8) ms in a magnetic trap. Simulations suggest it is feasible to extend this to more than 1 s using a smaller cloud in a biased magnetic trap. ",Long rotational coherence times of molecules in a magnetic trap
119,1163722841788993536,704142836736765952,Patrick Gaulme,"['Joyce Guzik and I just published this paper on @arxiv today <LINK> We looked for any kind of stellar pulsators among the @NASAKepler eclipsing binaries... and found plenty! #asteroseismology #binarystars #starsandspace', ""@arxiv @NASAKepler This one's is likely a delta Scuti in a contact binary. To prove it, we're doing spectorscopic observations at #apachepointobservatory https://t.co/QCu2aAvEsZ""]",https://arxiv.org/abs/1908.06773,"Eclipsing binaries (EBs) are unique targets for measuring precise stellar properties and constrain stellar evolution models. In particular, it is possible to measure at the percent level masses and radii of both components of a double-lined spectroscopic EB. Since the advent of high-precision photometric space missions (MOST, CoRoT, Kepler, BRITE, TESS), the use of stellar pulsation properties to infer stellar interiors and dynamics constitutes a revolution for low-mass star studies. The Kepler mission has led to the discovery of thousands of classical pulsators such as $\delta$ Scuti and solar-like oscillators (main sequence and evolved), but also almost 3000 EBs with orbital periods shorter than 1100 days. We report the first systematic search for stellar pulsators in the entire Kepler eclipsing binary catalog. The focus is mainly aimed at discovering $\delta$ Scuti, $\gamma$ Doradus, red giant, and tidally excited pulsators. We developed a data inspection tool (DIT) that automatically produces a series of plots from the Kepler light-curves that allows us to visually identify whether stellar oscillations are present in a given time series. We applied the DIT to the whole Kepler eclipsing binary database and identified 303 systems whose light curves display oscillations, including 163 new discoveries. A total of 149 stars are flagged as $\delta$ Scuti (100 from this paper), 115 stars as $\gamma$ Doradus (69 new), 85 stars as red giants (27 new), 59 as tidally excited oscillators (29 new). There is some overlap among these groups, as some display several types of oscillations. Despite many of these systems are likely to be false positives, i. e., when an EB light curve is blended with a pulsator, this catalog gathers a vast sample of systems that are valuable for a better understanding of stellar evolution. ","Systematic search for stellar pulsators in the eclipsing binaries
  observed by Kepler"
120,1159696764921733120,751985317742120961,Andrew Zic,"['New paper out on the arXiv: <LINK>\nWe were using ASKAP to target Proxima Cen as a VAST survey test obs, and we noticed a circularly polarised point source with no archival multiwavelength counterparts. Parkes UWL follow-up found a 2.7 ms pulsar! <LINK>', 'this finding is an excellent illustration of the strength of wide-field radio astronomy for finding unexpected things in your data!', 'Awesome work by the whole team to put this together quickly -  D Kaplan, S Dai, @elenchically, @taraxmurphy, @Astro_Dougal, @jfkaczmarek and others from @CSIRO_ATNF \n\nWhat a blast']",https://arxiv.org/abs/1908.03163,"We identified a highly-polarized, steep-spectrum radio source in a deep image with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope at 888 MHz. After considering and rejecting a stellar origin for this source, we discovered a new millisecond pulsar (MSP) using observations from the Parkes radio telescope. This pulsar has period 2.77 ms and dispersion measure 228.27 pc/cm**3. Although this pulsar does not yet appear to be particularly remarkable, the short spin period, wide profile and high dispersion measure do make it relatively hard to discover through traditional blind periodicity searches. Over the course of several weeks we see changes in the barycentric period of this pulsar that are consistent with orbital motion in a binary system, but the properties of any binary need to be confirmed by further observations. While even a deep ASKAP survey may not identify large numbers of new MSPs compared to the existing population, it would be competitive with existing all-sky surveys and could discover interesting new MSPs at high Galactic latitude without the need for computationally-expensive all-sky periodicity searches. ","Serendipitous Discovery of PSR J1431-6328 as a Highly-Polarized Point
  Source with the Australian SKA Pathfinder"
