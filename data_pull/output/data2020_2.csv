,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1238550476074999808,1181013626418798592,Leo Duan,"[""Want to add *multi-scale* structure to your fancy model, but don't want to code a partition tree? Now you can do it with a simple matrix operation and run gradient based optimization/sampling. Check out our new paper <LINK>""]",https://arxiv.org/abs/2002.09606,"The multi-scale factor models are particularly appealing for analyzing matrix- or tensor-valued data, due to their adaptiveness to local geometry and intuitive interpretation. However, the reliance on the binary tree for recursive partitioning creates high complexity in the parameter space, making it extremely challenging to quantify its uncertainty. In this article, we discover an alternative way to generate multi-scale matrix using simple matrix operation: starting from a random matrix with each column having two unique values, its Cholesky whitening transform obeys a recursive partitioning structure. This allows us to consider a generative distribution with large prior support on common multi-scale factor models, and efficient posterior computation via Hamiltonian Monte Carlo. We demonstrate its potential in a multi-scale factor model to find broader regions of interest for human brain connectivity. ","Bayesian Multi-scale Modeling of Factor Matrix without using Partition
  Tree"
1,1237806463516430337,53464710,Eric Wong,"['1/ We recently released a new paper studying robust overfitting in adversarial training, which can have a dramatic effect on the final robust test error (see plot)! \n\nPaper: <LINK>\nCode: <LINK>\n\nJoint work with @_leslierice and @zicokolter <LINK>', '2/ The plot here depicts the learning curves for standard PGD adversarial training for CIFAR10. \n\nUnlike in standard training, the best robust checkpoint occurs well before training has converged. \n\nThis happens across multiple datasets, threat models, and training algorithms.', '3/ Leveraging this behavior, we were able to use PGD adversarial training with early stopping to achieve results comparable to top performing algorithms in adversarial robustness (e.g. Trades), whereas training to convergence recovers the result from Madry et al. (2017).', '4/ Strangely enough, the optimal learning rate strategy is to simply decay the learning rate once and train for 1-2 epochs. \n\nAfter this, robust test performance only gets worse. Smoother learning rate schedules only seem to dampen the initial improvement after decay.', '5/ Beyond early stopping, we tested other methods for combatting overfitting (explicit regularization, data augmentation). \n\nIn short, the only approach that improved upon vanilla early stopping was semisupervised data augmentation in combination with early stopping.', ""6/ Future work in adversarially robust training should make explicit whether they're using the best or final checkpoints, since early stopping can make such a large difference. \n\nIf using early stopping, it should be done with a proper held-out validation set (not the test set)!""]",https://arxiv.org/abs/2002.11569,"It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models ($\ell_\infty$ and $\ell_2$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at this https URL ",Overfitting in adversarially robust deep learning
2,1237457907601551362,2716076245,Wei Hu,['New paper <LINK> (w/ @ShamKakade6 @jasondeanlee @SimonShaoleiDu &amp; Qi Lei) showing why samples from multiple source tasks can be pooled together for representation learning. <LINK>'],https://arxiv.org/abs/2002.09434,"This paper studies few-shot learning via representation learning, where one uses $T$ source tasks with $n_1$ data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only $n_2 (\ll n_1)$ data. Specifically, we focus on the setting where there exists a good \emph{common representation} between source and target, and our goal is to understand how much of a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a fast rate of $O\left(\frac{\mathcal{C}\left(\Phi\right)}{n_1T} + \frac{k}{n_2}\right)$; here, $\Phi$ is the representation function class, $\mathcal{C}\left(\Phi\right)$ is its complexity measure, and $k$ is the dimension of the representation. When specialized to linear representation functions, this rate becomes $O\left(\frac{dk}{n_1T} + \frac{k}{n_2}\right)$ where $d (\gg k)$ is the ambient input dimension, which is a substantial improvement over the rate without using representation learning, i.e. over the rate of $O\left(\frac{d}{n_2}\right)$. This result bypasses the $\Omega(\frac{1}{T})$ barrier under the i.i.d. task assumption, and can capture the desired property that all $n_1T$ samples from source tasks can be \emph{pooled} together for representation learning. Next, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural network learning. Our results demonstrate representation learning can fully utilize all $n_1T$ samples from source tasks. ","Few-Shot Learning via Learning the Representation, Provably"
3,1237019424185606144,185910194,Graham Neubig,"['Super-excited about our new #ICASSP2020 paper on ""Universal Phone Recognition with a Multilingual Allophone System"" <LINK>\n\nWe create a multi-lingual ASR model that can do zero-shot phone recognition in up to 2,186 languages! How? A little linguistics :) 1/5', 'In our speech there are phonemes (sounds that can support lexical contrasts in a *particular* language) and their corresponding phones (the sounds that are actually spoken, which are language *independent*). Most multilingual ASR models conflate these two concepts. 2/5 https://t.co/z5UPLqz9w6', 'We create a model that first recognizes to language-independent phones, and then converts these phones to language-specific phonemes. This makes our underlying representations of phones more universal and generalizable across languages. 3/5 https://t.co/vwy9u4gwYs', 'We create a DB of phone-phoneme mappings, AlloVera that contains these mappings for several languages: https://t.co/0ox2hvUcem\n\nWe also demonstrate that we can use PHOIBLE, a large phonetic DB with 2,186 languages to restrict phones in new languages: https://t.co/VNGtbriAMx 4/5', ""Results show that these methods work, helping both more standard multilingual recognition, and recognition on entirely new languages.\n\nNext, we're interested in incorporating these into real systems for low- or zero-resource ASR. Please contact us if you're interested! 5/5 https://t.co/EPcWHvTS9M"", 'Thanks to the *many* people who supported the project, including the participants at the LTLDR 2019 workshop (https://t.co/x0r7rwJUM2), and most of all Xinjian, our fearless leader who actually got the project done.']",https://arxiv.org/abs/2002.11800,"Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. ",Universal Phone Recognition with a Multilingual Allophone System
4,1235922433380683777,796463131925053440,Jamie Dougherty,['Excited to see some discussion of our new paper (<LINK>) already! <LINK>'],https://arxiv.org/abs/2002.08111,"Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets. ",Hierarchical Quantized Autoencoders
5,1235639646962380801,1722330138,Raed Al Kontar,"['Our new paper: ""Weakly-supervised Multi-output Regression via Correlated Gaussian Processes"" [<LINK>], provides a unified framework for label assignment when a subset of data points from multiple groups are unlabeled (or Weekly labeled !)']",https://arxiv.org/abs/2002.08412,"Multi-output regression seeks to infer multiple latent functions using data from multiple groups/sources while accounting for potential between-group similarities. In this paper, we consider multi-output regression under a weakly-supervised setting where a subset of data points from multiple groups are unlabeled. We use dependent Gaussian processes for multiple outputs constructed by convolutions with shared latent processes. We introduce hyperpriors for the multinomial probabilities of the unobserved labels and optimize the hyperparameters which we show improves estimation. We derive two variational bounds: (i) a modified variational bound for fast and stable convergence in model inference, (ii) a scalable variational bound that is amenable to stochastic optimization. We use experiments on synthetic and real-world data to show that the proposed model outperforms state-of-the-art models with more accurate estimation of multiple latent functions and unobserved labels. ","Weakly-supervised Multi-output Regression via Correlated Gaussian
  Processes"
6,1235309708061351938,288623330,Aaron Hertzmann,"['Why are we so good at understanding line drawings, even though they are not part of the natural world? My new paper proposes an answer to this age-old question.\n\n1/\n\nPreprint: <LINK>\nOfficial: <LINK>', 'I’m excited about this paper because it shows a way that computer graphics research can provide insight into human perception. Also, I’ve never published in perceptual psychology before. https://t.co/XHEy9YVSaO', 'The results in Figure 6 really surprised me. In the submission, I made a hypothesis about future computer vision algorithms. A reviewer suggested I try it with current algorithms. I thought there was no way that it would work, but I figured I should humor the reviewer. It worked. https://t.co/vYBO8t1pKv', ""Many studies show that someone who's never seen a line drawing can understand one. An odd study I came across while preparing this article was by Hochberg and Brooks (1962), who raised their son without letting him see any photos or drawings."", 'At 19 months, they tested if he could recognize objects in pictures. He could. https://t.co/2PWFTxgGfa\n(There have been better studies since then.)', '@surya501 @liu_mingyu @scottmccloud Yes, I read the trilogy, it was very interesting.']",https://arxiv.org/abs/2002.06260,"Why is it that we can recognize object identity and 3D shape from line drawings, even though they do not exist in the natural world? This paper hypothesizes that the human visual system perceives line drawings as if they were approximately realistic images. Moreover, the techniques of line drawing are chosen to accurately convey shape to a human observer. Several implications and variants of this hypothesis are explored. ",Why Do Line Drawings Work? A Realism Hypothesis
7,1235194511170719744,448941996,Indro Spinelli,"['New (first for me) paper on graph neural network with Simone Scardapane and Aurelio Uncini.\n\n<LINK>\n\nOur goal was to find the best number of propagation steps for each node in the graph. #geometricdeeplearning #graphneuralnetworks', 'Edit: @s_scardapane has joined twitter!']",https://arxiv.org/abs/2002.10306,"Graph convolutional networks (GCNs) are a family of neural network models that perform inference on graph data by interleaving vertex-wise operations and message-passing exchanges across nodes. Concerning the latter, two key questions arise: (i) how to design a differentiable exchange protocol (e.g., a 1-hop Laplacian smoothing in the original GCN), and (ii) how to characterize the trade-off in complexity with respect to the local updates. In this paper, we show that state-of-the-art results can be achieved by adapting the number of communication steps independently at every node. In particular, we endow each node with a halting unit (inspired by Graves' adaptive computation time) that after every exchange decides whether to continue communicating or not. We show that the proposed adaptive propagation GCN (AP-GCN) achieves superior or similar results to the best proposed models so far on a number of benchmarks, while requiring a small overhead in terms of additional parameters. We also investigate a regularization term to enforce an explicit trade-off between communication and accuracy. The code for the AP-GCN experiments is released as an open-source library. ",Adaptive Propagation Graph Convolutional Network
8,1235001383356518401,231076444,Vasileios Lioutas,"['Excited about our new paper on sequence learning. In the NLP community, attention is considered a necessity for achieving SoTA results but we propose a fast novel attention-free method with comparative performance <LINK>\n\n#NLProc #DeepLearning #MachineLearning']",https://arxiv.org/abs/2002.03184,"To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches. ",Time-aware Large Kernel Convolutions
9,1234585950358888449,75498067,Dr. Dennis Foren 🎨,"[""Hi, all! I'm breaking through my relative radio silence to announce: I've got a NEW PAPER up on the arXiv!\n\n⭐️ Massive Spin-2 Scattering Amplitudes in Extra-Dimensional Theories ⭐️\n<LINK>\n\nThis is the culmination of ~2 years of work, so... I'm pretty stoked! 🥳"", '@LorraineBlack90 Thank you, Esperanza! 😁']",https://arxiv.org/abs/2002.12458,"In this paper we describe in detail the computation of the scattering amplitudes of massive spin-2 Kaluza-Klein excitations in a gravitational theory with a single compact extra dimension, whether flat or warped. These scattering amplitudes are characterized by intricate cancellations between different contributions: although individual contributions may grow as fast as ${\cal O}(s^5)$, the full results grow only as ${\cal O}(s)$. We demonstrate that the cancellations persist for all incoming and outgoing particle helicities and examine how truncating the computation to only include a finite number of intermediate states impacts the accuracy of the results. We also carefully assess the range of validity of the low energy effective Kaluza-Klein theory. In particular, for the warped case we demonstrate directly how an emergent low energy scale controls the size of the scattering amplitude, as conjectured by the AdS/CFT correspondence. ",Massive Spin-2 Scattering Amplitudes in Extra-Dimensional Theories
10,1234544990082064387,951532406414151680,Oz Amram,"['My paper with @crissms21 on ""Tag N\' Train"" is out today! We explore a new way to train ML classifiers directly on real data and demonstrate one use of the technique in a model-agnostic dijet search\n\n(thread)\n\n<LINK>', ""The elephant in the room in HEP right now is that the LHC hasn't found the new physics that many hoped it would. There have been searches for the popular BSM models, we know that there are models we aren't currently covering. The theory space is just too big!"", 'The challenge to us experimentalists is whether we can come up with search strategies that are model-agnostic, which means they would be sensitive to a wide variety of possible BSM models.', ""One idea that was proposed was to use autoencoders (networks that compress and decompress data) to look for anomalous jets in a model-agnostic way. These can be trained directly on data, but because they are trained to do compression they don't end up being the best classifiers https://t.co/JhkcRHZDHe"", ""So our idea was whether we could use these autoencoders + the real data (which is unlabeled) to train better classifiers. \nThis is where the Tag N' Train (TNT) comes in"", ""The basic idea is that if your data 2 sub-objects (ie jet1 and jet2). You can tag an event as signal-like or background-like using jet1. Then you can train a classifier to distinguish between the jet2's in these two samples and it will learn to find the signal!"", 'Using mixed samples like this for training is called (in HEP at least) Classification Without Labels (CWoLa). And you can show that if you select them without biasing the backgrounds the resulting classifier will asymptotically converge to the fully supervised case', ""So by starting with autoencoders and using this Tag N' Train method we are able train significantly better classifiers than what we started with! https://t.co/qQErAszo4H"", ""So then using data you haven't used in training, you can select events using these classifiers and look for a resonance.\nThe classifiers don't distort the QCD dijet mass distribution, allowing you to do a bump hunt in a straightforward way https://t.co/QS36ldt8MC"", 'Overall I had a lot of fun working on this project! I think that there is a lot of room to explore using other techniques or looking at other final states for these model-agnostic searches. Its also an area that feels like there is a lot room for creativity', ""The next step is to try out these search strategies for real within CMS! Also, the Tag N' Train technique I think can have interesting uses beyond just these model-agnostic searches, so maybe I will try them out when I have some time"", ""Also  I can't resist adding this cartoon I made. TNT + CWoLa!! (CWoLa is pronounced 'koala') https://t.co/6C1JzbNqM7""]",https://arxiv.org/abs/2002.12376,"There has been substantial progress in applying machine learning techniques to classification problems in collider and jet physics. But as these techniques grow in sophistication, they are becoming more sensitive to subtle features of jets that may not be well modeled in simulation. Therefore, relying on simulations for training will lead to sub-optimal performance in data, but the lack of true class labels makes it difficult to train on real data. To address this challenge we introduce a new approach, called Tag N' Train (TNT), that can be applied to unlabeled data that has two distinct sub-objects. The technique uses a weak classifier for one of the objects to tag signal-rich and background-rich samples. These samples are then used to train a stronger classifier for the other object. We demonstrate the power of this method by applying it to a dijet resonance search. By starting with autoencoders trained directly on data as the weak classifiers, we use TNT to train substantially improved classifiers. We show that Tag N' Train can be a powerful tool in model-agnostic searches and discuss other potential applications. ","Tag N' Train: A Technique to Train Improved Classifiers on Unlabeled
  Data"
11,1234541606369296384,1210312444221935616,Cyrus Rashtchian,"['Very excited about our new paper on explainable clustering with Sanjoy Dasgupta, Nave Frost, and Michal Moshkovitz <LINK> #XAI #MachineLearning', 'We consider k-means/medians clustering using decision trees with k leaves, so that cluster assignments can be explained with a handful of single feature thresholds', ""Some surprises (1) existing DT algs don't work well (2) we get an O(k) approx for k-medians and O(k^2) for k-means (3) for two centers, we get a constant factor approx with a single threshold cut! and (4) proof techniques are very different than usual clustering arguments"", 'The set-up in pictures: we determine clusters using axis-aligned cuts, which gives globally consistent explanations for every point in the dataset https://t.co/QP2BHRNpKL', 'We are currently working on a series of blog posts and an implementation + empirical evaluation. Stay tuned!']",https://arxiv.org/abs/2002.12538,"Clustering is a popular form of unsupervised learning for geometric data. Unfortunately, many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the $k$-means and $k$-medians objectives: Must there exist a tree-induced clustering whose cost is comparable to that of the best unconstrained clustering, and if so, how can it be found? In terms of negative results, we show, first, that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and second, that any tree-induced clustering must in general incur an $\Omega(\log k)$ approximation factor compared to the optimal clustering. On the positive side, we design an efficient algorithm that produces explainable clusters using a tree with $k$ leaves. For two means/medians, we show that a single threshold cut suffices to achieve a constant factor approximation, and we give nearly-matching lower bounds. For general $k \geq 2$, our algorithm is an $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$ approximation to the optimal $k$-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size. ",Explainable $k$-Means and $k$-Medians Clustering
12,1234522555995783168,88806960,Dr. Vivienne Baldassare,"['New paper out today on an unusually long lived transient in the tiny blue compact dwarf galaxy galaxy PHL 293B (<LINK>). This paper was led by Colin Burke, a grad student at UIUC. This mysterious source was lots fun to work on!', 'PHL 293B weighs a few tens of millions of solar masses and is extremely metal poor (~1/10th of solar metallicity). The image below is from the Legacy Survey (https://t.co/sVOtCBrAqw). https://t.co/vEJ1ZoHJH5', 'PHL 293B was found to have broad emission lines with blue-shifted absorption (800 km/s) in its SDSS spectroscopy in 2000. The broad emission was persistent for at least 10 years, and the source of the broad lines has been the topic of a lot of back and forth in the literature.', 'Broad lines, blue shifted absorption, but no strong optical variability or outbursts. Active galactic nucleus? Luminous blue variable? Stellar wind? Nothing seemed to quite fit the observational properties of PHL 293B.', 'In my 2018 paper searching for active galactic nuclei in Stripe 82 through optical photometric variability, I found this object to have very low-level variability. PHL 293B also turned up as variable in a similar search being done by Colin using the Dark Energy Survey.', 'We joined forces, and in our combined 20 year light curve, we see a very slow fading. This was a newly discovered feature of PHL 293B! https://t.co/lt5DrbrV8T', 'We decided to investigate further, and obtained new optical spectroscopy with Gemini in December 2019. This revealed that the broad H-alpha emission had finally faded! https://t.co/fJ4IxiQoa1', 'Between the (1) broad H-alpha emission lasting over a decade (2) recent disappearance of the broad lines (3) blue shifted absorption lines and (4) low-level photometric variability indicating a slow fade, we think that PHL 293B hosted an extremely long lived Type IIn supernova.', ""Broad lines from supernovae don't usually stick around for a decade+, but there are some indications that these extremely long lived transients are more common in these metal poor, gas-rich environments."", 'This really demonstrates the power of decades long light curves and multi-epoch spectroscopy! You can read lots more about the various scenarios we explored for PHL 293B here:  https://t.co/fSVnxcEdGp', ""Special thank you to @GeminiObs for granting us Director's Discretionary time! This work also uses repeat imaging data from @sdssurveys and @theDESurvey.""]",https://arxiv.org/abs/2002.12369,"We report on small-amplitude optical variability and recent dissipation of the unusually persistent broad emission lines in the blue compact dwarf galaxy PHL 293B. The galaxy's unusual spectral features (P Cygni-like profiles with $\sim$800 km s$^{-1}$ blueshifted absorption lines) have resulted in conflicting interpretations of the nature of this source in the literature. However, analysis of new Gemini spectroscopy reveals the broad emission has begun to fade after being persistent for over a decade prior. Precise difference imaging light curves constructed with the Sloan Digital Sky Survey and the Dark Energy Survey reveal small-amplitude optical variability of $\sim$0.1 mag in the g band offset by $100\pm21$ pc from the brightest pixel of the host. The light curve is well-described by an active galactic nuclei (AGN)-like damped random walk process. However, we conclude that the origin of the optical variability and spectral features of PHL 293B is due to a long-lived stellar transient, likely a Type IIn supernova or non-terminal outburst, mimicking long-term AGN-like variability. This work highlights the challenges of discriminating between scenarios in such extreme environments, relevant to searches for AGNs in dwarf galaxies. This is the second long-lived transient discovered in a blue compact dwarf, after SDSS1133. Our result implies such long-lived stellar transients may be more common in metal-deficient galaxies. Systematic searches for low-level variability in dwarf galaxies will be possible with the upcoming Legacy Survey of Space and Time at Vera C. Rubin Observatory. ","The Curious Case of PHL 293B: A Long-Lived Transient in a Metal-Poor
  Blue Compact Dwarf Galaxy"
13,1234487925464760322,936008568263860224,Claudia Ratti,['Check out our new paper on the “Influence of hadronic resonances on the chemical freeze-out in heavy-ion collisions”! <LINK>'],https://arxiv.org/abs/2002.12395,"Detailed knowledge of the hadronic spectrum is still an open question, which has phenomenological consequences on the study of heavy-ion collisions. A previous lattice QCD study concluded that additional strange resonances are missing in the currently tabulated lists provided by the Particle Data Group (PDG). That study identified the list labeled PDG2016+ as the ideal spectrum to be used as an input in thermal-model-based analyses. In this work, we determine the effect of additional resonances on the freeze-out parameters of systems created in heavy-ion collisions. These parameters are obtained from thermal fits of particle yields and net-particle fluctuations. For a complete picture, we compare several hadron lists including both experimentally discovered and theoretically predicted states. We find that the inclusion of additional resonances mildly influences the extracted parameters -- with a general trend of progressively lowering the temperature -- but is not sufficient to close the gap in temperature between light and strange hadrons previously observed in the literature. ","Influence of hadronic resonances on the chemical freeze-out in heavy-ion
  collisions"
14,1234392325952036866,892059194240532480,Mikel Artetxe,"['Check out our new paper ""Do all Roads Lead to Rome? Understanding the Role of Initialization in Iterative Back-Translation""!\n\nWe show that iterative back-translation tends to converge to similar solutions even if the initial systems are very different.\n\n<LINK>', 'w/ @glabaka, @noecasas &amp; @eagirre']",http://arxiv.org/abs/2002.12867,"Back-translation provides a simple yet effective approach to exploit monolingual corpora in Neural Machine Translation (NMT). Its iterative variant, where two opposite NMT models are jointly trained by alternately using a synthetic parallel corpus generated by the reverse model, plays a central role in unsupervised machine translation. In order to start producing sound translations and provide a meaningful training signal to each other, existing approaches rely on either a separate machine translation system to warm up the iterative procedure, or some form of pre-training to initialize the weights of the model. In this paper, we analyze the role that such initialization plays in iterative back-translation. Is the behavior of the final system heavily dependent on it? Or does iterative back-translation converge to a similar solution given any reasonable initialization? Through a series of empirical experiments over a diverse set of warmup systems, we show that, although the quality of the initial system does affect final performance, its effect is relatively small, as iterative back-translation has a strong tendency to convergence to a similar solution. As such, the margin of improvement left for the initialization method is narrow, suggesting that future research should focus more on improving the iterative mechanism itself. ","Do all Roads Lead to Rome? Understanding the Role of Initialization in
  Iterative Back-Translation"
15,1234297233157369857,3018222517,Samuel Stanton,"['How do you use the same neural net architecture to learn equivariant models for molecules, physical systems, and images? Find out in our new paper!\n<LINK> <LINK>']",https://arxiv.org/abs/2002.12880,"The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum. ","Generalizing Convolutional Neural Networks for Equivariance to Lie
  Groups on Arbitrary Continuous Data"
16,1234294393718427648,2800204849,Andrew Gordon Wilson,"['Translation equivariance on images gives CNNs key generalization abilities. Our new paper ""Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data"": <LINK>. With @m_finzi, @sam_d_stanton, @Pavel_Izmailov. 1/6 <LINK>', 'Our convolutional layer, LieConv, works by (1) lifting raw inputs into group elements; (2) NN convolutional kernel; (3) enforcing kernel locality by defining an invariant distance; (4) defining a distribution equivariant estimator for the group convolution integral. 2/6 https://t.co/UDnfzL8mCh', 'LieConv permits a variety of equivariances, such as rotation, translation, and scaling, on a variety of data types (images, molecules, ball and stick…), by operating on general point cloud data. The same points in the red regions enter computations. 3/6 https://t.co/X96nfla8yw', 'We apply LieConv to images, molecules, and dynamical systems. Molecules are hard because they have no canonical origin or orientation, and the target distribution is invariant to translation, rotation, and reflection. 4/6', 'LieConv is natural for Hamiltonian dynamics; its equivariance to a variety of continuous Lie Group symmetries enables exact conservation of linear and angular momentum. 5/6 https://t.co/iBpDnOIxCD', ""The goal is for models to 'see' molecules, dynamical systems, multi-scale objects, heterogeneous measurements, and higher mathematical objects, how CNNs perceive images. Code available: \nhttps://t.co/Nd8h1CYXT3\n6/6""]",https://arxiv.org/abs/2002.12880,"The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum. ","Generalizing Convolutional Neural Networks for Equivariance to Lie
  Groups on Arbitrary Continuous Data"
17,1234293645274075136,2781150596,Innes Bigaran,"['New paper today on the arXiv today: ""Getting chirality right"": <LINK>\nExploring the chiral scalar LQ solutions to g-2 (e/mu) in a top-philic framework.  @RVolkas #AcademicChatter #phdchat 😎', '@will_pietrak @RVolkas Not entirely sure yet, waiting excitedly for \n@Fermilab :)']",https://arxiv.org/abs/2002.12544,"We identify the two scalar leptoquarks capable of generating sign-dependent contributions to leptonic magnetic moments, $R_2\sim (\mathbf{3}, \mathbf{2}, 7/6)$ and $S_1\sim (\mathbf{3}, \mathbf{1}, -1/3)$, as favoured by current measurements. We consider the case in which the electron and muon sectors are decoupled, and real-valued Yukawa couplings are specified using an up-type quark mass-diagonal basis. Contributions to $\Delta a_e$ arise from charm-containing loops and $\Delta a_\mu$ from top-containing loops -- hence avoiding dangerous LFV constraints, particularly from $\mu \to e \gamma$. The strongest constraints on these models arise from contributions to the Z leptonic decay widths, high-$p_T$ leptonic tails at the LHC, and from (semi)leptonic kaon decays. To be a comprehensive solution to the $(g-2)_{e/\mu}$ puzzle we find that the mass of either leptoquark must be $\lesssim 65$ TeV. This analysis can be embedded within broader flavour anomaly studies, including those of hierarchical leptoquark coupling structures. It can also be straightforwardly adapted to accommodate future measurements of leptonic magnetic moments, such as those expected from the Muon $g-2$ collaboration in the near future. ","Getting chirality right: single scalar leptoquark solutions to the
  $(g-2)_{e,\mu}$ puzzle"
18,1233587940212822016,106962512,Juan Felipe Carrasquilla Álvarez,"['Here is a new paper with Ehsan Khatami from SJSU, the Laboratory for Ultracold Quantum Gases at Princeton, and the incomparable R. Scalettar at UC  Davis: Visualizing Correlations in the 2D Fermi-Hubbard Model with AI <LINK>']",https://arxiv.org/abs/2002.12310,"Strongly correlated phases of matter are often described in terms of straightforward electronic patterns. This has so far been the basis for studying the Fermi-Hubbard model realized with ultracold atoms. Here, we show that artificial intelligence (AI) can provide an unbiased alternative to this paradigm for phases with subtle, or even unknown, patterns. Long- and short-range spin correlations spontaneously emerge in filters of a convolutional neural network trained on snapshots of single atomic species. In the less well-understood strange metallic phase of the model, we find that a more complex network trained on snapshots of local moments produces an effective order parameter for the non-Fermi-liquid behavior. Our technique can be employed to characterize correlations unique to other phases with no obvious order parameters or signatures in projective measurements, and has implications for science discovery through AI beyond strongly correlated systems. ","Visualizing Strange Metallic Correlations in the 2D Fermi-Hubbard Model
  with AI"
19,1233417280974983169,131132279,Jean-Luc Thiffeault,['New paper with my PhD student Bryan Oakley and with Charlie Doering where we examine the relationship between rates of decay of mix-norms and correlations.  We give lots of examples to help understand the proofs for the different cases.  <LINK>'],https://arxiv.org/abs/2002.09953,"Two quantitative notions of mixing are the decay of correlations and the decay of a mix-norm -- a negative Sobolev norm -- and the intensity of mixing can be measured by the rates of decay of these quantities. From duality, correlations are uniformly dominated by a mix-norm; but can they decay asymptotically faster than the mix-norm? We answer this question by constructing an observable with correlation that comes arbitrarily close to achieving the decay rate of the mix-norm. Therefore the mix-norm is the sharpest rate of decay of correlations in both the uniform sense and the asymptotic sense. Moreover, there exists an observable with correlation that decays at the same rate as the mix-norm if and only if the rate of decay of the mix-norm is achieved by its projection onto low-frequency Fourier modes. In this case, the function being mixed is called q-recurrent; otherwise it is q-transient. We use this classification to study several examples and raise questions for future investigations. ",On mix-norms and the rate of decay of correlations
20,1233394656584454144,162293874,Jeff Clune,"['Here is a new talk entirely on ANML (a Neuromodulated Meta-Learning Algorithm) @reworkAI  ANML meta-learns to reduce catastrophic forgetting, and can learn at least 600 tasks sequentially!\n<LINK> paper: Learning to Continually Learn (<LINK>) #AI <LINK>', '@reworkAI Work done with a great team: Shawn Beaulieu (first author), Lapo Frati, @ThomasMiconi, Joel Lehman (\n@joelbot3000), @kenneth0stanley, and Nick Cheney.', '@QuantRob @reworkAI You can do ANML with any optimization algorithm if you like, including a genetic algorithm. But a GA is likely less efficient for supervised learning tasks.']",https://arxiv.org/abs/2002.09571,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates). ",Learning to Continually Learn
21,1233326298749112320,1136271364766294019,Malte Esders,['We have a new paper on arxiv! 🥳\n\nReinforcement Learning for autonomous nanofabrication. Hopefully paving the way towards assembling more complex molecular structures.\n\n<LINK>'],https://arxiv.org/abs/2002.11952,"The ability to handle single molecules as effectively as macroscopic building-blocks would enable the construction of complex supramolecular structures inaccessible to self-assembly. The fundamental challenges obstructing this goal are the uncontrolled variability and poor observability of atomic-scale conformations. Here, we present a strategy to work around both obstacles, and demonstrate autonomous robotic nanofabrication by manipulating single molecules. Our approach employs reinforcement learning (RL), which finds solution strategies even in the face of large uncertainty and sparse feedback. We demonstrate the potential of our RL approach by removing molecules autonomously with a scanning probe microscope from a supramolecular structure -- an exemplary task of subtractive manufacturing at the nanoscale. Our RL agent reaches an excellent performance, enabling us to automate a task which previously had to be performed by a human. We anticipate that our work opens the way towards autonomous agents for the robotic construction of functional supramolecular structures with speed, precision and perseverance beyond our current capabilities. ",Autonomous robotic nanofabrication with reinforcement learning
22,1233324742284922880,348355646,Victor See,"[""It’s new paper day! We try to determine how good torque estimates based on Zeeman-Doppler maps are. This one's been in the works for a while and I'm excited to release it into the wild! @AdamF_Astro, @seanpmatt (and Lisa Lehmann who's not on twitter) <LINK>"", 'Our work builds on the work of Lehmann+18: https://t.co/QacFmvGQUZ This was a great bit of work that demonstrated that ZDI does a reasonable job of recovering the large-scale field geometry but misses magnetic flux even at the largest scales, e.g. the dipole, quadrupole, etc.', 'This missing flux will impact any spin-down torque estimates you make using ZDI maps! We find that the torque can be underestimated by a factor of up to ~10 for stars with weak magnetic fields but that torque estimates for stars with stronger fields are relatively good! https://t.co/t9vjOfnXKD']",https://arxiv.org/abs/2002.11774,"Numerous attempts to estimate the rate at which low-mass stars lose angular momentum over their lifetimes exist in the literature. One approach is to use magnetic maps derived from Zeeman-Doppler imaging (ZDI) in conjunction with so-called ""braking laws"". The use of ZDI maps has advantages over other methods because it allows information about the magnetic field geometry to be incorporated into the estimate. However, ZDI is known to underestimate photospheric field strengths due to flux cancellation effects. Recently, Lehmann et al. (2018) conducted synthetic ZDI reconstructions on a set of flux transport simulations to help quantify the amount by which ZDI underestimates the field strengths of relatively slowly rotating and weak activity solar-like stars. In this paper, we evaluate how underestimated angular momentum-loss rate estimates based on ZDI maps may be. We find that they are relatively accurate for stars with strong magnetic fields but may be underestimated by a factor of up to $\sim$10 for stars with weak magnetic fields. Additionally, we re-evaluate our previous work that used ZDI maps to study the relative contributions of different magnetic field modes to angular momentum-loss. We previously found that the dipole component dominates spin-down for most low-mass stars. This conclusion still holds true even in light of the work of Lehmann et al. (2018). ","How much do underestimated field strengths from Zeeman-Doppler imaging
  affect spin-down torque estimates?"
23,1233295124777590784,10046412,fwilhelm,['New paper: simplifying the implementation of qaoa. With @KikeSolanoPhys @qmisanz @DLR_de @Daimler <LINK>'],https://arxiv.org/abs/2002.12215,"The Quantum Approximate Optimisation Algorithm was proposed as a heuristic method for solving combinatorial optimisation problems on near-term quantum computers and may be among the first algorithms to perform useful computations in the post-supremacy, noisy, intermediate scale era of quantum computing. In this work, we exploit the recently proposed digital-analog quantum computation paradigm, in which the versatility of programmable universal quantum computers and the error resilience of quantum simulators are combined to improve platforms for quantum computation. We show that the digital-analog paradigm is suited to the variational quantum approximate optimisation algorithm, due to its inherent resilience against coherent errors, by performing large-scale simulations and providing analytical bounds for its performance in devices with finite single-qubit operation times. We observe regimes of single-qubit operation speed in which the considered variational algorithm provides a significant improvement over non-variational counterparts. ",Approximating the Quantum Approximate Optimisation Algorithm
24,1233038166984028160,2235411914,Surya Ganguli,"['1/ Our new paper: A universal energy accuracy tradeoff in non-equilibrium cellular sensing, lead by @SarahLizHarvey and Subhaneil Lahiri (@Deephype), <LINK> has new results on a classic question of Berg/Purcell from 1977: how accurately can cells sense stimuli? <LINK>', '2/ We combine Fisher information theory, stochastic thermodynamics, and large deviation theory to show that: no matter how complex the cell surface receptor is, and no matter how much energy the receptor spends... https://t.co/29inFhzrhT', '3/ cells cannot sense stimuli more accurately than a simple two-state receptor, if estimation is done by a downstream ideal observer.   However, if estimation is done by a biologically plausible simple observer that measures the fraction of time bound.... https://t.co/zm0MtIA4iX', '4/ then we find a universal energy accuracy tradeoff for a large class of receptors, no matter how complex they are.  The more energy the receptor spends, the more accurate cellular sensing by a simple observer becomes.  At high (but not low) energy, ring receptors are optimal https://t.co/Nux5p8SnzL', '5/  We also derive a new thermodynamic uncertainty relation for the time any physical process spends in a subset of states. This relation tells us that any attempt to reduce fluctuations in this time requires an energetic cost.  Thus spending energy can improve timing reliability https://t.co/rKo2siGaRD', '6/ Congrats again to @SarahLizHarvey and Subhaneil Lahiri (@Deephype) on some tour de force work combining information theory, non-equilibrium statistical mechanics and biophysics to provide new results on a 40+ year old problem!']",https://arxiv.org/abs/2002.10567,"We combine stochastic thermodynamics, large deviation theory, and information theory to derive fundamental limits on the accuracy with which single cell receptors can estimate external concentrations. As expected, if estimation is performed by an ideal observer of the entire trajectory of receptor states, then no energy consuming non-equilibrium receptor that can be divided into bound and unbound states can outperform an equilibrium two-state receptor. However, when estimation is performed by a simple observer that measures the fraction of time the receptor is bound, we derive a fundamental limit on the accuracy of general nonequilibrium receptors as a function of energy consumption. We further derive and exploit explicit formulas to numerically estimate a Pareto-optimal tradeoff between accuracy and energy. We find this tradeoff can be achieved by nonuniform ring receptors with a number of states that necessarily increases with energy. Our results yield a novel thermodynamic uncertainty relation for the time a physical system spends in a pool of states, and generalize the classic 1977 Berg-Purcell limit on cellular sensing along multiple dimensions. ",Universal energy-accuracy tradeoffs in nonequilibrium cellular sensing
25,1232849367549214721,1556664198,Kyle Cranmer,"['New paper!\nCompact Representation of Uncertainty in Hierarchical Clustering led by Sebastian Macaluso &amp; Craig Greenberg with @andrewmccallum and collaborators.\n\nWe present dynamic-programming algorithms for exact inference in hierarchical clustering.\n<LINK> <LINK>', 'The paper is an excellent example of use-inspired research. Some problems in jet physics at the #LHC can be formulated in terms of hierarchical clustering. Some modification and generalization of previous work on flat clustering led us to these new algorithms.', 'Existing algorithms in jet physics are greedy approximations of the most likely clustering. NLP inspired Beam search approaches, which are better but still approximate. This algorithm allows for exact inference over huge numbers of trees!', 'For N leaves there are (2N-3)!! Unordered binary hierarchies. That grows insanely fast!! \n\n# leaves#trees\n3   3\n4   15\n5   105\n6   945\n7   10395\n8   135135\n9   2027025\n10  34459425\n11   654729075\n12   13749310575\n13   316234143225\n\nhttps://t.co/Wb5iMgjAhT https://t.co/Q5VcktwVIQ', 'We actually used the algorithm to count the number of binary trees and found the equation using the online encyclopedia of integer sequences \nhttps://t.co/eNECobwQFx', 'Scalability is an issue, and we have ideas for sparse approximate versions. But there are problems where the exact algorithm is feasible and interesting. We also consider a genomics example']",https://arxiv.org/abs/2002.11661,"Hierarchical clustering is a fundamental task often used to discover meaningful structures in data, such as phylogenetic trees, taxonomies of concepts, subtypes of cancer, and cascades of particle decays in particle physics. Typically approximate algorithms are used for inference due to the combinatorial number of possible hierarchical clusterings. In contrast to existing methods, we present novel dynamic-programming algorithms for \emph{exact} inference in hierarchical clustering based on a novel trellis data structure, and we prove that we can exactly compute the partition function, maximum likelihood hierarchy, and marginal probabilities of sub-hierarchies and clusters. Our algorithms scale in time and space proportional to the powerset of $N$ elements which is super-exponentially more efficient than explicitly considering each of the (2N-3)!! possible hierarchies. Also, for larger datasets where our exact algorithms become infeasible, we introduce an approximate algorithm based on a sparse trellis that compares well to other benchmarks. Exact methods are relevant to data analyses in particle physics and for finding correlations among gene expression in cancer genomics, and we give examples in both areas, where our algorithms outperform greedy and beam search baselines. In addition, we consider Dasgupta's cost with synthetic data. ","Data Structures & Algorithms for Exact Inference in Hierarchical
  Clustering"
26,1232605935526457344,1018371301,Jonathan Conrad,['New paper online! Thanks to Barbara Terhal and @CVuillot.\n\n<LINK>'],https://arxiv.org/abs/2002.11008,"We review some of the recent efforts in devising and engineering bosonic qubits for superconducting devices, with emphasis on the Gottesman-Kitaev-Preskill (GKP) qubit. We present some new results on decoding repeated GKP error correction using finitely-squeezed GKP ancilla qubits, exhibiting differences with previously studied stochastic error models. We discuss circuit-QED ways to realize CZ gates between GKP qubits and we discuss different scenario's for using GKP and regular qubits as building blocks in a scalable superconducting surface code architecture. ",Towards Scalable Bosonic Quantum Error Correction
27,1232601013892403200,142201024,Sainbayar,"[""Transformers are great, but they're inherently limited when processing sequences. Our new paper proposes a simple change to Transformers that remove this limitation <LINK>""]",https://arxiv.org/abs/2002.09402,"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers. ",Addressing Some Limitations of Transformers with Feedback Memory
28,1232577871098601473,1000951360404312065,Alexei Moiseev,"['Our new study with 3D spectroscopic mapping of the NaD nuclear outflow in AGN Markarian 938. It\'s very interesting to work together with a colleague who can say : ""this galaxy was classified as Sy2 in my paper with Markarian..."" #BTA6m #voorwerpjes\n<LINK> <LINK>']",https://arxiv.org/abs/2002.10812,"We present the results of a detailed study of the central part of the Seyfert galaxy Mkn 938. Observational data were obtained with the 6-m telescope of the Special Astrophysical Observatory of the Russian Academy of Sciences using integral-field spectrograph MPFS and a scanning Fabry--Perot interferometer. Mkn 938 is interesting for being a result of a merger of two gas-rich galaxies, and we observe the final stage of this interaction accompanied with an extremely powerful burst of star formation and nuclear activity. Our analysis of the kinematics of gas and stars revealed the presence of gas outflow in the circumnuclear region Mkn 938 with velocities ranging from -370 to -480 km/s, and allowed us for the first time to map the high-velocity galactic wind in NaD absorption line on large spatial scale in this galaxy. ",Internal Kinematics of the Seyfert Galaxy Mkn 938
29,1232484147051913216,2324423269,Peyman 𝕄𝕀𝕃𝔸ℕ𝔽𝔸ℝ,['Our new paper on an interactive real-time framework for stylization: It uses our learned BLADE filters – an interpretable shallow machine learning method that simulates complex filter blocks in real time. \nBlade: <LINK>\nImage Stylization: <LINK> <LINK>'],https://arxiv.org/abs/2002.10945,"We present a framework for interactive design of new image stylizations using a wide range of predefined filter blocks. Both novel and off-the-shelf image filtering and rendering techniques are extended and combined to allow the user to unleash their creativity to intuitively invent, modify, and tune new styles from a given set of filters. In parallel to this manual design, we propose a novel procedural approach that automatically assembles sequences of filters, leading to unique and novel styles. An important aim of our framework is to allow for interactive exploration and design, as well as to enable videos and camera streams to be stylized on the fly. In order to achieve this real-time performance, we use the \textit{Best Linear Adaptive Enhancement} (BLADE) framework -- an interpretable shallow machine learning method that simulates complex filter blocks in real time. Our representative results include over a dozen styles designed using our interactive tool, a set of styles created procedurally, and new filters trained with our BLADE approach. ",Image Stylization: From Predefined to Personalized
30,1232482909044400129,755924666,Brant Robertson,"['Congrats to @navelation (w/ Eve Ostriker, @chargedcurrent , and yours truly) for submitting her important new paper on the physics of galactic outflows:\n<LINK>', '@navelation @chargedcurrent Oh you’re too kind!']",https://arxiv.org/abs/2002.10468,"We present the fourth of the Cholla Galactic OutfLow Simulations suite (CGOLS). Using a physically-motivated prescription for clustered supernova feedback, we successfully drive a multiphase outflow from a disk galaxy. The high resolution ($< 5\,\mathrm{pc}$) across a relatively large domain ($20\,\mathrm{kpc}$) allows us to capture the hydrodynamic mixing and dynamical interactions between the hot and cool ($T \sim 10^4\,\mathrm{K}$) phases in the outflow, which in turn leads to direct evidence of a qualitatively new mechanism for cool gas acceleration in galactic winds. We show that mixing of momentum from the hot phase to the cool phase accelerates the cool gas to $800\,\mathrm{km}\,\mathrm{s}^{-1}$ on kpc scales, with properties inconsistent with the physical models of ram pressure acceleration or with bulk cooling from the hot phase. The mixing process also affects the hot phase, modifying its radial profiles of temperature, density, and velocity from the expectations of radial supersonic flow. This mechanism provides a physical explanation for the high velocity, blue shifted, low ionization absorption lines often observed in the spectra of starburst and high redshift galaxies. ",The Physical Nature of Starburst-Driven Galactic Outflows
31,1232479515533725697,112529649,Bart Wronski 🇺🇦,"['A fun new paper that I helped with! <LINK> ""Image Stylization: From Predefined to Personalized"" we optimize some common (classically slow) image processing operations through a set of learned, discrete filters and allow for real-time image stylization creation. <LINK>', ""This is an extension and application of BLADE filter framework https://t.co/uNPnenSZjX that we combine and use in context of image stylization.\nIt's earlier work of my colleagues, my contribution was mostly just to optimize the hell out of it for fast GPU application. :)""]",https://arxiv.org/abs/2002.10945,"We present a framework for interactive design of new image stylizations using a wide range of predefined filter blocks. Both novel and off-the-shelf image filtering and rendering techniques are extended and combined to allow the user to unleash their creativity to intuitively invent, modify, and tune new styles from a given set of filters. In parallel to this manual design, we propose a novel procedural approach that automatically assembles sequences of filters, leading to unique and novel styles. An important aim of our framework is to allow for interactive exploration and design, as well as to enable videos and camera streams to be stylized on the fly. In order to achieve this real-time performance, we use the \textit{Best Linear Adaptive Enhancement} (BLADE) framework -- an interpretable shallow machine learning method that simulates complex filter blocks in real time. Our representative results include over a dozen styles designed using our interactive tool, a set of styles created procedurally, and new filters trained with our BLADE approach. ",Image Stylization: From Predefined to Personalized
32,1232476273911320576,774299949580357632,Milan Curcic,"['New paper from the SUSTAIN lab: Revised estimates of ocean surface drag in strong winds.\n\nRead the pre-print here: <LINK>\n\nIn review for Geophysical Research Letters.', 'We find further evidence of drag saturation in hurricane winds, which was independently discovered in the field and laboratory in 2003.', ""Drag saturation means that the air-sea drag coefficient ceases to increase with wind speed beyond some value. \n\nThis happens in strong winds, about tropical storm force.\n\nIt's one of the key unknowns for accurate hurricane prediction."", 'Data from the leading laboratory study that discovered drag saturation (Donelan et al., 2004) was used soon after to implement the surface drag parameterization for tropical cyclones in the most widely used numerical weather prediction model (WRF). @wrfmodel', 'The original study is highly cited and referred to by influential review papers.\n\nThe WRF parameterization in question has been used by many tropical cyclone prediction studies.', 'During our analysis, we discovered an error in the original Matlab source code used by Donelan et al. (2004).\n\nThe error appears in the scaling of in situ wind to the reference height of 10 m.\n\nWith the original stress data, which we have, the correction is straightforward.', 'With the correction to the data, drag coefficients from the laboratory by Donelan et al. (2004) are now in agreement with the field data in low-to-moderate winds.', 'The bottom line is, the original data underestimated the level of drag saturation by 34%, and overestimated the wind speed threshold\nat which the saturation occurs by 12%.\n\nIf you use any isftcflx /= 0 setting, the air-sea drag in your WRF model is significantly underestimated.', 'Data and notebooks here: https://t.co/Kqo7aNlEt7\n\nThanks a lot @Lacksagoo for review on early draft. Made the paper much better! 🙏']",https://arxiv.org/abs/2002.10590,"Air-sea drag governs the momentum transfer between the atmosphere and the ocean, and remains largely unknown in hurricane winds. We revisit the momentum budget and eddy-covariance methods to estimate the surface drag coefficient in the laboratory. Our drag estimates agree with field measurements in low-to-moderate winds, and previous laboratory measurements in hurricane-force winds. The drag coefficient saturates at $2.6 \times 10^{-3}$ and $U_{10} \approx 25\ m\ s^{-1}$, in agreement with previous laboratory results by Takagaki et al. (2012). During our analysis, we discovered an error in the original source code used by Donelan et al. (2004). We present the corrected data and describe the correction procedure. Although the correction to the data does not change the key finding of drag saturation in strong winds, its magnitude and wind speed threshold are significantly changed. Our findings emphasize the need for an updated and unified drag parameterization based on field and laboratory data. ",Revised Estimates of Ocean Surface Drag in Strong Winds
33,1232403203154751488,1141380787163582464,Niko Hauzenberger,"['New working paper: Combining shrinkage and sparsity in conjugate VAR models, co-authored with @FlorianHuber8 and Luca Onorante! <LINK>']",https://arxiv.org/abs/2002.08760,"Conjugate priors allow for fast inference in large dimensional vector autoregressive (VAR) models but, at the same time, introduce the restriction that each equation features the same set of explanatory variables. This paper proposes a straightforward means of post-processing posterior estimates of a conjugate Bayesian VAR to effectively perform equation-specific covariate selection. Compared to existing techniques using shrinkage alone, our approach combines shrinkage and sparsity in both the VAR coefficients and the error variance-covariance matrices, greatly reducing estimation uncertainty in large dimensions while maintaining computational tractability. We illustrate our approach by means of two applications. The first application uses synthetic data to investigate the properties of the model across different data-generating processes, the second application analyzes the predictive gains from sparsification in a forecasting exercise for US data. ","Combining Shrinkage and Sparsity in Conjugate Vector Autoregressive
  Models"
34,1232378173993578496,40285266,Stanislav Fort at EAGx Prague ¬(🔥📎🔥📎),"['Exciting times! Our new paper /The Break-Even Point on Optimization Trajectories of Deep Neural Networks/ (<LINK>) got accepted as a *spotlight* at @iclr_conf. A break-even point early in training determines properties of the entire optimization trajectory. <LINK> <LINK>', '@iclr_conf Many thanks to the amazing Maciek, Devansh, Jacek, @kchonyc, @kjgeras, and especially @kudkudakpl for leading the project!']",http://arxiv.org/abs/2002.09572,"The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the ""break-even"" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction. ","The Break-Even Point on Optimization Trajectories of Deep Neural
  Networks"
35,1232356540616568832,962886465502851072,Maral Khosroshahi,"['Read our paper “Training Large Neural Networks with Constant Memory using a New Execution Algorithm”.\nWe propose an L2L algorithm that allows you to run transformer-based models on a single GPU with a large batch size requiring less memory.\n#MicrosoftLife \n<LINK> <LINK>', 'We have used #BERT as an example and show that our algorithm is able to fit a 96 layer BERT on a single V100 with batch size of 32, requiring only 11GB memory while the original model with only 24 layers requires at least 10GB for a maximum batch size of 2.', 'The constant-memory nature of this approach allows to scale to arbitrary depth in the number of layers. We enable developers to run large models on affordable hardware. Also, each layer can be structurally agnostic to another, encouraging dynamic modeling approaches such as #NAS.', 'We used the popular @huggingface library as a baseline for development and testing. The L2L version of the BERT-large model and the EPS will soon be available in open source.']",https://arxiv.org/abs/2002.05645,"Widely popular transformer-based NLP models such as BERT and Turing-NLG have enormous capacity trending to billions of parameters. Current execution methods demand brute-force resources such as HBM devices and high speed interconnectivity for data parallelism. In this paper, we introduce a new relay-style execution technique called L2L (layer-to-layer) where at any given moment, the device memory is primarily populated only with the executing layer(s)'s footprint. The model resides in the DRAM memory attached to either a CPU or an FPGA as an entity we call eager param-server (EPS). To overcome the bandwidth issues of shuttling parameters to and from EPS, the model is executed a layer at a time across many micro-batches instead of the conventional method of minibatches over whole model. L2L is implemented using 16GB V100 devices for BERT-Large running it with a device batch size of up to 256. Our results show 45% reduction in memory and 40% increase in the throughput compared to the state-of-the-art baseline. L2L is also able to fit models up to 50 Billion parameters on a machine with a single 16GB V100 and 512GB CPU memory and without requiring any model partitioning. L2L scales to arbitrary depth allowing researchers to develop on affordable devices which is a big step toward democratizing AI. By running the optimizer in the host EPS, we show a new form of mixed precision for faster throughput and convergence. In addition, the EPS enables dynamic neural architecture approaches by varying layers across iterations. Finally, we also propose and demonstrate a constant memory variation of L2L and we propose future enhancements. This work has been performed on GPUs first, but also targeted towards all high TFLOPS/Watt accelerators. ","Training Large Neural Networks with Constant Memory using a New
  Execution Algorithm"
36,1232334282800140288,2698179823,Yaron Lipman,"['New paper: \nNoticing a surprising incarnation of implicit regularization of neural networks in the geometric setting, and harnessing  its power for high fidelity shape representation.\n\n<LINK>\n\n@AmosGropp @YarivLior @HaimNiv @matanatzmon <LINK>']",https://arxiv.org/abs/2002.10099,"Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods. ",Implicit Geometric Regularization for Learning Shapes
37,1232319835528400896,732544572723695616,Kris Cao,"[""New paper: <LINK>. @DaniYogatama and I investigate multi-task learning for natural language generation. We find that modelling latent 'skills' (that is, a per-datapoint latent variable) helps prevent task interference."", '@DaniYogatama However, this effect only happens if we structure the latent space with task knowledge: attempting to induce latent skills directly does not help prevent task interference. (2/n)', '@DaniYogatama A side product of of having a latent skill space is that we can do few-shot adaptation in the latent space only, rather than adapting all model parameters. We show that this few-shot adaptation method is more robust to hyperparameter choice and performs comparably to SGD. (3/3)']",https://arxiv.org/abs/2002.09543,"We present a generative model for multitask conditional language generation. Our guiding hypothesis is that a shared set of latent skills underlies many disparate language generation tasks, and that explicitly modelling these skills in a task embedding space can help with both positive transfer across tasks and with efficient adaptation to new tasks. We instantiate this task embedding space as a latent variable in a latent variable sequence-to-sequence model. We evaluate this hypothesis by curating a series of monolingual text-to-text language generation datasets - covering a broad range of tasks and domains - and comparing the performance of models both in the multitask and few-shot regimes. We show that our latent task variable model outperforms other sequence-to-sequence baselines on average across tasks in the multitask setting. In the few-shot learning setting on an unseen test dataset (i.e., a new task), we demonstrate that model adaptation based on inference in the latent task space is more robust than standard fine-tuning based parameter adaptation and performs comparably in terms of overall performance. Finally, we examine the latent task representations learnt by our model and show that they cluster tasks in a natural way. ",Modelling Latent Skills for Multitask Language Generation
38,1232280955731181574,1212714725571612672,David Holzmüller,"['1. Neural Networks can get stuck in bad local minima, but how often does it happen? In a new paper with Ingo Steinwart (<LINK>), I prove that for certain datasets/distributions, two-layer ReLU networks with Kaiming init and GD on LS-loss get stuck frequently. <LINK>', '2. The video shows the evolution of such a NN with 16 hidden neurons during training with gradient descent on the dataset given by the black crosses. The video speeds up exponentially since training gets very slow (over 200,000 epochs total).', '3. With increasing width, the network is more and more likely to converge to the two optimal affine regression lines if these lines pass through (0, 0). Equivalently, the kinks (non-differentiable points) of the NN start at x = 0 and don‘t move far enough to reach a data point. https://t.co/fBUJAH1XDY', '4. Experiments for a simple probability distribution confirm our theoretical result that the probability of not remaining affine on the two parts of the dataset decreases (almost) like O(m^(-1/2)), where m is the number of hidden neurons. (ES stands for early stopping.) https://t.co/JKafpjD1rS', '5. This shows that a central statistical property called universal consistency cannot be satisfied for the investigated class of neural networks. We also show how to extend this result to higher input dimensions.', '6. If the distribution/dataset is shifted upwards by Δ != 0, the hypotheses are not satisfied and the probabilities eventually go back towards 1 (which does not mean that the found network must be good, it is just likely not affine anymore). The plot below shows GD without ES. https://t.co/iRWZSYNTLd', '7. We show that training dynamics follow a linear system (like Neural Tangent Kernels). This system has -O(1) and -O(m) eigenvalues. Under the above conditions, the initial vector is close to the -O(m) eigenspace and we are in the ""lazy"" regime where weights don‘t change much.', '8. Especially, the movement speed of kinks and the convergence speed of the loss depends on the speed of convergence to the affine regression optimum, which is initially fast (-O(m) eigenvalues) and then slow (-O(1) eigenvalues). https://t.co/7OOLjYpRQV', '9. Intuitively speaking, in our case, learning mostly happens in the last layer. The last layer has only one bias but m weights, so learning the intercept takes more time than learning the slope, which is why we required the optimal affine regression lines to pass through (0, 0).', '10. Our negative result is related to positive results on over-parameterized NNs by @SimonShaoleiDu, @ZhaiXiyu, @ZeyuanAllenZhu, Yuanzhi Li, @prfsanjeevarora and others as well as work on spurious local minima by Chulhee Yun, @optiML, @jababi.']",https://arxiv.org/abs/2002.04861,"We prove that two-layer (Leaky)ReLU networks initialized by e.g. the widely used method proposed by He et al. (2015) and trained using gradient descent on a least-squares loss are not universally consistent. Specifically, we describe a large class of one-dimensional data-generating distributions for which, with high probability, gradient descent only finds a bad local minimum of the optimization landscape. It turns out that in these cases, the found network essentially performs linear regression even if the target function is non-linear. We further provide numerical evidence that this happens in practical situations, for some multi-dimensional distributions and that stochastic gradient descent exhibits similar behavior. ",Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent
39,1232220230220439552,157973000,Michael Pfarrhofer,"['New working paper online on estimating high-dimensional time-varying parameter models by @FlorianHuber8, Gary Koop and me #Econometrics \n<LINK>']",https://arxiv.org/abs/2002.10274,"Researchers increasingly wish to estimate time-varying parameter (TVP) regressions which involve a large number of explanatory variables. Including prior information to mitigate over-parameterization concerns has led to many using Bayesian methods. However, Bayesian Markov Chain Monte Carlo (MCMC) methods can be very computationally demanding. In this paper, we develop computationally efficient Bayesian methods for estimating TVP models using an integrated rotated Gaussian approximation (IRGA). This exploits the fact that whereas constant coefficients on regressors are often important, most of the TVPs are often unimportant. Since Gaussian distributions are invariant to rotations we can split the the posterior into two parts: one involving the constant coefficients, the other involving the TVPs. Approximate methods are used on the latter and, conditional on these, the former are estimated with precision using MCMC methods. In empirical exercises involving artificial data and a large macroeconomic data set, we show the accuracy and computational benefits of IRGA methods. ","Bayesian Inference in High-Dimensional Time-varying Parameter Models
  using Integrated Rotated Gaussian Approximations"
40,1232131038828101634,2203468841,Dr Jade Powell,['My new supernova simulation paper is on arXiv today 😁 <LINK>'],https://arxiv.org/abs/2002.10115,"We present three-dimensional simulations of the core-collapse of massive rotating and non-rotating progenitors performed with the general relativistic neutrino hydrodynamics code CoCoNuT-FMT and analyse their explosion properties and gravitational-wave signals. The progenitor models include Wolf-Rayet stars with initial helium star masses of $39\,M_{\odot}$ and $20\,M_{\odot}$, and an $18\,M_{\odot}$ red supergiant. The $39\,M_{\odot}$ model is a rapid rotator, whereas the two other progenitors are non-rotating. Both Wolf-Rayet models produce healthy neutrino-driven explosions, whereas the red supergiant model fails to explode. By the end of the simulations, the explosion energies have already reached $1.1\times 10^{51}\,\mathrm{erg}$ and $0.6\times 10^{51}\,\mathrm{erg}$ for the $39\,M_{\odot}$ and $20\,M_{\odot}$ model, respectively. The explosions produce neutron stars of relatively high mass, but with modest kicks. Due to the alignment of the bipolar explosion geometry with the rotation axis, there is a relatively small misalignment of $30^\circ$ between the spin and the kick in the $39\,M_{\odot}$ model. In terms of gravitational-wave signals, the massive and rapidly rotating $39\,M_{\odot}$ progenitor stands out by large gravitational-wave amplitudes that would make it detectable out to almost 2 Mpc by the Einstein Telescope. For this model, we find that rotation significantly changes the dependence of the characteristic gravitational-wave frequency of the f-mode on the proto-neutron star parameters compared to the non-rotating case. The other two progenitors have considerably smaller detection distances, despite significant low-frequency emission in the most sensitive frequency band of current gravitational-wave detectors due to the standing accretion shock instability in the $18\,M_{\odot}$ model. ","Three-dimensional core-collapse supernova simulations of massive and
  rotating progenitors"
41,1231989978793742336,1012717203324657665,Marco Pegoraro is at CAiSE 2022,"['It\'s #Rosenmontag! I can\'t throw sweets from here, but maybe I can hit you with other kinds of goodies! New paper accepted at @BISconf: ""Efficient Construction of Behavior Graphs for Uncertain Event Data"". With M. S. Uysal and @wvdaalst! Preprint at <LINK> (1/2)', ""@BISconf @wvdaalst But there's more! Want a quick summary? Head to the PADS blogpost: https://t.co/6epmaZnwHD Want to give it a spin? Full code with experiments at https://t.co/uY4R2Scbv3\n#Alaaf! #ProcessMining #ProcessScience #RWTH (2/2) https://t.co/PT9IJg2Cs7""]",https://arxiv.org/abs/2002.08225,"The discipline of process mining deals with analyzing execution data of operational processes, extracting models from event data, checking the conformance between event data and normative models, and enhancing all aspects of processes. Recently, new techniques have been developed to analyze event data containing uncertainty; these techniques strongly rely on representing uncertain event data through graph-based models capturing uncertainty. In this paper we present a novel approach to efficiently compute a graph representation of the behavior contained in an uncertain process trace. We present our new algorithm, analyze its time complexity, and report experimental results showing order-of-magnitude performance improvements for behavior graph construction. ",Efficient Construction of Behavior Graphs for Uncertain Event Data
42,1231978126281969666,1556664198,Kyle Cranmer,"['New paper! Set2Graph: Learning Graphs From Sets\nled by my collaborators at @WeizmannScience @lipmanya @HaggaiMaron @SegolNimrod \nAnother case where jet physics fits nicely specific structured data representations. #ML4HEP\n<LINK> <LINK>', '@lipmanya @WeizmannScience @HaggaiMaron @SegolNimrod They need to get twitter handles :-)']",https://arxiv.org/abs/2002.08772,"Many problems in machine learning can be cast as learning functions from sets to graphs, or more generally to hypergraphs; in short, Set2Graph functions. Examples include clustering, learning vertex and edge features on graphs, and learning features on triplets in a collection. A natural approach for building Set2Graph models is to characterize all linear equivariant set-to-hypergraph layers and stack them with non-linear activations. This poses two challenges: (i) the expressive power of these networks is not well understood; and (ii) these models would suffer from high, often intractable computational and memory complexity, as their dimension grows exponentially. This paper advocates a family of neural network models for learning Set2Graph functions that is both practical and of maximal expressive power (universal), that is, can approximate arbitrary continuous Set2Graph functions over compact sets. Testing these models on different machine learning tasks, mainly an application to particle physics, we find them favorable to existing baselines. ",Set2Graph: Learning Graphs From Sets
43,1231923878299144192,846422863,Carlos Sarraute ⚡️,"['📢 New paper published!\n\n🔥 Snel: SQL Native Execution for LLVM 🔥\n<LINK>\n\nSnel is a relational database engine featuring JIT compilation of queries, designed for fast on-line analytics\n\nCongrats to @mottalli for this work!\n\n@gusajz @GrandataLabs <LINK>', '@mottalli @gusajz @GrandataLabs @GrandataCorp Special thanks to Alejo Sanchez who originated and worked on the Snel project while at @GrandataCorp', '@mottalli @gusajz @GrandataLabs @GrandataCorp Snel generated interest in Hacker News!\n\nhttps://t.co/Lz0WNqCT96 https://t.co/PO9HMKE2IC']",http://arxiv.org/abs/2002.09449,"Snel is a relational database engine featuring Just-In-Time (JIT) compilation of queries and columnar data representation. Snel is designed for fast on-line analytics by leveraging the LLVM compiler infrastructure. It also has custom special methods like resolving histograms as extensions to the SQL language. ""Snel"" means ""SQL Native Execution for LLVM"". Unlike traditional database engines, it does not provide a client-server interface. Instead, it exposes its interface as an extension to SQLite, for a simple interactive usage from command line and for embedding in applications. Since Snel tables are read-only, it does not provide features like transactions or updates. This allows queries to be very fast since they don't have the overhead of table locking or ensuring consistency. At its core, Snel is simply a dynamic library that can be used by client applications. It has an SQLite extension for seamless integration with a traditional SQL environment and simple interactive usage from command line. ",Snel: SQL Native Execution for LLVM
44,1231908709359636484,788878862,Sam Gill,"['New paper, inbound! <LINK>']",http://arxiv.org/abs/2002.09311,"The Transiting Exoplanet Survey Satellite (TESS) has produced a large number of single transit event candidates which are being monitored by the Next Generation Transit Survey (NGTS). We observed a second epoch for the TIC-231005575 system (Tmag = 12.06, Teff = 5500 +- 85 K) with NGTS and a third epoch with Las Cumbres Observatory's (LCO) telescope in South Africa to constrain the orbital period (P = 61.777 d). Subsequent radial velocity measurements with CORALIE revealed the transiting object has a mass of M2 = 0.128 +- 0.003 M$_\odot$, indicating the system is a G-M binary. The radius of the secondary is R2 = 0.154 +- 0.008 R$_\odot$ and is consistent with models of stellar evolution to better than 1-$\sigma$. ","A long period (P = 61.8-d) M5V dwarf eclipsing a Sun-like star from TESS
  and NGTS"
45,1231871156468084736,1078963404356767745,Prof. Jared Cole,"['Very excited to announce a new paper on the arXiv, using surface acoustic waves to probe TLS physics.\nAbsolute pleasure working with the team @chalmersuniv.\n@ResearchRMIT \n<LINK>']",https://arxiv.org/abs/2002.09389,"Microscopic two-level system (TLS) defects at dielectric surfaces and interfaces are among the dominant sources of loss in superconducting quantum circuits, and their properties have been extensively probed using superconducting resonators and qubits. We report on spectroscopy of TLSs coupling to the strain field in a surface acoustic wave (SAW) resonator. The narrow free spectral range of the resonator allows for two-tone spectroscopy where a strong pump is applied at one resonance while a weak signal is used to probe a different mode. We map the spectral hole burnt by the pump tone as a function of frequency and extract parameters of the TLS ensemble. Our results suggest that detuned acoustic pumping can be used to enhance the coherence of superconducting devices by saturating TLSs. ",Acoustic spectral hole-burning in a two-level system ensemble
46,1231799551251578880,23980621,"Brett Morris, PhD","['New paper! In a tweet: I measured stellar hemispheric starspot coverage `f_S` as a function of age for F, G and K stars in six associations/clusters and find a Skumanich-like t^{−1/2} decay of starspot coverage with age. \n\n<LINK> <LINK>', 'I start with a quest to measure a property of the light curves of 531 FGK stars, which @stephtdouglas called the ""smoothed amplitude"",\xa0which is the peak-to-trough amplitude of the light curve of rotationally variable stars, phase folded on the rotation period and smoothed.', 'For one cluster the smoothed amplitudes are already published (thanks @stephtdouglas!); for the others I dug through @KeplerGO, @NASA_TESS and K2 photometry, measured photometry on the FFIs, found the best rotation period, phase folded, smoothed, and measured smoothed amplitudes. https://t.co/IhoZzZ0W1z', 'The smoothed amp is related to the stellar rotation period and age. As stars spin down (stars drift upward in plot), their smoothed amplitudes shrink (stars drift to the left). Each cluster is shown with a different color, purple=young, yellow=old. https://t.co/vBLwFnuFpB', ""Viewed another way, here's the smoothed amplitudes of the light curves of ~500 FGK stars as a function of cluster age. There's a lot of scatter, but on average, the smoothed amplitude is indeed decreasing with age. https://t.co/2FsV2cMxAq"", ""Now that's cool, but how do we convert the\xa0light curve smoothed amplitudes to the physically interesting parameter, the starspot area covering fraction f_S? This is tough to do for individual stars because we don't typically know the stellar inclination."", ""But here's an idea: imagine for a moment that all stars in each cluster are actually *identical*, but viewed from different inclinations. Then the variation in smoothed amplitudes within a cluster is due strictly to inclination, i.e.\xa0stars viewed pole-on have smaller amps. https://t.co/gK5bzDLO80"", ""That means we can model the light curves in the *ensemble* of assuming that we're viewing the stars with random inclinations. How do you model an ensemble of light curves for spotted stars? Enter: fleck! https://t.co/8EPMKdO3qf"", 'Fleck is a fast Python package which simulates smoothed amplitude distributions for ensembles of stars, given starspot properties (contrasts, latitudes, radii). Note the distribution on the right has a very specific shape: peak on the right and tail on the left, due to sin(inc). https://t.co/8RGex7uaLx', ""Now it's cheap to simulate new smoothed amplitude distributions and compare them to the observations, but it's really difficult to map the starspot properties onto a *likelihood* of observing a given smoothed amplitude distribution. Enter Approximate Bayesian Computation (ABC)!"", 'We construct a simple rejection sampling algorithm which: \n1. Forward models smoothed amp measurements for given spot params\n2. Compares amps to the observations\n3. If amps are within some tolerance of ""closeness"" to obs, accept the spot parameters\n4. Repeat! https://t.co/vpfA0jatZt', 'We repeat this process many many times to build up an approximate posterior distribution for the spot parameters which accurately reproduced the observed smoothed amplitude distribution. So even without writing down a likelihood, we can extract parameter posterior distributions!', 'Marginalizing over the other spot parameters and the unknown stellar inclinations, we estimate the approximate posterior distributions for the spot coverage f_S on stars in each cluster below. As you go from young to old, the required spot coverage decreases, as expected. https://t.co/gw98Q07J0q', 'Viewed another way, here\'s the approximate posterior spot coverage as a function of stellar age (we finally made it back to the first plot in the thread, phew!). We\'ve now successfully mapped the ""Smoothed Amplitude"" axis in the earlier age-plot onto a spot coverage f_S axis. https://t.co/7JDVF3y3O2', 'But what does it work for individual spotted stars w/ precise ages? We compared the predicted spot coverages from the previous figure for stars of various ages, and then directly modeled their rotational modulation to find their spot coverage. https://t.co/lec7buAsi3', 'We find good agreement between the predicted spot coverage for young stars (black curve) and the measured spot coverage on planet-hosting stars (points), which we assume are viewed nearly equator-on. The Sun even falls nicely on the plot if you take it at its absolute maximum. https://t.co/emLb2XBKoI', 'In 1972, Skumanich showed that stars decrease in activity with increasing Prot and age. This work extends that relationship, which showed that CaII H &amp; K emission declines as age t^{-1/2} to starspot area coverage f_S, which declines with a similar exponent. Go, Skumanich! https://t.co/oLOTDI2bXU', 'Young exoplanet folks (cc @EllieInSpace, @amannastro, @benmontet+): you can use this relationship to estimate the extent to which starspots might contaminate the transmission spectra of planets orbiting young FGK stars. This is key for understanding planet radius evolution! https://t.co/xW7Rj5tIqM', ""This paper took a village. I couldn't have done it without conversations over the last year with Suzanne Hawley, @jradavenport, @astrokiwi, @astrofleming1, @Tiana_Athriel, @KevinHeng1, @jasonleecurtis_, @cfisher94, and @eteq."", ""@jradavenport @astrokiwi @astrofleming1 @Tiana_Athriel @KevinHeng1 @jasonleecurtis_ @cfisher94 @eteq I'm dedicating this paper to Thomas Affatigato, my high school teacher who first showed me the Praesepe cluster through a telescope in 2006, who put up with my incessant questions, and who planted other important seeds that made this work possible."", '@astrokiwi Thanks for helping me organize it! https://t.co/efWctFMpeN', '@JustinCKasper Nope! We showed that rotational modulation is unlikely to produce detectable differential rotation signals on Sun-like stars in this paper (see Figure 7): https://t.co/5H7vcMpkuq', '@AstroVictorSee I don’t know enough about dynamos to answer this question - I’m hoping to chat with theorists to learn the answer to this question.', '@AstroVictorSee If you get a chance to chat with Matt Browning I’d love to hear his take on paper.', '@AstroVictorSee Yes there are masses but only for a small subset of the stars, so I didn’t wade into the Rossby number business.', '@stephtdouglas The main driver was availability of easy-to-parse cluster membership catalogs, and rotation period catalogs for FGK stars.', '@cosmodrake Thanks so much! 🤩', '@evgenya Thanks for your kind words 🙂 https://t.co/fQOycvxJ6X', ""@zpenoyre I'm not sure I follow –\xa0are you saying that the smoothed amplitude is directly related to the spot coverage? This would be true if starspots had a unique spot contrast or radius, but so far as we know, they don't. So I marginalize over these unknowns with ABC."", '@Tiana_Athriel @jradavenport @astrokiwi @astrofleming1 @KevinHeng1 @jasonleecurtis_ @cfisher94 @eteq So glad you enjoyed it! https://t.co/gYc0p8Z71O']",https://arxiv.org/abs/2002.09135,"We investigate starspot distributions consistent with space-based photometry of F, G, and K stars in six stellar associations ranging in age from 10 Myr to 4 Gyr. We show that a simple light curve statistic called the ""smoothed amplitude"" is proportional to stellar age as $t^{-1/2}$, following a Skumanich-like spin-down relation. We marginalize over the unknown stellar inclinations by forward modeling the ensemble of light curves for direct comparison with the Kepler, K2 and TESS photometry. We sample the posterior distributions for spot coverage with Approximate Bayesian Computation. We find typical spot coverages in the range 1-10% which decrease with increasing stellar age. The spot coverage is proportional to $t^n$ where $n =-0.37 \pm 0.16$, also statistically consistent with a Skumanich-like $t^{-1/2}$ decay of starspot coverage with age. We apply two techniques to estimate the spot coverage of young exoplanet-hosting stars likely to be targeted for transmission spectroscopy with the James Webb Space Telescope, and estimate the bias in exoplanet radius measurements due to varying starspot coverage. ",A Relationship Between Stellar Age and Spot Coverage
47,1231779284592812033,1105632244092321793,Dr. Jiayi Sun,"[""My new paper is out!\n\n<LINK>\n\nIt's about testing a decades-old hypothesis re: molecular clouds in various galactic environments. Yet it requires a joint analysis of so many types of data (from UV to IR to mm to radio). Super happy that I finally succeeded!"", '@j_tharindu Thanks ^_^']",https://arxiv.org/abs/2002.08964,"We compare the observed turbulent pressure in molecular gas, $P_\mathrm{turb}$, to the required pressure for the interstellar gas to stay in equilibrium in the gravitational potential of a galaxy, $P_\mathrm{DE}$. To do this, we combine arcsecond resolution CO data from PHANGS-ALMA with multi-wavelength data that traces the atomic gas, stellar structure, and star formation rate (SFR) for 28 nearby star-forming galaxies. We find that $P_\mathrm{turb}$ correlates with, but almost always exceeds the estimated $P_\mathrm{DE}$ on kiloparsec scales. This indicates that the molecular gas is over-pressurized relative to the large-scale environment. We show that this over-pressurization can be explained by the clumpy nature of molecular gas; a revised estimate of $P_\mathrm{DE}$ on cloud scales, which accounts for molecular gas self-gravity, external gravity, and ambient pressure, agrees well with the observed $P_\mathrm{turb}$ in galaxy disks. We also find that molecular gas with cloud-scale ${P_\mathrm{turb}}\approx{P_\mathrm{DE}}\gtrsim{10^5\,k_\mathrm{B}\,\mathrm{K\,cm^{-3}}}$ in our sample is more likely to be self-gravitating, whereas gas at lower pressure appears more influenced by ambient pressure and/or external gravity. Furthermore, we show that the ratio between $P_\mathrm{turb}$ and the observed SFR surface density, $\Sigma_\mathrm{SFR}$, is compatible with stellar feedback-driven momentum injection in most cases, while a subset of the regions may show evidence of turbulence driven by additional sources. The correlation between $\Sigma_\mathrm{SFR}$ and kpc-scale $P_\mathrm{DE}$ in galaxy disks is consistent with the expectation from self-regulated star formation models. Finally, we confirm the empirical correlation between molecular-to-atomic gas ratio and kpc-scale $P_\mathrm{DE}$ reported in previous works. ","Dynamical Equilibrium in the Molecular ISM in 28 Nearby Star-Forming
  Galaxies"
48,1231762070481199106,1196266674954985472,Nirmal Raj,"['New paper (my first astro-ph!) with @QuantumMessage &amp; @davemckeen on glimpsing dark matter substructure using gravitational microlensing! <LINK> Thread follows, with images from my talk (<LINK>) at @McDonaldInst in @queensu. <LINK>', 'Dark matter -- the invisible, mysterious substance lurking everywhere and outweighing five-fold all that\'s visible -- could clump into giant clouds (""minihalos"" &amp; ""boson stars""). This paper is about how to find these clouds if they weigh between asteroid and solar masses.', 'Light bends around matter. Newton said it first in 1704, Cavendish (1784) &amp; Soldner (1804) calculated the deflection, all assuming light is made of massive corpuscles. Then Einstein (1911) got this deflection with the equivalence principle. https://t.co/Ina9SoJCOg', 'But in 1915 Einstein completed general relativity and found that the curved geometry of space actually bends light twice more than formerly believed. Soon, he also realized that light-bending gives rise to a spectacular phenomenon in the sky. https://t.co/8tlqqOXXSS', 'Stare at a star for long enough, and all of a sudden it may brighten up and dim back down. This would happen when a faint object passes between you and the star, bending a lot of starlight into your eye! This brief dazzle of stars is called ""gravitational microlensing"". https://t.co/KcL2JvfPUN', ""The flicker of stars gravitationally lensed makes the unseeable seeable. It has helped us glimpse faraway, faint exoplanets acting as lenses. It's helped us see a star, acting as a source, 14 billion ly away [https://t.co/75iB58oP9X]! Can it help us find &amp; ID dark matter?"", 'Microlensing *was* once used to look for dark matter in the form of black holes and other massive compact objects (""MACHOs""), but without conclusive success. Nobody has looked for the microlensing signals of dark matter in cloudy structures. https://t.co/5pvQT0sn8C', 'The size and internal structure of these clouds impact the signal non-trivially. To see why, consider a crepe, a cookie and a donut. All equally heavy, but the masses are spread around differently, so they warp space differently, and bend light by different amounts. https://t.co/C3rNB8S3mv', ""At @TRIUMF we worked out microlensing signals under multiple assumptions about how the clouds' mass is distributed within: density profiles that scale as -9/4 and -3/2 powers of the radius, NFW, and boson star."", '(These distributions are governed by the conditions that formed the clouds when the universe was very young, much like how various types of clouds above our head get their looks from weather conditions.) https://t.co/V62FwdxjER', 'We found many surprising signal features. E.g. a lens of the right size at the right distance can cross a ""caustic"", where the number of lensed images abruptly changes and amplifies starlight infinitely! (Side note: this is the hallmark signature of exoplanet-star binaries.)', 'Such features determine the volume of the detector in the experiment, the ""lensing tube"". For a point-like (MACHO) lens this tube is a thin ellipsoid, with star and telescope on either end. For some of our lenses, the relative thickness of the tube looks like this: https://t.co/HNm8XfCGB5', '(To the particle theorist in me, this is the fabbest, maddest aspect of hunting for dark matter with light-bending. Unlike an Earth-bound particle detector built as a  mousetrap for mice of all sizes &amp; breeds, here each mouse individually shapes the dimensions of its trap.)', ""Using such information, we have now constrained the populations of dark matter clouds with data from the EROS-2 and OGLE-IV surveys that had stared for years at star fields in our galaxy's Bulge and the Magellanic Clouds. https://t.co/3DMlrUaNJs"", '@astroduff @RKLeane @QuantumMessage @davemckeen @McDonaldInst @queensu @Grace_astro Thanks a lot, @astroduff !']",https://arxiv.org/abs/2002.08962,"Dark matter may be in the form of non-baryonic structures such as compact subhalos and boson stars. Structures weighing between asteroid and solar masses may be discovered via gravitational microlensing, an astronomical probe that has in the past helped constrain the population of primordial black holes and baryonic MACHOs. We investigate the non-trivial effect of the size of and density distribution within these structures on the microlensing signal, and constrain their populations using the EROS-2 and OGLE-IV surveys. Structures larger than a solar radius are generally constrained more weakly than point-like lenses, but stronger constraints may be obtained for structures with mass distributions that give rise to caustic crossings or produce larger magnifications. ",Gravitational microlensing by dark matter in extended structures
49,1230896596545613829,2264368966,Rafael Müller,"['Our new paper ""Subclass Distillation"" w/ @skornblith and @geoffreyhinton is out: <LINK>. We improve distillation on binary classification by forcing the teacher to divide its predictions into subclasses. The student then mimics the subclass predictions. (1/4) <LINK>', '@skornblith @geoffreyhinton For datasets where there are known, natural subclasses we demonstrate that the teacher learns similar subclasses. Here, we make a binary version of CIFAR10 and our method discovers the original subclasses. (2/4) https://t.co/JDNn6ajalj', '@skornblith @geoffreyhinton When the student mimics the teacher’s subclasses (subclass distillation SC-D), we outperform distillation (D) and penultimate layer distillation (PL-D) and we match the performance of a network fully trained directly on the natural subclasses. (3/4) https://t.co/Mxyo5rb3qI', '@skornblith @geoffreyhinton For more details and results on other datasets, check https://t.co/L2vu6rp9Xs (4/4)']",https://arxiv.org/abs/2002.03936,"After a large ""teacher"" neural network has been trained on labeled data, the probabilities that the teacher assigns to incorrect classes reveal a lot of information about the way in which the teacher generalizes. By training a small ""student"" model to match these probabilities, it is possible to transfer most of the generalization ability of the teacher to the student, often producing a much better small model than directly training the student on the training data. The transfer works best when there are many possible classes because more is then revealed about the function learned by the teacher, but in cases where there are only a few possible classes we show that we can improve the transfer by forcing the teacher to divide each class into many subclasses that it invents during the supervised training. The student is then trained to match the subclass probabilities. For datasets where there are known, natural subclasses we demonstrate that the teacher learns similar subclasses and these improve distillation. For clickthrough datasets where the subclasses are unknown we demonstrate that subclass distillation allows the student to learn faster and better. ",Subclass Distillation
50,1230858703244595202,310350900,Alejandro Cruz-Osorio,"['Our new paper ""Neutron and quark stars: constraining the parameters for simple EoS using the GW170817 is now available in @arxiv (<LINK>) 🥳']",https://arxiv.org/abs/2002.08879,"It is well known that the equation of state (EoS) of compact objects like neutron and quark stars is not determined despite there are several sophisticated models to describe it. From the electromagnetic observations, summarized in \cite{Lattimer01}, and the recent observation of gravitational waves from binary neutron star inspiral GW170817 \cite{Abbott2017_etal} and GW190425 \cite{Abbott2019}, it is possible to make an estimation of the range of masses and so constraint the mass of the neutron and quark stars, determining not only the best approximation for the EoS, but which kind of stars we would be observing. In this paper we explore several configurations of neutron stars assuming a simple polytropic equation of state, using a single layer model without crust. In particular, when the EoS depends on the mass rest density, $p=K \rho_{0}^{\Gamma}$, and when it depends on the energy density $p=K \rho^{\Gamma}$, considerable differences in the mass-radius relationships are found. On the other hand, we also explore quark stars models using the MIT bag EoS for different values of the vacuum energy density $B$. ","Neutron and quark stars: constraining the parameters for simple EoS
  using the GW170817"
51,1230699445244715009,969629299933315073,Timur Garipov,"['First paper written at a new lab is out on arxiv!\n\n""The Benefits of Pairwise Discriminators for Adversarial Training""\n\nJoint work with @ShangyuanTong and Tommi Jaakkola.\n\nArxiv: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2002.08621,"Adversarial training methods typically align distributions by solving two-player games. However, in most current formulations, even if the generator aligns perfectly with data, a sub-optimal discriminator can still drive the two apart. Absent additional regularization, the instability can manifest itself as a never-ending game. In this paper, we introduce a family of objectives by leveraging pairwise discriminators, and show that only the generator needs to converge. The alignment, if achieved, would be preserved with any discriminator. We provide sufficient conditions for local convergence; characterize the capacity balance that should guide the discriminator and generator choices; and construct examples of minimally sufficient discriminators. Empirically, we illustrate the theory and the effectiveness of our approach on synthetic examples. Moreover, we show that practical methods derived from our approach can better generate higher-resolution images. ",The Benefits of Pairwise Discriminators for Adversarial Training
52,1230685393223372800,106843613,Jacob Haqq Misra,"['New paper by myself, @ravi_kopparapu &amp; @nogreenstars - we argue that technosignatures are a logical continuation of the search for biosignatures, with the search for technosignatures providing important information about the future of civilization on Earth\n<LINK>']",https://arxiv.org/abs/2002.08776,"The search for spectroscopic biosignatures with the next-generation of space telescopes could provide observational constraints on the abundance of exoplanets with signs of life. An extension of this spectroscopic characterization of exoplanets is the search for observational evidence of technology, known as technosignatures. Current mission concepts that would observe biosignatures from ultraviolet to near-infrared wavelengths could place upper limits on the fraction of planets in the galaxy that host life, although such missions tend to have relatively limited capabilities of constraining the prevalence of technosignatures at mid-infrared wavelengths. Yet searching for technosignatures alongside biosignatures would provide important knowledge about the future of our civilization. If planets with technosignatures are abundant, then we can increase our confidence that the hardest step in planetary evolution--the Great Filter--is probably in our past. But if we find that life is commonplace while technosignatures are absent, then this would increase the likelihood that the Great Filter awaits to challenge us in the future. ",Observational Constraints on the Great Filter
53,1230681349415198722,891665233919569921,Valeria,['New paper: Anisotropic infall in the outskirts of OmegaWINGS galaxy clusters\nSubmitted to MNRAS. In press. With @DarkHeraclitus\n <LINK>\n#filaments #clustersofgalaxies #starforming #astronomy'],https://arxiv.org/abs/2002.07825,"We study the effects of the environment on galaxy quenching in the outskirts of clusters at $0.04 < z < 0.08$. We use a subsample of 14 WINGS and OmegaWINGS clusters that are linked to other groups/clusters by filaments and study separately galaxies located in two regions in the outskirts of these clusters according to whether they are located towards the filaments' directions or not. We also use samples of galaxies in clusters and field as comparison. Filamentary structures linking galaxy groups/clusters were identified over the Six Degree Field Galaxy Redshift Survey Data Release 3. We find a fraction of passive galaxies in the outskirts of clusters intermediate between that of the clusters and the field's. We find evidence of a more effective quenching in the direction of the filaments. We also analyse the abundance of post-starburst galaxies in the outskirts of clusters focusing our study on two extreme sets of galaxies according to their phase-space position: backsplash and true infallers. We find that up to $\sim70\%$ of post-starburst galaxies in the direction of filaments are likely backsplash, while this number drops to $\sim40\%$ in the isotropic infall region. The presence of this small fraction of galaxies in filaments that are falling into clusters for the first time and have been recently quenched, supports a scenario in which a significant number of filament galaxies have been quenched long time ago. ",Anisotropic infall in the outskirts of OmegaWINGS galaxy clusters
54,1230669857840123906,2800204849,Andrew Gordon Wilson,"['Our new paper ""Bayesian Deep Learning and a Probabilistic Perspective of Generalization"": <LINK>. Includes (1) benefits of BMA; (2) BMA &lt;-&gt; Deep Ensembles; (3) new methods; (4) BNN priors; (5) generalization in DL; (6) tempering in BDL. With @Pavel_Izmailov. 1/19 <LINK>', 'Since neural nets can fit images with noisy labels, it has been suggested we should rethink generalization. But this behaviour is understandable from a probabilistic perspective: we want to support any possible solution, but also have good inductive biases. 2/19', 'The inductive biases determine what solutions are a priori likely. Indeed, we show this seemingly mysterious behaviour is not unique to neural nets: GPs with RBF kernels can perfectly fit noisy CIFAR, but also generalize on the noise free problem. 3/19 https://t.co/HA1GwqxHFS', 'We should not conflate flexibility with complexity. RBF-GPs are highly flexible, but have simple inductive biases. While BNNs *can* fit noise, noisy datasets are unlikely in the prior over functions induced by a simple parameter prior, as per the marginal likelihood. 4/19', 'Indeed, the prior over functions p(f) induced by p(w)=N(0, a I) encapsulates many of the desirable properties of neural nets that help them generalize, including a reasonable prior correlation structure over images. 5/19 https://t.co/hSD8nR0zxZ', ""These properties of the prior matter more than e.g. signal variance. In the cold posterior paper, it appears sample prior functions assign mostly 1 class to all data. While this behaviour appears dramatic, it is simply an artifact of a bad signal variance ‘a'… 6/19 https://t.co/UURGe7mCds"", ""A large signal variance causes the softmax to quickly saturate. This behaviour is easy to fix by tuning ‘a', or using what is standard for L2 reg. And even a bad ‘a’ leads to a reasonable prior predictive distribution, and a posterior that quickly adapts to data. 7/19 https://t.co/Aa2xyLB27U"", 'We present several views on tempering. (1) It would be surprising for T=1 to in fact be the best setting of this hyperparameter; tempering reflects misspecification, and we all know our models are misspecified. 8/19', '(2) In this sense, the tempered posterior is closer to representing our true beliefs that the model is misspecified than a posterior with no tempering. And Bayesian inference is all about marginalization and how our honest beliefs interact with data to form a posterior. 9/19', '(3) Further, tempering is a simply a modification of our observation model. Learning the tempering parameter is the same as learning noise in a regression model, which is what we would always do in a Bayesian approach. 10/19', '(4) If we are worried about T&lt;1 overcounting data, then we must also say we believe the T=1 likelihood, which is not likely true. Moreover, empirical Bayes counts the same data in the prior and likelihood, but is embraced and championed in foundational work on BNNs. 11/19', '(5) There is a body of work on understanding tempering in a Bayesian setting, referred to as Safe Bayes and Generalized Bayesian Inference (for both T&lt;1 and T&gt;1). 12/19', '(6) Also there are cases where given finite computation, tempering can be preferred even if we believe the prior and untempered likelihood. We consider these questions further in the paper. 13/19', 'We also emphasize that one should not conflate simple MC with BMA. Our goal is not to produce accurate samples from a posterior, but to evaluate an integral subject to computational constraints. 14/19', 'Indeed, we show from this perspective of integration, deep ensembles can provide a *better* approximation to BMA than standard approaches to approximate Bayesian inference. 15/19 https://t.co/xNAFWbQ65X', 'Inspired by deep ensembles, we propose MultiSWAG, which marginalizes within basins. We see with the same training time, we can get much better performance than deep ensembles, especially for highly corrupted data. 16/19 https://t.co/JwzoInVvjf', 'While it has been claimed recently that Bayesian methods are typically applied in the underparametrized regime, this is not true. See this quote from David MacKay, 25 years ago. BMA is in fact most valuable when the posterior is underspecified. 17/19 https://t.co/QredGj8CFg', 'The appendix has several additional results, including correlation structures induced by a BNN prior subject to a variety of data perturbations, and additional results for deep ensembles, multiSWA, and multiSWAG. 18/19 https://t.co/QxW3zXZMwi', 'Additional experiments, results, discussions, perspectives, thoughts, in the paper. Code (including MultiSWAG) also available:\nhttps://t.co/DtwpufjDHt\nAnd it was wonderful working with @izmailovpavel, an amazing collaborator. 19/19', 'Argh... apple :) \nHere are the correct figures. https://t.co/IVBfVYgc33', '@PierreAlquier Good catch. I meant the principle of learning T is similar to learning the noise level (and have related interpretations in how the data are weighted). In any case, this claim is not made in the paper.', '@roydanroy @Pavel_Izmailov I don’t think it suggests that. In any case, I’m happy to add a discussion of the PAC Bayes papers you mention, which would I think add value. Thanks for the pointers.']",https://arxiv.org/abs/2002.08791,"The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions. ",Bayesian Deep Learning and a Probabilistic Perspective of Generalization
55,1230644029311901703,494134136,Krzysztof Geras,"['We just released a new paper on deep learning for screening mammography!\n\n“An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization”.\n\n<LINK>\n\n1/7 <LINK>', 'Despite learning only with image-level labels, the model achieves an AUC of 0.93, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, it is 4.1x faster while using 78.4% less memory. In a reader study, it surpasses radiologist-level AUC by a margin of 0.11.\n\n2/7 https://t.co/d0PdA0ZBg9', 'Our model works in 3 stages. (1) Looking at the entire image with a network of a relatively low capacity to identify the most informative patches. (2) Looking at these patches with a network of a higher capacity. (3) Integrating information obtained in stages (1) and (2).\n\n3/7 https://t.co/YsoNpVmhjs', 'We conducted extensive experiments on the importance of various design choices in the model, the type of pooling aggregating saliency maps, the number of patches extracted to examine further, ...\n\n4/7 https://t.co/4uZr23aEcC', 'We are expecting that our model will be especially useful when applied to data for which it is difficult or impossible to collect pixel-level annotations. We also hypothesize that this or similar models could be useful to discover new biomarkers.\n\n5/7 https://t.co/I4S9N87lcX', 'We made the code and the model public at https://t.co/xATXueLLWw. We are hoping to enable others to experiment with and build upon our model! Please let us know if you find it useful.\n\n6/7', 'This paper is another product of a collaboration between @cai2r and @NYUDataScience. It was led by @ArtieShen, supported by @NanWu__, @zhansheng, @jpatrickpark, Kangning Liu, @TyagiSudarshini, Laura Heacock, S. Gene Kim, @DrLindaMoy, @kchonyc and myself.\n\n7/7']",http://arxiv.org/abs/2002.07613,"Medical images differ from natural images in significantly higher resolutions and smaller regions of interest. Because of these differences, neural network architectures that work well for natural images might not be applicable to medical image analysis. In this work, we extend the globally-aware multiple instance classifier, a framework we proposed to address these unique properties of medical images. This model first uses a low-capacity, yet memory-efficient, network on the whole image to identify the most informative regions. It then applies another higher-capacity network to collect details from chosen regions. Finally, it employs a fusion module that aggregates global and local information to make a final prediction. While existing methods often require lesion segmentation during training, our model is trained with only image-level labels and can generate pixel-level saliency maps indicating possible malignant findings. We apply the model to screening mammography interpretation: predicting the presence or absence of benign and malignant lesions. On the NYU Breast Cancer Screening Dataset, consisting of more than one million images, our model achieves an AUC of 0.93 in classifying breasts with malignant findings, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, our model is 4.1x faster for inference while using 78.4% less GPU memory. Furthermore, we demonstrate, in a reader study, that our model surpasses radiologist-level AUC by a margin of 0.11. The proposed model is available online: this https URL ","An interpretable classifier for high-resolution breast cancer screening
  images utilizing weakly supervised localization"
56,1230594601834041344,731527518910447616,"Jose Unpingco, PhD",['Check out our new paper: Multiple Imputation with Denoising Autoencoder using Metamorphic Truth and Imputation Feedback <LINK> #Datascience #MachineLearning'],https://arxiv.org/abs/2002.08338,"Although data may be abundant, complete data is less so, due to missing columns or rows. This missingness undermines the performance of downstream data products that either omit incomplete cases or create derived completed data for subsequent processing. Appropriately managing missing data is required in order to fully exploit and correctly use data. We propose a Multiple Imputation model using Denoising Autoencoders to learn the internal representation of data. Furthermore, we use the novel mechanisms of Metamorphic Truth and Imputation Feedback to maintain statistical integrity of attributes and eliminate bias in the learning process. Our approach explores the effects of imputation on various missingness mechanisms and patterns of missing data, outperforming other methods in many standard test cases. ","Multiple Imputation with Denoising Autoencoder using Metamorphic Truth
  and Imputation Feedback"
57,1230581423280574464,583637312,Srijan Kumar,"['New paper alert ""Higher-Order Label Homogeneity and Spreading in Graphs"" to appear at WWW 2020 @TheWebConf #www2020 <LINK>']",https://arxiv.org/abs/2002.07833,"Do higher-order network structures aid graph semi-supervised learning? Given a graph and a few labeled vertices, labeling the remaining vertices is a high-impact problem with applications in several tasks, such as recommender systems, fraud detection and protein identification. However, traditional methods rely on edges for spreading labels, which is limited as all edges are not equal. Vertices with stronger connections participate in higher-order structures in graphs, which calls for methods that can leverage these structures in the semi-supervised learning tasks. To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels using higher-order structures. HOLS has strong theoretical guarantees and reduces to standard label spreading in the base case. Via extensive experiments, we show that higher-order label spreading using triangles in addition to edges is up to 4.7% better than label spreading using edges alone. Compared to prior traditional and state-of-the-art methods, the proposed method leads to statistically significant accuracy gains in all-but-one cases, while remaining fast and scalable to large graphs. ",Higher-Order Label Homogeneity and Spreading in Graphs
58,1230407626862858241,281711973,Dr. Emily Rickman,"['Today I have a new paper up on arXiv👇of a new benchmark brown dwarf😱\n\n<LINK>\n\nWe followed up some previously detected giant planets &amp; low-mass brown dwarfs from CORALIE (<LINK>) with VLT/SPHERE @SPHERE_outreach &amp; LOOK HOW BEAUTIFUL IT IS 🔭✨ <LINK>', '@SPHERE_outreach But what is even cooler is that we have can look at its atmosphere and we see indicators of methane and water 😃🌟\n\nDetections like these are vital for calibrating atmospheric models of ultracool objects &amp; act as analogues towards understanding exoplanetary atmospheres 🪐⭐️🔭 https://t.co/jDw6RprYXK', ""@ClaireEsau @SPHERE_outreach This!! I'm just writing up my thesis now &amp; someone said to me the other day you have to think of it as a snapshot in time of where your research currently is rather than a complete A-Z of every problem solved. I like this way of thinking.""]",https://arxiv.org/abs/2002.08319,"Context. HD13724 is a nearby solar-type star at 43.48 $\pm$ 0.06 pc hosting a long-period low-mass brown dwarf detected with the CORALIE echelle spectrograph as part of the historical CORALIE radial-velocity search for extra-solar planets. The companion has a minimum mass of $26.77^{+4.4}_{-2.2} M_{\mathrm{Jup}}$ and an expected semi-major axis of $\sim$ 240 mas making it a suitable target for further characterisation with high-contrast imaging, in particular to measure its inclination, mass, and spectrum and thus establish its substellar nature. Aims. Using high-contrast imaging with the SPHERE instrument on the Very Large Telescope (VLT), we are able to directly image a brown dwarf companion to HD13724 and obtain a low-resolution spectrum. Methods. We combine the radial-velocity measurements of CORALIE and HARPS taken over two decades and high contrast imaging from SPHERE to obtain a dynamical mass estimate. From the SPHERE data we obtain a low resolution spectrum of the companion from Y to J band, as well as photometric measurements from IRDIS in the J, H and K bands. Results. Using high-contrast imaging with the SPHERE instrument at the VLT, we report the first images of a brown dwarf companion to the host star HD13724. It has an angular separation of 175.6 $\pm$ 4.5 mas and H-band contrast of $10.61\pm0.16$ mag and, using the age estimate of the star to be $\sim$1 Gyr, gives an isochronal mass estimate of $\sim$44 $M_{\mathrm{Jup}}$. By combining radial-velocity and imaging data we also obtain a dynamical mass of $50.5^{+3.3}_{-3.5} M_{\mathrm{Jup}}$. Through fitting an atmospheric model, we estimate a surface gravity of $\log g = 5.5$ and an effective temperature of 1000K. A comparison of its spectrum with observed T dwarfs estimates a spectral type of T4 or T4.5, with a T4 object providing the best fit. ","Spectral and atmospheric characterisation of a new benchmark brown dwarf
  HD13724B"
59,1230403600729477120,75249390,Axel Maas,"['We have published a new paper in which we show how the photon could be actually made up by other particles, but still can be massless. We obtained this with large scale numerical simulations. It is available at <LINK>', 'This is a big step in understanding a mathematically consistent approach to so-called grand-unified theories - theories which combine electromagnetism, and the weak and strong nuclear force into one.', 'We have predicted that this should be possible some years ago in https://t.co/Tl2jGHjFt4 - it was quite hard technically to get simulations for this up and running.', 'This was mainly possible due to the hard work of my (current) PhD student Vincenzo Afferante and my (previous) PhD student @ptoerek - a great team to get something so tremendously important as a new perspective on the nature of the photon going.', 'You can meet Vincenzo and hear all about the results also at the upcoming #ALPS2020 conference in April.']",https://arxiv.org/abs/2002.08221,"In a non-perturbative gauge-invariant formulation of grand-unified theories all low energy vector states need to be composite with respect to the high-scale gauge group, including the photon. We investigate this by using lattice methods to spectroscopically analyze the vector channel in a toy grand-unified theory, an SU(2) adjoint Higgs model. Our results support indeed the existence of a massless composite vector particle. ",A composite massless vector boson
60,1230358061883297793,23980621,"Brett Morris, PhD","[""New paper! ESA's PLATO mission will hunt for Earths orbiting Sun-like stars. Sun-like stars vary in brightness due to p-mode oscillations and granulation. We study how these phenomena affect planet radius precision, and the results may surprise you:\n\n<LINK> <LINK>"", ""Stellar granulation is the pattern of hot, bright upwelling plasma surrounded by cooler and dimmer down-flowing plasma which tiles the surface of a Sun-like star. Here's what the Earth would look like in-transit on a *real* image of the solar granulation pattern by SDO's HMI: https://t.co/e43OrIF7bB"", 'One of the neat things we did in this paper was devise a numerically efficient method for simulating transits of real SDO HMI images, with some heroic CPU-efficiency help from  @manodeepsinha at #pyastro that sped up the solar transit simulator code by a few orders of magnitude.', 'Here are 282 simulated transit light curves of an Earth transiting the SDO/HMI images of the quiet Sun (left: full transit, right: zoom into mid-transit). The spread at flux minimum is due to solar surface brightness variations due to granulation and activity. https://t.co/3SlHW8ZVo2', 'Good news: surface granulation of Sun-like stars imparts a flux variation of a few parts per million – too small to affect most exoplanet observations in the present or near-future. (Image credit: #DKIST/NSO/NSF/AURA) https://t.co/H9VrodYXi9', 'But standing pressure waves (p-modes) in the solar atmosphere with periods ~5 minutes are a bigger threat. They impart stochastic O(100 ppm) variations on transit light curves. This is what the full-mission PLATO light curve of the Earth on the Sun would look like due to p-modes: https://t.co/c1iiqt3IRq', '🌟Important🌟: we found that a degeneracy between the impact parameter, limb-darkening, and the exoplanet radius enforce a lower limit on the radius precision near ~3%, which accounts for the full PLATO mission error budget, before accounting for detector/photon noise.', ""If it's possible to constrain the impact parameter or to obtain follow-up observations at longer wavelengths where limb-darkening is less significant, this may enable higher precision radius measurements –\xa0but we'll have to think hard about how to do this efficiently. https://t.co/Yd5hVIJYPS"", 'Big shoutouts to Monica Bobra for bringing her fantastic solar expertise to the project, @AgolEric for insightful transit light curve insights, and the talented undergrad Yu Jin Lee for keeping us inspired on this project.', ""@exoZafar It'd be awfully hard –\xa0the p-mode signals are &gt;40 peaks in frequency space, each contributing a slightly different, super-imposed oscillation pattern. It would be difficult (though maybe not impossible?) to model directly in the time domain. https://t.co/n3UOAvwBA0"", '@exoZafar The other major problem is that these simulated observations were done *without any\xa0observational/systematic noise* –\xa0when you add a white noise component it becomes much more difficult to uniquely model the oscillations in the time domain.', ""@exoZafar It's a hurdle to reaching the *extreme* radius precision of 3%. You might reasonably ask what we need that kind of precision for. I'd argue that 5% is still good enough to do interesting exoplanet demographics, for example."", ""@exoZafar M dwarfs aren't the focus of PLATO so I've mostly left them out of the discussion. The science reason for this is that the stellar *magnetic activity* (spots, faculae and the like) will be the dominant phenomena shaping the transit residuals."", '@exoZafar Thanks for your enthusiasm! https://t.co/80frHOgHpZ', '@nespinozap Thanks, and yes! See Sect 4.2 for the full discussion, and Appendix Fig A1 for the corner plot. If you had really good constraints on the LD parameters I think the degeneracy would persist.', ""@exohugh @nespinozap I'm pretty sure the stellar density helps you constrain a/R_star, but you can still exchange orbital inclination (or b) for transit duration. Also, I don't know where you'd get a prior on eccentricity from since these targets are RV-inaccessible.""]",https://arxiv.org/abs/2002.08072,"One of the main science motivations for the ESA PLAnetary Transit and Oscillations (PLATO) mission is to measure exoplanet transit radii with 3% precision. In addition to flares and starspots, stellar oscillations and granulation will enforce fundamental noise floors for transiting exoplanet radius measurements. We simulate light curves of Earth-sized exoplanets transiting continuum intensity images of the Sun taken by the HMI instrument aboard SDO to investigate the uncertainties introduced on the exoplanet radius measurements by stellar granulation and oscillations. After modeling the solar variability with a Gaussian process, we find that the amplitude of solar oscillations and granulation is of order 100 ppm -- similar to the depth of an Earth transit -- and introduces a fractional uncertainty on the depth of transit of 0.73% assuming four transits are observed over the mission duration. However, when we translate the depth measurement into a radius measurement of the planet, we find a much larger radius uncertainty of 3.6%. This is due to a degeneracy between the transit radius ratio, the limb-darkening, and the impact parameter caused by the inability to constrain the transit impact parameter in the presence of stellar variability. We find that surface brightness inhomogeneity due to photospheric granulation contributes a lower limit of only 2 ppm to the photometry in-transit. The radius uncertainty due to granulation and oscillations, combined with the degeneracy with the transit impact parameter, accounts for a significant fraction of the error budget of the PLATO mission, before detector or observational noise is introduced to the light curve. If it is possible to constrain the impact parameter or to obtain follow-up observations at longer wavelengths where limb-darkening is less significant, this may enable higher precision radius measurements. ","The Stellar Variability Noise Floor for Transiting Exoplanet Photometry
  with PLATO"
61,1230306852912525312,2337598033,Geraint F. Lewis,['New paper on the @arXiver with PhD student Florian List. “A unified framework for 21cm tomography sample generation and parameter inference with Progressively Growing GANs” <LINK> <LINK>'],https://arxiv.org/abs/2002.07940,"Creating a database of 21cm brightness temperature signals from the Epoch of Reionisation (EoR) for an array of reionisation histories is a complex and computationally expensive task, given the range of astrophysical processes involved and the possibly high-dimensional parameter space that is to be probed. We utilise a specific type of neural network, a Progressively Growing Generative Adversarial Network (PGGAN), to produce realistic tomography images of the 21cm brightness temperature during the EoR, covering a continuous three-dimensional parameter space that models varying X-ray emissivity, Lyman band emissivity, and ratio between hard and soft X-rays. The GPU-trained network generates new samples at a resolution of $\sim 3'$ in a second (on a laptop CPU), and the resulting global 21cm signal, power spectrum, and pixel distribution function agree well with those of the training data, taken from the 21SSD catalogue \citep{Semelin2017}. Finally, we showcase how a trained PGGAN can be leveraged for the converse task of inferring parameters from 21cm tomography samples via Approximate Bayesian Computation. ","A unified framework for 21cm tomography sample generation and parameter
  inference with Progressively Growing GANs"
62,1230127951548690434,3325105445,Ben F. Maier,"['In our new paper we show why lab-confirmed case numbers of the #Coronavirus #COVID19 do not grow exponentially: <LINK>\n\nAnswer: Effective containment policies isolate the healthy population well enough for case numbers to follow an algebraic scaling law. 1/9 <LINK>', 'classically, uncontained epidemic outbreaks can be modeled by means of the SIR model: individuals are either (S)usceptible, (I)nfectious, or (R)emoved from the process. \n\n""S"" enter the ""I"" compartment with rate alpha, and ""I"" are transferred to ""R"" with rate beta\n\n2/9 https://t.co/KRhNfSrMEq', ""In this basic system, the number of infecteds grows exponentially, initially.\n\nSo why don't we see that in the current outbreak?\n\n3/9 https://t.co/b3MRF664mO"", 'The first thing one may think of is that symptomatic infecteds are usually quarantined. One can model that by including a quarantine rate kappa with which infecteds are isolated from the transmission process\n\n4/9 https://t.co/1kJJDzFv50', 'However, this would still yield an exponential growth behavior, albeit with a slower velocity\n\n5/9 https://t.co/qVcV6BuSly', 'Now, the implemented containment policies in several provinces in China did not only affect infecteds but also the general public. We can model that by including a public isolation rate kappa0\n\n6/9 https://t.co/xVg0G1BVUP', 'Doing this, we see that the exponential growth is suppressed by the depletion of susceptible individuals from the transmission process\n\n7/9 https://t.co/nYBuuGrCb6', 'additionally, the lab-confirmed cases are not reflected by the (I) compartment -- rather, we have to assume that infecteds can only be counted after a time lag, approximately when they are quarantined in a new compartment (X) which follows the empirically observed behavior.\n\n8/9 https://t.co/6fPRq5IjBM', 'our analysis material is available online: \n\nhttps://t.co/jAobWd7nd8\n\nThank you, @DirkBrockmann, for this fruitful and fast collaboration!\n\n9/9', ""@balzacdiet @DirkBrockmann yes. this would be reflected in the infecteds' quarantine rate. we also tested a variation of that model where confirmation is decoupled from quarantine and find similar growth patterns. The growth behavior is dominated by the removal of healthy individuals from the process."", ""@bene_beck @DirkBrockmann @ewyler so far, this seems to be the case. However, the model assumes that eventually all susceptibles become and remain isolated. This is impossible in reality, so we'll see what happens in the following few weeks. I suspect the model underestimates total number of cases (see Discuss.)""]",http://arxiv.org/abs/2002.07572,"The recent outbreak of COVID-19 in Mainland China is characterized by a distinctive algebraic, sub-exponential increase of confirmed cases during the early phase of the epidemic, contrasting an initial exponential growth expected for an unconstrained outbreak with sufficiently large reproduction rate. Although case counts vary significantly between affected provinces in Mainland China, the scaling law $t^{\mu}$ is surprisingly universal, with a range of exponents $\mu=2.1\pm0.3$. The universality of this behavior indicates that despite social, regional, demographical, geographical, and socio-economical heterogeneities of affected Chinese provinces, this outbreak is dominated by fundamental mechanisms that are not captured by standard epidemiological models. We show that the observed scaling law is a direct consequence of containment policies that effectively deplete the susceptible population. To this end we introduce a parsimonious model that captures both, quarantine of symptomatic infected individuals as well as population wide isolation in response to mitigation policies or behavioral changes. For a wide range of parameters, the model reproduces the observed scaling law in confirmed cases and explains the observed exponents. Quantitative fits to empirical data permit the identification of peak times in the number of asymptomatic or oligo-symptomatic, unidentified infected individuals, as well as estimates of local variations in the basic reproduction number. The model implies that the observed scaling law in confirmed cases is a direct signature of effective contaiment strategies and/or systematic behavioral changes that affect a substantial fraction of the susceptible population. These insights may aid the implementation of containment strategies in potential export induced COVID-19 secondary outbreaks elsewhere or similar future outbreaks of other emergent infectious diseases. ","Effective containment explains sub-exponential growth in confirmed cases
  of recent COVID-19 outbreak in Mainland China"
63,1230121812480188416,595554321,Jason D Lotay,"['Really pleased to have finished my new paper with Spiro Karigiannis on coassociative fibrations of Bryant-Salamon G2-manifolds <LINK>. There are many interesting connections to work of Donaldson, Madsen-Swann, Atiyah-Witten, Acharya-Bryant-Salamon and more...']",https://arxiv.org/abs/2002.06444,"Bryant-Salamon constructed three 1-parameter families of complete manifolds with holonomy $\mathrm{G}_2$ which are asymptotically conical to a holonomy $\mathrm{G}_2$ cone. For each of these families, including their asymptotic cone, we construct a fibration by asymptotically conical and conically singular coassociative 4-folds. We show that these fibrations are natural generalizations of the following three well-known coassociative fibrations on $\mathbb R^7$: the trivial fibration by 4-planes, the product of the standard Lefschetz fibration of $\mathbb C^3$ with a line, and the Harvey-Lawson coassociative fibration. In particular, we describe coassociative fibrations of the bundle of anti-self-dual 2-forms over the 4-sphere $\mathcal{S}^4$, and the cone on $\mathbb C \mathbb P^3$, whose smooth fibres are $T^*\mathcal{S}^2$, and whose singular fibres are $\mathbb R^4/\{\pm 1\}$. We relate these fibrations to hypersymplectic geometry, Donaldson's work on Kovalev-Lefschetz fibrations, harmonic 1-forms and the Joyce--Karigiannis construction of holonomy $\mathrm{G}_2$ manifolds, and we construct vanishing cycles and associative ""thimbles"" for these fibrations. ",Bryant-Salamon $\mathrm{G}_2$ manifolds and coassociative fibrations
64,1230103939506483205,127058544,Russell Smith,"['New paper by @phdwcollier on searching for nearby lenses in new and archival data from MUSE: <LINK>', ""@phdwcollier There's a couple of new z&lt;0.05 strong-lensing *clusters* (with lensing mass dominated by DM not stars). https://t.co/uNKFUpP7aq"", ""@phdwcollier Among the field galaxies, the big find (J0403-0239) was summarised in Will's previous paper.   \n\nThe new paper has some nice HST follow-up imaging and blue spectroscopy to address the age-vs-IMF degeneracy. https://t.co/z7YMZSQYzx"", 'There are no more multiply-imaged sources among the sample, but quite a number of close-projected singly-imaged cases, which can add some information on the distribution of IMF variation.', 'One of these has *three* singly-imaged background sources within 5 arcsec, at different redshifts, any which could have had detectable counter-images if the stellar M/L were high enough (i.e. IMF heavy enough). https://t.co/uGmAWLZN9C']",https://arxiv.org/abs/2002.07191v1,"Low-redshift strong-lensing galaxies can provide robust measurements of the stellar mass-to-light ratios in early-type galaxies (ETG), and hence constrain variations in the stellar initial mass function (IMF). At present, only a few such systems are known. Here, we report the first results from a blind search for gravitationally-lensed emission line sources behind 52 massive $z$ $<$ 0.07 ETGs with MUSE integral field spectroscopy. For 16 galaxies, new observations were acquired, whilst the other 36 were analysed from archival data. This project has previously yielded one confirmed galaxy-scale strong lens (J0403-0239) which we report in an earlier paper. J0403-0239 has since received follow-up observations, presented here, which indicate support for our earlier IMF results. Three cluster-scale, and hence dark-matter-dominated, lensing systems were also discovered (central galaxies of A4059, A2052 and AS555). For nine further galaxies, we detect a singly-imaged but closely-projected source within 6 arcsec (including one candidate with sources at three different redshifts); such cases can be exploited to derive upper limits on the IMF mass-excess factor, $\alpha$. Combining the new lens and new upper limits, with the previously-discovered systems, we infer an average $\langle \alpha \rangle$ = 1.06 $\pm$ 0.08 (marginalised over the intrinsic scatter), which is inconsistent with a Salpeter-like IMF ($\alpha$ = 1.55) at the 6$\sigma$ level. We test the detection threshold in these short-exposure MUSE observations with the injection and recovery of simulated sources, and predict that one in twenty-five observations is expected to yield a new strong-lens system. Our observational results are consistent with this expected yield. ",] MNELLS: The MUSE Nearby Early-Type Galaxy Lens Locator Survey
65,1230051699856986113,945445796098473984,Patrick Schnider,"['A new paper on @arxiv .\nTogether with Justin, we give algorithms to compute stabbing points in the setting of the Hadwiger-Debrunner (p,q)-theorem and generalize the tight bounds to abstractions of convex spaces. #research #computational #geometry \n\n<LINK>']",https://arxiv.org/abs/2002.06947,"Hadwiger and Debrunner showed that for families of convex sets in $\mathbb{R}^d$ with the property that among any $p$ of them some $q$ have a common point, the whole family can be stabbed with $p-q+1$ points if $p \geq q \geq d+1$ and $(d-1)p < d(q-1)$. This generalizes a classical result by Helly. We show how such a stabbing set can be computed for a family of convex polygons in the plane with a total of $n$ vertices in $O((p-q+1)n^{4/3}\log^{8} n(\log\log n)^{1/3} + np^2)$ expected time. For polyhedra in $\mathbb{R}^3$, we get an algorithm running in $O((p-q+1)n^{5/2}\log^{10} n(\log\log n)^{1/6} + np^3)$ expected time. We also investigate other conditions on convex polygons for which our algorithm can find a fixed number of points stabbing them. Finally, we show that analogous results of the Hadwiger and Debrunner $(p,q)$-theorem hold in other settings, such as convex sets in $\mathbb{R}^d\times\mathbb{Z}^k$ or abstract convex geometries. ","Efficiently stabbing convex polygons and variants of the
  Hadwiger-Debrunner $(p, q)$-theorem"
66,1229964580283179008,2766925212,Andrew Childs,"['New paper with Arunachalam, Belovs, @RobinKothari, Rosmanis, &amp; de Wolf solves a coupon collector problem with quantum states instead of random samples. Unlike the classical case, the cost of learning all coupons depends on the number of possible coupons. <LINK>']",http://arxiv.org/abs/2002.07688,"We study how efficiently a $k$-element set $S\subseteq[n]$ can be learned from a uniform superposition $|S\rangle$ of its elements. One can think of $|S\rangle=\sum_{i\in S}|i\rangle/\sqrt{|S|}$ as the quantum version of a uniformly random sample over $S$, as in the classical analysis of the ``coupon collector problem.'' We show that if $k$ is close to $n$, then we can learn $S$ using asymptotically fewer quantum samples than random samples. In particular, if there are $n-k=O(1)$ missing elements then $O(k)$ copies of $|S\rangle$ suffice, in contrast to the $\Theta(k\log k)$ random samples needed by a classical coupon collector. On the other hand, if $n-k=\Omega(k)$, then $\Omega(k\log k)$ quantum samples are~necessary. More generally, we give tight bounds on the number of quantum samples needed for every $k$ and $n$, and we give efficient quantum learning algorithms. We also give tight bounds in the model where we can additionally reflect through $|S\rangle$. Finally, we relate coupon collection to a known example separating proper and improper PAC learning that turns out to show no separation in the quantum case. ",Quantum Coupon Collector
67,1229956402354872320,795877354266456064,KoheiKamadaPhys,"['New paper appeared. \n<LINK>\nWe discuss the photon graviton conversion via BG magnetic fields in the early Universe. Th GWs generated in this mechanism can be relatively large, but their frequency is very high in an ordinary situation.']",https://arxiv.org/abs/2002.07548,"We explore a novel process in the early Universe in which thermalized photons are converted into gravitons in the presence of strong primordial magnetic fields. It is found that the frequency of generated gravitational waves (GWs) is typically of order of GHz and their amplitude can be up to $ \Omega_\mathrm{GW}h^2 \sim 10^{-10}$. If detected with future developments of the technology to explore this frequency region, the produced stochastic GW background enables us to know when and how strong the primordial magnetic fields are generated. From the peak frequency of the GWs, we can also probe the number of relativistic degrees of freedom at that time. ","Gravitational Waves from Primordial Magnetic Fields via Photon-Graviton
  Conversion"
68,1229770555911557121,75249390,Axel Maas,['I have published a new blog entry at <LINK> where I give a rundown of the main ideas of our recent paper <LINK> about the valence Higgs - a rather bold new idea on what is in the proton. And whether we can see it at the LHC.'],http://arxiv.org/abs/arXiv:2002.01688,"Non-perturbative gauge-invariance under the strong and the weak interactions dictates that the proton contains a non-vanishing valence contribution from the Higgs particle. By introducing an additional parton distribution function (PDF), we investigate the experimental consequences of this prediction. The Herwig 7 event generator and a parametrized CMS detector simulation are used to obtain predictions for a scenario amounting to the LHC Run II data set. We use those to assess the impact of the Higgs PDF on the pp->ttbar process in the single lepton final state. Comparing to nominal simulation we derive expected limits as a function of the shape of the valence Higgs PDF. We also investigate the process pp->ttZ at the parton level to add further constraints. ",Constraining the Higgs valence contribution in the proton
69,1229749898804944898,2472614119,Minas Karamanis,"['New ""black-box"" MCMC paper! with @fbeutler1 @UoPCosmology !! \n\narXiv: <LINK>  \ncode: <LINK>\ndocs: <LINK> <LINK>', '@fbeutler1 @UoPCosmology Like emcee @exoplaneteer , we employed an ensemble of walkers but instead of Metropolis updates our sampler performs Slice Sampling updates! https://t.co/fhFG6d9YX5', '@fbeutler1 @UoPCosmology @exoplaneteer The result is improved sampling efficiency, robustness, and the capacity sample even from very challenging target distributions (e.g. the notoriously difficult correlated funnel). https://t.co/hLRFwjKXVT', ""@fbeutler1 @UoPCosmology @exoplaneteer Our python implementation is called zeus and it's lightning fast... https://t.co/WyDPI3P3oU https://t.co/HkhR5DrMIC"", '@fbeutler1 @UoPCosmology @exoplaneteer We would love to hear your comments and questions about the paper and the code 😀']",https://arxiv.org/abs/2002.06212,"Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm that adapts to the characteristics of the target distribution with minimal hand-tuning. However, Slice Sampling's performance is highly sensitive to the user-specified initial length scale hyperparameter and the method generally struggles with poorly scaled or strongly correlated distributions. This paper introduces Ensemble Slice Sampling (ESS), a new class of algorithms that bypasses such difficulties by adaptively tuning the initial length scale and utilising an ensemble of parallel walkers in order to efficiently handle strong correlations between parameters. These affine-invariant algorithms are trivial to construct, require no hand-tuning, and can easily be implemented in parallel computing environments. Empirical tests show that Ensemble Slice Sampling can improve efficiency by more than an order of magnitude compared to conventional MCMC methods on a broad range of highly correlated target distributions. In cases of strongly multimodal target distributions, Ensemble Slice Sampling can sample efficiently even in high dimensions. We argue that the parallel, black-box and gradient-free nature of the method renders it ideal for use in scientific fields such as physics, astrophysics and cosmology which are dominated by a wide variety of computationally expensive and non-differentiable models. ","Ensemble Slice Sampling: Parallel, black-box and gradient-free inference
  for correlated & multimodal distributions"
70,1229749749470892034,2279045005,Samir Bhatt,"['New paper from @creswapi  and @flaxter  <LINK>\nWanted to write a little bit about why i like it and some caveats. Even since i started working in epi, everything had to be Bayesian. At the time it was for the uncertainty. As time went by 1/n', 'It became clear that marginalisation together with evaluating expectations was a more sensible way to do inference than just optimising point estimates. Also there were amazing software like JAGS, Stan and PyMC. However, at the time I was mainly using Gaussian processes 2/n', 'And the cubic cost with poor mixing made MCMC prohibitive. So I started using approximate Bayes approaches like the amazing INLA. This led me down a rabbit hole of trying to find the best approximate Bayesian approach for spatial mapping. 3/n', ""I've tried most things, flavours of Laplace and VB, MCML, SGLD and varieties, ensembles, bootstraps, MCdropout, and data splitting approaches. In most of these you can get good estimates, but you have to be extremely careful. And often, the point estimate is 4/n"", 'reasonable, but checks of uncertainty were wrong. And almost none could tick the box of coverage, and with the kind of health data I use, really few could be validated by state of the art SBC @dan_p_simpson 5/n', ""So what to do? You want something expressive that can be fitted using the uber MCMC of stan, so you can use principled workflows and validate your model. But then it's impossible to fit. So you resort to approximate methods and sacrifice a well calibrated model. 6/n"", ""So some time back @flaxter had a great idea, forget finding this perfect approximate inference algorithm and let's just try to get a model that be fitted in software that works like Stan. Let all the brilliant minds who designed Stan worry about the inference.7/n"", 'And this is what \\piVAE is. Its dead simple. Sample a ton of functions, embed these in some low dimensional probabilistic space using a VAE. Then use that for fitting directly in Stan. Not massively elegant, but It does seem to work very well. 8/n', ""And if the point estimates are competitive, and in one case state-of-the-art, and our Rhats and SBCs are good, what's the problem? Ok here are the caveats 9/n"", 'You have to pretrain the \\piVAE. And this can take a while, but you only have to do it once. We tried to test it on a range of problems but, ofcourse, we need to see if it works just as well on other applications. We need some more theory on it. 10/n', 'But for me, i can live with those caveats if i can get a posterior that is correct. The parameters of \\piVAE are uncorrelated and few in number and so MCMC is really fast once you have trained the model. Also you can encode properties of functions like integrals! 11/n', 'Looking forward to hearing what people think and hope there is some interest in the idea. Forgive any strong opinions in this thread too!']",https://arxiv.org/abs/2002.06873,"Stochastic processes provide a mathematically elegant way model complex data. In theory, they provide flexible priors over function classes that can encode a wide range of interesting assumptions. In practice, however, efficient inference by optimisation or marginalisation is difficult, a problem further exacerbated with big data and high dimensional input spaces. We propose a novel variational autoencoder (VAE) called the prior encoding variational autoencoder ($\pi$VAE). The $\pi$VAE is finitely exchangeable and Kolmogorov consistent, and thus is a continuous stochastic process. We use $\pi$VAE to learn low dimensional embeddings of function classes. We show that our framework can accurately learn expressive function classes such as Gaussian processes, but also properties of functions to enable statistical inference (such as the integral of a log Gaussian process). For popular tasks, such as spatial interpolation, $\pi$VAE achieves state-of-the-art performance both in terms of accuracy and computational efficiency. Perhaps most usefully, we demonstrate that the low dimensional independently distributed latent space representation learnt provides an elegant and scalable means of performing Bayesian inference for stochastic processes within probabilistic programming languages such as Stan. ","$\pi$VAE: Encoding stochastic process priors with variational
  autoencoders"
71,1229725123302875136,934426700,Anthony Steed,"['We have a new draft paper on a concept and demonstrator that we have worked on called ""Docking Haptics"", where you dynamically combine different robots together to generate hybrid haptic devices <LINK> #haptics <LINK>', 'The prototype combines one of our @DextaRobotics \nhand robots with a @haption Virtuose 6D. The two robots dynamically connect using an electromagnet, so you get the advantage of weight feedback when within range of the haption arm.']",http://arxiv.org/abs/2002.06093,"Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user's location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces. ","Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations
  of Grounded and Worn Devices"
72,1229676212521492480,168432023,Manu Tom,['How to use deep learning for lake ice detection from space?\n\nCheck out our new paper:\n<LINK>'],https://arxiv.org/abs/2002.07040,"Lake ice, as part of the Essential Climate Variable (ECV) lakes, is an important indicator to monitor climate change and global warming. The spatio-temporal extent of lake ice cover, along with the timings of key phenological events such as freeze-up and break-up, provide important cues about the local and global climate. We present a lake ice monitoring system based on the automatic analysis of Sentinel-1 Synthetic Aperture Radar (SAR) data with a deep neural network. In previous studies that used optical satellite imagery for lake ice monitoring, frequent cloud cover was a main limiting factor, which we overcome thanks to the ability of microwave sensors to penetrate clouds and observe the lakes regardless of the weather and illumination conditions. We cast ice detection as a two class (frozen, non-frozen) semantic segmentation problem and solve it using a state-of-the-art deep convolutional network (CNN). We report results on two winters ( 2016 - 17 and 2017 - 18 ) and three alpine lakes in Switzerland. The proposed model reaches mean Intersection-over-Union (mIoU) scores >90% on average, and >84% even for the most difficult lake. Additionally, we perform cross-validation tests and show that our algorithm generalises well across unseen lakes and winters. ",Lake Ice Detection from Sentinel-1 SAR with Deep Learning
73,1229669766035578880,223144852,Suchita Kulkarni,"['Paper day! <LINK>\n\nWhat if our searches for heavy Higgses fall short because we only look for its Standard Model decays? \n\nWorry not, if the Higgs decays to new particles, we can look for it at the LHC! High luminosity LHC will be great for this.', 'Albeit the paper considers only supersymmetric models, it is possible to generalise this to extended Higgs sectors containing additional BSM particles.', 'Upshot: consider Higgs production mechanisms due to b-quarks inside proton (bbH mode). Consider backgrounds from known SM processes but also model dependent BSM processes. If the Higgs decays to charged particles, it is worth considering long lived particle searches.', 'Look at kinematics of the final states, given that the heavy Higgs is a resonance, it can leave imprint on the kinematics. This usually results in kinematic end points, distinct visible and missing energy imbalances, and long-er lived particles due to boosts.', 'This was absolute fun to work with. Learned a lot! Thanks @OeAD_worldwide and @IndiaDST for generous funding to make this collaboration possible.', '@VM_Lozano Hope we cite you ;)\nYes, non-SM decays of heavy Higgs are not well studied so far. It has a lot of potential as I see it.', '@VM_Lozano Sounds great! I have thoughts :)']",http://arxiv.org/abs/2002.07137,"In this work, we analyse and demonstrate possible strategies to explore extended Higgs sector of the Minimal Supersymmetric Standard Model (MSSM). In particular we concentrate on heavy Higgs decays to electroweakinos. We analyse the Higgs to electroweakino decays in the allowed MSSM parameter space after taking into account 13 TeV LHC searches for supersymmetric particles and phenomenological constraints such as flavour physics, Higgs measurements and dark matter constraints. We explore some novel aspects of these Higgs decays. The final states resulting from Higgs to electroweakino decays will have backgrounds arising from the Standard Model as well as direct electroweakino production at the LHC. We demonstrate explicit kinematical differences between Higgs to electroweakino decays and associated backgrounds. Furthermore, we demonstrate for a few specific example points, optimised analysis search strategies at the high luminosity LHC (HL-LHC) run. Finally, we comment on possible search strategies for heavy Higgs decays to exotic final states, where the lightest chargino is long lived and leads to a disappearing track at the LHC. ",Searching for heavy Higgs in supersymmetric final states at the LHC
74,1229620661586235392,338922968,Jess Thorne,"['New Paper Time! My supervisor Aaron Robotham @ICRAR wrote a new spectral energy distribution fitting tool called ProSpect and the paper has just been put on the #astroph #arXiv!  see: <LINK> (with @CDPLagos @SabineBellstedt @astrowelshluke) <LINK>', 'ProSpect combines stellar models, dust models, and an AGN model to allow for the generation of spectral energy distributions and the recovery of star formation and metallicity histories of galaxies. Here is a graphic I made for the paper showing how we model galaxy components! https://t.co/HIXv3uTW7X', 'ProSpect allows the user to pick almost any reasonable star formation history to fit for including parametric and non-parametric options. Here are just a few of the options provided, but the user is also able to provide their own. https://t.co/uwsKeN4rgH', 'ProSpect also includes the ability to model an evolving metallicity using a number of different models. This allows for more thorough modeling of the evolution of galaxies! https://t.co/U6yfHguRPR', 'What is really cool is that we can take star formation histories from semi-analytic models such as SHARK (@CDPLagos) and calculate their spectral energy distributions, then re-fit them using ProSpect to test whether we can re-create star formation histories. The answer is yes! https://t.co/SJy1kSQ9Wb', ""Overall, ProSpect is able to extract similar stellar masses and star formation rates to those extracted from MAGPHYS! This means that we're not significantly biased in our recovery of stellar masses! https://t.co/WAV4RhKdNZ"", ""There's heaps of exciting science to be done with ProSpect which will form the basis of the next three years of my PhD so watch this space!"", '@astro_jje It is!']",https://arxiv.org/abs/2002.06980,"We introduce ProSpect, a generative galaxy spectral energy distribution (SED) package that encapsulates the best practices for SED methodologies in a number of astrophysical domains. ProSpect comes with two popular families of stellar population libraries (BC03 and EMILES), and a large variety of methods to construct star formation and metallicity histories. It models dust through the use of a Charlot & Fall attenuation model, with re-emission using Dale far-infrared templates. It also has the ability to model AGN through the inclusion of a simple AGN and hot torus model. Finally, it makes use of MAPPINGS-III photoionisation tables to produce line emission features. We test the generative and inversion utility of ProSpect through application to the Shark galaxy formation semi-analytic code, and informed by these results produce fits to the final ultraviolet to far-infrared photometric catalogues produces by the Galaxy and Mass Assembly Survey (GAMA). As part of the testing of ProSpect, we also produce a range of simple photometric stellar mass approximations covering a range of filters for both observed frame and rest frame photometry. ","ProSpect: Generating Spectral Energy Distributions with Complex Star
  Formation and Metallicity Histories"
75,1229601542430285825,383142451,Shinnosuke Takamichi (高道 慎之介),"['Our new paper ""Lifter Training and Sub-band Modeling for Computationally Efficient and High-Quality Voice Conversion Using Spectral Differentials"" accepted for #ICASSP is available in arXiv. A data-driven approach to estimate Hilbert-transform-based phase.\n<LINK> <LINK>']",https://arxiv.org/abs/2002.06778,"In this paper, we propose computationally efficient and high-quality methods for statistical voice conversion (VC) with direct waveform modification based on spectral differentials. The conventional method with a minimum-phase filter achieves high-quality conversion but requires heavy computation in filtering. This is because the minimum phase using a fixed lifter of the Hilbert transform often results in a long-tap filter. One of our methods is a data-driven method for lifter training. Since this method takes filter truncation into account in training, it can shorten the tap length of the filter while preserving conversion accuracy. Our other method is sub-band processing for extending the conventional method from narrow-band (16 kHz) to full-band (48 kHz) VC, which can convert a full-band waveform with higher converted-speech quality. Experimental results indicate that 1) the proposed lifter-training method for narrow-band VC can shorten the tap length to 1/16 without degrading the converted-speech quality and 2) the proposed sub-band-processing method for full-band VC can improve the converted-speech quality than the conventional method. ","Lifter Training and Sub-band Modeling for Computationally Efficient and
  High-Quality Voice Conversion Using Spectral Differentials"
76,1229573640817037312,1075649842955866114,Luca Cortese,"['New @ICRAR @UWAresearch @ARC_ASTRO3D paper investigating whether or not passive disks host unusually large reservoirs of cold atomic hydrogen, and highlighting the importance of getting star formation rate estimates right. #xGASS <LINK> <LINK>']",https://arxiv.org/abs/2002.05824v1,"We use the extended GALEX Arecibo SDSS Survey (xGASS) to quantify the relationship between atomic hydrogen (HI) reservoir and current star formation rate (SFR) for central disk galaxies. This is primarily motivated by recent claims for the existence, in this sample, of a large population of passive disks harbouring HI reservoirs as large as those observed in main sequence galaxies. Across the stellar mass range 10$^{9}<$M$_{*}$/M$_{\odot}<$10$^{11}$, we practically find no passive ($\gtrsim$2$\sigma$ below the star-forming main sequence) disk galaxies with HI reservoirs comparable to those typical of star-forming systems. Even including HI non detections at their upper limits, passive disks typically have $\geq$0.5 dex less HI than their active counterparts. We show that previous claims are due to the use of aperture-corrected SFR estimates from the MPA/JHU SDSS DR7 catalog, which do not provide a fair representation of the global SFR of HI-rich galaxies with extended star-forming disks. Our findings confirm that the bulk of the passive disk population in the local Universe is HI-poor. These also imply that the reduction of star formation, even in central disk galaxies, has to be accompanied by a reduction in their HI reservoir. ","] xGASS: passive disks do not host unexpectedly large reservoirs of cold
  atomic hydrogen"
77,1229503229773467648,2780460254,Dr Deanna C. Hooper,"[""Here's the new paper I put out today, where we look at Dark Matter - Dark Energy interactions as a possible solution to the Hubble tension (spoiler: these models don't seem to be able to solve the tension🙂) :\n<LINK>""]",https://arxiv.org/abs/2002.06127,"The emergence of an increasingly strong tension between the Hubble rate inferred from early- and late-time observations has reinvigorated interest in nonstandard scenarios, with the aim of reconciling these measurements. One such model involves interactions between Dark Matter and Dark Energy. Here we consider a specific form of the coupling between these two fluids proportional to the Dark Energy energy density, which has been studied extensively in the literature and claimed to substantially alleviate the Hubble tension. We complement the work already discussed in several previous analyses and show that, once all relevant cosmological probes are included simultaneously, the value of the Hubble parameter in this model is $H_0=69.82_{-0.76}^{+0.63}$ km/(s Mpc), which reduces the Hubble tension to $2.5\sigma$. Furthermore, we also perform a statistical model comparison, finding a $\Delta\chi^2$ of $-2.15$ (corresponding to a significance of 1.5$\sigma$) with the inclusion of one additional free parameter, showing no clear preference for this model with respect to $\Lambda$CDM, which is further confirmed with an analysis of the Bayes ratio. ","Tensions in the dark: shedding light on Dark Matter-Dark Energy
  interactions"
78,1229323333402906624,739505640326987777,Guido Roberts-Borsani,"['Another new paper out today, with @AmelieSaintonge, @KarenLMasters and Dave Stark! <LINK>\n\n“Outflows in Star-forming Galaxies: Stacking Analyses of Resolved Winds and the Relation to Their Hosts’ Properties”', '@AmelieSaintonge @KarenLMasters (1/4) Galactic-scale outflows form an integral component in the gas cycling of galaxies and thus the fuelling of their star formation. However, much about them - including their quenching potential - remains largely unconstrained, particularly in “normal”, star-forming galaxies.', '@AmelieSaintonge @KarenLMasters (2/4) One of the main reasons for this is the limited spatial coverage afforded by single spectra. However, with the advent of integral field spectroscopic observations over statistical samples of normal galaxies, this is beginning to change.', '@AmelieSaintonge @KarenLMasters (3/4) Here, we analyse neutral gas tracers in &gt;57,500 spectra from MaNGA DR15 IFU observations of 405 high mass galaxies with stacking techniques to determine the kpc prevalence and properties of outflows, inflows and their correlations to host properties.', '@AmelieSaintonge @KarenLMasters (4/4) We determine the main regulating props of outflows, the SF histories of hosts, and the kpc quenching potential of outflows. With GBT follow up observations, we compare the HI gas fractions of galaxies with and without outflows to assess the impact on neutral gas reservoirs.']",https://arxiv.org/abs/2002.05724v1,"Outflows form an integral component in regulating the gas cycling in and out of galaxies, although their impact on the galaxy hosts is still poorly understood. Here we present an analysis of 405 high mass (log M$_{*}$/M$_{\odot}\geqslant10$), star-forming galaxies (excluding AGN) with low inclinations at $z\sim$0, using stacking techniques of the NaD $\lambda\lambda$5889,5895 A neutral gas tracer in IFU observations from the MaNGA DR15 survey. We detect outflows in the central regions of 78/405 galaxies and determine their extent and power through the construction of stacked annuli. We find outflows are most powerful in central regions and extend out to $\sim$1R$_{e}$, with declining mass outflow rates and loading factors as a function of radius. The stacking of spaxels over key galaxy quantities reveals outflow detections in regions of high $\Sigma_{\text{SFR}}$ ($\gtrsim$0.01 M$_{\odot}$yr$^{-1}$kpc$^{-2}$) and $\Sigma_{M_{*}}$ ($\gtrsim$10$^{7}$ M$_{\odot}$kpc$^{-2}$) along the resolved main sequence. Clear correlations with $\Sigma_{\text{SFR}}$ suggest it is the main regulator of outflows, with a critical threshold of $\sim$0.01 M$_{\odot}$yr$^{-1}$kpc$^{-2}$ needed to escape the weight of the disk and launch them. Furthermore, measurements of the H$\delta$ and D$_{n}$4000 indices reveal virtually identical star formation histories between galaxies with outflows and those without. Finally, through stacking of HI 21 cm observations for a subset of our sample, we find outflow galaxies show reduced HI gas fractions at central velocities compared to their non-detection control counterparts, suggestive of some removal of HI gas, likely in the central regions of the galaxies, but not enough to completely quench the host. ","] Outflows in Star-forming Galaxies: Stacking Analyses of Resolved Winds
  and the Relation to Their Hosts' Properties"
79,1228374235153723392,989392279,Thomas Helfer,"['Finally, a new paper (<LINK>) out with my amazing collaborators @jc_aurrekoetxea and @tukohbin. We explored Cosmic Strings Loops collapsing to a Black Hole and studied their gravitational wave signal. <LINK>']",https://arxiv.org/abs/2002.05177,"We construct, for the first time, the time-domain gravitational wave strain waveform from the collapse of a strongly gravitating Abelian Higgs cosmic string loop in full general relativity. We show that the strain exhibits a large memory effect during merger, ending with a burst and the characteristic ringdown as a black hole is formed. Furthermore, we investigate the waveform and energy emitted as a function of string width, loop radius and string tension $G\mu$. We find that the mass normalized gravitational wave energy displays a strong dependence on the inverse of the string tension $E_{\mathrm{GW}}/M_0\propto 1/G\mu$, with $E_{\mathrm{GW}}/M_0 \sim {\cal O}(1)\%$ at the percent level, for the regime where $G\mu\gtrsim10^{-3}$. Conversely, we show that the efficiency is only weakly dependent on the initial string width and initial loop radii. Using these results, we argue that gravitational wave production is dominated by kinematical instead of geometrical considerations. ",Coherent Gravitational Waveforms and Memory from Cosmic String Loops
80,1228287058990501888,846422863,Carlos Sarraute ⚡️,"[""I'm happy to announce our new paper! 🚀\n\n💥WibsonTree: Efficiently Preserving Seller's Privacy in a Decentralized Data Marketplace\n\nDesigned by @AFutoransky from @Disarmista \ntogether with A. Waissbein, @mtravizano, D. Fernandez \n\nFull paper:\n<LINK> <LINK>""]",https://arxiv.org/abs/2002.03810,"We present a cryptographic primitive called WibsonTree designed to preserve users' privacy by allowing them to demonstrate predicates on their personal attributes, without revealing the values of those attributes. We suppose that there are three types of agents --buyers, sellers and notaries-- who interact in a decentralized privacy-preserving data marketplace (dPDM) such as the Wibson marketplace. We introduce the WibsonTree protocol as an efficient cryptographic primitive that enables the exchange of private information while preserving the seller's privacy. Using our primitive, a data seller can efficiently prove that he/she belongs to the target audience of a buyer's data request, without revealing any additional information. ","WibsonTree: Efficiently Preserving Seller's Privacy in a Decentralized
  Data Marketplace"
81,1228232663934201858,96779364,Arnab Bhattacharyya,"['New paper: <LINK> with Gayen, @ksmeel and Vinodchandran. We design efficient algorithms to estimate distance between high-dim distributions from samples. Our technique applies to many popular models for high-dim data.', ""Writing this up was very fun because there's just one idea which powers everything else. That idea: if you can learn the parameters of a distribution from samples, then for many natural distribution classes, you can also efficiently evaluate the probability mass function."", ""From samples and prob mass evaluations, @ccanonne_  and Ronitt Rubinfeld (https://t.co/3qid0fkeVR) already observed how to approximate the distance. There are some technicalities, like taking care of sampling errors and approximate evaluations...but basically that's it!"", 'One application to #Causalinference: given observations from two causal Bayes nets P and Q, we show how to efficienty compute the distance between them after an intervention do(X=x). This uses the main results of https://t.co/ESygJG9Wkl and work of @yudapearl  and Tian. #causalAI']",https://arxiv.org/abs/2002.05378,"We design efficient distance approximation algorithms for several classes of structured high-dimensional distributions. Specifically, we show algorithms for the following problems: - Given sample access to two Bayesian networks $P_1$ and $P_2$ over known directed acyclic graphs $G_1$ and $G_2$ having $n$ nodes and bounded in-degree, approximate $d_{tv}(P_1,P_2)$ to within additive error $\epsilon$ using $poly(n,\epsilon)$ samples and time - Given sample access to two ferromagnetic Ising models $P_1$ and $P_2$ on $n$ variables with bounded width, approximate $d_{tv}(P_1, P_2)$ to within additive error $\epsilon$ using $poly(n,\epsilon)$ samples and time - Given sample access to two $n$-dimensional Gaussians $P_1$ and $P_2$, approximate $d_{tv}(P_1, P_2)$ to within additive error $\epsilon$ using $poly(n,\epsilon)$ samples and time - Given access to observations from two causal models $P$ and $Q$ on $n$ variables that are defined over known causal graphs, approximate $d_{tv}(P_a, Q_a)$ to within additive error $\epsilon$ using $poly(n,\epsilon)$ samples, where $P_a$ and $Q_a$ are the interventional distributions obtained by the intervention $do(A=a)$ on $P$ and $Q$ respectively for a particular variable $A$. Our results are the first efficient distance approximation algorithms for these well-studied problems. They are derived using a simple and general connection to distribution learning algorithms. The distance approximation algorithms imply new efficient algorithms for {\em tolerant} testing of closeness of the above-mentioned structured high-dimensional distributions. ","Efficient Distance Approximation for Structured High-Dimensional
  Distributions via Learning"
82,1228217556932026369,1176867972163559424,MartinHuber,['Our new paper (joint with Lukas Laffers) on sensitivity analysis for #causalmechanisms (#causalmediation) under treatment/mediator endogeneity and outcome attrition using #linearprogramming:\n<LINK> \n#epitwitter #econtwitter #stattwitter #statstwitter #Rstats'],https://arxiv.org/abs/2002.05253,"Causal mediation analysis aims at disentangling a treatment effect into an indirect mechanism operating through an intermediate outcome or mediator, as well as the direct effect of the treatment on the outcome of interest. However, the evaluation of direct and indirect effects is frequently complicated by non-ignorable selection into the treatment and/or mediator, even after controlling for observables, as well as sample selection/outcome attrition. We propose a method for bounding direct and indirect effects in the presence of such complications using a method that is based on a sequence of linear programming problems. Considering inverse probability weighting by propensity scores, we compute the weights that would yield identification in the absence of complications and perturb them by an entropy parameter reflecting a specific amount of propensity score misspecification to set-identify the effects of interest. We apply our method to data from the National Longitudinal Survey of Youth 1979 to derive bounds on the explained and unexplained components of a gender wage gap decomposition that is likely prone to non-ignorable mediator selection and outcome attrition. ","Bounds on direct and indirect effects under treatment/mediator
  endogeneity and outcome attrition"
83,1228161267669463040,2956121356,Russ Salakhutdinov,"[""#ICLR2020 paper on Capsule Nets with Inverted Dot-Product Attention Routing: A new routing method where a child capsule is routed to a parent based on agreement b/w parent's state &amp; child's vote:\n\n<LINK>\n\nCode <LINK>\n\nw/t Tsai, @nitishsr, @Hanlin <LINK>""]",https://arxiv.org/abs/2002.04764,"We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it performs at-par with a powerful CNN (ResNet-18) with 4x fewer parameters. On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer. We believe that our work raises the possibility of applying capsule networks to complex real-world tasks. Our code is publicly available at: this https URL An alternative implementation is available at: this https URL ",Capsules with Inverted Dot-Product Attention Routing
84,1228142219049070592,746440524052082688,Nicolas Delfosse,['Would you believe me if I tell you that we can do sub-single-shot quantum error correction? It seems too good to be true.\n\nCheck out our new paper with my amazing collaborators Ben Reichardt and @krystasvore\n<LINK>\n\n#Microsoft #QuantumComputing'],https://arxiv.org/abs/2002.05180,"Extensive quantum error correction is necessary in order to perform a useful computation on a noisy quantum computer. Moreover, quantum error correction must be implemented based on imperfect parity check measurements that may return incorrect outcomes or inject additional faults into the qubits. To achieve fault-tolerant error correction, Shor proposed to repeat the sequence of parity check measurements until the same outcome is observed sufficiently many times. Then, one can use this information to perform error correction. A basic implementation of this fault tolerance strategy requires $\Omega(r d^2)$ parity check measurements for a distance-d code defined by r parity checks. For some specific highly structured quantum codes, Bombin has shown that single-shot fault-tolerant quantum error correction is possible using only r measurements. In this work, we demonstrate that fault-tolerant quantum error correction can be achieved using $O(d \log(d))$ measurements for any code with distance $d \geq \Omega(n^\alpha)$ for some constant $\alpha > 0$. Moreover, we prove the existence of a sub-single-shot fault-tolerant quantum error correction scheme using fewer than r measurements. In some cases, the number of parity check measurements required for fault-tolerant quantum error correction is exponentially smaller than the number of parity checks defining the code. ",Beyond single-shot fault-tolerant quantum error correction
85,1228139407280046082,928425413554135040,Xiaoyu Li,['What step sizes should we choose for non-convex SGD?  Exponential step sizes are good candidates! Check out our new paper: <LINK>  Joint work with @ZhenxunZhuang and @bremen79'],https://arxiv.org/abs/2002.05273,"Stochastic Gradient Descent (SGD) is a popular tool in training large-scale machine learning models. Its performance, however, is highly variable, depending crucially on the choice of the step sizes. Accordingly, a variety of strategies for tuning the step sizes have been proposed, ranging from coordinate-wise approaches (a.k.a. ``adaptive'' step sizes) to sophisticated heuristics to change the step size in each iteration. In this paper, we study two step size schedules whose power has been repeatedly confirmed in practice: the exponential and the cosine step sizes. For the first time, we provide theoretical support for them proving convergence rates for smooth non-convex functions, with and without the Polyak-\L{}ojasiewicz (PL) condition. Moreover, we show the surprising property that these two strategies are \emph{adaptive} to the noise level in the stochastic gradients of PL functions. That is, contrary to polynomial step sizes, they achieve almost optimal performance without needing to know the noise level nor tuning their hyperparameters based on it. Finally, we conduct a fair and comprehensive empirical evaluation of real-world datasets with deep learning architectures. Results show that, even if only requiring at most two hyperparameters to tune, these two strategies best or match the performance of various finely-tuned state-of-the-art strategies. ","A Second look at Exponential and Cosine Step Sizes: Simplicity,
  Adaptivity, and Performance"
86,1227956128174075905,2377407248,Daniel Whiteson,"['New paper out!  \n\nResonance Searches with Machine Learned Likelihood Ratios\n\n <LINK>', '@DanielWhiteson Builds on https://t.co/KS1KZYfkIv by @KyleCranmer and others.', '@KyleCranmer https://t.co/vKRpwPJ19v', ""@KyleCranmer Jacob did all the real work. He's a gem."", ""@OzAmram I'm sure its somewhere in between.  Better than making a cut would be to do a coarse binning in rapidity, and a finer binning would approach the optimal 2D.""]",https://arxiv.org/abs/2002.04699,"We demonstrate the power of machine-learned likelihood ratios for resonance searches in a benchmark model featuring a heavy Z' boson. The likelihood ratio is expressed as a function of multivariate detector level observables, but rather than being calculated explicitly as in matrix-element-based approaches, it is learned from a joint likelihood ratio which depends on latent information from simulated samples. We show that bounds drawn using the machine learned likelihood ratio are tighter than those drawn using a likelihood ratio calculated from histograms. ",Resonance Searches with Machine Learned Likelihood Ratios
87,1227954759459647489,223440240,Nathan Kallus,"['Optimizing efficient policy value estimates does *not* imply efficient learning of policy parameters! In a new paper (<LINK>) we consider what would actually be efficient for the common reduction of policy learning to weighted (cost-sensitive) classification 1/n', 'Turns out assuming the policy class is well specified for a convex surrogate loss relaxation of the classification reduction only implies a semiparam model (contrast: in true classification it implies a param model). So: empirical risk minimization is actually *not* efficient 2/n', 'Instead we use a conditional moment formulation, to which we can apply efficient methods. In particular, we show (theoretically+empirically) how a DeepGMM-based approach (https://t.co/7SA5R1H3eV) yields significant improvements *both* in policy regret and policy parameter MSE n/n']",https://arxiv.org/abs/2002.05153,"Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification reduction of policy learning with any score function, either direct, inverse-propensity weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semiparametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically. ",Efficient Policy Learning from Surrogate-Loss Classification Reductions
88,1227792851213393921,223440240,Nathan Kallus,"['Masa and I just posted a new paper on *efficient* off-policy policy gradients: <LINK>. We establish a lower bound on how well one can estimate policy gradients and develop an algo that achieves this bound &amp; exhibits 3-way double robustness. ☘️ 1/n <LINK> <LINK>', 'We show (theoretically and empirically) how gradient ascent using our new off-policy policy gradients translates to better off-policy learning that can overcome the curse of horizon. This is crucial for reliably applying off-policy RL in practice. ⛑️ 2/n', 'Connections to Double Reinforcement Learning: While our previous work (https://t.co/4CFbV6r75R, https://t.co/LpSXwyTFxL) shows how structure like Makovianness can significantly improve off-policy eval, this new work shows how this translates to off-policy *learning* 🧐 n/n', ""And fyi, for those interested in policy gradients and in case you haven't seen it yet, @lilianweng has a super helpful and succinct reference on the zoo of policy gradient algos: https://t.co/bjP7wQc8g6 🐒🦁🐍🐯""]",https://arxiv.org/abs/2002.04014,"Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. In this paper, we consider the statistically efficient estimation of policy gradients from off-policy data, where the estimation is particularly non-trivial. We derive the asymptotic lower bound on the feasible mean-squared error in both Markov and non-Markov decision processes and show that existing estimators fail to achieve it in general settings. We propose a meta-algorithm that achieves the lower bound without any parametric assumptions and exhibits a unique 3-way double robustness property. We discuss how to estimate nuisances that the algorithm relies on. Finally, we establish guarantees on the rate at which we approach a stationary point when we take steps in the direction of our new estimated policy gradient. ",Statistically Efficient Off-Policy Policy Gradients
89,1227611938689077250,14551614,Jason Weston,"['Dream: a setting to study (RL) agents that can _speak_ and act, grounded in a rich, diverse world, interacting with other agents. Open-domain and goal-oriented.\n\nReality: you can do this in LIGHT! New paper: <LINK> <LINK>', '@parlai_parley @em_dinan @MargaretMLi @shrimai_ @JackUrbs Paper details:\nI love your chain mail! Making knights smile in a fantasy game world: Open-domain goal-oriented dialogue agents\n@shrimai_ @MargaretMLi @JackUrbs @em_dinan @douwekiela @jaseweston Arthur Szlam\n\nhttps://t.co/3nyMQVp4W6']",https://arxiv.org/abs/2002.02878,"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. ","I love your chain mail! Making knights smile in a fantasy game world:
  Open-domain goal-oriented dialogue agents"
90,1227573049488052225,96779364,Arnab Bhattacharyya,"['New paper out! ""Efficiently learning and sampling interventional distributions from observations"" with Gayen, @Saravanan_CU, Maran and Vinodchandran: <LINK> #causalML #causalinference', ""Causal inference is about predicting what happens in an imagined world that you don't have access to. E.g.: Am I more likely to get better if I take the medicine versus if I don't? How is the sale for product X affected if ads for X are slashed by 20%?"", '@yudapearl and collaborators have thought long and hard about such questions. They realized that to properly formulate causal problems, one needs a model to describe how variables causally depend on each other. What they proposed is using Bayes nets to encode causal info.', 'A basic question in this setup: given a Bayes net P on a set of variables, infer how the distribution would change if a particular variable is externally set (""intervened"") to a fixed value. @yudapearl &amp; Jin Tian characterized the class of graphs for which this task is possible.', 'What we do in our paper is make their result algorithmic. We show conditions (nearly tight) under which there are efficient algorithms (both in terms of samples &amp; time) to infer the interventional distribution using samples from the observational distribution. TCS meets CI!', 'Some thoughts from behind-the-curtain: reasoning about interventions is really very, very slippery! In particular, efficiently generating samples from the interventional distribution was unexpectedly quite tricky to do.', 'Open problems! Efficient non-parametric algorithms for inferring interventions on several variables, estimating individual causal effects, estimating transportability error, etc. Can we do better if we add realistic parametric assumptions? Efficiently mitigating selection bias?']",https://arxiv.org/abs/2002.04232,"We study the problem of efficiently estimating the effect of an intervention on a single variable (atomic interventions) using observational samples in a causal Bayesian network. Our goal is to give algorithms that are efficient in both time and sample complexity in a non-parametric setting. Tian and Pearl (AAAI `02) have exactly characterized the class of causal graphs for which causal effects of atomic interventions can be identified from observational data. We make their result quantitative. Suppose P is a causal model on a set $\vec{V}$ of n observable variables with respect to a given causal graph G with observable distribution $P$. Let $P_x$ denote the interventional distribution over the observables with respect to an intervention of a designated variable X with x. Assuming that $G$ has bounded in-degree, bounded c-components ($k$), and that the observational distribution is identifiable and satisfies certain strong positivity condition, we give an algorithm that takes $m=\tilde{O}(n\epsilon^{-2})$ samples from $P$ and $O(mn)$ time, and outputs with high probability a description of a distribution $\hat{P}$ such that $d_{\mathrm{TV}}(P_x, \hat{P}) \leq \epsilon$, and: 1. [Evaluation] the description can return in $O(n)$ time the probability $\hat{P}(\vec{v})$ for any assignment $\vec{v}$ to $\vec{V}$ 2. [Generation] the description can return an iid sample from $\hat{P}$ in $O(n)$ time. We also show lower bounds for the sample complexity showing that our sample complexity has an optimal dependence on the parameters $n$ and $\epsilon$, as well as if $k=1$ on the strong positivity parameter. ",Learning and Sampling of Atomic Interventions from Observations
91,1227512201579257857,1196494921,Steve McCormick,"[""Woo!! Slipped one through with the ol' cross-list switcharoo: <LINK>\n\n(New paper on arXiv today, with Po-Ning Chen: Quasi-local Penrose inequalities with electric charge) <LINK>"", ""@gregeganSF I'm so glad I'm not the only one 😂\n\nAnd all dates are (yy)yymmdd in Sweden, so I should really be used to it by now..."", '@CreeepyJoe Oh yeh, I forgot there was a big UCR contingency here on mathtwitter! :)']",https://arxiv.org/abs/2002.04557,"The Riemannian Penrose inequality is a remarkable geometric inequality between the ADM mass of an asymptotically flat manifold with non-negative scalar curvature and the area of its outermost minimal surface. A version of the Riemannian Penrose inequality has also been established for the Einstein-Maxwell equations, where the lower bound on the mass also depends on the electric charge. In the context of quasi-local mass, one is interested in determining if, and for which quasi-local mass definitions, a quasi-local version of these inequalities also holds. It is known that the Brown-York quasi-local mass satisfies a quasi-local Riemannian Penrose inequality, however in the context of the Einstein-Maxwell equations, one expects that a quasi-local Riemannian Penrose inequality should also include a contribution from the electric charge. This article builds on ideas of Lu and Miao and of the first-named author to prove some charged quasi-local Penrose inequalities for a class of compact manifolds with boundary. In particular, we impose that the boundary is isometric to a closed surface in a suitable Reissner-Nordstr\""om manifold, which serves as a reference manifold for the quasi-local mass that we work with. In the case where the reference manifold has zero mass and non-zero electric charge, the lower bound on quasi-local mass is exactly the lower bound on the ADM mass given by the charged Riemannian Penrose inequality. ",Quasi-local Penrose inequalities with electric charge
92,1227500730820415491,2233489694,Theofanis Karaletsos,"['1/4: More stuff! New pre-print on priors for Bayesian Neural Networks with Gaussian Processes. Along with Thang Bui, we extend my previous paper on unit embeddings for graph-inspired BNN priors with kernel machinery to build hierarchical GP weight priors  <LINK>', ""2/4:We also develop an input-dependent version of this model using product kernels, which produces context-dependent priors. If you ever wanted to imbue BNNs with inductive biases as kernels would allow you for GPs, now you can do it by 'cooking them into the prior'."", '3/4: In practice, this allows us to model uncertainty better than N(0,1) priors would allow, and to interpolate and extrapolate better (i.e. for timeseries). Unlike GPs, our BNN can generalize beyond the inductive biases of the input-kernel in the weight posterior. https://t.co/D4EZ2JZpxF', '4/4: You can think of this casually speaking as a highly structured GP-LVM hypernetwork over weights, while also  modeling input-dependence gracefully. \nAlso: it works! Check out the predictive entropy.\nWe also presented an earlier version of this paper at AABI 2019. https://t.co/ReTmJgs5Qq', '5/4 : In addition, this allows us to obtain strong uncertainty estimates on tasks compared to deep kernel learning or N(0,1) neural networks. We test this in a suite of out of distribution experiments and consistently get results where our model says ""I don\'t know"" when it should https://t.co/qSrzuG3FCp', '6/fin: And on a separate note: it is intellectually quite satisfactory having an ML background from Tübingen to propose a principled Bayesian merger of BNNs and Kernel methods that inherits promising aspects of both worlds, as both perspectives bring utility to this model.']",https://arxiv.org/abs/2002.04033,"Probabilistic neural networks are typically modeled with independent weight priors, which do not capture weight correlations in the prior and do not provide a parsimonious interface to express properties in function space. A desirable class of priors would represent weights compactly, capture correlations between weights, facilitate calibrated reasoning about uncertainty, and allow inclusion of prior knowledge about the function space such as periodicity or dependence on contexts such as inputs. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for network weights based on unit embeddings that can flexibly encode correlated weight structures, and (ii) input-dependent versions of these weight priors that can provide convenient ways to regularize the function space through the use of kernels defined on contextual inputs. We show these models provide desirable test-time uncertainty estimates on out-of-distribution data, demonstrate cases of modeling inductive biases for neural networks with kernels which help both interpolation and extrapolation from training data, and demonstrate competitive predictive performance on an active learning benchmark. ",Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights
93,1227284117152129024,739505640326987777,Guido Roberts-Borsani,"['New paper out today! <LINK>\n\n“Interpreting the Spitzer/IRAC Colours of 7&lt;z&lt;9 Galaxies: Distinguishing Between Line Emission and Starlight Using ALMA”', '(1/4) When looking at the Spitzer/IRAC colours of bright, high redshift (z&gt;7) galaxies, quite a few show a red [3.6]-[4.5] micron colour (a so-called “IRAC excess”), typically attributed to the presence of strong nebular emission lines which contaminate the photometric bands.', '(2/4) However, in some cases (as seen in galaxies at z&gt;9), the colour can actually be due to starlight from older stars (i.e., a Balmer break). In this work, we show that at redshifts z=7-9, the colours resulting from these two interpretations are degenerate (until JWST).', '(3/4) Fortunately, we also find that [OIII] 88 micron and continuum constraints from ALMA can go a long way toward breaking this degeneracy and we thus model the SEDs of all z&gt;7 galaxies displaying a red IRAC colour and where such constraints are available.', '(4/4) A Balmer break model is preferred for 3/4 sources and we discuss the implications of our findings for the mass build up of galaxies in the early Universe and the timing of Cosmic Dawn.']",http://arxiv.org/abs/2002.02968,"Prior to the launch of JWST, Spitzer/IRAC photometry offers the only means of studying the rest-frame optical properties of z>7 galaxies. Many such high redshift galaxies display a red [3.6] - [4.5] micron colour, often referred to as the ""IRAC excess"", which has conventionally been interpreted as arising from intense [OIII]+Hbeta emission within the [4.5] micron bandpass. An appealing aspect of this interpretation is similarly intense line emission seen in star-forming galaxies at lower redshift as well as the redshift-dependent behaviour of the IRAC colours beyond z~7 modelled as the various nebular lines move through the two bandpasses. In this paper we demonstrate that, given the photometric uncertainties, established stellar populations with Balmer (4000 A, rest-frame) breaks, such as those inferred at z>9 where line emission does not contaminate the IRAC bands, can equally well explain the redshift-dependent behaviour of the IRAC colours in 7<z<9 galaxies. We discuss possible ways of distinguishing between the two hypotheses using ALMA measures of [OIII] 88 micron and dust continuum fluxes. Prior to further studies with JWST, we show that the distinction is important in determining the assembly history of galaxies in the first 500 Myr. ","Interpreting the Spitzer/IRAC Colours of 7&lt;z&lt;9 Galaxies: Distinguishing
  Between Line Emission and Starlight Using ALMA"
94,1227264424655781888,1218553574138888192,Brooks Tellekamp,"[""New paper is online in @CGD_ACS and on @arxiv. Successful epitaxial integration of II-IV-N's with GaN is a major step towards hybrid optoelectronic devices! <LINK> <LINK>""]",https://arxiv.org/abs/2002.03980,"Recently theorized hybrid II-IV-N{_2} / III-N heterostructures, based on current commercialized (In,Ga)N devices, are predicted to significantly advance the design space of highly efficient optoelectronics in the visible spectrum, yet there are few epitaxial studies of II-IV-N{_2} materials. In this work, we present heteroepitaxial ZnGeN{_2} grown on GaN buffers and AlN templates. We demonstrate that a GaN nucleating surface is crucial for increasing the ZnGeN{_2} crystallization rate to combat Zn desorption, extending the stoichiometric growth window from 215 {\degree}C on AlN to 500 {\degree}C on GaN buffers. Structural characterization reveals well crystallized films with threading dislocations extending from the GaN buffer. These films have a critical thickness for relaxation of 20 nm - 25 nm as determined by reflection high energy electron diffraction (RHEED) and cross-sectional scanning electron microscopy (SEM). The films exhibit a cation-disordered wurtzite structure, with lattice constants a = 3.216 {\AA} {\pm} 0.004 {\AA} and c = 5.215 {\AA} {\pm} 0.005 {\AA} determined by RHEED and X-ray diffraction (XRD). This work demonstrates a significant step towards the development of hybrid ZnGeN{_2}-GaN integrated devices. ","Heteroepitaxial integration of ZnGeN2 on GaN buffers using molecular
  beam epitaxy"
95,1227203031571337216,1029786016742289410,Ruth Keogh,"['New paper on ""Simulating longitudinal data from marginal structural models using the additive hazard model"" with @jomgran, @SVansteelandt <LINK>  Aim is to help people evaluate causal inference methods via simulation. Simulation algorithm provided (+R code).']",https://arxiv.org/abs/2002.03678,"Observational longitudinal data on treatments and covariates are increasingly used to investigate treatment effects, but are often subject to time-dependent confounding. Marginal structural models (MSMs), estimated using inverse probability of treatment weighting or the g-formula, are popular for handling this problem. With increasing development of advanced causal inference methods, it is important to be able to assess their performance in different scenarios to guide their application. Simulation studies are a key tool for this, but their use to evaluate causal inference methods has been limited. This paper focuses on the use of simulations for evaluations involving MSMs in studies with a time-to-event outcome. In a simulation, it is important to be able to generate the data in such a way that the correct form of any models to be fitted to those data is known. However, this is not straightforward in the longitudinal setting because it is natural for data to be generated in a sequential conditional manner, whereas MSMs involve fitting marginal rather than conditional hazard models. We provide general results that enable the form of the correctly-specified MSM to be derived based on a conditional data generating procedure, and show how the results can be applied when the conditional hazard model is an Aalen additive hazard or Cox model. Using conditional additive hazard models is advantageous because they imply additive MSMs that can be fitted using standard software. We describe and illustrate a simulation algorithm. Our results will help researchers to effectively evaluate causal inference methods via simulation. ","Simulating longitudinal data from marginal structural models using the
  additive hazard model"
96,1227177397755224064,805477439648440321,Rob Kavanagh,['Our new paper is up on arXiv today! We show how stellar wind and planetary orbital properties affect the radio eclipse of planets by the winds of their host stars:\n\n<LINK> <LINK>'],https://arxiv.org/abs/2002.03901,"The search for exoplanetary radio emission has resulted in zero conclusive detections to date. Various explanations for this have been proposed, from the observed frequency range, telescope sensitivity, to beaming of the emission. In a recent paper, we illustrated that exoplanets can orbit through the radio photosphere of the wind of the host star, a region that is optically thick at a specific frequency, for a large fraction of their orbits. As a result, radio emission originating from the planet could be absorbed or `eclipsed' by the wind of the host star. Here we investigate how the properties of the stellar wind and orbital parameters affect the fraction of the orbit where the planet is eclipsed by the stellar wind. We show that planets orbiting stars with low density winds are more favourable for detection in the radio. In terms of the orbital parameters, emission from transiting planets can escape the stellar wind easiest. We apply our model to the $\tau$~Boo planetary system, and show that observing the fraction of the planet's orbit where it is eclipsed by the wind of the host star could be used to constrain the properties of the stellar wind. However, our model developed would need to be used in conjunction with a separate method to disentangle the mass-loss rate and temperature of the stellar wind. ",Radio eclipses of exoplanets by the winds of their host stars
97,1227165513593348096,3088703037,Stefano Zapperi,['Our new paper is the on the @arxiv : Automatic Design of Mechanical Metamaterial Actuators <LINK> <LINK>'],https://arxiv.org/abs/2002.03032,"Mechanical metamaterials actuators achieve pre-determined input--output operations exploiting architectural features encoded within a single 3D printed element, thus removing the need of assembling different structural components. Despite the rapid progress in the field, there is still a need for efficient strategies to optimize metamaterial design for a variety of functions. We present a computational method for the automatic design of mechanical metamaterial actuators that combines a reinforced Monte Carlo method with discrete element simulations. 3D printing of selected mechanical metamaterial actuators shows that the machine-generated structures can reach high efficiency, exceeding human-designed structures. We also show that it is possible to design efficient actuators by training a deep neural network, eliminating the need for lengthy mechanical simulations. The elementary actuators devised here can be combined to produce metamaterial machines of arbitrary complexity for countless engineering applications. ",Automatic Design of Mechanical Metamaterial Actuators
98,1227138684090245120,449758465,Clément Sayrin,['Live from @lkb_lab: We have a new paper on the @arxiv!\n\nYou can read about circular Rydberg levels from laser-cooled atoms in a cryostat and everything about them here\n<LINK>\n\n#RydbergAtom #QuantumPhysics <LINK>'],https://arxiv.org/abs/2002.02893,"The exquisite properties of Rydberg levels make them particularly appealing for emerging quantum technologies. The lifetime of low-angular-momentum laser-accessible levels is however limited to a few $100\,\mu\mathrm{s}$ by optical transitions and microwave blackbody radiation (BBR) induced transfers at room temperature. A considerable improvement would be obtained with the few $10\,\mathrm{ms}$ lifetime of circular Rydberg levels in a cryogenic environment reducing the BBR temperature. We demonstrate the preparation of long-lived circular Rydberg levels of laser-cooled Rubidium atoms in a cryostat. We observe a $3.7\,\mathrm{ms}$ lifetime for the circular level of principal quantum number $n=52$. By monitoring the transfers between adjacent circular levels, we estimate in situ the microwave BBR temperature to be $(11\pm 2)\,\mathrm{K}$. The measured atomic coherence time ($270\,\mu\mathrm{s}$) is limited here only by technical magnetic field fluctuations. This work opens interesting perspectives for quantum simulation and sensing with cold circular Rydberg atoms. ","Long-lived circular Rydberg states of laser-cooled Rubidium atoms in a
  cryostat"
99,1226947903391969282,4463847508,Dr. Dreia Carrillo,"[""New paper day!!! (a thread)\n\nMy work on the stellar populations and assembly of NGC 2903's bulge, bar, and disk is OUT: <LINK>\n\nHere are the main takeaways (1/n)"", 'I used some SUPERB integral field unit data called VENGA that spatially resolves the nearby galaxy NGC 2903 at a resolution of 185 pc per spaxel (the ""pixel"" in IFU world). I have &gt;6000 spaxels i.e.  &gt;6000 different spectra in different parts of this galaxy! (2/n)', 'But how did you determine what part of the galaxy a spaxel belongs to? you ask. WELL, I did bulge-bar-disk light decomposition and I said ""If &gt;50% of the light in that spaxel comes from the disk component, then I assign that to the disk"" (3/n) https://t.co/DH3NvW0B09', ""There's not one best way to do this because different parts of the galaxy overlap, especially if you have an inclined galaxy. But that's as much as I can do folks! (4/n)"", 'Then to get the stellar population ages and metallicities, we did full-spectrum fitting where we determined the best combination of single stellar populations (with certain ages, metallicities, alpha-enhancement) that gives the integrated light that we observe (5/n)', ""In the end, we get a proxy for star formation history (we looked at the mass growth), knowing how many stellar populations of a certain age exists. Here's the mass growth plot for the whole galaxy showing that a lot of its stars formed &lt;5 Gyr ago (6/n) https://t.co/387zVCitjd"", 'We did this for all the other galactic components of NGC 2903! For example, here is one for its bulge, which shows that most of the stars in the bulge formed earlier on (7/n) https://t.co/HKK23Gkhic', 'From these (and the other plots in my paper), we have concluded that NGC 2903, a galaxy similar to our own in star formation rate and also has nice spiral arms has mostly grown through internal processes (""secular evolution"") in the recent past (8/n)', ""There's more stuff to the project and I've just outlined some of the main conclusions. Do check it out!!! https://t.co/bjf1xJ3mAM""]",https://arxiv.org/abs/2002.02858,"We study the stellar populations and assembly of the nearby spiral galaxy NGC 2903's bulge, bar, and outer disc using the VIRUS-P Exploration of Nearby Galaxies IFS survey. We observe NGC 2903 with a spatial resolution of 185 pc using the Mitchell Spectrograph's 4.25 arcsec fibres at the 2.7 Harlan J. Smith telescope. Bulge-bar-disc decomposition on the 2MASS Ks-band image of NGC 2903 shows that it has ~6%, 6%, and 88%, of its stellar mass in the bulge, bar, and outer disc, respectively, and its bulge has a low Sersic index of ~0.27, suggestive of a disky bulge. We perform stellar population synthesis and find that the outer disc has 46% of its mass in stars >5 Gyr, 48% in stars between 1 and 5 Gyr, and <10% in younger stars. Its stellar bar has 65% of its mass in ages 1-5 Gyr and has metallicities similar to the outer disc, suggestive of the evolutionary picture where the bar forms from disc material. Its bulge is mainly composed of old high-metallicity stars though it also has a small fraction of young stars. We find enhanced metallicity in the spiral arms and central region, tracing areas of high star formation as seen in the Halpha map. These results are consistent with the idea that galaxies of low bulge-to-total mass ratio and low bulge Sersic index like NGC 2903 has not had a recent major merger event, but has instead grown mostly through minor mergers and secular processes. ","The VIRUS-P Exploration of Nearby Galaxies (VENGA): The stellar
  populations and assembly of NGC 2903's bulge, bar, and outer disc"
100,1226840097351446528,790183810973458432,Jakub Tomczak,"['A great collaboration between @SILS_UvA (@TomczakWeglarz) and @CIGroupVU (@gusz_e and me) on developing new differential evolution! With applications to function optimization, systems biology and neural networks. Paper: <LINK> Code: <LINK> <LINK>']",https://arxiv.org/abs/2002.02869,"Differential evolution (DE) is a well-known type of evolutionary algorithms (EA). Similarly to other EA variants it can suffer from small populations and loose diversity too quickly. This paper presents a new approach to mitigate this issue: We propose to generate new candidate solutions by utilizing reversible linear transformation applied to a triplet of solutions from the population. In other words, the population is enlarged by using newly generated individuals without evaluating their fitness. We assess our methods on three problems: (i) benchmark function optimization, (ii) discovering parameter values of the gene repressilator system, (iii) learning neural networks. The empirical results indicate that the proposed approach outperforms vanilla DE and a version of DE with applying differential mutation three times on all testbeds. ",Differential Evolution with Reversible Linear Transformations
101,1226687839976296449,759118366468481024,Decker French,"[""And because I'm terrible at conference tweeting, now for something (mostly) unrelated, I have a new paper out on TDE host galaxies today. <LINK>""]",https://arxiv.org/abs/2002.02498,"We explore the galaxy structure of four tidal disruption event (TDE) host galaxies on 30 pc to kpc scales using HST WFC3 multi-band imaging. The star formation histories of these hosts are diverse, including one post-starburst galaxy (ASASSN-14li), two hosts with recent weak starbursts (ASASSN-14ae and iPTF15af), and one early type (PTF09ge). Compared to early type galaxies of similar stellar masses, the TDE hosts have higher central surface brightnesses and stellar mass surface densities on 30-100 pc scales. The TDE hosts do not show the large, kpc-scale tidal disruptions seen in some post-starburst galaxies; the hosts have low morphological asymmetries similar to those of early type galaxies. The lack of strong asymmetries are inconsistent with a recent major (~1:1 mass) merger, although minor ($\lesssim$1:3) mergers are possible. Given the time elapsed since the end of the starbursts in the three post-burst TDE hosts and the constraints on the merger mass ratios, it is unlikely that a bound supermassive black hole binary (SMBHB) has had time to coalesce. The TDE hosts have low central (<140 pc) ellipticities compared to early type galaxies. The low central ellipticities disfavor a strong radial anisotropy as the cause for the enhanced TDE rate, although we cannot rule out eccentric disks at the scale of the black hole gravitational radius of influence (~1 pc). These observations suggest that the high central stellar densities are a more important driver than SMBHBs or radial anisotropies in increasing the TDE rate in galaxies with recent starbursts. ","The Structure of Tidal Disruption Event Host Galaxies on Scales of Tens
  to Thousands of Parsecs"
102,1226542326157213696,2790700645,Edgar Dobriban,"['New paper on iterative sketching with Jonathan Lacotte, Sifan (Sophia) Liu (@liu_sifan), and Mert Pilanci (all from Stanford): <LINK>.\n\nGoes beyond our previous works because it analyzes iterative methods instead of just ""sketch and solve"" methods.']",https://arxiv.org/abs/2002.00864,"Random projections or sketching are widely used in many algorithmic and learning contexts. Here we study the performance of iterative Hessian sketch for least-squares problems. By leveraging and extending recent results from random matrix theory on the limiting spectrum of matrices randomly projected with the subsampled randomized Hadamard transform, and truncated Haar matrices, we can study and compare the resulting algorithms to a level of precision that has not been possible before. Our technical contributions include a novel formula for the second moment of the inverse of projected matrices. We also find simple closed-form expressions for asymptotically optimal step-sizes and convergence rates. These show that the convergence rate for Haar and randomized Hadamard matrices are identical, and asymptotically improve upon Gaussian random projections. These techniques may be applied to other algorithms that employ randomized dimension reduction. ","Optimal Iterative Sketching with the Subsampled Randomized Hadamard
  Transform"
103,1225839045088927745,2180565864,Robert Fisher,"['Two of the most fundamental processes in astrophysics are nuclear burning and turbulence. What happens when both are combined? This is the topic of a new paper out on the arXiv today which I wrote with @TechnionLive PhD student Yossef Zenati. <LINK>', 'We were inspired by work w/ my former MS students Gabriel Casabona (@funky_physics, now  @doecsgf fellow @NUCIERA) and Pritom Mozumdar (now a PhD student at @ucdavis). There we showed small temperature fluctuations lead to large fluctuations in the nuclear burning rate. https://t.co/XesIAJxeJ1', 'With Yossef, we demonstrated that the turbulent nuclear burning enhancement has a universal scaling law in homogeneous isotropic turbulence. And even more interestingly, we showed we could accurately calculate the enhancement even for relatively strong turbulence. https://t.co/sv2hP342q9', 'It was really fascinating to see the hidden order underlying the complex physics of turbulent nuclear burning. Our findings will help elucidate the problem of detonation initiation in thermonuclear supernovae, and may have potential applications in other stellar transients.']",https://arxiv.org/abs/2002.01937,"Nuclear burning plays a key role in a wide range of astrophysical stellar transients, including thermonuclear, pair instability, and core collapse supernovae, as well as kilonovae and collapsars. Turbulence is now understood to also play a key role in these astrophysical transients. Here we demonstrate that turbulent nuclear burning may lead to large enhancements above the uniform background burning rate, since turbulent dissipation gives rise to temperature fluctuations, and in general the nuclear burning rates are highly sensitive to temperature. We derive results for the turbulent enhancement of the nuclear burning rate under the influence of strong turbulence in the distributed burning regime in homogeneous isotropic turbulence, using probability distribution function (PDF) methods. We demonstrate that the turbulent enhancement obeys a universal scaling law in the limit of weak turbulence. We further demonstrate that, for a wide range of key nuclear reactions, such as C$^{12}$(O$^{16}$, $\alpha$)Mg$^{24}$ and triple-$\alpha$, even relatively modest temperature fluctuations, of the order ten percent, can lead to enhancements of 1 - 3 orders of magnitude in the turbulent nuclear burning rate. We verify the predicted turbulent enhancement directly against numerical simulations, and find very good agreement. We also present an estimation for the onset of turbulent detonation initiation, and discuss implications of our results for the modeling of stellar transients. ","Universality and Non-Universality in Distributed Nuclear Burning in
  Homogeneous Isotropic Turbulence"
104,1225816800107139073,1140025148004810752,Pierre Arthuis,['🗞 New paper from us on arXiv today! 🗞\n\nWe looked at charge radii and densities for nuclei at the limit of the ab initio domain and reproduced experimental cross-section from the SCRIT experiment in RIKEN.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2002.02214,"We present the first ab initio calculations for open-shell nuclei past the tin isotopic line, focusing on Xe isotopes as well as doubly-magic Sn isotopes. We show that, even for moderately hard interactions, it is possible to obtain meaningful predictions and that the NNLOsat chiral interaction predicts radii and charge density distributions close to the experiment. We then make a new prediction for ${}^{100}$Sn. This paves the way for ab initio studies of exotic charge density distributions at the limit of the present ab initio mass domain, where experimental data is becoming available. The present study closes the gap between the largest isotopes reachable by ab initio methods and the smallest exotic nuclei accessible to electron scattering experiments. ",Ab initio computation of charge densities for Sn and Xe isotopes
105,1225803379957522432,1211580702480748544,Andrea Idini,"[""New Paper is out:\n<LINK>\n\nThe idea is to use the Levy-Lieb formulation of DFT, and use Self Consistent Green's function calculations from first principle to derive a possible first principle functional.\n\n*Spoiler: doesn't work.\n\nWe know from Hohenberg-Kohn...\n1/6"", '... that a universal functional, that can reproduce in a one-body fashion the true ground state density and energy of a correlated many-body system, exists.\n\nTo find it is a different story, and literally a trillion dollar question.\n\n2/6', 'Finding the universal density functional is a Quantum Merlin-Arthur problem. So a problem that can be solved in decent time only having a quantum oracle (Merlin) that can perfectly and immediately answer questions or a smart classical Arthur.\n\nWe try our best using \n\n3/6', ""Using Self-Consistent Green's function to do a proper simulation from first principle of the nuclear state using Chiral interaction and density functional generators. That's our Merlin. \nThe constrained DFT then tries to reproduce the one-body densities and energies.\n4/6"", 'The results of this ambitious work present us with mixed feeling: we did not get nice results in finite nuclei that would have been so epic.\nHowever we identified problems in the ab initio calculation, and the Skyrme generators used, opening the door for more studies.\n\n5/6', 'We tried to cheat the Quantum Merlin Arthur and it costed us years and several bones. But I learned a lot from it.\n\nThanks Gianluca for this bold attempt in cracking this impossible problem.\n\n@jacekdob512 @AleStyle81 @NucTheorySurrey \n\n6/6']",https://arxiv.org/abs/2002.01903,"We present the first application of a new approach, proposed in [Journal of Physics G: Nuclear and Particle Physics, 43, 04LT01 (2016)] to derive coupling constants of the Skyrme energy density functional (EDF) from ab initio Hamiltonian. By perturbing the ab initio Hamiltonian with several functional generators defining the Skyrme EDF, we create a set of metadata that is then used to constrain the coupling constants of the functional. We use statistical analysis to obtain such an ab initio-equivalent Skyrme EDF. We find that the resulting functional describes properties of atomic nuclei and infinite nuclear matter quite poorly. This may point out to the necessity of building up the ab initio-equivalent functionals from more sophisticated generators. However, we also indicate that the current precision of the ab initio calculations may be insufficient for deriving meaningful nuclear EDFs. ","Model nuclear energy density functionals derived from ab initio
  calculations"
106,1225785674487517185,4249537197,Christian Wolf,"['New paper: we imbue Deep-RL agents for 3D environments with inductive bias using projective geometry, and we show that the algorithm automatically discovers objet affordances and places objects on a map. Work by @edwardbeeching + J. Dibangoye, O. Simonin. <LINK> <LINK>']",https://arxiv.org/abs/2002.02286,"Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent's performance in 3D environments on challenging tasks with multi-step objectives. The EgoMap architecture incorporates several inductive biases including a differentiable inverse projection of CNN feature vectors onto a top-down spatially structured map. The map is updated with ego-motion measurements through a differentiable affine transform. We show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. We demonstrate that incorporating these inductive biases into an agent's architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories. A detailed ablation study demonstrates the impact of key aspects of the architecture and through extensive qualitative analysis, we show how the agent exploits its structured internal memory to achieve higher performance. ",EgoMap: Projective mapping and structured egocentric memory for Deep RL
107,1225698131280457730,944291984675614721,Tobias de Jong,"['New paper on arXiv 😁 <LINK> In a collaboration with (amongst others) University of Geneva, @ICFOnians, @elettrasincro and @LeidenPhysics, we use STM and LEEM to aid ARPES to study the band structure of (near) magic-angle bilayer graphene and observe a flat band', 'We (i.e. @sensemolen , @j_jobst and myself) used LEEM to map out the device of stacked 2D flakes at high resolution, identifying areas of monolayer, multilayer, normal bilayer and near-magic-angle bilayer graphene, guiding ARPES and STM collaborators where to measure.']",https://arxiv.org/abs/2002.02289,"Transport experiments in twisted bilayer graphene revealed multiple superconducting domes separated by correlated insulating states. These properties are generally associated with strongly correlated states in a flat mini-band of the hexagonal moir\'e superlattice as it was predicted by band structure calculations. Evidence for such a flat band comes from local tunneling spectroscopy and electronic compressibility measurements, reporting two or more sharp peaks in the density of states that may be associated with closely spaced van Hove singularities. Direct momentum resolved measurements proved difficult though. Here, we combine different imaging techniques and angle resolved photoemission with simultaneous real and momentum space resolution (nano-ARPES) to directly map the band dispersion in twisted bilayer graphene devices near charge neutrality. Our experiments reveal large areas with homogeneous twist angle that support a flat band with spectral weight that is highly localized in momentum space. The flat band is separated from the dispersive Dirac bands which show multiple moir\'e hybridization gaps. These data establish the salient features of the twisted bilayer graphene band structure. ","Direct evidence for flat bands in twisted bilayer graphene from
  nano-ARPES"
108,1225625702629724160,1177063549606203394,Tommi Tenkanen,"['A new paper out! <LINK> \n\nWe studied initial conditions for cosmic inflation, concentrating on scenarios where the inflaton potential has a ""plateau"" shape, as such models are among those most favored by observations of the Cosmic Microwave Background data. 1/', 'As a representative example, we considered a ""Higgs inflation"" model in the context of so-called ""Palatini"" gravity. We showed that inflation generically occurs in a large part of the model parameter space even when the scale of inflation is much smaller than the Planck scale. 2/']",https://arxiv.org/abs/2002.02420,"We study initial conditions for inflation in scenarios where the inflaton potential has a plateau shape. Such models are those most favored by Planck data and can be obtained in a large number of model classes. As a representative example, we consider Higgs inflation with and without an $R^2$ term in the context of Palatini gravity. We show that inflation with a large number of e-folds generically occurs in a large part of the parameter space without any fine-tuning of parameters even when the scale of inflation and the inflaton field value during inflation are much smaller than the Planck scale. We discuss consequences for detection of primordial gravitational waves and spectral tilt of curvature perturbations, as well as the recently proposed ""Trans-Planckian Censorship"" conjecture. ",Initial conditions for plateau inflation: a case study
109,1225391757589991424,1100033188985032704,Jonas Kemmer,['Don’t miss our new @CARMENES_exopl paper which appeared on arxiv today: Characterization of the nearby ultra-compact multiplanetary system YZ Ceti (<LINK>). Thanks to @IsabelMMartin for graphically summarizing our results! <LINK>'],https://arxiv.org/abs/2002.01772,"The nearby ultra-compact multiplanetary system YZ Ceti consists of at least three planets. The orbital period of each planet is the subject of discussion in the literature due to strong aliasing in the radial velocity data. The stellar activity of this M dwarf also hampers significantly the derivation of the planetary parameters. With an additional 229 radial velocity measurements obtained since the discovery publication, we reanalyze the YZ Ceti system and resolve the alias issues. We use model comparison in the framework of Bayesian statistics and periodogram simulations based on a method by Dawson and Fabrycky to resolve the aliases. We discuss additional signals in the RV data, and derive the planetary parameters by simultaneously modeling the stellar activity with a Gaussian process regression model. To constrain the planetary parameters further we apply a stability analysis on our ensemble of Keplerian fits. We resolve the aliases: the three planets orbit the star with periods of $2.02$ d, $3.06$ d, and $4.66$ d. We also investigate an effect of the stellar rotational signal on the derivation of the planetary parameters, in particular the eccentricity of the innermost planet. Using photometry we determine the stellar rotational period to be close to $68$ d. From the absence of a transit event with TESS, we derive an upper limit of the inclination of $i_\mathrm{max} = 87.43$ deg. YZ Ceti is a prime example of a system where strong aliasing hindered the determination of the orbital periods of exoplanets. Additionally, stellar activity influences the derivation of planetary parameters and modeling them correctly is important for the reliable estimation of the orbital parameters in this specific compact system. Stability considerations then allow additional constraints to be placed on the planetary parameters. ","The CARMENES search for exoplanets around M dwarfs. Characterization of
  the nearby ultra-compact multiplanetary system YZ Ceti"
110,1225390135916867585,49798096,Stephan Stock,"['Do not miss our new @CARMENES_exopl paper about the YZ Ceti multiplanetary system. @JKemmer2 will provide you with a short sketch on the most important results. For details have a look into the manuscript on arXiv: <LINK>', 'https://t.co/5EPOEQT9iA']",https://arxiv.org/abs/2002.01772,"The nearby ultra-compact multiplanetary system YZ Ceti consists of at least three planets. The orbital period of each planet is the subject of discussion in the literature due to strong aliasing in the radial velocity data. The stellar activity of this M dwarf also hampers significantly the derivation of the planetary parameters. With an additional 229 radial velocity measurements obtained since the discovery publication, we reanalyze the YZ Ceti system and resolve the alias issues. We use model comparison in the framework of Bayesian statistics and periodogram simulations based on a method by Dawson and Fabrycky to resolve the aliases. We discuss additional signals in the RV data, and derive the planetary parameters by simultaneously modeling the stellar activity with a Gaussian process regression model. To constrain the planetary parameters further we apply a stability analysis on our ensemble of Keplerian fits. We resolve the aliases: the three planets orbit the star with periods of $2.02$ d, $3.06$ d, and $4.66$ d. We also investigate an effect of the stellar rotational signal on the derivation of the planetary parameters, in particular the eccentricity of the innermost planet. Using photometry we determine the stellar rotational period to be close to $68$ d. From the absence of a transit event with TESS, we derive an upper limit of the inclination of $i_\mathrm{max} = 87.43$ deg. YZ Ceti is a prime example of a system where strong aliasing hindered the determination of the orbital periods of exoplanets. Additionally, stellar activity influences the derivation of planetary parameters and modeling them correctly is important for the reliable estimation of the orbital parameters in this specific compact system. Stability considerations then allow additional constraints to be placed on the planetary parameters. ","The CARMENES search for exoplanets around M dwarfs. Characterization of
  the nearby ultra-compact multiplanetary system YZ Ceti"
111,1225369093815095296,1032283657015316480,Josh Dorrington,"['V. happy to have submitted my 1st ever paper, w\\ @meteo_Isla, and others!\n\nHow useful really are sub-seasonal forecasts to end-users?\n\nWe present a new user-focused framework for answering this question, based on an energy sector case-study\n\npreprint at: <LINK>']",https://arxiv.org/abs/2002.01728,"We quantify the value of sub-seasonal forecasts for a real-world prediction problem: the forecasting of French month-ahead energy demand. Using surface temperature as a predictor, we construct a trading strategy and assess the financial value of using meteorological forecasts, based on actual energy demand and price data. We show that forecasts with lead times greater than 2 weeks can have value for this application, both on their own and in conjunction with shorter range forecasts, especially during boreal winter. We consider a cost/loss framework based on this example, and show that while it captures the performance of the short range forecasts well, it misses the marginal value present in the longer range forecasts. We also contrast our assessment of forecast value to that given by traditional skill scores, which we show could be misleading if used in isolation. We emphasise the importance of basing assessment of forecast skill on variables actually used by end-users. ","Beyond skill scores: exploring sub-seasonal forecast value through a
  case study of French month-ahead energy prediction"
112,1225337667082215425,75249390,Axel Maas,"[""We have put out a new paper, among others with @SimonPlaetzer, on the 'valence Higgs' of the proton. That may sounds weird, as usually it is told of only three 'valence quarks' in the proton. So let me explain a little - you can find the paper at <LINK>"", 'A particle is called valence, if it contributes to the quantum number of the proton. The quarks make up its electric charge and baryon number. So why would the Higgs be there? Which quantum number does it contribute to?\n\nIt is quite involved,and has to do with weak interactions.', 'The weak interactions couple to something we call flavor, and which is usually said to make the difference between proton and neutron.\n\nThe underlying theory is more complicated, and formally says something like flavor is not observable - see the review https://t.co/jVo2JEcpFO', ""To get a physical version of flavor, you need to add something which acts 'like flavor', and creates the distinction. In the standard model, the only particle which can do this without changing the spin is the Higgs. This has been worked out for leptons in 1980 by Fröhlich et al."", 'However, a proton is a composite particle, and things are a bit more weird. But essentially it needs to have three quarks and a Higgs to get all quantum numbers right and observable in the standard model. We deduced this in https://t.co/EhNqRI8CIn', 'But this should have observable consequences, if we have enough energy to get the sluggish Higgs to react - at least something like the LHC.\n\nThis paper is our first attempt to determine how much we would see, and where we would see it.', 'We find that the effect needs to be tiny, but not impossible.\n\nIt is worthwhile to invest more effort into it, as we did a lot of very crude estimates. But confirming it would be a big step in understanding the field theory underlying the standard model.']",https://arxiv.org/abs/2002.01688,"Non-perturbative gauge-invariance under the strong and the weak interactions dictates that the proton contains a non-vanishing valence contribution from the Higgs particle. By introducing an additional parton distribution function (PDF), we investigate the experimental consequences of this prediction. The Herwig 7 event generator and a parametrized CMS detector simulation are used to obtain predictions for a scenario amounting to the LHC Run II data set. We use those to assess the impact of the Higgs PDF on the pp->ttbar process in the single lepton final state. Comparing to nominal simulation we derive expected limits as a function of the shape of the valence Higgs PDF. We also investigate the process pp->ttZ at the parton level to add further constraints. ",Constraining the Higgs valence contribution in the proton
113,1225166638909050882,39865873,Ryan Lowe,"['New (ICLR) paper out on arXiv! 🥳 We started with the question ""how do you go from emergent communication to natural language?"", and ended up here: <LINK>\n\nSpoiler: emergent communication may not be super useful for NLP, but self-play can be. \n\nThread 👇: (1/n) <LINK>', 'Back in 2017 when interning at OpenAI and working on multi-agent RL, @j_foerst and I came up with an idea. what if we: \n\n(1) trained a *population* of agents via emergent communication, and then \n\n(2) trained a *meta-agent* to adapt to all these populations? (2/n)', 'The hope would be that after (2), you had an agent that can quickly learn new languages, and could hopefully learn English with fewer samples (probably with appropriate conditions on the initial distribution over languages) (3/n)', ""@backpropper and I finally started working on this in early 2019. Initial results weren't promising: even in a  simple game with an artificial compositional language (the 'target language'), it was hard for the agents to emerge languages that were useful for the meta-agent (4/n)"", '(Although if you pre-defined the set of emergent languages to train on, it worked okay. See our early workshop submission here: https://t.co/3ml6ukO6RR, which also reveals how we were thinking about it at the time.) (5/n)', 'We eventually realized that if you first did a bit of supervised learning on the target language (which is feasible in practice), things worked a lot better. \n\nSo our focus shifted towards: ""how do we combine supervised learning + self-play for language learning?"" (6/n)', ""The conclusion: it seems like (in our games) it's not actually a good idea to emerge a language from scratch! \n\nInstead, better to start with some supervised learning, then put some self-play updates in. (7/n)"", 'This is maybe obvious in retrospect, but for me it was surprising, since one of my main motivations for working on emergent communication was that it could be a way to get really good natural language-using agents. (8/n)', ""So I'm now less optimistic on emergent communication as a path to good NLP, though I think self-play could still be a very useful signal to use in some cases. (9/n)"", 'All of this is joint work with the fantastic @backpropper, @j_foerst, @douwekiela, and Joelle Pineau. 👃 (n/n)']",https://arxiv.org/abs/2002.01093,"A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods. ","On the interaction between supervision and self-play in emergent
  communication"
114,1225157385397985280,4189511729,Marius Millea,"['Detecting the CMB polarization B-modes from primordial gravitational waves would revolutionize our understanding of inflation. But this signal is highly obscured by gravitational lensing distortion. Our new paper shows how this can be cleaned optimally.\n\n<LINK> <LINK>', 'The method is fully Bayesian and so can take the data and reconstruct all of the available information about the projected mass causing the gravitational lensing, the primordial signal, and parameters controlling the statistics of these fields.', 'In practice our code samples the ~million dimensional parameter space, which we can do efficiently thanks to #JuliaLang, GPUs, Hamiltonian Monte Carlo, and some important tricks we discuss in the paper. The software package is ready for anyone to run:\n\nhttps://t.co/N2QVNJcp47', 'Excited to have this work with @EthanAnderes and @bwandelt out so we can build towards using this for current and future CMB experiments.']",https://arxiv.org/abs/2002.00965,"The search for primordial gravitational waves in the Cosmic Microwave Background (CMB) will soon be limited by our ability to remove the lensing contamination to $B$-mode polarization. The often-used quadratic estimator for lensing is known to be suboptimal for surveys that are currently operating and will continue to become less and less efficient as instrumental noise decreases. While foregrounds can in principle be mitigated by observing in more frequency bands, progress in delensing hinges entirely on algorithmic advances. We demonstrate here a new inference method that solves this problem by sampling the exact Bayesian posterior of any desired cosmological parameters, of the gravitational lensing potential, and of the delensed CMB maps, given lensed temperature and polarization data. We validate the method using simulated CMB data with non-white noise and masking on up to 650\,deg$^2$ patches of sky. A unique strength of this approach is the ability to jointly estimate cosmological parameters which control both the primordial CMB and the lensing potential, which we demonstrate here for the first time by sampling both the tensor-to-scalar ratio, $r$, and the amplitude of the lensing potential, $A_\phi$. The method allows us to perform the most precise check to-date of several important approximations underlying CMB-S4 $r$ forecasting, and we confirm these yield the correct expected uncertainty on $r$ to better than 10%. ","Bayesian delensing delight: sampling-based inference of the primordial
  CMB and gravitational lensing"
115,1225066801358393345,289494499,"Adi Foord, PhD","['📢 New Paper Alert 📢\n\nCheck out our latest accepted paper on the arXiv today, titled ""A Second Look at 12 Candidate Dual AGNs using #BAYMAX"" :  \n\n<LINK>\n\n(1/7)', 'We used #BAYMAX to analyze a sample of 12 candidate dual AGNs, targeted due to double peaked [O III] emission lines found in their SDSS spectra. They each received follow-up @chandraxray observations, which told a slightly different story (an example of CXO vs. HST below) (2/7) https://t.co/fbyq6fTqdh', '@chandraxray The @chandraxray observations were low-count and each candidate\'s projected separation was small (&lt;1"") .... thus we employed the power of Bayesian statistics and used #BAYMAX to analyze them (3/7) https://t.co/vHE3nIoqBH', '@chandraxray We found that only 4/12 candidates were likely composed of dual X-ray point source systems (unbinned CXO observations shown below, with symbols denoting whether associated with pri. or sec. point source, and HST contours overplotted) #thatsneat #BAYMAX (4/7) https://t.co/bMfbnmJiMb', '@chandraxray Because #BAYMAX is incredibly powerful &amp; cool, it can disentangle the counts associated with each point source and analyze their spectra individually. Carrying out this spectral analysis, we confirmed that 1/12 is a dual AGN (5/7) https://t.co/LpgWNQJLhl', '@chandraxray Overall, better understanding the mismatch in dual AGN classifications using various detection techniques will be important for understanding the true active fraction &amp; their preferential environments #BAYMAXcanhelpwiththat (6/7) https://t.co/kZl88xxuj0', '@chandraxray Future work will include unleashing #BAYMAX on the *thousands* of AGN in the @chandraxray archive and measuring the fraction of dual AGN as a function of redshift #staytuned (7/7)', '@lmallickastro Thanks 😊', '@HansDaltrey @chandraxray @ChandraArchive I’m excited, as the @ChandraArchive most likely has many undiscovered dual AGN !']",https://arxiv.org/abs/2002.01033,"We present an analysis of 12 optically selected dual AGN candidates at $z < 0.34$. Each candidate was originally identified via double-peaked [O III] $\lambda$5007 emission lines, and have received follow-up $Chandra$ and $HST$ observations. Because the X-ray data are low-count ($<100$ counts) with small separations ($<1$""), a robust analysis is necessary for classifying each source. Pairing long-slit [O III] observations with existing $Chandra$ observations, we re-analyze the X-ray observations with ${\tt BAYMAX}$ to determine whether the X-ray emission from each system is more likely a single or dual point source. We find that 4 of the 12 sources are likely dual X-ray point source systems. We examine each point source's spectra via a Monte Carlo method that probabilistically identifies the likely origin of each photon. When doing so, we find that (i) the secondary X-ray point sources in 2 of the systems have $L_{\mathrm{X}}<10^{40}$ erg s$^{-1}$, such that we cannot rule out a non-AGN origin, (ii) one source has a secondary with $L_{\mathrm{X}}>10^{40}$ erg s$^{-1}$ but a spectrum that is too soft to definitively preclude being X-ray emitting diffuse gas that was photoionized by the primary AGN, and (iii) one system (SDSS J1126+2944) is a dual AGN. Additionally, using complementary $HST$ observations, we analyze a sub-sample of systems that are visually identified as merging. Our results suggest that dual AGNs may preferentially reside in mergers with small separations, consistent with both simulations and observations. ",A Second Look at 12 Candidate Dual AGNs using ${\tt BAYMAX}$
116,1225038269299281923,1728419371,Dr Joanna Barstow,"[""It's a while since I've got to do this so I'm excited...\n\nda dada da Da DA DAAAAA....\n\n**NEW PAPER ALERT**\n<LINK>\n\nA comparison of exoplanet spectroscopic retrieval tools, feat. @IngoWaldmann + @DrRyanGarland + (half of) @ExoLemons + Marco Rocchetto and Mike Line"", ""@IngoWaldmann @DrRyanGarland @ExoLemons I appreciate that the title makes it sound like Compare the Market's nerdy cousin, and if you're shopping around for a retrieval tool to use you should definitely give it a read. But it won't tell you which code is the 'best' or 'correct' one."", '@IngoWaldmann @DrRyanGarland @ExoLemons Instead, we explore the effects of tiny differences in the way that these tools are set up, and what that means for performing retrievals on real exoplanets.', '@IngoWaldmann @DrRyanGarland @ExoLemons These plots compare model spectra from our three codes, CHIMERA (blue), NEMESIS (black) and TauREX (red). They all look really similar, which is a good thing, because we put the exact same stuff in the model atmospheres. The physics in all the codes is consistent - big relief! https://t.co/HpLhUHsoBb', '@IngoWaldmann @DrRyanGarland @ExoLemons BUT: the spectra are not *exactly* the same for all the codes. Little differences creep in at different stages - maybe in significant figures and rounding errors, or slightly different ways of tabulating absorption data.', '@IngoWaldmann @DrRyanGarland @ExoLemons So, we did an experiment: what happens if we pretend that our synthetic spectra from say CHIMERA and TauREX are real observations, e.g. from @NASAWebb, and then we perform a retrieval with NEMESIS? Do we recover the correct input values?', ""@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb The answer to is, 'it depends', because by 'correct' we mean 'the same to within 1 sigma'. If we add photon noise of 100 ppm to our synthetic observation, we do a good job; the true values, indicated by the black lines, fall nicely within the 1 sigma bounds for both synthetics. https://t.co/0q3CzoMIvi"", ""@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb BUT: if instead we add photon noise of 30 ppm (what we expect from JWST for the best targets), simulating a more precise spectrum, it all goes wrong! That might seem counterintuitive, but it's a nice illustration of the difference between accuracy and precision. https://t.co/VVYWlvjjRu"", '@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb https://t.co/oYtYi8I24r', ""@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb This doesn't mean our codes are broken or that everything we do is a disaster, but it does mean that for super-precise spectra where the photon noise is low, we need to be especially wary of systematic error."", '@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb Systematic error could come from our models, because no model is a perfect representation of reality; or it could come from the telescope, or gaps in our knowledge about the parent star.', '@IngoWaldmann @DrRyanGarland @ExoLemons @NASAWebb TL:DR; our retrieval codes produce consistent model spectra (yay) which means the physics in them is consistent (more yay) but no model is perfect (boo) and systematics are a pain (more boo).']",https://arxiv.org/abs/2002.01063,"Over the last several years, spectroscopic observations of transiting exoplanets have begun to uncover information about their atmospheres, including atmospheric composition and indications of the presence of clouds and hazes. Spectral retrieval is the leading technique for interpretation of transmission spectra and is employed by several teams using a variety of forward models and parameter estimation algorithms. However, different model suites have mostly been used in isolation and so it is unknown whether the results from each are comparable. As we approach the launch of the James Webb Space Telescope we anticipate advances in wavelength coverage, precision, and resolution of transit spectroscopic data, so it is important that the tools that will be used to interpret these information rich spectra are validated. To this end, we present an inter-model comparison of three retrieval suites: TauREx, NEMESIS and CHIMERA. We demonstrate that the forward model spectra are in good agreement (residual deviations on the order of 20 - 40 ppm), and discuss the results of cross retrievals between the three tools. Generally, the constraints from the cross retrievals are consistent with each other and with input values to within 1 sigma However, for high precision scenarios with error envelopes of order 30 ppm, subtle differences in the simulated spectra result in discrepancies between the different retrieval suites, and inaccuracies in retrieved values of several sigma. This can be considered analogous to substantial systematic/astrophysical noise in a real observation, or errors/omissions in a forward model such as molecular linelist incompleteness or missing absorbers. ",A comparison of exoplanet spectroscopic retrieval tools
117,1225023278156328961,280083723,Yoh Tanimoto,"['new paper~ we find an embedding of (scalar) lattice fields into the continuum field using wavelets <LINK>', '@fraarici thanks! the full paper is coming (hopefully) soon~']",https://arxiv.org/abs/2002.01442,We report on a rigorous operator-algebraic renormalization group scheme and construct the continuum free field as the scaling limit of Hamiltonian lattice systems using wavelet theory. A renormalization group step is determined by the scaling equation identifying lattice observables with the continuum field smeared by compactly supported wavelets. Causality follows from Lieb-Robinson bounds for harmonic lattice systems. The scheme is related with the multi-scale entanglement renormalization ansatz and augments the semi-continuum limit of quantum systems. ,Operator-algebraic renormalization and wavelets
118,1224985999606001664,1009527883151310848,Kresten Lindorff-Larsen,"['Pleased to share a ""new"" preprint that deals with a subtle, but sometimes important point when fitting protein denaturation data using the Linear Extrapolation Method. The paper is very simple, so if you read this thread you\'ve essentially read it.  1/10 <LINK>', ""TL;DR: If you fit protein denaturation data using dG=dG0-m[D], then dG0 and m are highly correlated. This means that if you just report dG0 and m with errors you cannot get the errors of the D50 by simple error propagation. That's (mostly) it. 2/"", 'When fitting denaturant induced unfolding curves, we generally use the ""Santoro &amp; Bolen"" linear extrapolation model that stipulates that the free energy of unfolding depends linearly on the denaturant concentration 3/ https://t.co/3YzM2Rk7jB', 'This relationship is generally parameterized using two of three parameters that correspond to the stability in absence of denaturant (dG0), the slope (so-called m-value) and the midpoint of the transition (D50) 3/ https://t.co/rNXsJHUaed', 'When fitting eg. fluorescence-vs-denturant most information about these parameters comes from the middle of curve where there is a sizable population of folded and unfolded states. This in turn means that the slope (m) and intercept (dG0) are often _highly_ correlated. 4/ https://t.co/arydJRQ2xy', 'This in turn means that one overestimates several fold the error of D50 if this is estimated by error propagation from dG0 and m and their errors (unless the covariation is reported which it never is). So better report all three parms, or at least (dG0,D50) or (m,D50). 5/ https://t.co/VjrnCtns38', 'I tested a few different methods to estimate the confidence intervals (standard way of looking at the variance/covariance matrix, bootstrapping, line search) and, fortunately, they all gave more or less the same results. So the errors from most fitting software should be OK 6/', 'I did this work back in 2000/1 just before and while I was starting my PhD to analyse some data I had generated during my MSc (https://t.co/ucCi6oaWzE). But then the CD with the manuscript went missing 🙉, and so am pleased to have it available online now. 7/', 'It also was my first substantial programming experience and writing a full LM optimizer etc and doing this analysis taught me much about non-linear regression and error analysis that has since been useful. This includes looking out for parameter correlations. 8/', 'I later learnt about Bayesian methods which are in particular useful when there are sources of error that are unknown or difficult to control for using standard methods. Here is a very nice example from @jchodera et al with ITC data. 9/ https://t.co/VST0FMgm5T', 'I realize that most of the above is simple stuff that is almost trivial. But sometimes these things are forgotten when we pack away statistical analyses and data fitting in polished packages that work by the push of a button. 10/10']",https://arxiv.org/abs/2002.01018,"When protein stability is measured by denaturant induced unfolding the linear extrapolation method is usually used to analyse the data. This method is based on the observation that the change in Gibbs free energy associated with unfolding, $\Delta_rG$, is often found to be a linear function of the denaturant concentration, $D$. The free energy change of unfolding in the absence of denaturant, $\Delta_rG_0$, is estimated by extrapolation from this linear relationship. Data analysis is generally done by nonlinear least-squares regression to obtain estimates of the parameters as well as confidence intervals. We have compared different methods for calculating confidence intervals of the parameters and found that a simple method based on linear theory gives as good, if not better, results than more advanced methods. We have also compared three different parameterizations of the linear extrapolation method and show that one of the forms, $\Delta_rG(D) = \Delta_rG_0 - mD$, is problematic since the value of $\Delta_rG_0$ and that of the $m$-value are correlated in the nonlinear least-squares analysis. Parameter correlation can in some cases cause problems in the estimation of confidence-intervals and -regions and should be avoided when possible. Two alternative parameterizations, $\Delta_rG(D) = -m(D-D_{50})$ and $\Delta_rG(D) = \Delta_rG_0(1-D/D_{50})$, where $D_{50}$ is the midpoint of the transition region show much less correlation between parameters. ","Dissecting the statistical properties of the Linear Extrapolation Method
  of determining protein stability"
119,1224877393887629312,326843207,Yuta Notsu,"['Our new paper ""Temporal Evolution of Spatially-Resolved Individual Star Spots on a Planet-Hosting Solar-type Star: Kepler 17""  is accepted to ApJ and now in arXiv !!  <LINK> \n\nAuthors:  @KosOlo8, @jradavenport, @brettmor  @astronomy_stars , and many !', '@KosOlo8 @jradavenport @brettmor Using exoplanet transits and rotational modulations of Kepler-17,we investigated number of spots, spot locations, and the temporal evolution. Although the temporal evolution derived from the rotational modulation differs from those of in-transit spots to a certain degree, ..... https://t.co/p0kcZKulA8', '@KosOlo8 @jradavenport @brettmor .... the emergence/decay rates of in-transit spots are within an order of magnitude of those derived for sunspots as well as our previous research based only on rotational modulations.  This supports a hypothesis that .... https://t.co/vyS3JAIkvu', '@KosOlo8 @jradavenport @brettmor ....  that the emergence/decay of sunspots and extremely-large star spots on solar-type stars occur through the same underlying processes.\n\nAlso, we can say large star spots having a potential to produce superflares are found to survive more than\n100 days (up to 1 year)...😲 https://t.co/LQ4TMDwgWr']",https://arxiv.org/abs/2002.01086,"Star spot evolution is visible evidence of the emergence/decay of the magnetic field on stellar surface, and it is therefore important for the understanding of the underlying stellar dynamo and consequential stellar flares. In this paper, we report the temporal evolution of individual star spot area on the hot-Jupiter-hosting active solar-type star Kepler 17 whose transits occur every 1.5 days. The spot longitude and area evolution are estimated (1) from the stellar rotational modulations of Kepler data and (2) from the brightness enhancements during the exoplanet transits caused by existence of large star spots. As a result of the comparison, number of spots, spot locations, and the temporal evolution derived from the rotational modulations is largely different from those of in-transit spots. We confirm that although only two light curve minima appear per rotation, there are clearly many spots present on the star. We find that the observed differential intensity changes are sometimes consistent with the spot pattern detected by transits, but they sometimes do not match with each other. Although the temporal evolution derived from the rotational modulation differs from those of in-transit spots to a certain degree, the emergence/decay rates of in-transit spots are within an order of magnitude of those derived for sunspots as well as our previous research based only on rotational modulations. This supports a hypothesis that the emergence/decay of sunspots and extremely-large star spots on solar-type stars occur through the same underlying processes. ","Temporal Evolution of Spatially-Resolved Individual Star Spots on a
  Planet-Hosting Solar-type Star: Kepler 17"
120,1224820291232124929,759249,Dean Eckles,"['Which products are purchased together can reveal a lot about their relationship. But so can which products are viewed together, but not co-purchased.\n\nOur (@madhavkumar2005 &amp; @sinanaral) new working paper leverages both types of baskets... <LINK> <LINK>', '@madhavkumar2005 @sinanaral Embeddings using either can capture many fine-grained categories. Here you can see existing categories in a 2D projection. Close products are predicted to be purchased together. https://t.co/GfK3Vg3MZX', ""@madhavkumar2005 @sinanaral We use products' closeness as a heuristic to construct thousands of discounted bundles of products and try these out in a field experiment. Both scores positively predict responses to the bundles, especially the purchase-based complementarity score. https://t.co/8uSQHniM8P"", '@madhavkumar2005 @sinanaral This includes some interesting ""cross-category"" bundles, which can be of strategic importance to a firm looking to expand the categories a customer shops in. https://t.co/LY21ZVDtkV', '@madhavkumar2005 @sinanaral This paper reflects a lot of hard work by @madhavkumar2005 creating these dual embeddings and designing, running, and analyzing a field experiment with so many distinct ""treatments"".', '@madhavkumar2005 @sinanaral If you want to see what products are closest to Neutrogena Oil-Free Acne Wash Redness Soothing Cream Facial Cleanser, 6 Fl. Oz, in the purchase and view/search spaces, you will have to look at the paper: https://t.co/7pPYNuHkTN', '@dade_us @madhavkumar2005 @sinanaral Will have a read—thanks. All very recent stuff! Our focus here is on using this for bundling/recommendation, though, yeah, clear applications to market mapping &amp; demand estimation.', ""@Andrew___Baker @madhavkumar2005 @sinanaral Thanks! One thing that helped make this legible was  sorting by clustering the categories, which I'm sure @madhavkumar2005 could say more about""]",https://arxiv.org/abs/2002.00100,"Bundling, the practice of jointly selling two or more products at a discount, is a widely used strategy in industry and a well examined concept in academia. Historically, the focus has been on theoretical studies in the context of monopolistic firms and assumed product relationships, e.g., complementarity in usage. We develop a new machine-learning-driven methodology for designing bundles in a large-scale, cross-category retail setting. We leverage historical purchases and consideration sets created from clickstream data to generate dense continuous representations of products called embeddings. We then put minimal structure on these embeddings and develop heuristics for complementarity and substitutability among products. Subsequently, we use the heuristics to create multiple bundles for each product and test their performance using a field experiment with a large retailer. We combine the results from the experiment with product embeddings using a hierarchical model that maps bundle features to their purchase likelihood, as measured by the add-to-cart rate. We find that our embeddings-based heuristics are strong predictors of bundle success, robust across product categories, and generalize well to the retailer's entire assortment. ",Scalable bundling via dense product embeddings
121,1224762472814600192,737878789636694016,Kshitij Jain,"['Our new paper on Unsupervised Multilingual Alignment using Wasserstein Barycenter is now online on Arxiv <LINK>. In this work, we show that using Wasserstein Barycenter as a pivot/mean language, we are able to achieve state-of-the-art translation accuracies.', 'This work was done in collaboration with Xin Lian (intern at @BorealisAI), Jakub Truszkowski, Pascal Poupart, and Yaoliang Yu.']",https://arxiv.org/abs/2002.00743,"We study unsupervised multilingual alignment, the problem of finding word-to-word translations between multiple languages without using any parallel data. One popular strategy is to reduce multilingual alignment to the much simplified bilingual setting, by picking one of the input languages as the pivot language that we transit through. However, it is well-known that transiting through a poorly chosen pivot language (such as English) may severely degrade the translation quality, since the assumed transitive relations among all pairs of languages may not be enforced in the training process. Instead of going through a rather arbitrarily chosen pivot language, we propose to use the Wasserstein barycenter as a more informative ""mean"" language: it encapsulates information from all languages and minimizes all pairwise transportation costs. We evaluate our method on standard benchmarks and demonstrate state-of-the-art performances. ",Unsupervised Multilingual Alignment using Wasserstein Barycenter
122,1224724623910154242,458603378,Marian,"['New Paper: Adversarial Generation of Continuous Implicit Shape Representations\n\nWe propose two GANs with a DeepSDF network as the generator and either a 3D CNN or a Pointnet as the discriminator. Written by @rusty1s and me!\n\nPaper: <LINK> <LINK>', '@BriamMor I started with 3D GANs and only did 2D GANs after that. My advice would be to start with 2D GANs since that makes data preparation and visualization a lot easier. If you want to get results quickly, try to get existing implementations to run.', '@BriamMor But if you want to understand how it works, try to implement it yourself. I would start with a fully connected GAN on something like MNIST, then a DCGAN, then WGAN-GP, then implement progressive growing, then StyleGAN. You could also go for StyleGAN right away.', '@rusty1s Check out my blog post about the paper. It has animated visualizations!\nhttps://t.co/wLFpUKgMv7 https://t.co/5QVT7xUx0w']",https://arxiv.org/abs/2002.00349,"This work presents a generative adversarial architecture for generating three-dimensional shapes based on signed distance representations. While the deep generation of shapes has been mostly tackled by voxel and surface point cloud approaches, our generator learns to approximate the signed distance for any point in space given prior latent information. Although structurally similar to generative point cloud approaches, this formulation can be evaluated with arbitrary point density during inference, leading to fine-grained details in generated outputs. Furthermore, we study the effects of using either progressively growing voxel- or point-processing networks as discriminators, and propose a refinement scheme to strengthen the generator's capabilities in modeling the zero iso-surface decision boundary of shapes. We train our approach on the ShapeNet benchmark dataset and validate, both quantitatively and qualitatively, its performance in generating realistic 3D shapes. ",Adversarial Generation of Continuous Implicit Shape Representations
123,1224697712454467584,229598367,Dr. Claire Bowern,['<LINK> new paper on Arxiv about phylogenetic signal! (with Macklin-Cordes and Round)'],https://arxiv.org/abs/2002.00527,"Phylogenetic methods have broad potential in linguistics beyond tree inference. Here, we show how a phylogenetic approach opens the possibility of gaining historical insights from entirely new kinds of linguistic data--in this instance, statistical phonotactics. We extract phonotactic data from 111 Pama-Nyungan vocabularies and apply tests for phylogenetic signal, quantifying the degree to which the data reflect phylogenetic history. We test three datasets: (1) binary variables recording the presence or absence of biphones (two-segment sequences) in a lexicon (2) frequencies of transitions between segments, and (3) frequencies of transitions between natural sound classes. Australian languages have been characterized as having a high degree of phonotactic homogeneity. Nevertheless, we detect phylogenetic signal in all datasets. Phylogenetic signal is greater in finer-grained frequency data than in binary data, and greatest in natural-class-based data. These results demonstrate the viability of employing a new source of readily extractable data in historical and comparative linguistics. ",Phylogenetic signal in phonotactics
124,1224626042486317056,158659687,Sibylle Anderl,"['New CALYPSO-Paper online! It‘s probably the most work intensive co-authorship that I‘ve ever had, Arnaud Belloche as first author did a truly impressive job (92 pages in total)! It‘s a detailed study of complex organic molecules in Class0 protostars ⭐️ <LINK>', '@SB_AlphaCephei Naja, so gut es geht. Für dieses Paper hatte ich die Analysepipeline noch während des Postdocs geschrieben.']",https://arxiv.org/abs/2002.00592,"Complex organic molecules (COMs) have been detected in a few Class 0 protostars but their origin is not well understood. Going beyond studies of individual objects, we want to investigate the origin of COMs in young protostars on a statistical basis. We use the CALYPSO survey performed with the IRAM PdBI to search for COMs at high angular resolution in a sample of 26 solar-type protostars, including 22 Class 0 and four Class I objects. Methanol is detected in 12 sources and tentatively in one source, which represents half of the sample. Eight sources (30%) have detections of at least three COMs. We find a strong chemical differentiation in multiple systems with five systems having one component with at least three COMs detected but the other component devoid of COM emission. The internal luminosity is found to be the source parameter impacting the most the COM chemical composition of the sources, while there is no obvious correlation between the detection of COM emission and that of a disk-like structure. A canonical hot-corino origin may explain the COM emission in four sources, an accretion-shock origin in two or possibly three sources, and an outflow origin in three sources. The CALYPSO sources with COM detections can be classified into three groups on the basis of the abundances of oxygen-bearing molecules, cyanides, and CHO-bearing molecules. These chemical groups correlate neither with the COM origin scenarii, nor with the evolutionary status of the sources if we take the ratio of envelope mass to internal luminosity as an evolutionary tracer. We find strong correlations between molecules that are a priori not related chemically (for instance methanol and methyl cyanide), implying that the existence of a correlation does not imply a chemical link. [abridged] ","Questioning the spatial origin of complex organic molecules in young
  protostars with the CALYPSO survey"
125,1224599799451746304,1013746596222308352,Trifon Trifonov,"[""Don't miss our new CARMENES paper on the dynamical properties of the peculiar GJ 1148 system, which is composed of two Saturn-mass planets on eccentric orbits around an M-dwarf star! <LINK>"", 'You will learn that even though GJ1148 b resides in the HZ, ""habitable"" exomoons are not possible to remain stable!', 'The RV analysis of the CARMENES and HIRES data \nof GJ 1148 and the stability analysis were powered by \nThe Exo-Striker https://t.co/bq19WfzCT3']",https://arxiv.org/abs/2002.00906,"Context. GJ 1148 is an M-dwarf star hosting a planetary system composed of two Saturn-mass planets in eccentric orbits with periods of 41.38 and 532.02 days. Aims. We reanalyze the orbital configuration and dynamics of the GJ 1148 multi-planetary system based on new precise radial velocity (RV) measurements taken with CARMENES. Methods. We combined new and archival precise Doppler measurements from CARMENES with those available from HIRES for GJ 1148 and modeled these data with a self-consistent dynamical model. We studied the orbital dynamics of the system using the secular theory and direct N-body integrations. The prospects of potentially habitable moons around GJ 1148 b were examined. Results. The refined dynamical analyses show that the GJ 1148 system is long-term stable in a large phase-space of orbital parameters with an orbital configuration suggesting apsidal alignment, but not in any particular high-order mean-motion resonant commensurability. GJ 1148 b orbits inside the optimistic habitable zone (HZ). We find only a narrow stability region around the planet where exomoons can exist. However, in this stable region exomoons exhibit quick orbital decay due to tidal interaction with the planet. Conclusions. The GJ 1148 planetary system is a very rare M-dwarf planetary system consisting of a pair of gas giants, the inner of which resides in the HZ. We conclude that habitable exomoons around GJ 1148 b are very unlikely to exist. ","The CARMENES search for exoplanets around M dwarfs. Dynamical
  characterization of the multiple planet system GJ 1148 and prospects of
  habitable exomoons around GJ 1148 b"
126,1224537748628881409,1169196060130123782,Gergely Neu,"['new paper on adversarial linear contextual bandits!\n\nfeat. one of the first practical algorithms that can handle model misspecification + the first one to achieve optimal regret in an adversarial contextual bandit problem.\n\nw/ my student Julia Olkhovskaya\n\n<LINK> <LINK>', '@lreyzin thanks!\nyeah, i like to think that there are some surprising tricks in this one (e.g., lemma 3). hope they will be useful for others too...', '@CsabaSzepesvari well the COLT deadline speeds things up ;)']",https://arxiv.org/abs/2002.00287,"We consider an adversarial variant of the classic $K$-armed linear contextual bandit problem where the sequence of loss functions associated with each arm are allowed to change without restriction over time. Under the assumption that the $d$-dimensional contexts are generated i.i.d.~at random from a known distributions, we develop computationally efficient algorithms based on the classic Exp3 algorithm. Our first algorithm, RealLinExp3, is shown to achieve a regret guarantee of $\widetilde{O}(\sqrt{KdT})$ over $T$ rounds, which matches the best available bound for this problem. Our second algorithm, RobustLinExp3, is shown to be robust to misspecification, in that it achieves a regret bound of $\widetilde{O}((Kd)^{1/3}T^{2/3}) + \varepsilon \sqrt{d} T$ if the true reward function is linear up to an additive nonlinear error uniformly bounded in absolute value by $\varepsilon$. To our knowledge, our performance guarantees constitute the very first results on this problem setting. ","Efficient and Robust Algorithms for Adversarial Linear Contextual
  Bandits"
127,1233302860781228037,3236251346,Mikel Sanz,"['New paper resulting from a collaboration led by @fwilhelm on applying our digital-analog techniques to improve the applicability of the QAOA optimization algorithm. Great work, guys!! <LINK>\n@quantum_ana @KikeSolanoPhys @QUTIS3 @fwilhelm @upvehu @DLR_de @Daimler <LINK> <LINK>']",https://arxiv.org/abs/2002.12215,"The Quantum Approximate Optimisation Algorithm was proposed as a heuristic method for solving combinatorial optimisation problems on near-term quantum computers and may be among the first algorithms to perform useful computations in the post-supremacy, noisy, intermediate scale era of quantum computing. In this work, we exploit the recently proposed digital-analog quantum computation paradigm, in which the versatility of programmable universal quantum computers and the error resilience of quantum simulators are combined to improve platforms for quantum computation. We show that the digital-analog paradigm is suited to the variational quantum approximate optimisation algorithm, due to its inherent resilience against coherent errors, by performing large-scale simulations and providing analytical bounds for its performance in devices with finite single-qubit operation times. We observe regimes of single-qubit operation speed in which the considered variational algorithm provides a significant improvement over non-variational counterparts. ",Approximating the Quantum Approximate Optimisation Algorithm
128,1231866737630511105,758680559212228609,Puneet Dokania,"['New work on miscalibration of NNs (@AmartyaSanyal @JishnuMukhoti @vivekakulharia @tvg\n@_FiveAI ).  Message: focal loss makes a model more calibrated compared to label-smoothing, tempt scaling, Brier score, MMCE. Tested on plenty of tasks.\nPaper: <LINK>', ""@AmartyaSanyal @JishnuMukhoti @vivekakulharia @TVG @_FiveAI 2. focal loss inherently tries to maximize the entropy while minimizing the KL. This might explain better why it's confident, accurate, and calibrated.\n3. provide some insights on how to select the hyper-parameter\n\nTitle: Calibrating Deep Neural Networks using Focal Loss""]",https://arxiv.org/abs/2002.09437,"Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at this https URL ",Calibrating Deep Neural Networks using Focal Loss
129,1230970197428670466,1359972247,Jay Thiagarajan,"['How does prediction calibration affect the performance of lottery tickets? Our new paper answers this question - By calibrating predictions, we can produce sub-networks that demonstrate improved transferability and reliability.\nLink: <LINK>', 'Joint Work with @BindyaVenkatesh @kowshik0808 @prasatti']",https://arxiv.org/abs/2002.03875,"The hypothesis that sub-network initializations (lottery) exist within the initializations of over-parameterized networks, which when trained in isolation produce highly generalizable models, has led to crucial insights into network initialization and has enabled efficient inferencing. Supervised models with uncalibrated confidences tend to be overconfident even when making wrong prediction. In this paper, for the first time, we study how explicit confidence calibration in the over-parameterized network impacts the quality of the resulting lottery tickets. More specifically, we incorporate a suite of calibration strategies, ranging from mixup regularization, variance-weighted confidence calibration to the newly proposed likelihood-based calibration and normalized bin assignment strategies. Furthermore, we explore different combinations of architectures and datasets, and make a number of key findings about the role of confidence calibration. Our empirical studies reveal that including calibration mechanisms consistently lead to more effective lottery tickets, in terms of accuracy as well as empirical calibration metrics, even when retrained using data with challenging distribution shifts with respect to the source dataset. ","Calibrate and Prune: Improving Reliability of Lottery Tickets Through
  Prediction Calibration"
130,1229821005247336448,3557386648,Gabriel Ilharco,"[""New paper out!\n\nIn NLP, fine-tuning large pretrained models like BERT can be a very brittle process. If you're curious about this, this paper is for you! <LINK>\n\nWork with the amazing @JesseDodge, @royschwartz02, Ali Farhadi, @HannaHajishirzi &amp; @nlpnoah\n\n1/n <LINK>"", 'When finetuning, we find large variance in performance across trials, changing only random seeds. With a large set of experiments and without changing any hyperparameter values, we find substantial gains in performance (e.g. 7% absolute improvement on RTE and CoLA!)\n\n2/n', ""We show how the number of experiments impact the expected performance of the best model (https://t.co/2Ak745Ahv8), finding that in some datasets, even with hundreds of experiments, performance still hasn't fully converged\n\n3/n"", 'This demonstrates how model comparisons\nthat only take into account performance in a leaderboard (and not number of experiments or compute) can be misleading. We hope this reminds the community of the value of more rigorous reporting practices \n\n3/n', 'We further explore how weight intialization of the fine-tuning layer (only 0006% of weights!) and the order of the training data affects performance, finding that they have comparable effect on the variance across trials. \n\n4/n', 'We find that some training data orders and some weight initializations are consistently bettern than others, and some weight initilizations perform well across all studied tasks\n\n5/n', 'We include a simple early stopping algorithm with recommendations for practictioners on how to use it, in order to make best possible use of the computational resources one has\n\n6/n', ""Finally, we'll release all of our experimental data, with 2100 fine-tuning episodes, with training curves and multiple evaluations per epoch. We hope this will encourage the community to further explore fine-tuning dynamics, which has become ubiquitous in the last few years.\n\nn/n""]",https://arxiv.org/abs/2002.06305,"Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning. ","Fine-Tuning Pretrained Language Models: Weight Initializations, Data
  Orders, and Early Stopping"
131,1229734858341134338,2880029134,Marco Pangallo,"['Really happy to share *a first version* of my new paper! <LINK>. I use non-linear dynamics &amp; network theory to address two long-standing puzzles in macroeconomics: how business cycles originate from sectoral fluctuations, and how they synchronize internationally. <LINK>', ""@cokemp Hi Jorge, thanks for your comments. I'm mentioning all these things in the intro and lit review (apart from the Atalay paper, thanks!); sync of endogenous dynamics is a complement, rather than a substitute, for the existing answers to the puzzles!""]",https://arxiv.org/abs/2002.06555,"Comovement of economic activity across sectors and countries is a defining feature of business cycles. However, standard models that attribute comovement to propagation of exogenous shocks struggle to generate a level of comovement that is as high as in the data. In this paper, we consider models that produce business cycles endogenously, through some form of non-linear dynamics---limit cycles or chaos. These models generate stronger comovement, because they combine shock propagation with synchronization of endogenous dynamics. In particular, we study a demand-driven model in which business cycles emerge from strategic complementarities across sectors in different countries, synchronizing their oscillations through input-output linkages. We first use a combination of analytical methods and extensive numerical simulations to establish a number of theoretical results. We show that the importance that sectors or countries have in setting the common frequency of oscillations depends on their eigenvector centrality in the input-output network, and we develop an eigendecomposition that explores the interplay between non-linear dynamics, shock propagation and network structure. We then calibrate our model to data on 27 sectors and 17 countries, showing that synchronization indeed produces stronger comovement, giving more flexibility to match the data. ",Synchronization of endogenous business cycles
132,1229623648983515136,140933540,Kuldeep S. Meel,"[""New Paper: Sampling-based approach for quantittive quantitativeverification of Deep Neural Nets. (with @teobaluta , Z.L. Chua, and P. Saxena). We propose a new attack agnostic metric adversarial hardness to capture model's robustness: <LINK>""]",https://arxiv.org/abs/2002.06864,"Despite the functional success of deep neural networks (DNNs), their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep neural networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep neural networks, with provable guarantees, even where testers only have black-box access to the neural network. ",Scalable Quantitative Verification For Deep Neural Networks
133,1229621730785755137,918851521416134656,Chin-Wei Huang,"['Check out my new paper ""Augmented Normalizing Flows""! \n<LINK>\n\njoint work with @laurent_dinh and @AaronCourville <LINK>', '@laurent_dinh @AaronCourville Upshot: your VAE is secretly a flow and you should treat it like one ;) https://t.co/dg9I57PGXx', '@laurent_dinh @AaronCourville Like flows, ANFs can be used to perform latent-space interpolation. https://t.co/USdAmivfbl', '@laurent_dinh @AaronCourville Like VAEs, ANFs can have multiple lossy representations and reconstructions. https://t.co/PloX9T3Z2r']",https://arxiv.org/abs/2002.07101,"In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate state-of-the-art performance on standard benchmarks of flow-based generative modeling. ","Augmented Normalizing Flows: Bridging the Gap Between Generative Flows
  and Latent Variable Models"
134,1228253588771737601,971035265933479936,Josu C. Aurrekoetxea,"['New paper with Thomas Helfer (@Thomas_Italy) and Eugene Lim (@tukohbin)! Coherent Gravitational Waveforms and Memory from Cosmic String Loops \n \n<LINK> <LINK>', '@Thomas_Italy @tukohbin We construct,  for the first time,  the time-domain gravitational wave strain waveform from the collapse  of  a  strongly  gravitating  Abelian  Higgs  cosmic  string  loop  in  full  general  relativity.\n \nHere is a summary video we put on YouTube: https://t.co/lLoi5r7JHl', '@Thomas_Italy @tukohbin We found  that  the  strain  exhibits  a  large  memory  effect  during  merger,  ending  with  a  burst  and  the characteristic ringdown as a black hole is formed. https://t.co/idZkuyBOio', '@Thomas_Italy @tukohbin We think the nature  of  this  memory  arises  from  the fact that post-merger, there is a loss of matter emitted axially in ultra-relativistic jets – and hence is highly aspherical.\n\nCheck these jets in this video: https://t.co/qPJoIwzBnx', '@Thomas_Italy @tukohbin We also investigated the waveform and energy emitted  as a function of string width, loop radius and string  tension Gμ and we found that while it doesnt show a strong dependence on the width and loop radius, the lighter the strings (lower Gμ), the **more** GWs! https://t.co/lCUZDISJ5h', '@Thomas_Italy @tukohbin These are ultra-relativistic events, the BH forms when the loop is moving at speed v &gt; 0.99c (see the Lorentz contraction in the previous video). We believe that E_GW is dominated by kinematics since lower tension loops collapse at higher velocities (\\gamma &gt; 40!)']",https://arxiv.org/abs/2002.05177,"We construct, for the first time, the time-domain gravitational wave strain waveform from the collapse of a strongly gravitating Abelian Higgs cosmic string loop in full general relativity. We show that the strain exhibits a large memory effect during merger, ending with a burst and the characteristic ringdown as a black hole is formed. Furthermore, we investigate the waveform and energy emitted as a function of string width, loop radius and string tension $G\mu$. We find that the mass normalized gravitational wave energy displays a strong dependence on the inverse of the string tension $E_{\mathrm{GW}}/M_0\propto 1/G\mu$, with $E_{\mathrm{GW}}/M_0 \sim {\cal O}(1)\%$ at the percent level, for the regime where $G\mu\gtrsim10^{-3}$. Conversely, we show that the efficiency is only weakly dependent on the initial string width and initial loop radii. Using these results, we argue that gravitational wave production is dominated by kinematical instead of geometrical considerations. ",Coherent Gravitational Waveforms and Memory from Cosmic String Loops
135,1228180085657591808,4666231375,Konstantin Batygin,"[""Our new paper on the solar system's infancy is out: <LINK> Based on the dynamical structure of the cold classical Kuiper belt, we conclude that the closest approach of a passing star within the solar system’s birth cluster must have been greater than ~240 AU! <LINK>"", ""@planefag I'm partial to disk-torquing by a primordial binary stellar  companion as a plausible explanation for the solar obliquity."", '@ChristianPeel I think in-situ formation of P9 is unlikely even in absence of this argument. But one plausible story for P9 formation is scattering off of Jupiter, followed by modification of the orbit by the cluster gravity. For this narrative, the derived constraints are very relevant.', '@astrokiwi no -- saw the DDA talk a while back, but was under the impression that the concentration was on the ~80AU warp previously reported by Volk &amp; Malhotra. Just found it on ads, and looks like Christa+ get ~1.7 deg for the cold belt as do Brown &amp; Pan. Thanks for pointing it out.', ""@eringreeson Wonderful! Welcome back to 'dena. Hope to see you again soon!""]",https://arxiv.org/abs/2002.05656,"Most planetary systems -- including our own -- are born within stellar clusters, where interactions with neighboring stars can help shape the system architecture. This paper develops an orbit-averaged formalism to characterize the cluster's mean-field effects as well as the physics of long-period stellar encounters. Our secular approach allows for an analytic description of the dynamical consequences of the cluster environment on its constituent planetary systems. We analyze special cases of the resulting Hamiltonian, corresponding to eccentricity evolution driven by planar encounters, as well as hyperbolic perturbations upon dissipative disks. We subsequently apply our results to the early evolution of our solar system, where the cluster's collective potential perturbs the solar system's plane, and stellar encounters act to increase the velocity dispersion of the Kuiper belt. Our results are two-fold: first, we find that cluster effects can alter the mean plane of the solar system by $\lesssim1\deg$, and are thus insufficient to explain the $\psi\approx6\deg$ obliquity of the sun. Second, we delineate the extent to which stellar flybys excite the orbital dispersion of the cold classical Kuiper belt, and show that while stellar flybys may grow the cold belt's inclination by the observed amount, the resulting distribution is incompatible with the data. Correspondingly, our calculations place an upper limit on the product of the stellar number density and residence time of the sun in its birth cluster, $\eta\,\tau\lesssim2\times10^4\,$Myr/pc$^3$. ","Dynamics of Planetary Systems Within Star Clusters: Aspects of the Solar
  System's Early Evolution"
136,1227303297880657921,2733642475,Soheil Feizi,"['Check out our new work on ""Curse of Dimensionality on Randomized Smoothing"". This is a joint work with\xa0@tomgoldsteincs and\xa0@umdcs students\xa0Aounon Kumar and Alex Levine. \nPaper: <LINK>\nWe have three main results (see the sub-tweets): <LINK>', '(1) We show that the robustness radius of almost any i.i.d. smoothing distribution to defend against L_p attacks decreases as 1/(d^(1/2-1/p)). Note that unlike the standard case of L_2, for p&gt;2, this decreases with the input dimension d. https://t.co/Ssdu366X0y', '(2) Using an isotropic Gaussian smoothing as in\xa0Cohen @deepcohen , Kolter \n@zicokolte et al.\xa0is almost as good as any other\xa0i.i.d. smoothing distributions for p&gt;2 within a constant factor of the robustness radius. The gap becomes tighter by using generalized Gaussian dists https://t.co/HwBsTyRL8c', '(3) We show that for smoothing over an L_{\\infty} ball, this dependency to the input dimension d becomes worst as 1/d^(1-1/p) https://t.co/ih2ipb4CMB']",https://arxiv.org/abs/2002.03239,"Randomized smoothing, using just a simple isotropic Gaussian distribution, has been shown to produce good robustness guarantees against $\ell_2$-norm bounded adversaries. In this work, we show that extending the smoothing technique to defend against other attack models can be challenging, especially in the high-dimensional regime. In particular, for a vast class of i.i.d.~smoothing distributions, we prove that the largest $\ell_p$-radius that can be certified decreases as $O(1/d^{\frac{1}{2} - \frac{1}{p}})$ with dimension $d$ for $p > 2$. Notably, for $p \geq 2$, this dependence on $d$ is no better than that of the $\ell_p$-radius that can be certified using isotropic Gaussian smoothing, essentially putting a matching lower bound on the robustness radius. When restricted to {\it generalized} Gaussian smoothing, these two bounds can be shown to be within a constant factor of each other in an asymptotic sense, establishing that Gaussian smoothing provides the best possible results, up to a constant factor, when $p \geq 2$. We present experimental results on CIFAR to validate our theory. For other smoothing distributions, such as, a uniform distribution within an $\ell_1$ or an $\ell_\infty$-norm ball, we show upper bounds of the form $O(1 / d)$ and $O(1 / d^{1 - \frac{1}{p}})$ respectively, which have an even worse dependence on $d$. ","Curse of Dimensionality on Randomized Smoothing for Certifiable
  Robustness"
137,1227220079659454464,1001064375229304832,Hongyang Zhang,"['New work with Avrim Blum, Travis Dick, Naren Manoj.\nPaper: <LINK>…\nCode: <LINK>…\nComments welcome!\n\nWe show that random smoothing---a SOTA defense with certified L_2 robustness---might be unable to certify L_p robustness for p&gt;2. Intuition: <LINK> <LINK>']",https://arxiv.org/abs/2002.03517,"We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the $\ell_p$ ball of radius $\epsilon$ when $p>2$. Although random smoothing has been well understood for the $\ell_2$ case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of $p>2$. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the $\ell_\infty$ threat model. In this work, we show that any noise distribution $\mathcal{D}$ over $\mathbb{R}^d$ that provides $\ell_p$ robustness for all base classifiers with $p>2$ must satisfy $\mathbb{E}\eta_i^2=\Omega(d^{1-2/p}\epsilon^2(1-\delta)/\delta^2)$ for 99% of the features (pixels) of vector $\eta\sim\mathcal{D}$, where $\epsilon$ is the robust radius and $\delta$ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in $[0,255]$, the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers. ","Random Smoothing Might be Unable to Certify $\ell_\infty$ Robustness for
  High-Dimensional Images"
138,1227056583030558721,1890694861,Rose Yu,"['Want to improve the generalization of #DeepLearning for #TimeSeries prediction? Check out our new paper on incorporating #symmetries into deep dynamics models \n\n<LINK>\n\nLed by awesome @Rayw1521 and postdoc @Northeastern', '@KyleCranmer @Rayw1521 @Northeastern thank you! would love to hear your comments!', '@guyvdb @Rayw1521 @Northeastern exactly :)']",https://arxiv.org/abs/2002.03061,"Recent work has shown deep learning can accelerate the prediction of physical dynamics relative to numerical solvers. However, limited physical accuracy and an inability to generalize under distributional shift limit its applicability to the real world. We propose to improve accuracy and generalization by incorporating symmetries into convolutional neural networks. Specifically, we employ a variety of methods each tailored to enforce a different symmetry. Our models are both theoretically and experimentally robust to distributional shift by symmetry group transformations and enjoy favorable sample complexity. We demonstrate the advantage of our approach on a variety of physical dynamics including Rayleigh B\'enard convection and real-world ocean currents and temperatures. Compared with image or text applications, our work is a significant step towards applying equivariant neural networks to high-dimensional systems with complex dynamics. We open-source our simulation, data, and code at \url{this https URL}. ","Incorporating Symmetry into Deep Dynamics Models for Improved
  Generalization"
139,1226689089119956992,280403336,Sean Welleck,"['new paper\n\n""Consistency of a Recurrent Language Model\nWith Respect to Incomplete Decoding"" <LINK>\n\nwe show that common decoding algorithms can yield infinite-length, zero-probability strings from neural LMs♾\n\nw/@uralik1, Jaedeok Kim, Richard Pang, @kchonyc (1/6) <LINK>', 'neural LMs can produce degenerate, repetitive text that appears to never terminate \n\nWhat is going on? (2/6) https://t.co/xRfdv2MtXv', ""we show that typical recurrent LMs place all of their probability mass on _finite_ strings - the model's distribution is 'consistent'\n\nThen how can we receive an infinite-length string? (3/6) https://t.co/5ww23nbNF0"", ""We show the _decoding algorithm_ can induce an inconsistent distribution:\n\nwhen the decoding algorithm is 'incomplete', discarding tokens at each step (e.g. greedy, beam, top-k, nucleus),\nit could always discard &lt;eos&gt;, resulting in♾(4/6) https://t.co/U5JHUM1TNT"", 'We show that inconsistency occurs in practice, resulting in non-terminating decoded sequences (5/6) https://t.co/AH7YZAuQa5', ""And propose variants of top-k and nucleus sampling, as well as a 'self-terminating' language model that prevents inconsistency\n\n(6/6) https://t.co/2upYhkg92d""]",https://arxiv.org/abs/2002.02492,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency. ","Consistency of a Recurrent Language Model With Respect to Incomplete
  Decoding"
140,1225317025498763264,2438091938,Rasmus Pagh ☮ 🇺🇦,"['New paper on summation with pure differential privacy in the shuffled model: We show that with log(n)  messages per user one can get error independent of number n of users, and that at least sqrt(log n) messages is needed to get reasonable error. <LINK>', 'This seems to be the first result placing the O(1)-message shuffled model firmly between the local and central models of differential privacy.']",https://arxiv.org/abs/2002.01919,"The shuffled (aka anonymous) model has recently generated significant interest as a candidate distributed privacy framework with trust assumptions better than the central model but with achievable errors smaller than the local model. We study pure differentially private (DP) protocols in the shuffled model for summation, a basic and widely used primitive: - For binary summation where each of n users holds a bit as an input, we give a pure $\epsilon$-DP protocol for estimating the number of ones held by the users up to an error of $O_\epsilon(1)$, and each user sends $O_\epsilon(\log n)$ messages each of 1 bit. This is the first pure protocol in the shuffled model with error $o(\sqrt{n})$ for constant $\epsilon$. Using this protocol, we give a pure $\epsilon$-DP protocol that performs summation of real numbers in $[0, 1]$ up to an error of $O_{\epsilon}(1)$, and where each user sends $O_{\epsilon}(\log^3 n)$ messages each of $O(\log\log n)$ bits. - In contrast, we show that for any pure $\epsilon$-DP protocol for binary summation in the shuffled model having absolute error $n^{0.5-\Omega(1)}$, the per user communication has to be at least $\Omega_{\epsilon}(\sqrt{\log n})$ bits. This implies the first separation between the (bounded-communication) multi-message shuffled model and the central model, and the first separation between pure and approximate DP protocols in the shuffled model. To prove our lower bound, we consider (a generalization of) the following question: given $\gamma$ in $(0, 1)$, what is the smallest m for which there are two random variables $X^0, X^1$ supported on $\{0, \dots ,m\}$ such that (i) the total variation distance between $X^0$ and $X^1$ is at least $1-\gamma$, and (ii) the moment generating functions of $X^0$ and $X^1$ are within a constant factor of each other everywhere? We show that the answer is $m = \Theta(\sqrt{\log(1/\gamma)})$. ",Pure Differentially Private Summation from Anonymous Messages
141,1225232542187413504,710917213994102784,Krishna D N,"['I am very excited to share our new research paper ""Identification of Indian Languages Using Ghost-VLAD Pooling"" by Krishna D N, Ankita Patil , MSP Raja, Sai Prasad H S and Prabhu Aashish Garapati. Here is the link to our paper <LINK>']",https://arxiv.org/abs/2002.01664,"In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features. ",Identification of Indian Languages using Ghost-VLAD pooling
142,1224611468517105666,1570476014,Yi-Hsuan Yang,"['""Pop Music Transformer: Generating Music with Rhythm and Harmony""  \n\\paper: <LINK>\n\\demo: <LINK>\n\\code: <LINK>\n\n```We propose a new event representation of music that make it easy for models to count the beats```\n#TaiwanAILabs', ""@chrisdonahuey Thanks! The tracker works fairly well for pop music.  We haven't tried other genres yet.""]",https://arxiv.org/abs/2002.00212,"A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models. ","Pop Music Transformer: Beat-based Modeling and Generation of Expressive
  Pop Piano Compositions"
143,1224514253731827713,247800333,Ahmad طه,"['🚨New Publication: We take an 80 year old problem in water systems research and dissect the hell out of it: This paper presents an ultra-scalable and simple algorithm to solve the Water Flow Problem (WFP) *with convergence guarantees.* \narXiv link: <LINK>', 'The paper just got accepted for publication in the Water Resources Research journal: one of the top publications in water resources. I am super excited about this. \nOfficial link: https://t.co/cgN7SOr9By', 'So, who cares, you might be asking? Well, for decades, 100s of algorithms have been developed to solve the WFP. But this paper offers more than just another algorithm. It offers insights into *why* a solution exists, and produces mild conditions for such a solution to exist.', 'It goes without saying that most of the credit goes to my student Shen Wang for the incredible work. The three anonymous reviews were fantastic and incredibly insightful.', ""Few more points:\n- The assumptions made in the paper hold after testing them on tens of realistic water network models &amp; conditions.\n- Code's on Github.\n- This work, just like all of our work, will never be devoid of errors. See something? Say something. We're happy to learn. :)"", '@theEnergyMads Thanks Mads! Let me know if you have questions.']",https://arxiv.org/abs/2002.00270,"Addressing challenges in urban water infrastructure systems including aging infrastructure, supply uncertainty, extreme events, and security threats, depend highly on water distribution networks modeling emphasizing the importance of realistic assumptions, modeling complexities, and scalable solutions. In this study, we propose a derivative-free, linear approximation for solving the network water flow problem (WFP). The proposed approach takes advantage of the special form of the nonlinear head loss equations and, after the transformation of variables and constraints, the WFP reduces to a linear optimization problem that can be efficiently solved by modern linear solvers. Ultimately, the proposed approach amounts to solving a series of linear optimization problems. We demonstrate the proposed approach through several case studies and show that the approach can model arbitrary network topologies and various types of valves and pumps, thus providing modeling flexibility. Under mild conditions, we show that the proposed linear approximation converges. We provide sensitivity analysis and discuss in detail the current limitations of our approach and suggest solutions to overcome these. All the codes, tested networks, and results are freely available on Github for research reproducibility. ","A New Derivative-Free Linear Approximation for Solving the Network Water
  Flow Problem with Convergence Guarantees"
144,1236913875804536834,1211825303388995584,Russell Tsuchida,"['Back in February, @Tea_Pearce, Chris van der Heide, Fred Roosta, @marcus_marcusg and I did some work on the kernels of infinitely wide deep neural networks with GELU and ELU activations. We also studied the fixed points of these kernels. Check it out here:\n<LINK>']",https://arxiv.org/abs/2002.08517,"Analysing and computing with Gaussian processes arising from infinitely wide neural networks has recently seen a resurgence in popularity. Despite this, many explicit covariance functions of networks with activation functions used in modern networks remain unknown. Furthermore, while the kernels of deep networks can be computed iteratively, theoretical understanding of deep kernels is lacking, particularly with respect to fixed-point dynamics. Firstly, we derive the covariance functions of multi-layer perceptrons (MLPs) with exponential linear units (ELU) and Gaussian error linear units (GELU) and evaluate the performance of the limiting Gaussian processes on some benchmarks. Secondly, and more generally, we analyse the fixed-point dynamics of iterated kernels corresponding to a broad range of activation functions. We find that unlike some previously studied neural network kernels, these new kernels exhibit non-trivial fixed-point dynamics which are mirrored in finite-width neural networks. The fixed point behaviour present in some networks explains a mechanism for implicit regularisation in overparameterised deep models. Our results relate to both the static iid parameter conjugate kernel and the dynamic neural tangent kernel constructions. Software at github.com/RussellTsuchida/ELU_GELU_kernels. ","Avoiding Kernel Fixed Points: Computing with ELU and GELU Infinite
  Networks"
145,1235063336913387520,2204361480,Arvind Mohan,"['How do you build neural networks for Turbulent flow with physics constraints + boundary conditions directly embedded in the architecture? Here, we embed mass conservation - Read on to find out. welcome feedback and ideas!\n<LINK>']",https://arxiv.org/abs/2002.00021,"In the recent years, deep learning approaches have shown much promise in modeling complex systems in the physical sciences. A major challenge in deep learning of PDEs is enforcing physical constraints and boundary conditions. In this work, we propose a general framework to directly embed the notion of an incompressible fluid into Convolutional Neural Networks, and apply this to coarse-graining of turbulent flow. These physics-embedded neural networks leverage interpretable strategies from numerical methods and computational fluid dynamics to enforce physical laws and boundary conditions by taking advantage the mathematical properties of the underlying equations. We demonstrate results on three-dimensional fully-developed turbulence, showing that this technique drastically improves local conservation of mass, without sacrificing performance according to several other metrics characterizing the fluid flow. ","Embedding Hard Physical Constraints in Neural Network Coarse-Graining of
  3D Turbulence"
146,1235001383356518401,231076444,Vasileios Lioutas,"['Excited about our new paper on sequence learning. In the NLP community, attention is considered a necessity for achieving SoTA results but we propose a fast novel attention-free method with comparative performance <LINK>\n\n#NLProc #DeepLearning #MachineLearning']",https://arxiv.org/abs/2002.03184,"To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches. ",Time-aware Large Kernel Convolutions
147,1234301978156634112,3245949691,Rebecca Leane,"['Galactic Center Excess update! \n<LINK>\n<LINK>\n\nWe discovered a substantial modeling issue that creates fake point source signals. \nIt can mimic properties seen in the excess.\n\nWe find evidence for it in the Fermi data.\n\nThread and backstory:', ""1/ The Galactic Center Excess is an excess of very high energy light radiating from the heart of our Galaxy, detected by the Fermi Gamma-Ray Space Telescope. It was first discovered now over 10 years ago, but we still don't know what's powering the glow."", '2/ One exciting possibility is that it is coming from annihilating dark matter particles. This has been a leading explanation as the excess can have all the features consistent with dark matter: the same energy distribution, same intensity, and (potentially) same morphology.', '3/ The other leading explanation is that the excess is a collection of faint stars. In particular, something called millisecond pulsars. These can be a good explanation because the energy and (potentially) spatial distribution can be comparable to that of the excess.', '4/ We want to be able to tell these hypotheses apart. To do this, we can use the fact that the two can have a distinguishing feature: their light can produce a different-looking image.', '5/ For dark matter, we expect a smooth diffuse glow. This is because it is spread approximately smoothly through a halo.', '6/For the faint stars, they can make more of a clumpy image. This is because the variance across pixels in the sky can be much higher. You can imagine one part of the sky has none of these stars, while another part has several, leading to more light. The image looks more grainy.', '7/ This increased variance (and therefore grainy picture) doesn\'t apply just to pulsars, but to any ""point sources"" of gamma rays. Point sources are just the broader term for individual gamma-ray-emitting sources. Gamma rays are the type of high-energy light in the excess.', '8/ To build up a complete picture of the gamma-ray sky, not only do we need to think about the smooth vs grainy signal, but we also need to include information about what the backgrounds might look like.', '9/ That is, there are other components present in the data (that are not the signal), but that need to be modeled, to make sure the picture is really complete. All of these spatial components, for the signal and the backgrounds, are called ""templates"".', '10/ This technique of building up a picture of the sky in terms of templates, and comparing hypotheses by their smoothness vs. clumpiness is called ""Non-Poissonian Template Fitting"" (NPTF).', '11/ In 2015, it was first developed and used to argue that the excess was better explained by point sources. Lee et al (PRL 2016) found that when given the option, the data prefer a grainy image for the excess, rather than a smooth one, very high statistical significance.', ""12/ But that wasn't all. In 2015, not only did the NPTF find the point source explanation was preferred, but an independent method by another group of authors (Bartels et al, PRL 2016), found consistent results. These results were a massive blow to the dark matter explanation."", '13/ Since then and until last year, the community generally appeared to have accepted that the excess was likely point sources, and not dark matter. What happened last year?\n\nLast year was the year that dark matter struck back.', '14/In our paper (Leane et al, PRL 2019) we showed that when really large artificial dark matter signals were injected into the real data, the NPTF misattributed the signal as coming from point sources. Even given a smooth image, the fit could misinterpret the image as grainy.', '14/ We concluded that dark matter might have been powering the excess after all. However, a number of questions remained. We knew there was some large systematic present that needed to be understood.', ""15/ But, we didn't know what would happen once it was resolved. Was it going to be the point source explanation, the dark matter explanation, or something else entirely?"", '16/ In our new double-paper, we set out to understand what was happening. In our 2019 paper, we noticed that in simulations, generally when we simulated data from a model, and analyzed the simulation with the exact model that was simulated, we got roughly the right answer back.', '17/ That was unlike the real data. This implied there was nothing fundamentally wrong with the NPTF, but rather, there seemed to be a mismatch between the templates and the data. Some component(s) of the gamma-ray sky were clearly being mismodeled.', '18/ To alleviate this, in our new papers, we aimed to simply give the templates more freedom, hoping this would help us find the problem. Specifically, we broke the excess signal template into two pieces: north and south.', '19/ We applied this splitting to both the point source template, and the smooth excess template. What happens to the point source evidence in real data when you do this? \n\nThe evidence for point sources *disappears*!!', '20/ This seems at first really crazy. Just allowing north and south pieces for the excess templates appears to have killed the previously large evidence for point sources in the region.', '21/ We then wondered if there was actually an asymmetry in the excess signal. We tested the smooth components alone, just to check the morphology. The fit strongly prefers asymmetry, with the north about twice as bright as the south: https://t.co/pNp2dfb6Tq', '22/ So the picture is: allowing extra template freedom made the point source evidence disappear. Given the fit seems to want an asymmetry in the region we are looking at, we tested if we could reproduce this overall behavior in simulations.', '23/ So what we did was simulate the best-fit to the real data (an asymmetric excess), and fit the simulated data with the old pipeline: one symmetric point source template, and one symmetric smooth template (plus backgrounds). How does this compare to the real data?', '24/ Real data looks like this (analyzed with old symmetric pipeline):\n(The left and middle panels show the amount of flux in each template; the right panel shows the number of sources as a function of how bright they are.) https://t.co/lLJZIYBe49', '25/ Note that there is no smooth template picked up (left), and instead a point source (PS) flux is picked up. If you let the smooth template go negative, it will (middle). We also see bright sources appearing (right).', '26/ Now, the simulated case. Simulated asymmetry with NO excess point-sources simulated (analyzed with old symmetric pipeline) looks like: https://t.co/J9j3i8LZDG', ""27/ Compare these figures' left, middle, and right panels. We can reproduce the real data *in detail*. What happened? The unmodeled asymmetry had created a spurious point source signal."", '28/ This fake signal is detected with high statistical significance, just like what was previously thought to be evidence for point sources in the real data.', '29/ It even makes (apparent) bright point sources, well above the flux where the smooth signal and point source signal could be degenerate. The fake signal mimics the real data in many aspects.', '30/ So why *fundamentally* is this happening? We sought to answer that in detail in the second, longer paper. We show that this behaviour of creating spurious point source signals can be broadly expected when you have mismodeled templates.', '31/ This is because the basic feature you are looking for, when identifying a point source signal, is a larger variance signal. I.e., pixel to pixel, you expect more variance. This is *also* a fundamental feature of *mismodeling*.', '32/ If it turns out that you are modeling the data in a way that is not quite right, the variance of the data relative to your incorrect model will be inflated. This appears to be a reason for caution with NPTF analyses in general.', ""33/ We don't currently know how to model the inner Galaxy perfectly, and any of our current mismodelling- be it in detector acceptance, energy / angular resolution, or template morphology - could be interpreted instead as evidence for point sources."", '34/ How bad your mismodeling is, can correlate with how bright the fake point sources are.', '35/ We also sought to understand the broader implications for our work. We found that in larger regions of the sky, when changing the background model, sometimes the asymmetry in the excess disappeared.', ""36/ As such, it's not yet clear if the asymmetry is an intrinsic feature of the excess. It could be background mismodeling transferring to the signal."", ""37/ A final important question is whether evidence for point sources in larger regions, where the asymmetry isn't always detected, should be taken as robust. We argue such results are also in question, and this is because:"", '1. We performed our main study in the region within 10 degrees of the Galactic Center. This is where the excess signal is brightest and most significantly detected. \n\n2. Any larger region contains our region as a subset.', '3. The inferred source-count function (number of sources as a function of their brightness) that we know is *fake* in our region, is consistent with that detected in larger regions. It would be a co-incidence if one was fake, and one was real, and they looked the same.', ""38/ So, where does that leave our understanding of the excess? What is the excess?? We still don't know! Our argument is that we should still consider non-point-source explanations, and we need to improve models of the signal and backgrounds."", '39/ For now, it still really could be dark matter, pulsars, or something else! Lots of interesting work still to be done.', 'I also want to say thanks to my awesome collaborator on these two papers, Tracy Slatyer!!', '@SunnyVagnozzi Thanks Sunny! :)', '@R_Trotta @JohnEvansTW1 Thanks for the nice comments! Agree its quite an interesting situation, going to be great to see how all this turns out in the end.']",https://arxiv.org/abs/2002.12370,"We re-examine evidence that the Galactic Center Excess (GCE) originates primarily from point sources (PSs). We show that in our region of interest, non-Poissonian template fitting (NPTF) evidence for GCE PSs is an artifact of unmodeled north-south asymmetry of the GCE. This asymmetry is strongly favored by the fit (although it is unclear if this is physical), and when it is allowed, the preference for PSs becomes insignificant. We reproduce this behavior in simulations, including detailed properties of the spurious PS population. We conclude that NTPF evidence for GCE PSs is highly susceptible to certain systematic errors, and should not at present be taken to robustly disfavor a dominantly smooth GCE. ",Spurious Point Source Signals in the Galactic Center Excess
148,1233160196647579648,1772480042,Yitao Liang,"['We propose ADAC. By constraining behavior actors/critics w.r.t their target counterparts, we achieve effective exploration and stable updates with theoretical guarantees. In addition, ADAC naturally incorporates intrinsic rewards. <LINK> (2/2)']",https://arxiv.org/abs/2002.10738,"Off-policy reinforcement learning (RL) is concerned with learning a rewarding policy by executing another policy that gathers samples of experience. While the former policy (i.e. target policy) is rewarding but in-expressive (in most cases, deterministic), doing well in the latter task, in contrast, requires an expressive policy (i.e. behavior policy) that offers guided and effective exploration. Contrary to most methods that make a trade-off between optimality and expressiveness, disentangled frameworks explicitly decouple the two objectives, which each is dealt with by a distinct separate policy. Although being able to freely design and optimize the two policies with respect to their own objectives, naively disentangling them can lead to inefficient learning or stability issues. To mitigate this problem, our proposed method Analogous Disentangled Actor-Critic (ADAC) designs analogous pairs of actors and critics. Specifically, ADAC leverages a key property about Stein variational gradient descent (SVGD) to constraint the expressive energy-based behavior policy with respect to the target one for effective exploration. Additionally, an analogous critic pair is introduced to incorporate intrinsic rewards in a principled manner, with theoretical guarantees on the overall learning stability and effectiveness. We empirically evaluate environment-reward-only ADAC on 14 continuous-control tasks and report the state-of-the-art on 10 of them. We further demonstrate ADAC, when paired with intrinsic rewards, outperform alternatives in exploration-challenging tasks. ","Off-Policy Deep Reinforcement Learning with Analogous Disentangled
  Exploration"
149,1232950775824965632,883039700,Lenka Zdeborova,"['You would think that supervised classification of two Gaussian clusters is as simple as it can get. Yet we find a bunch of surprises. The maximum likelihood works bad, infinitely regularized ERM matches the Bayes-optimal performance, and more in <LINK>', '@unsorsodicorda @iacopo_poli But we have no label noise ...', '@unsorsodicorda @iacopo_poli In our definition the oracle knows the centroids, but this is not enough to estimate all the labels correctly as the variance of the two Gaussians is so large that a fraction of the data points is closer to the ""wrong"" centroid.']",https://arxiv.org/abs/2002.11544,"We consider a high-dimensional mixture of two Gaussians in the noisy regime where even an oracle knowing the centers of the clusters misclassifies a small but finite fraction of the points. We provide a rigorous analysis of the generalization error of regularized convex classifiers, including ridge, hinge and logistic regression, in the high-dimensional limit where the number $n$ of samples and their dimension $d$ go to infinity while their ratio is fixed to $\alpha= n/d$. We discuss surprising effects of the regularization that in some cases allows to reach the Bayes-optimal performances. We also illustrate the interpolation peak at low regularization, and analyze the role of the respective sizes of the two clusters. ","The role of regularization in classification of high-dimensional noisy
  Gaussian mixture"
150,1232356540616568832,962886465502851072,Maral Khosroshahi,"['Read our paper “Training Large Neural Networks with Constant Memory using a New Execution Algorithm”.\nWe propose an L2L algorithm that allows you to run transformer-based models on a single GPU with a large batch size requiring less memory.\n#MicrosoftLife \n<LINK> <LINK>', 'We have used #BERT as an example and show that our algorithm is able to fit a 96 layer BERT on a single V100 with batch size of 32, requiring only 11GB memory while the original model with only 24 layers requires at least 10GB for a maximum batch size of 2.', 'The constant-memory nature of this approach allows to scale to arbitrary depth in the number of layers. We enable developers to run large models on affordable hardware. Also, each layer can be structurally agnostic to another, encouraging dynamic modeling approaches such as #NAS.', 'We used the popular @huggingface library as a baseline for development and testing. The L2L version of the BERT-large model and the EPS will soon be available in open source.']",https://arxiv.org/abs/2002.05645,"Widely popular transformer-based NLP models such as BERT and Turing-NLG have enormous capacity trending to billions of parameters. Current execution methods demand brute-force resources such as HBM devices and high speed interconnectivity for data parallelism. In this paper, we introduce a new relay-style execution technique called L2L (layer-to-layer) where at any given moment, the device memory is primarily populated only with the executing layer(s)'s footprint. The model resides in the DRAM memory attached to either a CPU or an FPGA as an entity we call eager param-server (EPS). To overcome the bandwidth issues of shuttling parameters to and from EPS, the model is executed a layer at a time across many micro-batches instead of the conventional method of minibatches over whole model. L2L is implemented using 16GB V100 devices for BERT-Large running it with a device batch size of up to 256. Our results show 45% reduction in memory and 40% increase in the throughput compared to the state-of-the-art baseline. L2L is also able to fit models up to 50 Billion parameters on a machine with a single 16GB V100 and 512GB CPU memory and without requiring any model partitioning. L2L scales to arbitrary depth allowing researchers to develop on affordable devices which is a big step toward democratizing AI. By running the optimizer in the host EPS, we show a new form of mixed precision for faster throughput and convergence. In addition, the EPS enables dynamic neural architecture approaches by varying layers across iterations. Finally, we also propose and demonstrate a constant memory variation of L2L and we propose future enhancements. This work has been performed on GPUs first, but also targeted towards all high TFLOPS/Watt accelerators. ","Training Large Neural Networks with Constant Memory using a New
  Execution Algorithm"
151,1232319835528400896,732544572723695616,Kris Cao,"[""New paper: <LINK>. @DaniYogatama and I investigate multi-task learning for natural language generation. We find that modelling latent 'skills' (that is, a per-datapoint latent variable) helps prevent task interference."", '@DaniYogatama However, this effect only happens if we structure the latent space with task knowledge: attempting to induce latent skills directly does not help prevent task interference. (2/n)', '@DaniYogatama A side product of of having a latent skill space is that we can do few-shot adaptation in the latent space only, rather than adapting all model parameters. We show that this few-shot adaptation method is more robust to hyperparameter choice and performs comparably to SGD. (3/3)']",https://arxiv.org/abs/2002.09543,"We present a generative model for multitask conditional language generation. Our guiding hypothesis is that a shared set of latent skills underlies many disparate language generation tasks, and that explicitly modelling these skills in a task embedding space can help with both positive transfer across tasks and with efficient adaptation to new tasks. We instantiate this task embedding space as a latent variable in a latent variable sequence-to-sequence model. We evaluate this hypothesis by curating a series of monolingual text-to-text language generation datasets - covering a broad range of tasks and domains - and comparing the performance of models both in the multitask and few-shot regimes. We show that our latent task variable model outperforms other sequence-to-sequence baselines on average across tasks in the multitask setting. In the few-shot learning setting on an unseen test dataset (i.e., a new task), we demonstrate that model adaptation based on inference in the latent task space is more robust than standard fine-tuning based parameter adaptation and performs comparably in terms of overall performance. Finally, we examine the latent task representations learnt by our model and show that they cluster tasks in a natural way. ",Modelling Latent Skills for Multitask Language Generation
152,1231863479293874177,57640264,Ariane Nunes Alves,"['.@Rebecca_Wade_C , @DariaKokh  and I wrote a review of computational methods to study ligand-protein binding kinetics. We assessed the performance of methods considering two benchmark systems, T4 lysozyme (T4L) and N-HSP90. (1/3)\n@HITStudies  #compchem\n<LINK> <LINK>', '@Rebecca_Wade_C @DariaKokh @HITStudies The figure below shows the times required for different methods to simulate complexes with different exp. residence times (RT). In the last two years many methods to compute relative RT were published. An increase of four orders of magnitude in exp. RT leads to a smaller,  (2/3) https://t.co/xfMw29mTgi', '@Rebecca_Wade_C @DariaKokh @HITStudies one order of magnitude increase in comp. time.\nFor T4L, results indicate that good RT estimation can be achieved without exhaustive path sampling so long as the most probable paths are sampled (table below).\nAnother highlight: two works aiming at computing RT prospectively. (3/3) https://t.co/Eaom4mwQPN', '@_Maicol_ @Rebecca_Wade_C @DariaKokh @HITStudies thanks!\n😊', '@sowmyaindrakum1 @Rebecca_Wade_C @DariaKokh @HITStudies what i showed you in your visit is related, but not the same thing.', '@Jmondal_tifrh @Rebecca_Wade_C @DariaKokh @HITStudies thanks!\n😊', '@Herr_Flow @Rebecca_Wade_C @DariaKokh @HITStudies you are welcome!\n😀\nis the arXiv paper published? send me the link, so I can correct the citation later. our work is under review on COSB right now.']",https://arxiv.org/abs/2002.08983,"Due to the contribution of drug-target binding kinetics to drug efficacy, there is a high level of interest in developing methods to predict drug-target binding kinetic parameters. During the review period, a wide range of enhanced sampling molecular dynamics simulation-based methods has been developed for computing drug-target binding kinetics and studying binding and unbinding mechanisms. Here, we assess the performance of these methods considering two benchmark systems in detail: mutant T4 lysozyme-ligand complexes and a large set of N-HSP90-inhibitor complexes. The results indicate that some of the simulation methods can already be usefully applied in drug discovery or lead optimization programs but that further studies on more high-quality experimental benchmark datasets are necessary to improve and validate computational methods. ","Recent progress in molecular simulation methods for drug binding
  kinetics"
153,1231209384396632066,632724692,Renaud Lambiotte,"['I hope that you will enjoy, as much as we did, to find out the connections between pitchfork transitions and the effective resistance in networks : <LINK>']",https://arxiv.org/abs/2002.08408,"We study a non-linear dynamical system on networks inspired by the pitchfork bifurcation normal form. The system has several interesting interpretations: as an interconnection of several pitchfork systems, a gradient dynamical system and the dominating behaviour of a general class of non-linear dynamical systems. The equilibrium behaviour of the system exhibits a global bifurcation with respect to the system parameter, with a transition from a single constant stationary state to a large range of possible stationary states. Our main result classifies the stability of (a subset of) these stationary states in terms of the effective resistances of the underlying graph; this classification clearly discerns the influence of the specific topology in which the local pitchfork systems are interconnected. We further describe exact solutions for graphs with external equitable partitions and characterize the basins of attraction on tree graphs. Our technical analysis is supplemented by a study of the system on a number of prototypical networks: tree graphs, complete graphs and barbell graphs. We describe a number of qualitative properties of the dynamics on these networks, with promising modeling consequences. ",Non-linear network dynamics with consensus-dissensus bifurcation
154,1230358061883297793,23980621,"Brett Morris, PhD","[""New paper! ESA's PLATO mission will hunt for Earths orbiting Sun-like stars. Sun-like stars vary in brightness due to p-mode oscillations and granulation. We study how these phenomena affect planet radius precision, and the results may surprise you:\n\n<LINK> <LINK>"", ""Stellar granulation is the pattern of hot, bright upwelling plasma surrounded by cooler and dimmer down-flowing plasma which tiles the surface of a Sun-like star. Here's what the Earth would look like in-transit on a *real* image of the solar granulation pattern by SDO's HMI: https://t.co/e43OrIF7bB"", 'One of the neat things we did in this paper was devise a numerically efficient method for simulating transits of real SDO HMI images, with some heroic CPU-efficiency help from  @manodeepsinha at #pyastro that sped up the solar transit simulator code by a few orders of magnitude.', 'Here are 282 simulated transit light curves of an Earth transiting the SDO/HMI images of the quiet Sun (left: full transit, right: zoom into mid-transit). The spread at flux minimum is due to solar surface brightness variations due to granulation and activity. https://t.co/3SlHW8ZVo2', 'Good news: surface granulation of Sun-like stars imparts a flux variation of a few parts per million – too small to affect most exoplanet observations in the present or near-future. (Image credit: #DKIST/NSO/NSF/AURA) https://t.co/H9VrodYXi9', 'But standing pressure waves (p-modes) in the solar atmosphere with periods ~5 minutes are a bigger threat. They impart stochastic O(100 ppm) variations on transit light curves. This is what the full-mission PLATO light curve of the Earth on the Sun would look like due to p-modes: https://t.co/c1iiqt3IRq', '🌟Important🌟: we found that a degeneracy between the impact parameter, limb-darkening, and the exoplanet radius enforce a lower limit on the radius precision near ~3%, which accounts for the full PLATO mission error budget, before accounting for detector/photon noise.', ""If it's possible to constrain the impact parameter or to obtain follow-up observations at longer wavelengths where limb-darkening is less significant, this may enable higher precision radius measurements –\xa0but we'll have to think hard about how to do this efficiently. https://t.co/Yd5hVIJYPS"", 'Big shoutouts to Monica Bobra for bringing her fantastic solar expertise to the project, @AgolEric for insightful transit light curve insights, and the talented undergrad Yu Jin Lee for keeping us inspired on this project.', ""@exoZafar It'd be awfully hard –\xa0the p-mode signals are &gt;40 peaks in frequency space, each contributing a slightly different, super-imposed oscillation pattern. It would be difficult (though maybe not impossible?) to model directly in the time domain. https://t.co/n3UOAvwBA0"", '@exoZafar The other major problem is that these simulated observations were done *without any\xa0observational/systematic noise* –\xa0when you add a white noise component it becomes much more difficult to uniquely model the oscillations in the time domain.', ""@exoZafar It's a hurdle to reaching the *extreme* radius precision of 3%. You might reasonably ask what we need that kind of precision for. I'd argue that 5% is still good enough to do interesting exoplanet demographics, for example."", ""@exoZafar M dwarfs aren't the focus of PLATO so I've mostly left them out of the discussion. The science reason for this is that the stellar *magnetic activity* (spots, faculae and the like) will be the dominant phenomena shaping the transit residuals."", '@exoZafar Thanks for your enthusiasm! https://t.co/80frHOgHpZ', '@nespinozap Thanks, and yes! See Sect 4.2 for the full discussion, and Appendix Fig A1 for the corner plot. If you had really good constraints on the LD parameters I think the degeneracy would persist.', ""@exohugh @nespinozap I'm pretty sure the stellar density helps you constrain a/R_star, but you can still exchange orbital inclination (or b) for transit duration. Also, I don't know where you'd get a prior on eccentricity from since these targets are RV-inaccessible.""]",https://arxiv.org/abs/2002.08072,"One of the main science motivations for the ESA PLAnetary Transit and Oscillations (PLATO) mission is to measure exoplanet transit radii with 3% precision. In addition to flares and starspots, stellar oscillations and granulation will enforce fundamental noise floors for transiting exoplanet radius measurements. We simulate light curves of Earth-sized exoplanets transiting continuum intensity images of the Sun taken by the HMI instrument aboard SDO to investigate the uncertainties introduced on the exoplanet radius measurements by stellar granulation and oscillations. After modeling the solar variability with a Gaussian process, we find that the amplitude of solar oscillations and granulation is of order 100 ppm -- similar to the depth of an Earth transit -- and introduces a fractional uncertainty on the depth of transit of 0.73% assuming four transits are observed over the mission duration. However, when we translate the depth measurement into a radius measurement of the planet, we find a much larger radius uncertainty of 3.6%. This is due to a degeneracy between the transit radius ratio, the limb-darkening, and the impact parameter caused by the inability to constrain the transit impact parameter in the presence of stellar variability. We find that surface brightness inhomogeneity due to photospheric granulation contributes a lower limit of only 2 ppm to the photometry in-transit. The radius uncertainty due to granulation and oscillations, combined with the degeneracy with the transit impact parameter, accounts for a significant fraction of the error budget of the PLATO mission, before detector or observational noise is introduced to the light curve. If it is possible to constrain the impact parameter or to obtain follow-up observations at longer wavelengths where limb-darkening is less significant, this may enable higher precision radius measurements. ","The Stellar Variability Noise Floor for Transiting Exoplanet Photometry
  with PLATO"
155,1230124242009763840,3433220662,Anthony Bonato,"['New on the @arxiv: ""Probabilistically Faulty Searching on a Half-Line""\n\nWe study p-Faulty Search, where a unit speed robot searches the half-line for a hidden item.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2002.07797,"We study $p$-Faulty Search, a variant of the classic cow-path optimization problem, where a unit speed robot searches the half-line (or $1$-ray) for a hidden item. The searcher is probabilistically faulty, and detection of the item with each visitation is an independent Bernoulli trial whose probability of success $p$ is known. The objective is to minimize the worst case expected detection time, relative to the distance of the hidden item to the origin. A variation of the same problem was first proposed by Gal in 1980. Then in 2003, Alpern and Gal [The Theory of Search Games and Rendezvous] proposed a so-called monotone solution for searching the line ($2$-rays); that is, a trajectory in which the newly searched space increases monotonically in each ray and in each iteration. Moreover, they conjectured that an optimal trajectory for the $2$-rays problem must be monotone. We disprove this conjecture when the search domain is the half-line ($1$-ray). We provide a lower bound for all monotone algorithms, which we also match with an upper bound. Our main contribution is the design and analysis of a sequence of refined search strategies, outside the family of monotone algorithms, which we call $t$-sub-monotone algorithms. Such algorithms induce performance that is strictly decreasing with $t$, and for all $p \in (0,1)$. The value of $t$ quantifies, in a certain sense, how much our algorithms deviate from being monotone, demonstrating that monotone algorithms are sub-optimal when searching the half-line. ",Probabilistically Faulty Searching on a Half-Line
156,1229945825784143872,972555245179064320,Jordy de Vries,"['Very happy this is out! <LINK>. My first paper with a great grad student, Guanghui Zhou, in my group UMass. We study the role of sterile neutrinos in neutrinoless double beta decay. Neutrinos with masses in the MeV-GeV range are particularly tricky to include.']",https://arxiv.org/abs/2002.07182,"We investigate neutrinoless double beta decay ($0\nu\beta\beta$) in the presence of sterile neutrinos with Majorana mass terms. These gauge-singlet fields are allowed to interact with Standard-Model (SM) fields via renormalizable Yukawa couplings as well as higher-dimensional gauge-invariant operators up to dimension seven in the Standard Model Effective Field Theory extended with sterile neutrinos. At the GeV scale, we use Chiral effective field theory involving sterile neutrinos to connect the operators at the level of quarks and gluons to hadronic interactions involving pions and nucleons. This allows us to derive an expression for $0\nu\beta\beta$ rates for various isotopes in terms of phase-space factors, hadronic low-energy constants, nuclear matrix elements, the neutrino masses, and the Wilson coefficients of higher-dimensional operators. The needed hadronic low-energy constants and nuclear matrix elements depend on the neutrino masses, for which we obtain interpolation formulae grounded in QCD and chiral perturbation theory that improve existing formulae that are only valid in a small regime of neutrino masses. The resulting framework can be used directly to assess the impact of $0\nu\beta\beta$ experiments on scenarios with light sterile neutrinos and should prove useful in global analyses of sterile-neutrino searches. We perform several phenomenological studies of $0\nu\beta\beta$ in the presence of sterile neutrinos with and without higher-dimensional operators. We find that non-standard interactions involving sterile neutrinos have a dramatic impact on $0\nu\beta\beta$ phenomenology, and next-generation experiments can probe such interactions up to scales of $\mathcal O(100)$ TeV. ","Sterile neutrinos and neutrinoless double beta decay in effective field
  theory"
157,1229824603683377152,928301283034914817,Lihua Lei,"['Our new work on adaptivity of stochastic gradient methods for smooth nonconvex optimization. <LINK>. We propose Geom-SARAH that achieves unusually strong adaptivity to both target accuracy and PL constant without knowing them! Main technique: Geometrization! 1/5', 'Adaptivity is an important yet under-studied property in stochastic optimization. Theoreticians usually pursue for mathematical optimality at the cost of obtaining specialized procedures in different regimes (e.g. high/low accuracy, strongly convex/non sc, PL/non PL). 2/5', 'However practitioners are usually not readily able to know which regime is appropriate to their problem.\xa0It is ideal to have a ""single"" algorithm that works for many regimes without treaking hyperparameters and without significant performance loss. 3/5', 'Geom-SARAH achieves adaptivity for smooth nonconvex stochastic/finite-sum optimization to both target accuracy \\epsilon and PL constant \\mu (aka strong convexity modulus) with solely the knowledge of the smoothness parameters L! See two tables below. 4/5 https://t.co/xRjhzitpiD', 'The rates in some regimes even match the best achievable ones that require extra knowledge of many other parameters. The main trick is ""geometrization"" coined in our earlier work. Check out https://t.co/2vZXEHqT4n if you\'re interested in the magic of geometric random vars! 5/5']",https://arxiv.org/abs/2002.05359,"Adaptivity is an important yet under-studied property in modern optimization theory. The gap between the state-of-the-art theory and the current practice is striking in that algorithms with desirable theoretical guarantees typically involve drastically different settings of hyperparameters, such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results, such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly without tweaking the hyperparameters. In this work, blending the ""geometrization"" technique introduced by Lei & Jordan 2016 and the \texttt{SARAH} algorithm of Nguyen et al., 2017, we propose the Geometrized \texttt{SARAH} algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity to both the magnitude of the target accuracy and the Polyak-\L{}ojasiewicz (PL) constant if present. In addition, it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming existing algorithms for PL objectives. ",Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization
158,1229546295192412163,4873857094,Dominik Zumbuhl,['On arXiv: Edge State Wave Functions from Tunneling Spectroscopy. We use the lowest five modes of a quantum wire to probe adjacent quantum Hall edge state wave functions with tunneling -- a powerful new technique to study edge state wave functions. \n<LINK> <LINK>'],https://arxiv.org/abs/2002.05301,"We perform momentum-conserving tunneling spectroscopy using a GaAs cleaved-edge overgrowth quantum wire to investigate adjacent quantum Hall edge states. We use the lowest five wire modes with their distinct wave functions to probe each edge state and apply magnetic fields to modify the wave functions and their overlap. This reveals an intricate and rich tunneling conductance fan structure which is succinctly different for each of the wire modes. We self-consistently solve the Poisson-Schr\""odinger equations to simulate the spectroscopy, reproducing the striking fans in great detail, thus confirming the calculations. Further, the model predicts hybridization between wire states and Landau levels, which is also confirmed experimentally. This establishes momentum-conserving tunneling spectroscopy as a powerful technique to probe edge state wave functions. ","Edge State Wave Functions from Momentum-Conserving Tunneling
  Spectroscopy"
159,1229272564729614336,1064069189143601153,Stephan Rasp,"[""Can AI predict weather? Let's find out!\n\nAnnouncing our brand-new benchmark dataset for data-driven weather forecasting: WeatherBench. We prepared the best available data and posed a clear challenge.\n\nData and code: <LINK>\nPaper: <LINK>\n\nThread. <LINK>"", 'Global weather forecasting is done with physical models which are very good for most applications but not so good for specific events. Plus they are expensive.\n\nEssentially weather forecasting is an image-to-image problem. We have data from the past, so why not use AI? https://t.co/ZbLPGMKpKz', 'Three studies have attempted an AI weather forecast in the last couple of years but using different data and metrics making them hard to compare. (The lead authors of all studies are WeatherBench co-authors.)\nhttps://t.co/bccgLVE0CS\nhttps://t.co/9kKI4MywpT\nhttps://t.co/MNG0nGZcqA', 'For WeatherBench, we preprocessed the best available data for past weather (ERA5) to make it easy to use for ML applications, even for non-meteorologists.\n\nThe data is hourly 1979-2018 regridded to three different resolutions [5.625 deg = 32x64, ...,  1.40625 deg = 128x256].', 'The task is to predict geopotential (basically pressure) at 500 hPa height and temperature at 850 hPa height 3/5 days ahead. \n\nWe included physical (operational plus IFS at lower resolution) and data-driven baselines. https://t.co/ZbE860OycF', 'The Github repo contains code to download and process additional data as well as a quickstart notebook (trains a simply convolutional neural network) which can also be launched interactively via @pangeo_data @mybinderteam.\n\nhttps://t.co/KUO3WGf8bI', 'Will we beat operational NWP models? Probably not for the fields we chose (geopotential and temperature). Plus, we still need physical models to provide initial conditions regardless. \n\nSo why even do AI weather forecasting?', 'There are certain weather events (e.g. thunderstorms over Africa) and specific applications (e.g. solar power forecasting) for which physical models struggle. By learning directly from observations maybe data-driven methods can outperform physical models.', ""But first we need to build models that have a solid grasp of atmospheric dynamics. That's what WeatherBench aims at. _Let's do it!_\n\nAlso: many interesting data-science challenges (data-loading ~TB, latitude-varying dynamics != convolutions, ...) See Discussion section in paper."", '@braaannigan This is a great point. I have been thinking about using self-supervision for pretraining since overfitting will be an issue. Another option is to pretrain on climate model simulations.', '@profamymcgovern @DrMukkavilli Thanks. Totally agree that Z500 is not weather and is in itself not interesting at all but it is the most common metric for global models. As mentioned in thread and paper, my hope is that this is a building block for more relevant applications.', '@profamymcgovern @DrMukkavilli I know that ML has been used for many applications in weather and climate but this is specifically referring to global state-to-state forecasting where to my knowledge these three studies are so far the only ones. Paper also discusses other applications (including yours)', '@profamymcgovern @DrMukkavilli Actually, I have heard but not seen Environet (Google search comes up blank). Was talking to @DJGagneDos recently that we as a community should compile a list of publicly available weather and climate benchmarks.', '@mnpinto_ @kaggle Check out this discussion\nhttps://t.co/4AJm1Jf8wv', '@jeremy_mcgibbon @profamymcgovern @DrMukkavilli You raise a good point. Will bookmark this for revisions. But we do discuss other state to state methods like now casting. Apart from that what other ml state to state weather prediction can you think of?', '@AgWxMan1 Yes good point. One would think that physical + ml &gt; only ml. However it’s trickier to get a lot of training data since nwp systems change a lot. Would love to see someone try with what’s available through TIGGE though.', '@TroyArcomano @AgWxMan1 That sounds really interesting. Is there anything (slides, preprint) you can share with me?', ""@jeremy_mcgibbon @profamymcgovern @DrMukkavilli Of course I know your study but I would put that into the ML parameterization category, where you replace a component of a physical model. But you do optimize with respect to ERA which the other ML parameterization studies don't do. Will include in revisions :)"", '@jeremy_mcgibbon @profamymcgovern @DrMukkavilli Care to share the references. I find the ML literature a little overwhelming sometimes. I know of this one: https://t.co/Pw5IFJ79V7\n\nBut to be honest I find the analysis a little limited for atmospheric science standards.', '@profamymcgovern @jeremy_mcgibbon @DrMukkavilli I would define what I am talking about as predicting one or several 2D fields some time into the future based on other current 2D fields as inputs using a purely data-driven method. Can you point me to other papers in that direction that are not precip nowcasting?', '@jeremy_mcgibbon @profamymcgovern @DrMukkavilli Ok, but to use it in a real 3D model you would have to combine it with a dycore, or am I mistaken?', ""@jeremy_mcgibbon @profamymcgovern @DrMukkavilli Hmm, for the WeatherBench direction I was more thinking of directly predicting impact variables (e.g. precip) at some point. I see Z500/T850 just as a start because these are common NWP verification metrics. But hey, it's great that there are some many different directions :)"", ""@jeremy_mcgibbon @profamymcgovern @DrMukkavilli I don't think in the paper I say it like this. I really tried to narrow down the scope and also mention other related approaches (parameterization, nowcasting). But please let me know if you disagree. On Twitter I was a little space limited ;)"", '@profamymcgovern @jeremy_mcgibbon @DrMukkavilli @AMS_AIML @ralager_Wx @AmandaLeo_wx @DJGagneDos @dharrisonwx Thanks! However it turns out that my institution @TU_Muenchen  does not have a subscription to Weather and Forecasting, so I cannot access some of your recent work.😳']",https://arxiv.org/abs/2002.00469,"Data-driven approaches, most prominently deep learning, have become powerful tools for prediction in many domains. A natural question to ask is whether data-driven methods could also be used to predict global weather patterns days in advance. First studies show promise but the lack of a common dataset and evaluation metrics make inter-comparison between studies difficult. Here we present a benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientific interest for atmospheric and computer scientists alike. We provide data derived from the ERA5 archive that has been processed to facilitate the use in machine learning models. We propose simple and clear evaluation metrics which will enable a direct comparison between different methods. Further, we provide baseline scores from simple linear regression techniques, deep learning models, as well as purely physical forecasting models. The dataset is publicly available at this https URL and the companion code is reproducible with tutorials for getting started. We hope that this dataset will accelerate research in data-driven weather forecasting. ",WeatherBench: A benchmark dataset for data-driven weather forecasting
160,1228408225567039490,2463105726,Eugene Bagdasaryan,"['Turns out, users don’t always benefit from participating in Federated Learning. Sounds discouraging for many proposed scenarios. The problem gets worse for robust and privacy-preserving FL. We propose: “Salvaging Federated Learning by Local Adaptation” <LINK>', '@jvmncs Thanks! We report accuracy on local data of that participant which is what makes the difference for them. We tried to avoid talking about global accuracy on some holdout dataset.', '@AgoryachAlex will do 😀']",https://arxiv.org/abs/2002.04758,"Federated learning (FL) is a heavily promoted approach for training ML models on sensitive data, e.g., text typed by users on their smartphones. FL is expressly designed for training on data that are unbalanced and non-iid across the participants. To ensure privacy and integrity of the fedeated model, latest FL approaches use differential privacy or robust aggregation. We look at FL from the \emph{local} viewpoint of an individual participant and ask: (1) do participants have an incentive to participate in FL? (2) how can participants \emph{individually} improve the quality of their local models, without re-designing the FL framework and/or involving other participants? First, we show that on standard tasks such as next-word prediction, many participants gain no benefit from FL because the federated model is less accurate on their data than the models they can train locally on their own. Second, we show that differential privacy and robust aggregation make this problem worse by further destroying the accuracy of the federated model for many participants. Then, we evaluate three techniques for local adaptation of federated models: fine-tuning, multi-task learning, and knowledge distillation. We analyze where each is applicable and demonstrate that all participants benefit from local adaptation. Participants whose local models are poor obtain big accuracy improvements over conventional FL. Participants whose local models are better than the federated model\textemdash and who have no incentive to participate in FL today\textemdash improve less, but sufficiently to make the adapted federated model better than their local models. ",Salvaging Federated Learning by Local Adaptation
161,1228374235153723392,989392279,Thomas Helfer,"['Finally, a new paper (<LINK>) out with my amazing collaborators @jc_aurrekoetxea and @tukohbin. We explored Cosmic Strings Loops collapsing to a Black Hole and studied their gravitational wave signal. <LINK>']",https://arxiv.org/abs/2002.05177,"We construct, for the first time, the time-domain gravitational wave strain waveform from the collapse of a strongly gravitating Abelian Higgs cosmic string loop in full general relativity. We show that the strain exhibits a large memory effect during merger, ending with a burst and the characteristic ringdown as a black hole is formed. Furthermore, we investigate the waveform and energy emitted as a function of string width, loop radius and string tension $G\mu$. We find that the mass normalized gravitational wave energy displays a strong dependence on the inverse of the string tension $E_{\mathrm{GW}}/M_0\propto 1/G\mu$, with $E_{\mathrm{GW}}/M_0 \sim {\cal O}(1)\%$ at the percent level, for the regime where $G\mu\gtrsim10^{-3}$. Conversely, we show that the efficiency is only weakly dependent on the initial string width and initial loop radii. Using these results, we argue that gravitational wave production is dominated by kinematical instead of geometrical considerations. ",Coherent Gravitational Waveforms and Memory from Cosmic String Loops
162,1227878535462899712,856802303533305856,Dr Aaron Jones,"['Put my first, first-author paper on the arxiv! <LINK>\n\nWe explore something called spatial mode-decomposition and propose using it in Gravitational Wave detectors. This could reduce losses and thus boost detection range 1/x <LINK>', ""Earth-based gravitational-wave detectors use optical resonators to boost the amount of light they use to detect the #GravitationalWaves. But detector imperfections can cause unwanted resonances (modes), which make it harder to detect GW's! 2/x"", 'We use a recently demonstrated technique (MODAN) to check the power in these modes. We explore the design limitations of the MODAN and improve the dynamic range - making it suitable for precision measurements - like in Gravitational-Wave detectors! 3/3']",https://arxiv.org/abs/2002.04864,Accurate readout of low-power optical higher-order spatial modes is of increasing importance to the precision metrology community. Mode sensors are used to prevent mode mismatches from degrading quantum and thermal noise mitigation strategies. Direct mode analysis sensors (MODAN) are a promising technology for real-time monitoring of arbitrary higher-order modes. We demonstrate MODAN with photo-diode readout to mitigate the typically low dynamic range of CCDs. We look for asymmetries in the response our sensor to break degeneracies in the relative alignment of the MODAN and photo-diode and consequently improve the dynamic range of the mode sensor. We provide a tolerance analysis and show methodology that can be applied for sensors beyond first-order spatial modes. ,High Dynamic Range Spatial Mode Decomposition
163,1227669720465174528,56619141,Abhishek Kumar,"['Implicit regularization in beta-VAEs\xa0(joint work w/ Ben Poole @poolio):\xa0<LINK> We study the regularizing effects of variational distributions on learning in generative models from two perspectives -- (1/3)', '@poolio (1) We analyze\xa0the role that the choice of variational family plays in imparting uniqueness by restricting the set of optimal generative models. (2/3)', '@poolio (2) We study the regularization effect of posterior covariance on the local geometry of decoder. This leads to deterministic approximation of beta-VAE consisting of regularizer that depends on the decoder Jacobians, formally connecting VAEs with regularized autoencoders. (3/3)']",https://arxiv.org/abs/2002.00041,"While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the $\beta$-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective exhibits similar behavior to the $\beta$-VAE in terms of objective value and sample quality. ",On Implicit Regularization in $\beta$-VAEs
164,1226845447907659776,1005395495949406208,Francesco Locatello,"['We propose a new setup for weakly-supervised disentanglement, which:\n* does not require group annotations\n* is widely applicable, and\n* is theoretically identifiable.\n<LINK> [1/10] <LINK>', 'We consider pairs of non-iid observations sharing an unknown, random subset of underlying factors of variation and develop VAE variants tailored to this type of observations. [2/10]', 'A feature of these algorithms is that their objective allows reliable model selection based on the reconstruction loss (or ELBO), without relying on supervised disentanglement metrics. [3/10]', 'We also have a new cool downstream task: strong generalization under covariate shifts. The goal is to solve a classification task with different interventions on the train and test sets. Weakly-supervised data + disentanglement help! [4/10]', 'Joint work with @poolio, @gxr, @bschoelkopf, @OlivierBachem and @mtschannen from @ETH_en, @GoogleAI and @MPI_IS. If you are at #AAAI, I’m giving an overview talk on disentanglement today at 11.15, room Nassau. [5/10]', 'My perspective on disentanglement changed a lot over the past year. After our ICML study https://t.co/Z4yv0CRdFT, I was very skeptical about it and wasn’t sure whether I should keep working on it. [6/10]', 'These two papers (https://t.co/7Xz2DBPiN6 and https://t.co/3B8ldaj9g8) really motivated me: I got excited about the potential applications but I still personally didn’t believe that unsupervised learning was the right approach. [7/10]', ""But how much supervision do we actually need? Turned out, not so much https://t.co/0fGSeJEHug. If very little and noisy labels are enough, what about zero labels and only weak supervision? That's how this paper came to be! [8/10]"", 'It’s exciting that there are now several papers considering semi-supervised and weakly-supervised approaches. I think there’s no “one setup to fit them all”: different applications/data sets come with both special needs and specific domain knowledge. [9/10]', 'This additional knowledge should make its way into representation learning algorithms. This is especially true for disentanglement because of our impossibility result. [10/10]']",https://arxiv.org/abs/2002.02886,"Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios. ",Weakly-Supervised Disentanglement Without Compromises
165,1226678661090332673,123591961,Sebastian Deffner,['Frist preprint of @AkramTouil1!! We have taken the first step to study the thermodynamics of chaotic quantum systems: <LINK>'],https://arxiv.org/abs/2002.02867,"Quantum information scrambling refers to the loss of local recoverability of quantum information, which has found widespread attention from high energy physics to quantum computing. In the present analysis we propose a possible starting point for the development of a comprehensive framework for the thermodynamics of scrambling. To this end, we prove that the growth of entanglement as quantified by the mutual information is lower bounded by the time-dependent change of Out-Of-Time-Ordered Correlator. We further show that the rate of increase of the mutual information can be upper bounded by the sum of local entropy productions, and the exchange entropy arising from the flow of information between separate partitions of a quantum system. Our results are illustrated for the ion trap system, that was recently used to verify information scrambling in an experiment, and for the Sachdev-Ye-Kitaev model. ",Quantum scrambling and the growth of mutual information
166,1225698131280457730,944291984675614721,Tobias de Jong,"['New paper on arXiv 😁 <LINK> In a collaboration with (amongst others) University of Geneva, @ICFOnians, @elettrasincro and @LeidenPhysics, we use STM and LEEM to aid ARPES to study the band structure of (near) magic-angle bilayer graphene and observe a flat band', 'We (i.e. @sensemolen , @j_jobst and myself) used LEEM to map out the device of stacked 2D flakes at high resolution, identifying areas of monolayer, multilayer, normal bilayer and near-magic-angle bilayer graphene, guiding ARPES and STM collaborators where to measure.']",https://arxiv.org/abs/2002.02289,"Transport experiments in twisted bilayer graphene revealed multiple superconducting domes separated by correlated insulating states. These properties are generally associated with strongly correlated states in a flat mini-band of the hexagonal moir\'e superlattice as it was predicted by band structure calculations. Evidence for such a flat band comes from local tunneling spectroscopy and electronic compressibility measurements, reporting two or more sharp peaks in the density of states that may be associated with closely spaced van Hove singularities. Direct momentum resolved measurements proved difficult though. Here, we combine different imaging techniques and angle resolved photoemission with simultaneous real and momentum space resolution (nano-ARPES) to directly map the band dispersion in twisted bilayer graphene devices near charge neutrality. Our experiments reveal large areas with homogeneous twist angle that support a flat band with spectral weight that is highly localized in momentum space. The flat band is separated from the dispersive Dirac bands which show multiple moir\'e hybridization gaps. These data establish the salient features of the twisted bilayer graphene band structure. ","Direct evidence for flat bands in twisted bilayer graphene from
  nano-ARPES"
167,1225625702629724160,1177063549606203394,Tommi Tenkanen,"['A new paper out! <LINK> \n\nWe studied initial conditions for cosmic inflation, concentrating on scenarios where the inflaton potential has a ""plateau"" shape, as such models are among those most favored by observations of the Cosmic Microwave Background data. 1/', 'As a representative example, we considered a ""Higgs inflation"" model in the context of so-called ""Palatini"" gravity. We showed that inflation generically occurs in a large part of the model parameter space even when the scale of inflation is much smaller than the Planck scale. 2/']",https://arxiv.org/abs/2002.02420,"We study initial conditions for inflation in scenarios where the inflaton potential has a plateau shape. Such models are those most favored by Planck data and can be obtained in a large number of model classes. As a representative example, we consider Higgs inflation with and without an $R^2$ term in the context of Palatini gravity. We show that inflation with a large number of e-folds generically occurs in a large part of the parameter space without any fine-tuning of parameters even when the scale of inflation and the inflaton field value during inflation are much smaller than the Planck scale. We discuss consequences for detection of primordial gravitational waves and spectral tilt of curvature perturbations, as well as the recently proposed ""Trans-Planckian Censorship"" conjecture. ",Initial conditions for plateau inflation: a case study
168,1225369093815095296,1032283657015316480,Josh Dorrington,"['V. happy to have submitted my 1st ever paper, w\\ @meteo_Isla, and others!\n\nHow useful really are sub-seasonal forecasts to end-users?\n\nWe present a new user-focused framework for answering this question, based on an energy sector case-study\n\npreprint at: <LINK>']",https://arxiv.org/abs/2002.01728,"We quantify the value of sub-seasonal forecasts for a real-world prediction problem: the forecasting of French month-ahead energy demand. Using surface temperature as a predictor, we construct a trading strategy and assess the financial value of using meteorological forecasts, based on actual energy demand and price data. We show that forecasts with lead times greater than 2 weeks can have value for this application, both on their own and in conjunction with shorter range forecasts, especially during boreal winter. We consider a cost/loss framework based on this example, and show that while it captures the performance of the short range forecasts well, it misses the marginal value present in the longer range forecasts. We also contrast our assessment of forecast value to that given by traditional skill scores, which we show could be misleading if used in isolation. We emphasise the importance of basing assessment of forecast skill on variables actually used by end-users. ","Beyond skill scores: exploring sub-seasonal forecast value through a
  case study of French month-ahead energy prediction"
169,1225337667082215425,75249390,Axel Maas,"[""We have put out a new paper, among others with @SimonPlaetzer, on the 'valence Higgs' of the proton. That may sounds weird, as usually it is told of only three 'valence quarks' in the proton. So let me explain a little - you can find the paper at <LINK>"", 'A particle is called valence, if it contributes to the quantum number of the proton. The quarks make up its electric charge and baryon number. So why would the Higgs be there? Which quantum number does it contribute to?\n\nIt is quite involved,and has to do with weak interactions.', 'The weak interactions couple to something we call flavor, and which is usually said to make the difference between proton and neutron.\n\nThe underlying theory is more complicated, and formally says something like flavor is not observable - see the review https://t.co/jVo2JEcpFO', ""To get a physical version of flavor, you need to add something which acts 'like flavor', and creates the distinction. In the standard model, the only particle which can do this without changing the spin is the Higgs. This has been worked out for leptons in 1980 by Fröhlich et al."", 'However, a proton is a composite particle, and things are a bit more weird. But essentially it needs to have three quarks and a Higgs to get all quantum numbers right and observable in the standard model. We deduced this in https://t.co/EhNqRI8CIn', 'But this should have observable consequences, if we have enough energy to get the sluggish Higgs to react - at least something like the LHC.\n\nThis paper is our first attempt to determine how much we would see, and where we would see it.', 'We find that the effect needs to be tiny, but not impossible.\n\nIt is worthwhile to invest more effort into it, as we did a lot of very crude estimates. But confirming it would be a big step in understanding the field theory underlying the standard model.']",https://arxiv.org/abs/2002.01688,"Non-perturbative gauge-invariance under the strong and the weak interactions dictates that the proton contains a non-vanishing valence contribution from the Higgs particle. By introducing an additional parton distribution function (PDF), we investigate the experimental consequences of this prediction. The Herwig 7 event generator and a parametrized CMS detector simulation are used to obtain predictions for a scenario amounting to the LHC Run II data set. We use those to assess the impact of the Higgs PDF on the pp->ttbar process in the single lepton final state. Comparing to nominal simulation we derive expected limits as a function of the shape of the valence Higgs PDF. We also investigate the process pp->ttZ at the parton level to add further constraints. ",Constraining the Higgs valence contribution in the proton
170,1225023278156328961,280083723,Yoh Tanimoto,"['new paper~ we find an embedding of (scalar) lattice fields into the continuum field using wavelets <LINK>', '@fraarici thanks! the full paper is coming (hopefully) soon~']",https://arxiv.org/abs/2002.01442,We report on a rigorous operator-algebraic renormalization group scheme and construct the continuum free field as the scaling limit of Hamiltonian lattice systems using wavelet theory. A renormalization group step is determined by the scaling equation identifying lattice observables with the continuum field smeared by compactly supported wavelets. Causality follows from Lieb-Robinson bounds for harmonic lattice systems. The scheme is related with the multi-scale entanglement renormalization ansatz and augments the semi-continuum limit of quantum systems. ,Operator-algebraic renormalization and wavelets
171,1224724623910154242,458603378,Marian,"['New Paper: Adversarial Generation of Continuous Implicit Shape Representations\n\nWe propose two GANs with a DeepSDF network as the generator and either a 3D CNN or a Pointnet as the discriminator. Written by @rusty1s and me!\n\nPaper: <LINK> <LINK>', '@BriamMor I started with 3D GANs and only did 2D GANs after that. My advice would be to start with 2D GANs since that makes data preparation and visualization a lot easier. If you want to get results quickly, try to get existing implementations to run.', '@BriamMor But if you want to understand how it works, try to implement it yourself. I would start with a fully connected GAN on something like MNIST, then a DCGAN, then WGAN-GP, then implement progressive growing, then StyleGAN. You could also go for StyleGAN right away.', '@rusty1s Check out my blog post about the paper. It has animated visualizations!\nhttps://t.co/wLFpUKgMv7 https://t.co/5QVT7xUx0w']",https://arxiv.org/abs/2002.00349,"This work presents a generative adversarial architecture for generating three-dimensional shapes based on signed distance representations. While the deep generation of shapes has been mostly tackled by voxel and surface point cloud approaches, our generator learns to approximate the signed distance for any point in space given prior latent information. Although structurally similar to generative point cloud approaches, this formulation can be evaluated with arbitrary point density during inference, leading to fine-grained details in generated outputs. Furthermore, we study the effects of using either progressively growing voxel- or point-processing networks as discriminators, and propose a refinement scheme to strengthen the generator's capabilities in modeling the zero iso-surface decision boundary of shapes. We train our approach on the ShapeNet benchmark dataset and validate, both quantitatively and qualitatively, its performance in generating realistic 3D shapes. ",Adversarial Generation of Continuous Implicit Shape Representations
172,1230779292386254849,174298756,Adel Bibi,"[""I'm very excited about this new work in collaboration with @MotassimF, Hasan Hammoud, Mohamed Gaafar and Bernard Ghanem.\n\nWe study the decision boundaries through the lens of tropical geometry in a single hidden layer neural network.\n\n<LINK> <LINK>"", '@MotassimF We revisit the lottery ticket hypothesis through our new framework. Moreover, we propose new novel geometrical approaches for network pruning and the construction of adversarial attacks.']",https://arxiv.org/abs/2002.08838,"This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piecewise linear non-linearity activations. We use tropical geometry, a new development in the area of algebraic geometry, to characterize the decision boundaries of a simple network of the form (Affine, ReLU, Affine). Our main finding is that the decision boundaries are a subset of a tropical hypersurface, which is intimately related to a polytope formed by the convex hull of two zonotopes. The generators of these zonotopes are functions of the network parameters. This geometric characterization provides new perspectives to three tasks. (i) We propose a new tropical perspective to the lottery ticket hypothesis, where we view the effect of different initializations on the tropical geometric representation of a network's decision boundaries. (ii) Moreover, we propose new tropical based optimization reformulations that directly influence the decision boundaries of the network for the task of network pruning. (iii) At last, we discuss the reformulation of the generation of adversarial attacks in a tropical sense. We demonstrate that one can construct adversaries in a new tropical setting by perturbing a specific set of decision boundaries by perturbing a set of parameters in the network. ","On the Decision Boundaries of Neural Networks: A Tropical Geometry
  Perspective"
173,1229623648983515136,140933540,Kuldeep S. Meel,"[""New Paper: Sampling-based approach for quantittive quantitativeverification of Deep Neural Nets. (with @teobaluta , Z.L. Chua, and P. Saxena). We propose a new attack agnostic metric adversarial hardness to capture model's robustness: <LINK>""]",https://arxiv.org/abs/2002.06864,"Despite the functional success of deep neural networks (DNNs), their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep neural networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep neural networks, with provable guarantees, even where testers only have black-box access to the neural network. ",Scalable Quantitative Verification For Deep Neural Networks
174,1228457156476907520,1178760732114116609,Andrea Celli,['Is it possible to be almost optimal and almost persuasive? \n\nCheck this out:  <LINK>\n\nWe study bi-criteria approximation in public Bayesian persuasion with no inter-agent externalities.\n\nJoint work with @MatteoCastigl11 and Nicola Gatti'],https://arxiv.org/abs/2002.05156,"Persuasion studies how an informed principal may influence the behavior of agents by the strategic provision of payoff-relevant information. We focus on the fundamental multi-receiver model by Arieli and Babichenko (2019), in which there are no inter-agent externalities. Unlike prior works on this problem, we study the public persuasion problem in the general setting with: (i) arbitrary state spaces; (ii) arbitrary action spaces; (iii) arbitrary sender's utility functions. We fully characterize the computational complexity of computing a bi-criteria approximation of an optimal public signaling scheme. In particular, we show, in a voting setting of independent interest, that solving this problem requires at least a quasi-polynomial number of steps even in settings with a binary action space, assuming the Exponential Time Hypothesis. In doing so, we prove that a relaxed version of the Maximum Feasible Subsystem of Linear Inequalities problem requires at least quasi-polynomial time to be solved. Finally, we close the gap by providing a quasi-polynomial time bi-criteria approximation algorithm for arbitrary public persuasion problems that, in specific settings, yields a QPTAS. ",Public Bayesian Persuasion: Being Almost Optimal and Almost Persuasive
175,1228360936903323648,2783857610,Dimitris Kalatzis,"['First PhD paper out! \n\nWe adapt the VAE latent space to data geometry and propose a prior that samples from the image of the embedded submanifold in the latent space.\n\n<LINK>\n\nAwesome co-authors: David Eklund, @arvanitg, Søren Hauberg']",https://arxiv.org/abs/2002.05227,"Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter. ",Variational Autoencoders with Riemannian Brownian Motion Priors
176,1225502692094758912,2911287964,Thomas Kupfer,"['Its paper time again: Together with many colleagues and friends (incl. @evbauer_astro, @Janvanroestel, @jotajotahermes, @ebellm, @vikdhillon) we found the first hot subdwarf which fills its Roche lobe and transfers matter to its companion:\n<LINK>... 1/12', 'We first spotted the object in our @ztfsurvey high-cadence Galactic Plane data. The light curve shape is just remarkable with two very different minima, brightness changes up to 30%, and a period of only 39min, which makes it the shortest period hot subdwarf binary known... 2/12 https://t.co/3xPis7bRUW', 'We measured velocities over the binary period and found a large velocity amplitude, and on top of that, we found a Rossiter-McLaughlin effect. This effect causes a small shift in the measured velocities when a large enough object occults a rapidly rotating star... 3/12 https://t.co/pGaU3JvbTd', 'So, we knew it is a hot subdwarf which is rapidly rotating, moving with about 400 km/s, is being eclipsed by another object which does not show up in the spectrum, and has an orbital period of 39min. But how do put this in context with the remarkable light curve?... 4/12 https://t.co/qrXtgBJa8b', 'Luckily we got a beautiful follow-up light curve from the HiPERCAM instrument. It produces high-speed photometry with basically no readout time for 5 bands (u,g,r,i,z) simultaneously. Its currently mounted on the @GTCtelescope which makes it an incredibly powerful machine... 5/12 https://t.co/1kKlR6vwPs', 'The only way we could explain that light curve was to have a tidally deformed, Roche lobe-filling hot subdwarf which is being eclipsed by an accretion disc. We we even had to account for a reflection effect where the hot subdwarf is heating one side of the accretion disc... 6/12 https://t.co/O7X8koG1M8', 'So, what we found was a low mass hot subdwarf on only ~33% the mass of the sun, transferring matter onto an accretion disc which is surrounding a white dwarf with a mass of ~55% the mass of the sun. Here is how we visualize this object, but whats the story for this object... 7/12 https://t.co/ZVOqXqXYUg', 'My MESA friends at @KITP_UCSB found that the progenitor of the hot subdwarf was a star which had ~2.5-2.8 the mass of the sun but lost most of its hydrogen rich envelope in a common envelope event when it crossed the Hertzsprung Gap. Left behind was our hot subdwarf star... 8/12 https://t.co/Hw6KJ7B9WP', 'this happened ~500 million years ago when the hot subdwarf star with its WD companion was born at a period of ~2.5 hours. For the last 500 million years gravitational waves did its job and tightened the orbit until it started to accrete ~1 million years ago... 9/12 https://t.co/F9mqMOF78g', 'But whats next? Once the hot subdwarf runs out of fuel, it will form a white dwarf and accretion stops. Gravitational waves will keep tightening the orbit, and in ~17 million years accretion starts again. Now, it could explode as SN Ia or merge to a single R CrB star. 10/12 https://t.co/1E7bzyQSGR', 'Interesting fact: Because the orbit is so tight it is a strong emitter of gravitational waves and we predict that @LISACommunity will be able to detect this object. I am also very positive that in a few years we will have good measurement of the orbital decay. 11/12 https://t.co/S6D5tXw6HZ', ""But, why haven't we seen this before? I think, we just searched in the wrong place. The object is young and can only be found in the Galactic disc. But, time-domain surveys of the Galactic disc are only ramping up and I expect many more such discoveries in the near future. 12/12 https://t.co/CB8fr14QI7"", '@di_goldene_pave @GTCtelescope Yes potentially, if the signal-to-noise ratio is high enough. We expect that the accreting white dwarf is really hot and would hope to see weak eclipses of the hot accreting white dwarf in the UV. This would help to constrain, radius and mass for accretor even better', '@di_goldene_pave @GTCtelescope Hard to say, but probably around 1% or a little less.']",https://arxiv.org/abs/2002.01485,"We report the discovery of the first short period binary in which a hot subdwarf star (sdOB) fills its Roche lobe and started mass transfer to its companion. The object was discovered as part of a dedicated high-cadence survey of the Galactic Plane named the Zwicky Transient Facility and exhibits a period of $P_{\rm orb}=39.3401(1)$ min, making it the most compact hot subdwarf binary currently known. Spectroscopic observations are consistent with an intermediate He-sdOB star with an effective temperature of $T_{\rm eff}=42,400\pm300$ K and a surface gravity of $\log(g)=5.77\pm0.05$. A high-signal-to noise GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the sdOB star and an eclipse of the sdOB by an accretion disk. We infer a low-mass hot subdwarf donor with a mass $M_{\rm sdOB}=0.337\pm0.015$ M$_\odot$ and a white dwarf accretor with a mass $M_{\rm WD}=0.545\pm0.020$ M$_\odot$. Theoretical binary modeling indicates the hot subdwarf formed during a common envelope phase when a $2.5-2.8$ M$_\odot$ star lost its envelope when crossing the Hertzsprung Gap. To match its current $P_{\rm orb}$, $T_{\rm eff}$, $\log(g)$, and masses, we estimate a post-common envelope period of $P_{\rm orb}\approx150$ min, and find the sdOB star is currently undergoing hydrogen shell burning. We estimate that the hot subdwarf will become a white dwarf with a thick helium layer of $\approx0.1$ M$_\odot$ and will merge with its carbon/oxygen white dwarf companion after $\approx17$ Myr and presumably explode as a thermonuclear supernova or form an R CrB star. ",The first ultracompact Roche lobe-filling hot subdwarf binary
177,1224611468517105666,1570476014,Yi-Hsuan Yang,"['""Pop Music Transformer: Generating Music with Rhythm and Harmony""  \n\\paper: <LINK>\n\\demo: <LINK>\n\\code: <LINK>\n\n```We propose a new event representation of music that make it easy for models to count the beats```\n#TaiwanAILabs', ""@chrisdonahuey Thanks! The tracker works fairly well for pop music.  We haven't tried other genres yet.""]",https://arxiv.org/abs/2002.00212,"A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models. ","Pop Music Transformer: Beat-based Modeling and Generation of Expressive
  Pop Piano Compositions"
