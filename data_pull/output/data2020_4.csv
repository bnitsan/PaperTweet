,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1260636171299958784,764362094191996928,TH Nguyen,['Our new paper on Building footprint extraction is currently under minor revision for the Remote Sensing MDPI (IF=4.118). Check out the preprint and the Github page for results right below:\n\nPaper: <LINK>\nGithub: <LINK>'],https://arxiv.org/abs/2004.08522,"Automatic extraction of buildings in urban and residential scenes has become a subject of growing interest in the domain of photogrammetry and remote sensing, particularly since mid-1990s. Active contour model, colloquially known as snake model, has been studied to extract buildings from aerial and satellite imagery. However, this task is still very challenging due to the complexity of building size, shape, and its surrounding environment. This complexity leads to a major obstacle for carrying out a reliable large-scale building extraction, since the involved prior information and assumptions on building such as shape, size, and color cannot be generalized over large areas. This paper presents an efficient snake model to overcome such challenge, called Super-Resolution-based Snake Model (SRSM). The SRSM operates on high-resolution LiDAR-based elevation images -- called z-images -- generated by a super-resolution process applied to LiDAR data. The involved balloon force model is also improved to shrink or inflate adaptively, instead of inflating the snake continuously. This method is applicable for a large scale such as city scale and even larger, while having a high level of automation and not requiring any prior knowledge nor training data from the urban scenes (hence unsupervised). It achieves high overall accuracy when tested on various datasets. For instance, the proposed SRSM yields an average area-based Quality of 86.57% and object-based Quality of 81.60% on the ISPRS Vaihingen benchmark datasets. Compared to other methods using this benchmark dataset, this level of accuracy is highly desirable even for a supervised method. Similarly desirable outcomes are obtained when carrying out the proposed SRSM on the whole City of Quebec (total area of 656 km2), yielding an area-based Quality of 62.37% and an object-based Quality of 63.21%. ","Super-Resolution-based Snake Model -- An Unsupervised Method for
  Large-Scale Building Extraction using Airborne LiDAR Data and Optical Image"
1,1260118823623168000,1258718388106469376,Giuseppe Cocco,"['How feasible is #drone-aided localization in\n#LoRaWAN  #IoT networks? Check out the new experimental results in our paper, to be presented at the 2020 IEEE International Conference on Robotics and Automation (ICRA)\n\nPaper: <LINK>\nVideo: <LINK>']",https://arxiv.org/abs/2004.03852,"Besides being part of the Internet of Things (IoT), drones can play a relevant role in it as enablers. The 3D mobility of UAVs can be exploited to improve node localization in IoT networks for, e.g., search and rescue or goods localization and tracking. One of the widespread IoT communication technologies is Long Range Wide Area Network (LoRaWAN), which allows achieving long communication distances with low power. In this work, we present a drone-aided localization system for LoRa networks in which a UAV is used to improve the estimation of a node's location initially provided by the network. We characterize the relevant parameters of the communication system and use them to develop and test a search algorithm in a realistic simulated scenario. We then move to the full implementation of a real system in which a drone is seamlessly integrated into Swisscom's LoRa network. The drone coordinates with the network with a two-way exchange of information which results in an accurate and fully autonomous localization system. The results obtained in our field tests show a ten-fold improvement in localization precision with respect to the estimation provided by the fixed network. Up to our knowledge, this is the first time a UAV is successfully integrated in a LoRa network to improve its localization accuracy. ",Drone-aided Localization in LoRa IoT Networks
2,1259955920412475394,972440922070888449,Jason Parisi,"['Excited to put out a new paper: we describe a new type of plasma instability in the edge of nuclear fusion reactors, as well as other aspects of linear pedestal physics. \n<LINK>']",https://arxiv.org/abs/2004.13634,"Local linear gyrokinetic simulations show that electron temperature gradient (ETG) instabilities are the fastest growing modes for $k_y \rho_i \gtrsim 0.1$ in the steep gradient region for a JET pedestal discharge (92174) where the electron temperature gradient is steeper than the ion temperature gradient. Here, $k_y$ is the wavenumber in the direction perpendicular to both the magnetic field and the radial direction, and $\rho_i$ is the ion gyroradius. At $k_y \rho_i \gtrsim 1$, the fastest growing mode is often a novel type of toroidal ETG instability. This toroidal ETG mode is driven at scales as large as $k_y \rho_i \sim (\rho_i/\rho_e) L_{Te} / R_0 \sim 1$ and at a sufficiently large radial wavenumber that electron finite Larmor radius effects become important; that is, $K_x \rho_e \sim 1$, where $K_x$ is the effective radial wavenumber. Here, $\rho_e$ is the electron gyroradius, $R_0$ is the major radius of the last closed flux surface, and $1/L_{Te}$ is an inverse length proportional to the logarithmic gradient of the equilibrium electron temperature. The fastest growing toroidal ETG modes are often driven far away from the outboard midplane. In this equilibrium, ion temperature gradient instability is subdominant at all scales and kinetic ballooning modes are shown to be suppressed by $\mathbf{ E} \times \mathbf{ B} $ shear. ETG modes are very resilient to $\mathbf{ E} \times \mathbf{ B}$ shear. Heuristic quasilinear arguments suggest that the novel toroidal ETG instability is important for transport. ","Toroidal and slab ETG instability dominance in the linear spectrum of
  JET-ILW pedestals"
3,1259857065843056641,2455538305,Seth J. Hill,"['New version of paper with Fowler, Obradovich, @RemyLevin  <LINK>\n\nExtensive revisions in response to feedback. We now use the counties that never issued stay-at-home as explicit controls, rather than the variable-treatment-timing diff-in-diff before.', 'Plots speak more than words. https://t.co/UuOpvaY3dc', 'https://t.co/vZZjyCNTsv']",https://arxiv.org/abs/2004.06098,"Governments issue ""stay at home"" orders to reduce the spread of contagious diseases, but the magnitude of such orders' effectiveness is uncertain. In the United States these orders were not coordinated at the national level during the coronavirus disease 2019 (COVID-19) pandemic, which creates an opportunity to use spatial and temporal variation to measure the policies' effect with greater accuracy. Here, we combine data on the timing of stay-at-home orders with daily confirmed COVID-19 cases and fatalities at the county level in the United States. We estimate the effect of stay-at-home orders using a difference-in-differences design that accounts for unmeasured local variation in factors like health systems and demographics and for unmeasured temporal variation in factors like national mitigation actions and access to tests. Compared to counties that did not implement stay-at-home orders, the results show that the orders are associated with a 30.2 percent (11.0 to 45.2) reduction in weekly cases after one week, a 40.0 percent (23.4 to 53.0) reduction after two weeks, and a 48.6 percent (31.1 to 61.7) reduction after three weeks. Stay-at-home orders are also associated with a 59.8 percent (18.3 to 80.2) reduction in weekly fatalities after three weeks. These results suggest that stay-at-home orders reduced confirmed cases by 390,000 (170,000 to 680,000) and fatalities by 41,000 (27,000 to 59,000) within the first three weeks in localities where they were implemented. ","The effect of stay-at-home orders on COVID-19 cases and fatalities in
  the United States"
4,1259420853026119682,2603024598,Ricardo Pérez-Marco,"['New paper ""The Cantor Riemannium"". First example of #Riemannium space.\n\nClassical Riemann surfaces are the minimal space where the Weierstrass extension of holomorphic germs live. Riemannium spaces are the same for the Borel monogenic extension. Thread ⬇️\n\n<LINK> <LINK>', 'Bernhard Riemann introduced Riemann surfaces in his Inaugural Dissertation on general principles of functions of a complex variable, presented in Göttingen in 1851. \n\nMultivalued analytic continuation of functions caused great trouble since Euler (ex: the complex logarithm).1/n', 'Riemann\'s brilliant idea was to pass the ""multivaluedness"" to the complex domain of definition, thus finding a minimal space where the function becomes univalued, and resolving in that way its multivaluedness. 2/n', ""This minimal space  is what nowadays we call a Riemann surface, but for Riemann they came equipped with some extra structure that the modern notion has forgotten. Riemann's Riemann surface of a holomorphic function come with a projection map onto the complex plane.3/n"", 'Riemann employed his fantastic idea to rebuild the theory of abelian integrals in his famous memoir from 1857. Their associated space which corresponds to our modern algebraic curves. 4/n', 'Riemann\'s Riemann surfaces included in their structure a not properly defined notion of ""ramification points"". This was well understood for algebraic curves since the ramification points appearing are the simplest ones: Isolated and with finite monodromy. 5/n', 'The notion of ""analytic continuation"" was also not properly defined in full generality in Riemann. Later, Karl Weierstrass (1876) defined rigorously the modern notion of analytic continuation. But people familiar with the work of A. Cauchy were aware of more general notions. 6/n', ""The difficulties in this early notion of Riemann surfaces, ramification points and analytic continuation are very visible in the early H. Poincaré's articles on the Uniformization Theorem (Bull. Soc. Math. France, 1883) which caused discomfort in the mathematical community. 7/n"", 'Poincaré was liberally using a naïve notion of Riemann surface coming from the simplicity of the ramification locus algebraic curves and missing the possible complexity of the ramification locus. These problems remained for many years. 8/n', ""Hilbert's 22nd problem in his famous list of problems delivered at the Paris 1900 Congress of Mathematicians is about these problems of the ramification locus in Poincaré's work (and it is not only about higher dimensional uniformization as many have misunderstood). 9/n"", 'The italian mathematician Giulio Vivanti pointed very early on these problems in Poincaré\'s work in his article ""Sulle funzioni ad infiniti valori"" (1888). He observed that Poincaré\'s argument fail when the number of sheets of the Riemann surface is uncountable. 10/n', ""He also proved, with the level of rigor as Poincaré's article on ramification points, that this did not happen when we consider a Weierstrass analytic extension. Shortly after that Poincaré and Volterra gave a satisfactory proof avoiding ramification points. 11/n"", ""This is the now called Poincaré-Volterra theorem, that should be called Vivanti's Theorem, or Vivanti-Poincaré-Volterra, or even Cantor-Vivanti-Poincaré-Volterra since Cantor seems to have been the first to discuss the result with Weierstrass and Vivant. 12/n"", 'Subsequently, Vivant\'s articles were unjustly dismissed as incorrect. Modern readers didn\'t seem to be aware of the non definite notion of analytic continuation at the time. There is a clear distinction between ""Weierstrass continuation"" and ""general continuation"". 13/n', 'Only a few years later, as early as 1894, Émile Borel in his Thesis, proved that there was nothing definite in the notion of Weierstrass continuation. He pursued his ideas for many years, and developed the notion of monogenic continuation of a holomorphic function. 14/n', 'In Borel\'s book ""Leçons sur les fonctions monogènes uniformes d\'une variable complexe"" (1917), he presents a theory of monogenic functions and monogenic continuation, that generalizes the classical theory of functions of a complex variable. 15/n', 'As he explains vividly in the introduction of his book, his new ideas were not well received by the Weierstrassian school, in particular there were misunderstood and criticized by G. Mittag-Leffler. His theory is in the foundations of the theory of quasi-analytic functions. 16/n', ""The theory of quasi-analytic functions was originally build by A. Denjoy (1921) and T. Carleman (1926), but restricted to real variable functions. Here, Borel's theory of monogenic continuation gives the unique continuation of quasi-analytic function along the real line. 17/n"", ""Unfortunately, Borel's theory of monogenic  continuation was forgotten by modern mathematicians, with some remarkable exceptions: The russian school, in particular A. Kolmogorov that mentions (1954) the monogenic nature of the dependence of rotation numbers in KAM theory. 18/n"", 'For Riemann surfaces, the problem with the ramification locus was ""solved"" (""avoided"" to be exact) by changing the definition of Riemann surface: The modern definition by H. Weyl of abstract Riemann surfaces (1913), evacuated form the theory ramification points. 19/n', 'Weyl\'s silence about ramifications points is very revealing. The nightmare of ramification points that haunted all articles since Riemann is taken out of the picture by this ""definition trick"". A great example of ""If you cannot solve the problem, change it"". 20/n', 'But also the original nature of Riemann surfaces as minimal spaces for Weierstrass continuation is evacuated from the picture. Also, the preferred chart of the projection on the complex plane that makes the link with classical special functions disappears. 21/n', 'Thus, abelian integrals become abelian differentials, and all classical results are formulated in coordinate independent statements. The link between the geometry and special functions, through the preferred chart given by the projection on the complex plane, is lost. 22/n', 'For example, few modern mathematicians are aware that the original formulation of Abel Theorem for Abelian integrals is stronger that the modern Abel Theorem for abelian differential that one learns in all textbooks. 23/n', ""Being aware of all of this, Riemannium spaces appear as a very natural generalization of Riemann's Riemann surfaces. We come back to their original nature as minimal spaces for the continuation of holomorphic maps. 24/n"", 'Riemannium spaces are the minimal spaces for the Borel monogenic extensions of a holomorphic map.  Thus they are a very natural generalization of Riemann surfaces but quite different spaces in nature. In particular they are not surfaces. 25/n', 'In general, they are not sigma-compact, but they are metric, Gromov length spaces, path connected and locally path connected spaces. But they are not semi-locally path connected spaces, which is one of the main assumptions in the classical theory of universal covers. 26/n', ""Borel didn't believe we could have a complete understanding of the minimal space for his monogenic extension. But the Cantor Riemannium is the first example where we have a complete understanding of such a space. 27/n"", ""The monodormy of the original holomorphic germ appears to be uncountable, and the Cantor Riemannium is build with an uncountable number of sheets. Vivanti-Poincaré-Volterra Theorem does no hold and this confirms Vivanti's original intuitions. 28/n"", 'An uncountable number of ""Krueger hands"" are sheets of the Cantor RIemannium. Below is the Krueger hand. An uncountable number of ""fingers"" have finite (and infinite) length. The Krueger hands are glued, with a precise combinatorics, by the end-points of finite fingers.  29/n https://t.co/9GpOFOWw2B', 'There is a notion of ""universal cover"" of Riemannium spaces, that is not a covering since the fiber of the projection map is not discrete (is neither a Hurewicz covering). The ""universal cover Riemannium"" comes equipped with a projection map on the complex plane. 30/n', 'The Cantor Riemannium has an open dense subset which has a Riemann surface structure (all Krueger hands). Each Riemann surface connected component can be thought as a ""sheet"" of the Riemannium space. 31/n', 'On the ""universal cover"", given a base point, the largest Riemann surface connected component is the Riemann surface corresponding to the Weierstrass continuation of the defining holomorphic germ. 32/n', ""Riemannium spaces generalize tube-log Riemann surfaces and log-Riemann surfaces defined by K. Biswas and the author some years ago with the idea of recovering and formalizing properly Riemann's idea of Riemann surface.\n\nhttps://t.co/5Y1HkSuSKz\nhttps://t.co/RoO6kUTWTX\n33/n"", 'Many classical results of Riemann surfaces do generalize to log-Riemann surfaces. See for instance,\n\nhttps://t.co/mAtOryWVuF\nhttps://t.co/aTPDjN4CsY\nhttps://t.co/t8oaXfwlGg\nhttps://t.co/CLB1MX9FKj\nhttps://t.co/hzWxFeaYWm\n\n34/n', ""With log-Riemann surfaces we come back to the original nature of Riemann's notion of RIemann surface. \n\nWith Riemannium spaces we extend Riemann's Riemann surface idea by considering Borel monogenic continuation. \n\nThis open a vast field for research for future generations. 35/n"", '""Hurewicz covering""--&gt;""Hurewicz fibration""', 'Enjoy the reading!']",https://arxiv.org/abs/2004.10541,"The Riemann surface of a holomorphic germ is the space generated by its Weierstrass analytic continuation. The Riemannium space of a holomorphic germ is the space generated by its Borel monogenic continuation. Riemannium spaces are metric, path connected, Gromov length spaces, not necessarily $\sigma$-compact. We construct an example of Riemannium space: The Cantor Riemannium. ",The Cantor Riemannium
5,1258795497487302661,260971421,Walid Magdy,['Working on Sarcasm Detection?\n\nCheck our preprints of papers at #CSCW2020 &amp; #acl2020nlp. We study sarcasm communication online and release a new dataset (iSarcasm) that models sarcasm as intended by author.\n\nPaper 1: <LINK>\nPaper 2: <LINK>'],https://arxiv.org/abs/2004.04945,"Online social networks (OSN) play an essential role for connecting people and allowing them to communicate online. OSN users share their thoughts, moments, and news with their network. The messages they share online can include sarcastic posts, where the intended meaning expressed by the written text is different from the literal one. This could result in miscommunication. Previous research in psycholinguistics has studied the sociocultural factors the might lead to sarcasm misunderstanding between speakers and listeners. However, there is a lack of such studies in the context of OSN. In this paper we fill this gap by performing a quantitative analysis on the influence of sociocultural variables, including gender, age, country, and English language nativeness, on the effectiveness of sarcastic communication online. We collect examples of sarcastic tweets directly from the authors who posted them. Further, we ask third-party annotators of different sociocultural backgrounds to label these tweets for sarcasm. Our analysis indicates that age, English language nativeness, and country are significantly influential and should be considered in the design of future social analysis tools that either study sarcasm directly, or look at related phenomena where sarcasm may have an influence. We also make observations about the social ecology surrounding sarcastic exchanges on OSNs. We conclude by suggesting ways in which our findings can be included in future work. ",The Effect of Sociocultural Variables on Sarcasm Communication Online
6,1258688481997578240,908427517182320640,Blerina Sinaimeri,['Our new tool for #cophylogeny is finally released.\xa0 It includes listing suboptimal reconciliations and equivalent classes of optimal reconciliations \xa0<LINK>\n\nFor theoretical aspects check\xa0our paper <LINK>'],https://arxiv.org/abs/2004.12143,"When a problem has more than one solution, it is often important, depending on the underlying context, to enumerate (i.e., to list) them all. Even when the enumeration can be done in polynomial delay, that is, spending no more than polynomial time to go from one solution to the next, this can be costly as the number of solutions themselves may be huge, including sometimes exponential. Furthermore, depending on the application, many of these solutions can be considered equivalent. The problem of an efficient enumeration of the equivalence classes or of one representative per class (without generating all the solutions), although identified as a need in many areas, has been addressed only for very few specific cases. In this paper, we provide a general framework that solves this problem in polynomial delay for a wide variety of contexts, including optimization ones that can be addressed by dynamic programming algorithms, and for certain types of equivalence relations between solutions. ",A general framework for enumerating equivalence classes of solutions
7,1257841300507951104,852542314996277253,Mats Julius Stensrud,"['Excited to share our new article, which extends and generalizes the notion of separable effects in competing event settings <LINK>  @_MiguelHernan @JessGeraldYoung\n\nPS: the 1st sep. effects paper (<LINK>) appears in a major stats journal v. soon']",https://arxiv.org/abs/2004.14824,"In competing event settings, a counterfactual contrast of cause-specific cumulative incidences quantifies the total causal effect of a treatment on the event of interest. However, effects of treatment on the competing event may indirectly contribute to this total effect, complicating its interpretation. We previously proposed the separable effects (Stensrud et al, 2019) to define direct and indirect effects of the treatment on the event of interest. This definition presupposes a treatment decomposition into two components acting along two separate causal pathways, one exclusively outside of the competing event and the other exclusively through it. Unlike previous definitions of direct and indirect effects, the separable effects can be subject to empirical scrutiny in a study where separate interventions on the treatment components are available. Here we extend and generalize the notion of the separable effects in several ways, allowing for interpretation, identification and estimation under considerably weaker assumptions. We propose and discuss a definition of separable effects that is applicable to general time-varying structures, where the separable effects can still be meaningfully interpreted, even when they cannot be regarded as direct and indirect effects. We further derive weaker conditions for identification of separable effects in observational studies where decomposed treatments are not yet available; in particular, these conditions allow for time-varying common causes of the event of interest, the competing events and loss to follow-up. For these general settings, we propose semi-parametric weighted estimators that are straightforward to implement. As an illustration, we apply the estimators to study the separable effects of intensive blood pressure therapy on acute kidney injury, using data from a randomized clinical trial. ","Generalized interpretation and identification of separable effects in
  competing event settings"
8,1257687908049354759,1235205731747540993,Simone Scardapane,"['New #Asilomar2019 paper out!\n\n*Distributed Stochastic Nonconvex Optimization and Learning based on Successive Convex Approximation*\n\nWe consider #distributed, #stochastic optimization of non-convex models, especially #deep networks.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2004.14882,"We study distributed stochastic nonconvex optimization in multi-agent networks. We introduce a novel algorithmic framework for the distributed minimization of the sum of the expected value of a smooth (possibly nonconvex) function (the agents' sum-utility) plus a convex (possibly nonsmooth) regularizer. The proposed method hinges on successive convex approximation (SCA) techniques, leveraging dynamic consensus as a mechanism to track the average gradient among the agents, and recursive averaging to recover the expected gradient of the sum-utility function. Almost sure convergence to (stationary) solutions of the nonconvex problem is established. Finally, the method is applied to distributed stochastic training of neural networks. Numerical results confirm the theoretical claims, and illustrate the advantages of the proposed method with respect to other methods available in the literature. ","Distributed Stochastic Nonconvex Optimization and Learning based on
  Successive Convex Approximation"
9,1257680666541592583,872081043746168833,Jaejun Yoo,['CVPR 2020 CutBlur and MoA: A powerful data augmentation for various low-level vision tasks. \nPaper:\xa0<LINK>\nGithub:\xa0<LINK>\nSlides: <LINK>\nYoutube:\xa0<LINK> <LINK>'],https://arxiv.org/abs/2004.00448,"Data augmentation is an effective way to improve the performance of deep networks. Unfortunately, current methods are mostly developed for high-level vision tasks (e.g., classification) and few are studied for low-level vision tasks (e.g., image restoration). In this paper, we provide a comprehensive analysis of the existing augmentation methods applied to the super-resolution task. We find that the methods discarding or manipulating the pixels or features too much hamper the image restoration, where the spatial relationship is very important. Based on our analyses, we propose CutBlur that cuts a low-resolution patch and pastes it to the corresponding high-resolution image region and vice versa. The key intuition of CutBlur is to enable a model to learn not only ""how"" but also ""where"" to super-resolve an image. By doing so, the model can understand ""how much"", instead of blindly learning to apply super-resolution to every given pixel. Our method consistently and significantly improves the performance across various scenarios, especially when the model size is big and the data is collected under real-world environments. We also show that our method improves other low-level vision tasks, such as denoising and compression artifact removal. ","Rethinking Data Augmentation for Image Super-resolution: A Comprehensive
  Analysis and a New Strategy"
10,1257354977032441856,377049708,Eddie Lee,"['Our new paper on ""A scaling theory for armed conflict"" is up on the arXiv (<LINK>). We build a minimal model capturing how armed conflict propagates through space and time building on a scaling framework from a previous paper (<LINK>). (1/6)', 'We break down armed conflict into 3 parts: geographic spread, local social dynamics, and variation in intensity. These distinct processes are captured by scaling relations connecting multiple measures of conflict magnitude like duration, extent, and fatalities. (2/6)', 'Geographic growth refers to how conflict ""contagion"" spreads to nearby regions like cities or provinces. Social growth refers to how conflict events multiply at conflict-infected regions. (3/6)', 'Conflict intensity, or virulence, refers to variation in conflict intensity perhaps mediated by factors like weak governance or prosperity. These all are necessary and separate components required to unify all the conflict properties we study. (4/6)', 'Overall, the apparent chaos of conflict shows strong manifest regularities captured by our model. Why such social scaling patterns emerge is touched on in scaling in cities or elections. (5/6)', 'Our work provides insight into sources of such regularities in armed conflict, prediction, and perhaps intervention in conflict. (6/6)']",https://arxiv.org/abs/2004.14311,"Armed conflict data display scaling and universal dynamics in both social and physical properties like fatalities and geographic extent. We propose a randomly branching, armed-conflict model that relates multiple properties to one another in a way consistent with data. The model incorporates a fractal lattice on which conflict spreads, uniform dynamics driving conflict growth, and regional virulence that modulates local conflict intensity. The quantitative constraints on scaling and universal dynamics we use to develop our minimal model serve more generally as a set of constraints for other models for armed conflict dynamics. We show how this approach akin to thermodynamics imparts mechanistic intuition and unifies multiple conflict properties, giving insight into causation, prediction, and intervention timing. ",A scaling theory of armed conflict avalanches
11,1257017731263668229,764362094191996928,TH Nguyen,"['Our new research work—contributing to the ""Flood risk anticipation in Quebec province"" project ORACLE-2, subsidized by the Ministère de la Sécurité Publique, Gouv. du Québec—is now available.\n\nPaper: <LINK>\nGithub: <LINK> <LINK>']",https://arxiv.org/abs/2004.08522,"Automatic extraction of buildings in urban and residential scenes has become a subject of growing interest in the domain of photogrammetry and remote sensing, particularly since mid-1990s. Active contour model, colloquially known as snake model, has been studied to extract buildings from aerial and satellite imagery. However, this task is still very challenging due to the complexity of building size, shape, and its surrounding environment. This complexity leads to a major obstacle for carrying out a reliable large-scale building extraction, since the involved prior information and assumptions on building such as shape, size, and color cannot be generalized over large areas. This paper presents an efficient snake model to overcome such challenge, called Super-Resolution-based Snake Model (SRSM). The SRSM operates on high-resolution LiDAR-based elevation images -- called z-images -- generated by a super-resolution process applied to LiDAR data. The involved balloon force model is also improved to shrink or inflate adaptively, instead of inflating the snake continuously. This method is applicable for a large scale such as city scale and even larger, while having a high level of automation and not requiring any prior knowledge nor training data from the urban scenes (hence unsupervised). It achieves high overall accuracy when tested on various datasets. For instance, the proposed SRSM yields an average area-based Quality of 86.57% and object-based Quality of 81.60% on the ISPRS Vaihingen benchmark datasets. Compared to other methods using this benchmark dataset, this level of accuracy is highly desirable even for a supervised method. Similarly desirable outcomes are obtained when carrying out the proposed SRSM on the whole City of Quebec (total area of 656 km2), yielding an area-based Quality of 62.37% and an object-based Quality of 63.21%. ","Super-Resolution-based Snake Model -- An Unsupervised Method for
  Large-Scale Building Extraction using Airborne LiDAR Data and Optical Image"
12,1256942013582368770,738769492122214400,Johannes Lischner,"['Our new paper on quasiparticle properties of twisted bilayer #graphene is on the arXiv. We carry out atomistic Hartree calculations for unit cells with more than 10,000 atoms (they are hard work!) for different twist angles and doping levels. <LINK> <LINK>']",https://arxiv.org/abs/2004.14784,"A detailed understanding of interacting electrons in twisted bilayer graphene (tBLG) near the magic angle is required to gain insights into the physical origin of the observed broken symmetry phases. Here, we present extensive atomistic Hartree theory calculations of the electronic properties of tBLG in the (semi-)metallic phase as function of doping and twist angle. Specifically, we calculate quasiparticle properties, such as the band structure, density of states (DOS) and local density of states (LDOS), which are directly accessible in photoemission and tunnelling spectroscopy experiments. We find that quasiparticle properties change significantly upon doping - an effect which is not captured by tight-binding theory. In particular, we observe that the partially occupied bands flatten significantly which enhances the density of states at the Fermi level. We predict a clear signature of this band flattening in the LDOS in the AB/BA regions of tBLG which can be tested in scanning tunneling experiments. We also study the dependence of quasiparticle properties on the dielectric environment of tBLG and discover that these properties are surprisingly robust as a consequence of the strong internal screening. Finally, we present a simple analytical expression for the Hartree potential which enables the determination of quasiparticle properties without the need for self-consistent calculations. ","Hartree theory calculations of quasiparticle properties in twisted
  bilayer graphene"
13,1256274375466192896,942694791707545600,Thomas Scialom,"['#NLProc community has recently devoted stong efforts to strengthen multilingual approaches and datasets.\n\nI am proud to contribute with our new paper: MLSUM: The Multilingual Summarization Corpus\n\n<LINK>', 'In this paper, we release the first large scale multilingual summarization dataset. It includes 1.5M articles in French, Russian, Spanish, Turkish and German. \n\nKeep posted, the code to reproduce or extend it will be made soon available.', 'Work done with @mlia_lip6 and @RecitalAI\nWith my incredible co-authors\nPaul-Alexis Day,  Sylvain Lamprier, @benjaminpiw and @stjaco']",https://arxiv.org/abs/2004.14900,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset. ",MLSUM: The Multilingual Summarization Corpus
14,1256265652941307906,636899073,Dr Ella Peltonen,['The first version is out now: read our new 6G White Paper on Edge Intelligence! <LINK> #edgecomputing #EdgeAI #AI #6G #5G @6Gflagship @UniOulu <LINK>'],https://arxiv.org/abs/2004.14850,"In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge. ",6G White Paper on Edge Intelligence
15,1256247812158783491,1019160894788505600,yehudit96,"['Our new paper ""Paraphrasing Vs. Coreferring: Two sides of the same coin"" is out <LINK>, a joint work with Avi Caciularu, @VeredShwartz and Ido Dagan\nWe study how Cross documents coreferece resolution (CDCR) and paraphrase identification can benefit each other', '➡️We use a CDCR dataset as distant supervision on predicate paraphrases resource, which improve its performance by 18 points!', '⬅️Than, the improved resource is utilized as external resource for CDCR model, which yields SOTA results on cross documents coreferece resolution.']",http://arxiv.org/abs/2004.14979,"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model's performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other. ",Paraphrasing vs Coreferring: Two Sides of the Same Coin
16,1256238813652881415,3352696934,Gabrielle Ras,"[""Our new survey paper is out!!\nExplainable Deep Learning: A Field Guide for the Uninitiated \n<LINK>\n\nIt's basically a field guide for newcomers in the field, covering many many explanation methods for deep learning and much more! <LINK>"", 'The three main goals of our paper:\n✔️ Discuss the traits of a DL system that researchers enhance in XAI research\n✔️ Place XAI in the context of other DL research areas\n✔️ Introduce 3 simple dimensions defining the space of foundational methods that contribute to XDL']",https://arxiv.org/abs/2004.14545,"Deep neural networks (DNNs) have become a proven and indispensable machine learning tool. As a black-box model, it remains difficult to diagnose what aspects of the model's input drive the decisions of a DNN. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN's decisions has thus blossomed into an active, broad area of research. A practitioner wanting to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field has taken. This complexity is further exacerbated by competing definitions of what it means ``to explain'' the actions of a DNN and to evaluate an approach's ``ability to explain''. This article offers a field guide to explore the space of explainable deep learning aimed at those uninitiated in the field. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) finally elaborates on user-oriented explanation designing and potential future directions on explainable deep learning. We hope the guide is used as an easy-to-digest starting point for those just embarking on research in this field. ",Explainable Deep Learning: A Field Guide for the Uninitiated
17,1256235102763470849,892059194240532480,Mikel Artetxe,"['Check out our new position paper: ""A Call for More Rigor in Unsupervised Cross-lingual Learning"" (w/ @seb_ruder, @DaniYogatama, @glabaka &amp; @eagirre), accepted at #acl2020nlp\n\n<LINK>']",https://arxiv.org/abs/2004.14958,"We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models. ",A Call for More Rigor in Unsupervised Cross-lingual Learning
18,1256228087894851585,3885912072,Marshall Johnson,"[""New paper alert! <LINK> an analysis of the TESS light curve of the ultra hot Jupiter KELT-9 b. AFAIK the first author, John Ahlers, isn't on Twitter, so here's a summary thread: 1/"", ""KELT-9 b is a massive, short-period planet transiting a B9.5-A0 star; it is the hottest planet known, with a dayside temperature hotter than many stars! The planetary orbit is also highly inclined, passing over the stars' poles each orbit 2/n"", 'Because the star is very rapidly rotating, it is dynamically oblate-- the rotation tends to counteract gravity, and the equatorial radius is about 9% larger than the polar radius. The poles are closer to the center of the star and so are hotter and brighter than the equator 3/n https://t.co/y4d7pI6tCg', 'Due to the inclined orbit, during the transit the planet passes over the stellar pole and then the equator. The brightness of the obscured stellar surface changes over the transit, which makes the transit shape asymmetric! 4/n https://t.co/33G1j9VjgX', ""We model this to measure the 3-D geometry of the system, which usually isn't possible, especially from a light curve alone. The planet's orbit is inclined with respect to the stellar rotation by 87° (+/- 11°). 5/n"", 'The stellar equatorial bulge should also be torquing the planetary orbit, which would result in the orbit precessing. I calculated that the precession rate could be as large as 0.4° per year, which could be detectable though changes in transit data over time. 6/n', 'Another extreme aspect of the system is that the incident radiation (heating) experienced by the planet changes over the course of the orbit as it passes successively over the bright poles and dim equator, which could have interesting consequences for the planetary atmosphere 7/n', 'Co-authors on Twitter include @super_knova @bsgaudi @Astro_JRod @ProfSaraSeager (apologies if I missed anyone), and particular thanks to John Ahlers for leading a nice paper! 8/8', 'Forgot to mention in the first tweet: this is an analysis only of the *transit* light curve; Wong et al. already published a nice analysis of the phase curve: https://t.co/k7TUAVPYZQ 9/8']",https://arxiv.org/abs/2004.14812,"KELT-9 b is an ultra hot Jupiter transiting a rapidly rotating, oblate early-A-type star in a polar orbit. We model the effect of rapid stellar rotation on KELT-9 b's transit light curve using photometry from the Transiting Exoplanet Survey Satellite (\tess) to constrain the planet's true spin-orbit angle and to explore how KELT-9 b may be influenced by stellar gravity darkening. We constrain the host star's equatorial radius to be $1.089\pm0.017$ times as large as its polar radius and its local surface brightness to vary by $\sim38$\% between its hot poles and cooler equator. We model the stellar oblateness and surface brightness gradient and find that it causes the transit light curve to lack the usual symmetry around the time of minimum light. We take advantage of the light curve asymmetry to constrain KELT-9 b's true spin orbit angle (${87^\circ}^{+10^\circ}_{-11^\circ}$), agreeing with \citet{gaudi2017giant} that KELT-9 b is in a nearly polar orbit. We also apply a gravity darkening correction to the spectral energy distribution model from \citet{gaudi2017giant} and find that accounting for rapid rotation gives a better fit to available spectroscopy and yields a more reliable estimate for the star's polar effective temperature. ","KELT-9 b's Asymmetric TESS Transit Caused by Rapid Stellar Rotation and
  Spin-Orbit Misalignment"
19,1256208864376168452,41835547,Tijmen Blankevoort,"['Our new paper is up on Arxiv! We found out that rounding-to-nearest in #neuralnetwork #quantization is actually really really bad. Our new method AdaRound requires no training, little data, and pushes networks to 4 bit weights with high accuracy! <LINK>', '@honualx @gsautiere Not yet, but we released our model efficiency toolkit open source today! Adaround code might find its way in there too! https://t.co/XeaGs0Se0P']",http://arxiv.org/abs/2004.10568,"When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%. ",Up or Down? Adaptive Rounding for Post-Training Quantization
20,1256092529633738757,132760007,Karl Oskar Ekvall,['Posted paper on new likelihood-based method for principal components regression. Uses information in both responses and predictors to form PCs and often outperforms classical PCR substantially.\n\nLink : <LINK>'],https://arxiv.org/abs/2004.14009,"We propose a principal components regression method based on maximizing a joint pseudo-likelihood for responses and predictors. Our method uses both responses and predictors to select linear combinations of the predictors relevant for the regression, thereby addressing an oft-cited deficiency of conventional principal components regression. The proposed estimator is shown to be consistent in a wide range of settings, including ones with non-normal and dependent observations; conditions on the first and second moments suffice if the number of predictors ($p$) is fixed and the number of observations ($n$) tends to infinity and dependence is weak, while stronger distributional assumptions are needed when $p \to \infty$ with $n$. We obtain the estimator's asymptotic distribution as the projection of a multivariate normal random vector onto a tangent cone of the parameter set at the true parameter, and find the estimator is asymptotically more efficient than competing ones. In simulations our method is substantially more accurate than conventional principal components regression and compares favorably to partial least squares and predictor envelopes. The method's practical usefulness is illustrated in a data example with cross-sectional prediction of stock returns. ",Targeted Principal Components Regression
21,1256028628527316994,387885717,Teague,"[""Check out our new paper on arXiv -- <LINK> -- we implement a new hybrid quantum-classical algorithm, recently proposed by @quantum_aram, and benchmark its performance using @IBMResearch's publicly available ibmq_rome processor!""]",https://arxiv.org/abs/2004.14970,"Many quantum algorithms for machine learning require access to classical data in superposition. However, for many natural data sets and algorithms, the overhead required to load the data set in superposition can erase any potential quantum speedup over classical algorithms. Recent work by Harrow introduces a new paradigm in hybrid quantum-classical computing to address this issue, relying on coresets to minimize the data loading overhead of quantum algorithms. We investigate using this paradigm to perform $k$-means clustering on near-term quantum computers, by casting it as a QAOA optimization instance over a small coreset. We compare the performance of this approach to classical $k$-means clustering both numerically and experimentally on IBM Q hardware. We are able to find data sets where coresets work well relative to random sampling and where QAOA could potentially outperform standard $k$-means on a coreset. However, finding data sets where both coresets and QAOA work well--which is necessary for a quantum advantage over $k$-means on the entire data set--appears to be challenging. ",Coreset Clustering on Small Quantum Computers
22,1256026534130757632,759894532649545732,Aravind Srinivas,"['New paper - Reinforcement Learning with Augmented Data (RAD)!  Data augmentation *alone*  can achieve SoTA on DMControl and test-time generalization on ProcGen! \nPaper: <LINK>\nProject Page: <LINK>\nCode: <LINK> <LINK>', 'We perform the first extensive study of data augmentation in the RL from pixels setting and show, surprisingly, that simple RL algorithms with data augmentation *alone* and *no other changes* can achieve SoTA data-efficiency on DMControl and test-time generalization on ProcGen! https://t.co/LOU43uOtFt', 'RAD outperforms previous SoTA method CURL on both the 100K and 500K timesteps benchmarks on DeepMind Control for data-efficiency. https://t.co/x7CSoPb3dp', 'We also observe that data augmentation leads to improved generalization to new levels on the OpenAI ProcGen benchmarks and requires only half the number of training levels compared to the baseline. https://t.co/quTF7d9lw1', 'Our method is very simple and can work with any RL algorithm (off-policy like SAC, on-policy like PPO) and makes *no changes at all* to the actual RL algorithm. Just add data augmentation and you get SoTA performance. https://t.co/aR8cdYwb04', 'We extensively ablated for different data augmentations and observed that random crop is the most effective on DeepMindControl! https://t.co/Mm2xQ4DoA3', 'We also tried to visualize why random crop is extremely effective and it turns out that the agent focuses on what matters and ignores everything else compared to other augmentations or the scenario with no augmentation. https://t.co/vijLht0Sq6', 'On OpenAI ProcGen, different data-augmentations seem to help according to the task at hand, example, random crop for BigFish, color-jitter for Jumper. https://t.co/RtbOFj2lrv', 'Our data-aug implementation is extremely simple, easy to use, fastest in terms of wall clock, and optimized for the usage of data-augmentations in deep reinforcement learning as a plug-and-play module: https://t.co/QzzRkuipsA (both on-policy and off-policy RL)', 'Data Augmentation has been critical to Supervised Learning. @quocleix has pushed on the importance of data augmentations for supervised learning. We take inspiration from that and make no other changes and see that the performance gains are significant.', 'RAD is a follow up to our earlier work on CURL which also used data augmentations with contrastive learning. We offer a discussion at the end of the paper on the tradeoffs between CURL and RAD, discussing the question of whether data augmentation *alone* is sufficient for RL.', 'Concurrent and independent to us, @denisyarats, @ikostrikov and Rob Fergus also saw similar conclusions with the DeepMind Control Suite. Tweet by @hardmaru below: https://t.co/2hVBMdBfpp', 'We hope through these papers, people begin to use data-augmentations as a standard practice in deep reinforcement learning from pixels. Thanks to co-leads @MishaLaskin and @kimin_le2 and collaborators @stookemon\n@LerrelPinto @pabbeel. Have fun using RAD in your research!', ""@hardmaru @MishaLaskin @kimin_le2 @stookemon @LerrelPinto @pabbeel Thanks David. It's more an artifact of simple things working with minimal tweaks quickly than us being unnaturally productive. 🙂""]",https://arxiv.org/abs/2004.14990,"Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at this https URL ",Reinforcement Learning with Augmented Data
23,1256025959695777793,3290526443,Jia-Bin Huang,"['Check out our #SIGGRAPH2020 paper on Consistent Video Depth Estimation.  Our geometrically consistent depth enables cool video effects to a whole new level!\n\nVideo: <LINK>\nPaper: <LINK>\nProject page: <LINK> <LINK>', 'Joint work with amazing co-authors Xuan Luo (@XuanLuo14), Richard Szeliski, Kevin Matzen, and Johannes Kopf (@JPKopf).', '@aiwakr Thanks, Avinash! Glad to see you here on Twitter!']",https://arxiv.org/abs/2004.15021,"We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects. ",Consistent Video Depth Estimation
24,1256021691131547650,1019072664600637440,Julian Michael,"['Tired of telling probing models what to think? Check out our new paper, Asking without Telling: Exploring Latent Ontologies in Contextual Encoders. <LINK>\n\nWe probe for latent linguistic variables, learning fine-grained clusters from binary supervision alone. <LINK>', 'In contrast to fully supervised correlation-finding, this draws *emergent structure* out of the encoder. Surprisingly, it recovers some familiar notions: for example, when probed to detect entities, ELMo provides a clear ""person"" subclass on its own! (left: BERT, right: ELMo) https://t.co/cjKBpSaTF0', 'We find more ontological matches, plus some interesting mismatches, such as distinguishing small/large numbers and preferring fine-grained semantic roles. This kind of result would be hard to find with fully supervised probing.', 'We can also visualize and explore ontological overlap on various tasks using normalized PMI between gold labels in the induced clusters (blue/positive: lumping, red/negative: splitting). https://t.co/S38Hwwb7gV', 'See the paper for more results, discussion, and future directions. Lucky to have done this work while interning with @iftenney and @jjjanbot at @GoogleAI. Many thanks to them and the team!']",https://arxiv.org/abs/2004.14513,"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods. ","Asking without Telling: Exploring Latent Ontologies in Contextual
  Representations"
25,1256017442389798912,1012125662117851136,Edward Kennedy,"['Very excited to share new work on flexible estimation of heterogeneous effects:\n\n<LINK>\n\n1st part is more practical &amp; gives flexible error bds. 2nd part is more theoretical &amp; tries to find best possible error.\n\nMight’ve learned more in this than any other paper <LINK>', '(Though a lot of the new stuff I learned didn’t actually make it into the paper... stay tuned)']",https://arxiv.org/abs/2004.14497,"Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations. ",Optimal doubly robust estimation of heterogeneous causal effects
26,1255992777520762880,93358739,Mandar Joshi,"['New paper on representing texts by contextualizing them jointly with textual encyclopedic knowledge (TEK) retrieved dynamically from multiple documents in Wikipedia. Joint work with @kentonctlee, Yi Luan &amp; @toutanova from my internship at @GoogleAI. <LINK> (1/4) <LINK>', 'Our approach complements knowledge encoded in pretrained Transformers with dynamically retrieved sentences about entities mentioned in input text. For reading comprehension, this can help better define phrases in the context &amp; their relationship to phrases in the question. (2/4) https://t.co/tJCHFyRqEg', 'Simply encoding input text with sentences retrieved using entity linking works pretty well since it allows direct reuse of powerful pretrained encoders. Pretraining on this kind of input---text augmented with more background about entities---further improves performance. (3/4)', 'Results: On TriviaQA, TEK-enriched representations see improvements of 1.6-3.1 F1 over comparable RoBERTa base/large models. On MRQA, we see gains in-domain along with larger improvements out-of-domain on BioASQ (2.1-4.2 F1), TextbookQA (1.6-2.0 F1), and DuoRC (1.1-2.0 F1). (4/4) https://t.co/d9UhvYLqW3']",http://arxiv.org/abs/2004.12006,"We present a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents. We apply our method to reading comprehension tasks by encoding questions and passages together with background sentences about the entities they mention. We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders. Moreover, knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text. On TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically. On MRQA, a large collection of diverse QA datasets, we see consistent gains in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2 F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1). ",Contextualized Representations Using Textual Encyclopedic Knowledge
27,1255961693190082560,520665133,R. T. Pierrehumbert,['Our new paper on silicate weathering and the outer edge of the habitable zone: <LINK>'],https://arxiv.org/abs/2004.14058,"The ""liquid water habitable zone"" (HZ) concept is predicated on the ability of the silicate weathering feedback to stabilize climate across a wide range of instellations. However, representations of silicate weathering used in current estimates of the effective outer edge of the HZ do not account for the thermodynamic limit on concentration of weathering products in runoff set by clay precipitation, nor for the energetic limit on precipitation set by planetary instellation. We find that when the thermodynamic limit is included in an idealized coupled climate/weathering model, steady-state planetary climate loses sensitivity to silicate dissolution kinetics, becoming sensitive to temperature primarily through the effect of temperature on runoff and to pCO$_2$ through an effect on solute concentration mediated by pH. This increases sensitivity to land fraction, CO$_2$ outgassing, and geological factors such as soil age and lithology, all of which are found to have a profound effect on the position of the effective outer edge of the HZ. The interplay between runoff sensitivity and the energetic limit on precipitation leads to novel warm states in the outer reaches of the HZ, owing to the decoupling of temperature and precipitation. We discuss strategies for detecting the signature of silicate weathering feedback through exoplanet observations in light of insights derived from the revised picture of weathering. ","Thermodynamic and Energetic Limits on Continental Silicate Weathering
  Strongly Impact the Climate and Habitability of Wet, Rocky Worlds"
28,1255924647633264643,977394713278820355,Shreyas Vissapragada,"['newest paper is out!! <LINK>\n\nwe came up with a new way to detect outflowing atmospheres on exoplanets using the metastable helium triplet. \n\nhere’s the story of how we came up with this, and what we can do going forward (1/n) <LINK>', 'around winter break last year, i was finishing up a paper on IR photometry at palomar. we had a neat lil thing called a diffuser that made our transit light curves really precise, and used it to follow up faint systems from kepler (2/n) https://t.co/vvpaF81OWJ', 'i was trying to think of unique applications of the diffuser in the IR. around the same time i was reading some beautiful work on the helium 1083 nm line from Jess Spake, Romain Allart, and Lisa Nortmann (3/n) https://t.co/kkWqu1e1Mg', 'i asked myself (not super seriously), wouldn’t it be cool if we could do that? we don’t have a spectrograph, but what if we had a suuuuper narrow filter centered on this line? i did the calculation, and lo and behold...the numbers worked?? (4/n) https://t.co/cf4xQUvKjH', 'i was surprised and intrigued so i brought the idea to my advisor heather. she was super encouraging! every week we pushed and made the calculation more realistic until we both believed this filter was a real possibility (5/n) https://t.co/aH3haoiDLr', 'we brought it up at the WIRC instrument meeting, where others brought up an effect we hadn’t considered: the AOI shift. filters let through slightly different wavelengths of light at different tilts! not a big prob normally, but a HUGE issue for an ultra-narrowband filter (6/n) https://t.co/5pAK47C5yw', 'we needed a way to beat the AOI shift. one idea was some kind of discharge lamp. by filling a tube with helium and running a discharge through it, we could make the helium line in emission and see it in our setup. that way we’d know exactly where to place our target (7/n) https://t.co/1hAvF4iOrH', 'i was getting ready to try and make it myself (TERRIBLE idea) when an engineer at palomar named Jamey Eriksen found a spare helium arc lamp from an old spectrograph, giving us a calibration source to beat the AOI shift! (8/n) https://t.co/Rb4qIEYiLz', 'we then ordered the filter from Alluxa. more than i’ve spent on something for science ever, which was terrifying. i didn’t want to be wrong and waste people’s time and money (9/n) https://t.co/kO99CzUvz1', 'so...the wait for production was rough for me. i’d wake up some nights convinced i did the SNR calculation wrong and i wasted months of folks’ time and thousands of dollars and wouldn’t be able to sleep until i re-did the calculation (10/n) https://t.co/DsNkhlTEIV', 'then we got it, tested it, got it installed at palomar, and it was time for our first real integration test. if our detector glowed when viewing the helium lamp through our filter, we would know we had beaten the AOI shift for real (11/n) https://t.co/RBZpYq6Mpi', 'and...boom. we did it. (12/n) https://t.co/2laDXGfoRq', 'we then measured the 1083.3 nm light curve of WASP-69b, and got something that matched previous observations! @astronoleb and Antonija Oklopčić modeled the light curve and placed great constraints on the outflowing atmosphere (13/n) https://t.co/ZXl19aDbEb', 'the next month, we measured the 1083.3 nm for WASP-52b, a good bit fainter than the last target. non-detection, but we were able to rule out a large outflow and *still* place a strong constraint on any outflows (14/n) https://t.co/9snqwsX8N2', 'i’m so excited to use our new machine to observe atmospheric escape from exoplanets in the coming years!!! (15/n) https://t.co/LpVxIAqtdn', '@nataliadreyes thanks so much!!! 🎈🎈🎈', '@exoZafar thanks!!! the filter is 0.635 nm FWHM. that’s the goal right now, but with some additional considerations taken in building the target list 😊', '(to be clear this is NOT a direct image of a planet! this is light from our lamp imaged onto the detector. on this scale, stars are ~10 px in width, and planets are imperceptible. we observe an exoplanet when it passes in front of and dims its host star)', '@sylvia_bisco thanks a bunch!!', '@astro_catherine thank you!!!😊', '@AstroDaria thanks so much!!', '@blogdiva @ProfAbelMendez thank you for the kind words 😊', '@CapricePhillips lmao it captures *exactly* how i felt the moment it was mentioned', '@CapricePhillips thanks so much!! btw, i really enjoyed reading your recent paper — what a mysterious object 🤔']",https://arxiv.org/abs/2004.13728,"Infrared observations of metastable 2$^3$S helium absorption with ground- and space-based spectroscopy are rapidly maturing, as this species is a unique probe of exoplanet atmospheres. Specifically, the transit depth in the triplet feature (with vacuum wavelengths near 1083.3 nm) can be used to constrain the temperature and mass loss rate of an exoplanet's upper atmosphere. Here, we present a new photometric technique to measure metastable 2$^3$S helium absorption using an ultra-narrowband filter (full-width at half-maximum of 0.635 nm) coupled to a beam-shaping diffuser installed in the Wide-field Infrared Camera (WIRC) on the 200-inch Hale Telescope at Palomar Observatory. We use telluric OH lines and a helium arc lamp to characterize refractive effects through the filter and to confirm our understanding of the filter transmission profile. We benchmark our new technique by observing a transit of WASP-69b and detect an excess absorption of $0.498\pm0.045$% (11.1$\sigma$), consistent with previous measurements after considering our bandpass. Then, we use this method to study the inflated gas giant WASP-52b and place a 95th-percentile upper limit on excess absorption in our helium bandpass of 0.47%. Using an atmospheric escape model, we constrain the mass loss rate for WASP-69b to be $5.25^{+0.65}_{-0.46}\times10^{-4}~M_\mathrm{J}/\mathrm{Gyr}$ ($3.32^{+0.67}_{-0.56}\times10^{-3}~M_\mathrm{J}/\mathrm{Gyr}$) at 7,000 K (12,000 K). Additionally, we set an upper limit on the mass loss rate of WASP-52b at these temperatures of $2.1\times10^{-4}~M_\mathrm{J}/\mathrm{Gyr}$ ($2.1\times10^{-3}~M_\mathrm{J}/\mathrm{Gyr}$). These results show that ultra-narrowband photometry can reliably quantify absorption in the metastable helium feature. ","Constraints on Metastable Helium in the Atmospheres of WASP-69b and
  WASP-52b with Ultra-Narrowband Photometry"
29,1255908830246035456,2842875815,Raquel Fernández,"['Excited to share a preprint of a new paper led by @glnmario “Analysing Lexical Semantic Change with Contextualised Word Representations”, including a new evaluation dataset.\n#ACL2020nlp  <LINK>\n\nThread👇🏽 <LINK>']",http://arxiv.org/abs/2004.14118,"This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction. ","Analysing Lexical Semantic Change with Contextualised Word
  Representations"
30,1255889440083447810,560473379,nick frosst,"[""my new paper with @rishabh_467 @geoffreyhinton, Rich Caruana and Xuezhou Zhang is out on arxiv today! It’s about interpretability and neural additive models. Don't have time to read the paper? Read this tweet thread instead :) \n\n<LINK>\n\n1/7"", 'We present Neural Additive Models (NAMs) a simple extension to Generalized Additive Models (GAMs) making use of Neural Nets\n\nNAMs handle each input dimension separately\n\nSpecifically we learn \nE[y] = g(x) = β + f1(x1) + f2(x2) + · · · + fK(xK) \nWhere each f is a neural net\n\n2/7 https://t.co/ijSMEBX27K', 'This odd formulation allows us to perfectly visualize the model.\n\nEach input contributes to the prediction individually, so we can plot each input on a graph and show how the prediction changes as the input changes.\n\n3/7 https://t.co/9XyerOFVlj', 'This work effectively makes our models as explainable as logistic regression but with significantly increased expressibility\n\nIn fact once these graphs have been made, we can actually throw away the parameters of the model and simply make predictions based on the graphs.\n\n4/7 https://t.co/KyoHJHAMog', 'Crucially it also allows us to see and correct bias in trained models. \n\nFor instance, we can clearly see racial bias in the data by training a NAM on a popular recidivism dataset. \n\nBut we can correct this bias in the model by manually adjusting the graphs.\n\n5/7 https://t.co/2RG0hFCVg8', 'We can also query the model to understand exactly why a particular decision was made. \n\nHere we have visualized the effects of each input variable for a person given a high predicted credit score, and a person given a low predicted credit score.\n\n6/7 https://t.co/SEtdioXJNr', 'Check out the paper for more fun graphs and a bunch of weird tricks we needed to do to get these things to work.  :) \n\nhttps://t.co/eGPsL55v8G \n\n7/7', '@tw_killian @rishabh_467 @geoffreyhinton thanks man :)', '@rishabh16_ @kaixhin @rishabh_467 @geoffreyhinton not a silly question at all :)', '@F_Vaggi yeah this is a good point. The model could still rely a feature that is correlated with a feature we would like to remove. The paper addresses some tricks we use to try to get correlated features to not spread out their effects, but we cant guarantee that it works 100%', '@F_Vaggi i dont know what a Grahm-Schmidt orthogonalization is :P but it sounds like it could work. i will do some wikipedia reading :)', '@F_Vaggi Oh yeah that would definitely do the trick :) thanks for explaining', '@AlexLau96140796 @rishabh_467 @geoffreyhinton Thanks :) this model explicitly handles each variable independently. Inter variable interactions would need to be hand engineered as input features.']",https://arxiv.org/abs/2004.13912,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19. ",Neural Additive Models: Interpretable Machine Learning with Neural Nets
31,1255870325448941568,764054029,Dr. Sarah E Moran,"['So, despite ~the world~ and its current of global pandemic...some new science! Very happy to share my second 1st author paper, which will be published soon in PSJ.\n\nOn exoplanet hazes, and what they might be made of. \n<LINK>\nThread! 1/n', ""You might know that @horstlab has been busy cooking up exoplanet hazes in the lab. While we've learned lots of stuff about these hazes in super-Earth and mini-Neptunes, we still hadn't figured out what the stuff was that we had made.\n\nEnter me, a baby grad student.\n2/n"", ""I spent a month in France my first summer of grad school (a hard life) doing mass spectrometry measurements on the solid particles made in the lab. \n\nTurns out the particles were kind of mean, tho.\nTo put them in the mass spec, I had to dissolve them. And some didn't want to.\n3/n"", ""I ended up with this plot, showing that which conditions dissolved and which didn't (we did a range of cool-ish to warm-ish atmospheres, depending who you ask over a range of compositions). This is helpful for more than just mass spec measurements, tho!\n\n4/n https://t.co/yZWXmVoNlU"", 'The solvents that dissolved the lab hazes were polar. These hazes could therefore be good cloud seeds-- soluble particles tend to be better at it. So maybe polar material (cough*water*cough) clouds can form with the help of these hazes! (@JonesKuma might tell us more soon!)\n5/n', 'Now the actual mass spec data. It was MESSY. It was CONFUSING. It took 2 years of MY LIFE. I spent ages battling the data to make it play with code to analyze it. \n\nWhy? Because these hazes are super different than the Titan hazes that the code was written for by my advisor.\n6/n', 'How so? These hazes have WAY MORE OXYGEN than Titan haze. Which means all the  oxygen from the water and CO and CO2 in our initial gas mixtures is happy to join the solid particles! https://t.co/JjbUBa4NAQ', ""8/n (I'm bad at counting)\n\nAfter we had the bulk composition, it was time to find specific molecules.  \n\nThere were LOTS. I'm talking 6 full tables in the paper listing all the potential prebiotic formulas we found\n\n-- Nucleotide bases, amino acids, and SUGAR formulas"", ""To our knowledge, no one's seen sugar formulas in the solids out of an atmosphere experiment w/o liquid water. Super interesting. (Word of warning that the mass spec data only shows us the formulas, can't do molecular structure this way to confirm)\n\n9/n"", 'If we CAN confirm that these molecules are really there (and not just impostor formulas, the structure could be different), that means these experimental exoplanet atmospheres alone can start to make some pretty important building blocks for life.\n\n(Have your obligatory gif)\n10/n https://t.co/lH6LkeMfPX', ""There's more discussion in the paper about lots more stuff, but you should probably just go read it ;)\n\nI'm so proud that this project is complete and so thankful to my advisors @PlanetDr &amp; @NikoleKLewis for the help and encouragement along the way\n11/n"", 'Also thanks to all my coauthors, @vFrogette (who also graciously hosted me in her lab for a month), Chao, Laurene, Julie, Nicole, FROD, Josh, Cedric, @ElizaKempton, @astromarkmarley, @AstroCaroline, and Jeff.\n12/12\n\nFIN. https://t.co/UEFyqUgJw6']",https://arxiv.org/abs/2004.13794,"Very little experimental work has been done to explore the properties of photochemical hazes formed in atmospheres with very different compositions or temperatures than that of the outer solar system or of early Earth. With extrasolar planet discoveries now numbering thousands, this untapped phase space merits exploration. This study presents measured chemical properties of haze particles produced in laboratory analogues of exoplanet atmospheres. We used very high resolution mass spectrometry to measure the chemical components of solid particles produced in atmospheric chamber experiments. Many complex molecular species with general chemical formulas C$_w$H$_x$N$_y$O$_z$ were detected. We detect molecular formulas of prebiotic interest in the data, including those for the monosaccharide glyceraldehyde, a variety of amino acids and nucleotide bases, and several sugar derivatives. Additionally, the experimental exoplanetary haze analogues exhibit diverse solubility characteristics, which provide insight into the possibility of further chemical or physical alteration of photochemical hazes in super-Earth and mini-Neptune atmospheres. These exoplanet analogue particles can help us better understand chemical atmospheric processes and suggest a possible source of in situ atmospheric prebiotic chemistry on distant worlds. ","Chemistry of Temperate Super-Earth and Mini-Neptune Atmospheric Hazes
  from Laboratory Experiments"
32,1255846693247111168,704142836736765952,Patrick Gaulme,"[""Our new paper is out today on Arxiv ! It's about rotation or red giant stars <LINK>""]",https://arxiv.org/abs/2004.13792,"The objective of this work is to determine what fraction of red-giant (RG) stars shows photometric rotational modulation, and understand its origin. One of the underlying questions is the role of close binarity in this population, standing upon the fact that RGs in short-period binary systems (<150 days or so) have been observed to display strong rotational modulation. We select a sample of about 4500 relatively bright RGs observed by Kepler, and show that 370 of them (8%) display rotational modulation. Almost all have oscillation amplitudes below the median of the sample, while 30 of them are not oscillating at all. Of the 85 of these RGs with rotational modulation chosen for follow-up radial-velocity observation and analysis, 34 show clear evidence of spectroscopic binarity. Surprisingly, 26 of the 30 non-oscillators are in this group of binaries. To the contrary, about 85% of the active RGs with detectable oscillations are not part of close binaries. With the help of stellar masses and evolutionary states computed from the oscillation properties, it appears that low-mass red-giant branch stars tend to be magnetically inactive, while intermediate-mass ones tend to be highly active. The opposite trends are true for helium-core burning (red clump) stars, whereby the lower-mass clump stars are comparatively more active and the higher-mass ones less so. In other words, we find that low-mass red-giant branch stars gain angular momentum as they evolve to clump stars, while higher-mass ones lose angular momentum. The trend observed with low-mass stars leads to possible scenarios of planet engulfment or other merging events during the shell-burning phase. Regarding intermediate-mass stars, the rotation periods are long with respect to theoretical expectations reported in the literature, which reinforces the existence of an unidentified sink of angular momentum after the main sequence. ",Active red giants: close binaries versus single rapid rotators
33,1255837446702616576,1006267857875931136,Alfonso Cevallos,"['<LINK> New paper together with Alistair Stewart from @web3foundation, on how to elect validators in #proofofstake based Polkadot to ensure security and decentralization.']",https://arxiv.org/abs/2004.12990,"The property of proportional representation in approval-based committee elections has appeared in the social choice literature for over a century, and is typically understood as avoiding the underrepresentation of minorities. However, we argue that the security of some distributed systems is directly linked to the opposite goal of avoiding the overrepresentation of any minority, a goal not previously formalized that leads us to an optimization objective known as maximin support. After providing a thorough analysis of the computational complexity of this objective, we propose a new efficient election rule that simultaneously achieves a) a constant-factor approximation guarantee for it, and b) the property of proportional justified representation (PJR) - one of the strongest forms of proportional representation. However, the most striking feature of the new rule is that one can verify in linear time that the winning committee satisfies the two aforementioned guarantees, even when the algorithm is executed by an untrusted party who only communicates the output. As a result, the rule can be adapted into a verifiable computing scheme. Moreover, its verification procedure easily admits parallel processing for further efficiency. Our work is motivated by an application on blockchain networks that implement Nominated Proof-of-Stake, where the community elects a committee of validators to participate in the consensus protocol, and where preventing overrepresentation protects the network against attacks by an adversarial minority. Our election rule enables a validator selection protocol with formal guarantees on security and proportionality, and its adaptation as a verifiable computing scheme with a parallelized verification proves to be key for its successful implementation given the computationally limited nature of the blockchain architecture. ",A verifiably secure and proportional committee election rule
34,1255819317431631873,14077578,Pete Maynard,"[""New paper, 'Towards Understanding Man-on-the-Side Attacks (MotS) in SCADA Networks' - Anyone remember Quantum Insert? <LINK>""]",https://arxiv.org/abs/2004.14334,"We describe a new class of packet injection attacks called Man-on-the-Side Attacks (MotS), previously only seen where state actors have ""compromised"" a number of telecommunication companies. MotS injection attacks have not been widely investigated in scientific literature, despite having been discussed by news outlets and security blogs. MotS came to attention after the Edward Snowden revelations, which described large scale pervasive monitoring of the Internet's infrastructure. For an advanced adversary attempting to interfere with IT connected systems, the next logical step is to adapt this class of attack to a smaller scale, such as enterprise or critical infrastructure networks. MotS is a weaker form of attack compared to a Man-in-the-Middle (MitM). A MotS attack allows an adversary to read and inject packets, but not modify packets sent by other hosts. This paper presents practical experiments where we have implemented and performed MotS attacks against two testbeds: 1) on HTTP connections, by redirecting a victim to a host controlled by an adversary; and 2) on an Industrial Control network, where we inject falsified command responses to the victim. In both cases, the victims accept the injected packets without generating a suspiciously large number of unusual packets on the network. We then perform an analysis of three leading Network IDS to determine whether the attacks are detected, and discuss mitigation methods. ",Towards Understanding Man-on-the-Side Attacks (MotS) in SCADA Networks
35,1255790835221872640,1087849177642610690,R.J. Graham☆彡,"['A new paper on the arxiv by yours truly and @ClimateBook (the 1st paper that will be in my thesis), examining the consequences of thermodynamic and energetic limits to continental silicate weathering for climate on Earth-like planets 🌋🌧️🏔️🌊\n\n<LINK>', '@AndrewIWilliams @ClimateBook lmao i am now going to include an image or gif of Hannibal saying this in all of my future discussions of the WHAK model', '@AndrewIWilliams @ClimateBook Bye bye Ray!! https://t.co/toEIYgKLWL']",https://arxiv.org/abs/2004.14058,"The ""liquid water habitable zone"" (HZ) concept is predicated on the ability of the silicate weathering feedback to stabilize climate across a wide range of instellations. However, representations of silicate weathering used in current estimates of the effective outer edge of the HZ do not account for the thermodynamic limit on concentration of weathering products in runoff set by clay precipitation, nor for the energetic limit on precipitation set by planetary instellation. We find that when the thermodynamic limit is included in an idealized coupled climate/weathering model, steady-state planetary climate loses sensitivity to silicate dissolution kinetics, becoming sensitive to temperature primarily through the effect of temperature on runoff and to pCO$_2$ through an effect on solute concentration mediated by pH. This increases sensitivity to land fraction, CO$_2$ outgassing, and geological factors such as soil age and lithology, all of which are found to have a profound effect on the position of the effective outer edge of the HZ. The interplay between runoff sensitivity and the energetic limit on precipitation leads to novel warm states in the outer reaches of the HZ, owing to the decoupling of temperature and precipitation. We discuss strategies for detecting the signature of silicate weathering feedback through exoplanet observations in light of insights derived from the revised picture of weathering. ","Thermodynamic and Energetic Limits on Continental Silicate Weathering
  Strongly Impact the Climate and Habitability of Wet, Rocky Worlds"
36,1255778470681997312,1243024780640448513,Spyros Tserkis,"['Our new paper about ""entanglement potential"" appeared on arXiv today\n\n<LINK>']",https://arxiv.org/abs/2004.13948,"We quantify the maximum amount of entanglement of formation (EoF) that can be achieved by continuous-variable states under passive operations, which we refer to as EoF-potential. Focusing, in particular, on two-mode Gaussian states we derive analytical expressions for the EoF-potential for specific classes of states. For more general states, we demonstrate that this quantity can be upper-bounded by the minimum amount of squeezing needed to synthesize the Gaussian modes, a quantity called squeezing of formation. Our work, thus, provides a new link between non-classicality of quantum states and the non-classicality of correlations. ","Maximum entanglement of formation for a two-mode Gaussian state over
  passive operations"
37,1255768143596830721,135150782,Michele Ginolfi,"['My new paper is out!\n""CGM pollution and gas mixing by tidal stripping in a merging system at z~4.57"" <LINK>\nHere we show ALMA/HST+ observations of an interesting major merging system at z~4.5 (observed in ALPINE), close to a density peak of a protocluster.\n1/ <LINK>', 'ALMA reveals [CII] arising from an extended structure (~30 kpc) surrounding the system, and about 50% of the total flux resides *between* the individual galaxy components, in a sort of metal-enriched gaseous envelope with a disturbed morphology and complex kinematics. \n2/', 'Similarly to shock-excited [CII] observed in tidal tails in local groups, we interpret our results as a possible signature of ISM stripped by strong gravitational interactions, with some contribution from material ejected by outflows and SF in small faint satellites.\n3/', 'Our findings suggest that strong dynamical interactions in major merging systems at high-z can be an efficient mechanism for extracting gas out of galaxies and mixing the CGM with metals. This might also represent a natural channel to feed and enrich the nascent proto-ICM.\n4/4', 'PS: the paper is submitted to A&amp;A. Any comments are welcome!\n\nPPS: stay tuned for future ALPINE papers on the morpho-kinematical characterisation of high-z galaxies.']",http://arxiv.org/abs/2004.13737,"We present ALMA observations of a merging system at z ~ 4.57, observed as a part of the ALMA Large Program to INvestigate [CII] at Early times (ALPINE) survey. Combining ALMA [CII] 158 micron and far-infrared continuum data with multi-wavelength ancillary data we find that the system is composed of two massive (Mstar >~ 10^10 Msun) star-forming galaxies experiencing a major merger (stellar mass ratio r_mass ~ 0.9) at close spatial (~13 kpc; projected) and velocity (delta_v < 300 km/s) separations, and two additional faint narrow [CII]-emitting satellites. The overall system belongs to a larger-scale protocluster environment and is coincident to one of its overdensity peaks. ALMA reveals also the presence of [CII] emission arising from a circumgalactic gas structure, extending up to a diameter-scale of ~30 kpc. Our morpho-spectral decomposition analysis shows that about 50% of the total flux resides between the individual galaxy components, in a metal-enriched gaseous envelope characterized by a disturbed morphology and complex kinematics. Similarly to observations of shock-excited [CII] emitted from tidal tails in local groups, our results can be interpreted as a possible signature of interstellar gas stripped by strong gravitational interactions, with a possible contribution from material ejected by galactic outflows and emission triggered by star formation in small faint satellites. Our findings suggest that mergers could be an efficient mechanism of gas mixing in the circumgalactic medium around high-z galaxies, and thus play a key role in the galaxy baryon cycle at early epochs. ","The ALPINE-ALMA [CII] Survey: CGM pollution and gas mixing by tidal
  stripping in a merging system at z~4.57"
38,1255605888024379392,7984662,Clayton Shonkwiler,['New paper: “Distributions of distances and volumes of balls in homogeneous lens spaces” with Chris Peterson and our student Brenden Balch\n\n<LINK> <LINK>'],https://arxiv.org/abs/2004.13196,"Lens spaces are a family of manifolds that have been a source of many interesting phenomena in topology and differential geometry. Their concrete construction, as quotients of odd-dimensional spheres by a free linear action of a finite cyclic group, allows a deeper analysis of their structure. In this paper, we consider the problem of moments for the distance function between randomly selected pairs of points on homogeneous three-dimensional lens spaces. We give a derivation of a recursion relation for the moments, a formula for the $k$th moment, and a formula for the moment generating function, as well as an explicit formula for the volume of balls of all radii in these lens spaces. ","Distributions of Distances and Volumes of Balls in Homogeneous Lens
  Spaces"
39,1255597564247801867,280514581,Hector,"['There\'s a new improved revision of our paper on ""Merkle-CRDTs: Merkle-DAGs meet CRDTs"" on <LINK>\n\nThese concepts are live in multiple based on OrbitDB and go-ds-crdt!']",https://arxiv.org/abs/2004.00107,"We study Merkle-DAGs as a transport and persistence layer for Conflict-Free Replicated Data Types (CRDTs), coining the term Merkle-CRDTs and providing an overview of the different concepts, properties, advantages and limitations involved. We show how Merkle-DAGs can act as logical clocks giving Merkle-CRDTs the potential to greatly simplify the design and implementation of convergent data types in systems with weak messaging layer guarantees and a very large number of replicas. Merkle-CRDTs can leverage highly scalable distributed technologies like DHTs and PubSub algorithms running underneath to take advantage of the security and de-duplication properties of content-addressing. Examples of such content-oriented systems could include peer-to-peer content exchange and synchronisation applications between opportunistically connected mobile devices, IoT devices or user applications running in a web browser. ",Merkle-CRDTs: Merkle-DAGs meet CRDTs
40,1255507946391379969,2210861,Jacob Andreas,"['New paper led by @alanamarzoev: can we build effective models for language understanding using only data from a hand-written grammar, and *no* hand annotated training examples? What does ""sim-to-real transfer"" look like for NLP?\n\n<LINK>\n<LINK> <LINK>', 'Some observations: (1) pretrained LM representations are pretty good at modeling similarity between human-generated and synthetic sentences, but (2) models trained only on LM representations of synthetic sentences still overfit and generalize badly to real ones.', ""What can we do instead? Train a model that's only good at synthetic sentences, then reduce to *paraphrasing*: given a (real) sentence, find the most similar synth sentence using pretrained representations for similarity. (Some fun algorithmic problems here---details in paper.)"", ""This work owes a lot to @JonathanBerant &amp; @percyliang's work on paraphrasing for semantic parsing. Now the paraphrases aren't features---they're the whole model! This work resolves some past frustrations with work I've been involved in like https://t.co/TotGLztNfl..."", ""where it wasn't obvious the techniques were sample-efficient enough for human-sized datasets. If pretraining lets us generalize for free from synthetic utterances to real ones, maybe this doesn't matter!"", ""Next step---better search &amp; similarity: we've done just about the simplest thing possible, and there's definitely room to make the basic paraphrase procedure better. Also..."", ""better synthetic data! There's a big literature on grammar engineering (eg https://t.co/7rxVu7Nzpo) that's sort of fallen out of the NLP mainstream. If synth data becomes part of our pipeline, these tools can fill the role that renderers &amp; physics engines have in CV &amp; robotics.""]",https://arxiv.org/abs/2004.13645,"Large, human-annotated datasets are central to the development of natural language processing models. Collecting these datasets can be the most challenging part of the development process. We address this problem by introducing a general purpose technique for ``simulation-to-real'' transfer in language understanding problems with a delimited set of target behaviors, making it possible to develop models that can interpret natural utterances without natural training data. We begin with a synthetic data generation procedure, and train a model that can accurately interpret utterances produced by the data generator. To generalize to natural utterances, we automatically find projections of natural language utterances onto the support of the synthetic language, using learned sentence embeddings to define a distance metric. With only synthetic training data, our approach matches or outperforms state-of-the-art models trained on natural language data in several domains. These results suggest that simulation-to-real transfer is a practical framework for developing NLP applications, and that improved models for transfer might provide wide-ranging improvements in downstream tasks. ","Unnatural Language Processing: Bridging the Gap Between Synthetic and
  Natural Language Data"
41,1255471676923445250,306687043,Lorenzo Ruffoni,"['New paper available! Take a graph, a bunch of numbers, two awesome collaborators, and some cool math happens ✅😍 <LINK> @FSUPostdocs @FSUMath @FSUResearch @FSU_CRE #raag #grouptheory <LINK>']",https://arxiv.org/abs/2004.13206,"We study Artin kernels, i.e. kernels of discrete characters of right-angled Artin groups, and we show that they decompose as graphs of groups in a way that can be explicitly computed from the underlying graph. When the underlying graph is chordal we show that every such subgroup either surjects to an infinitely generated free group or is a generalized Baumslag-Solitar group of variable rank. In particular for block graphs (e.g. trees), we obtain an explicit rank formula, and discuss some features of the space of fibrations of the associated right-angled Artin group. ",Graphical splittings of Artin kernels
42,1255401831523594243,3269288695,Archit Sharma,"['We are moving unsupervised learning to real-world robotics! In our new work, we present off-DADS which enables sample-efficient skill discovery on real robots  without any rewards.\n\npaper: <LINK>\noverview: <LINK>\nwebsite: <LINK> <LINK>', 'We can repurpose the learned skills to solve downstream tasks, without any additional training!\n\nw/ @shaneguML, @hausman_k, @Vikashplus, @svlevine, M. Ahn https://t.co/pPHj1K0z9r', ""If you are attending #ICLR2020, check out our long talk for the original DADS submission at https://t.co/C6AWVtCxr0.\n\nWe'll be having the poster sessions on Thu @ 10 am PT and 1 pm PT. I will also be answering the questions throughout the conference.""]",https://arxiv.org/abs/2004.12974,"Reinforcement learning provides a general framework for learning robotic skills while minimizing engineering effort. However, most reinforcement learning algorithms assume that a well-designed reward function is provided, and learn a single behavior for that single reward function. Such reward functions can be difficult to design in practice. Can we instead develop efficient reinforcement learning methods that acquire diverse skills without any reward function, and then repurpose these skills for downstream tasks? In this paper, we demonstrate that a recently proposed unsupervised skill discovery algorithm can be extended into an efficient off-policy method, making it suitable for performing unsupervised reinforcement learning in the real world. Firstly, we show that our proposed algorithm provides substantial improvement in learning efficiency, making reward-free real-world training feasible. Secondly, we move beyond the simulation environments and evaluate the algorithm on real physical hardware. On quadrupeds, we observe that locomotion skills with diverse gaits and different orientations emerge without any rewards or demonstrations. We also demonstrate that the learned skills can be composed using model predictive control for goal-oriented navigation, without any additional training. ","Emergent Real-World Robotic Skills via Unsupervised Off-Policy
  Reinforcement Learning"
43,1255396953728352256,561899047,Aki Vehtari,"['While Federico Pavone @FritzPfau was visiting @CSAalto, he analyzed the properties of projpred <LINK>, and now with @JuhoPiironen, @paulbuerkner and I, we have a new paper ""Using reference models in variable selection"" <LINK> <LINK>', 'Previously @JuhoPiironen , Markus and I had demonstrated that projpred (projection predictive variable selection) can find small models with similar predictive performance as the full model (beating, e.g. lasso and glmnet), e.g. https://t.co/DezfrN1hhG and https://t.co/Iw8cY5CCF2 https://t.co/XdVdft2xnJ', 'We are not usually interested in something like FDR in variable selection, as most of time we don\'t assume zero effects, but, e.g., @f2harrel did ask and we now demonstrate the stability, FDR, etc. of various variable selection methods when there are also ""true zero coefficients""', 'We also wanted to see how much of the good performance of projpred comes from 1) Bayesian inference, 2) a reference model, and 3) projection after the selection. We demonstrate that other variable selection methods can also be improved using a reference model. https://t.co/UmICXrNSHb', ""projpred excels in minimal subset variable selection measured with predictive performance, FDR and selection stability. projpred wasn't designed for complete variable selection and the simple iterative approach we tested didn't beat methods specifically designed to control FDR. https://t.co/0z8rOLJuJO"", ""Excellent @FritzPfau is now a PhD student at @Unibocconi, Milan. The code is available at https://t.co/Opd3QqnOAj (we don't have Python implementation, yet). There will be soon a new projpred release with many new goodies."", ""@dan_p_simpson @FritzPfau @CSAalto @JuhoPiironen @paulbuerkner Yes, please! It's a coincidence that this week turned out to be an Aalto visitor paper theme week, but yes I very much like having visitors!"", '@vianey_lb @dan_p_simpson @FritzPfau @CSAalto @JuhoPiironen @paulbuerkner I hope you can visit us, too!']",https://arxiv.org/abs/2004.13118,"Variable selection, or more generally, model reduction is an important aspect of the statistical workflow aiming to provide insights from data. In this paper, we discuss and demonstrate the benefits of using a reference model in variable selection. A reference model acts as a noise-filter on the target variable by modeling its data generating mechanism. As a result, using the reference model predictions in the model selection procedure reduces the variability and improves stability leading to improved model selection performance. Assuming that a Bayesian reference model describes the true distribution of future data well, the theoretically preferred usage of the reference model is to project its predictive distribution to a reduced model leading to projection predictive variable selection approach. Alternatively, reference models may also be used in an ad-hoc manner in combination with common variable selection methods. In several numerical experiments, we investigate the performance of the projective prediction approach as well as alternative variable selection methods with and without reference models. Our results indicate that the use of reference models generally translates into better and more stable variable selection. Additionally, we demonstrate that the projection predictive approach shows superior performance as compared to alternative variable selection methods independently of whether or not they use reference models. ",Using reference models in variable selection
44,1255325685628968961,3279117936,Denis Yarats 🇺🇦,"['Exciting to announce our new work together with @ikostrikov and @rob_fergus: Image Augmentation Is All You Need:  Regularizing Deep Reinforcement Learning from Pixels. \n\nPaper: <LINK>\nCode: <LINK>\nWebsite: <LINK>\n\n[1/N]', 'We set new SOTA on DMControl without using any auxiliary supervision or training a world model:\n\n[2/N] https://t.co/d2TMa435LK', 'We identify that overfitting is a major problem in RL from pixels, as increased model capacity negatively correlates with performance. This makes sense, as an off-policy RL initially trains on a very small replay buffer and can easily overfit on it:\n\n[3/N] https://t.co/JXydJDhicF', 'We attempt to bring some common regularization techniques from CV, such as data augmentation. For example, adding plain image translation completely changes the ball game: \n\n[4/N] https://t.co/DSKtyK7VHO', 'We then tailor this data augmentation approach towards RL and propose a novel way to regularize Q function, this gives a rise to our algorithm that we dub DrQ (Data-regularize Q):\n\n[5/N] https://t.co/uGZKzcqfL4', 'Our methods DrQ outperforms other recent SOTA image-based methods on DMControl and sets the new SOTA on the PlaNet benchmark:\n\n[6/N] https://t.co/FuhmHL3HZ9', 'We also evaluate DrQ on an extended set of tasks -- the Dreamer benchmark. Again we outperform the prior work:\n\n[7/N] https://t.co/2V3NT9zr8k']",https://arxiv.org/abs/2004.13649,"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL ","Image Augmentation Is All You Need: Regularizing Deep Reinforcement
  Learning from Pixels"
45,1255302432252465152,90047221,Robin Kothari,"['New paper on the arXiv with Scott Aaronson, Shalev Ben-David, and Avishay Tal: ""Quantum Implications of Huang’s Sensitivity Theorem.""\n<LINK> <LINK>']",https://arxiv.org/abs/2004.13231,"Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function $f$, the deterministic query complexity, $D(f)$, is at most quartic in the quantum query complexity, $Q(f)$: $D(f) = O(Q(f)^4)$. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017). We also use the result to resolve the quantum analogue of the Aanderaa-Karp-Rosenberg conjecture. We show that if $f$ is a nontrivial monotone graph property of an $n$-vertex graph specified by its adjacency matrix, then $Q(f) = \Omega(n)$, which is also optimal. ",Quantum Implications of Huang's Sensitivity Theorem
46,1255153222068523008,855118392348610560,Joost Huizinga,"['New Go-Explore paper “First return then explore”, featuring: superhuman performance on all unsolved* and all hard-exploration Atari games, tackling of a hard-exploration robotics task, goal-conditioned policies to deal with stochasticity, and more! Paper: <LINK> <LINK>', '* Here we consider a game solved when an agent obtains super human performance when evaluated on an environment with sticky actions. Shoutout to Agent57, which recently managed to achieve superhuman performance on all Atari games, though without sticky actions.', 'The new paper also features even further improved scores on Montezuma’s Revenge and Pitfall and it demonstrates the potential of leveraging a policy for exploration. Paper in collaboration with @AdrienLE (shared first author), @joelbot3000, @kenneth0stanley, and @jeffclune']",http://arxiv.org/abs/2004.12919,"The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (""detachment"") and from failing to first return to a state before exploring from it (""derailment""). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents. ","First return, then explore"
47,1255151465808592902,855118392348610560,Joost Huizinga,"['New Go-Explore paper “First return then explore”, featuring: superhuman performance on all unsolved* and all hard-exploration Atari games, tackling of a hard-exploration robotics task, goal-conditioned policies to deal with stochasticity, and more! Paper: <LINK>', '* Here we consider a game solved when an agent obtains super human performance when evaluated on an environment with sticky actions. Shoutout to Agent57, which recently managed to achieve superhuman performance on all Atari games, though without sticky actions.', 'The new paper also features even further improved scores on Montezuma’s Revenge and Pitfall and it demonstrates the potential of leveraging a policy for exploration. Paper in collaboration with @AdrienLE (shared first author), @joelbot3000, @kenneth0stanley, and @jeffclune']",http://arxiv.org/abs/2004.12919,"The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (""detachment"") and from failing to first return to a state before exploring from it (""derailment""). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents. ","First return, then explore"
48,1255150640986087425,35724743,Adrien Ecoffet,"['Go-Explore now solves all unsolved* and hard-exploration Atari games, tackles a hard-exploration robotics task, trains in stochastic envs with a goal-conditioned policy and learns its own exploration policy! New paper, “First return then explore” at <LINK> <LINK>', '* Unsolved: previous state of the art performance below human performance when evaluated with sticky actions. Concurrently, Agent57 also solved all unsolved games, though without sticky actions.', 'Also: further improved scores on Montezuma’s Revenge and Pitfall! Co-authors: @Joost_Huizinga (co-first author), @joelbot3000, @kenneth0stanley, and @jeffclune', '@akhil_bagaria Good question! We use a downscaled representation like in the original work, but the difference here is that the parameters that control the downscaling are learned by Go-Explore instead of being predetermined. This is how it can work across games with the same hyperparameters.', ""@akhil_bagaria I'm not 100% sure if I understand your question correctly, but I think the answer is yes: all frames that correspond to the exact same downsampled version are part of the same cell, and a difference of even one pixel creates a different cell.""]",https://arxiv.org/abs/2004.12919,"The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (""detachment"") and from failing to first return to a state before exploring from it (""derailment""). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents. ","First return, then explore"
49,1255142015659999239,28840722,Phil Long,"['New paper with @niladrichat called ""Finite-sample analysis of interpolating linear classifiers in the overparameterized regime"": <LINK>.', '@LiangTengyuan @niladrichat Looks interesting.  Thanks!']",https://arxiv.org/abs/2004.12019,"We prove bounds on the population risk of the maximum margin algorithm for two-class linear classification. For linearly separable training data, the maximum margin algorithm has been shown in previous work to be equivalent to a limit of training with logistic loss using gradient descent, as the training error is driven to zero. We analyze this algorithm applied to random data including misclassification noise. Our assumptions on the clean data include the case in which the class-conditional distributions are standard normal distributions. The misclassification noise may be chosen by an adversary, subject to a limit on the fraction of corrupted labels. Our bounds show that, with sufficient over-parameterization, the maximum margin algorithm trained on noisy data can achieve nearly optimal population risk. ","Finite-sample Analysis of Interpolating Linear Classifiers in the
  Overparameterized Regime"
50,1255120633278709760,1242224281,Laura Parker,['New paper by PhD student Ian Roberts identifying a large sample of ram pressure stripping candidates in Coma and showing that they have enhanced star formation over other Coma galaxies and the field <LINK> @MacSciResearch <LINK>'],https://arxiv.org/abs/2004.12033,"The Coma cluster is the nearest massive ($M \gtrsim 10^{15}\,\mathrm{M_\odot}$) galaxy cluster, making it an excellent laboratory to probe the influence of the cluster environment on galaxy star formation. Here, we present a sample of 41 galaxies with disturbed morphologies consistent with ram pressure stripping. These galaxies are identified visually using high-quality, multi-band imaging from the Canada-France-Hawaii telescope covering ~9 $\mathrm{deg^2}$ of the Coma cluster. These ""stripping candidates"" are clear outliers in common quantitative morphological measures, such as concentration-asymmetry and Gini-$M_{20}$, confirming their disturbed nature. Based on the orientations of observed asymmetries, as well as the galaxy positions in projected phase-space, these candidates are consistent with galaxies being stripped shortly after infall onto the Coma cluster. Finally, the stripping candidates show enhanced star formation rates, both relative to ""normal"" star-forming Coma galaxies and isolated galaxies in the field. Ram pressure is likely driving an enhancement in star formation during the stripping phase, prior to quenching. On the whole, ram pressure stripping appears to be ubiquitous across all regions of the Coma cluster. ","Ram pressure stripping candidates in the Coma Cluster: Evidence for
  enhanced star formation"
51,1255106867501637634,54638897,Urs Gasser,"['Mapping ethical, legal &amp; societal challenges when using digital tech vs. COVID-19 and working towards a navigation aid: pls share feedback on our new working paper w/ @MarcelloIenca @VisualizeEthics J. Scheibner &amp; @EffyVayena <LINK> cc: @BKCHarvard @Harvard_Law <LINK>', '@hackylawyER @MarcelloIenca @VisualizeEthics @EffyVayena @BKCHarvard @Harvard_Law It’s an established public health ethics framework we used - please check out the sources in the paper. https://t.co/roFbUP2HYi']",https://arxiv.org/abs/2004.10236,"Data collection and processing via digital public health technologies are being promoted worldwide by governments and private companies as strategic remedies for mitigating the COVID-19 pandemic and loosening lockdown measures. However, the ethical and legal boundaries of deploying digital tools for disease surveillance and control purposes are unclear, and a rapidly evolving debate has emerged globally around the promises and risks of mobilizing digital tools for public health. To help scientists and policymakers navigate technological and ethical uncertainty, we present a typology of the primary digital public health applications currently in use. Namely: proximity and contact tracing, symptom monitoring, quarantine control, and flow modeling. For each, we discuss context-specific risks, cross-sectional issues, and ethical concerns. Finally, in recognition of the need for practical guidance, we propose a navigation aid for policymakers made up of ten steps for the ethical use of digital public health tools. ","Digital tools against COVID-19: Framing the ethical challenges and how
  to address them"
52,1255105744963239937,62604839,Paulo Simões,"['We have a new paper accepted for publication in ApJ! Spectral signatures of chromospheric condensation in a major solar flare, led by @SolarDaveG. Another paper derived from the @fchroma project <LINK>']",https://arxiv.org/abs/2004.05075,"We study the evolution of chromospheric line and continuum emission during the impulsive phase of the X-class SOL2014-09-10T17:45 solar flare. We extend previous analyses of this flare to multiple chromospheric lines of Fe I, Fe II, Mg II, C I, and Si II, observed with IRIS, combined with radiative-hydrodynamical (RHD) modeling. For multiple flaring kernels, the lines all show a rapidly evolving double-component structure: an enhanced, emission component at rest, and a broad, highly red-shifted component of comparable intensity. The red-shifted components migrate from 25-50 km s$^{-1}$ towards the rest wavelength within $\sim$30 seconds. Using Fermi hard X-ray observations, we derive the parameters of an accelerated electron beam impacting the dense chromosphere, using them to drive a RHD simulation with the RADYN code. As in Kowalski et al. 2017a, our simulations show that the most energetic electrons penetrate into the deep chromosphere, heating it to T$\sim$10,000 K, while the bulk of the electrons dissipate their energy higher, driving an explosive evaporation, and its counterpart condensation -- a very dense (n$_e \sim 2 \times 10^{14}$ cm$^{-3}$), thin layer (30--40 km thickness), heated to 8--12,000 K, moving towards the stationary chromosphere at up to 50 km s$^{-1}$. The synthetic Fe II 2814.45A profiles closely resemble the observational data, including a continuum enhancement, and both a stationary and a highly red-shifted component, rapidly moving towards the rest wavelength. Importantly, the absolute continuum intensity, ratio of component intensities, relative time of appearance, and red-shift amplitude, are sensitive to the model input parameters, showing great potential as diagnostics. ",Spectral signatures of chromospheric condensation in a major solar flare
53,1255095770627428352,3308167500,"David C. Norris, MD 🌻","['NEW WORKING PAPER @arxiv …\n\nA retrospective, model-based analysis of a fatal dose-finding trial. [THREAD] 1/\n<LINK> <LINK>', '@arxiv The trial in question came to my attention 1.5 years ago, thanks (as is so often the case) to @Lymphomation: 2/\nhttps://t.co/DLVOsKD5rk', '@arxiv @Lymphomation As you see in the thread linked above, my immediate response was in terms of the #PrecautionaryCoherence principle, explained in the quick video below: 3/\nhttps://t.co/W9tMGnJYqx', '@arxiv @Lymphomation This latest paper, however, advances past the generalities and abstraction of the PC principle, into the more concrete realm of a model-based analysis grounded in mechanistic realism and using actual trial data. 4/', '@arxiv @Lymphomation But let me stress I’m not abandoning the PC principle, but rather pursuing its natural development: Technically, the key to a useful model was using #latentvariables—an idea at the core of this Letter published last month on #PrecautionaryCoherence 5/\nhttps://t.co/zps1G5USoa', '@Lymphomation @arxiv Thanks, Karl! I’ve posted (link below) @FDAOncology’s formal response to a query through my Congresswoman’s office. I’m still trying to read these tea-leaves accurately. Would be interested in your own reflections on it!\nhttps://t.co/cF2uxy1ra5', 'But before laying out the model, we need background story. 6a/\n\n8 Oct 2018\nhttps://t.co/GHZQ3skY4h\n$AFMD puts 2 #AFM11 trials on clinical hold for SAEs in 3 patients:\n• 2 life-threatening AEs in https://t.co/PGwSrK7ABx (R/R NHL)\n• 1 death in https://t.co/Wg9LCxoQdB (R/R B-ALL)', '12 Oct 2018: @US_FDA places a full clinical hold on #AFM11 program. 6b/\nhttps://t.co/8Vle9ePIKu', '@US_FDA 17 Apr 2019: $AFMD Regulatory Update indicates ongoing discussions with FDA over the clinical hold. 6c/\nhttps://t.co/C0wQOXLREf', '@US_FDA 22 May 2019: $AFMD terminates #AFM11 Phase 1 program. 6d/\nhttps://t.co/geP3YgXPuN', '@US_FDA So here we have a phase 1 oncology trial that vividly contravenes the commonplace description of such trials as “primarily concerned with safety.”\n\nA trial such as this demands the closest and most concrete analysis we can bring to it. 7/\n[my abstract below] https://t.co/5UHkdBxlMh', '@US_FDA For my analysis, I’ve abstracted (as best I can) doses and toxicities from the B-ALL trial, which was reported at #ASH2018. (AFAIK, the NHL trial has not been reported at a scientific meeting or in the literature.) 8/\nhttps://t.co/hWBBDXbUmA', 'Here’s Table 1 from my paper, where I abstract, for each of the N=17 participants:\n• a low dose causing no tox\n• highest dose received\n• CTCAE Grade at high dose.\n\nOf note, the step-up dosing used in this trial was crucial for yielding these dose pairs. 9/ https://t.co/EDLRuVvnqn', 'My primary data come from the SAFETY section of the #ASH2018 poster by Salogub et al. 10/\nhttps://t.co/Z5FT3WAjsP\n[images below] https://t.co/NkyWV0pvyV', 'I’ve tried to be quite transparent about my uncertainties in this abstraction. (See esp. the note about Patient 16 highlighted below.) 11/ https://t.co/6mHKrTQs0x', 'THE MODEL\n\nSo now I need a dose-response model for toxicity that incorporates BOTH:\n(a) ordinal toxicities AND\n(b) inter-individual heterogeneity.\n\nSeveral extant models already do (a), and I’ll discuss their connections with my own model below.\n\nBut (b) is a new requirement. 12/', 'My approach is to take the MTDᵢ notion at the heart of my opening #DTAT argument 3+ years ago, and extend it to ordinal toxicities.\n\nThe key to this notationally is to posit that a DLT occurs at the boundary between CTCAE grades 2–3, and to write: 13/\n\n  MTDᵢ ≡ MTDᵢᵍ, g=3', 'Thus, we have {MTDᵢᵍ | g=1,…,5}:\n\nMTDᵢ¹ = max dose patient i can tolerate before getting a Grade 1 toxicity\n\nMTDᵢ² = max dose patient i can tolerate before getting a Grade 2 toxicity\n…\n\nMTDᵢ⁵ = max dose patient i can tolerate before getting a Grade 5 (fatal) toxicity. 14/', 'Whereas in previous work I have supposed a Gamma distribution for MTDᵢ (remember, this is the same as MTDᵢ³), here a lognormal distribution proves more convenient:   15/\n\n   log MTDᵢ ~ 𝒩(μ,τ), τ ≡ 1/σ².', 'From a #trialsafety perspective, a model of ordinal toxic dose-response will be useless unless it supports extrapolation from the low-grade toxicities we may be willing to encounter during careful #titration, to the higher-grade toxicities we hope to avoid.  16/ https://t.co/TEZevk5ue1', 'So (as you see above), I’ve posited a link between the {MTDᵢᵍ | g=1,…,5}. The manner in which I do this:\n\n    MTDᵢᵍ = rᵢ⁽ᵍ⁻³⁾·MTDᵢᵍ    (Eq. 3)\n\neffectively assumes that the grading system is superbly aligned with the underlying dose-response.  17/', 'That is, each individual i’s toxic response is characterized by a ratio rᵢ that links—in a geometric sequence—the maximum tolerable doses within each toxicity grade. (Equivalently, the logarithms of these doses are an arithmetic sequence with constant difference log rᵢ.)  18/', 'Now you know I wouldn’t be caught dead NOT subscripting rᵢ to indicate its likely inter-individual heterogeneity. But in this analysis, with so few participants—and even fewer toxicities—I’ve required a further identifying restriction:  19/\n\nlog rᵢ ~ 𝒩(log r₀, τᵣ) ,  τᵣ≫1.', 'Before explaining the priors I’ve placed on the parameters in this Bayesian model, it’s worth skipping ahead a bit in the paper, to its discussion of some formal connections with an ordinal toxicity model of Bekele &amp; Thall (2004)   20/\nhttps://t.co/7xsDibOGqr https://t.co/8tbSs2uGfi', 'The first thing to note about the Bekele &amp; Thall model is that it incorporates *multiple* toxicities, in contrast to the neurotoxicity of chief concern in the AFM11 trials. Thus, their model has an extra j index. (Note too that they adopt a logarithmic dose scaling as I did.) 21/ https://t.co/hDbw45dv0d', 'The key point, however, is that they introduce a normally distributed #latentvariable Z that serves exactly the same role *formally* as my log MTDᵢ:  22/ https://t.co/DfTQ2d2Q8Y', 'The difference is, however, that this Bekele &amp; Thall treat Z as a mere technical convenience. Thus, they overlook the opportunity to lend it a realistic, pharmacologic interpretation such as I give to MTDᵢ.  23/ https://t.co/BxI1fHIThb', 'This easy dismissal of #latentvariables pervades the statistical #dosefinding literature. I tend to see it as the symptom of a fecklessness pervading medical statistics as a whole.  24/\n\n[WARNING DO NOT READ THREAD:]\nhttps://t.co/ow1eS4sMUL', 'More to the present point, this results in  Bekele &amp; Thall’s failure to acknowledge PKPD heterogeneity in their model. Their model remains, for all the weight of its matrix notation, a 1-size-fits-all #dosefinding model.  25/', 'One notable methodologic implication of this is that they must conduct their prior elicitation with physicians in terms of statisticians’ dose-cohort abstractions. \n\nHow much easier would this elicitation be, conducted in terms of #titration in the individual patient!  26/ https://t.co/Du9nQIo50s', 'THE PRIORS\n\nBut now back to my own priors, inferred from the actual design of the trial. To begin, the prior on μ:\n\n   μ ~ 𝒰[2.9, 7.5]\n\naimed to center\n\n   log MTDᵢ ~ 𝒩(μ,τ)\n\nat a median of log(180), the Cohort 5 target dose [ng/kg/week] ±1 order of magnitude either side.  27/', 'In earlier work on the #economics of #dosefinding, I focused attention on the coefficient of variation, CV(MTDᵢ).  28/\nhttps://t.co/lN9f5qLjN9', 'So I have chosen to place my prior on this CV instead of the much less intuitive τ:\n\n  CV ~ 𝒩(0.5, σ=1/6).\n\nI argue that adopting a dose-escalation design requires holding an expectation that CV is modest. The σ=1/6 puts CV&gt;1 in the normal distribution’s 3σ upper tail.  29/', 'Finally (for the priors) I place a vague uniform prior on r₀, centered on 3 (which is the ratio between the trial’s dose levels, as well as the step-up dosing ratio) and extending ±2:\n\n   r₀ ~ 𝒰(1, 5).\n\nBoth endpoints here are safely outside the bounds of reason.  30/', 'This all comes together in the following JAGS model:   31/ https://t.co/sb4NSgvEMC', 'As someone who delights in the clarity of thought one can achieve through declarative programming, I absolutely adore JAGS. Thank you @martyn_plummer for this magnificent tool!   32/\n\nhttps://t.co/EWDqplZNGe', '@martyn_plummer The model runs fast, yielding effective samples sizes in the 1000’s in just seconds.  33/ https://t.co/gqxOO1wCgr', '@martyn_plummer Figs. 1 and 2 show the posterior densities of MTDᵢ for the patients who experienced toxicities and for those who didn’t.  34/ https://t.co/ocpqk8pbKB', 'The contrast between those Figures [note also the change of scale] clearly shows that we learn most from the patients who experience toxicities.\n\nJudicious #titration yields better dose-finding trials by every measure—including #informativeness!  35/\nhttps://t.co/WqXMp19NQg', 'How about them #hyperparameters?\n\nWhile μ≈5 puts median MTDᵢ around e⁵≈150 ng/kg weekly, close to the Cohort 5 dose of 180, finding CV≈1 is a ‘surprise’, and the r₀≈1.3 deserves at least some comment.   36/ https://t.co/JVhPxXEqLe', 'Recall that we had placed a low prior probability on CV≈1. Otherwise, this dose-escalation trial would have looked neither ethical nor economical ex ante. 37/\nhttps://t.co/v1XpNoYuk4', 'Of course, our ex post ‘discovery’ of the substantial heterogeneity represented by CV≈1 is the formal correlate of the dismay that would have attended the severe toxicities in the AFM11 trials.  38/', 'The finding r₀≈1.3 isn’t at all surprising to pharmacologic intuition, since it means that a 30% increase in dose bumps up the CTCAE Grade by 1 level.\n\nBut it DOES conflict in retrospect with the 3⨉ multiplier between dose levels (and the 3⨉ step-up dosing) in this trial. 39/', 'Even under circumstances of low inter-individual heterogeneity (CV≪1), using dose increments within or between individuals that greatly exceed r₀ would be manifestly unsafe. (I would like to think even a #OneSizeFitsAllogist would know better.)  40/ https://t.co/RZx3MjY4Ik', 'The final bit of the analysis exploits a lovely feature of JAGS (declarative programming FTW!) to ask what might have been known before the escalation to Cohort 6, and whether this might have averted the fatal toxicity of Patient 17.  41/ https://t.co/IeUOibSPFb', 'Here, conditional on what was known at the end of Cohort 5, are the (forward-looking HT @f2harrell) #probabilities of each grade of toxicity, during the 1st (step-up) week of dosing, and upon graduation (after no toxicity) to the target dose.  42/ https://t.co/PmYDp4SkaF', 'Would YOU have enrolled Cohort 6, facing these probabilities?   43/', 'IN CONCLUSION\n\nBeneath the retrospective surface of this discussion is a prospective opportunity:\n\nSince this model may be used in this same way at EVERY dosing decision, it actually forms a basis for designing and implementing dose-#titration trials with ordinal outcomes! /44 https://t.co/m29de94YbS', 'I may extend this thread sometime soon with #PhilSci aspects of the paper. There is e.g. an interesting 2012 paper https://t.co/P8wFMG235d by @EmilyVDressler @LGM_Biostats &amp; @bandipu that, though not formally aligned with my model, aims similarly at pharmacologic #realism.  45/ https://t.co/WDlg0eH6Ss', '@EmilyVDressler @LGM_Biostats @bandipu The code is of course available as always @OSFramework under the permissive MIT License:  46/\nhttps://t.co/6ZJgHO49jb', '@EmilyVDressler @LGM_Biostats @bandipu @OSFramework In closing, I’d like to thank especially @dhovekamp42 for engaging me over several extended and extensive convos here, helping orient me to available data sources. (All errors are mine alone, however!)  47/', '@EmilyVDressler @LGM_Biostats @bandipu @OSFramework @dhovekamp42 And finally, this is a WORKING PAPER!\n\nI will be most glad for your engagement here or @PubPeer, and will be eager to update/qualify/rectify my analysis as needed.  48/48\n\nTHANKS FOR READING!']",https://arxiv.org/abs/2004.12755,"The commonplace description of phase 1 clinical trials in oncology as ""primarily concerned with safety"" is belied by their near universal adoption of dose-escalation practices which are inherently unsafe. In contrast with dose titration, cohort-wise dose escalation regards patients as exchangeable, an indefensible assumption in the face of widely appreciated inter-individual heterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have previously advanced this argument in terms of a precautionary coherence principle that brings the well-known coherence notion of Cheung (2005) into contact with modern imperatives of patient-centeredness and precision dosing. Here, however, I explore these matters in some mechanistic detail by analyzing a trial of the bispecific T cell engager AFM11, in which a fatal toxicity occurred. To this end, I develop a Bayesian dose-response model for a single ordinal toxicity. By constructing this model's priors to align with the AFM11 trial as designed and conducted, I demonstrate the incompatibility of that design with any reasonable expectation of safety. Indeed, the model readily yields prospective estimates of toxic response probabilities that suggest the fatality in this trial could have been foreseen as likely. ",Retrospective analysis of a fatal dose-finding trial
54,1254999932186103809,268919557,Joshua T. Vogelstein (jovo/he/we),"['incidentally, i noticed today another paper of mine made it to arXiv, ""A New Age of Computing and the Brain""\n<LINK>\nwhich is work from a few years ago, but highly relevant today, eg, in light of our approach to #lifelonglearning: \n<LINK>']",https://arxiv.org/abs/2004.12926,"The history of computer science and brain sciences are intertwined. In his unfinished manuscript ""The Computer and the Brain,"" von Neumann debates whether or not the brain can be thought of as a computing machine and identifies some of the similarities and differences between natural and artificial computation. Turing, in his 1950 article in Mind, argues that computing devices could ultimately emulate intelligence, leading to his proposed Turing test. Herbert Simon predicted in 1957 that most psychological theories would take the form of a computer program. In 1976, David Marr proposed that the function of the visual system could be abstracted and studied at computational and algorithmic levels that did not depend on the underlying physical substrate. In December 2014, a two-day workshop supported by the Computing Community Consortium (CCC) and the National Science Foundation's Computer and Information Science and Engineering Directorate (NSF CISE) was convened in Washington, DC, with the goal of bringing together computer scientists and brain researchers to explore these new opportunities and connections, and develop a new, modern dialogue between the two research communities. Specifically, our objectives were: 1. To articulate a conceptual framework for research at the interface of brain sciences and computing and to identify key problems in this interface, presented in a way that will attract both CISE and brain researchers into this space. 2. To inform and excite researchers within the CISE research community about brain research opportunities and to identify and explain strategic roles they can play in advancing this initiative. 3. To develop new connections, conversations and collaborations between brain sciences and CISE researchers that will lead to highly relevant and competitive proposals, high-impact research, and influential publications. ",A New Age of Computing and the Brain
55,1254933105284014080,315718949,Clément Canonne,"['New (short) paper w/ Karl Wimmer on ""Testing Data Binnings"": in short, identity testing *when your domain is wrong*.\n<LINK>\nMaybe your measurements were too accurate. Maybe your quantization scheme is funky. Is there *some* binning which the data fits the model? <LINK>', 'So you have a model on domain {1,..,k} (reference distrib. q), and data coming from domain {1,...,n} (unknown distrib. p), with k ≪ n. Is p actually ""q in disguise""?\n\nAnyways, I think it\'s a cute question, and we have future directions, if you\'re interested. Comments welcome! https://t.co/FcIRqB8bUg', 'Yes, I did choose the colors. /end', '*for which the data [sorry, typo]']",https://arxiv.org/abs/2004.12893,"Motivated by the question of data quantization and ""binning,"" we revisit the problem of identity testing of discrete probability distributions. Identity testing (a.k.a. one-sample testing), a fundamental and by now well-understood problem in distribution testing, asks, given a reference distribution (model) $\mathbf{q}$ and samples from an unknown distribution $\mathbf{p}$, both over $[n]=\{1,2,\dots,n\}$, whether $\mathbf{p}$ equals $\mathbf{q}$, or is significantly different from it. In this paper, we introduce the related question of 'identity up to binning,' where the reference distribution $\mathbf{q}$ is over $k \ll n$ elements: the question is then whether there exists a suitable binning of the domain $[n]$ into $k$ intervals such that, once ""binned,"" $\mathbf{p}$ is equal to $\mathbf{q}$. We provide nearly tight upper and lower bounds on the sample complexity of this new question, showing both a quantitative and qualitative difference with the vanilla identity testing one, and answering an open question of Canonne (2019). Finally, we discuss several extensions and related research directions. ",Testing Data Binnings
56,1254853966942240768,384104802,Matthew Petroff,"['I have a new paper out, ""Full-sky Cosmic Microwave Background Foreground Cleaning Using Machine Learning""! It uses a neural network trained on simulations to produce a foreground-cleaned CMB temperature map from separate frequency maps. <LINK> 1/4 <LINK>', 'Crucially, it also produces an error estimate. This is important for scientific applications but is less common in many areas of machine learning. 2/4 https://t.co/gc2yDQwOUj', 'It can also be used to help improve foreground simulations by comparing the residual on different simulations with the residual on observations when compared to an external foreground cleaning method as well as by comparing the error estimate. 3/4', 'It also works up to around \\ell = 900. Thanks to my co-authors, @AddisonGraeme, Chuck, and Janet for their insights on foregrounds and simulations and for helping to guide this research and turn it into a polished manuscript. 4/4 https://t.co/9hbuTyBd4k']",https://arxiv.org/abs/2004.11507,"In order to extract cosmological information from observations of the millimeter and submillimeter sky, foreground components must first be removed to produce an estimate of the cosmic microwave background (CMB). We developed a machine-learning approach for doing so for full-sky temperature maps of the millimeter and submillimeter sky. We constructed a Bayesian spherical convolutional neural network architecture to produce a model that captures both spectral and morphological aspects of the foregrounds. Additionally, the model outputs a per-pixel error estimate that incorporates both statistical and model uncertainties. The model was then trained using simulations that incorporated knowledge of these foreground components that was available at the time of the launch of the Planck satellite. On simulated maps, the CMB is recovered with a mean absolute difference of $<4\mu$K over the full sky after masking map pixels with a predicted standard error of $>50\mu$K; the angular power spectrum is also accurately recovered. Once validated with the simulations, this model was applied to Planck temperature observations from its 70GHz through 857GHz channels to produce a foreground-cleaned CMB map at a Healpix map resolution of NSIDE=512. Furthermore, we demonstrate the utility of the technique for evaluating how well different simulations match observations, particularly in regard to the modeling of thermal dust. ","Full-sky Cosmic Microwave Background Foreground Cleaning Using Machine
  Learning"
57,1254753666654969860,861775999,Laura A. Hayes,['Check out my new paper on the statistical study of quasi-periodic pulsations (QPPs) in solar flares from the past solar cycle observed with GOES on @arxiv accepted to ApJ <LINK> with @petertgallagher @ehsteve 🌞'],https://arxiv.org/abs/2004.11775,"Small amplitude quasi-periodic pulsations (QPPs) detected in soft X-ray emission are commonplace in many flares. To date, the underpinning processes resulting in the QPPs are unknown. In this paper, we attempt to constrain the prevalence of \textit{stationary} QPPs in the largest statistical study to date, including a study of the relationship of QPP periods to the properties of the flaring active region, flare ribbons, and CME affiliation. We build upon the work of \cite{inglis2016} and use a model comparison test to search for significant power in the Fourier spectra of lightcurves of the GOES 1--8~\AA\ channel. We analyze all X-, M- and C- class flares of the past solar cycle, a total of 5519 flares, and search for periodicity in the 6-300~s timescale range. Approximately 46\% of X-class, 29\% of M-class and 7\% of C-class flares show evidence of stationary QPPs, with periods that follow a log-normal distribution peaked at 20~s. The QPP periods were found to be independent of flare magnitude, however a positive correlation was found between QPP period and flare duration. No dependence of the QPP periods to the global active region properties was identified. A positive correlation was found between QPPs and ribbon properties including unsigned magnetic flux, ribbon area and ribbon separation distance. We found that both flares with and without an associated CME can host QPPs. Furthermore, we demonstrate that for X- and M- class flares, decay phase QPPs have statistically longer periods than impulsive phase QPPs. ","Statistical Study of GOES X-ray Quasi-Periodic Pulsations in Solar
  Flares"
58,1254711104653135872,63441844,David Roberson,"['A few days late, but I put a new paper on the arxiv last week: <LINK>\n\nWe show how to formulate (quantum) graph isomorphism as a conic program and then investigate the same program but relaxed to the PSD and doubly nonnegative cones.', 'The relaxations turn out to have nice algebraic characterizations. In particular, for the DNN cone the relation you get is equivalent to the graphs having isomorphic coherent algebras/configurations, such that this isomorphism maps the adjacency mtx of one to the other.', 'This is also equivalent to the graphs not being distinguished by the Weisfeiler-Leman algorithm - a well known graph isomorphism heuristic based on partitioning the ordered pairs of vertices of the graphs. This is known to have many equivalent characterizations.', ""Our results show that it is further equivalent to feasibility of the 1st level of the Lasserre hierarchy for graph isomorphism, and to Schrijver's theta function achieving a certain value on a product of the two graphs.""]",https://arxiv.org/abs/2004.10893,"In the $(G,H)$-isomorphism game, a verifier interacts with two non-communicating players (called provers) by privately sending each of them a random vertex from either $G$ or $H$, whose aim is to convince the verifier that two graphs $G$ and $H$ are isomorphic. In recent work along with Atserias, \v{S}\'amal and Severini [Journal of Combinatorial Theory, Series B, 136:89--328, 2019] we showed that a verifier can be convinced that two non-isomorphic graphs are isomorphic, if the provers are allowed to share quantum resources. In this paper we model classical and quantum graph isomorphism by linear constraints over certain complicated convex cones, which we then relax to a pair of tractable convex models (semidefinite programs). Our main result is a complete algebraic characterization of the corresponding equivalence relations on graphs in terms of appropriate matrix algebras. Our techniques are an interesting mix of algebra, combinatorics, optimization, and quantum information. ","Graph isomorphism: Physical resources, optimization models, and
  algebraic characterizations"
59,1253744057639206912,1253740214012559365,Piotr Teterwak,"[""I guess it's time I make an academic twitter!\n\nNew paper out: <LINK>\n\nWe improve on the cross-entropy loss with a supervised version of the batch-contrastive loss.\n\nThank you to  @PrannayKhosla @YonglongT\n@phillip_isola @dilipkay @aaronysarna and others!""]",https://arxiv.org/abs/2004.11362,"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at this https URL ",Supervised Contrastive Learning
60,1253726646571544576,60893773,James Bullock,"['New paper! @UCIPhysAstro PhD student @DarthLazar et al. introduces a new density profile, core-Einasto, that characterizes the feedback affected dark matter halos in FIRE-2 simulations w/ @MBKplus @PFHopkins_Astro @kjb_astro @AndrewWetzel @coralrosew\n\n<LINK> <LINK>']",https://arxiv.org/abs/2004.10817,"We analyze the cold dark matter density profiles of 54 galaxy halos simulated with FIRE-2 galaxy formation physics, each resolved within $0.5\%$ of the halo virial radius. These halos contain galaxies with masses that range from ultra-faint dwarfs ($M_\star \simeq 10^{4.5} M_{\odot}$) to the largest spirals ($M_\star \simeq 10^{11} M_{\odot}$) and have density profiles that are both cored and cuspy. We characterize our results using a new analytic density profile that extends the standard Einasto form to allow for a pronounced constant-density core in the resolved innermost radius. With one additional core-radius parameter, $r_{c}$, this ""core-Einasto"" profile is able to characterize the shape and normalization of our feedback-impacted dark matter halos. In order to enable comparisons with observations, we provide fitting functions for $r_{c}$ and other profile parameters as a function of both $M_\star$ and $M_{\star}/M_{\rm halo}$. In agreement with similar studies done in the literature, we find that dark matter core formation is most efficient at the characteristic stellar-mass to halo-mass ratio $M_\star/M_{\rm halo} \simeq 5 \times 10^{-3}$, or $M_{\star} \sim 10^9 \, M_{\odot}$, with cores that are roughly the size of the galaxy half-light radius, $r_{c} \simeq 1-5$ kpc. Furthermore, we find no evidence for core formation at radii $\gtrsim 100\ \rm pc$ in galaxies with $M_{\star}/M_{\rm halo} < 5\times 10^{-4}$ or $M_\star \lesssim 10^6 \, M_{\odot}$. For Milky Way-size galaxies, baryonic contraction often makes halos significantly more concentrated and dense at the stellar half-light radius than dark matter only runs. However, even at the Milky Way scale, FIRE-2 galaxy formation still produces small dark matter cores of $\simeq 0.5-2$ kpc in size. Recent evidence for a ${\sim} 2$ kpc core in the Milky Way's dark matter halo is consistent with this expectation. ","A dark matter profile to model diverse feedback-induced core sizes of
  $\Lambda$CDM haloes"
61,1253718089490825216,1109974447954382848,「Alexandres Lazar」,"['Morning astro peeps. Our new paper (w/ @jbprime, @MBKplus, and collaborators) models feedback-affected dark matter profiles for FIRE-2 galaxies. Pretty stellar stuff: <LINK> <LINK>', 'Incredible thanks to our collaborators: @chantsangkeung @PFHopkins_Astro @AndrewWetzel @kjb_astro @coralrosew @Alex_Fitts @SheaGKosmo Andrew S. Graus, Maria C. Straight, Dušan Kereš, and Claude-André Faucher-Giguère']",https://arxiv.org/abs/2004.10817,"We analyze the cold dark matter density profiles of 54 galaxy halos simulated with FIRE-2 galaxy formation physics, each resolved within $0.5\%$ of the halo virial radius. These halos contain galaxies with masses that range from ultra-faint dwarfs ($M_\star \simeq 10^{4.5} M_{\odot}$) to the largest spirals ($M_\star \simeq 10^{11} M_{\odot}$) and have density profiles that are both cored and cuspy. We characterize our results using a new analytic density profile that extends the standard Einasto form to allow for a pronounced constant-density core in the resolved innermost radius. With one additional core-radius parameter, $r_{c}$, this ""core-Einasto"" profile is able to characterize the shape and normalization of our feedback-impacted dark matter halos. In order to enable comparisons with observations, we provide fitting functions for $r_{c}$ and other profile parameters as a function of both $M_\star$ and $M_{\star}/M_{\rm halo}$. In agreement with similar studies done in the literature, we find that dark matter core formation is most efficient at the characteristic stellar-mass to halo-mass ratio $M_\star/M_{\rm halo} \simeq 5 \times 10^{-3}$, or $M_{\star} \sim 10^9 \, M_{\odot}$, with cores that are roughly the size of the galaxy half-light radius, $r_{c} \simeq 1-5$ kpc. Furthermore, we find no evidence for core formation at radii $\gtrsim 100\ \rm pc$ in galaxies with $M_{\star}/M_{\rm halo} < 5\times 10^{-4}$ or $M_\star \lesssim 10^6 \, M_{\odot}$. For Milky Way-size galaxies, baryonic contraction often makes halos significantly more concentrated and dense at the stellar half-light radius than dark matter only runs. However, even at the Milky Way scale, FIRE-2 galaxy formation still produces small dark matter cores of $\simeq 0.5-2$ kpc in size. Recent evidence for a ${\sim} 2$ kpc core in the Milky Way's dark matter halo is consistent with this expectation. ","A dark matter profile to model diverse feedback-induced core sizes of
  $\Lambda$CDM haloes"
62,1253714073125916677,171674815,Mark Marley,"['Proud to be co-author on the great new paper by @_BEMILES \n<LINK>', 'Highlight is an M band spectral sequence of cold brown dwarfs. Tons of Gemini time on these faint objects. https://t.co/0WV5fXJ8ul', 'This bandpass has long been a very productive fishing hole for studies of Jupiter since it has low gas opacity, allowing flux from the deeper atmosphere to escape. The long path length permits detection of low abundance species, like CO, GeH4, PH3', ""Brittany was able to measure CO abundance for the sample but we don't see obvious PH3, unlike the case for Jupiter. https://t.co/F8zfBHx3Dt"", 'Papers sometimes find disequilibrium chemistry and stop there. Here we used the observed abundance of CO to infer atmospheric mixing, parameterized as Kzz. Interestingly we find lower Kzz for the brown dwarfs than for Jupiter.', 'I think there is a lot more opportunity across exoplanets and brown dwarfs to drill more deeply into the atmosphere using these kinds of techniques. This means we can move past ""we see disequilibrium chemistry"" to ""here is what we learn, in detail, about the atmosphere."" https://t.co/3vZ4XszM0e', 'Also highlighting the co-authors including @AstroCaroline @jjfplanet @KatelynAllers @jfaherty and Gordon Bjoraker who is probably world expert in spectroscopy of Jupiter at 5 microns.', 'There has been a lot of talk lately about how there should be more exoplanet/solar system synergy. This paper is a living and breathing example of such synergy, today applied to brown dwarfs. (p/c @LeighFletcher) https://t.co/qoPhNrOuSU', 'These same methods can be applied to self-luminous extrasolar giant planets and will be a great science topic for coronagraphs on 30-m class telescopes.', ""Also, important: a LaTex typo caused co-author @ChannonVisscher's name to be left out of the arXiv author list. It's getting fixed. Correct here: https://t.co/6BHiZk27i0"", '@jjfplanet @_BEMILES We talk about the detached zone in the paper in fact. Really shows we need to start thinking about Kzz profiles instead of constant value.']",https://arxiv.org/abs/2004.10770,"Cold brown dwarfs are excellent analogs of widely separated, gas giant exoplanets, and provide insight into the potential atmospheric chemistry and physics we may encounter in objects discovered by future direct imaging surveys. We present a low resolution R $\sim$ 300 $M$-band spectroscopic sequence of seven brown dwarfs with effective temperatures between 750 K and 250 K along with Jupiter. These spectra reveal disequilibrium abundances of carbon monoxide (CO) produced by atmospheric quenching. We use the eddy diffusion coefficient (K$_{zz}$) to estimate the strength of vertical mixing in each object. The K$_{zz}$ values of cooler gaseous objects are close to their theoretical maximum and warmer objects show weaker mixing, likely due to less efficient convective mixing in primarily radiative layers. The CO-derived K$_{zz}$ values imply that disequilibrium phosphine (PH$_{3}$) should be easily observable in all of the brown dwarfs, but none as yet show any evidence for PH$_{3}$ absorption. We find that ammonia is relatively insensitive to atmospheric quenching at these effective temperatures. We are able to improve the fit to WISE 0855's $M$-band spectrum by including both CO and water clouds in the atmospheric model. ",Observations of Disequilibrium CO Chemistry in the Coldest Brown Dwarfs
63,1253713653724921858,60893773,James Bullock,['One thing many galaxy simulation groups agree on: galaxy formation will most alter dark matter density in galaxies ~50 times less massive than the milky way.  Here is a figure from @DarthLazar ’s new paper summarizing FIRE2 results compared to others\n\n<LINK> <LINK>'],https://arxiv.org/abs/2004.10817,"We analyze the cold dark matter density profiles of 54 galaxy halos simulated with FIRE-2 galaxy formation physics, each resolved within $0.5\%$ of the halo virial radius. These halos contain galaxies with masses that range from ultra-faint dwarfs ($M_\star \simeq 10^{4.5} M_{\odot}$) to the largest spirals ($M_\star \simeq 10^{11} M_{\odot}$) and have density profiles that are both cored and cuspy. We characterize our results using a new analytic density profile that extends the standard Einasto form to allow for a pronounced constant-density core in the resolved innermost radius. With one additional core-radius parameter, $r_{c}$, this ""core-Einasto"" profile is able to characterize the shape and normalization of our feedback-impacted dark matter halos. In order to enable comparisons with observations, we provide fitting functions for $r_{c}$ and other profile parameters as a function of both $M_\star$ and $M_{\star}/M_{\rm halo}$. In agreement with similar studies done in the literature, we find that dark matter core formation is most efficient at the characteristic stellar-mass to halo-mass ratio $M_\star/M_{\rm halo} \simeq 5 \times 10^{-3}$, or $M_{\star} \sim 10^9 \, M_{\odot}$, with cores that are roughly the size of the galaxy half-light radius, $r_{c} \simeq 1-5$ kpc. Furthermore, we find no evidence for core formation at radii $\gtrsim 100\ \rm pc$ in galaxies with $M_{\star}/M_{\rm halo} < 5\times 10^{-4}$ or $M_\star \lesssim 10^6 \, M_{\odot}$. For Milky Way-size galaxies, baryonic contraction often makes halos significantly more concentrated and dense at the stellar half-light radius than dark matter only runs. However, even at the Milky Way scale, FIRE-2 galaxy formation still produces small dark matter cores of $\simeq 0.5-2$ kpc in size. Recent evidence for a ${\sim} 2$ kpc core in the Milky Way's dark matter halo is consistent with this expectation. ","A dark matter profile to model diverse feedback-induced core sizes of
  $\Lambda$CDM haloes"
64,1253686621150629889,50901426,Rafael Alves Batista,"['New paper: \n""Search for magnetically-induced signatures in the arrival directions of ultra-high-energy cosmic rays measured at the Pierre Auger Observatory""\n<LINK> <LINK>']",https://arxiv.org/abs/2004.10591,"We search for signals of magnetically-induced effects in the arrival directions of ultra-high-energy cosmic rays detected at the Pierre Auger Observatory. We apply two different methods. One is a search for sets of events that show a correlation between their arrival direction and the inverse of their energy, which would be expected if they come from the same point-like source, they have the same electric charge and their deflection is relatively small and coherent. We refer to these sets of events as ""multiplets"". The second method, called ""thrust"", is a principal axis analysis aimed to detect the elongated patterns in a region of interest. We study the sensitivity of both methods using a benchmark simulation and we apply them to data in two different searches. The first search is done assuming as source candidates a list of nearby active galactic nuclei and starburst galaxies. The second is an all-sky blind search. We report the results and we find no statistically significant features. We discuss the compatibility of these results with the indications on the mass composition inferred from data of the Pierre Auger Observatory. ","Search for magnetically-induced signatures in the arrival directions of
  ultra-high-energy cosmic rays measured at the Pierre Auger Observatory"
65,1253533570720645120,3319563187,Xiaohui Fan,"['Our new paper on ""a significantly neutral IGM around the luminous z=7 quasar J0252-0503"", by @feigewang, @freddavies, Jinyi Yang, @joe_hennawi and collaborators. <LINK> <LINK>']",https://arxiv.org/abs/2004.10877,"Luminous $z\ge7$ quasars provide direct probes of the evolution of supermassive black holes (SMBHs) and the intergalactic medium (IGM) during the epoch of reionization (EoR). The Ly$\alpha$ damping wing absorption imprinted by neutral hydrogen in the IGM can be detected in a single EoR quasar spectrum, allowing the measurement of the IGM neutral fraction towards that line of sight. However, damping wing features have only been detected in two $z>7$ quasars in previous studies. In this paper, we present new high quality optical and near-infrared spectroscopy of the $z=7.00$ quasar DES J025216.64--050331.8 obtained with Keck/NIRES and Gemini/GMOS. By using the MgII single-epoch virial method, we find that it hosts a $\rm (1.39\pm0.16) \times10^{9} ~M_\odot$ SMBH accreting at an Eddington ratio of $\lambda_{\rm Edd}=0.7\pm0.1$, consistent with the values seen in other luminous $z\sim 7$ quasars. Furthermore, the Ly$\alpha$ region of the spectrum exhibits a strong damping wing absorption feature. The lack of associated metal absorption in the quasar spectrum indicates that this absorption is imprinted by a neutral IGM. Using a state-of-the-art model developed by Davies et al., we measure a volume-averaged neutral hydrogen fraction at $z=7$ of $\langle x_{\rm HI} \rangle = 0.70^{+0.20}_{-0.23} (^{+0.28}_{-0.48})$ within 68% (95%) confidence intervals when marginalizing over quasar lifetimes of $10^3\le t_{\rm Q}\le10^8$ yr. This is the highest IGM neutral fraction yet measured using reionization-era quasar spectra. ","A Significantly Neutral Intergalactic Medium Around the Luminous z=7
  Quasar J0252-0503"
66,1253493281008336897,601968022,Dilip Krishnan,"['New paper on *Supervised Contrastive Learning*: <LINK>\n\nA new loss function to train supervised deep networks, based on contrastive learning! Our new loss performs significantly better than cross-entropy across a range of architectures and data augmentations. <LINK>', 'It shows clear benefits in top-1 accuracy and robustness; and is more stable across a range of hyperparameters. Joint work with @PrannayKhosla @YonglongT @phillip_isola and other colleagues. https://t.co/AU4w68k9OA']",https://arxiv.org/abs/2004.11362,"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at this https URL ",Supervised Contrastive Learning
67,1253487271065653248,2252260178,Prannay Khosla,"['New paper out: <LINK>\n\nSupervised Contrastive Learning\n\nTLDR; representation learning by leveraging labels leads to more robust models and boosts in classification accuracy.', '@shivenmian No, this is submitted somewhere else. But we wanted to share our results.']",https://arxiv.org/abs/2004.11362,"Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at this https URL ",Supervised Contrastive Learning
68,1253452562805604352,911297187664949248,Jeff Dean (@🏡),"['I\'m very excited to point at a new paper on ""Chip Placement with Deep Reinforcement Learning"" that @Azaliamirh, @annadgoldie and many other co-authors and I put up on Arxiv.\n\nBlog:\n<LINK>\n\nPaper:\n<LINK> <LINK> <LINK>', 'Chip design problems are hard for RL because (1) the problem space is very large (10^9000 or more for modern chips, versus 10^360 for the game of Go &amp; 10^123 for chess), (2) there are multiple objectives that need to be balance (area, power, timing, design constraints, etc.), ...', '... and computing the true reward with EDA software tools often takes many hours (and for RL, we want to iterate and do thousands or millions of trials).', 'We  show that we can train the RL agent on many different designs &amp; then get an agent that can ""play the game of chip design"" on new, unseen designs, so that it can quickly produce high quality layouts, with a ""good"" layout done in seconds &amp; a high quality one in a few hours....', 'by fine tuning on that specific design.', 'Some of this was touched on in my ISSCC keynote earlier this year, starting around 26 minutes into https://t.co/HurdoHYdY8', 'These figures show placement process.  Squares are macros, empty represents std cells (""logic""). \n\nR: pre-trained policy asked to place a new design it has never seen.\nL: no pre-training.  \n\nR shows good intuition (e.g. logic in center).\nL gets there eventually https://t.co/Ts1lFN1yRX']",https://arxiv.org/abs/2004.10746,"In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks. ",Chip Placement with Deep Reinforcement Learning
69,1253448263526727680,73090248,Prof. Danushka Bollegala,['New paper on using contextualised graph attention for relation extraction with Angrosh Mandya and Frans Coenon <LINK> Considering multiple dependency sub-graphs and applying graph attention improves performance on SemEval 2010 Task 8 <LINK>'],https://arxiv.org/abs/2004.10624,"This paper presents a contextualized graph attention network that combines edge features and multiple sub-graphs for improving relation extraction. A novel method is proposed to use multiple sub-graphs to learn rich node representations in graph-based networks. To this end multiple sub-graphs are obtained from a single dependency tree. Two types of edge features are proposed, which are effectively combined with GAT and GCN models to apply for relation extraction. The proposed model achieves state-of-the-art performance on Semeval 2010 Task 8 dataset, achieving an F1-score of 86.3. ",Contextualised Graph Attention for Improved Relation Extraction
70,1253447184835645444,446634627,Roberto Santana,"['In our new paper (<LINK>) we use a variant of the #Deepfool algorithm to produce #adversarial class probability distributions. So, given a target distribution of the classes, the method generates #adversarial_examples with a distribution close to the target one. <LINK>']",https://arxiv.org/abs/2004.06383,"Despite the remarkable performance and generalization levels of deep learning models in a wide range of artificial intelligence tasks, it has been demonstrated that these models can be easily fooled by the addition of imperceptible yet malicious perturbations to natural inputs. These altered inputs are known in the literature as adversarial examples. In this paper, we propose a novel probabilistic framework to generalize and extend adversarial attacks in order to produce a desired probability distribution for the classes when we apply the attack method to a large number of inputs. This novel attack strategy provides the attacker with greater control over the target model, and increases the complexity of detecting that the model is being systematically attacked. We introduce four different strategies to efficiently generate such attacks, and illustrate our approach by extending multiple adversarial attack algorithms. We also experimentally validate our approach for the spoken command classification task, an exemplary machine learning problem in the audio domain. Our results demonstrate that we can closely approximate any probability distribution for the classes while maintaining a high fooling rate and by injecting imperceptible perturbations to the inputs. ","Extending Adversarial Attacks to Produce Adversarial Class Probability
  Distributions"
71,1253383314406354944,1038238746,Ulf Mertens,"['Our new paper on amortized Bayesian model comparison using evidential deep learning is finally on #arXiv! \n\n<LINK>', '@paulbuerkner I think you are the only one of the coauthors with a Twitter account ;).']",https://arxiv.org/abs/2004.10629,"Comparing competing mathematical models of complex natural processes is a shared goal among many branches of science. The Bayesian probabilistic framework offers a principled way to perform model comparison and extract useful metrics for guiding decisions. However, many interesting models are intractable with standard Bayesian methods, as they lack a closed-form likelihood function or the likelihood is computationally too expensive to evaluate. With this work, we propose a novel method for performing Bayesian model comparison using specialized deep learning architectures. Our method is purely simulation-based and circumvents the step of explicitly fitting all alternative models under consideration to each observed dataset. Moreover, it requires no hand-crafted summary statistics of the data and is designed to amortize the cost of simulation over multiple models and observable datasets. This makes the method particularly effective in scenarios where model fit needs to be assessed for a large number of datasets, so that per-dataset inference is practically infeasible.Finally, we propose a novel way to measure epistemic uncertainty in model comparison problems. We demonstrate the utility of our method on toy examples and simulated data from non-trivial models from cognitive science and single-cell neuroscience. We show that our method achieves excellent results in terms of accuracy, calibration, and efficiency across the examples considered in this work. We argue that our framework can enhance and enrich model-based analysis and inference in many fields dealing with computational models of natural processes. We further argue that the proposed measure of epistemic uncertainty provides a unique proxy to quantify absolute evidence even in a framework which assumes that the true data-generating model is within a finite set of candidate models. ",Amortized Bayesian model comparison with evidential deep learning
72,1253374007581249543,2714186617,Miho Janvier,['New paper out! Very happy to share this new research on the electric current evolution at the footpoints of solar eruptions 🌞💥 by Barczinski &amp; @LesiaAstro colleagues incl. @BrigitteSchmie1 🤩<LINK> #solarflares #solarphysics #magneticreconnection'],https://arxiv.org/abs/2004.07990,"Electric currents play a critical role in the triggering of solar flares and their evolution. The aim of the present paper is to test whether the surface electric current has a surface or subsurface fixed source as predicts the circuit approach of flare physics, or is the response of the surface magnetic field to the evolution of the coronal magnetic field as the MHD approach proposes. Out of all 19 X-class flares as observed by SDO from 2011 to 2016 near the disk center, we analyzed the only 9 eruptive flares for which clear ribbon-hooks were identifiable. Flare ribbons with hooks are considered to be the footprints of eruptive flux ropes in MHD flare models. For the first time, fine measurements of time-evolution of electric currents inside the hooks in the observations as well as in the OHM 3D MHD simulation are performed. Our analysis shows a decrease of the electric current in the area surrounded by the ribbon hooks during and after the eruption. We interpret the decrease of the electric currents as due to the expansion of the flux rope in the corona during the eruption. Our analysis brings a new contribution to the standard flare model in 3D. ",Electric current evolution at the footpoints of solar eruptions
73,1253312752241848321,967415814646267905,Anton Pichler,"['NEW PAPER on the Rise of Science in Low-Carbon Energy Technologies: <LINK>\n\nK.Hötte @unibielefeld, F.Lafond @INET_Complexity and I give a quantitative history of low-carbon energy innovations and show why it matters for policy! <LINK>', 'All data is available for download: https://t.co/FjDPDlpXDZ']",https://arxiv.org/abs/2004.09959,"Successfully combating climate change will require substantial technological improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient allocation of R\&D budgets requires a better understanding of how LCETs rely on scientific knowledge. Using data covering almost all US patents and scientific articles that are cited by them over the past two centuries, we describe the evolution of knowledge bases of ten key LCETs and show how technological interdependencies have changed over time. The composition of low-carbon energy innovations shifted over time, from Hydro and Wind energy in the 19th and early 20th century, to Nuclear fission after World War II, and more recently to Solar PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels (including energy from waste) have 35-65\% of their citations directed toward scientific papers, while this ratio is less than 10\% for Wind, Solar thermal, Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing science and the share of citations that are to scientific papers has been increasing for all technology types. The analysis of the scientific knowledge base of each LCET reveals three fairly separate clusters, with nuclear energy technologies, Biofuels and Waste, and all the other LCETs. Our detailed description of knowledge requirements for each LCET helps to design of targeted innovation policies. ",The rise of science in low-carbon energy technologies
74,1253247752848519171,841031248839618560,Relja Arandjelović,"['New paper by my student Ignacio Rocco - a 10x faster and less memory hungry NCNet with equivalent results (or more accurate with a smaller speedup) <LINK> . Code and models available <LINK> <LINK>', ""@ducha_aiki You mean to the challenge? I'll mention it but I'm not sure we have the time (NeurIPS, personal reasons, etc)""]",https://arxiv.org/abs/2004.10566,"In this work we target the problem of estimating accurately localised correspondences between a pair of images. We adopt the recent Neighbourhood Consensus Networks that have demonstrated promising performance for difficult correspondence problems and propose modifications to overcome their main limitations: large memory consumption, large inference time and poorly localised correspondences. Our proposed modifications can reduce the memory footprint and execution time more than $10\times$, with equivalent results. This is achieved by sparsifying the correlation tensor containing tentative matches, and its subsequent processing with a 4D CNN using submanifold sparse convolutions. Localisation accuracy is significantly improved by processing the input images in higher resolution, which is possible due to the reduced memory footprint, and by a novel two-stage correspondence relocalisation module. The proposed Sparse-NCNet method obtains state-of-the-art results on the HPatches Sequences and InLoc visual localisation benchmarks, and competitive results in the Aachen Day-Night benchmark. ","Efficient Neighbourhood Consensus Networks via Submanifold Sparse
  Convolutions"
75,1253205923511877638,1177063549606203394,Tommi Tenkanen,"['A new paper out! Here me and Erwin Tanin, a PhD student at Johns Hopkins, calculated a new upper limit on how much our observable universe could have expanded during an event called cosmic inflation. A preprint is available here: <LINK> 1/n <LINK>', 'Cosmic inflation, an era of accelerated expansion of the universe before the Big Bang epoch, generates gravitational waves which propagate through the space. 2/n', 'If the energy these gravitational waves carry was too high, they would mess up the formation of lights elements (the so-called Big Bang Nucleosynthesis or BBN), so that the amount of observed elements would not match to the amount predicted from theory. 3/n', 'The energy of gravitational waves at the time of BBN is not known but one can calculate that if the universe expanded in some funny way between inflation and BBN, it is bigger than what one would normally expect. 4/n', 'By using the limits on the amount of gravitational waves at the time of BBN, we infer an upper limit on how much the universe could have expanded between end of inflation and BBN - and how much during inflation itself. 5/n', 'We also showed that even the most optimistic future ground- or space-based gravitational wave observatories are unlikely to be able to improve this limit. 6/n', ""All in all, this was a nice project which I enjoyed a lot. This was also a new opening for me, as I had not really worked on gravitational waves before. It was great and a lot of fun to learn new things. Hope you'll like the outcome! 7/7""]",https://arxiv.org/abs/2004.10702,"Gravitational waves (GW) produced in the early Universe contribute to the number of relativistic degrees of freedom, $N_{\rm eff}$, during Big Bang Nucleosynthesis (BBN). By using the constraints on $N_{\rm eff}$, we present a new bound on how much the Universe could have expanded between horizon exit of the largest observable scales today and the end of inflation. We discuss the implications on inflationary models and show how the new constraints affect model selection. We also discuss the sensitivities of the current and planned GW observatories such as LIGO and LISA, and show that the constraints they could impose are always less stringent than the BBN bound. ",Gravitational wave constraints on the observable inflation
76,1253162036105601024,88519494,Mike Pritchard,"['[thread] New paper, from a ML collaboration with great CS colleagues  \n@UCIbrenICS. Life-changing new software to ease testing of keras-trained neural networks in fortran-based codes, and vice versa! Led by CS PhD student in Baldi group @jordanott_ — <LINK> ...', '...this new functionality has finally allowed me to test an unprecedented biodiversity of candidate NNs of convection within a large climate model code, a superpower I’ve lusted after for a long time...', '... prelim results shown in the preprint from large ensemble tests resolve an open debate about whether offline skill predicts online performance for this climate application of NN based sub grid convection (reassuringly, yes!)... @stephanrasp @DJGagneDos \n @NoahBrenowitz', '... I’ve been troubled by how hard it’s been to reproduce and generalize our original #AI based climate simulation led by @stephanrasp and w/ @PierreGentine . But this new semi-automated tuning+testing flow seems to finally reproduce similarly performant NNs in harder settings!', 'The trick is that this new fortran-2018 library takes the pain out of testing many NNs “online…” (coupled to explicit fortran based physics codes) w/o performance hit &amp; clean arteries to an external library that feels almost as object oriented as keras itself to interact with.', 'Basically: Can’t rave enough about this software if you are trying to test modern NNs within fortran codes (or even if you wanted to train one within a fortran code then interpret in python). Let me or @jordanott_ know if you use it or if  documentation / features are lacking', ""I'm hoping this will help fuel a step change in “online” testing of NN-based submodules in legacy fortran codes to better explore their actual coupled potential. It has done wonders for me in climate-world!"", 'Meant to  shout out co-author @realmilancurcic for laying the  open source foundation that vastly accelerated all of this, via his excellent ""neural-fortran"" git project (#OpenScience) and my constant #ML #climate co-conspirator @PierreGentine for supporting this community need.', '@redouanelg @jordanott_ Thanks for your interest! We don’t have a near term need for conv layers in our current approach to emulation of cloud physics for climate sims. But I agree it’d be great. I’ll defer to Jordan Ott and @realmilancurcic  on what it would take to do this... may I ask your use case?', '@omrjml @UCIbrenICS @jordanott_ Wonderful to hear. Please, please let us know if you do try FKB and especially where the documentation could use more from view of a new user; we’d like to help. Thanks! Knowing there is this broader interest helps our reporting to continue this kind of community tool dev work.', '@omrjml @UCIbrenICS @jordanott_ May I ask about these instabilities you are experiencing with emulated radiative transfer when used online in the Met Office model? Interesting &amp; I feel your pain! For convection it is easy to go unstable; see https://t.co/xx657bxmf6 for more, fun new paper led by @NoahBrenowitz', '@omrjml @UCIbrenICS @jordanott_ Speaking of the great @NoahBrenowitz he has cool ideas for how to access much more of the python/tensorflow stack from within climate models, albeit with overhead (I think). I imagine @jeremy_mcgibbon and others at their rad Vulcan team might have thoughts about this issue too.', '@omrjml @UCIbrenICS @jordanott_ Thanks! That was the intent. But it does take work. Fortunately @realmilancurcic and @jordanott_ are ninjas when it comes to migrating the small % of TF needed to do impactful basic NN work into a performant external f90 lib; I like that mode a lot!', '@ChaopengShen @DJGagneDos @UCIbrenICS @jordanott_ Not any time soon in mainstream operational climate and weather modeling, for better or worse. There are some promising new model prototypes emerging that wrap python around handy legacy fortran kernels though, which seems a great way to go too.', '@omrjml @UCIbrenICS @jordanott_ What are the upsides of online learning? I used to be excited about it but now that our workflow for reproducibly finding quality NNs seems to depend on big auto-hyperparameter searches (not clear to me these can be done online) I am less so. What’s your take lately @raspstephan?', '@redouanelg @jordanott_ @realmilancurcic @NoahBrenowitz Thanks! Please let us know if you do end up using it and esp if anything is unclear about how to interface; we’d like to improve the documentation for people just like you.', '@omrjml @UCIbrenICS @jordanott_ @NoahBrenowitz @raspstephan Very cool to know about this! My gut feeling these days is that it is so easy to miss a good fit but the combination of formal automated hyper parameter tuning (fitting 100s of candidate NNs in an intelligent search; e.g. SHERPA) coupled with rapid online testing boosts odds.', '@omrjml @UCIbrenICS @jordanott_ @NoahBrenowitz @raspstephan This is a zeitgeist inherited from my new CS collaborators in Pierre Baldi’s AI group at UCI... similar NN autotuning workflows seem important in their collars with high energy physicists, medicine, etc.', '@omrjml @UCIbrenICS @jordanott_ @NoahBrenowitz @raspstephan I’d like to write up some details about how moving to that workflow is improving reliability of our group’s emulation of SP soonish. Meanwhile if you/your team would like I’d be happy to speak over a few slides that I put together for a talk at Vulcan about it back in Feb. DM me!']",https://arxiv.org/abs/2004.10652,"Implementing artificial neural networks is commonly achieved via high-level programming languages like Python and easy-to-use deep learning libraries like Keras. These software libraries come pre-loaded with a variety of network architectures, provide autodifferentiation, and support GPUs for fast and efficient computation. As a result, a deep learning practitioner will favor training a neural network model in Python, where these tools are readily available. However, many large-scale scientific computation projects are written in Fortran, making it difficult to integrate with modern deep learning methods. To alleviate this problem, we introduce a software library, the Fortran-Keras Bridge (FKB). This two-way bridge connects environments where deep learning resources are plentiful, with those where they are scarce. The paper describes several unique features offered by FKB, such as customizable layers, loss functions, and network ensembles. The paper concludes with a case study that applies FKB to address open questions about the robustness of an experimental approach to global climate simulation, in which subgrid physics are outsourced to deep neural network emulators. In this context, FKB enables a hyperparameter search of one hundred plus candidate models of subgrid cloud and radiation physics, initially implemented in Keras, to be transferred and used in Fortran. Such a process allows the model's emergent behavior to be assessed, i.e. when fit imperfections are coupled to explicit planetary-scale fluid dynamics. The results reveal a previously unrecognized strong relationship between offline validation error and online performance, in which the choice of optimizer proves unexpectedly critical. This reveals many neural network architectures that produce considerable improvements in stability including some with reduced error, for an especially challenging training dataset. ",A Fortran-Keras Deep Learning Bridge for Scientific Computing
77,1253046693370540033,1093387119148462081,Daniel Green,"['New paper today.\n\nEver wonder about all the possible symmetries of cosmological correctors? Here is an answer:\n\n<LINK>\n\nThis is our first stab at a Coleman-Mandula theorem for cosmology.\n\nFun implication: conformal + single-field = Gaussian correlations', '@AnzeSlosar Apparently, careful proof-reading was a luxury of a simpler time']",https://arxiv.org/abs/2004.09587,"The space of inflationary models is vast, containing wide varieties of mechanisms, symmetries, and spectra of particles. Consequently, the space of observational signatures is similarly complex. Hence, it is natural to look for boundaries of the space of models and their signatures. In this paper, we explore the possible symmetries associated with the primordial cosmological perturbations and their correlators in the asymptotic future. Assuming the observed homogeneity, isotropy and (approximate) scale invariance, we prove three main results. First, correlation functions of scalar metric fluctuations are uniquely characterized by soft theorems and are free from ambiguity under field redefinitions. Second, whatever the particle content and interactions, when the standard soft theorems apply, invariance under de Sitter boosts (linearly realized conformal invariance) is only possible if all connected correlators vanish identically, i.e. if the theory is free. Third, conformal invariance is the largest set of linearly realized (bosonic) symmetries of the correlators of any single scalar, irrespectively of any soft theorems or particle content. ",On the Symmetries of Cosmological Perturbations
78,1253017585890648071,66175375,Jason Wang,"[""Just wanted to share this new paper on the protoplanets around PDS 70 based on new Keck data that I've been working on with a great team of early career scientists: <LINK>. Here's a tweet summary for those interested."", ""Using the new infrared pyramid wavefront sensor at Keck, we were able to get the cleanest images of PDS 70 b and c in L'-band. To characterize them, we had to remove contamination from the protoplanetary disk, thanks to modeling by @renrenbin and @Nicole_Wallack. https://t.co/58ixXeuth8"", ""We don't have much orbital motion, so I had to use dynamically motivated constraints of non-crossing orbits and being &lt; 20° misaligned from the disk to get realistic orbits. I used the custom likelihood function in orbitize! which allows users to add orbital constraints like this https://t.co/d3AEr1Fd9L"", ""We fit some atmospheric models to the planets' SEDs, but found that a blackbody is preferred with the current data. We also found that the total inferred luminosity is pretty consistent regardless of which atmosphere model is used. We'll come back to if a blackbody is realistic. https://t.co/MnB97DqPQh"", 'I teamed up with fellow @HSFdn #51Pegb fellows Sivan Ginzburg and @PlanetaryGao to interpret these results. Sivan used the my luminosities and his evolutionary models for protoplanets to infer that these planets are 1-4 Jupiter masses (some of the lowest mass imaged planets!)', ""@PlanetaryGao used the mass accretion rates inferred by Sivan to show that dust accretes so fast that it should shroud the atmosphere of this planet. This means that a blackbody is not a terribly unphysical model for what we're seeing. It makes sense, for now...""]",https://arxiv.org/abs/2004.09597,"We present $L$'-band imaging of the PDS 70 planetary system with Keck/NIRC2 using the new infrared pyramid wavefront sensor. We detected both PDS 70 b and c in our images, as well as the front rim of the circumstellar disk. After subtracting off a model of the disk, we measured the astrometry and photometry of both planets. Placing priors based on the dynamics of the system, we estimated PDS 70 b to have a semi-major axis of $20^{+3}_{-4}$~au and PDS 70 c to have a semi-major axis of $34^{+12}_{-6}$~au (95\% credible interval). We fit the spectral energy distribution (SED) of both planets. For PDS 70 b, we were able to place better constraints on the red half of its SED than previous studies and inferred the radius of the photosphere to be 2-3~$R_{Jup}$. The SED of PDS 70 c is less well constrained, with a range of total luminosities spanning an order of magnitude. With our inferred radii and luminosities, we used evolutionary models of accreting protoplanets to derive a mass of PDS 70 b between 2 and 4 $M_{\textrm{Jup}}$ and a mean mass accretion rate between $3 \times 10^{-7}$ and $8 \times 10^{-7}~M_{\textrm{Jup}}/\textrm{yr}$. For PDS 70 c, we computed a mass between 1 and 3 $M_{\textrm{Jup}}$ and mean mass accretion rate between $1 \times 10^{-7}$ and $5 \times~10^{-7} M_{\textrm{Jup}}/\textrm{yr}$. The mass accretion rates imply dust accretion timescales short enough to hide strong molecular absorption features in both planets' SEDs. ","Keck/NIRC2 $L$'-Band Imaging of Jovian-Mass Accreting Protoplanets
  around PDS 70"
79,1252990395195293702,863128474743189508,Giuseppe Cuccu,"['RL is hard! How do we estimate the complexity of different environments? Our new AAMAS20 paper (@heyitdeclan, Tobias and @giuse_tweets) answers using tiny neural networks and *no training*. <LINK> (1/3) <LINK>', 'We generate statistically founded measures of environment complexity, by studying the performance of networks generated with Random Weight Guessing. Our plots answer at a glance: is initialization hard? Does the fitness have discontinuities / plateaus? And more! (2/3) https://t.co/XS3qcatf6L', 'Further discussion is happening here: reddit https://t.co/t4NpZBHOaM deepai https://t.co/KrUIbc2l98 Ask us anything! (3/3)', 'A few people who I think could be interested in hearing about this -- no spam intended ;) -- @togelius @hardmaru @dennybritz @Miles_Brundage @danbri @andrey_kurenkov', ""Also ping to @eXascaleInfolab and @unifr on my side, and @ruhrunibochum for Tobias, while our talented first author @heyitdeclan is currently up for grabs! https://t.co/MrgofCJTDB yeah that's a PhD in Physics from Brown right there ;)"", '@Zergylord @heyitdeclan Yeah actually I can agree with that. Reason though is simply that I cut down ""No training algorithm"" to fit in 280 chars :) I should have swapped for ""no learning"". Confusing though? (Note it is *not* Random Search though, as the mean is not updated. No learning I promise :) )', ""@andrey_kurenkov @togelius @hardmaru @dennybritz @Miles_Brundage @danbri :D eheh follow up indeed ;) I'm working on waaay too many projects man, that's the way when you're having fun! :D but yeah spoiler alert, convolution keeps the input weight matrix still relatively small, as do random patches. I'm looking at you Atari."", '@aaronsnoswell @heyitdeclan Hi Aaron Declan is a great first author and speaks for all three of us :) thanks for the great questions! We really would like to bring more attention to these graphs, they make a fundamental point of our contribution.', '@aaronsnoswell @heyitdeclan Depending on the RL benchmark the controller will change, the network generation (or even train) would too, but what we try to get across is (i) you can generate these plots and (ii) they are intuitively informative, (iii) metrics can be derived for automating the evaluation.', ""@aaronsnoswell @heyitdeclan There's a lot of great work going in this direction, especially from the perspective of automated environment (/level) generation (could cite @togelius GIL lab's work for days here). Still a lot to go, and here's our tiny (but realistically usable!) contribution :)"", ""@heyitdeclan @aaronsnoswell We found the immediacy of looking at these graphs to be just illuminating -- I remember the enthusiasm of Tobias the first time we got this right :) clearly there's work to do to extend to more complex environment, but it's a doable road (in progress!). Contributions are welcome!""]",https://arxiv.org/abs/2004.07707,"We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights specific types of challenges, and provides a proper foundation for the statistical analysis of the task's difficulty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks. ",Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing
80,1252913147951943680,725183149769105411,Ryan Mann,"['New paper with Luke Mathieson and Catherine Greenhill --- ""On the Parameterised Complexity of Induced Multipartite Graph Parameters"" --- <LINK>']",https://arxiv.org/abs/2004.09938,"We introduce a family of graph parameters, called induced multipartite graph parameters, and study their computational complexity. First, we consider the following decision problem: an instance is an induced multipartite graph parameter $p$ and a given graph $G$, and for natural numbers $k\geq2$ and $\ell$, we must decide whether the maximum value of $p$ over all induced $k$-partite subgraphs of $G$ is at most $\ell$. We prove that this problem is W[1]-hard. Next, we consider a variant of this problem, where we must decide whether the given graph $G$ contains a sufficiently large induced $k$-partite subgraph $H$ such that $p(H)\leq\ell$. We show that for certain parameters this problem is para-NP-hard, while for others it is fixed-parameter tractable. ",On the Parameterised Complexity of Induced Multipartite Graph Parameters
81,1252848314187255808,39688015,Chris Usher,"['New paper out that I am a coauthor on <LINK>', 'Massive star clusters older than 2 Gyr show internal spreads in the abundances of light elements like He, N and Na. Star clusters younger than this show homogenous chemistry.', 'But the stars we typically study on the red giant branch are different masses at different ages so to see if the differences between old and young are due to a stellar evolution effect we looked at stars on the main sequence in a 1.5 Gyr star cluster in the Small Magellanic Cloud https://t.co/VMAvfnjdwX', 'Unfortunately we could not see any N spreads in the NGC 419 stars that have the same masses as the stars we study in old star clusters', 'The image is a @HubbleTelescope image of the cluster we studied, NGC 419, I quickly grabbed off the Hubble Legacy Archive.']",https://arxiv.org/abs/2004.09636,"The spectroscopic and photometric signals of the star-to-star abundance variations found in globular clusters seem to be correlated with global parameters like the cluster's metallicity, mass and age. Understanding this behaviour could bring us closer to the origin of these intriguing abundance spreads. In this work we use deep HST photometry to look for evidence of abundance variations in the main sequence of a young massive cluster NGC 419 ($\sim10^5$ M$_{\odot}$, $\sim1.4$ Gyr). Unlike previous studies, here we focus on stars in the same mass range found in old globulars ($\sim0.75-1$ M$_{\odot}$), where light elements variations are detected. We find no evidence for N abundance variations among these stars in the $Un-B$ and $U-B$ CMD of NGC 419. This is at odds with the N-variations found in old globulars like 47 Tuc, NGC 6352 and NGC 6637 with similar metallicity to NGC 419. Although the signature of the abundance variations characteristic of old globulars appears to be significantly smaller or absent in this young cluster, we cannot conclude if this effect is mainly driven by its age or its mass. ","Searching for globular cluster chemical anomalies on the main sequence
  of a young massive cluster"
82,1252806732163584000,347828252,Yasser Shoukry,"['[1/3] Very excited about our new results on architectural assurance for Neural Networks when used in control applications <LINK>. In a new paper with my postdoc @JamesFerlez and Ph.D. student Xiaowu Sun, we extended our previous results on Two-Level Lattice NNs', '[2/3] to design “assured” architectures for neural networks when the underlying system evolves according to some nonlinear dynamics. Whereas current techniques are based on hand-picked architectures or heuristic-based search to find such architectures,', '[3/3] our approach exploits the given nonlinear model of the system to find the NN architecture “without” having access to training data. We provide a guarantee that the resulting NN architecture is sufficient to implement a controller that satisfies an achievable specification.']",https://arxiv.org/abs/2004.09628,"In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and number of neurons per layer) with the guarantee that it is sufficiently parametrized to control a nonlinear system. Whereas current state-of-the-art techniques are based on hand-picked architectures or heuristic based search to find such NN architectures, our approach exploits the given model of the system to design an architecture; as a result, we provide a guarantee that the resulting NN architecture is sufficient to implement a controller that satisfies an achievable specification. Our approach exploits two basic ideas. First, assuming that the system can be controlled by an unknown Lipschitz-continuous state-feedback controller with some Lipschitz constant upper-bounded by $K_\text{cont}$, we bound the number of affine functions needed to construct a Continuous Piecewise Affine (CPWA) function that can approximate the unknown Lipschitz-continuous controller. Second, we utilize the authors' recent results on a novel NN architecture named as the Two-Level Lattice (TLL) NN architecture, which was shown to be capable of implementing any CPWA function just from the knowledge of the number of affine functions that compromises this CPWA function. ","Two-Level Lattice Neural Network Architectures for Control of Nonlinear
  Systems"
83,1252723664740446208,14986849,Alex Smola 🇺🇦,"[""There's a new backbone network for CV in town - ResNeSt (Split Attention Networks). 84.53% top-1 accuracy on ImageNet. Check out the paper at <LINK> and code in GluonCV 0.7 at <LINK>""]",https://arxiv.org/abs/2004.08955,"It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available. ",ResNeSt: Split-Attention Networks
84,1252710198038466566,2704467561,Melinda Mills,['In our new #COVIDー19 social network paper we show that repeated contact within networks is the most effective #exitstrategy to #FlattenTheCurve but also closed &amp; similar networks <LINK> @OxfordDemSci <LINK>'],http://arxiv.org/abs/2004.07052,"Social distancing and isolation have been introduced widely to counter the COVID-19 pandemic. However, more moderate contact reduction policies become desirable owing to adverse social, psychological, and economic consequences of a complete or near-complete lockdown. Adopting a social network approach, we evaluate the effectiveness of three targeted distancing strategies designed to 'keep the curve flat' and aid compliance in a post-lockdown world. These are limiting interaction to a few repeated contacts, seeking similarity across contacts, and strengthening communities via triadic strategies. We simulate stochastic infection curves that incorporate core elements from infection models, ideal-type social network models, and statistical relational event models. We demonstrate that strategic reduction of contact can strongly increase the efficiency of social distancing measures, introducing the possibility of allowing some social contact while keeping risks low. This approach provides nuanced insights to policy makers for effective social distancing that can mitigate negative consequences of social isolation. ","Social network-based distancing strategies to flatten the COVID 19 curve
  in a post-lockdown world"
85,1252654952658411525,82497649,Moin Nadeem,"['As pretrained language models grow more common in #NLProc, it is crucial to evaluate their societal biases. We launch a new task, evaluation metrics, and a large dataset to measure stereotypical biases in LMs: \nPaper: <LINK>\nSite: <LINK>\nThread👇 <LINK>', '[2/] The task and metrics are based on an ideal language model (LM). An ideal LM should perform well at language modeling, but not have a preference for stereotypes or anti-stereotypes. We create the Context Association Test (CAT), which measures LM ability and stereotype ability', ""[3/] We measure LM ability based on the model's preferences for meaningful contexts over meaningless contexts. The LM score of an ideal model is 100. Similarly, we measure stereotype bias based on how often the model prefers stereotypical contexts vs anti-stereotypical contexts."", '[4/] The stereotypical bias score of an idealistic model is 50. The combination of these two scores gives the Idealized CAT score, which measures the unbiased LM ability.', '[5/] We choose to measure bias in four domains: gender, profession, race, and religion, and collect 16,995 sentences that characterize the human stereotypical biases for these domains. https://t.co/e94qk00s2E', '[6/] We find that as a model size (# parameters) increases, so does it’s LM ability and stereotypical behavior! However, we find that this isn’t necessarily correlated with idealistic LM ability. https://t.co/UzSWFEIAwF', '[7/] We find that GPT2 is relatively more idealistic than BERT, XLNet and RoBERTa. We conjecture this is due to nature of pretraining data (Reddit data is likely to see more stereotypes and anti-stereotypes. c.f. Section 8). However, GPT is still 27 ICAT points behind an ideal LM', '[8/] We also study an ensemble of BERT-large, GPT2-large, and GPT2-medium, and conjecture that the most biased terms are the ones that have well-established stereotypes in society (but with some surprising exceptions). https://t.co/EBU4rx7UTL', '[End] Joint work with @data_beth and @sivareddyg \nCode: https://t.co/rHa9UDn59I\n\nHappy to answer any questions as well!']",https://arxiv.org/abs/2004.09456,"A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present StereoSet, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and XLNet on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at this https URL ",StereoSet: Measuring stereotypical bias in pretrained language models
86,1252649152158355462,2818695390,Sasho Nikolov,"['New paper with Vivek Madan, @mohitsinghr, and Tao Tantipongpipat, the result of an awesome visit to Atlanta last August (which feels approximately a century ago). ""Maximizing Determinants under Matroid Constraints"" <LINK>', 'The problem: given n rank-1 d-by-d matrices, and a matroid of rank k over them, find a basis B of the matroid so that the sum of the matrices in B has the largest determinant. This shows up in many settings: optimal design, network design, allocation of goods, ML.', 'We give the first algorithms with approximation factor that only depends on the dimension d, and not on the rank k. The main idea is to show that a convex relaxation of the problem has an optimal solution with only d^2 fractional variables. Surprising given the non-linearity.', 'We also leverage known and cool connections with real stable &amp; completely log-concave polynomials. We show that a relaxation studied by myself and Mohit, and also by Straszak and @NisheethVishnoi, is at least as strong as one by @nimaanari and @oveisgharan.']",https://arxiv.org/abs/2004.07886,"Given vectors $v_1,\dots,v_n\in\mathbb{R}^d$ and a matroid $M=([n],I)$, we study the problem of finding a basis $S$ of $M$ such that $\det(\sum_{i \in S}v_i v_i^\top)$ is maximized. This problem appears in a diverse set of areas such as experimental design, fair allocation of goods, network design, and machine learning. The current best results include an $e^{2k}$-estimation for any matroid of rank $k$ and a $(1+\epsilon)^d$-approximation for a uniform matroid of rank $k\ge d+\frac d\epsilon$, where the rank $k\ge d$ denotes the desired size of the optimal set. Our main result is a new approximation algorithm with an approximation guarantee that depends only on the dimension $d$ of the vectors and not on the size $k$ of the output set. In particular, we show an $(O(d))^{d}$-estimation and an $(O(d))^{d^3}$-approximation for any matroid, giving a significant improvement over prior work when $k\gg d$. Our result relies on the existence of an optimal solution to a convex programming relaxation for the problem which has sparse support; in particular, no more than $O(d^2)$ variables of the solution have fractional values. The sparsity results rely on the interplay between the first-order optimality conditions for the convex program and matroid theory. We believe that the techniques introduced to show sparsity of optimal solutions to convex programs will be of independent interest. We also give a randomized algorithm that rounds a sparse fractional solution to a feasible integral solution to the original problem. To show the approximation guarantee, we utilize recent works on strongly log-concave polynomials and show new relationships between different convex programs studied for the problem. Finally, we use the estimation algorithm and sparsity results to give an efficient deterministic approximation algorithm with an approximation guarantee that depends solely on the dimension $d$. ",Maximizing Determinants under Matroid Constraints
87,1252579597255835649,526115229,Kevin Heng,"[""Another comprehensive paper from our infatiguable Jens Hoeijmakers (@HoeijmakersJens), who is a joint #PlanetS postdoc with me and @davehrenreich. This time on MASCARA-2b with Debra Fischer's new EXPRES spectrograph. More metals and ions from another UHJ! <LINK>""]",https://arxiv.org/abs/2004.08415,"We report detections of atomic species in the atmosphere of MASCARA-2 b, using the first transit observations obtained with the newly commissioned EXPRES spectrograph. EXPRES is a highly stabilised optical echelle spectrograph, designed to detect stellar reflex motions with amplitudes down to 30 cm/s, and was recently deployed at the Lowell Discovery Telescope. By analysing the transmission spectrum of the ultra-hot Jupiter MASCARA-2 b using the cross-correlation method, we confirm previous detections of Fe I, Fe II and Na I, which likely originate in the upper regions of the inflated atmosphere. In addition, we report significant detections of Mg I and Cr II. The absorption strengths change slightly with time, possibly indicating different temperatures and chemistry in the day-side and night-side terminators. Using the effective stellar line-shape variation induced by the transiting planet, we constrain the projected spin-orbit misalignment of the system to $1.6\pm3.1$ degrees, consistent with an aligned orbit. We demonstrate that EXPRES joins a suite of instruments capable of phase-resolved spectroscopy of exoplanet atmospheres. ",High-resolution Transmission Spectroscopy of MASCARA-2 b with EXPRES
88,1252463626306256896,22809805,Dana C. Ernst,"['New paper! My 5th paper with @bretbenesh &amp; 6th with Nandor Sieben. I\'m extremely grateful to have these 2 as collaborators! I wouldn\'t be nearly as productive without them. ""The spectrum of nim-values for achievement games for generating finite groups"". <LINK>']",https://arxiv.org/abs/2004.08980,"We study an impartial achievement game introduced by Anderson and Harary. The game is played by two players who alternately select previously unselected elements of a finite group. The game ends when the jointly selected elements generate the group. The last player able to make a move is the winner of the game. We prove that the spectrum of nim-values of these games is $\{0,1,2,3,4\}$. This positively answers two conjectures from a previous paper by the last two authors. ","The spectrum of nim-values for achievement games for generating finite
  groups"
89,1252437773832740864,2228370313,Damien Teney,"[""(1/4) There's been a few new datasets with contrastive/counterfactual/adversarial examples in vision and NLP. Our latest paper shows a better way to use them for training than just data augmentation. Use them to supervise the orientation of your gradient <LINK> <LINK>"", '(2/4) A few of these new datasets:\nEvaluating NLP Models via Contrast Sets @nlpmattg\nhttps://t.co/kf8LDwjFzL\n\nNatural Perturbation for Robust QA @DanielKhashabi\nhttps://t.co/Q31kaRo4Rs', '(3/4) Contrastive Examples for Addressing the Tyranny of the Majority  @trevordarrell\nhttps://t.co/7lJoVK2Gpv\n\nTowards Causal VQA - Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing\nhttps://t.co/aMG1A5eUOL', '(4/4) Learning the difference that makes a difference @dkaushik96 @zacharylipton\nhttps://t.co/uMP7i74kzK']",http://arxiv.org/abs/2004.09034,"One of the primary challenges limiting the applicability of deep learning is its susceptibility to learning spurious correlations rather than the underlying mechanisms of the task of interest. The resulting failure to generalise cannot be addressed by simply using more data from the same distribution. We propose an auxiliary training objective that improves the generalization capabilities of neural networks by leveraging an overlooked supervisory signal found in existing datasets. We use pairs of minimally-different examples with different labels, a.k.a counterfactual or contrasting examples, which provide a signal indicative of the underlying causal structure of the task. We show that such pairs can be identified in a number of existing datasets in computer vision (visual question answering, multi-label image classification) and natural language processing (sentiment analysis, natural language inference). The new training objective orients the gradient of a model's decision function with pairs of counterfactual examples. Models trained with this technique demonstrate improved performance on out-of-distribution test sets. ","Learning What Makes a Difference from Counterfactual Examples and
  Gradient Supervision"
90,1252406595775893505,777916820984475648,Homanga Bharadhwaj,"['New L4DC2020 paper: \n\nWe investigate the problem of planning in Model-based RL through the lens of CEM and Gradient-Descent, analyzing their resp. benefits and pitfalls. \n\npaper: <LINK>\ncode: <LINK>\n\nw/ Kevin Xie, and @florian_shkurti \n\n1/3 <LINK>', 'We consider both the settings where the underlying dynamics+reward models are known and where they are learned, and based on the analyses propose a *simple* scheme of interleaving  CEM and gradient descent update steps for planning.\n\n2/3 https://t.co/ZYokLv3WUL', 'CEM does not  scale with increasing action dim. while g.d. fails when there are multiple obstacles. The *simple* proposed scheme scales well to high action dim., to multiple obstacles, and converges faster than CEM while achieving similar/better asymptotic performance.\n\n3/3 https://t.co/Hr5hnV4u34', '@UofTCompSci @UofTRobotics @VectorInst']",https://arxiv.org/abs/2004.08763,"Recent works in high-dimensional model-predictive control and model-based reinforcement learning with learned dynamics and reward models have resorted to population-based optimization methods, such as the Cross-Entropy Method (CEM), for planning a sequence of actions. To decide on an action to take, CEM conducts a search for the action sequence with the highest return according to the dynamics model and reward. Action sequences are typically randomly sampled from an unconditional Gaussian distribution and evaluated on the environment. This distribution is iteratively updated towards action sequences with higher returns. However, this planning method can be very inefficient, especially for high-dimensional action spaces. An alternative line of approaches optimize action sequences directly via gradient descent, but are prone to local optima. We propose a method to solve this planning problem by interleaving CEM and gradient descent steps in optimizing the action sequence. Our experiments show faster convergence of the proposed hybrid approach, even for high-dimensional action spaces, avoidance of local minima, and better or equal performance to CEM. Code accompanying the paper is available here this https URL ","Model-Predictive Control via Cross-Entropy and Gradient-Based
  Optimization"
91,1252404828849836032,2196938186,Johnathon Jordan,"['When I was a kid, there was a while where I wanted to be a paleontologist. This may be as close as I get. New paper out on measuring atmospheric neutrinos with paleo-detectors: <LINK>']",https://arxiv.org/abs/2004.08394,"Measuring the cosmic ray flux over timescales comparable to the age of the solar system, $\sim 4.5\,$Gyr, could provide a new window on the history of the Earth, the solar system, and even our galaxy. We present a technique to indirectly measure the rate of cosmic rays as a function of time using the imprints of atmospheric neutrinos in paleo-detectors, natural minerals which record damage tracks from nuclear recoils. Minerals commonly found on Earth are $\lesssim 1\,$Gyr old, providing the ability to look back across cosmic ray history on timescales of the same order as the age of the solar system. Given a collection of differently aged samples dated with reasonable accuracy, this technique is particularly well-suited to measuring historical changes in the cosmic ray flux at Earth and is broadly applicable in astrophysics and geophysics. ","Measuring Changes in the Atmospheric Neutrino Rate Over Gigayear
  Timescales"
92,1252403092349317125,20507533,Kohei Kawaguchi-Sunada,"['We released a new working paper ""Estimating High-Dimensional Discrete Choice Model of Differentiated Products with Random Coefficients"". This is a preliminary version. We add simulation and application. So far, the work  is all done by Masayuki Sawada. <LINK>']",https://arxiv.org/abs/2004.08791,"We propose an estimation procedure for discrete choice models of differentiated products with possibly high-dimensional product attributes. In our model, high-dimensional attributes can be determinants of both mean and variance of the indirect utility of a product. The key restriction in our model is that the high-dimensional attributes affect the variance of indirect utilities only through finitely many indices. In a framework of the random-coefficients logit model, we show a bound on the error rate of a $l_1$-regularized minimum distance estimator and prove the asymptotic linearity of the de-biased estimator. ","Estimating High-Dimensional Discrete Choice Model of Differentiated
  Products with Random Coefficients"
93,1252402128351444994,1226577054,Kari Dalnoki-Veress,"['Excited about our new paper in EPJE on wrinkling of bilayer films, a collaboration with @JamesSSharp from @UniofNottingham. #patternformation #SciArt \n<LINK> \n<LINK>\n@McMasterScience <LINK>']",https://arxiv.org/abs/2004.08624,"Periodic wrinkling of a rigid capping layer on a deformable substrate provides a useful method for templating surface topography for a variety of novel applications. Many experiments have studied wrinkle formation during the compression of a rigid film on a relatively soft pre-strained elastic substrate, and most have focused on the regime where the substrate thickness can be considered semi-infinite relative to that of the film. As the relative thickness of the substrate is decreased, the bending stiffness of the film dominates, causing the bilayer to transition to either local wrinkling or a global buckling instability. In this work optical microscopy was used to study the critical parameters that determine the emergence of local wrinkling or global buckling of freestanding bilayer films consisting of a thin rigid polymer capping layer on a pre-strained elastomeric substrate. The thickness ratio of the film and substrate as well as the pre-strain were controlled and used to create a buckling phase diagram which describes the behaviour of the system as the ratio of the thickness of the substrate is decreased. A simple force balance model was developed to understand the thickness and strain dependences of the wrinkling and buckling modes, with excellent quantitative agreement being obtained with experiments using only independently measured material parameters. ","The emergence of local wrinkling or global buckling in thin freestanding
  bilayer films"
94,1251514551717199875,1156325882157522944,Akbar Namin,['A new paper accepted in @SNApplSci. The paper presents an encoder-decoder deep learning-based model to cluster time series data. A joint work with collaborators from US (Texas Tech and Georgia Tech) and Hungary (University of Debrecen). \nPre-print: <LINK>'],https://arxiv.org/abs/2004.07296,"Machine learning and in particular deep learning algorithms are the emerging approaches to data analysis. These techniques have transformed traditional data mining-based analysis radically into a learning-based model in which existing data sets along with their cluster labels (i.e., train set) are learned to build a supervised learning model and predict the cluster labels of unseen data (i.e., test set). In particular, deep learning techniques are capable of capturing and learning hidden features in a given data sets and thus building a more accurate prediction model for clustering and labeling problem. However, the major problem is that time series data are often unlabeled and thus supervised learning-based deep learning algorithms cannot be directly adapted to solve the clustering problems for these special and complex types of data sets. To address this problem, this paper introduces a two-stage method for clustering time series data. First, a novel technique is introduced to utilize the characteristics (e.g., volatility) of given time series data in order to create labels and thus be able to transform the problem from unsupervised learning into supervised learning. Second, an autoencoder-based deep learning model is built to learn and model both known and hidden features of time series data along with their created labels to predict the labels of unseen time series data. The paper reports a case study in which financial and stock time series data of selected 70 stock indices are clustered into distinct groups using the introduced two-stage procedure. The results show that the proposed procedure is capable of achieving 87.5\% accuracy in clustering and predicting the labels for unseen time series data. ","Clustering Time Series Data through Autoencoder-based Deep Learning
  Models"
95,1251280387738791937,1169037315823398913,Benjamin Avanzi,"['New paper on arXiv: <LINK> In this paper, we determine the optimal periodic dividend strategy with fixed transaction costs for spectrally negative Lévy processes in presence of fixed transaction costs.']",https://arxiv.org/abs/2004.01838,"Maximising dividends is one classical stability criterion in actuarial risk theory. Motivated by the fact that dividends are paid periodically in real life, $\textit{periodic}$ dividend strategies were recently introduced (Albrecher, Gerber and Shiu, 2011). In this paper, we incorporate fixed transaction costs into the model and study the optimal periodic dividend strategy with fixed transaction costs for spectrally negative L\'evy processes. The value function of a periodic $(b_u,b_l)$ strategy is calculated by means of exiting identities and It\^o's excusion when the surplus process is of unbounded variation. We show that a sufficient condition for optimality is that the L\'evy measure admits a density which is completely monotonic. Under such assumptions, a periodic $(b_u,b_l)$ strategy is confirmed to be optimal. Results are illustrated. ","Optimal periodic dividend strategies for spectrally negative L\'evy
  processes with fixed transaction costs"
96,1251168981244940288,355616314,Murray Shanahan,"['New paper with Christos Kaplanis and Claudia Clopath ""Continual Reinforcement Learning with Multi-Timescale Replay"" - <LINK>']",https://arxiv.org/abs/2004.07530,"In this paper, we propose a multi-timescale replay (MTR) buffer for improving continual learning in RL agents faced with environments that are changing continuously over time at timescales that are unknown to the agent. The basic MTR buffer comprises a cascade of sub-buffers that accumulate experiences at different timescales, enabling the agent to improve the trade-off between adaptation to new data and retention of old knowledge. We also combine the MTR framework with invariant risk minimization, with the idea of encouraging the agent to learn a policy that is robust across the various environments it encounters over time. The MTR methods are evaluated in three different continual learning settings on two continuous control tasks and, in many cases, show improvement over the baselines. ",Continual Reinforcement Learning with Multi-Timescale Replay
97,1251148473354731521,742027674403672064,Johnny Greco,"['🚨PAPER ALERT🚨 \n\nHow far is a galaxy? Almost everything we want to know about it depends on the answer. In our new paper, we take a deep dive into the theory and feasibility of measuring distances to diffuse dwarf galaxies using *imaging data alone*: <LINK> <LINK>', 'We use surface brightness fluctuations, which arise from pixel-to-pixel Poisson fluctuations in the number of bright stars in a galaxy image. The more distant the galaxy, the smaller the fluctuations. The effect is shown in this fig. Ω = resolution element. #DistancesAreHard https://t.co/cOzrXKbgKJ', ""@rareflwr41 For you, anything🙂here's another: https://t.co/7C7ILHnGR2"", 'We compare our calculations to observations of dwarf galaxies with independent distance measurements based on the TRGB. Single-age stellar pop models are in good agreement with red dwarfs, but blue dwarfs are more consistent with composite stellar pop models. #DistancesAreHard https://t.co/Ba9ov8xSQQ', 'To study SBF in practice, we inject artificial galaxies star-by-star into images from the #HyperSuprimeCam survey using one of my favorite tools: #ArtPop! @DanieliShany #DistancesAreHard \n\n👇👇👇👇\n\nhttps://t.co/Phq6a86JjG', 'There’s more, but I’ll end with an exciting plot for the future. LSST will be capable of measuring SBF distances to dwarf galaxies out to ~25 Mpc! HUGE thanks to my amazing collaborators @DokkumPieter, @DanieliShany, Scott Carlsten, and Charlie Conroy! https://t.co/YKYKw12onM https://t.co/ZgzMNjKsPi', '@dstndstn Thanks, Dustin! That means a lot coming from you. \n\nNot naive all -- You want as many resolution elements as possible, so my expectation is that you want GOOD seeing (and you need to understand your PSF). And for the regime we are interested in, we are already short on pixels!', '@BenneHolwerda https://t.co/cEsKO4zgnC', '@BenneHolwerda You mean for the gif? I certainly can! @rareflwr41 wanted one too, so I can make a higher resolution version and put it somewhere public. But we are working on making ArtPop open source sometime soon-ish so you can make one too!', '@JohnFeldmeier Thank you 🙏']",https://arxiv.org/abs/2004.07273,"We present an in-depth study of surface brightness fluctuations (SBFs) in low-luminosity stellar systems. Using the MIST models, we compute theoretical predictions for absolute SBF magnitudes in the LSST, HST ACS/WFC, and proposed Roman Space Telescope filter systems. We compare our calculations to observed SBF-color relations of systems that span a wide range of age and metallicity. Consistent with previous studies, we find that single-age population models show excellent agreement with observations of low-mass galaxies with $0.5 \lesssim g - i \lesssim 0.9$. For bluer galaxies, the observed relation is better fit by models with composite stellar populations. To study SBF recovery from low-luminosity systems, we perform detailed image simulations in which we inject fully populated model galaxies into deep ground-based images from real observations. Our simulations show that LSST will provide data of sufficient quality and depth to measure SBF magnitudes with precisions of ${\sim}0.2$-0.5 mag in ultra-faint $\left(\mathrm{10^4 \leq M_\star/M_\odot \leq 10^5}\right)$ and low-mass classical (M$_\star\leq10^7$ M$_\odot$) dwarf galaxies out to ${\sim}4$ Mpc and ${\sim}25$ Mpc, respectively, within the first few years of its deep-wide-fast survey. Many significant practical challenges and systematic uncertainties remain, including an irreducible ""sampling scatter"" in the SBFs of ultra-faint dwarfs due to their undersampled stellar mass functions. We nonetheless conclude that SBFs in the new generation of wide-field imaging surveys have the potential to play a critical role in the efficient confirmation and characterization of dwarf galaxies in the nearby universe. ","Measuring distances to low-luminosity galaxies using surface brightness
  fluctuations"
98,1251146972246839312,1640407356,Dirk Hovy,"['Cross-lingual neural topic models: train on the language you have, apply to the languages you want.\nPaper: <LINK>\nCode: <LINK>\nCool new 0-shot learning work by @fb_vinid, @debora_nozza of @MilaNLProc w/ @TerragniSilvia and @FersiniE #NLProc <LINK>']",https://arxiv.org/abs/2004.07737,"Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. ",Cross-lingual Contextualized Topic Models with Zero-shot Learning
99,1251038089549508608,1155943178710634497,Dong-Ho Lee,"['How can we obtain supervision for NER in a cost-effective way? In our new #acl2020nlp paper (w/ @billyuchenlin @xiangrenNLP ), We propose “entity triggers”, a proxy of human explanations, and ""Trigger Matching Network”, which makes effective use of it. <LINK> <LINK>']",https://arxiv.org/abs/2004.07493,"Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce ""entity triggers,"" an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences. ","TriggerNER: Learning with Entity Triggers as Explanations for Named
  Entity Recognition"
100,1251033365236649984,547776192,Chris Lovell,"['dropping on the arXiv today, our new paper presenting FLARES (First Light And Reionisation Epoch Simulations) (sorry)\n\n<LINK> <LINK>', 'You can find details on the project, as well as more visualisations and data products, at our dedicated website:\n\nhttps://t.co/eNyEQdo3CM https://t.co/HD9FrBprj5']",https://arxiv.org/abs/2004.07283,"We introduce the First Light And Reionisation Epoch Simulations (FLARES), a suite of zoom simulations using the EAGLE model. We resimulate a range of overdensities during the Epoch of Reionisation (EoR) in order to build composite distribution functions, as well as explore the environmental dependence of galaxy formation and evolution during this critical period of galaxy assembly. The regions are selected from a large $(3.2 \;\mathrm{cGpc})^{3}$ parent volume, based on their overdensity within a sphere of radius $14\,h^{-1}\;\mathrm{cMpc}$. We then resimulate with full hydrodynamics, and employ a novel weighting scheme that allows the construction of composite distribution functions that are representative of the full parent volume. This significantly extends the dynamic range compared to smaller volume periodic simulations. We present an analysis of the galaxy stellar mass function (GSMF), the star formation rate distribution function (SFRF) and the star forming sequence (SFS) predicted by \flares, and compare to a number of observational and model constraints. We also analyse the environmental dependence over an unprecedented range of overdensity. Both the GSMF and the SFRF exhibit a clear double-Schechter form, up to the highest redshifts ($z = 10$). We also find no environmental dependence of the SFS normalisation. The increased dynamic range probed by FLARES will allow us to make predictions for a number of large area surveys that will probe the EoR in coming years, such as WFIRST and Euclid. ","First Light And Reionisation Epoch Simulations (FLARES) I: Environmental
  Dependence of High-Redshift Galaxy Evolution"
101,1250998764388614144,4883662141,Roy Schwartz,"['Do we really need to run the most expensive models on all instances? In a new #acl2020nlp paper (w/ @GabiStanovsky @swabhz @JesseDodge @nlpnoah) we propose a method to dynamically exit BERT early &amp; fast w/o losing much performance <LINK> <LINK> <LINK>', '@ctongfei @OfirPress @GabiStanovsky @swabhz @JesseDodge @nlpnoah Thanks for the pointer, we will check it out!']",http://arxiv.org/abs/2004.07453,"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) ""exit"" from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code. ",The Right Tool for the Job: Matching Model and Instance Complexities
102,1250804931021492229,1099694742596608005,Will Hipson,"[""New preprint! @SaifMMohammad and I introduce PoKi, a dataset of ~ 62,000 POems by KIds. We analyzed PoKi to detect developmental trends in emotion language. We've also made PoKi freely available for research.\n\nPaper: <LINK>\nGithub: <LINK>""]",https://arxiv.org/abs/2004.06188,"Child language studies are crucial in improving our understanding of child well-being; especially in determining the factors that impact happiness, the sources of anxiety, techniques of emotion regulation, and the mechanisms to cope with stress. However, much of this research is stymied by the lack of availability of large child-written texts. We present a new corpus of child-written text, PoKi, which includes about 62 thousand poems written by children from grades 1 to 12. PoKi is especially useful in studying child language because it comes with information about the age of the child authors (their grade). We analyze the words in PoKi along several emotion dimensions (valence, arousal, dominance) and discrete emotions (anger, fear, sadness, joy). We use non-parametric regressions to model developmental differences from early childhood to late-adolescence. Results show decreases in valence that are especially pronounced during mid-adolescence, while arousal and dominance peaked during adolescence. Gender differences in the developmental trajectory of emotions are also observed. Our results support and extend the current state of emotion development research. ",PoKi: A Large Dataset of Poems by Children
103,1250786272043393026,332196424,JB Rubinovitz,"['New paper I was a part of, ""Toward Trustworthy AI Development:\nMechanisms for Supporting Verifiable Claims”, is out today. Major props to @Miles_Brundage, @GretchenMarina and anyone I missed who helped facilitate this cross-institution collaboration\n <LINK>']",https://arxiv.org/abs/2004.07213,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms. ","Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable
  Claims"
104,1250744045149859841,765357414,Thomas Edwards,"[""New paper <LINK> with @horngsheng! We tried to understand whether signals from general binary systems could be missed in the GW search pipelines. Turns out it's possible! Good news is that we can search in the data from @LIGO and @ego_virgo""]",https://arxiv.org/abs/2004.06729,"We study whether binary black hole template banks can be used to search for the gravitational waves emitted by general binary coalescences. To recover binary signals from noisy data, matched-filtering techniques are typically required. This is especially true for low-mass systems, with total mass $M \lesssim 10 \, M_\odot$, which can inspiral in the LIGO and Virgo frequency bands for thousands of cycles. In this paper, we focus on the detectability of low-mass binary systems whose individual components can have large spin-induced quadrupole moments and small compactness. The quadrupole contributes to the phase evolution of the waveform whereas the compactness affects the merger frequency of the binary. We find that binary black hole templates (with dimensionless quadrupole $\kappa=1$) cannot be reliably used to search for objects with large quadrupoles ($\kappa\gtrsim 20$) over a wide range of parameter space. This is especially true if the general object is highly spinning and has a larger mass than its binary companion. A binary that consists of objects with small compactness could merge in the LIGO and Virgo frequency bands, thereby reducing its accumulated signal-to-noise ratio during the inspiraling regime. Template banks which include these more general waveforms must therefore be constructed. These extended banks would allow us to realistically search for the existence of new astrophysical and beyond the Standard Model compact objects. ",Searching for General Binary Inspirals with Gravitational Waves
105,1250728036426752000,538507797,Luigi Alfonsi,"[""I started a new nLab page about Double Copy theory:\n\n<LINK>\n\nfollowing my collaboration in today's paper [<LINK>] exploring some geometric aspects of the theory.\n\nAnyone more expert in the field can contribute!""]",http://arxiv.org/abs/2004.07181,"The Kerr-Schild double copy relates exact solutions of gauge and gravity theories. In all previous examples, the gravity solution is associated with an abelian-like gauge theory object, which linearises the Yang-Mills equations. This appears to be at odds with the double copy for scattering amplitudes, in which the non-abelian nature of the gauge theory plays a crucial role. Furthermore, it is not yet clear whether or not global properties of classical fields - such as non-trivial topology - can be matched between gauge and gravity theories. In this paper, we clarify these issues by explicitly demonstrating how magnetic monopoles associated with arbitrary gauge groups can be double copied to the same solution (the pure NUT metric) in gravity. We further describe how to match up topological information on both sides of the double copy correspondence, independently of the nature of the gauge group. This information is neatly expressed in terms of Wilson line operators, and we argue through specific examples that they provide a useful bridge between the classical double copy and the BCJ double copy for scattering amplitudes. ",Topology and Wilson lines: global aspects of the double copy
106,1250702034153676800,1146521605457272832,Jorinde van de Vis,['New paper with Felix Giese and Thomas Konstandin!\nWe show how to compute the fraction of energy going into gravitational waves in a cosmological phase transition for any new physics model - an essential quantity for producing LISA sensitivity curves. \n\n<LINK>'],https://arxiv.org/abs/2004.06995,"We study the energy budget of a first-order cosmological phase transition, which is an important factor in the prediction of the resulting gravitational wave spectrum. Formerly, this analysis was based mostly on simplified models as for example the bag equation of state. Here, we present a model-independent approach that is exact up to the temperature dependence of the speed of sound in the broken phase. We find that the only relevant quantities that enter in the hydrodynamic analysis are the speed of sound in the broken phase and a linear combination of the energy and pressure differences between the two phases which we call pseudotrace (normalized to the enthalpy in the broken phase). The pseudotrace quantifies the strength of the phase transition and yields the conventional trace of the energy-momentum tensor for a relativistic plasma (with speed of sound squared of one third). We study this approach in several realistic models of the phase transition and also provide a code snippet that can be used to determine the efficiency coefficient for a given phase transition strength and speed of sound. It turns out that our approach is accurate to the percent level for moderately strong phase transitions, while former approaches give at best the right order of magnitude. ","Model-independent energy budget of cosmological first-order phase
  transitions"
107,1250689426482741249,526115229,Kevin Heng,"['Very proud of this new paper led by one of my #ERC postdocs, Pierre Auclair-Desrotour, who came to me from Jacques Laskar. It studies atmospheric collapses on rocky exoplanets. Pierre was able to advance the state of the art in ways I had not anticipated.\n\n<LINK>']",https://arxiv.org/abs/2004.07134,"Over large timescales, a terrestrial planet may be driven towards spin-orbit synchronous rotation by tidal forces. In this particular configuration, the planet exhibits permanent dayside and nightside, which may induce strong day-night temperature gradients. The nightside temperature depends on the efficiency of the day-night heat redistribution and determines the stability of the atmosphere against collapse. To better constrain the atmospheric stability, climate, and surface conditions of rocky planets located in the habitable zone of their host star, it is thus crucial to understand the complex mechanism of heat redistribution. Building on early works and assuming dry thermodynamics, we developed a hierarchy of analytic models taking into account the coupling between radiative transfer, dayside convection, and large-scale atmospheric circulation in the case of slowly rotating planets. There are two types of these models: a zero-dimensional two-layer approach and a two-column radiative-convective-subsiding-upwelling (RCSU) model. They yield analytical solutions and scaling laws characterising the dependence of the collapse pressure on physical features, which are compared to the results obtained by early works using 3D global climate models (GCMs). The analytical theory captures (i) the dependence of temperatures on atmospheric opacities and scattering in the shortwave and in the longwave, (ii) the behaviour of the collapse pressure observed in GCM simulations at low stellar fluxes that are due to the non-linear dependence of the atmospheric opacity on the longwave optical depth at the planet's surface, (iii) the increase of stability generated by dayside sensible heating, and (iv) the decrease of stability induced by the increase of the planet size. ",Atmospheric stability and collapse on tidally locked rocky planets
108,1250598901591109632,15861003,Jon McCormack,"['The @Evostar2020 conference is currently underway (virtually). I have a new #CreativeAI paper with Andy Lomas on ""Understanding Aesthetic Evaluation using Deep Learning"" that has been nominated for best paper. You can download a preprint here: <LINK>', 'And also another paper with @SimonGColton and colleagues on ""Adapting and Enhancing Evolutionary Art for Casual Creation"" https://t.co/pkcdKDXd9G']",https://arxiv.org/abs/2004.06874,"A bottleneck in any evolutionary art system is aesthetic evaluation. Many different methods have been proposed to automate the evaluation of aesthetics, including measures of symmetry, coherence, complexity, contrast and grouping. The interactive genetic algorithm (IGA) relies on human-in-the-loop, subjective evaluation of aesthetics, but limits possibilities for large search due to user fatigue and small population sizes. In this paper we look at how recent advances in deep learning can assist in automating personal aesthetic judgement. Using a leading artist's computer art dataset, we use dimensionality reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in any generative system. Convolutional Neural Networks trained on the user's prior aesthetic evaluations are used to suggest new possibilities similar or between known high quality genotype-phenotype mappings. ",Understanding Aesthetic Evaluation using Deep Learning
109,1250443231965450240,177416255,Daniel Litt,"['New paper (joint with Dean Bisogno, Wanlin Li, and Padma Srinivasan): <LINK>! This came out of a recent @amermathsoc MRC program. <LINK>', 'The paper is a pretty fun mix (IMO) of ideas from low-dimensional topology and arithmetic, and it ends with a bit of a mystery. Namely, we produce (what we think is) the first known example of a non-hyperelliptic curve whose Ceresa cycle vanishes under the l-adic Abel Jacobi map.', ""It's the Hurwitz curve of genus 7, also known as the Fricke-Macbeath curve; a picture of it, drawn by Klein, is below. It's not too hard to see that the Ceresa cycle also has torsion image under the Hodge-theoretic Abel-Jacobi map. https://t.co/TcSSr0lvQ9"", ""So here's the mystery -- is this cycle actually algebraically or rationally equivalent to zero? This seems pretty hard! Let me know if you have any thoughts, or would like more details on how to make the question precise.""]",https://arxiv.org/abs/2004.06146,"Let l be a prime and G a pro-l group with torsion-free abelianization. We produce group-theoretic analogues of the Johnson/Morita cocycle for G -- in the case of surface groups, these cocycles appear to refine existing constructions when l=2. We apply this to the pro-l etale fundamental groups of smooth curves to obtain Galois-cohomological analogues, and discuss their relationship to work of Hain and Matsumoto in the case the curve is proper. We analyze many of the fundamental properties of these classes and use them to give an example of a non-hyperelliptic curve whose Ceresa class has torsion image under the l-adic Abel-Jacobi map. ","Group-theoretic Johnson classes and a non-hyperelliptic curve with
  torsion Ceresa class"
110,1250429495732920326,2850683124,Paul Michel,"['New paper by Keita Kurita, @gneubig and myself accepted at ACL 2020: ""Weight Poisoning Attacks on Pre-trained Models"" in which we uncover a security flaw in the ubiquitous pretrain-distribute-finetune paradigm in NLP\n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'Specifically, it is possible to modify the weights of a pretrained to introduce a backdoor exposed *after* the model has been finetuned on a downstream task. These backdoors enable an attacker to control the output of the model by injecting trigger tokens in the input. https://t.co/7GWRK0xKux', 'We examine two possible attacks: RIPPLe (a poisoning objective inspired by first order meta-learning algorithm) and embedding surgery (a heuristic attack modifying token embeddings directly). https://t.co/ZhfZTtfwZr', 'We find that the poisoned models can be fine-tuned to a level of accuracy on par with the original model, while exhibiting 100% attack success rate (Label Flip Rate, LFR) on sentiment classification, toxicity detection and spam detection. https://t.co/v21o6aqXnY', ""The attack is successful even if the attacker doesn't have access to the same dataset/domain as the one their victim will ultimately fine-tune on, and is somewhat robust to the choice of finetuning hyper-parameters. https://t.co/QDJFSobbUb"", 'This was possible both when using low-frequency sub-words as trigger tokens as well as proper nouns (name of private institutions).', 'We also discuss a ""brute force"" defense to identify candidate trigger tokens by comparing their frequency to their propensity to influence the model\'s predictions. https://t.co/FaBzrBocSp', 'As publicly distributed pretrained models become more common, we hope that this makes clear the necessity for ""sanity-checking"" distributed models: simply fine-tuning is not a valid guarantee.', 'Finally, I and @gneubig would like to thank the first author Keita for his hard work and ingenuity -- he was very much the brain and muscles behind this entire project. This paper is dedicated to him.', ""@kalpeshk2011 @gneubig Thanks, glad you liked it. Re your question: no, we didn't try that but that would be a good potential defense. Possible issues I foresee with this are"", ""@kalpeshk2011 @gneubig (1) in order to overwrite the embeddings of low-frequency trigger tokens you'd have to train a lot (which somehat defeats the purpose of using a pre-trained model in the first place)"", '@kalpeshk2011 @gneubig (2) This is the classical chicken and egg scenario of computer security: an attacker might be able to account for this in their poisoning objective. But this is definitely an interesting avenue for future work', '@Wjrgo @gneubig Good question. My hunch is that it would definitively help, particularly with the embedding surgery side of things (since the poisoning behaviour localized in the modified trigger embeddings is unlikely to transfer without explicitly training on examples containing the triggers)', '@TalSchuster @gneubig Thank you for the heads up. This does seem very relevant, I will add it to our related work']",https://arxiv.org/abs/2004.06660,"Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at this https URL ",Weight Poisoning Attacks on Pre-trained Models
111,1250242797455593472,739505640326987777,Guido Roberts-Borsani,"['New paper out, accepted for pub. in MNRAS! <LINK>\n\n""Observational Constraints on the Multiphase Nature of Outflows Using Large Spectroscopic Surveys at z∼0"" \n\nHere I aim to place constraints on the multiphase nature of outflows in normal, star-forming galaxies.', 'Selecting samples of high mass, star-forming galaxies from the MaNGA, xCOLD GASS, xGASS and ALFALFA surveys, I employ stacking techniques of NaD, CO, HI and H-alpha tracers to determine a total mass outflow rate and assess each phase’s relative contributions.', 'The molecular gas is found to likely contribute the highest fractions (~70%), with neutral gas coming second (~30%) and ionised gas contributing negligible amounts. Such a trend is compared with and observed for other galaxies in the literature.', 'Such analyses suggest that single gas phases are likely to severely underestimate the total mass outflow rate, and  that a degree of quenching from ejective feedback could be present in normal galaxies, even in the absence of an AGN.']",https://arxiv.org/abs/2004.06116,"Mass outflow rates and loading factors are typically used to infer the quenching potential of galactic-scale outflows. However, these generally rely on observations of a single gas phase which can severely underestimate the total ejected gas mass. To address this, we use observations of high mass ($\geqslant$10$^{10}$ M$_{\odot}$), normal star-forming galaxies at $z\sim$0 from the MaNGA, xCOLD GASS, xGASS and ALFALFA surveys and a stacking of NaD, H$\alpha$, CO(1-0) and HI 21cm tracers with the aim of placing constraints on an average, total mass outflow rate and loading factor. We find detections of outflows in both neutral and ionised gas tracers, with no detections in stacks of molecular or atomic gas emission. Modelling of the outflow components reveals velocities of $|$v$_{\text{NaD}}|$=131 km s$^{-1}$ and $|$v$_{\text{H}\alpha}|$=439 km s$^{-1}$ and outflow rates of $\dot{M}_{\text{NaD}}$=7.55 M$_{\odot}$yr$^{-1}$ and $\dot{M}_{\text{H}\alpha}$=0.10 M$_{\odot}$yr$^{-1}$ for neutral and ionised gas, respectively. Assuming a molecular/atomic outflow velocity of 200 km s$^{-1}$, we derive upper limits of $\dot{M}_{\text{CO}}<$19.43 M$_{\odot}$yr$^{-1}$ and $\dot{M}_{\text{HI}}<$26.72 M$_{\odot}$yr$^{-1}$ for the molecular and atomic gas, respectively. Combining the detections and upper limits, we find average total outflow rates of $\dot{M}_{\text{tot}}\lesssim$27 M$_{\odot}$yr$^{-1}$ and a loading factor of $\eta_{\text{tot}}\lesssim$6.39, with molecular gas likely contributing $\lesssim$72% of the total mass outflow rate, and neutral and ionised gas contributing $\sim$28% and $<$1%, respectively. Our results suggest that, to first order, a degree of quenching via ejective feedback could occur in normal galaxies when considering all gas phases, even in the absence of an AGN. ","Observational Constraints on the Multiphase Nature of Outflows Using
  Large Spectroscopic Surveys at $z\sim$0"
112,1250237846692208642,76331020,Caius Selhorst,"['Our  new paper (with @pjasimoes, @cassioleandro, @AlineVidotto and A. Valio)  has been accepted for publication in ApJ!! <LINK>: Planetary transits at radio wavelengths: secondary eclipses of hot Jupiter extended atmospheres']",https://arxiv.org/abs/2004.06528,"When a planet transits in front of its host star, a fraction of its light is blocked, decreasing the observed flux from the star. The same is expected to occur when observing the stellar radio flux. However, at radio wavelengths, the planet also radiates, depending on its temperature, and thus modifies the transit depths. We explore this scenario simulating the radio lightcurves of transits of hot-Jupiters, Kepler-17b and WASP-12b, around solar-like stars. We calculated the bremsstrahlung radio emission at 17, 100, and 400 GHz originated from the star, considering a solar atmospheric model. The planetary radio emission was calculated modelling the planets in two scenarios: as a blackbody or with a dense and hot extended atmosphere. In both cases the planet radiates and contributes to the total radio flux. For a blackbody planet, the transit depth is in the order of 2-4% and it is independent of the radio frequency. Hot-Jupiters planets with atmospheres appear bigger and brighter in radio, thus having a larger contribution to the total flux of the system. Therefore, the transit depths are larger than in the case of blackbody planets, reaching up to 8% at 17 GHz. Also the transit depth is frequency-dependent. Moreover, the transit caused by the planet passing behind the star is deeper than when the planet transits in front of the star, being as large as 18% at 400GHz. In all cases, the contribution of the planetary radio emission to the observed flux is evident when the planet transits behind the star. ","Planetary transits at radio wavelengths: secondary eclipses of hot
  Jupiter extended atmospheres"
113,1250105363304787970,1204845487263752198,Ning Yu,"['New paper on improving minority mode coverage of StyleGAN2: we combine GAN and IMLE objectives to get the best of both worlds. Joint work with @KL_Div , Peng Zhou, Jitendra Malik, Larry Davis, and Mario Fritz. More details in <LINK>. Code and models coming soon. <LINK>']",https://arxiv.org/abs/2004.03355,"Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub. ",Inclusive GAN: Improving Data and Minority Coverage in Generative Models
114,1250088960396648449,1131272636321845248,Sohtaro Kanda,['We have submitted a new article to arXiv. <LINK> This paper describes new microwave spectroscopy of the hyperfine structure in muonium atom under a near-zero magnetic field to test quantum electrodynamics theory and search for physics beyond the standard model.'],https://arxiv.org/abs/2004.05862,"A hydrogen-like atom consisting of a positive muon and an electron is known as muonium. It is a near-ideal two-body system for a precision test of bound-state theory and fundamental symmetries. The MuSEUM collaboration performed a new precision measurement of the muonium ground-state hyperfine structure at J-PARC using a high-intensity pulsed muon beam and a high-rate capable positron counter. The resonance of hyperfine transition was successfully observed at a near-zero magnetic field, and the muonium hyperfine structure interval of ${\nu}_{\text{HFS}}$ = 4.463302(4) GHz was obtained with a relative precision of 0.9 ppm. The result was consistent with the previous ones obtained at Los Alamos National Laboratory and the current theoretical calculation. We present a demonstration of the microwave spectroscopy of muonium for future experiments to achieve the highest precision. ","New precise spectroscopy of the hyperfine structure in muonium with a
  high-intensity pulsed muon beam"
115,1250075982817452032,68538286,Dan Hendrycks,"[""Does BERT break down in new test distributions, or does it generalize? In our #ACL2020 paper, we show it's surprisingly robust. It 1) greatly reduces accuracy declines and 2) often knows when it doesn't know the answer.\n\n<LINK>\n\nw/ @Eric_Wallace_ @dawnsongtweets <LINK>"", '@KarimiRabeeh @Eric_Wallace_ @dawnsongtweets Yes, in our paper we show the robustness declines depend on the task and how datasets are split. This is why we evaluate with seven datasets (~20 splits), so we can give a broad account of OOD generalization.']",http://arxiv.org/abs/2004.06100,"Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness. ",Pretrained Transformers Improve Out-of-Distribution Robustness
116,1250024587003338753,946726588200218624,Laurent Bétermin,"['New paper on @arxiv titled ""On the optimality of the rock-salt structure among lattices with charge distributions"", written with Markus Faulhuber @univienna @RWTH and Hans Knüpfer @UniHeidelberg. See <LINK> <LINK>']",https://arxiv.org/abs/2004.04553,"The goal of this work is to investigate the optimality of the $d$-dimensional rock-salt structure, i.e., the cubic lattice $V^{1/d}\mathbb{Z}^d$ of volume $V$ with an alternation of charges $\pm 1$ at lattice points, among periodic distribution of charges and lattice structures. We assume that the charges are interacting through two types of radially symmetric interaction potentials, according to their signs. We first restrict our study to the class of orthorhombic lattices. We prove that, for our energy model, the $d$-dimensional rock-salt structure is always a critical point among periodic structures of fixed density. This holds for a large class of potentials. We then investigate the minimization problem among orthorhombic lattices with an alternation of charges for inverse power laws and Gaussian interaction potentials. High density minimality results and low-density non-optimality results are derived for both types of potentials. Numerically, we investigate several particular cases in dimensions $2$, $3$ and $8$. The numerics support the conjecture that the rock-salt structure is the global optimum among all lattices and periodic charges, satisfying some natural constraints. For $d=2$, we observe a phase transition of the type 'triangular-rhombic-square-rectangular' for the minimizer's shape as the density decreases. ","On the optimality of the rock-salt structure among lattices with charge
  distributions"
117,1249971905630789632,864804348613844992,Tony Bagnall,"['New paper on HIVE-COTE, release v1.0, now much faster, contractable, tunable and more usable but still competitive in accuracy <LINK>']",https://arxiv.org/abs/2004.06069,"The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. Since it was first proposed in 2016, the algorithm has undergone some minor changes and there is now a configurable, scalable and easy to use version available in two open source repositories. We present an overview of the latest stable HIVE-COTE, version 1.0, and describe how it differs to the original. We provide a walkthrough guide of how to use the classifier, and conduct extensive experimental evaluation of its predictive performance and resource usage. We compare the performance of HIVE-COTE to three recently proposed algorithms. ","A tale of two toolkits, report the third: on the usage and performance
  of HIVE-COTE v1.0"
118,1249954624276226049,1249097324556627973,Dvir Samuel,"['Happy to share our new paper ""Long-tail learning with attributes"" - joint work with @AtzmonYuval and @GalChechik.\nWe introduce DRAGON - the first modular architecture for long-tail (unbalanced data) learning with semantic attributes. <LINK> (1/2)', 'DRAGON fuses visual and textual information while correcting for a ""familiarity bias"" that models have towards head classes. It outperforms long-tail learning models on unbalanced data and introduces new SoTA results on benchmarks (GFSL with attributes and long-tailed CIFAR)(2/2)']",https://arxiv.org/abs/2004.02235,"Real-world data is predominantly unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. Often, classes can be accompanied by side information like textual descriptions, but it is not fully clear how to use them for learning with unbalanced long-tail data. Such descriptions have been mostly used in (Generalized) Zero-shot learning (ZSL), suggesting that ZSL with class descriptions may also be useful for long-tail distributions. We describe DRAGON, a late-fusion architecture for long-tail learning with class descriptors. It learns to (1) correct the bias towards head classes on a sample-by-sample basis; and (2) fuse information from class-descriptions to improve the tail-class accuracy. We also introduce new benchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with class-descriptions, building on existing learning-with-attributes datasets and a version of Imagenet-LT with class descriptors. DRAGON outperforms state-of-the-art models on the new benchmark. It is also a new SoTA on existing benchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only) long-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365. ",From Generalized zero-shot learning to long-tail with class descriptors
119,1249919234127400961,1176867972163559424,MartinHuber,['In our new paper (joint with D Imhof and H Wallimann) we suggest a #machinelearning approach for detecting partial #collusion and incomplete #cartels using data from the Swiss road construction sector:\n<LINK>\n#EconTwitter #IndustrialOrganization #DataScience'],https://arxiv.org/abs/2004.05629,"We propose a new method for flagging bid rigging, which is particularly useful for detecting incomplete bid-rigging cartels. Our approach combines screens, i.e. statistics derived from the distribution of bids in a tender, with machine learning to predict the probability of collusion. As a methodological innovation, we calculate such screens for all possible subgroups of three or four bids within a tender and use summary statistics like the mean, median, maximum, and minimum of each screen as predictors in the machine learning algorithm. This approach tackles the issue that competitive bids in incomplete cartels distort the statistical signals produced by bid rigging. We demonstrate that our algorithm outperforms previously suggested methods in applications to incomplete cartels based on empirical data from Switzerland. ",A Machine Learning Approach for Flagging Incomplete Bid-rigging Cartels
120,1249742288571400197,15254510,Tobias Weyand,"[""Excited to announce our CVPR'20 paper on the Google Landmarks Dataset v2: <LINK>\n\nGLDv2 is a new benchmark for image retrieval and instance-level object recognition with 5M images and over 200k classes and a special focus on application-relevant challenges. <LINK>""]",https://arxiv.org/abs/2004.01804,"While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at this https URL ","Google Landmarks Dataset v2 -- A Large-Scale Benchmark for
  Instance-Level Recognition and Retrieval"
121,1249636945719951360,797888987675365377,Tom Rainforth,"['Our new paper <LINK> shows how we can do blind contact tracing at scale for COVID-19 *without* requiring people to install an app or giving governments access to tracking data.  All credit goes to my collaborators @jk_fitzsimons @atulmantri91 @jansen_zhao and Rob <LINK>', 'With @oblivious_AI we will shortly be deploying this in the wild in a city of 5mil+, shout @jk_fitzsimons for more details']",https://arxiv.org/abs/2004.05116,"The current COVID-19 pandemic highlights the utility of contact tracing, when combined with case isolation and social distancing, as an important tool for mitigating the spread of a disease [1]. Contact tracing provides a mechanism of identifying individuals with a high likelihood of previous exposure to a contagious disease, allowing additional precautions to be put in place to prevent continued transmission. Here we consider a cryptographic approach to contact tracing based on secure two-party computation (2PC). We begin by considering the problem of comparing a set of location histories held by two parties to determine whether they have come within some threshold distance while at the same time maintaining the privacy of the location histories. We propose a solution to this problem using pre-shared keys, adapted from an equality testing protocol due to Ishai et al [2]. We discuss how this protocol can be used to maintain privacy within practical contact tracing scenarios, including both app-based approaches and approaches which leverage location history held by telecoms and internet service providers. We examine the efficiency of this approach and show that existing infrastructure is sufficient to support anonymised contact tracing at a national level. ","A note on blind contact tracing at scale with applications to the
  COVID-19 pandemic"
122,1249597906551791618,157973000,Michael Pfarrhofer,"['New working paper online (@arxiv) on forecasting with Bayesian VARs under real time conditions vs. using pseudo out-of-sample simulations to evaluate predictions, with US &amp; euro area applications #econometrics #forecasting\n<LINK>', 'Econometrics: Bayesian vector autoregressions (VARs) of different sizes with constant or time-varying parameters (TVPs) and stochastic volatilities (SVs). Shrinkage via a global- local (horseshoe) prior. Some models are augmented with factors from high-dimensional datasets.', 'Data: Large-scale real time datasets; FRED-MD (@stlouisfed) and EA-RTD (@ecb), monthly vintages starting in late 1998 and early 2001, respectively.', 'Findings: (1) differences in relative ordering of model performance for point and density forecasts (real time/pseudo out-of-sample evaluation); (2) severity of this problem differs between the US/EA, and over time; (3) unsurprisingly, forecasting is harder in real time.']",https://arxiv.org/abs/2004.04984,"This paper investigates the sensitivity of forecast performance measures to taking a real time versus pseudo out-of-sample perspective. We use monthly vintages for the United States (US) and the Euro Area (EA) and estimate a set of vector autoregressive (VAR) models of different sizes with constant and time-varying parameters (TVPs) and stochastic volatility (SV). Our results suggest differences in the relative ordering of model performance for point and density forecasts depending on whether real time data or truncated final vintages in pseudo out-of-sample simulations are used for evaluating forecasts. No clearly superior specification for the US or the EA across variable types and forecast horizons can be identified, although larger models featuring TVPs appear to be affected the least by missing values and data revisions. We identify substantial differences in performance metrics with respect to whether forecasts are produced for the US or the EA. ","Forecasts with Bayesian vector autoregressions under real time
  conditions"
123,1249504000979931140,15327263,Carl-Johan Haster,"['New paper led by @sylvia_bisco (together with me, @sasomao and Jonathan Davies who visited @MITKavli from @ImperialPhysics last summer).\nWe look at assumptions of known noise behaviour in GW data, and present a method to account for these uncertainties.\n<LINK>']",https://arxiv.org/abs/2004.05149,"In order to perform Bayesian parameter estimation to infer the source properties of gravitational waves from compact binary coalescences (CBCs), the noise characteristics of the detector must be understood. It is typically assumed that the detector noise is stationary and Gaussian, characterized by a power spectral density (PSD) that is measured with infinite precision. We present a new method to incorporate the uncertainty in the power spectral density estimation into the Bayesian inference of the binary source parameters and apply it to the first 11 CBC detections reported by the LIGO- Virgo Collaboration. We find that incorporating the PSD uncertainty only leads to variations in the positions and widths of the binary parameter posteriors on the order of a few percent. Our results are publicly available for download on git [1]. ","Quantifying the Effect of Power Spectral Density Uncertainty on
  Gravitational-Wave Parameter Estimation for Compact Binary Sources"
124,1249498499235729409,20174338,Daniel Cotton,"[""I'd like to announce the publication of our new paper on the polarization of white dwarf G29-38: <LINK> with @JeremyBailey5, Jim Pringle, Bill Sparks and @astrojonty."", 'G29-38 has an infrared excess which is indicative of dust around the white dwarf. This is often thought of as the remnants of a destroyed planetary system. Learning about it can potentially tell us more about planetary systems.', 'The dust is probably in the shape of a disk around the white dwarf. If so we find that the disk is near face-on or the Bond albedo is small similar to other debris disks. The Bond albedo is basically the fraction of light scattered back from it, and is material dependant.', 'This is the first polarisation measurement of this type of object. It is a difficult measurement and required HIPPI-2 and 4 &amp; 8-m telescopes. In fact, one TAC rejected a proposal to do this as unfeasible. But we did it anyway when we got extra time when another instrument failed.', 'and Ted von Hippel! \n\n(Really need an edit button.)', ""@JontiHorner I honestly don't have time. My current job is as a technician at the AAT, so I'm not paid for science. All my spare time is used for my science output (4 papers so far this year). I'm working on something big though. I will try and write something overarching on polarimetry then."", '@JontiHorner Thanks by the way. (Maybe @astrojonty wants to write something.)']",https://arxiv.org/abs/2004.04952,"We have made high precision polarimetric observations of the polluted white dwarf G29-38 with the HIgh Precision Polarimetric Instrument 2. The observations were made at two different observatories -- using the 8.1-m Gemini North Telescope and the 3.9-m Anglo AustralianTelescope -- and are consistent with each other. After allowing for a small amount of interstellar polarization, the intrinsic linear polarization of the system is found to be 275.3 +/- 31.9 parts-per-million at a position angle of 90.8 +/- 3.8 degrees in the SDSS g' band. We compare the observed polarization with the predictions of circumstellar disc models. The measured polarization is small in the context of the models we develop which only allows us to place limits on disc inclination and Bond albedo for optically thin disc geometries. In this case either the inclination is near face-on or the albedo is small -- likely in the range 0.05 to 0.15 -- which is in line with other debris disc measurements. A preliminary search for the effects of G29-38's pulsations in the polarization signal produced inconsistent results. This may be caused by beating effects, indicate a clumpy dust distribution, or be a consequence of measurement systematics. ",Polarization measurements of the polluted white dwarf G29-38
125,1249006606270050311,4903660834,Joseph Vandehey,['New paper up on the arXiv: <LINK>\nThis is a big generalization of the work of Kamae and Weiss on normal numbers to the world of semigroups.'],https://arxiv.org/abs/2004.02811,"A classical Kamae-Weiss theorem states that an increasing sequence $(n_i)_{i\in\mathbb N}$ of positive lower density is \emph{normality preserving}, i.e. has the property that for any normal binary sequence $(b_n)_{n\in\mathbb N}$, the sequence $(b_{n_i})_{i\in\mathbb N}$ is normal, if and only if $(n_i)_{i\in\mathbb N}$ is a deterministic sequence. Given a countable cancellative amenable semigroup $G$, and a F\o lner sequence $\mathcal F=(F_n)_{n\in\mathbb N}$ in $G$, we introduce the notions of normality preservation, determinism and subexponential complexity for subsets of $G$ with respect to $\mathcal F$, and show that for sets of positive lower $\mathcal F$-density these three notions are equivalent. The proof utilizes the apparatus of the theory of tilings of amenable groups and the notion of tile-entropy. We also prove that under a natural assumption on $\mathcal F$, positive lower $\mathcal F$-density follows from normality preservation. Finally, we provide numerous examples of normality preserving sets in various semigroups ","Deterministic functions on amenable semigroups and a generalization of
  the Kamae-Weiss theorem on normality preservation"
126,1248599767619420160,892059194240532480,Mikel Artetxe,"['Check out our new paper on ""Translation Artifacts in Cross-lingual Transfer Learning"" (w/ @glabaka &amp; @eagirre)\n\nWe show that translation can alter spurious patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning\n\n<LINK>']",https://arxiv.org/abs/2004.04721,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively. ",Translation Artifacts in Cross-lingual Transfer Learning
127,1248599744747798528,1068545181576773632,Kenneth Brown,"['New paper on a weighted Union-Find decoder for the toric code with @Huang_Shilin and Mike Newman (<LINK>). #DukeQuantum @DukeEngineering', 'A weighted Union-Find decoder increases the threshold for the Union-Find decoder but at what cost? In previous work with @Huang_Shilin (https://t.co/rdqpuzwKfa) on 2D compass codes, we predicted it should be quadratically slower.', 'In this paper, we show that with truncated weights that the speed is comparable to unweighted Union-Find with negligible loss in performance.', 'The truncated version can be readily implemented using a microarchitecture based on https://t.co/P0WpSu6FZx by @krystasvore @mointweets, @nico_delf and co-workers.']",http://arxiv.org/abs/2004.04693,"Quantum error correction requires decoders that are both accurate and efficient. To this end, union-find decoding has emerged as a promising candidate for error correction on the surface code. In this work, we benchmark a weighted variant of the union-find decoder on the toric code under circuit-level depolarizing noise. This variant preserves the almost-linear time complexity of the original while significantly increasing the performance in the fault-tolerance setting. In this noise model, weighting the union-find decoder increases the threshold from 0.38% to 0.62%, compared to an increase from 0.65% to 0.72% when weighting a matching decoder. Further assuming quantum non-demolition measurements, weighted union-find decoding achieves a threshold of 0.76% compared to the 0.90% threshold when matching. We additionally provide comparisons of timing as well as low error rate behavior. ",Fault-Tolerant Weighted Union-Find Decoding on the Toric Code
128,1248598759350599682,1068545181576773632,Kenneth Brown,"['New paper on QCCD ion trap architectures for near-term devices (<LINK>).\nA great collaboration with Prakash Murali (@MartonosiGroup) , @margmartonosi, &amp; @DriptoDebroy (@DukePhysics). Paper accepted at @ISCAConfOrg.  #DukeQuantum @DukeEngineering @Princeton', 'In the ion trap community there is a bit of a divide between what is the right size of an ion chain.  On the one hand, small ion chains have been shown to have incredible fidelities. On the other hand, long ion chains allow you to grow Hilbert space more easily.', 'There are two main camps: (1) shuttling with 2-4 ion chains is the best and (2) the longest chain that works is the best.  Here we study what is the best ion chain length in an architecture that has long chains and shuttling. It turns out to depend on the application.', 'The collaboration between Duke and Princeton made possible by @NSF @EPiQCExpedition.']",http://arxiv.org/abs/2004.04706,"Trapped ions (TI) are a leading candidate for building Noisy Intermediate-Scale Quantum (NISQ) hardware. TI qubits have fundamental advantages over other technologies such as superconducting qubits, including high qubit quality, coherence and connectivity. However, current TI systems are small in size, with 5-20 qubits and typically use a single trap architecture which has fundamental scalability limitations. To progress towards the next major milestone of 50-100 qubits, a modular architecture termed the Quantum Charge Coupled Device (QCCD) has been proposed. In a QCCD-based TI device, small traps are connected through ion shuttling. While the basic hardware components for such devices have been demonstrated, building a 50-100 qubit system is challenging because of a wide range of design possibilities for trap sizing, communication topology and gate implementations and the need to match diverse application resource requirements. Towards realizing QCCD systems with 50-100 qubits, we perform an extensive architectural study evaluating the key design choices of trap sizing, communication topology and operation implementation methods. We built a design toolflow which takes a QCCD architecture's parameters as input, along with a set of applications and realistic hardware performance models. Our toolflow maps the applications onto the target device and simulates their execution to compute metrics such as application run time, reliability and device noise rates. Using six applications and several hardware design points, we show that trap sizing and communication topology choices can impact application reliability by up to three orders of magnitude. Microarchitectural gate implementation choices influence reliability by another order of magnitude. From these studies, we provide concrete recommendations to tune these choices to achieve highly reliable and performant application executions. ",Architecting Noisy Intermediate-Scale Trapped Ion Quantum Computers
129,1248502690126020611,1020088099,Umberto Picchini,"['new paper: ""Adaptive MCMC for synthetic likelihoods and correlated synthetic likelihoods"", with Umberto Simola and Jukka Corander <LINK> Proposes a sequentially updated MCMC kernel, partly inspired by the SNL method of @gpapamak @DavidSterratt  and @driainmurray <LINK>']",https://arxiv.org/abs/2004.04558,"Synthetic likelihood (SL) is a strategy for parameter inference when the likelihood function is analytically or computationally intractable. In SL, the likelihood function of the data is replaced by a multivariate Gaussian density over summary statistics of the data. SL requires simulation of many replicate datasets at every parameter value considered by a sampling algorithm, such as Markov chain Monte Carlo (MCMC), making the method computationally-intensive. We propose two strategies to alleviate the computational burden. First, we introduce an algorithm producing a proposal distribution that is sequentially tuned and made conditional to data, thus it rapidly \textit{guides} the proposed parameters towards high posterior density regions. In our experiments, a small number of iterations of our algorithm is enough to rapidly locate high density regions, which we use to initialize one or several chains that make use of off-the-shelf adaptive MCMC methods. Our ""guided"" approach can also be potentially used with MCMC samplers for approximate Bayesian computation (ABC). Second, we exploit strategies borrowed from the correlated pseudo-marginal MCMC literature, to improve the chains mixing in a SL framework. Moreover, our methods enable inference for challenging case studies, when the posterior is multimodal and when the chain is initialised in low posterior probability regions of the parameter space, where standard samplers failed. To illustrate the advantages stemming from our framework we consider five benchmark examples, including estimation of parameters for a cosmological model and a stochastic model with highly non-Gaussian summary statistics. ","Sequentially guided MCMC proposals for synthetic likelihoods and
  correlated synthetic likelihoods"
130,1248484478550302721,1115880604560691200,NII Yamagishi Lab,"['Preprint of our new paper submitted to Interspeech, ""Noise Tokens: Learning Neural Noise Templates for Environment-Aware Speech Enhancement,"" is now online!\npaper: <LINK>\naudio samples: <LINK>']",https://arxiv.org/abs/2004.04001,"In recent years, speech enhancement (SE) has achieved impressive progress with the success of deep neural networks (DNNs). However, the DNN approach usually fails to generalize well to unseen environmental noise that is not included in the training. To address this problem, we propose ""noise tokens"" (NTs), which are a set of neural noise templates that are jointly trained with the SE system. NTs dynamically capture the environment variability and thus enable the DNN model to handle various environments to produce STFT magnitude with higher quality. Experimental results show that using NTs is an effective strategy that consistently improves the generalization ability of SE systems across different DNN architectures. Furthermore, we investigate applying a state-of-the-art neural vocoder to generate waveform instead of traditional inverse STFT (ISTFT). Subjective listening tests show the residual noise can be significantly suppressed through mel-spectrogram correction and vocoder-based waveform synthesis. ","Noise Tokens: Learning Neural Noise Templates for Environment-Aware
  Speech Enhancement"
131,1248457029083004928,1146409804610584577,Alexander Jahn,"['Our new paper explores how hyperbolic tensor networks naturally produce quasiregular CFTs, and how this relates to QEC and strong disorder. Joint work with Zoltán Zimborás and @jenseisert! <LINK>']",https://arxiv.org/abs/2004.04173,"The study of critical quantum many-body systems through conformal field theory (CFT) is one of the pillars of modern quantum physics. Certain CFTs are also understood to be dual to higher-dimensional theories of gravity via the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. To reproduce various features of AdS/CFT, a large number of discrete models based on tensor networks have been proposed. Some recent models, most notably including toy models of holographic quantum error correction, are constructed on regular time-slice discretizations of AdS. In this work, we show that the symmetries of these models are well suited for approximating CFT states, as their geometry enforces a discrete subgroup of conformal symmetries. Based on these symmetries, we introduce the notion of a quasiperiodic conformal field theory (qCFT), a critical theory less restrictive than a full CFT and with characteristic multi-scale quasiperiodicity. We discuss holographic code states and their renormalization group flow as specific implementations of a qCFT with fractional central charges and argue that their behavior generalizes to a large class of existing and future models. Beyond approximating CFT properties, we show that these can be best understood as belonging to a paradigm of discrete holography. ",Tensor network models of AdS/qCFT
132,1248445548601933825,34078684,Dipanjan Das,"['New #acl2020nlp paper on automatic evaluation of text generation models with @ThiboIbo and @ank_parikh, where we introduce a “learned” metric called BLEURT.  Preprint is available here: <LINK> 1/5 <LINK>', 'The recipe behind BLEURT is straightforward:  you take a pretrained BERT checkpoint, tune it on synthetically generated sentence pairs with automatic scores, then further tune on human ratings of pairs of NLG system outputs and human-written references. 2/5', 'We achieve the best results on the last three years of the WMT metrics shared task, and on the WebNLG Challenge dataset 3/5', 'The tuning of BLEURT on synthetic pairs is crucial:  it introduces robustness to quality drifts of generation systems, where over time, the metric may be tested on better and better systems.  The plot below shows its robustness compared to BLEU and BERTScore 4/5 https://t.co/EYuaI6LxGR', 'In the language research group @GoogleAI, we are doing fundamental research on NLG, including building benchmarks, improving generation models, as well as evaluation.  This paper falls under the last bucket.  Stay tuned for more papers on NLG in the near future! 5/5']",https://arxiv.org/abs/2004.04696,"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution. ",BLEURT: Learning Robust Metrics for Text Generation
133,1248411057426542592,850356925535531009,Mor Geva,"['Numerical reasoning skills are difficult to learn from a LM objective. In our new paper, we show how to inject the skills into pre-trained LMs, such that numerical computations are performed internally by the model.\n<LINK>\n@ankgup2 @JonathanBerant']",https://arxiv.org/abs/2004.04487,"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation. ",Injecting Numerical Reasoning Skills into Language Models
134,1248283584915439616,1033698977248698370,Silvia Terragni,['Curious to know how to combine topic models and BERT pre-trained representations? Check out our new paper with @fb_vinid and @dirk_hovy! Paper: <LINK> \nCode: <LINK>'],https://arxiv.org/abs/2004.03974,"Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models. ","Pre-training is a Hot Topic: Contextualized Document Embeddings Improve
  Topic Coherence"
135,1248280487547801601,2332157006,Federico Bianchi,['Our new paper is finally online (with @TerragniSilvia and @dirk_hovy). Increase the topic coherence of your neural variational topic models using pre-trained representations from BERT! \n\nPaper: <LINK> \nPython-package: <LINK>'],https://arxiv.org/abs/2004.03974,"Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models. ","Pre-training is a Hot Topic: Contextualized Document Embeddings Improve
  Topic Coherence"
136,1248264740058435585,17108894,Rowan Zellers,"['In our new paper, ""Evaluating Machines by their Real-World Language Use"" we present a new framework, dataset, &amp; leaderboard for just that - narrowing the gap between NLP benchmarks and human language use. Even today\'s biggest transformers struggle. (1/3)\n\n<LINK> <LINK>', ""Given a written situation, a machine must generate advice - that's at least as helpful to the situation-writer as human advice. Despite lots of finetuning data, today's largest models show core reading comprehension/NLI/commonsense errors, and much room for progress! (2/3)"", 'This was joint work with @universeinanegg @eaclark07 @Lianhuiq Ali Farhadi and @YejinChoinka, at @uwcse and @allen_ai. Demo and project page at https://t.co/6gmpM7Sb9W (3/3)', ""@semanticbeeng I'm aiming to release code+model checkpoints ASAP, probably sometime next week :)"", '@colinraffel good question! From what I\'ve seen the answer is ""no"" -- the automated metrics measure text similarity, but text similarity isn\'t a good measure of whether the advice is helpful (or more broadly whether a language user is successful at their objective)', ""@colinraffel I think perplexity is the most useful out of these metrics, which is why we validated hyperparameters using it, but to me it's problematic because (1) it's also a 'correctness' objective and (2) it's hard to compare perplexity between different models""]",http://arxiv.org/abs/2004.03607,"We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress. ",TuringAdvice: A Generative and Dynamic Evaluation of Language Use
137,1248232346785988608,29682697,Robin Scheibler,"['We just released a new preprint about MM algorithms for joint independent subspace analysis (JISA-MM), that is blind source separation where some sources are correlated. We apply it to blind audio source(s) extraction. Paper: <LINK> Code: <LINK> <LINK>']",https://arxiv.org/abs/2004.03926,"In this work, we propose efficient algorithms for joint independent subspace analysis (JISA), an extension of independent component analysis that deals with parallel mixtures, where not all the components are independent. We derive an algorithmic framework for JISA based on the majorization-minimization (MM) optimization technique (JISA-MM). We use a well-known inequality for super-Gaussian sources to derive a surrogate function of the negative log-likelihood of the observed data. The minimization of this surrogate function leads to a variant of the hybrid exact-approximate diagonalization problem, but where multiple demixing vectors are grouped together. In the spirit of auxiliary function based independent vector analysis (AuxIVA), we propose several updates that can be applied alternately to one, or jointly to two, groups of demixing vectors. Recently, blind extraction of one or more sources has gained interest as a reasonable way of exploiting larger microphone arrays to achieve better separation. In particular, several MM algorithms have been proposed for overdetermined IVA (OverIVA). By applying JISA-MM, we are not only able to rederive these in a general manner, but also find several new algorithms. We run extensive numerical experiments to evaluate their performance, and compare it to that of full separation with AuxIVA. We find that algorithms using pairwise updates of two sources, or of one source and the background have the fastest convergence, and are able to separate target sources quickly and precisely from the background. In addition, we characterize the performance of all algorithms under a large number of noise, reverberation, and background mismatch conditions. ","MM Algorithms for Joint Independent Subspace Analysis with Application
  to Blind Single and Multi-Source Extraction"
138,1248187297905991682,2770204252,Michael Gygli,"['New short paper on arXiv. We explore if we can get networks trained on different tasks to produce compatible features, enabling to mix and match network components. Short answer: Yes, we can <LINK> <LINK>']",https://arxiv.org/abs/2004.03898,"This paper proposes to make a first step towards compatible and hence reusable network components. Rather than training networks for different tasks independently, we adapt the training process to produce network components that are compatible across tasks. In particular, we split a network into two components, a features extractor and a target task head, and propose various approaches to accomplish compatibility between them. We systematically analyse these approaches on the task of image classification on standard datasets. We demonstrate that we can produce components which are directly compatible without any fine-tuning or compromising accuracy on the original tasks. Afterwards, we demonstrate the use of compatible components on three applications: Unsupervised domain adaptation, transferring classifiers across feature extractors with different architectures, and increasing the computational efficiency of transfer learning. ","Towards Reusable Network Components by Learning Compatible
  Representations"
139,1248165573852348416,1237350319891374081,Dimitris Papoulias,"['New paper today!\n\nUsing nuclear structure calculations, we present the expected WIMP-nucleus and neutrino-nucleus event rates for both elastic and inelastic scattering.\n\nlink: <LINK>\nR. Sahu, D.K. Papoulias, V.K.B. Kota, T.S. Kosmas <LINK>']",https://arxiv.org/abs/2004.04055,"The event rates for WIMP-nucleus and neutrino-nucleus scattering processes, expected to be detected in ton-scale rare-event detectors, are investigated. We focus on nuclear isotopes that correspond to the target nuclei of current and future experiments looking for WIMP- and neutrino-nucleus events. The nuclear structure calculations, performed in the context of the deformed shell model, are based on Hartree-Fock intrinsic states with angular momentum projection and band mixing for both the elastic and the inelastic channels. Our predictions in the high-recoil-energy tail show that detectable distortions of the measured/expected signal may be interpreted through the inclusion of the non-negligible incoherent channels ","Elastic and inelastic scattering of neutrinos and weakly interacting
  massive particles on nuclei"
140,1248118873016037376,47851397,Tessa Fisher,"['Hey, remember that project I got interviewed about by New Scientist about two years back? The paper on it is finally published!\n\n<LINK>']",https://arxiv.org/abs/2004.03631,"The search for life on exoplanets is one of the grand scientific challenges of our time. The strategy to date has been to find (e.g., through transit surveys like Kepler) Earth-like exoplanets in their stars habitable zone, then use transmission spectroscopy to measure biosignature gases, especially oxygen, in the planets atmospheres (e.g., using JWST, the James Webb Space Telescope). Already there are more such planets than can be observed by JWST, and missions like the Transiting Exoplanet Survey Satellite and others will find more. A better understanding of the geochemical cycles relevant to biosignature gases is needed, to prioritize targets for costly follow-up observations and to help design future missions. We define a Detectability Index to quantify the likelihood that a biosignature gas could be assigned a biological vs. non-biological origin. We apply this index to the case of oxygen gas, O2, on Earth-like planets with varying water contents. We demonstrate that on Earth-like exoplanets with 0.2 weight percent (wt%) water (i.e., no exposed continents) a reduced flux of bioessential phosphorus limits the export of photosynthetically produced atmospheric O2 to levels indistinguishable from geophysical production by photolysis of water plus hydrogen escape. Higher water contents >1wt% that lead to high-pressure ice mantles further slow phosphorus cycling. Paradoxically, the maximum water content allowing use of O2 as a biosignature, 0.2wt%, is consistent with no water based on mass and radius. Thus, the utility of an O2 biosignature likely requires the direct detection of both water and land on a planet. ",Detectability of Life Using Oxygen on Pelagic Planets and Water Worlds
141,1248049713149906945,759894532649545732,Aravind Srinivas,"['New paper - CURL: Contrastive Unsupervised Representations for RL! We use the simplest form of contrastive learning (instance-based) as an auxiliary task in model-free RL. SoTA by *significant* margin on DMControl and Atari for data-efficiency.  <LINK> <LINK>', 'Highlights:\nSolves most of DMControl envs from pixels within 100K timesteps.\nLearning from pixels nearly matches learning from physical state for the first time\nSoTA on every single DMControl environment and 10x more data-efficient than previous SoTA by Dreamer https://t.co/4M9f9XJrbS', 'Highlights (cont):\nAtari100K timesteps: Competitive with SimPLE without learning any world model\nSoTA median human normalized score for 100K timesteps \nSoTA on 14/26 games for 100k timesteps https://t.co/5HQyqrdLhI', 'Method:\nTake any model-free RL algorithm (example, SAC or Rainbow). Create two augmentations (views) of your input. Add instance contrastive loss in the latent space. Optimize both the RL and contrastive loss. That’s it. https://t.co/X9pN0F6eEr', 'Contrastive Learning (CPCv2, MoCo, SimCLR) helps in data-efficiency on downstream vision tasks. Data-efficiency is particularly useful in RL. @maxjaderberg proposed UNREAL to improve sample-efficiency with reconstruction tasks. Turns out Contrastive &gt;&gt; Reconstruction.', 'Implementation is extremely simple (follows MoCo / SimCLR style instance contrastive learning with InfoNCE loss) https://t.co/JPLMlOsgUc', 'We use random crop as data-augmentation for contrastive learning (highly effective) https://t.co/ObpCTHjxU8', 'We outperform model-based variants in both continuous and discrete settings by a significant margin. Code has been released by @MishaLaskin (joint first author) here: https://t.co/i0hGmvgnjJ', 'Instance Contrastive Self-Supervised Learning (Siamese Networks with InfoNCE loss) is simple yet powerful and this result in RL adds on to previous successes in computer vision by MoCo and SimCLR.', 'Thanks to my collaborators @MishaLaskin and @pabbeel', '@pabbeel @MishaLaskin @ylecun We have an exact LeCake ablation where encoder only learns from Siamese and no RL gradient (green) (It almost works as well as using both RL  and contrastive gradients (red)): https://t.co/SI8aque2O5']",https://arxiv.org/abs/2004.04136,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL ","CURL: Contrastive Unsupervised Representations for Reinforcement
  Learning"
142,1247903624694337536,2427184074,Christopher Berry,"['Fun new paper led by @ScooperF1 @annacgreen &amp; @hannahmidd8 on our mini-Michelson interferometer, currently living in @thinktankmuseum \n<LINK> <LINK>', 'How did we build a gravitational-wave exhibit for the @thinktankmuseum? @annacgreen explained in this @LIGO Magazine article https://t.co/WhbfhlWco4 #scicomm https://t.co/olMdMkLGtD', 'Our mini-Michelson was designed so it could be part of a stand-alone museum exhibit, as well as part of a science fair exhibit. We used it as part of the @royalsociety summer exhibition @Listen2Universe \nhttps://t.co/0ml5G4pSw5', 'If you would like to play with lasers and build your own mini-@LIGO demonstrator, we have a parts list online https://t.co/DKMG3BQObY and building instructions will be coming soon!']",https://arxiv.org/abs/2004.03052,"In 2015 the first observation of gravitational waves marked a breakthrough in astrophysics, and in technological research and development. The discovery of a gravitational-wave signal from the collision of two black holes, a billion light-years away, received considerable interest from the media and public. We describe the development of a purpose-built exhibit explaining this new area of research to a general audience. The core element of the exhibit is a working Michelson interferometer: a scaled-down version of the key technology used in gravitational-wave detectors. The Michelson interferometer is integrated into a hands-on exhibit, which allows for user interaction and simulated gravitational-wave observations. An interactive display provides a self-guided explanation of gravitational-wave-related topics through video, animation, images and text. We detail the hardware and software used to create the exhibit and discuss two installation variants: an independent learning experience in a museum setting (the Thinktank Birmingham Science Museum), and a science-festival with the presence of expert guides (the 2017 Royal Society Summer Science Exhibition). We assess audience reception in these two settings, describe the improvements we have made given this information, and discuss future public-engagement projects resulting from this work. The exhibit is found to be effective in communicating the new and unfamiliar field of gravitational-wave research to general audiences. An accompanying website provides parts lists and information for others to build their own version of this exhibit. ",An Interactive Gravitational-Wave Detector Model for Museums and Fairs
143,1247891046370922499,990346198908223488,Nikolai Matni,"[""Really excited about our new paper on learning control barrier functions from expert demonstrations! Take a look if you're interested in provably safe data-driven control of nonlinear systems: <LINK> <LINK>"", 'With @HaiminHu']",https://arxiv.org/abs/2004.03315,"Inspired by the success of imitation and inverse reinforcement learning in replicating expert behavior through optimal control, we propose a learning based approach to safe controller synthesis based on control barrier functions (CBFs). We consider the setting of a known nonlinear control affine dynamical system and assume that we have access to safe trajectories generated by an expert - a practical example of such a setting would be a kinematic model of a self-driving vehicle with safe trajectories (e.g., trajectories that avoid collisions with obstacles in the environment) generated by a human driver. We then propose and analyze an optimization-based approach to learning a CBF that enjoys provable safety guarantees under suitable Lipschitz smoothness assumptions on the underlying dynamical system. A strength of our approach is that it is agnostic to the parameterization used to represent the CBF, assuming only that the Lipschitz constant of such functions can be efficiently bounded. Furthermore, if the CBF parameterization is convex, then under mild assumptions, so is our learning process. We end with extensive numerical evaluations of our results on both planar and realistic examples, using both random feature and deep neural network parameterizations of the CBF. To the best of our knowledge, these are the first results that learn provably safe control barrier functions from data. ",Learning Control Barrier Functions from Expert Demonstrations
144,1247761390992932867,1115880604560691200,NII Yamagishi Lab,"['Preprint of our new paper submitted to Interspeech, ""Using Cyclic Noise as the Source Signal for Neural Source-Filter-based Speech Waveform Model,"" is now online!\npaper:  <LINK>\nsource code and samples:  <LINK>']",https://arxiv.org/abs/2004.02191,"Neural source-filter (NSF) waveform models generate speech waveforms by morphing sine-based source signals through dilated convolution in the time domain. Although the sine-based source signals help the NSF models to produce voiced sounds with specified pitch, the sine shape may constrain the generated waveform when the target voiced sounds are less periodic. In this paper, we propose a more flexible source signal called cyclic noise, a quasi-periodic noise sequence given by the convolution of a pulse train and a static random noise with a trainable decaying rate that controls the signal shape. We further propose a masked spectral loss to guide the NSF models to produce periodic voiced sounds from the cyclic noise-based source signal. Results from a large-scale listening test demonstrated the effectiveness of the cyclic noise and the masked spectral loss on speaker-independent NSF models in copy-synthesis experiments on the CMU ARCTIC database. ","Using Cyclic Noise as the Source Signal for Neural Source-Filter-based
  Speech Waveform Model"
145,1247702763049533442,1184838562677710848,Stefano Martiniani,"['Can we quantify correlation lengths when we don\'t know the origin of the order? Check out our new paper\n\n""Correlation lengths in the language of computable information"" with Yuval Lemberg, Paul M. Chaikin and Dov Levine (<LINK>, 5 pages, 2 equations)\n\nThread. <LINK>', 'The standard method for computing the correlation length ξ of a system is to calculate some correlation function, typically 2-point, and see how it decays with distance. This presupposes that the order and proper correlation function is known.', 'In this paper, we propose a method that does not require this knowledge, which is based on the fundamental idea that correlations reduce the information content of a system.', 'We thus exploit the intimate connection between order and information: it takes less information to completely describe a system with correlations than an uncorrelated one.', 'We sample a system on various length scales ∆ by culling out degrees of freedom on smaller scales. Image compression then provides a means of estimating the information content - the Computable Information Density (CID) - of the system.', 'If ∆ &lt; ξ, the remaining degrees of freedom still show correlations, albeit weakened, but if ∆ &gt; ξ, all correlations are lost and the information content attains its maximal value. See results for a 1D Ising model. Q is computed from the CID, see Eq. 1. https://t.co/5BBWItlEOn', 'The procedure readily generalizes to higher-dimensional systems, for instance the 2D Ising model (left). The effect of decimation can be visualized with an Image Pyramid (right), a well-known concept in machine vision. https://t.co/mTJELnKDZn', '""sure, but for these models we can get the correlation length from g(r)"". True, so we ""cloak"" an Ising configuration using the Rudin-Shapiro (RS) sequence, a zero-entropy sequence that destroys all 2-point correlations, so S(q) = 1 but the information content (CID) is preserved https://t.co/Ny0r4nxMkw', 'cf. with multiplication by a random sequence that destroys all information about the original configuration. In the figure below we show that for equilibrium and nonequilibrium models we can recover the correlation length ξ and it scaling even when g(r)=0 everywhere. https://t.co/MIzdybTCje', 'Collaboration between @UMNCSE  @NYUMRSEC @TechnionLive']",http://arxiv.org/abs/2004.03502,"Computable Information Density (CID), the ratio of the length of a losslessly compressed data file to that of the uncompressed file, is a measure of order and correlation in both equilibrium and nonequilibrium systems. Here we show that correlation lengths can be obtained by decimation - thinning a configuration by sampling data at increasing intervals and recalculating the CID. When the sampling interval is larger than the system's correlation length, the data becomes incompressible. The correlation length and its critical exponents are thus accessible with no a-priori knowledge of an order parameter or even the nature of the ordering. The correlation length measured in this way agrees well with that computed from the decay of two-point correlation functions $g_{2}(r)$ when they exist. But the CID reveals the correlation length and its scaling even when $g_{2}(r)$ has no structure, as we demonstrate by ""cloaking"" the data with a Rudin-Shapiro sequence. ",Correlation lengths in the language of computable information
146,1247697430285447168,1324039292,Hanxiao Liu,"['New paper: Evolving Normalization-Activation Layers.\n\nWe use evolution to design new layers called EvoNorms, which outperform BatchNorm-ReLU on many tasks. A promising use of AutoML to discover fundamental ML building blocks.\n\n<LINK>\n\nJoint work with @DeepMind <LINK>', ""Key ideas: (1) to start from low-level primitives, and (2) to evolve the layers' generalization over multiple architectures. EvoNorms achieved promising results on ResNets, MobileNets, EfficientNets, Mask R-CNN and BigGAN. Pseudocode available in the appendix. https://t.co/W2jwvKJwoQ""]",http://arxiv.org/abs/2004.02967,"Normalization layers and activation functions are fundamental components in deep networks and typically co-locate with each other. Here we propose to design them using an automated approach. Instead of designing them separately, we unify them into a single tensor-to-tensor computation graph, and evolve its structure starting from basic mathematical functions. Examples of such mathematical functions are addition, multiplication and statistical moments. The use of low-level mathematical functions, in contrast to the use of high-level modules in mainstream NAS, leads to a highly sparse and large search space which can be challenging for search methods. To address the challenge, we develop efficient rejection protocols to quickly filter out candidate layers that do not work well. We also use multi-objective evolution to optimize each layer's performance across many architectures to prevent overfitting. Our method leads to the discovery of EvoNorms, a set of new normalization-activation layers with novel, and sometimes surprising structures that go beyond existing design patterns. For example, some EvoNorms do not assume that normalization and activation functions must be applied sequentially, nor need to center the feature maps, nor require explicit activation functions. Our experiments show that EvoNorms work well on image classification models including ResNets, MobileNets and EfficientNets but also transfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers in many cases. ",Evolving Normalization-Activation Layers
147,1247686880314834944,293552287,Hoan Tran,"['Our new paper ""Topological Persistence Machine of Phase Transitions"" appeared (with @unlimitcycle).\n<LINK>\n\nTopological persistence machine to construct the shape of physical states; hence decipher phase transitions via the qualitative changes of the shape. <LINK>']",https://arxiv.org/abs/2004.03169,"The study of phase transitions using data-driven approaches is challenging, especially when little prior knowledge of the system is available. Topological data analysis is an emerging framework for characterizing the shape of data and has recently achieved success in detecting structural transitions in material science, such as the glass--liquid transition. However, data obtained from physical states may not have explicit shapes as structural materials. We thus propose a general framework, termed ""topological persistence machine,"" to construct the shape of data from correlations in states, so that we can subsequently decipher phase transitions via qualitative changes in the shape. Our framework enables an effective and unified approach in phase transition analysis. We demonstrate the efficacy of the approach in detecting the Berezinskii--Kosterlitz--Thouless phase transition in the classical XY model and quantum phase transitions in the transverse Ising and Bose--Hubbard models. Interestingly, while these phase transitions have proven to be notoriously difficult to analyze using traditional methods, they can be characterized through our framework without requiring prior knowledge of the phases. Our approach is thus expected to be widely applicable and will provide practical insights for exploring the phases of experimental physical systems. ",Topological Persistence Machine of Phase Transitions
148,1247553547907915779,1240603790,Victor Fragoso 💻,"['New report! I teamed up with Microsoft researchers to create a new fast &amp; accurate pose-and-scale solver; useful for self-driving cars, robots, &amp; AR/VR devices.\n\nSee our #CVPR2020 paper for how we exploit scale &amp; gravity priors to improve localization:\n👉 <LINK> <LINK>']",https://arxiv.org/abs/2004.02052,"Many real-world applications in augmented reality (AR), 3D mapping, and robotics require both fast and accurate estimation of camera poses and scales from multiple images captured by multiple cameras or a single moving camera. Achieving high speed and maintaining high accuracy in a pose-and-scale estimator are often conflicting goals. To simultaneously achieve both, we exploit a priori knowledge about the solution space. We present gDLS*, a generalized-camera-model pose-and-scale estimator that utilizes rotation and scale priors. gDLS* allows an application to flexibly weigh the contribution of each prior, which is important since priors often come from noisy sensors. Compared to state-of-the-art generalized-pose-and-scale estimators (e.g., gDLS), our experiments on both synthetic and real data consistently demonstrate that gDLS* accelerates the estimation process and improves scale and pose accuracy. ","gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity
  Priors"
149,1247432320249999360,2742282828,Andreea Font,"['Do you like galaxies? We have constructed a new suite of hydro simulations, called ARTEMIS, to understand their formation. This paper looks at the structure of stellar haloes: <LINK> <LINK>', '@profbradgibson Yes, I had in mind to write to you about that later on today, but you beat me to it :) Thank you!']",https://arxiv.org/abs/2004.01914,"We introduce the ARTEMIS simulations, a new set of 42 zoomed-in, high-resolution (baryon particle mass of ~ 2x10^4 Msun/h), hydrodynamical simulations of galaxies residing in haloes of Milky Way mass, simulated with the EAGLE galaxy formation code with re-calibrated stellar feedback. In this study, we analyse the structure of stellar haloes, specifically the mass density, surface brightness, metallicity, colour and age radial profiles, finding generally very good agreement with recent observations of local galaxies. The stellar density profiles are well fitted by broken power laws, with inner slopes of ~ -3, outer slopes of ~ -4 and break radii that are typically ~ 20-40 kpc. The break radii generally mark the transition between in situ formation and accretion-driven formation of the halo. The metallicity, colour and age profiles show mild large-scale gradients, particularly when spherically-averaged or viewed along the major axes. Along the minor axes, however, the profiles are nearly flat, in agreement with observations. Overall, the structural properties can be understood by two factors: that in situ stars dominate the inner regions and that they reside in a spatially-flattened distribution that is aligned with the disc. Observations targeting both the major and minor axes of galaxies are thus required to obtain a complete picture of stellar haloes. ",The ARTEMIS simulations: stellar haloes of Milky Way-mass galaxies
150,1247432011846991872,546765150,Alessandro Pluchino,['Our new paper about COVID-19 is online!!  <LINK>'],https://arxiv.org/abs/2004.02739,"We propose a novel data-driven framework for assessing the a-priori epidemic risk of a geographical area and for identifying high-risk areas within a country. Our risk index is evaluated as a function of three different components: the hazard of the disease, the exposure of the area and the vulnerability of its inhabitants. As an application, we discuss the case of COVID-19 outbreak in Italy. We characterize each of the twenty Italian regions by using available historical data on air pollution, human mobility, winter temperature, housing concentration, health care density, population size and age. We find that the epidemic risk is higher in some of the Northern regions with respect to Central and Southern Italy. The corresponding risk index shows correlations with the available official data on the number of infected individuals, patients in intensive care and deceased patients, and can help explaining why regions such as Lombardia, Emilia-Romagna, Piemonte and Veneto have suffered much more than the rest of the country. Although the COVID-19 outbreak started in both North (Lombardia) and Central Italy (Lazio) almost at the same time, when the first cases were officially certified at the beginning of 2020, the disease has spread faster and with heavier consequences in regions with higher epidemic risk. Our framework can be extended and tested on other epidemic data, such as those on seasonal flu, and applied to other countries. We also present a policy model connected with our methodology, which might help policy-makers to take informed decisions. ",A Novel Methodology for Epidemic Risk Assessment of COVID-19 outbreak
151,1247416544977661953,2617406750,Oier Lpz de Lacalle,"['“Evaluating Multimodal Representations on Visual Semantic Textual Similarity”. New paper at #ecai2020. We present a new language&amp;vision task and dataset for textual inference. #arXiv👉<LINK> #dataset 👉 <LINK> <LINK>', 'Experiments reveal, for the first time, the addition of images produce better textual inference, compared to text-only representations. \n\nJoint work with: Ander Salaberria(@AnderSala), Gorka Azkune (@gazkune), Aitor Soroa (@Aitor57 ) and Eneko Agirre (@eagirre)']",http://arxiv.org/abs/2004.01894,"The combination of visual and textual representations has produced excellent results in tasks such as image captioning and visual question answering, but the inference capabilities of multimodal representations are largely untested. In the case of textual representations, inference tasks such as Textual Entailment and Semantic Textual Similarity have been often used to benchmark the quality of textual representations. The long term goal of our research is to devise multimodal representation techniques that improve current inference capabilities. We thus present a novel task, Visual Semantic Textual Similarity (vSTS), where such inference ability can be tested directly. Given two items comprised each by an image and its accompanying caption, vSTS systems need to assess the degree to which the captions in context are semantically equivalent to each other. Our experiments using simple multimodal representations show that the addition of image representations produces better inference, compared to text-only representations. The improvement is observed both when directly computing the similarity between the representations of the two items, and when learning a siamese network based on vSTS training data. Our work shows, for the first time, the successful contribution of visual information to textual inference, with ample room for benchmarking more complex multimodal representation options. ","Evaluating Multimodal Representations on Visual Semantic Textual
  Similarity"
152,1247369972948430848,288623330,Aaron Hertzmann,"['Check out our new paper on discovering of controls for making images with GANs, without any supervision (other than optionally naming the controls afterward), by Erik Härkönen, with @jaakkolehtinen, @sylvain_paris, and myself.\n<LINK> <LINK>', 'The basic idea is to use PCA to identify the important modes of variation that have been learned by a GAN. For such a simple algorithm, it’s surprisingly effective. Restricting controls to modify only a few network layers makes them even more useful. https://t.co/BgpgAcyPsj', 'Our methods give you a way to explore the types of variations that your GAN has learned. Pushing the sliders far past their normal ranges can make some intriguingly-unrealistic images. https://t.co/DL93xN7P38', 'We also show a very simple way to add StyleGAN-like style mixing to BigGAN. https://t.co/3JbBHKm14S', 'The applications to owl customization are endless. https://t.co/eACEg3kpLm', '@c_dan4th I was, of course, thinking the exact same thing :)']",https://arxiv.org/abs/2004.02546,"This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches. ",GANSpace: Discovering Interpretable GAN Controls
153,1247218480685907969,795096464,Ariah Klages-Mundt,"['New paper, new tools for modeling #stablecoins and understanding and preventing crises. Probabilistic guarantees on stability and characterization of instability. To mitigate crises: USDC is one solution, we present another\n\n<LINK>\n\n<LINK>', ""TL;DR A seeming contradiction arises in decentralized stablecoins: while the goal is to make #noncustodial assets, these can only be fully stabilized by adding uncorrelated assets, which are currently centralized/custodial (e.g., @MakerDAO's inclusion of #USDC) 1/"", 'Applies more broadly to synthetic and #crosschain assets. We develop an alternative market-based mechanism to enhance stability in crises while remaining non-custodial 2/', 'This creates a buffer that separates those who are willing to have stablecoins swapped to custodial assets (in return for ongoing yield from option buyers) in a crisis from those who require full decentralization 3/', ""And btw I am especially curious if you're a CDP/vault holder and would use the insurance we describe, or if you're someone who would realistically use risk tools based on these methods 4/"", ""@dizhel Good question: it's a different idea. Those top up your collateral to avoid liquidation, but you're putting more at risk. Here you bound how much it costs to repurchase Dai to directly lower your Dai debt. And it can indeed be built on top of Dai :)"", ""@PrimozKordez @opyn_ Thanks! Potentially, though it's different from what they build now. Yes, vaults would have to pay the option fee, but they wouldn't need to lock up the capital to fulfill the option unless they want to exercise 1/3"", ""@PrimozKordez @opyn_ Correct, you could swap to USDC collateral in the current Maker formulation (if the debt ceiling isn't reached). But you may be able to avoid USDC collateral in the first place with the damping from these options, and so the system may be able to remain non-custodial 2/3"", '@PrimozKordez @opyn_ Also curious if any vaults are interested in something like this! Let me know if you hear anything 3/3', ""@PrimozKordez @opyn_ Know any who actively use CDPs/vaults and who'd be willing to chat?""]",http://arxiv.org/abs/2004.01304,"The `Black Thursday' crisis in cryptocurrency markets demonstrated deleveraging risks in over-collateralized lending and stablecoins. We develop a stochastic model of over-collateralized stablecoins that helps explain such crises. In our model, the stablecoin supply is decided by speculators who optimize the profitability of a leveraged position while incorporating the forward-looking cost of collateral liquidations, which involves the endogenous price of the stablecoin. We formally characterize regimes that are interpreted as stable and unstable for the stablecoin. We prove bounds on the probabilities of large deviations and quadratic variation in the stable domain and distinctly greater price variance in the unstable domain. The unstable domain can be triggered by large deviations, collapsed expectations, or liquidity problems from deleveraging. We formally characterize a deflationary deleveraging spiral as a submartingale that can cause the system to behave in counterintuitive ways due to liquidity problems in a crisis. These deleveraging spirals, which resemble short squeezes, lead to faster collateral drawdown (and potential shortfalls) and are accompanied by higher price variance, as experienced on Black Thursday. We also demonstrate `perfect' stability results in idealized settings and discuss mechanisms which could bring realistic settings closer to such idealized stable settings. ",While Stability Lasts: A Stochastic Model of Stablecoins
154,1247198777624018944,1002606609527443462,Dr. Angela Collier,['Paper day!  Dynamics explaining observed clustering in the outer solar system--removes the need for #planet9 (<LINK>) This is my first paper with my new group AND my first time venturing away from galactic dynamics! Very exciting! #AcademicTwitter <LINK>'],https://arxiv.org/abs/2004.01198,"Disks of low-mass bodies on high-eccentricity orbits in near-Keplerian potentials can be dynamically unstable to buckling out of the plane. In this letter, we present $N$-body simulations of the long-term behavior of such a system, finding apsidal clustering of the orbits in the disk plane. The timescale over which the clustering is maintained increases with number of particles, suggesting that lopsided configurations are stable at large $N$. This discovery may explain the observed apsidal ($\varpi$) clustering of extreme trans-Neptunian Objects in the outer solar system. ",Apsidal Clustering following the Inclination Instability
155,1247187859808927749,1074986430433296384,Anurag Deshpande,"['New paper from me and @tom_kitching today on the #arXiv! This is an extension of my #PhD first paper, now looking at how three approximations in #weaklensing interact, and how they affect upcoming  #cosmology surveys like #Euclid, #LSST, and #WFIRST (1/3)\n\n<LINK> <LINK>', 'In this work, we look at how the corrections for the reduced shear approximation and magnification bias (see https://t.co/6syd2V2xc2) depend on the Limber approximation, which is the assumption that only correlations in the plane of the sky contribute to the lensing signal (2/3)', 'We find that the effect of relaxing the Limber approximation is well below the statistical sample variance of upcoming surveys. So, the reduced shear and magnification corrections can be safely calculated using the Limber approximation for impending surveys! (3/3) https://t.co/8kTU2Z6m6q']",https://arxiv.org/abs/2004.01666,"The significant increase in precision that will be achieved by Stage IV cosmic shear surveys means that several currently used theoretical approximations may cease to be valid. An additional layer of complexity arises from the fact that many of these approximations are interdependent; the procedure to correct for one involves making another. Two such approximations that must be relaxed for upcoming experiments are the reduced shear approximation and the effect of neglecting magnification bias. Accomplishing this involves the calculation of the convergence bispectrum; typically subject to the Limber approximation. In this work, we compute the post-Limber convergence bispectrum, and the post-Limber reduced shear and magnification bias corrections to the angular power spectrum for a Euclid-like survey. We find that the Limber approximation significantly overestimates the bispectrum when any side of the bispectrum triangle, $\ell_i<60$. However, the resulting changes in the reduced shear and magnification bias corrections are well below the sample variance for $\ell\leq5000$. We also compute a worst-case scenario for the additional biases on $w_0w_a$CDM cosmological parameters that result from the difference between the post-Limber and Limber approximated forms of the corrections. These further demonstrate that the reduced shear and magnification bias corrections can safely be treated under the Limber approximation for upcoming surveys. ","Post-Limber Weak Lensing Bispectrum, Reduced Shear Correction, and
  Magnification Bias Correction"
156,1247117394549637120,1201898316595941377,Simone Sturniolo,"['I have a new paper published on arXiv today - so a little thread about what it\'s about, as I think it\'s a neat little problem.\n\n<LINK>\n\n""A Dyson equation approach for averaging of classical and quantum observables on multiple realizations of Markov processes""', ""Sounds like a mouthful, but it's really quite simple. Consider this example. Say you have a bunch of clocks with only one hand. The hand rotates at a certain frequency; it can be either Pi or 3Pi, for example. The frequency for each clock can randomly switch. You want to find..."", ""...the average of all those hands; namely, treat them as vectors, and sum them all over. There's two very simple limit cases. One: they never jump. Then the signal will be simply [cos(Pi*t)+cos(3*Pi*t)]/2. Here illustrated in this graph (green curve is the sum). https://t.co/RX1DI1X8dA"", ""The other easy case is if they're all swapping frequency really really fast. Then they will all behave as if they saw a single, average frequency of 2*Pi. Even simpler. https://t.co/8rv4UF89vu"", ""But here's the problem: what if the frequency of the swapping is on the order of magnitude of the frequencies of rotation? Then it gets tricky, and in general, there's no obvious solution."", 'This is an actual problem that appears a lot for example in Nuclear Magnetic Resonance spectroscopy. Generally, we use approximations to fit experimental signals, also because the noise and randomness seldom make it very useful to model things too accurately.', ""However, it's still interesting to me. In our two frequency case, for example, we'd have a bunch of possible ways in which clocks can switch randomly their frequency. Here illustrated are two such ways - basically glued pieces of cosinusoids - and in green, the overall sum. https://t.co/4ODheSPF8k"", ""How do we get that sum? We average over ALL possible ways in which the phase of each clock can evolve. So all possible histories in terms of which frequency is the rotation frequency at a given time.\n\nIf it sounds like a path integral in QM... it's because it is."", 'Back in 2012 I published a paper with Marco Pieruccini, who I worked with at the time, and who solved this problem *analytically* for a special case of NMR interest, dipoles diffusing in the spherical angle.\n\nhttps://t.co/D8V5AeJmh9', ""At the time, I could barely follow most of the derivation. Now I understand it far better - which led me to an idea, what if I could adapt it to work also in other cases?\n\nTurns out, it's not only possible, it's quite powerful!"", ""The clock problem described above is solved by a (path) integral like this. Thing is, this approach works for a bunch of other problems, especially because the exponential *doesn't have to be of a scalar*. This works for matrix exponentials too! So, propagators, for example. https://t.co/xxliLqWZO7"", ""And that's what's inside the paper above! The method has gone from analytical to computational, and it's really efficient only for dynamics controlled by Markov processes. But I think it works, and describes the clock problem as well as it does loss of coherence in spin systems."", ""I'll now submit the paper for proper peer review, so this is just a peek. I don't know if there's any other useful applications (partition functions, anyone?), or if it overlaps with some other existing approach. But it's a neat trick, and it can be useful to analyse..."", '...NMR and other spectroscopy results in a regime of ""spectral diffusion"", which is what I described above.']",https://arxiv.org/abs/2004.01183,"Time dependent signals in experimental techniques such as Nuclear Magnetic Resonance (NMR) and Muon Spin Relaxation (muSR) are often the result of an ensemble average over many microscopical dynamical processes. While there are a number of functions used to fit these signals, they are often valid only in specific regimes, and almost never properly describe the ""spectral diffusion"" regime, in which the dynamics happen on time scales comparable to the characteristic frequencies of the system. Full treatment of these problems would require one to carry out a path integral over all possible realizations of the dynamics of the time dependent Hamiltonian. In this paper we present a numerical approach that can potentially be used to solve such time evolution problems, and we benchmark it against a Monte Carlo simulations of the same problems. The approach can be used for any sort of dynamics, but is especially powerful for any dynamics that can be approximated as Markov processes, in which the dynamics at each step only depend on the previous state of the system. The approach is used to average both classical and quantum observables; in the latter case, a formalism making use of Liouvillians and density matrices is used. ","A Dyson equation approach for averaging of classical and quantum
  observables on multiple realizations of Markov processes"
157,1247069170740744192,832436423743664128,Nicholas Sedlmayr,"['How to measure a Majorana: The Majorana polarization of a topological planar Josephson junction - New paper on the arXiv: <LINK>', 'Majorana bound states are exotic quasiparticles which can live in some superconducting systems. In fact they are non-abelian anyons, which means that when swapping them around it matters which order you do it on for the result you get. (This is not true of ordinary particles!)', 'There are already several experiments where it seems likely that these quasiparticles have been seen. However this strange swapping behaviour has never been verified.', 'This paper looks at a different way we might be able to directly probe their strange intrinsic nature.']",https://arxiv.org/abs/2004.01420,"We analyze the topological superconductivity and investigate the spectroscopic properties manifested by the zero-energy modes, induced in a metallic strip embedded into a Josephson-type junction. Focusing on the Majorana polarization of such quasiparticles, we propose feasible means for its empirical detection, using the spin-selective Andreev reflection method. Our study reveals a gradual development of a transverse gradient of the Majorana polarization across the metallic strip upon increasing its width. We also inspect the spatial profile and polarization of the Majorana quasiparticles in the presence of a strong electrostatic impurity. We show that, depending on its position, such a defect can lead to a substantial localization of the Majorana mode. ","How to measure the Majorana polarization of a topological planar
  Josephson junction"
158,1247064004431929344,822867138,Bradley Kavanagh,"['""Measuring the local #DarkMatter density in the laboratory"" - <LINK>\n\nMy new paper with @TimonEmken &amp; Riccardo Catena shows that you can measure how many Dark Matter particles there are in the Solar System, if DM interacts strongly with ordinary matter <LINK>', ""The idea is that we could still discover Dark Matter that scatters often with ordinary nuclei or electrons (we may not have seen it with current detectors if it's too light). \n\nIn this scenario, DM can scatter many times before reaching a DM detector:\n\nhttps://t.co/KW1iBXeFyE"", ""The result: the signal you would see in an underground detector varies over the course of a day, as the DM particles pass through more or less of the Earth.\n\nHere is this 'modulation' in the S. Hemisphere for different interaction strengths (https://t.co/HXNoYcqJHu): https://t.co/qijNxDqMRT"", ""A frustrating thing about standard Dark Matter searches is that you generally can't measure both the local density ρ of DM particles *and* how strongly they interact σ.\n\nAre you seeing lots of events because you have lots of DM particles, or because DM interacts very often?"", ""But the size of this 'daily modulation' depends on the cross section σ. So in principle, you could use the total rate in your detector to measure σ x ρ, and use the time variation of the signal to recover σ. \n\nWe suggested this a while back (https://t.co/onctpFt53L): https://t.co/BKeryB1NJ0"", 'Using @TimonEmken\'s DaMaSCUS code (https://t.co/PzEKXUBVi5), we simulated light Dark Matter signals for different σ and asked ""how well can you recover σ and ρ?""\n\nIf you include the timing information, it turns out you could measure ρ to within 20%: https://t.co/XJmrN4skVp', 'So we should keep looking for light Dark Matter particles (with nucleus- and electron-scattering experiments). \n\nWe might find that they interact more strongly than we expected. \n\nAnd if they do, we could learn a huge amount about them!']",https://arxiv.org/abs/2004.01621,"Despite strong evidence for the existence of large amounts of dark matter (DM) in our Universe, there is no direct indication of its presence in our own solar system. All estimates of the local DM density rely on extrapolating results on much larger scales. We demonstrate for the first time the possibility of simultaneously measuring the local DM density and interaction cross-section with a direct detection experiment. It relies on the assumption that incoming DM particles frequently scatter on terrestrial nuclei prior to detection, inducing an additional time-dependence of the signal. We show that for sub-GeV DM, with a large spin-independent DM-proton cross section, future direct detection experiments should be able to reconstruct the local DM density with smaller than 50% uncertainty. ",Measuring the local Dark Matter density in the laboratory
159,1247057699826733056,477030336,Leon Derczynski 🏡🌱,"['""Directions in Abusive Language Training Data"": evidence-drive recommendations for working with abusive language data and building datasets for abusive language detection. New paper w. @bertievidgen. #nlproc #hatespeech\n\n<LINK>']",https://arxiv.org/abs/2004.01670,"Data-driven analysis and detection of abusive online content covers many different tasks, phenomena, contexts, and methodologies. This paper systematically reviews abusive language dataset creation and content in conjunction with an open website for cataloguing abusive language data. This collection of knowledge leads to a synthesis providing evidence-based recommendations for practitioners working with this complex and highly diverse data. ","Directions in Abusive Language Training Data: Garbage In, Garbage Out"
160,1246996356876529664,1115880604560691200,NII Yamagishi Lab,"['Preprint of our new paper submitted to Interspeech, ""iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using Generative Adversarial Network-based Metric Learning,"" is now online!  \nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/2004.00932,"The intelligibility of natural speech is seriously degraded when exposed to adverse noisy environments. In this work, we propose a deep learning-based speech modification method to compensate for the intelligibility loss, with the constraint that the root mean square (RMS) level and duration of the speech signal are maintained before and after modifications. Specifically, we utilize an iMetricGAN approach to optimize the speech intelligibility metrics with generative adversarial networks (GANs). Experimental results show that the proposed iMetricGAN outperforms conventional state-of-the-art algorithms in terms of objective measures, i.e., speech intelligibility in bits (SIIB) and extended short-time objective intelligibility (ESTOI), under a Cafeteria noise condition. In addition, formal listening tests reveal significant intelligibility gains when both noise and reverberation exist. ","iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using
  Generative Adversarial Network-based Metric Learning"
161,1246406840600182786,19730041,☯Evil Jim O'Donnell,['Preprint of a new paper discussing the advantages of a combined machine-learning and crowdsourced approach to detecting gravitational lenses in large surveys. <LINK>'],https://arxiv.org/abs/2004.00634,"Strong lenses are extremely useful probes of the distribution of matter on galaxy and cluster scales at cosmological distances, but are rare and difficult to find. The number of currently known lenses is on the order of 1,000. We wish to use crowdsourcing to carry out a lens search targeting massive galaxies selected from over 442 square degrees of photometric data from the Hyper Suprime-Cam (HSC) survey. We selected a sample of $\sim300,000$ galaxies with photometric redshifts in the range $0.2 < z_{phot} < 1.2$ and photometrically inferred stellar masses $\log{M_*} > 11.2$. We crowdsourced lens finding on this sample of galaxies on the Zooniverse platform, as part of the Space Warps project. The sample was complemented by a large set of simulated lenses and visually selected non-lenses, for training purposes. Nearly 6,000 citizen volunteers participated in the experiment. In parallel, we used YattaLens, an automated lens finding algorithm, to look for lenses in the same sample of galaxies. Based on a statistical analysis of classification data from the volunteers, we selected a sample of the most promising $\sim1,500$ candidates which we then visually inspected: half of them turned out to be possible (grade C) lenses or better. Including lenses found by YattaLens or serendipitously noticed in the discussion section of the Space Warps website, we were able to find 14 definite lenses, 129 probable lenses and 581 possible lenses. YattaLens found half the number of lenses discovered via crowdsourcing. Crowdsourcing is able to produce samples of lens candidates with high completeness and purity, compared to currently available automated algorithms. A hybrid approach, in which the visual inspection of samples of lens candidates pre-selected by discovery algorithms and/or coupled to machine learning is crowdsourced, will be a viable option for lens finding in the 2020s. ","Survey of Gravitationally-lensed Objects in HSC Imaging (SuGOHI). VI.
  Crowdsourced lens finding with Space Warps"
162,1246006174950002689,3000107776,Douglas Boubert,"['I have a new paper out (<LINK>) with @deniserkal and @astroalessia! I\'m just a little bit pleased with the title: ""Deflection of the hypervelocity stars by the dance of the Milky Way and Large Magellanic Cloud"". (1/9)', 'Hypervelocity stars are kicked from the centre of our Galaxy by the supermassive black hole that lives there. They move so quickly that they then escape from our Galaxy along a straight line. But how straight would it really be? (2/9) https://t.co/2sXIS8vzL3', ""This isn't just a rhetorical question. In a perfectly spherical Milky Way hypervelocity stars would escape radially away on straight lines. That means you can use hypervelocity stars to probe the non-sphericity of the Galaxy! (3/9)"", ""We simulated the ejection of hypervelocity stars with multiple different non-spherical perturbers: the Milky Way's disk and bulge, a flattened dark matter halo, or the Large Magellanic Cloud (LMC). (4/9) https://t.co/7hVXtaXNAB"", 'We found that the LMC causes a larger deflection than any non-spherical component of the Galaxy at distances greater than 50 kpc, thus it is crucial to include the LMC when working with hypervelocity stars on the outskirts of the Galaxy. (5/9) https://t.co/FnUHSsTLBp', ""BUT, you can't get away with just adding a fly-by of the LMC - you need to include the Milky Way being pulled around by the LMC! Compare the 3rd and 4th columns of the figure above. You get it more wrong if you ignore this galactic dance than if you'd kept ignoring the LMC. (6/9)"", 'This happens because the hypervelocity stars were ejected from where the Milky Way was at the time they were ejected, but we are measuring deflections relative to where the Milky Way is now! The Milky Way dancing with the LMC changes the geometry. (7/9)', ""Right now we don't have precise enough proper motions to detect any deflection of hypervelocity stars away from straight line trajectories, but we will get them if the proposed Gaia Near Infrared successor mission goes ahead! (8/9) https://t.co/934R5UGAeO"", '""At that point, it would be possible to use the hypervelocity stars’ deflections as an independent probe of the shape of the Milky Way’s dark matter halo, the mass of the Large Magellanic Cloud, and of the dance of these two galaxies about each other."" (9/9)']",https://arxiv.org/abs/2004.00633,"Stars slingshotted by the supermassive black hole at the Galactic centre will escape the Milky Way so quickly that their trajectories will be almost straight lines. Previous works have shown how these `hypervelocity stars' are subsequently deflected by the gravitational field of the Milky Way and the Large Magellanic Cloud (LMC), but have neglected to account for the reflex motion of the Milky Way in response to the fly by of the LMC. A consequence of this motion is that the hypervelocity stars we see on the outskirts of the Milky Way today were ejected from where the Milky Way centre was hundreds of millions of years ago. This change in perspective causes large apparent deflections in the trajectories of the hypervelocity stars, which are of the same order as the deflections caused by the gravitational force of the Milky Way and LMC. We quantify these deflections by simulating the production of hypervelocity stars in an isolated Milky Way (with a spherical or flattened dark matter halo), in a fixed-in-place Milky Way with a passing LMC, and in a Milky Way which responds to the passage of the LMC. The proper motion precision necessary to measure these deflections will be possible with the combination of Gaia with the proposed GaiaNIR successor mission, and these measurements will unlock the hypervelocity stars as probes of the shape of the Milky Way, the mass of the LMC, and of the dance of these two galaxies. ","Deflection of the hypervelocity stars by the dance of the Milky Way and
  Large Magellanic Cloud"
163,1246001047178481664,808313669570428928,P. Kontis,"['New #preprint on spinodal decomposition vs. classical nucleation of γ’ in a #superalloy powder. We did in-situ neutron diffraction and #atomprobe experiments. First paper with @chincp 😁  <LINK>', '@chincp https://t.co/C5lruvHSDQ']",http://arxiv.org/abs/2004.01146,"Contemporary powder-based polycrystalline nickel-base superalloys inherit microstructures and properties that are heavily determined by the thermo-mechanical treatments during processing. Here, the influence of a thermal exposure alone to an alloy powder is studied to elucidate the controlling formation mechanisms of the strengthening precipitates using a combination of atom probe tomography and in-situ neutron diffraction. The initial powder comprised a single-phase supersaturated gamma only; from this, the evolution of gamma-prime volume fraction and lattice misfit was assessed. The initial powder notably possessed elemental segregation of Cr and Co and elemental repulsion between Ni, Al and Ti with Cr; here proposed to be a precursor for subsequent gamma to gamma-prime phase transformations. Subsolvus heat treatments yielded a unimodal gamma-prime distribution, formed during heating, with evidence supporting its formation to be via spinodal decomposition. A supersolvus heat treatment led to the formation of this same gamma-prime population during heating, but dissolves as the temperature increases further. The gamma-prime then reprecipitates as a multimodal population during cooling, here forming by classical nucleation and growth. Atom probe characterisation provided intriguing precipitate characteristics, including clear differences in chemistry and microstructure, depending on whether the gamma-prime formed during heating or cooling. ","Spinodal decomposition versus classical gamma-prime nucleation in a
  nickel-base superalloy powder: An in-situ neutron diffraction and
  atomic-scale analysis"
164,1245939361843118081,1009467515183955968,Jens Zumbrägel,"['Well, at least these silent times make us finishing things. Here is a new paper with T. G. Nam on graphs, groupoids and semirings. <LINK>']",https://arxiv.org/abs/2004.00889,"We investigate the algebra of a Hausdorff ample groupoid, introduced by Steinberg, over a commutative semiring S. In particular, we obtain a complete characterization of congruence-simpleness for such Steinberg algebras, extending the well-known characterizations when S is a field or a commutative ring. We also provide a criterion for the Steinberg algebra of the graph groupoid associated to an arbitrary graph to be congruence-simple. Motivated by a result of Clark and Sims, we show that, over the Boolean semifield, the natural homomorphism from the Leavitt path algebra to the Steinberg algebra is an isomorphism if and only if the associated graph is row-finite. Moreover, we establish the Reduction Theorem and Uniqueness Theorems for Leavitt path algebras of row-finite graphs over the Boolean semifield. ","On Steinberg algebras of Hausdorff ample groupoids over commutative
  semirings"
165,1245634642146820097,384080522,Dr. Daniela Castro-Camilo,"['New paper 📰: <LINK>\nWe propose a general method for probabilistic prediction of extreme hot-spots in a spatio-temporal setting, with an application to the Sea Surface Temperature anomalies provided in a data challenge. We were late 🙁but got the best score 😌', 'Codes to implement the models available here: https://t.co/ihm3vHXoMl']",https://arxiv.org/abs/2004.00386,"We develop a method for probabilistic prediction of extreme value hot-spots in a spatio-temporal framework, tailored to big datasets containing important gaps. In this setting, direct calculation of summaries from data, such as the minimum over a space-time domain, is not possible. To obtain predictive distributions for such cluster summaries, we propose a two-step approach. We first model marginal distributions with a focus on accurate modeling of the right tail and then, after transforming the data to a standard Gaussian scale, we estimate a Gaussian space-time dependence model defined locally in the time domain for the space-time subregions where we want to predict. In the first step, we detrend the mean and standard deviation of the data and fit a spatially resolved generalized Pareto distribution to apply a correction of the upper tail. To ensure spatial smoothness of the estimated trends, we either pool data using nearest-neighbor techniques, or apply generalized additive regression modeling. To cope with high space-time resolution of data, the local Gaussian models use a Markov representation of the Mat\'ern correlation function based on the stochastic partial differential equations (SPDE) approach. In the second step, they are fitted in a Bayesian framework through the integrated nested Laplace approximation implemented in R-INLA. Finally, posterior samples are generated to provide statistical inferences through Monte-Carlo estimation. Motivated by the 2019 Extreme Value Analysis data challenge, we illustrate our approach to predict the distribution of local space-time minima in anomalies of Red Sea surface temperatures, using a gridded dataset (11315 days, 16703 pixels) with artificially generated gaps. In particular, we show the improved performance of our two-step approach over a purely Gaussian model without tail transformations. ","Bayesian space-time gap filling for inference on extreme hot-spots: an
  application to Red Sea surface temperatures"
166,1245630099409399808,374433827,Rodrigo Agerri,"['""Multilingual Stance Detection: The Catalonia Independence Corpus"". New paper at LREC 2020. Annotating tweets at user level is quicker and cheaper to produce large, good quality datasets. Plus SOTA in TW10 IberEval 2018 benchmark. <LINK> @Hitz_zentroa', 'Joint work with Elena Zotova, Manuel Nuñez and German Rigau']",https://arxiv.org/abs/2004.00050,"Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The TW-10 Referendum Dataset released at IberEval 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the independence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the TW-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the TW-10 dataset, both for Catalan and Spanish. ",Multilingual Stance Detection: The Catalonia Independence Corpus
167,1245620976022245380,374433827,Rodrigo Agerri,"['""Give your Text Representation Models some Love: the Case for Basque."" New paper at LREC 2020. Existing pre-trained LMs improve results also for low-resource languages; best results when training a specific monolingual model.  <LINK> @i_sanvi @Aitor57 @eagirre', 'Joint work with: @i_sanvi , Jon Ander Campos, Ander Barrena, Xabier Saralegi, @Aitor57  and @eagirre']",https://arxiv.org/abs/2004.00033,"Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available. ",Give your Text Representation Models some Love: the Case for Basque
168,1245552775049732098,4902145390,Gordan Krnjaic,"['Cheers to my awesome collaborators @DanHooperAstro, John March-Russell, Sam McDermott, and Rudin Petrossian-Byrne for our new paper on merging and evaporating black holes in the early universe <LINK> #Cosmology <LINK>']",https://arxiv.org/abs/2004.00618#BlackHole,"Any abundance of black holes that was present in the early universe will evolve as matter, making up an increasingly large fraction of the total energy density as space expands. This motivates us to consider scenarios in which the early universe included an era that was dominated by low-mass ($M < 5\times 10^8$ g) black holes which evaporate prior to primordial nucleosynthesis. In significant regions of parameter space, these black holes will become gravitationally bound within binary systems, and undergo mergers before evaporating. Such mergers result in three potentially observable signatures. First, any black holes that have undergone one or more mergers will possess substantial angular momentum, causing their Hawking evaporation to produce significant quantities of high-energy gravitons. These products of Hawking evaporation are predicted to constitute a background of hot ($\sim$eV-keV) gravitons today, with an energy density corresponding to $\Delta N_{\rm eff} \sim 0.01-0.03$. Second, these mergers will produce a stochastic background of high-frequency gravitational waves. And third, the energy density of these gravitational waves can be as large as $\Delta N_{\rm eff} \sim 0.3$, depending on the length of time between the mergers and evaporation. These signals are each potentially within the reach of future measurements. ","Hot Gravitons and Gravitational Waves From Kerr Black Holes in the Early
  Universe"
169,1259800998903717888,463735426,Varun Gangal,"['Our new paper, ""SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations"", with @XiangKong4 and @ehovy, is accepted at #ACL2020 @aclmeeting \n\nDataset: <LINK>\narXiv: <LINK>\nCode: <LINK> <LINK>', 'We introduce a ""sentence-level"" cloze dataset curated from English exams.\n\nEach Qn requires filling up multiple (~5) sentence-level blanks in the passage, from a shared candidate set.\n\nTo make things harder, the candidate set also includes one or more human-authored distractors. https://t.co/10fIXT0Y0b', 'Examples from each of the four broad reasoning types required to answer the blanks - Word Matching, Paraphrasing, Inference and Summarization https://t.co/Cuu3Mfj7hl']",https://arxiv.org/abs/2004.12934,"We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human-created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving and significantly impair each other's context. Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap. ","SCDE: Sentence Cloze Dataset with High Quality Distractors From
  Examinations"
170,1255740674810535937,247800333,Ahmad طه,"['New Paper🚨: The dynamic systems literature is obsessed with Lipschitz nonlinear dynamic networks. In this arxiv pre-print, we show how Lipschitz and non-Lipschitz networks can be classified and parameterized using elegant theory and practical experiments. <LINK>', ""@theEnergyMads I wish I'm as elegant as that paper. 😜""]",https://arxiv.org/abs/2004.12061,"Numerous state-feedback and observer designs for nonlinear dynamic systems (NDS) have been developed in the past three decades. These designs assume that NDS nonlinearities satisfy one of the following function set classifications: bounded Jacobian, Lipschitz continuity, one-sided Lipschitz, quadratic inner-boundedness, and quadratic boundedness. These function sets are characterized by constant scalars or matrices bounding the NDS' nonlinearities. These constants (i) depend on the NDS' operating region, topology, and parameters, and (ii) are utilized to synthesize observer/controller gains. Unfortunately, there is a near-complete absence of algorithms to compute such bounding constants. In this paper, we develop analytical then computational methods to compute such constants. First, for every function set classification, we derive analytical expressions for these bounding constants through global maximization formulations. Second, we utilize a derivative-free, interval-based global maximization algorithm based on branch-and-bound framework to numerically obtain the bounding constants. Third, we showcase the effectiveness of our approaches to compute the corresponding parameters on some NDS such as highway traffic networks and synchronous generator models. ","Nonlinear Dynamic Systems Parameterization Using Interval-Based Global
  Optimization: Computing Lipschitz Constants and Beyond"
171,1254683486138621952,604252261,Heidi Howard,"['New paper out today! This morning I will be presenting our latest paper ""Paxos vs Raft: Have we reached consensus on distributed consensus"" at the PaPoC workshop.  #papoc2020 #EuroSys20 <LINK> <LINK> <LINK>', 'In the paper, we consider the controversial question of which algorithm, Paxos or Raft, is the better solution to distributed consensus? In the process, we describe a simplified ""Raft-style"" Paxos algorithm. The images are a side-by-side comparison with the differences in red. https://t.co/079qWbKAPx', ""We conclude that much of the understandability of Raft comes from the paper's excellent presentation and focus on simplicity instead of the underlying algorithm."", 'A pre-recorded video of the talk is available on youtube https://t.co/Qg47zcsZfV', '@RichardPrice @justincormack You passed the ""I am not a robot test"" 😉']",https://arxiv.org/abs/2004.05074,"Distributed consensus is a fundamental primitive for constructing fault-tolerant, strongly-consistent distributed systems. Though many distributed consensus algorithms have been proposed, just two dominate production systems: Paxos, the traditional, famously subtle, algorithm; and Raft, a more recent algorithm positioned as a more understandable alternative to Paxos. In this paper, we consider the question of which algorithm, Paxos or Raft, is the better solution to distributed consensus? We analyse both to determine exactly how they differ by describing a simplified Paxos algorithm using Raft's terminology and pragmatic abstractions. We find that both Paxos and Raft take a very similar approach to distributed consensus, differing only in their approach to leader election. Most notably, Raft only allows servers with up-to-date logs to become leaders, whereas Paxos allows any server to be leader provided it then updates its log to ensure it is up-to-date. Raft's approach is surprisingly efficient given its simplicity as, unlike Paxos, it does not require log entries to be exchanged during leader election. We surmise that much of the understandability of Raft comes from the paper's clear presentation rather than being fundamental to the underlying algorithm being presented. ",Paxos vs Raft: Have we reached consensus on distributed consensus?
172,1252973359098466304,2818867628,Pengfei Liu,"[""#ACL2020 A new framework for Extractive Summarization: don't label sentences individually, directly M-A-T-C-H!\nEven instantiating the framework with a simple model, the SOTA result has been driven to a new level. But how does it work? [1/n]\nPaper&amp;code: <LINK> <LINK>"", 'We give a rigorous analysis of the two Qs: 1) is the summary-level extractor better than the sentence-level extractor? 2) how to quantify the inherent performance gap between these two extractors based on different datasets?  .. which allows for estimating expected gains [2/n] https://t.co/MwBKdiWWk5', 'The matching framework operates summary-levelly, discovering those ""buried talents"" (valuable candidate summaries that are unappreciated by the sentence-level extractor, a.k.a Pearl-Summary defined in this paper)']",https://arxiv.org/abs/2004.08795,"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in this https URL ",Extractive Summarization as Text Matching
173,1252792195297562624,1082541680811601920,Tanya Chowdhury,"['Check out our new paper ""Neural Abstractive Summarization with Structural Attention"" now accepted at #IJCAI2020.\n\nWork in collaboration with @shocheen and @Tanmoy_Chak, from when I was a part of @lcs2iiitd\n\nPaper Link : <LINK> <LINK>', 'We present a hierarchical encoder, based on structural attention, to model complex inter-sentence and document dependencies. We do so by implicitly modelling documents as non projective parse trees within end-to-end training. We see significant gains on unstructured MDS corpora.']",https://arxiv.org/abs/2004.09739,"Attentional, RNN-based encoder-decoder architectures have achieved impressive performance on abstractive summarization of news articles. However, these methods fail to account for long term dependencies within the sentences of a document. This problem is exacerbated in multi-document summarization tasks such as summarizing the popular opinion in threads present in community question answering (CQA) websites such as Yahoo! Answers and Quora. These threads contain answers which often overlap or contradict each other. In this work, we present a hierarchical encoder based on structural attention to model such inter-sentence and inter-document dependencies. We set the popular pointer-generator architecture and some of the architectures derived from it as our baselines and show that they fail to generate good summaries in a multi-document setting. We further illustrate that our proposed model achieves significant improvement over the baselines in both single and multi-document summarization settings -- in the former setting, it beats the best baseline by 1.31 and 7.8 ROUGE-1 points on CNN and CQA datasets, respectively; in the latter setting, the performance is further improved by 1.6 ROUGE-1 points on the CQA dataset. ",Neural Abstractive Summarization with Structural Attention
174,1252620624717139969,610427323,Desika Narayanan,"['Hey Astrotwitter!\n\nwe put out a fun new paper on the @arxiv today on the formation of giant space blobs (lyman alpha blobs) using the FIRE zoom in simulation series:\n\n<LINK>\n\n[1/]', 'In short - LAB formation seems to be a natural part of massive galaxy evolution at high-redshift.  The incredible Lya luminosities maybe unsurprisingly originate from a complex mix of phenomena.  \n\n[2/]', 'Much comes from recombination in ionized gas in galaxies, but the contribution from cooling streams into the halo  is also super important.   When we include a crude model for AGN, the ionization it drives can overwhelm the Lya luminosity.\n\n[3/]', '[4/]  Blobs have crazy morphologies that reflect their complex ISM and CGM structure.   But, when you convolve them with a MUSE-like PSF, they become...well...blobby :)', ""[5/5]  But here's the most important part.   (and @realDonaldTrump I'm looking at you).   Damn near 50% of the author list is first generation immigrants.  And when you include 2nd generation, it's about near all of us.\n\nFight xenophobia, and never let it be normalized."", ""@maximetrebitsch @arxiv Thanks!   We did a few quick experiments but then the paper was already getting absurdly long so we deferred profiles to another future work :) But, I don't think I was aware of the large EQWs from your work - I'll take a look at the sims to see!"", '@Knusper2000 @jorryt_m @maximetrebitsch @arxiv this is a good question! it may not be straightforward, but definitely worth a look to see if we can.\n\n(and apologies for the missed citations...will correct!)', '@maximetrebitsch @Knusper2000 @jorryt_m @arxiv not at all -- this is super interesting!!']",https://arxiv.org/abs/2004.08397,"High-redshift Lyman-alpha blobs (LABs) are an enigmatic class of objects that have been the subject of numerous observational and theoretical investigations. It is of particular interest to determine the dominant power sources for the copious luminosity, as direct emission from HII regions, cooling gas, and fluorescence due to the presence of active galactic nuclei (AGN) can all contribute significantly. In this paper, we present the first theoretical model to consider all of these physical processes in an attempt to develop an evolutionary model for the origin of high-z LABs. This is achieved by combining a series of high-resolution cosmological zoom-in simulations with ionization and Lyman-alpha (Lya) radiative transfer models. We find that massive galaxies display a range of Lya luminosities and spatial extents (which strongly depend on the limiting surface brightness used) over the course of their lives, though regularly exhibit luminosities and sizes consistent with observed LABs. The model LABs are typically powered from a combination of recombination in star-forming galaxies, as well as cooling emission from gas associated with accretion. When AGN are included in the model, the fluorescence caused by AGN-driven ionization can be a significant contributor to the total Lya luminosity as well. We propose that the presence of an AGN may be predicted from the Gini coefficient of the blob's surface brightness. Within our modeled mass range, there are no obvious threshold physical properties that predict appearance of LABs, and only weak correlations of the luminosity with the physical properties of the host galaxy. This is because the emergent Lya luminosity from a system is a complex function of the gas temperature, ionization state, and Lya escape fraction. ","The Origin and Evolution of Lyman-alpha Blobs in Cosmological Galaxy
  Formation Simulations"
175,1251175944284487680,1033698977248698370,Silvia Terragni,"['Can you tell the topics of an Italian document without speaking Italian? Now you can! 💪\nNew paper: ""Cross-lingual Contextualized Topic Models with Zero-shot Learning"" with @fb_vinid @dirk_hovy @debora_nozza @FersiniE\nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/2004.07737,"Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. ",Cross-lingual Contextualized Topic Models with Zero-shot Learning
176,1251149385607471104,2332157006,Federico Bianchi,"['Cross-lingual topic modeling with zero-shot? Yes 😺  \nUnderstanding cat language? Not yet 😿\n\nNew work with @dirk_hovy, @debora_nozza (@MilaNLProc), @TerragniSilvia and @FersiniE \n\nSee the paper: <LINK>\nPip-package to have fun with: <LINK> <LINK>']",https://arxiv.org/abs/2004.07737,"Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions. ",Cross-lingual Contextualized Topic Models with Zero-shot Learning
177,1251085461789032449,305625386,Davide Mazzini,"['Our new paper is on Arxiv!\nLight3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views. With @elmuz and Pietro\n\nHandles 10 cameras at 6 FPS on a single 1080Ti GPU\n\nArxiv: <LINK>\nDemo Video: <LINK>', '@moiseev_igor !']",https://arxiv.org/abs/2004.02688,"We present an approach to perform 3D pose estimation of multiple people from a few calibrated camera views. Our architecture, leveraging the recently proposed unprojection layer, aggregates feature-maps from a 2D pose estimator backbone into a comprehensive representation of the 3D scene. Such intermediate representation is then elaborated by a fully-convolutional volumetric network and a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our method achieves state of the art MPJPE on the CMU Panoptic dataset using a few unseen views and obtains competitive results even with a single input view. We also assess the transfer learning capabilities of the model by testing it against the publicly available Shelf dataset obtaining good performance metrics. The proposed method is inherently efficient: as a pure bottom-up approach, it is computationally independent of the number of people in the scene. Furthermore, even though the computational burden of the 2D part scales linearly with the number of input views, the overall architecture is able to exploit a very lightweight 2D backbone which is orders of magnitude faster than the volumetric counterpart, resulting in fast inference time. The system can run at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU. ","Light3DPose: Real-time Multi-Person 3D PoseEstimation from Multiple
  Views"
178,1250859615010672642,373525906,Weijie Su,"['How does training time in #deeplearning depend on the learning rate? A new paper (<LINK>) uncovers a fundamental *distinction* between #nonconvex and convex problems from this viewpoint, showing why learning rate decay is *more* powerful in nonconvex settings. <LINK>', 'Theorem 2 gives an explicit expression for the linear rate of convergence for SGD. The dependence on the learning rate is VERY sharp. Proofs based on a careful analysis of the spectrum of the Witten-Laplacian, a special kind of Schrödinger operator. https://t.co/VazsFOyZo1', 'The approach in the paper is to use *continuous-time* surrogates for discrete optimization methods, just like https://t.co/u7LdFWYSrU. Gaps remain to be closed for future research. The learning rate decay technique can be found, for example, in https://t.co/da2zlhSZz4.']",https://arxiv.org/abs/2004.06977,"The learning rate is perhaps the single most important parameter in the training of neural networks and, more broadly, in stochastic (nonconvex) optimization. Accordingly, there are numerous effective, but poorly understood, techniques for tuning the learning rate, including learning rate decay, which starts with a large initial learning rate that is gradually decreased. In this paper, we present a general theoretical analysis of the effect of the learning rate in stochastic gradient descent (SGD). Our analysis is based on the use of a learning-rate-dependent stochastic differential equation (lr-dependent SDE) that serves as a surrogate for SGD. For a broad class of objective functions, we establish a linear rate of convergence for this continuous-time formulation of SGD, highlighting the fundamental importance of the learning rate in SGD, and contrasting to gradient descent and stochastic gradient Langevin dynamics. Moreover, we obtain an explicit expression for the optimal linear rate by analyzing the spectrum of the Witten-Laplacian, a special case of the Schr\""odinger operator associated with the lr-dependent SDE. Strikingly, this expression clearly reveals the dependence of the linear convergence rate on the learning rate -- the linear rate decreases rapidly to zero as the learning rate tends to zero for a broad class of nonconvex functions, whereas it stays constant for strongly convex functions. Based on this sharp distinction between nonconvex and convex problems, we provide a mathematical interpretation of the benefits of using learning rate decay for nonconvex optimization. ","On Learning Rates and Schr\""odinger Operators"
179,1250849014196527104,3312021076,Arman Cohan,"['New #acl2020nlp paper: We show that pretraining Transformers on inter-document relatedness signals results in improved doc-level representation learning.\npaper: <LINK>\nw/ @SergeyFeldman @i_beltagy Doug Downey @dsweld', 'Existing Transformer LMs focus on token and sentence level representations. Our model SPECTER, learns document-level representation by pretraining Transformer LMs on the citation signal. SPECTER offers substantial improvements on a wide range of downstream document-level tasks', 'SPECTER is already being used in applications like similar paper recommendation. \nThis is helpful in discovering relevant papers to a given topic like #COVID19. Example application: \nhttps://t.co/IXwjXvfY3N']",https://arxiv.org/abs/2004.07180,"Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. ","SPECTER: Document-level Representation Learning using Citation-informed
  Transformers"
180,1250846486323060736,3409898008,〈 Berger | Dillon 〉,"['Our current cosmological picture gets fuzzy at temperatures above 10 MeV. \n\nWhat if QCD went through a period confinement in the early universe? How would it affect the Dark Matter density? Turns out that it saves WIMPs! \n\nCheck out my new paper 👇\n\n🔗<LINK> <LINK>', '@Astrophys_Adam let me know if you have any questions!', 'Agh i forgot that one of the authors Seyda Ipek (@sheydaipek) was on Twitter!\n\nShe was such a pleasure to work with, and is so knowledgable about thermal field theory &amp; cosmology. Talking with her made so many things so much more clear to me! It was an absolute pleasure.']",https://arxiv.org/abs/2004.06727,"Standard lore states that there is tension between the need to accommodate the relic density of a weakly interacting massive particle and direct searches for dark matter. However, the estimation of the relic density rests on an extrapolation of the cosmology of the early Universe to the time of freeze out, untethered by observations. We explore a nonstandard cosmology in which the strong coupling constant evolves in the early Universe, triggering an early period of QCD confinement at the time of freeze out. We find that depending on the nature of the interactions between the dark matter and the Standard Model, freeze out during an early period of confinement can lead to drastically different expectations for the relic density, allowing for regions of parameter space which realize the correct abundance but would otherwise be excluded by direct searches. ","Dark Matter Freeze Out during an Early Cosmological Period of QCD
  Confinement"
181,1249692757532139520,1086908209271582721,Ashish Sharma,"['New @ICWSM 2020 paper alert! \n\n""Engagement Patterns of Peer-to-Peer Interactions on Mental Health Platforms""\nJoint work w/ the amazing Monojit Choudhury, @timalthoff, and @amt_shrma\n\nPreprint: <LINK>', 'This work was done during my Research Fellowship @MSFTResearch. Special thanks to @kous2v, @Sci_Tai, @Mike_A_Merrill, @SachinPendse, and @TalkLifeApp for their help.', 'If you are curious what makes online support conversations engaging and how to improve online mental health peer-to-peer support, this paper is for you!']",https://arxiv.org/abs/2004.04999,"Mental illness is a global health problem, but access to mental healthcare resources remain poor worldwide. Online peer-to-peer support platforms attempt to alleviate this fundamental gap by enabling those who struggle with mental illness to provide and receive social support from their peers. However, successful social support requires users to engage with each other and failures may have serious consequences for users in need. Our understanding of engagement patterns on mental health platforms is limited but critical to inform the role, limitations, and design of these platforms. Here, we present a large-scale analysis of engagement patterns of 35 million posts on two popular online mental health platforms, TalkLife and Reddit. Leveraging communication models in human-computer interaction and communication theory, we operationalize a set of four engagement indicators based on attention and interaction. We then propose a generative model to jointly model these indicators of engagement, the output of which is synthesized into a novel set of eleven distinct, interpretable patterns. We demonstrate that this framework of engagement patterns enables informative evaluations and analysis of online support platforms. Specifically, we find that mutual back-and-forth interactions are associated with significantly higher user retention rates on TalkLife. Such back-and-forth interactions, in turn, are associated with early response times and the sentiment of posts. ","Engagement Patterns of Peer-to-Peer Interactions on Mental Health
  Platforms"
182,1249101075816169473,942238055607435264,Luca Carlone,['New paper on soft aerial manipulation from SPARK. Now you can hoard toilet paper from the safety of your apartment! (or do the right thing and relax by reading this paper <LINK>). Full video: <LINK> #mitSparkLab #MIT #drones #robots #toiletpaper <LINK>'],https://arxiv.org/abs/2004.04238,"Manipulation and grasping with unmanned aerial vehicles (UAVs) currently require accurate positioning and are often executed at reduced speed to ensure successful grasps. This is due to the fact that typical UAVs can only accommodate rigid manipulators with few degrees of freedom, which limits their capability to compensate for disturbances caused by the vehicle positioning errors. Moreover, UAVs have to minimize external contact forces in order to maintain stability. Biological systems, on the other hand, exploit softness to overcome similar limitations, and leverage compliance to enable aggressive grasping. This paper investigates control and trajectory optimization for a soft aerial manipulator, consisting of a quadrotor and a tendon-actuated soft gripper, in which the advantages of softness can be fully exploited. To the best of our knowledge, this is the first work at the intersection between soft manipulation and UAV control. We present a decoupled approach for the quadrotor and the soft gripper, combining (i) a geometric controller and a minimum-snap trajectory optimization for the quadrotor (rigid) base, with (ii) a quasi-static finite element model and control-space interpolation for the soft gripper. We prove that the geometric controller asymptotically stabilizes the quadrotor velocity and attitude despite the addition of the soft load. Finally, we evaluate the proposed system in a realistic soft dynamics simulator, and show that: (i) the geometric controller is fairly insensitive to the soft payload, (ii) the platform can reliably grasp unknown objects despite inaccurate positioning and initial conditions, and (iii) the decoupled controller is amenable for real-time execution. ",Control and Trajectory Optimization for Soft Aerial Manipulation
183,1248165283635789825,2742282828,Andreea Font,['New paper by Sam Stafford exploring the consequences of various extensions to the standard cosmological model (e.g. warm or self-interacting dark matter or a running of the scalar spectral index) on the properties of small scale structure: <LINK>'],https://arxiv.org/abs/2004.03872,"It has been claimed that the standard model of cosmology (LCDM) cannot easily account for a number of observations on relatively small scales, motivating extensions to the standard model. Here we introduce a new suite of cosmological simulations that systematically explores three plausible extensions: warm dark matter, self-interacting dark matter, and a running of the scalar spectral index of density fluctuations. Current observational constraints are used to specify the additional parameters that come with these extensions. We examine a large range of observable metrics on small scales, including the halo mass function, density and circular velocity profiles, the abundance of satellite subhaloes, and halo concentrations. For any given metric, significant degeneracies can be present between the extensions. In detail, however, the different extensions have quantitatively distinct mass and radial dependencies, suggesting that a multi-probe approach over a range of scales can be used to break the degeneracies. We also demonstrate that the relative effects on the radial density profiles in the different extensions (compared to the standard model) are converged down to significantly smaller radii than are the absolute profiles. We compare the derived cosmological trends with the impact of baryonic physics using the EAGLE and ARTEMIS simulations. Significant degeneracies are also present between baryonic physics and cosmological variations (with both having similar magnitude effects on some observables). Given the inherent uncertainties both in the modelling of galaxy formation physics and extensions to LCDM, a systematic and simultaneous exploration of both is strongly warranted. ","Exploring extensions to the standard cosmological model and the impact
  of baryons on small scales"
184,1247915582734942209,773069606,Federico Ardila,"[""New paper:\nThe Arithmetic of Coxeter Permutahedra\n  Federico Ardila\n  Matthias Beck\n  Jodi McWhirter\n  <LINK>\nThis was a very fun project to work on, based on Jodi's Master's thesis with us at @SFSU. (She's now a PhD student at @WUSTL) <LINK>""]",https://arxiv.org/abs/2004.02952,"Ehrhart theory measures a polytope P discretely by counting the lattice points inside its dilates P, 2P, 3P, .... We compute the Ehrhart quasipolynomials of the standard Coxeter permutahedra for the classical Coxeter groups, expressing them in terms of the Lambert W function. A central tool is a description of the Ehrhart theory of a rational translate of an integer zonotope. ",The Arithmetic of Coxeter Permutahedra
185,1247589198552608778,303553181,"Ben Horne, Ph.D.","['New preprint announcement: What is BitChute? Characterizing the ""Free Speech\'\' Alternative to YouTube, a collaboration between @illegaldaydream @GruppiMauricio @codybuntain and myself. Link to the paper: <LINK> . Thread of some results below.', 'In this paper, we perform an exploratory, mixed methods analysis of the video-hosting platform. Some of our core results include: BitChute is primarily used for news and political commentary, attracting many “news-like” channels that provide mostly conspiracy-driven content.', 'Only a handful of channels receive any engagement, but\nalmost all of those channels contain far-right conspiracies or extreme hate speech (i.e. 12% of the channels receive over 85% of the engagement).', 'Both the videos and comments on BitChute contain high\namounts of hate speech, mostly anti-Semitic. Evidence shows the rate of hate speech on BitChute is higher than on Gab, but less than 4chan’s “politically incorrect” board /pol/.', 'Data will be publicly available once the paper finds a peer-reviewed home. (and I will announce when its available!)', 'also, big thanks to @jhblackb and @iDRAMALab for their hate speech lexicon!']",https://arxiv.org/abs/2004.01984,"In this paper, we characterize the content and discourse on BitChute, a social video-hosting platform. Launched in 2017 as an alternative to YouTube, BitChute joins an ecosystem of alternative, low content moderation platforms, including Gab, Voat, Minds, and 4chan. Uniquely, BitChute is the first of these alternative platforms to focus on video content and is growing in popularity. Our analysis reveals several key characteristics of the platform. We find that only a handful of channels receive any engagement, and almost all of those channels contain conspiracies or hate speech. This high rate of hate speech on the platform as a whole, much of which is anti-Semitic, is particularly concerning. Our results suggest that BitChute has a higher rate of hate speech than Gab but less than 4chan. Lastly, we find that while some BitChute content producers have been banned from other platforms, many maintain profiles on mainstream social media platforms, particularly YouTube. This paper contributes a first look at the content and discourse on BitChute and provides a building block for future research on low content moderation platforms. ","What is BitChute? Characterizing the ""Free Speech"" Alternative to
  YouTube"
186,1245819249634967552,15710213,Tomáš Hodaň,"[""Check out our new method for 6D object pose estimation! It's called EPOS, can handle objects with global and partial symmetries, and achieves state-of-the-art RGB-only results on the T-LESS, Linemod-Occluded, and YCB-Video datasets.\n\nA CVPR 2020 paper: <LINK> <LINK>""]",https://arxiv.org/abs/2004.00605,"We present a new method for estimating the 6D pose of rigid objects with available 3D models from a single RGB input image. The method is applicable to a broad range of objects, including challenging ones with global or partial symmetries. An object is represented by compact surface fragments which allow handling symmetries in a systematic manner. Correspondences between densely sampled pixels and the fragments are predicted using an encoder-decoder network. At each pixel, the network predicts: (i) the probability of each object's presence, (ii) the probability of the fragments given the object's presence, and (iii) the precise 3D location on each fragment. A data-dependent number of corresponding 3D locations is selected per pixel, and poses of possibly multiple object instances are estimated using a robust and efficient variant of the PnP-RANSAC algorithm. In the BOP Challenge 2019, the method outperforms all RGB and most RGB-D and D methods on the T-LESS and LM-O datasets. On the YCB-V dataset, it is superior to all competitors, with a large margin over the second-best RGB method. Source code is at: cmp.felk.cvut.cz/epos. ",EPOS: Estimating 6D Pose of Objects with Symmetries
187,1258795497487302661,260971421,Walid Magdy,['Working on Sarcasm Detection?\n\nCheck our preprints of papers at #CSCW2020 &amp; #acl2020nlp. We study sarcasm communication online and release a new dataset (iSarcasm) that models sarcasm as intended by author.\n\nPaper 1: <LINK>\nPaper 2: <LINK>'],https://arxiv.org/abs/2004.04945,"Online social networks (OSN) play an essential role for connecting people and allowing them to communicate online. OSN users share their thoughts, moments, and news with their network. The messages they share online can include sarcastic posts, where the intended meaning expressed by the written text is different from the literal one. This could result in miscommunication. Previous research in psycholinguistics has studied the sociocultural factors the might lead to sarcasm misunderstanding between speakers and listeners. However, there is a lack of such studies in the context of OSN. In this paper we fill this gap by performing a quantitative analysis on the influence of sociocultural variables, including gender, age, country, and English language nativeness, on the effectiveness of sarcastic communication online. We collect examples of sarcastic tweets directly from the authors who posted them. Further, we ask third-party annotators of different sociocultural backgrounds to label these tweets for sarcasm. Our analysis indicates that age, English language nativeness, and country are significantly influential and should be considered in the design of future social analysis tools that either study sarcasm directly, or look at related phenomena where sarcasm may have an influence. We also make observations about the social ecology surrounding sarcastic exchanges on OSNs. We conclude by suggesting ways in which our findings can be included in future work. ",The Effect of Sociocultural Variables on Sarcasm Communication Online
188,1257700168964562944,33615681,Ganesh Bagler,['A Named Entity Based Approach to Model Recipes\n\nWe propose a model for representing traditional cooking recipes. \n\n<LINK> <LINK>'],https://arxiv.org/abs/2004.12184,"Traditional cooking recipes follow a structure which can be modelled very well if the rules and semantics of the different sections of the recipe text are analyzed and represented accurately. We propose a structure that can accurately represent the recipe as well as a pipeline to infer the best representation of the recipe in this uniform structure. The Ingredients section in a recipe typically lists down the ingredients required and corresponding attributes such as quantity, temperature, and processing state. This can be modelled by defining these attributes and their values. The physical entities which make up a recipe can be broadly classified into utensils, ingredients and their combinations that are related by cooking techniques. The instruction section lists down a series of events in which a cooking technique or process is applied upon these utensils and ingredients. We model these relationships in the form of tuples. Thus, using a combination of these methods we model cooking recipe in the dataset RecipeDB to show the efficacy of our method. This mined information model can have several applications which include translating recipes between languages, determining similarity between recipes, generation of novel recipes and estimation of the nutritional profile of recipes. For the purpose of recognition of ingredient attributes, we train the Named Entity Relationship (NER) models and analyze the inferences with the help of K-Means clustering. Our model presented with an F1 score of 0.95 across all datasets. We use a similar NER tagging model for labelling cooking techniques (F1 score = 0.88) and utensils (F1 score = 0.90) within the instructions section. Finally, we determine the temporal sequence of relationships between ingredients, utensils and cooking techniques for modeling the instruction steps. ",A Named Entity Based Approach to Model Recipes
189,1256335656793460736,808750328241799168,Rachel Bawden,"['Can we improve BLEU with multiple, diverse references? Find out about the possibilities for (and limitations of) automatically paraphrasing MT references with our new preprint (with coauthors @BZhangGo, Lisa Yankovskaya, Andre Tättar and @mjpost): <LINK>']",http://arxiv.org/abs/2004.14989,"We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional diverse references can provide better coverage of the space of valid translations and thereby improve its correlation with human judgments. Our experiments on the into-English language directions of the WMT19 metrics task (at both the system and sentence level) show that using paraphrased references does generally improve BLEU, and when it does, the more diverse the better. However, we also show that better results could be achieved if those paraphrases were to specifically target the parts of the space most relevant to the MT outputs being evaluated. Moreover, the gains remain slight even when human paraphrases are used, suggesting inherent limitations to BLEU's capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentence-level BLEU. ","A Study in Improving BLEU Reference Coverage with Diverse Automatic
  Paraphrasing"
190,1256247812158783491,1019160894788505600,yehudit96,"['Our new paper ""Paraphrasing Vs. Coreferring: Two sides of the same coin"" is out <LINK>, a joint work with Avi Caciularu, @VeredShwartz and Ido Dagan\nWe study how Cross documents coreferece resolution (CDCR) and paraphrase identification can benefit each other', '➡️We use a CDCR dataset as distant supervision on predicate paraphrases resource, which improve its performance by 18 points!', '⬅️Than, the improved resource is utilized as external resource for CDCR model, which yields SOTA results on cross documents coreferece resolution.']",http://arxiv.org/abs/2004.14979,"We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model's performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other. ",Paraphrasing vs Coreferring: Two Sides of the Same Coin
191,1255874684438089734,398398588,BarzelLab,['Thanks for the shout. Looking forward to insights/comments and comparisons. \n\nYou can find our comparisons here: <LINK>\n\nBut welcome others to try themselves. We want this to really work. Not just be a nice idea. <LINK>'],https://arxiv.org/abs/2004.01453,"Absent a drug or vaccine, containing epidemic outbreaks is achieved by means of social distancing, specifically mobility restrictions and lock-downs. Such measures impose a hurtful toll on the economy, and are difficult to sustain for extended periods. As an alternative, we propose here an alternating quarantine strategy, in which at every instance, half of the population remains under lock-down while the other half continues to be active, maintaining a routine of weekly succession between activity and quarantine. This regime affords a dual partition:\ half of the population interacts for only half of the time, resulting in a dramatic reduction in transmission, comparable to that achieved by a population-wide lock-down. All the while, it enables socioeconomic continuity at $50\%$ capacity. The proposed weekly alternations also address an additional challenge, with specific relevance to COVID-19. Indeed, SARS-CoV-2 exhibits a relatively long incubation period, in which individuals experience no symptoms, but may already contribute to the spread. Unable to selectively isolate these invisible spreaders, we resort to population-wide restrictions. However, under the alternating quarantine routine, if an individual was exposed during their active week, by the time they complete their quarantine they will, in most cases, begin to exhibit symptoms. Hence this strategy isolates the majority of pre-symptomatic individuals during their infectious phase, leading to a rapid decline in the viral spread, thus addressing one of the main challenges in COVID-19 mitigation. ",Alternating quarantine for sustainable epidemic mitigation
192,1255832795341115393,319518346,Jose Camacho-Collados,"[""Are unambiguous words useful for Word Sense Disambiguation? Surprisingly yes! Check out our latest work with @danielbloureiro to find out why and how. \n\nPaper 👉 <LINK>\n\n(Note: for full transparency, we've also made our #acl2020nlp reviews available) #NLProc 1/2 <LINK>"", 'Bonus: we freely release UWA, a large sense-annotated corpus based on OpenWebText and Wikipedia with unambiguous word annotations. Sense embeddings (based on this corpus and Semcor) to perform state-of-the-art WSD are also available for download. 2/2\n\nhttps://t.co/CU0s2DiWcF']",https://arxiv.org/abs/2004.14325,"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD. ","Don't Neglect the Obvious: On the Role of Unambiguous Words in Word
  Sense Disambiguation"
193,1255755772169945089,2687143885,Johannes Bjerva,"['Announcing SubjQA: A Dataset for Subjectivity and Review Comprehension, with @MegagonLabs. We study the relationship between subjectivity and QA in 6 domains in English. Includes subjectivity annotation, factual/subjective and (un)answerable Qs. #NLProc <LINK> <LINK>']",https://arxiv.org/abs/2004.14283,"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We therefore investigate the relationship between subjectivity and QA, while developing a new dataset. We compare and contrast with analyses from previous work, and verify that findings regarding subjectivity still hold when using recently developed NLP architectures. We find that subjectivity is also an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 distinct domains. ",SubjQA: A Dataset for Subjectivity and Review Comprehension
194,1255490977550213121,33615681,Ganesh Bagler,['Nutritional Profile Estimation in Cooking Recipes\n\nWe propose a scalable method for nutritional profile estimation of recipes from their ingredients section using United States Department of Agriculture (USDA-SR) Database for the nutritional values.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2004.12286,"The availability of an accurate nutrition profile of recipes is an important feature for food databases with several applications including nutritional assistance, recommendation systems, and dietary analytics. Often in online databases, recipes are obtained from diverse sources in an attempt to maximize the number of recipes and variety of the dataset. This leads to an incomplete and often unreliable set of nutritional details. We propose a scalable method for nutritional profile estimation of recipes from their ingredients section using a standard reliable database for the nutritional values. Previous studies have testified the efficiency of string-matching methods on small datasets. To demonstrate the effectiveness of our procedure, we apply the proposed method on a large dataset, RecipeDB, which contains recipes from multiple data sources, using the United States Department of Agriculture Standard Reference (USDA-SR) Database as a reference for computing nutritional profiles. We evaluate our method by calculating the average error across our database of recipes (36 calories per serving) which is well within the range of errors attributable to physical variations. ",Nutritional Profile Estimation in Cooking Recipes
195,1255088051467849728,775565084,Yiming Cui,['We propose RecAdam optimizer for fine-tuning pre-trained language models with less forgetting the prior knowledge. The results on GLUE with the state-of-the-art ALBERT-xxlarge model show significant improvements (0.7 on average). Check our paper😁: <LINK> #nlproc <LINK>'],https://arxiv.org/abs/2004.12651,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community. ","Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less
  Forgetting"
196,1255036000423862272,895581931974254592,Christian Dudel,"['New preprint with C Bohk-Ewald and M Myrsklä estimating  total number of #COVID19 infections based on death counts and infection fatality rates (IFRs). As IFRs are often not available we propose an approach to transfer them between countries <LINK>', 'Our approach is an indirect estimation approach accounting for the age structure of infections and deaths, and transfering IFRs between countries makes use of the concept of thanatological age (aka remaining life expectancy)', 'We cover ten countries/regions: US, Italy, Spain, France, UK, Belgium, Iran, Hubei, Germany, Netherlands.', 'On average, the total number of infections is estimated to be four times higher than the confirmed number of infections, but with a lot of heterogeneity between countries; e.g., in Germany, the total number and the confirmed number of cases seem to be quite close.', ""For reproducing our findings check out Christina's Github: https://t.co/IC3QshikoZ""]",https://arxiv.org/abs/2004.12836,"Understanding how widely COVID-19 has spread is critical for examining the pandemic's progression. Despite efforts to carefully monitor the pandemic, the number of confirmed cases may underestimate the total number of infections. We introduce a demographic scaling model to estimate COVID-19 infections using an broadly applicable approach that is based on minimal data requirements: COVID-19 related deaths, infection fatality rates (IFRs), and life tables. As many countries lack reliable estimates of age-specific IFRs, we scale IFRs between countries using remaining life expectancy as a marker to account for differences in age structures, health conditions, and medical services. Across 10 countries with most COVID-19 deaths as of May 13, 2020, the number of infections is estimated to be four [95% prediction interval: 2-11] times higher than the number of confirmed cases. Cross-country variation is high. The estimated number of infections is 1.4 million (six times the number of confirmed cases) for Italy; 3.1 million (2.2 times the number of confirmed cases) for the U.S.; and 1.8 times the number of confirmed cases for Germany, where testing has been comparatively extensive. Our prevalence estimates, however, are markedly lower than most others based on local seroprevalence studies. We introduce formulas for quantifying the bias that is required in our data on deaths in order to reproduce estimates published elsewhere. This bias analysis shows that either COVID-19 deaths are severely underestimated, by a factor of two or more; or alternatively, the seroprevalence based results are overestimates and not representative for the total population. ","A demographic scaling model for estimating the total number of COVID-19
  infections"
197,1255002595254587393,268919557,Joshua T. Vogelstein (jovo/he/we),"['An update on our #COVID work, where we find up to 56% reduction in mortality in patients with acute respiratory distress if they happen to be taking alpha-blockers for other reasons: \n<LINK>\n@MaxKonigMD  @HopkinsMedicine\n funding: \n@MSFTResearch &amp; #fastgrants']",https://arxiv.org/abs/2004.10117,"In severe viral pneumonia, including Coronavirus disease 2019 (COVID-19), the viral replication phase is often followed by hyperinflammation, which can lead to acute respiratory distress syndrome, multi-organ failure, and death. We previously demonstrated that alpha-1 adrenergic receptor ($\alpha_1$-AR) antagonists can prevent hyperinflammation and death in mice. Here, we conducted retrospective analyses in two cohorts of patients with acute respiratory distress (ARD, n=18,547) and three cohorts with pneumonia (n=400,907). Federated across two ARD cohorts, we find that patients exposed to $\alpha_1$-AR antagonists, as compared to unexposed patients, had a 34% relative risk reduction for mechanical ventilation and death (OR=0.70, p=0.021). We replicated these methods on three pneumonia cohorts, all with similar effects on both outcomes. All results were robust to sensitivity analyses. These results highlight the urgent need for prospective trials testing whether prophylactic use of $\alpha_1$-AR antagonists ameliorates lower respiratory tract infection-associated hyperinflammation and death, as observed in COVID-19. ","Alpha-1 adrenergic receptor antagonists to prevent hyperinflammation and
  death from lower respiratory tract infection"
198,1254804653683834882,2252332513,Duncan Ralph,"['Do you want to find high affinity antibodies in B cell receptor deep sequencing data? We have methods for you!\n\n<LINK>\n\nwith @ematsen', ""@petrelharp @ematsen There's definitely been people doing sequencing, and I'd expect to start seeing papers soon. Hopefully the data gets posted promptly to @bcorrie's awesome public ireceptor database https://t.co/SSZ4dbxi87""]",https://arxiv.org/abs/2004.11868,"We are frequently faced with a large collection of antibodies, and want to select those with highest affinity for their cognate antigen. When developing a first-line therapeutic for a novel pathogen, for instance, we might look for such antibodies in patients that have recovered. There exist effective experimental methods of accomplishing this, such as cell sorting and baiting; however they are time consuming and expensive. Next generation sequencing of B cell receptor (BCR) repertoires offers an additional source of sequences that could be tapped if we had a reliable method of selecting those coding for the best antibodies. In this paper we introduce a method that uses evolutionary information from the family of related sequences that share a naive ancestor to predict the affinity of each resulting antibody for its antigen. When combined with information on the identity of the antigen, this method should provide a source of effective new antibodies. We also introduce a method for a related task: given an antibody of interest and its inferred ancestral lineage, which branches in the tree are likely to harbor key affinity-increasing mutations? These methods are implemented as part of continuing development of the partis BCR inference package, available at this https URL ",Using B cell receptor lineage structures to predict affinity
199,1254131590877401088,1012448981207613440,Nisheeth Vishnoi,"['1/3\n\nGiven a manifold M with measure \\mu, and an A in the convex hull of M, how should one represent A as a continuous convex combination of points in M?\n\nIn a STOC 2020 paper, w/  Leake, we study mathematical &amp; computational aspects of this question\n\n<LINK> <LINK>', '2/3\n\nWe study the distribution \\nu with marginal A that minimizes the KL divergence to \\mu. \n\nWe show that such a distribution can be computed in polynomial time for various interesting manifolds. For instance for the Grassmannian by using the famous Harish-Chandra formula.', '3/3 Applications to computing an old notion of quantum entropy in polynomial time, to computing the entropic barrier studied by @SebastienBubeck and Eldan, and to an optimization interpretation of the measure chosen by Goemans-Williamson in their paper on Max-Cut.']",https://arxiv.org/abs/2004.07403,"We initiate a study of the following problem: Given a continuous domain $\Omega$ along with its convex hull $\mathcal{K}$, a point $A \in \mathcal{K}$ and a prior measure $\mu$ on $\Omega$, find the probability density over $\Omega$ whose marginal is $A$ and that minimizes the KL-divergence to $\mu$. This framework gives rise to several extremal distributions that arise in mathematics, quantum mechanics, statistics, and theoretical computer science. Our technical contributions include a polynomial bound on the norm of the optimizer of the dual problem that holds in a very general setting and relies on a ""balance"" property of the measure $\mu$ on $\Omega$, and exact algorithms for evaluating the dual and its gradient for several interesting settings of $\Omega$ and $\mu$. Together, along with the ellipsoid method, these results imply polynomial-time algorithms to compute such KL-divergence minimizing distributions in several cases. Applications of our results include: 1) an optimization characterization of the Goemans-Williamson measure that is used to round a positive semidefinite matrix to a vector, 2) the computability of the entropic barrier for polytopes studied by Bubeck and Eldan, and 3) a polynomial-time algorithm to compute the barycentric quantum entropy of a density matrix that was proposed as an alternative to von Neumann entropy in the 1970s: this corresponds to the case when $\Omega$ is the set of rank one projections matrices and $\mu$ corresponds to the Haar measure on the unit sphere. Our techniques generalize to the setting of Hermitian rank $k$ projections using the Harish-Chandra-Itzykson-Zuber formula, and are applicable even beyond, to adjoint orbits of compact Lie groups. ","On the computability of continuous maximum entropy distributions with
  applications"
200,1253689889540972546,114485232,Jimmy Lin,"['Building the Neural Covidex was step one. <LINK> Now we try to find out how good it is, via an effort led by @ralph_tang building on the amazing contributors at Kaggle: Rapidly Bootstrapping a Question Answering Dataset for COVID-19 <LINK>']",https://arxiv.org/abs/2004.11339,"We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at this http URL ",Rapidly Bootstrapping a Question Answering Dataset for COVID-19
201,1253658103800881153,1076183603242065920,Christina Göpfert,['Fresh on arXiv: Adversarial examples and where to find them with @niklas2484 and @janphigoe. <LINK> We analyze dependence of adversarial robustness on choice of Lp-norm. Also check out <LINK> to easily calculate your own robustness curves! #ML <LINK>'],http://arxiv.org/abs/2004.10882,"Adversarial robustness of machine learning models has attracted considerable attention over recent years. Adversarial attacks undermine the reliability of and trust in machine learning models, but the construction of more robust models hinges on a rigorous understanding of adversarial robustness as a property of a given model. Point-wise measures for specific threat models are currently the most popular tool for comparing the robustness of classifiers and are used in most recent publications on adversarial robustness. In this work, we use recently proposed robustness curves to show that point-wise measures fail to capture important global properties that are essential to reliably compare the robustness of different classifiers. We introduce new ways in which robustness curves can be used to systematically uncover these properties and provide concrete recommendations for researchers and practitioners when assessing and comparing the robustness of trained models. Furthermore, we characterize scale as a way to distinguish small and large perturbations, and relate it to inherent properties of data sets, demonstrating that robustness thresholds must be chosen accordingly. We release code to reproduce all experiments presented in this paper, which includes a Python module to calculate robustness curves for arbitrary data sets and classifiers, supporting a number of frameworks, including TensorFlow, PyTorch and JAX. ","How to compare adversarial robustness of classifiers from a global
  perspective"
202,1252978714784251904,953616889,Justin Read,"['Another fantastic paper from Martin Rey and the EDGE collaboration out today! We find that isolated low mass dwarf galaxies can grow in mass to accrete gas and reignite their star formation after reionisation:\n\n<LINK>\n\n[1/3]', 'We predict that at M200~3e9 Msun, dwarfs will transition from being gas rich quiescent ""ultra-faints"" to ""Leo-T""-like dwarfs that form stars at a rate of just ~1e-5 Msun/year!\n\n[2/3] https://t.co/M1PKu8tFCB', 'This may solve a long-standing puzzle as to how galaxies like Leo-T survive reionisation to remain star forming today. If so, a population of ""gas rich ultra-faints"" will be uncovered by up-coming surveys. Indeed, some may have already been found:\n\nhttps://t.co/YWrr1djLFL\n\n[3/3]']",https://arxiv.org/abs/2004.09530v1,"We study how star formation is regulated in low-mass field dwarf galaxies ($10^5 \leq M_{\star} \leq 10^6 \, \text{M}_{\odot}$), using cosmological high-resolution ($3 \, \text{pc}$) hydrodynamical simulations. Cosmic reionization quenches star formation in all our simulated dwarfs, but three galaxies with final dynamical masses of $3 \times 10^{9} \,\text{M}_{\odot}$ are subsequently able to replenish their interstellar medium by slowly accreting gas. Two of these galaxies re-ignite and sustain star formation until the present day at an average rate of $10^{-5} \, \text{M}_{\odot} \, \text{yr}^{-1}$, highly reminiscent of observed low-mass star-forming dwarf irregulars such as Leo T. The resumption of star formation is delayed by several billion years due to residual feedback from stellar winds and Type Ia supernovae; even at $z=0$, the third galaxy remains in a temporary equilibrium with a large gas content but without any ongoing star formation. Using the ""genetic modification'' approach, we create an alternative mass growth history for this gas-rich quiescent dwarf and show how a small $(0.2\,\mathrm{dex})$ increase in dynamical mass can overcome residual stellar feedback, re-igniting star formation. The interaction between feedback and mass build-up produces a diversity in the stellar ages and gas content of low-mass dwarfs, which will be probed by combining next-generation HI and imaging surveys. ",] EDGE: From quiescent to gas-rich to star-forming low-mass dwarf galaxies
203,1252877892813336577,127954917,Andrea Guzmán Mesa 🇨🇴🇨🇭🇪🇺,['My first first-author paper is today on Arxiv! We study the information content encoded in JWST NIRSpec transmission spectra for warm Neptunes using Machine Learning. Long story short: G395M/F290LP mode is the reasonable way to go. Comments are welcome! \n<LINK> <LINK>'],https://arxiv.org/abs/2004.10106,"Warm Neptunes offer a rich opportunity for understanding exo-atmospheric chemistry. With the upcoming James Webb Space Telescope (JWST), there is a need to elucidate the balance between investments in telescope time versus scientific yield. We use the supervised machine learning method of the random forest to perform an information content analysis on a 11-parameter model of transmission spectra from the various NIRSpec modes. The three bluest medium-resolution NIRSpec modes (0.7 - 1.27 microns, 0.97 - 1.84 microns, 1.66 - 3.07 microns) are insensitive to the presence of CO. The reddest medium-resolution mode (2.87 - 5.10 microns) is sensitive to all of the molecules assumed in our model: CO, CO2, CH4, C2H2, H2O, HCN and NH3. It competes effectively with the three bluest modes on the information encoded on cloud abundance and particle size. It is also competitive with the low-resolution prism mode (0.6 - 5.3 microns) on the inference of every parameter except for the temperature and ammonia abundance. We recommend astronomers to use the reddest medium-resolution NIRSpec mode for studying the atmospheric chemistry of 800-1200 K warm Neptunes; its corresponding high-resolution counterpart offers diminishing returns. We compare our findings to previous JWST information content analyses that favor the blue orders, and suggest that the reliance on chemical equilibrium could lead to biased outcomes if this assumption does not apply. A simple, pressure-independent diagnostic for identifying chemical disequilibrium is proposed based on measuring the abundances of H2O, CO and CO2. ","Information content of JWST-NIRSPEC transmission spectra of warm
  Neptunes"
204,1252262500919316480,61502457,Marinka Zitnik,"['We are live streaming, discussing our study on COVID-19 drug repurposing #AI #networks\n\nFull paper: <LINK> <LINK>']",https://arxiv.org/abs/2004.07229,"The current pandemic has highlighted the need for methodologies that can quickly and reliably prioritize clinically approved compounds for their potential effectiveness for SARS-CoV-2 infections. In the past decade, network medicine has developed and validated multiple predictive algorithms for drug repurposing, exploiting the sub-cellular network-based relationship between a drug's targets and disease genes. Here, we deployed algorithms relying on artificial intelligence, network diffusion, and network proximity, tasking each of them to rank 6,340 drugs for their expected efficacy against SARS-CoV-2. To test the predictions, we used as ground truth 918 drugs that had been experimentally screened in VeroE6 cells, and the list of drugs under clinical trial, that capture the medical community's assessment of drugs with potential COVID-19 efficacy. We find that while most algorithms offer predictive power for these ground truth data, no single method offers consistently reliable outcomes across all datasets and metrics. This prompted us to develop a multimodal approach that fuses the predictions of all algorithms, showing that a consensus among the different predictive methods consistently exceeds the performance of the best individual pipelines. We find that 76 of the 77 drugs that successfully reduced viral infection do not bind the proteins targeted by SARS-CoV-2, indicating that these drugs rely on network-based actions that cannot be identified using docking-based strategies. These advances offer a methodological pathway to identify repurposable drugs for future pathogens and neglected diseases underserved by the costs and extended timeline of de novo drug development. ","Network Medicine Framework for Identifying Drug Repurposing
  Opportunities for COVID-19"
205,1252250966205894658,1251488051337068547,Alexandru Topîrceanu,"['Our study of centralized and decentralized isolation strategies on the #COVID19 dynamics is here: <LINK> We adapt a custom SICARS epidemic model and use #NetSci #NetworkScience to quantify how essential isolation is.', 'Two specific states of SICARS allow for understanding centralized C (government-imposed) and decentralized D (self-imposed) isolation (through social distancing) and their combined effects on limiting the number of infected people. https://t.co/PvzewrnBRs', 'We also test what happens when the #COVID19  isolation measures are not applied immediately (like in Singapore, Hong Kong, China), but with a delay (like US, UK, Europe), or reactively (“waiting for it to happen”) instead of proactively (“let’s be prepared”).', 'We are able to show that:\n• D isolation helps, but is insufficient on its own: -42% less infected. \n• C isolation is a must (reduces maximum degree in the network, hence it disables hubs or superspreaders from spreading the virus): -76% less infected.', '• The best response strategy is a hybrid one (C+D). That’s why it’s important to respect the government-imposed quarantine, but also be responsible and #StayHome : -87% less infected! https://t.co/oV5f2yMzOO', '• If applied too late after the outbreak of the pandemic, even the combined (C+D) strategy loses its effectiveness significantly: +41% more infected (for a delay of 20 days), or 4x times more infected (for a delay of 50 days)! https://t.co/0Iv7m6J8jW', '• All isolation strategies applied proactively (at outbreak onset) are more effective than applied reactively (in response to the epidemic dynamics) even by progressively increasing the isolation severity. https://t.co/So4PKu7LmR', '• A higher patient relapse rate (e.g., 10%) than currently estimated for COVID-19 (&lt;1%) would not alter our conclusions on the effectiveness of the isolation strategies significantly.\nFind all the details in our #NetSci #COVID19  paper on arXiv: https://t.co/eoHXp0nxN0 https://t.co/eM3kwDu9Vi']",https://arxiv.org/abs/2004.04222,"The infectious diseases are spreading due to human interactions enabled by various social networks. Therefore, when a new pathogen such as SARS-CoV-2 causes an outbreak, the non-pharmaceutical isolation strategies (e.g., social distancing) are the only possible response to disrupt its spreading. To this end, we introduce the new epidemic model (SICARS) and compare the centralized (C), decentralized (D), and combined (C+D) social distancing strategies, and analyze their efficiency to control the dynamics of COVID-19 on heterogeneous complex networks. Our analysis shows that the centralized social distancing is necessary to minimize the pandemic spreading. The decentralized strategy is insufficient when used alone, but offers the best results when combined with the centralized one. Indeed, the (C+D) is the most efficient isolation strategy at mitigating the network superspreaders and reducing the highest node degrees to less than 10% of their initial values. Our results also indicate that stronger social distancing, e.g., cutting 75% of social ties, can reduce the outbreak by 75% for the C isolation, by 33% for the D isolation, and by 87% for the (C+D) isolation strategy. Finally, we study the impact of proactive versus reactive isolation strategies, as well as their delayed enforcement. We find that the reactive response to the pandemic is less efficient, and delaying the adoption of isolation measures by over one month (since the outbreak onset in a region) can have alarming effects; thus, our study contributes to an understanding of the COVID-19 pandemic both in space and time. We believe our investigations have a high social relevance as they provide insights into understanding how different degrees of social distancing can reduce the peak infection ratio substantially; this can make the COVID-19 pandemic easier to understand and control over an extended period of time. ","Centralized and decentralized isolation strategies and their impact on
  the COVID-19 pandemic dynamics"
206,1252078787144986625,1710697381,Diego F. Torres,"[""Today we release 'Introducing the HD+B model for pulsar wind nebulae: a hybrid hydrodynamics/radiative approach', MNRAS @RoyalAstroSoc, <LINK>. This is a novel method to study pulsar wind nebulae. A hopefully intuitive conceptual summary is shown in the figure. <LINK>""]",https://arxiv.org/abs/2004.08171,"Identification and characterization of a rapidly increasing number of pulsar wind nebulae is, and will continue to be, a challenge of high-energy gamma-ray astrophysics. Given that such systems constitute -- by far -- the most numerous expected population in the TeV regime, such characterization is important not only to learn about the sources per se from an individual and population perspective, but also to be able to connect them with observations at other frequencies, especially in radio and X-rays. Also, we need to remove the emission from nebulae in highly confused regions of the sky for revealing other underlying emitters. In this paper we present a new approach for theoretical modelling of pulsar wind nebulae: a hybrid hydrodynamic-radiative model able to reproduce morphological features and spectra of the sources, with relatively limited numerical cost. ","Introducing the HD+B model for pulsar wind nebulae: a hybrid
  hydrodynamics/radiative approach"
207,1251092435653935104,1197738606866993152,Fatemeh Saleh,"['Check out our CVPR 2020 (Oral) paper proposing UCNet:\nPaper:\xa0<LINK>\nCode:\xa0<LINK>\n\nIn UCNet, we propose the first framework to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. <LINK>', 'Existing RGBD saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline.', 'Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space.']",https://arxiv.org/abs/2004.05763,"In this paper, we propose the first framework (UCNet) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline. Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space. With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions. Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection. ","UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional
  Variational Autoencoders"
208,1251056123584622594,543247607,Marcel Neunhoeffer,['What is useful private synthetic data? And how to measure it? @chrisguarnold and I hope to help improve differentially private synthetic data. Find our working paper: <LINK> We love to hear from you! #differentialprivacy #syntheticdata #datascience #gan <LINK>'],https://arxiv.org/abs/2004.07740,"Recent advances in generating synthetic data that allow to add principled ways of protecting privacy -- such as Differential Privacy -- are a crucial step in sharing statistical information in a privacy preserving way. But while the focus has been on privacy guarantees, the resulting private synthetic data is only useful if it still carries statistical information from the original data. To further optimise the inherent trade-off between data privacy and data quality, it is necessary to think closely about the latter. What is it that data analysts want? Acknowledging that data quality is a subjective concept, we develop a framework to evaluate the quality of differentially private synthetic data from an applied researcher's perspective. Data quality can be measured along two dimensions. First, quality of synthetic data can be evaluated against training data or against an underlying population. Second, the quality of synthetic data depends on general similarity of distributions or specific tasks such as inference or prediction. It is clear that accommodating all goals at once is a formidable challenge. We invite the academic community to jointly advance the privacy-quality frontier. ","Really Useful Synthetic Data -- A Framework to Evaluate the Quality of
  Differentially Private Synthetic Data"
209,1251052750646935552,568044590,Christian Arnold,"[""What is useful private synthetic data? And how to measure it? @mneunho and I hope to help improve differentially private data synthesizers. Find our working paper here: <LINK>  We'd love to hear from you! #differentialprivacy #syntheticdata #datascience #gan <LINK>""]",https://arxiv.org/abs/2004.07740,"Recent advances in generating synthetic data that allow to add principled ways of protecting privacy -- such as Differential Privacy -- are a crucial step in sharing statistical information in a privacy preserving way. But while the focus has been on privacy guarantees, the resulting private synthetic data is only useful if it still carries statistical information from the original data. To further optimise the inherent trade-off between data privacy and data quality, it is necessary to think closely about the latter. What is it that data analysts want? Acknowledging that data quality is a subjective concept, we develop a framework to evaluate the quality of differentially private synthetic data from an applied researcher's perspective. Data quality can be measured along two dimensions. First, quality of synthetic data can be evaluated against training data or against an underlying population. Second, the quality of synthetic data depends on general similarity of distributions or specific tasks such as inference or prediction. It is clear that accommodating all goals at once is a formidable challenge. We invite the academic community to jointly advance the privacy-quality frontier. ","Really Useful Synthetic Data -- A Framework to Evaluate the Quality of
  Differentially Private Synthetic Data"
210,1251046232186851328,17094747,nuriaoliver,"['There is a lot of debate and discussion about #contacttracing via #smartphone #apps. We propose #ACDCTracing, a low-cost, secure and private solution for contact tracing. Would love to hear your thoughts about it! <LINK>  \n@datapopalliance @AmigosIng']",https://arxiv.org/abs/2004.07463,"As we enter the control phase of the COVID-19 pandemic, many efforts have been dedicated to developing smartphone-based contact tracing apps in order to automatically identify people that a person with COVID-19 might have infected. These applications while potentially useful, present significant adoption, societal, technical and privacy challenges. We propose ACDC-Tracing, a simpler, anonymous, voucher-based contact tracing solution that relies on peoples' knowledge of their own close contacts. People who test positive are given an anonymous voucher which they can share with a limited number of people whom they think they might be infected. The recipients can use this voucher to book a COVID-19 test and can receive their test results without ever revealing their identity. People receiving positive result are given vouchers to further backtrack the path of infection. This is a fully anonymous solution which does not require any sharing of location data, Bluetooth, or having an app installed on people's mobile device. Moreover, ACDC-Tracing can be tested for effectiveness at a small scale without requiring adoption by the entire population, which would enable acquiring fast evidence about its efficacy and scalability. Finally, it is compatible with and complementary to alternative approaches to contact tracing. ",ACDC-Tracing: Towards Anonymous Citizen-Driven Contact Tracing
211,1251038089549508608,1155943178710634497,Dong-Ho Lee,"['How can we obtain supervision for NER in a cost-effective way? In our new #acl2020nlp paper (w/ @billyuchenlin @xiangrenNLP ), We propose “entity triggers”, a proxy of human explanations, and ""Trigger Matching Network”, which makes effective use of it. <LINK> <LINK>']",https://arxiv.org/abs/2004.07493,"Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce ""entity triggers,"" an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences. ","TriggerNER: Learning with Entity Triggers as Explanations for Named
  Entity Recognition"
212,1250998764388614144,4883662141,Roy Schwartz,"['Do we really need to run the most expensive models on all instances? In a new #acl2020nlp paper (w/ @GabiStanovsky @swabhz @JesseDodge @nlpnoah) we propose a method to dynamically exit BERT early &amp; fast w/o losing much performance <LINK> <LINK> <LINK>', '@ctongfei @OfirPress @GabiStanovsky @swabhz @JesseDodge @nlpnoah Thanks for the pointer, we will check it out!']",http://arxiv.org/abs/2004.07453,"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) ""exit"" from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code. ",The Right Tool for the Job: Matching Model and Instance Complexities
213,1250371864251285506,702919640,Subir Sachdev,"['We propose that ""the time reparameterization soft mode offers a promising and generic mechanism for resolving the strange metal puzzle"". The resistivity exponent is pinned at 1 by the `graviton\', and can\'t vary with details, as in previous proposals. <LINK>']",https://arxiv.org/abs/2004.05182,"The most puzzling aspect of the 'strange metal' behavior of correlated electron compounds is that the linear in temperature resistivity often extends down to low temperatures, lower than natural microscopic energy scales. We consider recently proposed deconfined critical points (or phases) in models of electrons in large dimension lattices with random nearest-neighbor exchange interactions. The criticality is in the class of Sachdev-Ye-Kitaev models, and exhibits a time reparameterization soft mode representing gravity in dual holographic theories. We compute the low temperature resistivity in a large $M$ limit of models with SU($M$) spin symmetry, and find that the dominant temperature dependence arises from this soft mode. The resistivity is linear in temperature down to zero temperature at the critical point, with a co-efficient universally proportional to the product of the residual resistivity and the co-efficient of the linear in temperature specific heat. We argue that the time reparameterization soft mode offers a promising and generic mechanism for resolving the strange metal puzzle. ","Linear in temperature resistivity in the limit of zero temperature from
  the time reparameterization soft mode"
214,1250110155078828032,734995453813641216,Vered Shwartz,"[""New pre-print targeting commonsense QA tasks. We propose an unsupervised framework with one LM as an answer scorer and another as a knowledge source. This is what happens when you don't panic and shut down your models when they talk with each other 😉\n<LINK> <LINK>"", 'On most tasks, our model performs better than the baseline (no additional knowledge) and similarly to a model that uses external knowledge (e.g. KB) - despite the LM being a noisier knowledge source. Joint work with @PeterWestTM, @Ronan_LeBras, @_csBhagav, and @YejinChoinka']",https://arxiv.org/abs/2004.05483,"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as ""$\textit{what is the definition of ...}$"" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning. ",Unsupervised Commonsense Question Answering with Self-Talk
215,1250044207529672704,5935462,Marco De Nadai,"['Thread) How is crime connected to socio-economic, built environmental and human mobility characteristics? In our new work, we study criminal activity in Bogotá, Boston, Chicago and Los Angeles) <LINK> <LINK>', 'Existing literature focuses on a limited number of factors, in a single city, and often describe crimes at the neighbourhood level. These limitations result in a fragmented and incomplete picture of the factors correlated with crime, and limit the impact of the conclusions.', 'We instead study criminal activity at the level of blocks, and we jointly use census surveys, geographical data and mobile phone data to describe and predict crime in four very different cities with respect to cultural, urban and socio-economic conditions. https://t.co/UQTgKsqLTj', 'We use a Bayesian Spatial Filtered Negative Binomial regression to predict crime, taking into account model uncertainty, spatial auto-correlation and overdispersion of data.', 'Our results show that the variability of the dynamics and history of each city poses a challenge to the existence of a model that ""fits it all"", able to learn from one city and to predict on another one. https://t.co/XaIvbSwHT4', 'We found that different theories often seen as competing can complement each other in models that take into account the socio-economic, built environment and mobility conditions together. https://t.co/3oF2GXFUUc', 'And much more! https://t.co/QDj3cUWsHD (Feedbacks are welcome!) Thank you to all those who helped me on this. Particularly, my co-authors, @datapopalliance and Andrés Clavijo!', '@RobinMazumder @martikagv @brulepri @datapopalliance @yxu_mit @lucpappalard @DinoPedreschi @cecim @rschifan @danielequercia Hi Robin, thanks! We computed it from mobile phone data. Attractiveness is the number of trips towards a neighbourhood, from people coming from different neighbourhoods, and for purposes different from going to work and home locations :)', '@SRoyLee @martikagv @brulepri @datapopalliance @yxu_mit @lucpappalard @DinoPedreschi @cecim @rschifan @danielequercia Thank you, Roy! In this paper, we claim it is very difficult to suggest universal recommendations to reduce crimes. Each city has its own story. Academically, there might be space for a paper for each city, but a bit of caution and some crime and local experts are needed :))', ""@RobinMazumder @martikagv @brulepri @datapopalliance @yxu_mit @lucpappalard @DinoPedreschi @cecim @rschifan @danielequercia So we borrowed the idea from the literature (vaguely) defined), but you might be interested in some computational methods such as the work of @danielequercia (https://t.co/YZTd2qXpCQ), our https://t.co/MytOQ9D6tO, and @cesifoti's work https://t.co/shkxxEJedz""]",https://arxiv.org/abs/2004.05822,"Nowadays, 23% of the world population lives in multi-million cities. In these metropolises, criminal activity is much higher and violent than in either small cities or rural areas. Thus, understanding what factors influence urban crime in big cities is a pressing need. Mainstream studies analyse crime records through historical panel data or analysis of historical patterns combined with ecological factor and exploratory mapping. More recently, machine learning methods have provided informed crime prediction over time. However, previous studies have focused on a single city at a time, considering only a limited number of factors (such as socio-economical characteristics) and often at large spatial units. Hence, our understanding of the factors influencing crime across cultures and cities is very limited. Here we propose a Bayesian model to explore how crime is related not only to socio-economic factors but also to the built environmental (e.g. land use) and mobility characteristics of neighbourhoods. To that end, we integrate multiple open data sources with mobile phone traces and compare how the different factors correlate with crime in diverse cities, namely Boston, Bogot\'a, Los Angeles and Chicago. We find that the combined use of socio-economic conditions, mobility information and physical characteristics of the neighbourhood effectively explain the emergence of crime, and improve the performance of the traditional approaches. However, we show that the socio-ecological factors of neighbourhoods relate to crime very differently from one city to another. Thus there is clearly no ""one fits all"" model. ","Socio-economic, built environment, and mobility conditions associated
  with crime: A study of multiple cities"
216,1248156986191024128,794145137068830720,Potestio Lab,"['There are many ways to construct a coarse-grained protein, here we tell you how to find out the best one!\n\nAt the X-ing of stat physics, biology, and information theory - a great\xa0collaboration between @r_potestio lab @UniTrento and Scott Shell @UCSBChE!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2004.03988,"In the theoretical modelling of a physical system a crucial step consists in the identification of those degrees of freedom that enable a synthetic, yet informative representation of it. While in some cases this selection can be carried out on the basis of intuition and experience, a straightforward discrimination of the important features from the negligible ones is difficult for many complex systems, most notably heteropolymers and large biomolecules. We here present a thermodynamics-based theoretical framework to gauge the effectiveness of a given simplified representation by measuring its information content. We employ this method to identify those reduced descriptions of proteins, in terms of a subset of their atoms, that retain the largest amount of information from the original model; we show that these highly informative representations share common features that are intrinsically related to the biological properties of the proteins under examination, thereby establishing a bridge between protein structure, energetics, and function. ","An information theory-based approach for optimal model reduction of
  biomolecules"
217,1248054830880501761,1239937954778415111,Wes Pegden,"['In our new preprint with @ChikinaLab, we use an SIR-like model accounting for known age-contact patterns in the U.S. to examine the benefits of age-targeted mitigation strategies:\n<LINK>\nWe find a large benefit from mitigations which are age-targeted AND strict.', '@ArielProcaccia @ChikinaLab Definitely related!  Some qualitative differences: they model ""mitigations forever""; that is, their models don\'t end at steady state.  They also don\'t model the scenarios which work the best in our models, which are strict restrictions applying to several but not all age groups.']",https://arxiv.org/abs/2004.04144,"We use a simple SIR-like epidemic model which integrates known age-contact patterns for the United States to model the effect of age-targeted mitigation strategies for a COVID-19-like epidemic. We find that, among strategies which end with population immunity, strict age-targeted mitigation strategies have the potential to greatly reduce mortalities and ICU utilization for natural parameter choices. ",Modeling strict age-targeted mitigation strategies for COVID-19
218,1247815857268568065,875856356,giacomortona,['Paper out - Measuring the Higgs self-coupling at FCC-hh! <LINK> Now I will relax and wait 40 years to find out if we were right 😀'],https://arxiv.org/abs/2004.03505,"Higgs pair production provides a unique handle for measuring the strength of the Higgs self interaction and constraining the shape of the Higgs potential. Among the proposed future facilities, a circular 100 TeV proton-proton collider would provide the most precise measurement of this crucial quantity. In this work, we perform a detailed analysis of the most promising decay channels and derive the expected sensitivity of their combination, assuming an integrated luminosity of 30 ab$^{-1}$. Depending on the assumed systematic uncertainties, we observe that the Higgs self-coupling will be measured with a precision in the range 3.4 - 7.8% at 68% confidence level. ","Measuring the Higgs self-coupling via Higgs-pair production at a 100 TeV
  p-p collider"
219,1247800993862938624,856802303533305856,Dr Aaron Jones,['Did you join us at the @royalsociety #summerscience fair in 2017 and play with our @arduino based model Gravitational Wave detector? Or visit our @thinktankmuseum exhibit? Wondered what we used your survey information for? Find out <LINK> <LINK>'],https://arxiv.org/abs/2004.03052,"In 2015 the first observation of gravitational waves marked a breakthrough in astrophysics, and in technological research and development. The discovery of a gravitational-wave signal from the collision of two black holes, a billion light-years away, received considerable interest from the media and public. We describe the development of a purpose-built exhibit explaining this new area of research to a general audience. The core element of the exhibit is a working Michelson interferometer: a scaled-down version of the key technology used in gravitational-wave detectors. The Michelson interferometer is integrated into a hands-on exhibit, which allows for user interaction and simulated gravitational-wave observations. An interactive display provides a self-guided explanation of gravitational-wave-related topics through video, animation, images and text. We detail the hardware and software used to create the exhibit and discuss two installation variants: an independent learning experience in a museum setting (the Thinktank Birmingham Science Museum), and a science-festival with the presence of expert guides (the 2017 Royal Society Summer Science Exhibition). We assess audience reception in these two settings, describe the improvements we have made given this information, and discuss future public-engagement projects resulting from this work. The exhibit is found to be effective in communicating the new and unfamiliar field of gravitational-wave research to general audiences. An accompanying website provides parts lists and information for others to build their own version of this exhibit. ",An Interactive Gravitational-Wave Detector Model for Museums and Fairs
220,1247582666079248388,2811702236,Sebastian Pucher,"['We published our work on imaging and localizing individual atoms close to our #nanofiber on #arXiv. Below you can see images of single atoms. Here, you find more details: <LINK> <LINK>']",https://arxiv.org/abs/2004.02303,"Single particle-resolved fluorescence imaging is an enabling technology in cold-atom physics. However, so far, this technique was not available for nanophotonic atom-light interfaces. Here, we image single atoms that are trapped and optically interfaced using an optical nanofiber. Near-resonant light is scattered off the atoms and imaged while counteracting heating mechanisms via degenerate Raman cooling. We detect trapped atoms within 150 ms and record image sequences of given atoms. Building on our technique, we perform two experiments which are conditioned on the number and position of the nanofiber-trapped atoms. We measure the transmission of nanofiber-guided resonant light and verify its exponential scaling in the few-atom limit, in accordance with Beer-Lambert's law. Moreover, depending on the interatomic distance, we observe interference of the fields that two simultaneously trapped atoms emit into the nanofiber. The demonstrated technique enables post-selection and possible feedback schemes and thereby opens the road towards a new generation of experiments in quantum nanophotonics. ","Imaging and localizing individual atoms interfaced with a nanophotonic
  waveguide"
221,1247441503565348865,776322668509429761,Svenja Boberg,"['Our study on “Pandemic Populism”  is out today with my awesome colleagues @Kudusch, @thorstenquandt &amp; @Lenafrescamente! We analyzed Corona-related Fb-posts of alternative news media and the actors, topics, fake news and conspiracy theories they addressed: <LINK>']",https://arxiv.org/abs/2004.02566,"The COVID-19 pandemic has not only had severe political, economic, and societal effects, it has also affected media and communication systems in unprecedented ways. While traditional journalistic media has tried to adapt to the rapidly evolving situation, alternative news media on the Internet have given the events their own ideological spin. Such voices have been criticized for furthering societal confusion and spreading potentially dangerous ""fake news"" or conspiracy theories via social media and other online channels. The current study analyzes the factual basis of such fears in an initial computational content analysis of alternative news media's output on Facebook during the early Corona crisis, based on a large German data set from January to the second half of March 2020. Using computational content analysis, methods, reach, interactions, actors, and topics of the messages were examined, as well as the use of fabricated news and conspiracy theories. The analysis revealed that the alternative news media stay true to message patterns and ideological foundations identified in prior research. While they do not spread obvious lies, they are predominantly sharing overly critical, even anti-systemic messages, opposing the view of the mainstream news media and the political establishment. With this pandemic populism, they contribute to a contradictory, menacing, and distrusting worldview, as portrayed in detail in this analysis. ","Pandemic Populism: Facebook Pages of Alternative News Media and the
  Corona Crisis -- A Computational Content Analysis"
222,1247441187759427585,2695714700,René Heller,"['#Exomoons could be detectable if they are in orbit around a transiting planet. In our new study we show how apparent variations of the transit depth of an #exoplanet could be caused by an exomoon. <LINK> (Rodenbeck, Heller, Gizon A&amp;A accepted). @PLATOMissionCon <LINK>']",https://arxiv.org/abs/2004.02259,"While the solar system contains about 20 times more moons than planets, no moon has been confirmed around any of the thousands of extrasolar planets known so far. Tools for an uncomplicated identification of the most promising exomoon candidates could be beneficial to streamline follow-up studies.} Here we study three exomoon indicators that emerge if well-established planet-only models are fitted to a planet-moon transit light curve: transit timing variations (TTVs), transit duration variations (TDVs), and apparent planetary transit radius variations (TRVs). We re-evaluate under realistic conditions the previously proposed exomoon signatures in the TTV and TDV series. We simulate light curves of a transiting exoplanet with a single moon. These model light curves are then fitted with a planet-only transit model, pretending there were no moon, and we explore the resulting TTV, TDV, and TRV series for evidence of the moon. The previously described ellipse in the TTV-TDV diagram of an exoplanet with a moon emerges only for high-density moons. Low-density moons distort the sinusoidal shapes of the TTV and the TDV series due to their photometric contribution to the combined planet-moon transit. Sufficiently large moons can produce periodic apparent TRVs of their host planets that could be observable. We find that Kepler and PLATO have similar performances in detecting the exomoon-induced TRV effect around simulated bright ($m_V=8$) stars. These stars, however, are rare in the Kepler sample but will be abundant in the PLATO sample. Moreover, PLATO's higher cadence yields a stronger TTV signal. The periodogram of the sequence of transit radius measurements can indicate the presence of a moon. The TTV and TDV series of exoplanets with moons can be more complex than previously assumed. We propose that TRVs could be a more promising means to identify exomoons in large exoplanet surveys. ",Exomoon indicators in high-precision transit light curves
223,1247422666891460608,14650984,marcozennaro,"['Have you witnessed very long LoRa links, well beyond the line of sight? We developed TROPPO LoRa, a platform to study tropospheric propagation effects using TTN. <LINK> <LINK> @thethingsntwrk @pycomIOT  #IoT #IoT4D #TTN #LoRaWan', '@less_sebastian @nestorayuso @thethingsntwrk @pycomIOT with your indexes presented in the paper you can find out if it’s tropo ducting or super-refractivity. Radiosondes provide daily data for CPH!']",https://arxiv.org/abs/2004.02802,"With the growth of LoRa deployments there are plenty of anecdotal reports of very long wireless links, well beyond the line of sight. Most reports suggest that these links are related to anomalous tropospheric propagation. We developed a platform to study tropospheric links based on TheThingsNetwork, a popular LoRaWAN-based infrastructure. We present some preliminary results and call for the IoT community to participate in this radio propagation experiment. ",TROPPO LoRa: TROPospheric Personal Observatory using LoRa signals
224,1247105478410358790,2438508702,Carlos Gracia,"['Our last work in arXiv, with @cosnet_bifi &amp; Chengyi. Relying on experimental findings, we study cooperation via second-order reputation and memory. We show, analytically and numerically, that intolerance promotes cooperation under the Simple Standing rule. <LINK> <LINK>']",https://arxiv.org/abs/2004.01480,"The understanding of cooperative behavior in social systems has been the subject of intense research over the past decades. In this regard, the theoretical models used to explain cooperation in human societies have been complemented with a growing interest in experimental studies to validate the proposed mechanisms. In this work, we rely on previous experimental findings to build a theoretical model based on two cooperation driving mechanisms: second-order reputation and memory. Specifically, taking the Donation Game as a starting point, the agents are distributed among three strategies, namely Unconditional Cooperators, Unconditional Defectors, and Discriminators, where the latter follow a second-order assessment rule: Shunning, Stern Judging, Image Scoring, or Simple Standing. A discriminator will cooperate if the evaluation of the recipient's last actions contained in his memory is above a threshold of (in)tolerance. In addition to the dynamics inherent to the game, another imitation dynamics, involving much longer times (generations), is introduced. The model is approached through a mean-field approximation that predicts the macroscopic behavior observed in Monte Carlo simulations. We found that, while in most second-order assessment rules, intolerance hinders cooperation, it has the opposite (positive) effect under the Simple Standing rule. Furthermore, we show that, when considering memory, the Stern Judging rule shows the lowest values of cooperation, while stricter rules show higher cooperation levels. ","Effect of memory, intolerance and second-order reputation on cooperation"
225,1246007439239131136,626859202,Kevin Pimbblet,"['New #physics #research out today led by @theastrojim - <LINK>\nBy cross matching optical #GAMA data with early Square Kilometre Array #SKA radio data we find a rather rare, very gas rich early type galaxy.']",https://arxiv.org/abs/2004.00847,"We present early science results from the First Large Absorption Survey in HI (FLASH), a spectroscopically blind survey for 21-cm absorption lines in cold hydrogen HI gas at cosmological distances using the Australian Square Kilometre Array Pathfinder (ASKAP). We have searched for HI absorption towards 1253 radio sources in the GAMA 23 field, covering redshifts between $z = 0.34$ and $0.79$ over a sky area of approximately 50 deg$^{2}$. In a purely blind search we did not obtain any detections of 21-cm absorbers above our reliability threshold. Assuming a fiducial value for the HI spin temperature of $T_{\rm spin}$ = 100 K and source covering fraction $c_{\rm f} = 1$, the total comoving absorption path length sensitive to all Damped Lyman $\alpha$ Absorbers (DLAs; $N_{\rm HI} \geq 2 \times 10^{20}$ cm$^{-2}$) is $\Delta{X} = 6.6 \pm 0.3$ ($\Delta{z} = 3.7 \pm 0.2$) and super-DLAs ($N_{\rm HI} \geq 2 \times 10^{21}$ cm$^{-2}$) is $\Delta{X} = 111 \pm 6$ ($\Delta{z} = 63 \pm 3$). We estimate upper limits on the HI column density frequency distribution function that are consistent with measurements from prior surveys for redshifted optical DLAs, and nearby 21-cm emission and absorption. By cross matching our sample of radio sources with optical spectroscopic identifications of galaxies in the GAMA 23 field, we were able to detect 21-cm absorption at $z = 0.3562$ towards NVSS J224500$-$343030, with a column density of $N_{\rm HI} = (1.2 \pm 0.1) \times 10^{20} (T_{\rm spin}/100~\mathrm{K})$ cm$^{-2}$. The absorber is associated with GAMA J22450.05$-$343031.7, a massive early-type galaxy at an impact parameter of 17 kpc with respect to the radio source and which may contain a massive ($M_{\rm HI} \gtrsim 3 \times 10^{9}$ M$_{\odot}$) gas disc. Such gas-rich early types are rare, but have been detected in the nearby Universe. ","FLASH Early Science -- Discovery of an intervening HI 21-cm absorber
  from an ASKAP survey of the GAMA 23 field"
226,1245748659603603461,962748619194617856,Alan Morningstar,['New preprint: We find and study a phase transition between thermalizing and frozen phases in a system with the fracton-like conservation laws of total charge and dipole moment. <LINK>'],https://arxiv.org/abs/2004.00096,"We study a stochastic lattice gas of particles in one dimension with strictly finite-range interactions that respect the fracton-like conservation laws of total charge and dipole moment. As the charge density is varied, the connectivity of the system's charge configurations under the dynamics changes qualitatively. We find two distinct phases: Near half filling the system thermalizes subdiffusively, with almost all configurations belonging to a single dynamically connected sector. As the charge density is tuned away from half filling there is a phase transition to a frozen phase where locally active finite bubbles cannot exchange particles and the system fails to thermalize. The two phases exemplify what has recently been referred to as weak and strong Hilbert space fragmentation, respectively. We study the static and dynamic scaling properties of this weak-to-strong fragmentation phase transition in a kinetically constrained classical Markov circuit model, obtaining some conjectured exact critical exponents. ","Kinetically constrained freezing transition in a dipole-conserving
  system"
227,1245634642146820097,384080522,Dr. Daniela Castro-Camilo,"['New paper 📰: <LINK>\nWe propose a general method for probabilistic prediction of extreme hot-spots in a spatio-temporal setting, with an application to the Sea Surface Temperature anomalies provided in a data challenge. We were late 🙁but got the best score 😌', 'Codes to implement the models available here: https://t.co/ihm3vHXoMl']",https://arxiv.org/abs/2004.00386,"We develop a method for probabilistic prediction of extreme value hot-spots in a spatio-temporal framework, tailored to big datasets containing important gaps. In this setting, direct calculation of summaries from data, such as the minimum over a space-time domain, is not possible. To obtain predictive distributions for such cluster summaries, we propose a two-step approach. We first model marginal distributions with a focus on accurate modeling of the right tail and then, after transforming the data to a standard Gaussian scale, we estimate a Gaussian space-time dependence model defined locally in the time domain for the space-time subregions where we want to predict. In the first step, we detrend the mean and standard deviation of the data and fit a spatially resolved generalized Pareto distribution to apply a correction of the upper tail. To ensure spatial smoothness of the estimated trends, we either pool data using nearest-neighbor techniques, or apply generalized additive regression modeling. To cope with high space-time resolution of data, the local Gaussian models use a Markov representation of the Mat\'ern correlation function based on the stochastic partial differential equations (SPDE) approach. In the second step, they are fitted in a Bayesian framework through the integrated nested Laplace approximation implemented in R-INLA. Finally, posterior samples are generated to provide statistical inferences through Monte-Carlo estimation. Motivated by the 2019 Extreme Value Analysis data challenge, we illustrate our approach to predict the distribution of local space-time minima in anomalies of Red Sea surface temperatures, using a gridded dataset (11315 days, 16703 pixels) with artificially generated gaps. In particular, we show the improved performance of our two-step approach over a purely Gaussian model without tail transformations. ","Bayesian space-time gap filling for inference on extreme hot-spots: an
  application to Red Sea surface temperatures"
228,1245604768795381761,1098159698262589440,Marco Pleines,"['We just got our paper done and submitted about the Obstacle Tower Challenge (@awjuliani) where our approach ranked 7th. We further study generalization by training on 3 visual themes, while testing on the 2 left-out ones.\n\n<LINK>']",http://arxiv.org/abs/2004.00567,"The Obstacle Tower Challenge is the task to master a procedurally generated chain of levels that subsequently get harder to complete. Whereas the most top performing entries of last year's competition used human demonstrations or reward shaping to learn how to cope with the challenge, we present an approach that performed competitively (placed 7th) but starts completely from scratch by means of Deep Reinforcement Learning with a relatively simple feed-forward deep network structure. We especially look at the generalization performance of the taken approach concerning different seeds and various visual themes that have become available after the competition, and investigate where the agent fails and why. Note that our approach does not possess a short-term memory like employing recurrent hidden states. With this work, we hope to contribute to a better understanding of what is possible with a relatively simple, flexible solution that can be applied to learning in environments featuring complex 3D visual input where the abstract task structure itself is still fairly simple. ","Obstacle Tower Without Human Demonstrations: How Far a Deep Feed-Forward
  Network Goes with Reinforcement Learning"
229,1255657923797225472,1060569630857674752,Alysson Bessani,"[""For me, it is clear that we'll have a world with many blockchains, instead of a single blockchain ruling the world. Our recent paper (Smart Contracts on the Move, to appear on DSN'20) propose a mechanism to move assets and contracts between blockchains: <LINK>""]",https://arxiv.org/abs/2004.05933,"Blockchain systems have received much attention and promise to revolutionize many services. Yet, despite their popularity, current blockchain systems exist in isolation, that is, they cannot share information. While interoperability is crucial for blockchain to reach widespread adoption, it is difficult to achieve due to differences among existing blockchain technologies. This paper presents a technique to allow blockchain interoperability. The core idea is to provide a primitive operation to developers so that contracts and objects can switch from one blockchain to another, without breaking consistency and violating key blockchain properties. To validate our ideas, we implemented our protocol in two popular blockchain clients that use the Ethereum virtual machine. We discuss how to build applications using the proposed protocol and show examples of applications based on real use cases that can move across blockchains. To analyze the system performance we use a real trace from one of the most popular Ethereum applications and replay it in a multi-blockchain environment. ",Smart Contracts on the Move
230,1253321924060602368,778246363175907328,Xinyi Wang (Cindy),"['Multilingual training requires careful data sampling to handle imbalanced datasets. But how can we find the optimal data sampling strategy? Our #Acl2020 paper proposes MultiDDS, an algorithm that automatically learns to maximize accuracy on all languages: <LINK> <LINK>', 'MultiDDS optimizes a distribution over training datasets by unweighting the language that has similar gradient with the gradient of all dev languages. Experiments show that it outperforms the popular heuristic data sampling schedule under a variety of multilingual NMT setting. https://t.co/hjSvVNtmfl', 'MultiDDS also allows flexible control over the performance of which languages are optimized. We can define different dev set aggregation methods to reflect the desired optimization priority, such as prioritizing low-performing languages. https://t.co/Ev6kTD8XmA', 'Joint work with Yulia Tsvetkov and Graham Neubig(@gneubig)!']",https://arxiv.org/abs/2004.06748,"When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized. ",Balancing Training for Multilingual Neural Machine Translation
231,1251097917965897728,345288810,Lorenzo Lucchini,"['Code shapes our society! Here we study the co-development of cryptocurrency projects on @github in relation to their market behaviour.\nOur result? \n<LINK> <LINK>', '1. \nLots of cryptocurrency projects share their code on GitHub. \nSome developers (~4%) work on more than 1 crypto-project. A network of projects connected by co-developer is revealed. Each link represents a potential genotypical relation among two cryptos. https://t.co/L2d6rXxQN5', '2.\nEach link (between two co-developed cryptos) is created at the time of the first edit on the second cryptocurrency project. \nLinked cryptocurrencies show a synchronization of their returns (their comovement increases) at the turn of the connection time. https://t.co/ykoMLJGSZQ', '3. \n""Why? Hard to say Why!""\n\nFor more details and a more detailed discussion look at the paper: https://t.co/NrnQ8BtSHe', '@francescfont @a_baronca @lau_retti @brulepri @DrAngelaGallo There is no short answer to your question. Random pairs are sampled without considering any potential connection strategy while this does not necessarily hold true for linked pairs. Have a look at Fig.4. We find different market features comparing random with linked pairs.', '@francescfont @a_baronca @lau_retti @brulepri @DrAngelaGallo Moreover, from the network structure, you can see how Bitcoin and Ethereum have a central role in the web of linked pairs. Random pairs, in contrast, create a random graph, where no crypto is more ""important"" than others.']",https://arxiv.org/abs/2004.07290,"""Code is law"" is the funding principle of cryptocurrencies. The security, transferability, availability and other properties of a crypto-asset are determined by the code through which it is created. If code is open source, as it happens for most cryptocurrencies, this principle would prevent manipulations and grant transparency to users and traders. However, this approach considers cryptocurrencies as isolated entities thus neglecting possible connections between them. Here, we show that 4% of developers contribute to the code of more than one cryptocurrency and that the market reflects these cross-asset dependencies. In particular, we reveal that the first coding event linking two cryptocurrencies through a common developer leads to the synchronisation of their returns in the following months. Our results identify a clear link between the collaborative development of cryptocurrencies and their market behaviour. More broadly, our work reveals a so-far overlooked systemic dimension for the transparency of code-based ecosystems and we anticipate it will be of interest to researchers, investors and regulators. ","From code to market: Network of developers and correlated returns of
  cryptocurrencies"
