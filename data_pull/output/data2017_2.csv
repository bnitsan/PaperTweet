,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,841535146620788736,955602606,Daniele Avitabile,"['Our latest paper on the existence of a new type of coherent structure, spatio-temporal canards, is now on ArXiv <LINK>']",http://arxiv.org/abs/1702.00079,"Canards are special solutions to ordinary differential equations that follow invariant repelling slow manifolds for long time intervals. In realistic biophysical single cell models, canards are responsible for several complex neural rhythms observed experimentally, but their existence and role in spatially-extended systems is largely unexplored. We describe a novel type of coherent structure in which a spatial pattern displays temporal canard behaviour. Using interfacial dynamics and geometric singular perturbation theory, we classify spatio-temporal canards and give conditions for the existence of folded-saddle and folded-node canards. We find that spatio-temporal canards are robust to changes in the synaptic connectivity and firing rate. The theory correctly predicts the existence of spatio-temporal canards with octahedral symmetries in a neural field model posed on the unit sphere. ",Spatio-temporal canards in neural field equations
1,839273668416946177,359622624,Been Kim,"['What is interpretable ML and how can we evaluate it? New paper with Finale Doshi-velez "" <LINK>', '@jejomath thank you Jesse for great feedback on writing this paper!:)']",https://arxiv.org/abs/1702.08608,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning. ",Towards A Rigorous Science of Interpretable Machine Learning
2,838425751489757185,3199605543,Afonso S. Bandeira,['New paper about Multireference Alignment <LINK> solves part 2 of Open Problem 10.4. in <LINK>'],https://arxiv.org/abs/1702.08546,"In this paper, we establish optimal rates of adaptive estimation of a vector in the multi-reference alignment model, a problem with important applications in fields such as signal processing, image processing, and computer vision, among others. We describe how this model can be viewed as a multivariate Gaussian mixture model under the constraint that the centers belong to the orbit of a group. This enables us to derive matching upper and lower bounds that feature an interesting dependence on the signal-to-noise ratio of the model. Both upper and lower bounds are articulated around a tight local control of Kullback-Leibler divergences that showcases the central role of moment tensors in this problem. ",Optimal rates of estimation for multi-reference alignment
3,837556016677998592,5654412,Konstantin Berlin,['Please checkout @joshua_saxe and my new paper on featureless malware detection <LINK>'],https://arxiv.org/abs/1702.08568,"For years security machine learning research has promised to obviate the need for signature based detection by automatically learning to detect indicators of attack. Unfortunately, this vision hasn't come to fruition: in fact, developing and maintaining today's security machine learning systems can require engineering resources that are comparable to that of signature-based detection systems, due in part to the need to develop and continuously tune the ""features"" these machine learning systems look at as attacks evolve. Deep learning, a subfield of machine learning, promises to change this by operating on raw input signals and automating the process of feature design and extraction. In this paper we propose the eXpose neural network, which uses a deep learning approach we have developed to take generic, raw short character strings as input (a common case for security inputs, which include artifacts like potentially malicious URLs, file paths, named pipes, named mutexes, and registry keys), and learns to simultaneously extract features and classify using character-level embeddings and convolutional neural network. In addition to completely automating the feature design and extraction process, eXpose outperforms manual feature extraction based baselines on all of the intrusion detection problems we tested it on, yielding a 5%-10% detection rate gain at 0.1% false positive rate compared to these baselines. ","eXpose: A Character-Level Convolutional Neural Network with Embeddings
  For Detecting Malicious URLs, File Paths and Registry Keys"
4,836843385579266049,75249390,Axel Maas,"['We have published a new paper on the structure of a #neutronstar in a world similar to our own at <LINK> #np3', 'Why only a similar world? I wrote a blog entry about this at https://t.co/014aCi6JJG - and why #neutronstars at https://t.co/nVDq35zw7y #np3']",https://arxiv.org/abs/1702.08724,"The inner structure of neutron stars is still an open question. To make progress and understand the qualitative impact of gauge interactions on the neutron star structure we study neutron stars in a modified version of QCD. In this modification the gauge group of QCD is replaced by the exceptional Lie group G$_2$, which has neutrons and is accessible at finite density in lattice calculations. Using an equation of state constructed from lattice calculations we determine the mass-radius-relation for a neutron star in this theory using the Tolman-Oppenheimer-Volkoff equation. The results exhibit an influence of the non-trivial interactions on the mass-radius relation. However, the masses of the quarks are found to have little influence. We also give density profiles and the phase structure inside the neutron star. If the results carry over to full QCD, much of the internal structure of neutron stars could already be inferred from a precise measurement of the mass-radius relation. ",Constructing a neutron star in G2-QCD
5,836745138038784004,755852802446897152,Zeeshan Ahmad,"['New paper on electrodeposition stability at solid-solid interfaces, with applications to solid-state #batteries <LINK>']",https://arxiv.org/abs/1702.08406,"We generalize the conditions for stable electrodeposition at isotropic solid-solid interfaces using a kinetic model which incorporates the effects of stresses and surface tension at the interface. We develop a stability diagram that shows two regimes of stability: previously known pressure-driven mechanism and a new density-driven stability mechanism that is governed by the relative density of metal in the two phases. We show that inorganic solids and solid polymers generally do not lead to stable electrodeposition, and provide design guidelines for achieving stable electrodeposition. ","Stability of Electrodeposition at Solid-Solid Interfaces and
  Implications for Metal Anodes"
6,836614995291045890,4758937001,Duncan Christie,['New paper out with Ben Wu!\n\nGMC Collisions as Triggers of Star Formation III. \n\n<LINK> <LINK>'],https://arxiv.org/abs/1702.08117,"We study giant molecular cloud (GMC) collisions and their ability to trigger star cluster formation. We further develop our three dimensional magnetized, turbulent, colliding GMC simulations by implementing star formation sub-grid models. Two such models are explored: (1) ""Density-Regulated,"" i.e., fixed efficiency per free-fall time above a set density threshold; (2) ""Magnetically-Regulated,"" i.e., fixed efficiency per free-fall time in regions that are magnetically supercritical. Variations of parameters associated with these models are also explored. In the non-colliding simulations, the overall level of star formation is sensitive to model parameter choices that relate to effective density thresholds. In the GMC collision simulations, the final star formation rates and efficiencies are relatively independent of these parameters. Between non-colliding and colliding cases, we compare the morphologies of the resulting star clusters, properties of star-forming gas, time evolution of the star formation rate (SFR), spatial clustering of the stars, and resulting kinematics of the stars in comparison to the natal gas. We find that typical collisions, by creating larger amounts of dense gas, trigger earlier and enhanced star formation, resulting in 10 times higher SFRs and efficiencies. The star clusters formed from GMC collisions show greater spatial sub-structure and more disturbed kinematics. ","GMC Collisions as Triggers of Star Formation. III. Density and
  Magnetically Regulated Star Formation"
7,836523923852271616,30420963,Eivind Eriksen,['Our new paper is on arXiv: <LINK> <LINK>'],http://arxiv.org/abs/1702.07645#,"We consider the algebra $\mathcal O(\mathsf M)$ of observables and the (formally) versal morphism $\eta: A \to \mathcal O(\mathsf M)$ defined by the noncommutative deformation functor $\mathsf{Def}_{\mathsf M}$ of a family $\mathsf M = \{ M_1, \dots, M_r \}$ of right modules over an associative $k$-algebra $A$. By the Generalized Burnside Theorem, due to Laudal, $\eta$ is an isomorphism when $A$ is finite dimensional, $\mathsf M$ is the family of simple $A$-modules, and $k$ is an algebraically closed field. The purpose of this paper is twofold: First, we prove a form of the Generalized Burnside Theorem that is more general, where there is no assumption on the field $k$. Secondly, we prove that the $\mathcal O$-construction is a closure operation when $A$ is any finitely generated $k$-algebra and $\mathsf M$ is any family of finite dimensional $A$-modules, in the sense that $\eta_B: B \to \mathcal O^B(\mathsf M)$ is an isomorphism when $B = \mathcal O(\mathsf M)$ and $\mathsf M$ is considered as a family of $B$-modules. ",The algebra of observables in noncommutative deformation theory
8,836308608509849600,362160713,Michael Soltys,['A new paper on normalization of inconsistency indicators <LINK>'],https://arxiv.org/abs/1702.07205,"In this study, we provide mathematical and practice-driven justification for using $[0,1]$ normalization of inconsistency indicators in pairwise comparisons. The need for normalization, as well as problems with the lack of normalization, are presented. A new type of paradox of infinity is described. ",On normalization of inconsistency indicators in pairwise comparisons
9,836169582981304320,2974498451,Dr. Monika Moscibrodzka,['Our new paper about Event Horizon Telescope image reconstruction techniques is out:  <LINK>'],https://arxiv.org/abs/1702.07361,"We propose a new imaging technique for radio and optical/infrared interferometry. The proposed technique reconstructs the image from the visibility amplitude and closure phase, which are standard data products of short-millimeter very long baseline interferometers such as the Event Horizon Telescope (EHT) and optical/infrared interferometers, by utilizing two regularization functions: the $\ell_1$-norm and total variation (TV) of the brightness distribution. In the proposed method, optimal regularization parameters, which represent the sparseness and effective spatial resolution of the image, are derived from data themselves using cross validation (CV). As an application of this technique, we present simulated observations of M87 with the EHT based on four physically motivated models. We confirm that $\ell_1$+TV regularization can achieve an optimal resolution of $\sim 20-30$% of the diffraction limit $\lambda/D_{\rm max}$, which is the nominal spatial resolution of a radio interferometer. With the proposed technique, the EHT can robustly and reasonably achieve super-resolution sufficient to clearly resolve the black hole shadow. These results make it promising for the EHT to provide an unprecedented view of the event-horizon-scale structure in the vicinity of the super-massive black hole in M87 and also the Galactic center Sgr A*. ","Imaging the Schwarzschild-radius-scale Structure of M87 with the Event
  Horizon Telescope using Sparse Modeling"
10,835980572950417409,814382893573148672,Truyen Tran,"['Our new paper on multi-X learning, where X = label, view or instance: \n<LINK>\nAlso a gentle intro:\n<LINK>']",https://arxiv.org/abs/1702.07021,"Much recent machine learning research has been directed towards leveraging shared statistics among labels, instances and data views, commonly referred to as multi-label, multi-instance and multi-view learning. The underlying premises are that there exist correlations among input parts and among output targets, and the predictive performance would increase when the correlations are incorporated. In this paper, we propose Column Bundle (CLB), a novel deep neural network for capturing the shared statistics in data. CLB is generic that the same architecture can be applied for various types of shared statistics by changing only input and output handling. CLB is capable of scaling to thousands of input parts and output labels by avoiding explicit modeling of pairwise relations. We evaluate CLB on different types of data: (a) multi-label, (b) multi-view, (c) multi-view/multi-label and (d) multi-instance. CLB demonstrates a comparable and competitive performance in all datasets against state-of-the-art methods designed specifically for each type. ",One Size Fits Many: Column Bundle for Multi-X Learning
11,834934810527301632,50901426,Rafael Alves Batista,['My new paper: Squeezing #CosmicRays off of #Stars \n<LINK>  #Astrophysics #Science #BlackHoles'],https://arxiv.org/abs/1702.06978,"Ultra-high-energy cosmic rays (UHECRs) can be accelerated by tidal disruption events of stars by black holes. We suggest a novel mechanism for UHECR acceleration wherein white dwarfs (WDs) are tidally compressed by intermediate-mass black holes (IMBHs), leading to their ignition and subsequent explosion as a supernova. Cosmic rays accelerated by the supernova may receive an energy boost when crossing the accretion-powered jet. The rate of encounters between WDs and IMBHs can be relatively high, as the number of IMBHs may be substantially augmented once account is taken of their likely presence in dwarf galaxies. Here we show that this kind of tidal disruption event naturally provides an intermediate composition for the observed UHECRs, and suggest that dwarf galaxies and globular clusters are suitable sites for particle acceleration to ultra-high energies. ",Ultrahigh-energy cosmic rays from tidally-ignited white dwarfs
12,834699603354214400,2676457430,MAGIC telescopes üå¥üå∫,['A new MAGIC paper accepted in MNRAS! The first detection of VHE gamma-ray emission from the BL Lac  1ES 1741+196\n<LINK> <LINK>'],https://arxiv.org/abs/1702.06795,"We present the first detection of the nearby (z=0.084) low-luminosity BL Lac object 1ES 1741+196 in the very high energy (VHE: E$>$100 GeV) band. This object lies in a triplet of interacting galaxies. Early predictions had suggested 1ES 1741+196 to be, along with several other high-frequency BL Lac sources, within the reach of MAGIC detectability. Its detection by MAGIC, later confirmed by VERITAS, helps to expand the small population of known TeV BL Lacs. The source was observed with the MAGIC telescopes between 2010 April and 2011 May, collecting 46 h of good quality data. These observations led to the detection of the source at 6.0 $\sigma$ confidence level, with a steady flux $\mathrm{F}(> 100 {\rm GeV}) = (6.4 \pm 1.7_{\mathrm{stat}}\pm 2.6_{\mathrm{syst}}) \cdot 10^{-12}$ ph cm$^{-2}$ s$^{-1}$ and a differential spectral photon index $\Gamma = 2.4 \pm 0.2_{\mathrm{stat}} \pm 0.2_{\mathrm{syst}}$ in the range of $\sim$80 GeV - 3 TeV. To study the broad-band spectral energy distribution (SED) simultaneous with MAGIC observations, we use KVA, Swift/UVOT and XRT, and Fermi/LAT data. One-zone synchrotron-self-Compton (SSC) modeling of the SED of 1ES 1741+196 suggests values for the SSC parameters that are quite common among known TeV BL Lacs except for a relatively low Doppler factor and slope of electron energy distribution. A thermal feature seen in the SED is well matched by a giant elliptical's template. This appears to be the signature of thermal emission from the host galaxy, which is clearly resolved in optical observations. ","MAGIC detection of very high energy gamma-ray emission from the
  low-luminosity blazar 1ES 1741+196"
13,834592991314456576,22399655,Ryota Kanaiüí°,"['Our new paper on counterfactual control with generative models. This can induce policies on the fly for new tasks.  \n<LINK>', ""This paper is not directly about consciousness, but I'm also speculating that counterfactuals may give consciousness a function."", '@hardmaru @ngutten  as nicholas wrote, we encountered a new problem for more complex situations. it needs to learn self-other distinction.']",https://arxiv.org/abs/1702.06676,"We introduce a method by which a generative model learning the joint distribution between actions and future states can be used to automatically infer a control scheme for any desired reward function, which may be altered on the fly without retraining the model. In this method, the problem of action selection is reduced to one of gradient descent on the latent space of the generative model, with the model itself providing the means of evaluating outcomes and finding the gradient, much like how the reward network in Deep Q-Networks (DQN) provides gradient information for the action generator. Unlike DQN or Actor-Critic, which are conditional models for a specific reward, using a generative model of the full joint distribution permits the reward to be changed on the fly. In addition, the generated futures can be inspected to gain insight in to what the network 'thinks' will happen, and to what went wrong when the outcomes deviate from prediction. ",Counterfactual Control for Free from Generative Models
14,834121051613204481,460069521,Andrew Francis,"['Thanks to visit from Vincent Moulton and Katharina Huber in November, new paper just out on arxiv: <LINK>', 'The new paper is on the diameter of the space of #phylogenetic networks under NNI: how far apart can networks be? https://t.co/FQlxf9xb28', 'We also discuss versions of the subtree prune and regraft (SPR) and TBR moves for unrooted phylogenetic networks. https://t.co/FQlxf9xb28']",https://arxiv.org/abs/1702.05609,"Phylogenetic networks are a generalization of phylogenetic trees that allow for representation of reticulate evolution. Recently, a space of unrooted phylogenetic networks was introduced, where such a network is a connected graph in which every vertex has degree 1 or 3 and whose leaf-set is a fixed set $X$ of taxa. This space, denoted $\mathcal{N}(X)$, is defined in terms of two operations on networks -- the nearest neighbor interchange and triangle operations -- which can be used to transform any network with leaf set $X$ into any other network with that leaf set. In particular, it gives rise to a metric $d$ on $\mathcal N(X)$ which is given by the smallest number of operations required to transform one network in $\mathcal N(X)$ into another in $\mathcal N(X)$. The metric generalizes the well-known NNI-metric on phylogenetic trees which has been intensively studied in the literature. In this paper, we derive a bound for the metric $d$ as well as a related metric $d_{N\!N\!I}$ which arises when restricting $d$ to the subset of $\mathcal{N}(X)$ consisting of all networks with $2(|X|-1+i)$ vertices, $i \ge 1$. We also introduce two new metrics on networks -- the SPR and TBR metrics -- which generalize the metrics on phylogenetic trees with the same name and give bounds for these new metrics. We expect our results to eventually have applications to the development and understanding of network search algorithms. ",Bounds for phylogenetic network space metrics
15,834005515793489920,446634627,Roberto Santana,"['Take home message from my new paper <LINK> : When used with #word2vec,  genetic programming can solve word analogy tasks.']",https://arxiv.org/abs/1702.05624v1,"Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task. ","] Reproducing and learning new algebraic operations on word embeddings
  using genetic programming"
16,833959282274136066,129802464,Niall Deacon,['We‚Äôve got a new paper out today. Finding binaries with Pan-STARRS1 &amp; techniques developed to measure galaxy shapes \n\n<LINK>'],https://arxiv.org/abs/1702.05491,"Using shape measurement techniques developed for weak lensing surveys we have identified three new ultracool binaries in the Pan-STARRS1 survey. Binary companions which are not completely resolved can still alter the shapes of stellar images. These shape distortions can be measured if PSF anisotropy caused by the telescope is properly accounted for. We show using both a sample of known binary stars and simulated binaries that we can reliably recover binaries wider than around 0.3"" and with flux ratios greater than around 0.1. We then applied our method to a sample of ultracool dwarfs within 30pc with 293 objects having sufficient Pan-STARRS1 data for our method. In total we recovered all but one of the 11 binaries wider than 0.3"" in this sample. Our one failure was a true binary detected with a significant but erroneously high ellipticity which led it to be rejected in our analysis. We identify three new binaries, one a simultaneous discovery, with primary spectral types M6.5, L1 and T0.5. These latter two were confirmed with Keck/NIRC2 follow-up imaging. This technique will be useful for identifying large numbers of stellar and substellar binaries in the upcoming LSST and DES sky surveys. ",Identification of partially resolved binaries in Pan-STARRS1 data
17,833730868774580224,2861255332,Jozsef Vinko,['Our new paper on the lost hydrogen envelope around Type I supernovae:\n<LINK>'],https://arxiv.org/abs/1702.05143,"We report the first results from our long-term observational survey aimed at discovering late-time interaction between the ejecta of hydrogen-poor Type I supernovae and the hydrogen-rich envelope expelled from the progenitor star several decades/centuries before explosion. The expelled envelope, moving with a velocity of ~10 -- 100 km s$^{-1}$, is expected to be caught up by the fast-moving SN ejecta several years/decades after explosion depending on the history of the mass-loss process acting in the progenitor star prior to explosion. The collision between the SN ejecta and the circumstellar envelope results in net emission in the Balmer-lines, especially in H-alpha. We look for signs of late-time H-alpha emission in older Type Ia/Ibc/IIb SNe having hydrogen-poor ejecta, via narrow-band imaging. Continuum-subtracted H-alpha emission has been detected for 13 point sources: 9 SN Ibc, 1 SN IIb and 3 SN Ia events. Thirty-eight SN sites were observed on at least two epochs, from which three objects (SN 1985F, SN 2005kl, SN 2012fh) showed significant temporal variation in the strength of their H-alpha emission in our DIAFI data. This suggests that the variable emission is probably not due to nearby H II regions unassociated with the SN, and hence is an important additional hint that ejecta-CSM interaction may take place in these systems. Moreover, we successfully detected the late-time H-alpha emission from the Type Ib SN 2014C, which was recently discovered as a strongly interacting SN in various (radio, infrared, optical and X-ray) bands. ","Searching for the expelled hydrogen envelope in Type I supernovae via
  late-time H-alpha emission"
18,833597768996384768,292313052,Martin Kilbinger,"['New paper: <LINK>: Limber &amp;  flat-sky approximations of weak-lensing projections sufficient for current surveys.', '@energie_sombre For sub-percent accuract needed for Euclid/LSST/... we recommend 2nd-order Limber approximation.', '@energie_sombre Code available at https://t.co/j4LVZrNlZJ, version 2.7 released']",http://arxiv.org/abs/1702.05301,"We compute the spherical-sky weak-lensing power spectrum of the shear and convergence. We discuss various approximations, such as flat-sky, and first- and second- order Limber equations for the projection. We find that the impact of adopting these approximations is negligible when constraining cosmological parameters from current weak lensing surveys. This is demonstrated using data from the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS). We find that the reported tension with Planck Cosmic Microwave Background (CMB) temperature anisotropy results cannot be alleviated. For future large-scale surveys with unprecedented precision, we show that the spherical second-order Limber approximation will provide sufficient accuracy. In this case, the cosmic-shear power spectrum is shown to be in agreement with the full projection at the sub-percent level for l > 3, with the corresponding errors an order of magnitude below cosmic variance for all l. When computing the two-point shear correlation function, we show that the flat-sky fast Hankel transformation results in errors below two percent compared to the full spherical transformation. In the spirit of reproducible research, our numerical implementation of all approximations and the full projection are publicly available within the package nicaea at this http URL ",Precision calculations of the cosmic shear power spectrum projection
19,833596692465381377,232642215,Jukka Suomela,"['A new paper: LCL problems on grids, <LINK>\n\nSome slides: <LINK> <LINK>']",https://arxiv.org/abs/1702.05456,"LCLs or locally checkable labelling problems (e.g. maximal independent set, maximal matching, and vertex colouring) in the LOCAL model of computation are very well-understood in cycles (toroidal 1-dimensional grids): every problem has a complexity of $O(1)$, $\Theta(\log^* n)$, or $\Theta(n)$, and the design of optimal algorithms can be fully automated. This work develops the complexity theory of LCL problems for toroidal 2-dimensional grids. The complexity classes are the same as in the 1-dimensional case: $O(1)$, $\Theta(\log^* n)$, and $\Theta(n)$. However, given an LCL problem it is undecidable whether its complexity is $\Theta(\log^* n)$ or $\Theta(n)$ in 2-dimensional grids. Nevertheless, if we correctly guess that the complexity of a problem is $\Theta(\log^* n)$, we can completely automate the design of optimal algorithms. For any problem we can find an algorithm that is of a normal form $A' \circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for finding a maximal independent set in $k$th power of the grid, and $k$ is a constant. Finally, partially with the help of automated design tools, we classify the complexity of several concrete LCL problems related to colourings and orientations. ",LCL problems on grids
20,833596117724061696,1689286236,Goodwin Group Oxford,"[""Like a supramolecular mattress: @clobar's new paper on Au...Au helices! <LINK> @Matthew_R_Ryder @MMCLabOx @nowatchesnomaps""]",https://arxiv.org/abs/1702.05145,"We report the mechanical properties of the `giant' negative compressibility material zinc(II) dicyanoaurate, as determined using a combination of single-crystal nanoindentation measurements and \emph{ab initio} density functional theory calculations. While the elastic response of zinc dicyanoaurate is found to be intermediate to the behaviour of dense and open framework structures, we discover the material to exhibit a particularly strong elastic recovery, which is advantageous for a range of practical applications. We attribute this response to the existence of supramolecular helices that function as atomic-scale springs, storing mechanical energy during compressive stress and hence inhibiting plastic deformation. Our results are consistent with the relationship noted in [Cheng \& Cheng, \textit{Appl. Phys. Lett.}, 1998, {\textbf{73}}, 614] between the magnitude of elastic recovery, on the one hand, and the ratio of material hardness to Young's modulus, on the other hand. Drawing on comparisons with other metal--organic frameworks containing helical structure motifs, we suggest helices as an attractive supramolecular motif for imparting resistance to plastic deformation in the design of functional materials. ",Large elastic recovery of zinc dicyanoaurate
21,833280388571209728,4833028707,David Lederman,"['Our new review paper on #bioelectronics available at <LINK>. In collab w @WeizmannScience', 'Written at @ucsc @ucscphysics @WestVirginiaU @WeizmannScience']",http://arxiv.org/abs/1702.05028,"We review the status of protein-based molecular electronics. First we discuss fundamental concepts of electron transfer and transport in and across proteins and proposed mechanisms for these processes. We then describe the immobilization of proteins to solid-state surfaces in both nanoscale and macroscopic approaches, and highlight how different methodologies can alter protein electronic properties. Because immobilizing proteins while retaining biological activity is crucial to the successful development of bioelectronic devices, we discuss this process at length. We briefly discuss computational predictions and their link to experimental results. We then summarize how the biological activity of immobilized proteins is beneficial for bioelectronics devices, and how conductance measurements can shed light on protein properties. Finally, we consider how the research to date could influence the development of future bioelectronics devices. ",Protein bioelectronics: a review of what we do and do not know
22,832567255586177024,15421904,Lance Fortnow,['New paper: Compression Complexity <LINK>'],https://arxiv.org/abs/1702.04779,"The Kolmogorov complexity of x, denoted C(x), is the length of the shortest program that generates x. For such a simple definition, Kolmogorov complexity has a rich and deep theory, as well as applications to a wide variety of topics including learning theory, complexity lower bounds and SAT algorithms. Kolmogorov complexity typically focuses on decompression, going from the compressed program to the original string. This paper develops a dual notion of compression, the mapping from a string to its compressed version. Typical lossless compression algorithms such as Lempel-Ziv or Huffman Encoding always produce a string that will decompress to the original. We define a general compression concept based on this observation. For every m, we exhibit a single compression algorithm q of length about m which for n and strings x of length n >= m, the output of q will have length within n-m+O(1) bits of C(x). We also show this bound is tight in a strong way, for every n >= m there is an x of length n with C(x) about m such that no compression program of size slightly less than m can compress x at all. We also consider a polynomial time-bounded version of compression complexity and show that similar results for this version would rule out cryptographic one-way functions. ",Compression Complexity
23,832523697261015040,26716739,Mattias Villani,"[""New paper 'Tree Ensembles with Rule Structured Horseshoe Regularization' <LINK> <LINK>""]",https://arxiv.org/abs/1702.05008,"We propose a new Bayesian model for flexible nonlinear regression and classification using tree ensembles. The model is based on the RuleFit approach in Friedman and Popescu (2008) where rules from decision trees and linear terms are used in a L1-regularized regression. We modify RuleFit by replacing the L1-regularization by a horseshoe prior, which is well known to give aggressive shrinkage of noise predictor while leaving the important signal essentially untouched. This is especially important when a large number of rules are used as predictors as many of them only contribute noise. Our horseshoe prior has an additional hierarchical layer that applies more shrinkage a priori to rules with a large number of splits, and to rules that are only satisfied by a few observations. The aggressive noise shrinkage of our prior also makes it possible to complement the rules from boosting in Friedman and Popescu (2008) with an additional set of trees from random forest, which brings a desirable diversity to the ensemble. We sample from the posterior distribution using a very efficient and easily implemented Gibbs sampler. The new model is shown to outperform state-of-the-art methods like RuleFit, BART and random forest on 16 datasets. The model and its interpretation is demonstrated on the well known Boston housing data, and on gene expression data for cancer classification. The posterior sampling, prediction and graphical tools for interpreting the model results are implemented in a publicly available R package. ",Tree Ensembles with Rule Structured Horseshoe Regularization
24,832494701672534016,733640801914343425,Adam Santoro,['Our new paper on reasoning about relations (accepted at the ICLR Worksop): <LINK>'],https://arxiv.org/abs/1702.05068,"Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning. ","Discovering objects and their relations from entangled scene
  representations"
25,832440882779480066,21114887,Raphael Gottardo üìäüß™üñ•Ô∏è,['New paper for clustering flow cytometry data <LINK> using sequential DPM'],https://arxiv.org/abs/1702.04407,"Flow cytometry is a high-throughput technology used to quantify multiple surface and intracellular markers at the level of a single cell. This enables to identify cell sub-types, and to determine their relative proportions. Improvements of this technology allow to describe millions of individual cells from a blood sample using multiple markers. This results in high-dimensional datasets, whose manual analysis is highly time-consuming and poorly reproducible. While several methods have been developed to perform automatic recognition of cell populations, most of them treat and analyze each sample independently. However, in practice, individual samples are rarely independent (e.g. longitudinal studies). Here, we propose to use a Bayesian nonparametric approach with Dirichlet process mixture (DPM) of multivariate skew $t$-distributions to perform model based clustering of flow-cytometry data. DPM models directly estimate the number of cell populations from the data, avoiding model selection issues, and skew $t$-distributions provides robustness to outliers and non-elliptical shape of cell populations. To accommodate repeated measurements, we propose a sequential strategy relying on a parametric approximation of the posterior. We illustrate the good performance of our method on simulated data, on an experimental benchmark dataset, and on new longitudinal data from the DALIA-1 trial which evaluates a therapeutic vaccine against HIV. On the benchmark dataset, the sequential strategy outperforms all other methods evaluated, and similarly, leads to improved performance on the DALIA-1 data. We have made the method available for the community in the R package NPflow. ","Sequential Dirichlet Process Mixtures of Multivariate Skew
  t-distributions for Model-based Clustering of Flow Cytometry Data"
26,832284422283988993,43560393,Maurice Herlihy,['New paper: Adding Concurrency to Smart Contracts \n<LINK>'],https://arxiv.org/abs/1702.04467,"Modern cryptocurrency systems, such as Ethereum, permit complex financial transactions through scripts called smart contracts. These smart contracts are executed many, many times, always without real concurrency. First, all smart contracts are serially executed by miners before appending them to the blockchain. Later, those contracts are serially re-executed by validators to verify that the smart contracts were executed correctly by miners. Serial execution limits system throughput and fails to exploit today's concurrent multicore and cluster architectures. Nevertheless, serial execution appears to be required: contracts share state, and contract programming languages have a serial semantics. This paper presents a novel way to permit miners and validators to execute smart contracts in parallel, based on techniques adapted from software transactional memory. Miners execute smart contracts speculatively in parallel, allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a serializable concurrent schedule for a block's transactions, This schedule is captured and encoded as a deterministic fork-join program used by validators to re-execute the miner's parallel schedule deterministically but concurrently. Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of of 1.33x can be obtained for miners and 1.69x for validators with just three concurrent threads. ",Adding Concurrency to Smart Contracts
27,831900313279934469,14118089,Henry Segerman,"['New paper on hyperbolic virtual reality, with @vihartvihart , @AndreaHawksley and @Sabetta_. <LINK> <LINK>']",https://arxiv.org/abs/1702.04004,We describe our initial explorations in simulating non-euclidean geometries in virtual reality. Our simulations of three-dimensional hyperbolic space are available at this http URL ,Non-euclidean virtual reality I: explorations of $\mathbb{H}^3$
28,831693781208485888,795877354266456064,KoheiKamadaPhys,['My new paper has appeared. On the primordial MFs &amp; their relation to particle physics. Please have a look at it! \n<LINK>'],https://arxiv.org/abs/1702.03928,"Production of axionlike particles (ALPs) by primordial magnetic fields may have significant impacts on cosmology. We discuss the production of ALPs in the presence of the primordial magnetic fields. We find a region of the ALP mass and photon coupling which realizes the observed properties of the dark matter with appropriate initial conditions for the magnetic fields. This region may be interesting in light of recent indications for the 3.5 keV lines from galaxy clusters. For a small axion mass, a region of previously allowed parameter spaces is excluded by overproduction of ALPs as a hot/warm dark matter component. Since the abundance of ALPs strongly depends on the initial conditions of primordial magnetic fields, our results provide implications for scenarios of magnetogenesis. ",Axion production from primordial magnetic fields
29,831633920164716544,1580467730,Michael Busch,['The boundary between comets and asteroids continues to be fuzzy.  New paper by Moreno et al. (including @astrokiwi): <LINK>'],https://arxiv.org/abs/1702.03665,"We present deep imaging observations, orbital dynamics, and dust tail model analyses of the double-component asteroid P/2016 J1 (J1-A and J1-B). The observations were acquired at the Gran Telescopio Canarias (GTC) and the Canada-France-Hawaii Telescope (CFHT) from mid March to late July, 2016. A statistical analysis of backward-in-time integrations of the orbits of a large sample of clone objects of P/2016 J1-A and J1-B shows that the minimum separation between them occurred most likely $\sim$2300 days prior to the current perihelion passage, i.e., during the previous orbit near perihelion. This closest approach was probably linked to a fragmentation event of their parent body. Monte Carlo dust tail models show that those two components became active simultaneously $\sim$250 days before the current perihelion, with comparable maximum loss rates of $\sim$0.7 kg s$^{-1}$ and $\sim$0.5 kg s$^{-1}$, and total ejected masses of 8$\times$10$^{6}$ kg and 6$\times$10$^{6}$ kg for fragments J1-A and J1-B, respectively. In consequence, the fragmentation event and the present dust activity are unrelated. The simultaneous activation times of the two components and the fact that the activity lasted 6 to 9 months or longer, strongly indicate ice sublimation as the most likely mechanism involved in the dust emission process. ",The splitting of double-component active asteroid P/2016 J1 (PANSTARRS)
30,831459512951439360,549226519,Katarina Stensson (PP),"['We got a new paper up on #arxiv ! ""Two-photon interference from two blinking quantum emitters"" <LINK> Now, back to the lab.']",https://arxiv.org/abs/1702.03278,We investigate the effect of blinking on the two-photon interference measurement from two independent quantum emitters. We find that blinking significantly alters the statistics in the second-order intensity correlation function g$^{(2)}(\tau)$ and the outcome of two-photon interference measurements performed with independent quantum emitters. We theoretically demonstrate that the presence of blinking can be experimentally recognized by a deviation from the g$^{(2)}_{D}(0)=0.5$ value when distinguishable photons impinge on a beam splitter. Our results show that blinking imposes a mandatory cross-check measurement to correctly estimate the degree of indistinguishablility of photons emitted by independent quantum emitters. ,Two-photon interference from two blinking quantum emitters
31,830411530357395457,779636032476176385,Alan McKenzie,['Just posted a new paper on arXiv (<LINK>).\n<LINK>'],https://arxiv.org/abs/1702.02019,"Anthony Aguirre and Max Tegmark have famously speculated that the Level I Multiverse is the same as the Level III Multiverse. By this, they mean that the parallel universes of the Level III Multiverse can be regarded as similar or identical copies of our own Hubble volume distributed throughout the whole of our (possibly infinite) bubble universe. However, we show that our bubble universe is in a single quantum eigenstate that extends to regions of space that are receding from each other at superluminal velocities because of general relativistic expansion. Such a bubble universe cannot accommodate Hubble volumes in the different orthogonal eigenstates required by the Level III Multiverse. Instead, quantum uncertainty arises from large numbers of alternative bubble universes in Hilbert space, isolated from each other. The conclusion of the paper is that the Level I Multiverse is not the same as the Level III Multiverse. ",The Level I Multiverse is not the same as the Level III Multiverse
32,830110988473077760,333597222,Matt Lease,"['New paper ""Exploiting Domain Knowledge via Grouped Weight Sharing..."" with Ye Zhang &amp; @byron_c_wallace: <LINK> @UTiSchool']",https://arxiv.org/abs/1702.02535,"A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing. ","Exploiting Domain Knowledge via Grouped Weight Sharing with Application
  to Text Categorization"
33,830017220189188096,16674284,Mirco Musolesi,['New survey paper: Sensing and Modeling Human Behavior Using Social Media and Mobile Data <LINK> \\w @mehrotrabhinav'],https://arxiv.org/abs/1702.01181,"In the past years we have witnessed the emergence of the new discipline of computational social science, which promotes a new data-driven and computation-based approach to social sciences. In this article we discuss how the availability of new technologies such as online social media and mobile smartphones has allowed researchers to passively collect human behavioral data at a scale and a level of granularity that were just unthinkable some years ago. We also discuss how these digital traces can then be used to prove (or disprove) existing theories and develop new models of human behavior. ",Sensing and Modeling Human Behavior Using Social Media and Mobile Data
34,829822071551188993,1523174335,Kolja Kleineberg,['Geometric correlations and the robustness of multiplex networks! Supp. Vid. of new paper <LINK> <LINK>'],https://arxiv.org/abs/1702.02246,"We show that real multiplex networks are unexpectedly robust against targeted attacks on high degree nodes, and that hidden interlayer geometric correlations predict this robustness. Without geometric correlations, multiplexes exhibit an abrupt breakdown of mutual connectivity, even with interlayer degree correlations. With geometric correlations, we instead observe a multistep cascading process leading into a continuous transition, which apparently becomes fully continuous in the thermodynamic limit. Our results are important for the design of efficient protection strategies and of robust interacting networks in many domains. ","Geometric correlations mitigate the extreme vulnerability of multiplex
  networks against targeted attacks"
35,829598056806903809,1523174335,Kolja Kleineberg,"['New paper: Geometric correlations mitigate the extreme vulnerability of multiplex networks against targeted attacks <LINK> <LINK>', '@KoljaKleineberg ... and now with geometric correlations https://t.co/olmclANaHO @FuturICT @net_science @cxdig @clabbarcelona https://t.co/cXbDJ2VUdl']",https://arxiv.org/abs/1702.02246,"We show that real multiplex networks are unexpectedly robust against targeted attacks on high degree nodes, and that hidden interlayer geometric correlations predict this robustness. Without geometric correlations, multiplexes exhibit an abrupt breakdown of mutual connectivity, even with interlayer degree correlations. With geometric correlations, we instead observe a multistep cascading process leading into a continuous transition, which apparently becomes fully continuous in the thermodynamic limit. Our results are important for the design of efficient protection strategies and of robust interacting networks in many domains. ","Geometric correlations mitigate the extreme vulnerability of multiplex
  networks against targeted attacks"
36,828905962555715584,3306423411,Andrew Bond,"['New paper: <LINK> ! Possible extensions to the standard model based on UV fixed point in gauge couplings #AsymptoticSafety', 'Possible experimental collider signatures: changes to running of weak &amp; strong coupling constants, diboson signatures, R-hadrons.']",https://arxiv.org/abs/1702.01727,"Building on recent advances in the understanding of gauge-Yukawa theories we explore possibilities to UV-complete the Standard Model in an asymptotically safe manner. Minimal extensions are based on a large flavor sector of additional fermions coupled to a scalar singlet matrix field. We find that asymptotic safety requires fermions in higher representations of $SU(3)_C\times SU(2)_L$. Possible signatures at colliders are worked out and include $R$-hadron searches, diboson signatures and the evolution of the strong and weak coupling constants. ",Directions for model building from asymptotic safety
37,828839617197637632,7984662,Clayton Shonkwiler,"[""What's the probability that a random triangle is obtuse?\n\nAnswers to this question and more in our new paper: <LINK> <LINK>"", ""@SylviaFysica How so? We're taking the uniform distribution on the state space, so all triangles are treated as equally likely."", '@SylviaFysica Sure. There are lots of different measures people use, and lots of different answers have been given, as we discuss.', '@SylviaFysica Our measure is the only one that has been proposed with a transitive group of measure-preserving transformations‚Ä¶', '@SylviaFysica ‚Ä¶so we think it is a particularly natural choice.']",https://arxiv.org/abs/1702.01027,"We consider the problem of finding the probability that a random triangle is obtuse, which was first raised by Lewis Caroll. Our investigation leads us to a natural correspondence between plane polygons and the Grassmann manifold of 2-planes in real $n$-space proposed by Allen Knutson and Jean-Claude Hausmann. This correspondence defines a natural probability measure on plane polygons. In these terms, we answer Caroll's question. We then explore the Grassmannian geometry of planar quadrilaterals, providing an answer to Sylvester's four-point problem, and describing explicitly the moduli space of unordered quadrilaterals. All of this provides a concrete introduction to a family of metrics used in shape classification and computer vision. ",Random Triangles and Polygons in the Plane
38,828658764018507776,579299426,Steven Strogatz,"['Links between Pok√©mon, networks, contagion &amp; cancer, via the math of ""the coupon collector\'s problem = our new paper <LINK>']",https://arxiv.org/abs/1702.00881,"We study a stochastic model of infection spreading on a network. At each time step a node is chosen at random, along with one of its neighbors. If the node is infected and the neighbor is susceptible, the neighbor becomes infected. How many time steps $T$ does it take to completely infect a network of $N$ nodes, starting from a single infected node? An analogy to the classic ""coupon collector"" problem of probability theory reveals that the takeover time $T$ is dominated by extremal behavior, either when there are only a few infected nodes near the start of the process or a few susceptible nodes near the end. We show that for $N \gg 1$, the takeover time $T$ is distributed as a Gumbel for the star graph; as the sum of two Gumbels for a complete graph and an Erd\H{o}s-R\'{e}nyi random graph; as a normal for a one-dimensional ring and a two-dimensional lattice; and as a family of intermediate skewed distributions for $d$-dimensional lattices with $d \ge 3$ (these distributions approach the sum of two Gumbels as $d$ approaches infinity). Connections to evolutionary dynamics, cancer, incubation periods of infectious diseases, first-passage percolation, and other spreading phenomena in biology and physics are discussed. ",Takeover times for a simple model of network infection
39,827572235225427969,1876364984,Alice Zocchi,"['Hey people, check out our new paper! <LINK> #omegaCen #anisotropy']",https://arxiv.org/abs/1702.00725,"Finding an intermediate-mass black hole (IMBH) in a globular cluster (or proving its absence) would provide valuable insights into our understanding of galaxy formation and evolution. However, it is challenging to identify a unique signature of an IMBH that cannot be accounted for by other processes. Observational claims of IMBH detection are indeed often based on analyses of the kinematics of stars in the cluster core, the most common signature being a rise in the velocity dispersion profile towards the centre of the system. Unfortunately, this IMBH signal is degenerate with the presence of radially-biased pressure anisotropy in the globular cluster. To explore the role of anisotropy in shaping the observational kinematics of clusters, we analyse the case of omega Cen by comparing the observed profiles to those calculated from the family of LIMEPY models, that account for the presence of anisotropy in the system in a physically motivated way. The best-fit radially anisotropic models reproduce the observational profiles well, and describe the central kinematics as derived from Hubble Space Telescope proper motions without the need for an IMBH. ","Radial anisotropy in omega Cen limiting the room for an
  intermediate-mass black hole"
40,827528425510432768,785934358867685376,Sam Snodgrass,['Hey all! Check out this new survey paper (submitted) on Procedural Content Generation via Machine Learning (PCGML). <LINK>'],https://arxiv.org/abs/1702.00539,"This survey explores Procedural Content Generation via Machine Learning (PCGML), defined as the generation of game content using machine learning models trained on existing content. As the importance of PCG for game development increases, researchers explore new avenues for generating high-quality content with or without human involvement; this paper addresses the relatively new paradigm of using machine learning (in contrast with search-based, solver-based, and constructive methods). We focus on what is most often considered functional game content such as platformer levels, game maps, interactive fiction stories, and cards in collectible card games, as opposed to cosmetic content such as sprites and sound effects. In addition to using PCG for autonomous generation, co-creativity, mixed-initiative design, and compression, PCGML is suited for repair, critique, and content analysis because of its focus on modeling existing content. We discuss various data sources and representations that affect the resulting generated content. Multiple PCGML methods are covered, including neural networks, long short-term memory (LSTM) networks, autoencoders, and deep convolutional networks; Markov models, $n$-grams, and multi-dimensional Markov chains; clustering; and matrix factorization. Finally, we discuss open problems in the application of PCGML, including learning from small datasets, lack of training data, multi-layered learning, style-transfer, parameter tuning, and PCG as a game mechanic. ",Procedural Content Generation via Machine Learning (PCGML)
41,827485495395549184,91162844,Dr Adam A Stokes,"['Check out our new Open-Source paper on @arxiv_org ""Integrating Soft Robotics with ROS - A hybrid pick and place arm"" <LINK>']",https://arxiv.org/abs/1702.00694,"Soft robotic systems present a variety of new opportunities for solving complex problems. The use of soft robotic grippers, for example, can simplify the complexity in tasks such as the of grasping irregular and delicate objects. Adoption of soft robotics by academia and industry, however, has been slow and this is, in-part, due to the amount of hardware and software that must be developed from scratch for each use of soft system components. In this paper we detail the design, fabrication and validation of an open-source framework that we designed to lower the barrier to entry for integrating soft robotic subsystems. This framework is built on ROS (the Robot Operating System) and we use it to demonstrate a modular, soft-hard hybrid system which is capable of completing pick and place tasks. By lowering this barrier to entry we hope that system designers and researchers will find it easy to integrate soft components into their existing ROS-enabled robotic systems. ",Integrating Soft Robotics with ROS - A hybrid pick and place arm
42,827458645231271937,19465243,Hannah Earnshaw,['New paper! Looking at an interesting group of soft extragalactic X-ray binaries: <LINK>'],https://arxiv.org/abs/1702.00313,"The luminosity range at and just below the 10^39 erg/s cut-off for defining ultraluminous X-ray sources (ULXs) is a little-explored regime. It none-the-less hosts a large number of X-ray sources, and has great potential for improving our understanding of sources with ~Eddington accretion rates. We select a sample of four sources in this Eddington Threshold regime with good data for further study; these objects possess a variety of soft spectral shapes. We perform X-ray spectral and timing analysis on the XMM-Newton and Chandra data for these objects to gain insight into their accretion mechanisms, and also examine their optical counterparts using HST images. NGC 300 X-1 is a highly luminous and well-known example of the canonical steep power-law accretion state. M51 ULS exhibits a cool blackbody-like spectrum and is consistent with being an ultraluminous supersoft source (ULS), possibly a super-Eddington accreting object viewed at a high inclination through an optically thick outflowing wind. NGC 4395 ULX-1 and NGC 6946 ULX-1 have unusually steep power-law tails, for which we discuss a variety of possible physical mechanisms and links to similar features in Galactic microquasars, and we conclude that these sources are likely intermediate objects between the soft ultraluminous regime of ULXs and classic ULSs. ",Soft Extragalactic X-Ray Binaries at the Eddington Threshold
43,827421556779384832,1069568448,Steve Crawford,['New paper by Ando Ratsimbazafy on age dating Luminous Red Galaxies with @SALT_Astro  <LINK>'],https://arxiv.org/abs/1702.00418,"We measure a value for the cosmic expansion of $H(z) = 89 \pm 23$(stat) $\pm$ 44(syst) km s$^{-1}$ Mpc$^{-1}$ at a redshift of $z \simeq 0.47$ based on the differential age technique. This technique, also known as cosmic chronometers, uses the age difference between two redshifts for a passively evolving population of galaxies to calculate the expansion rate of the Universe. Our measurement is based on analysis of high quality spectra of Luminous Red Galaxies (LRGs) obtained with the Southern African Large Telescope (SALT) in two narrow redshift ranges of $z \simeq 0.40$ and $z \simeq 0.55$ as part of an initial pilot study. Ages were estimated by fitting single stellar population models to the observed spectra. This measurement presents one of the best estimates of $H(z)$ via this method at $z\sim0.5$ to date. ","Age-dating Luminous Red Galaxies observed with the Southern African
  Large Telescope"
44,827361587740676097,19510090,Julian Togelius,"['New survey paper (just submitted) on Procedural Content Generation via Machine Learning (PCGML)\n<LINK>', '@togelius With @Autumnsburg @SamPSnodgrass @MatthewGuz @holmgard @amykhoover @aireye @nealen']",https://arxiv.org/abs/1702.00539,"This survey explores Procedural Content Generation via Machine Learning (PCGML), defined as the generation of game content using machine learning models trained on existing content. As the importance of PCG for game development increases, researchers explore new avenues for generating high-quality content with or without human involvement; this paper addresses the relatively new paradigm of using machine learning (in contrast with search-based, solver-based, and constructive methods). We focus on what is most often considered functional game content such as platformer levels, game maps, interactive fiction stories, and cards in collectible card games, as opposed to cosmetic content such as sprites and sound effects. In addition to using PCG for autonomous generation, co-creativity, mixed-initiative design, and compression, PCGML is suited for repair, critique, and content analysis because of its focus on modeling existing content. We discuss various data sources and representations that affect the resulting generated content. Multiple PCGML methods are covered, including neural networks, long short-term memory (LSTM) networks, autoencoders, and deep convolutional networks; Markov models, $n$-grams, and multi-dimensional Markov chains; clustering; and matrix factorization. Finally, we discuss open problems in the application of PCGML, including learning from small datasets, lack of training data, multi-layered learning, style-transfer, parameter tuning, and PCG as a game mechanic. ",Procedural Content Generation via Machine Learning (PCGML)
45,827335694691491841,1556664198,Kyle Cranmer,['New paper: #deeplearning models that use an NLP analogy to incorporate prior physics knowledge. @glouppe @kchonyc <LINK> <LINK>'],https://arxiv.org/abs/1702.00748,"Recent progress in applying machine learning for jet physics has been built upon an analogy between calorimeters and images. In this work, we present a novel class of recursive neural networks built instead upon an analogy between QCD and natural languages. In the analogy, four-momenta are like words and the clustering history of sequential recombination jet algorithms is like the parsing of a sentence. Our approach works directly with the four-momenta of a variable-length set of particles, and the jet-based tree structure varies on an event-by-event basis. Our experiments highlight the flexibility of our method for building task-specific jet embeddings and show that recursive architectures are significantly more accurate and data efficient than previous image-based networks. We extend the analogy from individual jets (sentences) to full events (paragraphs), and show for the first time an event-level classifier operating on all the stable particles produced in an LHC event. ",QCD-Aware Recursive Neural Networks for Jet Physics
46,837448440300912641,97210403,Jonathan Goodman,['New paper -- MCMC sampling on manifolds and integration/volume estimation <LINK>'],https://arxiv.org/abs/1702.08446,"We describe and analyze some Monte Carlo methods for manifolds in Euclidean space defined by equality and inequality constraints. First, we give an MCMC sampler for probability distributions defined by un-normalized densities on such manifolds. The sampler uses a specific orthogonal projection to the surface that requires only information about the tangent space to the manifold, obtainable from first derivatives of the constraint functions, hence avoiding the need for curvature information or second derivatives. Second, we use the sampler to develop a multi-stage algorithm to compute integrals over such manifolds. We provide single-run error estimates that avoid the need for multiple independent runs. Computational experiments on various test problems show that the algorithms and error estimates work in practice. The method is applied to compute the entropies of different sticky hard sphere systems. These predict the temperature or interaction energy at which loops of hard sticky spheres become preferable to chains. ",Monte Carlo on manifolds: sampling densities and integrating functions
47,836834798303838208,472380778,Elias Jarlebring,"['New paper ""Disguised... nonlinear eigenprobs"" <LINK> w/ Koskela&amp; @giampaolo_mele. All simulations online 4 #reproducability <LINK>']",https://arxiv.org/abs/1702.08492,"In this paper we take a quasi-Newton approach to nonlinear eigenvalue problems (NEPs) of the type $M(\lambda)v=0$, where $M:\mathbb{C}\rightarrow\mathbb{C}^{n\times n}$ is a holomorphic function. We investigate which types of approximations of the Jacobian matrix lead to competitive algorithms, and provide convergence theory. The convergence analysis is based on theory for quasi-Newton methods and Keldysh's theorem for NEPs. We derive new algorithms and also show that several well-established methods for NEPs can be interpreted as quasi-Newton methods, and thereby provide insight to their convergence behavior. In particular, we establish quasi-Newton interpretations of Neumaier's residual inverse iteration and Ruhe's method of successive linear problems. ",Disguised and new Quasi-Newton methods for nonlinear eigenvalue problems
48,836629146054840321,709141305927057408,Zachary Feinstein,"[""It's not all about studying fiction. New working paper online <LINK> #SystemicRisk #FinancialContagion""]",https://arxiv.org/abs/1702.07936,"This paper provides a general framework for modeling financial contagion in a system with obligations in multiple illiquid assets (e.g., currencies). In so doing, we develop a multi-layered financial network that extends the single network of Eisenberg and Noe (2001). In particular, we develop a financial contagion model with fire sales that allows institutions to both buy and sell assets to cover their liabilities in the different assets and act as utility maximizers. We prove that, under standard assumptions and without market impacts, equilibrium portfolio holdings exist and are unique. However, with market impacts, we prove that equilibrium portfolio holdings and market prices exist which clear the multi-layered financial system. In general, though, these clearing solutions are not unique. We extend this result by considering the t\^atonnement process to find the unique attained equilibrium. The attained equilibrium need not be continuous with respect to the initial shock; these points of discontinuity match those stresses in which a financial crisis becomes a systemic crisis. We further provide mathematical formulations for payment rules and utility functions satisfying the necessary conditions for these existence and uniqueness results. We demonstrate the value of our model through illustrative numerical case studies. In particular, we study a counterfactual scenario on the event that Greece re-instituted the drachma on a dataset from the European Banking Authority. ",Obligations with Physical Delivery in a Multi-Layered Financial Network
49,829262793094356993,129550400,Giona Casiraghi,['How do relations drive interactions? My new paper on multiplex network regression is on arXiv: <LINK> <LINK>'],https://arxiv.org/abs/1702.02048,"We introduce a statistical regression model to investigate the impact of dyadic relations on complex networks generated from observed repeated interactions. It is based on generalised hypergeometric ensembles (gHypEG), a class of statistical network ensembles developed recently to deal with multi-edge graph and count data. We represent different types of known relations between system elements by weighted graphs, separated in the different layers of a multiplex network. With our method, we can regress the influence of each relational layer, the explanatory variables, on the interaction counts, the dependent variables. Moreover, we can quantify the statistical significance of the relations as explanatory variables for the observed interactions. To demonstrate the power of our approach, we investigate an example based on empirical data. ",Multiplex Network Regression: How do relations drive interactions?
50,828890400639303681,3121738289,Latouche Pierre,['new paper on a dynamic version of SBM with @NialFriel from @UCDMathStat and @riccardoras <LINK> #sna #clustering #networks'],https://arxiv.org/abs/1702.01418,"Latent stochastic block models are flexible statistical models that are widely used in social network analysis. In recent years, efforts have been made to extend these models to temporal dynamic networks, whereby the connections between nodes are observed at a number of different times. In this paper we extend the original stochastic block model by using a Markovian property to describe the evolution of nodes' cluster memberships over time. We recast the problem of clustering the nodes of the network into a model-based context, and show that the integrated completed likelihood can be evaluated analytically for a number of likelihood models. Then, we propose a scalable greedy algorithm to maximise this quantity, thereby estimating both the optimal partition and the ideal number of groups in a single inferential framework. Finally we propose applications of our methodology to both real and artificial datasets. ","Choosing the number of groups in a latent stochastic block model for
  dynamic networks"
51,829291524659740673,526115229,Kevin Heng,"['Our new study on a hidden demon in transmission spectra. Frankly, we are dismayed by what we found:\n<LINK>', '@planetremco Mike Line just mentioned it too, but our study goes deeper and provides a formula to cleanly understand these degeneracies.', '@planetremco Also, this was noticed numerically by Benneke &amp; Seager (2012). One only realizes the full extent of the problem analytically...', '@planetremco I agree. Call it a literature review oversight.', '@planetremco I know for a fact it does, c.f. Jaemin Lee. One of the key points is that R0 and P0 cannot be treated as independent fit params', '@planetremco Read the paper. You will be surprised. :)', '@planetremco Hence the use of the terms ""revisited"" and ""unresolved"". No one is claiming it\'s new. We\'re shining a different light on it.', '@ExoSing @planetremco @DrJoVian Refs are easy to fix. The novelty of our approach is in clarifying the formalism and unification of details.']",https://arxiv.org/abs/1702.02051,"The computation of transmission spectra is a central ingredient in the study of exoplanetary atmospheres. First, we revisit the theory of transmission spectra, unifying ideas from several workers in the literature. Transmission spectra lack an absolute normalization due to the a priori unknown value of a reference transit radius, which is tied to an unknown reference pressure. We show that there is a degeneracy between the uncertainty in the transit radius, the assumed value of the reference pressure (typically set to 10 bar) and the inferred value of the water abundance when interpreting a WFC3 transmission spectrum. Second, we show that the transmission spectra of isothermal atmospheres are nearly isobaric. We validate the isothermal, isobaric analytical formula for the transmission spectrum against full numerical calculations and show that the typical errors are ~0.1% (~10 ppm) within the WFC3 range of wavelengths for temperatures of 1500 K (or higher). Third, we generalize the previous expression for the transit radius to include a small temperature gradient. Finally, we analyze the measured WFC3 transmission spectrum of WASP-12b and demonstrate that we obtain consistent results with the retrieval approach of Kreidberg et al. (2015) if the reference transit radius and reference pressure are fixed to assumed values. The unknown functional relationship between the reference transit radius and reference pressure implies that it is the product of the water abundance and reference pressure that is being retrieved from the data, and not just the water abundance alone. This degeneracy leads to a limitation on how accurately we may extract molecular abundances from transmission spectra using WFC3 data alone. Finally, we compare our study to that of Griffith (2014) and discuss why the degeneracy was missed in previous retrieval studies. [abridged] ","The theory of transmission spectra revisited: a semi-analytical method
  for interpreting WFC3 data and an unresolved challenge"
