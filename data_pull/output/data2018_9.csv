,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1049688294806687744,759249,Dean Eckles,['Our new paper shows how to empirically study these friendship-paradox-inspired seeding (or vaccination) strategies\n<LINK> <LINK>'],https://arxiv.org/abs/1809.09561,"When trying to maximize the adoption of a behavior in a population connected by a social network, it is common to strategize about where in the network to seed the behavior, often with an element of randomness. Selecting seeds uniformly at random is a basic but compelling strategy in that it distributes seeds broadly throughout the network. A more sophisticated stochastic strategy, one-hop targeting, is to select random network neighbors of random individuals; this exploits a version of the friendship paradox, whereby the friend of a random individual is expected to have more friends than a random individual, with the hope that seeding a behavior at more connected individuals leads to more adoption. Many seeding strategies have been proposed, but empirical evaluations have demanded large field experiments designed specifically for this purpose and have yielded relatively imprecise comparisons of strategies. Here we show how stochastic seeding strategies can be evaluated more efficiently in such experiments, how they can be evaluated ""off-policy"" using existing data arising from experiments designed for other purposes, and how to design more efficient experiments. In particular, we consider contrasts between stochastic seeding strategies and analyze nonparametric estimators adapted from policy evaluation and importance sampling. We use simulations on real networks to show that the proposed estimators and designs can increase precision while yielding valid inference. We then apply our proposed estimators to two field experiments, one that assigned households to an intensive marketing intervention and one that assigned students to an anti-bullying intervention. ",Evaluating stochastic seeding strategies in networks
1,1048306734853447680,3199605543,Afonso S. Bandeira,['New paper On the Landscape of Synchronization Networks with Shuyang Ling and Ruitu Xu <LINK>'],https://arxiv.org/abs/1809.11083,"Studying the landscape of nonconvex cost function is key towards a better understanding of optimization algorithms widely used in signal processing, statistics, and machine learning. Meanwhile, the famous Kuramoto model has been an important mathematical model to study the synchronization phenomena of coupled oscillators over various network topologies. In this paper, we bring together these two seemingly unrelated objects by investigating the optimization landscape of a nonlinear function $E(\boldsymbol{\theta}) = \frac{1}{2}\sum_{1\leq i,j\leq n} a_{ij}(1-\cos(\theta_i - \theta_j))$ associated to an underlying network and exploring the relationship between the existence of local minima and network topology. This function arises naturally in Burer-Monteiro method applied to $\mathbb{Z}_2$ synchronization as well as matrix completion on the torus. Moreover, it corresponds to the energy function of the homogeneous Kuramoto model on complex networks for coupled oscillators. We prove the minimizer of the energy function is unique up to a global translation under deterministic dense graphs and Erd\H{o}s-R\'enyi random graphs with tools from optimization and random matrix theory. Consequently, the stable equilibrium of the corresponding homogeneous Kuramoto model is unique and the basin of attraction for the synchronous state of these coupled oscillators is the whole phase space minus a set of measure zero. In addition, our results address when the Burer-Monteiro method recovers the ground truth exactly from highly incomplete observations in $\mathbb{Z}_2$ synchronization and shed light on the robustness of nonconvex optimization algorithms against certain types of so-called monotone adversaries. Numerical simulations are performed to illustrate our results. ","On the Landscape of Synchronization Networks: A Perspective from
  Nonconvex Optimization"
2,1047751272798068736,1045572914421141504,Fischer-Friedrich lab,['Check out our new paper on protein condensates\n<LINK>'],http://arxiv.org/abs/1809.09832,"An increasing number of proteins with intrinsically disordered domains have been shown to phase separate in buffer to form liquid-like phases. These protein condensates serve as simple models for the investigation of the more complex membrane-less organelles in cells. To understand the function of such proteins in cells, the material properties of the condensates they form are important. However, these material properties are not well understood. Here, we develop a novel method based on optical traps to study the frequency-dependent rheology and the surface tension of PGL-3 condensates as a function of salt concentration. We find that PGL-3 droplets are predominantly viscous but also exhibit elastic properties. As the salt concentration is reduced, their elastic modulus, viscosity and surface tension increase. Our findings show that salt concentration has a strong influence on the rheology and dynamics of protein condensates suggesting an important role of electrostatic interactions for their material properties. ","Salt-dependent rheology and surface tension of protein condensates using
  optical traps"
3,1047264397192646656,2800204849,Andrew Gordon Wilson,"['Our new paper, GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU acceleration, is appearing as a #NIPS2018 spotlight, with code! The paper is about GP inference and blackbox linear algebra that exploits hardware for major acceleration. <LINK> <LINK>']",https://arxiv.org/abs/1809.11165,"Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch. ","GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU
  Acceleration"
4,1046810151146201088,20939629,Alan Champneys,"['New paper on the arxXiv with Nicolas Versheuren that  ""discects the snake""  <LINK> \n@IMIBath @TiinaRoose @mcapellanus @BristolEngMaths @paglend @S_K_Wilson @ahmerwadee <LINK>']",https://arxiv.org/abs/1809.07847,"An investigation is undertaken of coupled reaction-diffusion systems in one spatial dimension that are able to support, in different regions of their parameter space, either an isolated spike solution, or stable localized patterns with an arbitrary number of peaks. The distinction between the two cases is drawn through the behavior of the far field, where there is either an oscillatory or a monotonic decay. Several examples are studied, including the Lugiato-Lefever model, a generalized Schakenberg system that arises in cellular-level morphogensis and a continuum model of urban crime spread. In each, it is found that localized patterns connected via a so-called homoclinic snaking curve in parameter space transition into a single spike solution as a second parameter is varied, via a change in topology of the snake into a series of disconnected branches. In each case, the transition is caused by a so-called Belyakov-Devaney transition between complex and real spatial eigenvalues of the fair field of the primary pulse. A codimension-two problem is studied in detail where a non-transverse homoclinic orbit undergoes this transition. A Shilnikov-style analysis is undertaken which reveals the asymptotics of how the infinite family of folds of multi-pulse orbits are all destroyed at the same parameter value. The results are shown to be consistent with numerical experiments on two of the examples. ","Dissecting the snake: the transition from localized patterns to isolated
  spikes in pattern formation systems"
5,1046784816060936192,3423739275,Felix Leditzky,"['New paper on ""Asymptotic performance of port-based teleportation"", in which we determine the leading-order asymptotics of PBT in a few different settings.\nJoint work with M. Christandl, @cmajenz, @quantum_graeme, F. Speelman, and @michael_quantum \n<LINK>']",https://arxiv.org/abs/1809.10751,"Quantum teleportation is one of the fundamental building blocks of quantum Shannon theory. While ordinary teleportation is simple and efficient, port-based teleportation (PBT) enables applications such as universal programmable quantum processors, instantaneous non-local quantum computation and attacks on position-based quantum cryptography. In this work, we determine the fundamental limit on the performance of PBT: for arbitrary fixed input dimension and a large number $N$ of ports, the error of the optimal protocol is proportional to the inverse square of $N$. We prove this by deriving an achievability bound, obtained by relating the corresponding optimization problem to the lowest Dirichlet eigenvalue of the Laplacian on the ordered simplex. We also give an improved converse bound of matching order in the number of ports. In addition, we determine the leading-order asymptotics of PBT variants defined in terms of maximally entangled resource states. The proofs of these results rely on connecting recently-derived representation-theoretic formulas to random matrix theory. Along the way, we refine a convergence result for the fluctuations of the Schur-Weyl distribution by Johansson, which might be of independent interest. ",Asymptotic performance of port-based teleportation
6,1046442311041896449,41462754,Claudio Orlandi,"['New paper online: ""Sharing Information with Competitors"" with Simina Brânzei and Guang Yang  <LINK>']",https://arxiv.org/abs/1809.10637v1,"We study the mechanism design problem in the setting where agents are rewarded using information only. This problem is motivated by the increasing interest in secure multiparty computation techniques. More specifically, we consider the setting of a joint computation where different agents have inputs of different quality and each agent is interested in learning as much as possible while maintaining exclusivity for information. Our high level question is to design mechanisms that motivate all agents (even those with high-quality input) to participate in the computation and we formally study problems such as set union, intersection, and average. ",] Sharing Information with Competitors
7,1046010721010700288,1869370692,Disk Detective ES,"['Y tenemos un post en el blog de @diskdetective resumiendo el paper también. Lean tanto el paper como el post, si están interesados, en <LINK> <LINK> <LINK>']",https://arxiv.org/abs/1809.09663,"The Disk Detective citizen science project aims to find new stars with excess 22-$\mu$m emission from circumstellar dust in the AllWISE data release from the Wide-field Infrared Survey Explorer (WISE). We evaluated 261 Disk Detective objects of interest with imaging with the Robo-AO adaptive optics instrument on the 1.5m telescope at Palomar Observatory and with RetroCam on the 2.5m du Pont telescope at Las Campanas Observatory to search for background objects at 0.15''-12'' separations from each target. Our analysis of these data lead us to reject 7% of targets. Combining this result with statistics from our online image classification efforts implies that at most $7.9\% \pm 0.2\%$ of AllWISE-selected infrared excesses are good disk candidates. Applying our false positive rates to other surveys, we find that the infrared excess searches of McDonald et al. (2012), McDonald et al. (2017), and Marton et al. (2016) all have false positive rates $>70\%$. Moreover, we find that all thirteen disk candidates in Theissen & West (2014) with W4 signal-to-noise >3 are false positives. We present 244 disk candidates that have survived vetting by follow-up imaging. Of these, 213 are newly-identified disk systems. Twelve of these are candidate members of comoving pairs based on \textit{Gaia} astrometry, supporting the hypothesis that warm dust is associated with binary systems. We also note the discovery of 22 $\mu$m excess around two known members of the Scorpius-Centaurus association, and identify known disk host WISEA J164540.79-310226.6 as a likely Sco-Cen member. Thirty-one of these disk candidates are closer than $\sim 125$ pc (including 27 debris disks), making them good targets for direct imaging exoplanet searches. ","Follow-up Imaging of Disk Candidates from the Disk Detective Citizen
  Science Project: New Discoveries and False-Positives in WISE Circumstellar
  Disk Surveys"
8,1045703554839851008,383671015,Jo(hanna),['New paper out on... um... false positive rates of circumstellar disks in WISE data. #PhD #procrastivation <LINK>'],https://arxiv.org/abs/1809.09663,"The Disk Detective citizen science project aims to find new stars with excess 22-$\mu$m emission from circumstellar dust in the AllWISE data release from the Wide-field Infrared Survey Explorer (WISE). We evaluated 261 Disk Detective objects of interest with imaging with the Robo-AO adaptive optics instrument on the 1.5m telescope at Palomar Observatory and with RetroCam on the 2.5m du Pont telescope at Las Campanas Observatory to search for background objects at 0.15''-12'' separations from each target. Our analysis of these data lead us to reject 7% of targets. Combining this result with statistics from our online image classification efforts implies that at most $7.9\% \pm 0.2\%$ of AllWISE-selected infrared excesses are good disk candidates. Applying our false positive rates to other surveys, we find that the infrared excess searches of McDonald et al. (2012), McDonald et al. (2017), and Marton et al. (2016) all have false positive rates $>70\%$. Moreover, we find that all thirteen disk candidates in Theissen & West (2014) with W4 signal-to-noise >3 are false positives. We present 244 disk candidates that have survived vetting by follow-up imaging. Of these, 213 are newly-identified disk systems. Twelve of these are candidate members of comoving pairs based on \textit{Gaia} astrometry, supporting the hypothesis that warm dust is associated with binary systems. We also note the discovery of 22 $\mu$m excess around two known members of the Scorpius-Centaurus association, and identify known disk host WISEA J164540.79-310226.6 as a likely Sco-Cen member. Thirty-one of these disk candidates are closer than $\sim 125$ pc (including 27 debris disks), making them good targets for direct imaging exoplanet searches. ","Follow-up Imaging of Disk Candidates from the Disk Detective Citizen
  Science Project: New Discoveries and False-Positives in WISE Circumstellar
  Disk Surveys"
9,1045699504014610432,2235411914,Surya Ganguli,"['New #machinelearning paper: An analytic theory of generalization dynamics and transfer learning in deep linear networks: <LINK> Many puzzles of generalization in #deeplearning already appear in the linear setting and can be completely understood analytically! #ai', '@MStoudenmire Definitely worth thinking about!', ""@roydanroy Yep - I'll email you soon - you are one of the people I wanted to discuss this with!""]",https://arxiv.org/abs/1809.10374,"Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks. ","An analytic theory of generalization dynamics and transfer learning in
  deep linear networks"
10,1045533496595206145,1000564185221087232,Sripad Devalla,"['Our new paper uses deep learning to denoise optical coherence tomography images. Our network can significantly reduce the scanning time (7 folds), offering reduced patient discomfort without compromising on image quality. \n\n@ARVOinfo\n@GlaucomaToday\n\n <LINK> <LINK>']",https://arxiv.org/abs/1809.10589,"Purpose: To develop a deep learning approach to de-noise optical coherence tomography (OCT) B-scans of the optic nerve head (ONH). Methods: Volume scans consisting of 97 horizontal B-scans were acquired through the center of the ONH using a commercial OCT device (Spectralis) for both eyes of 20 subjects. For each eye, single-frame (without signal averaging), and multi-frame (75x signal averaging) volume scans were obtained. A custom deep learning network was then designed and trained with 2,328 ""clean B-scans"" (multi-frame B-scans), and their corresponding ""noisy B-scans"" (clean B-scans + gaussian noise) to de-noise the single-frame B-scans. The performance of the de-noising algorithm was assessed qualitatively, and quantitatively on 1,552 B-scans using the signal to noise ratio (SNR), contrast to noise ratio (CNR), and mean structural similarity index metrics (MSSIM). Results: The proposed algorithm successfully denoised unseen single-frame OCT B-scans. The denoised B-scans were qualitatively similar to their corresponding multi-frame B-scans, with enhanced visibility of the ONH tissues. The mean SNR increased from $4.02 \pm 0.68$ dB (single-frame) to $8.14 \pm 1.03$ dB (denoised). For all the ONH tissues, the mean CNR increased from $3.50 \pm 0.56$ (single-frame) to $7.63 \pm 1.81$ (denoised). The MSSIM increased from $0.13 \pm 0.02$ (single frame) to $0.65 \pm 0.03$ (denoised) when compared with the corresponding multi-frame B-scans. Conclusions: Our deep learning algorithm can denoise a single-frame OCT B-scan of the ONH in under 20 ms, thus offering a framework to obtain superior quality OCT B-scans with reduced scanning times and minimal patient discomfort. ","A Deep Learning Approach to Denoise Optical Coherence Tomography Images
  of the Optic Nerve Head"
11,1045529486517710848,930764003277643777,Matias Quiroz,['New paper on arXiv taking a closer look at the variance reduction properties of the reparameterization trick (widely used in stochastic variational Bayes): <LINK>. With Ming Xu @robertjk59 @ABC_Research'],https://arxiv.org/abs/1809.10330,"The reparameterization trick is widely used in variational inference as it yields more accurate estimates of the gradient of the variational objective than alternative approaches such as the score function method. Although there is overwhelming empirical evidence in the literature showing its success, there is relatively little research exploring why the reparameterization trick is so effective. We explore this under the idealized assumptions that the variational approximation is a mean-field Gaussian density and that the log of the joint density of the model parameters and the data is a quadratic function that depends on the variational mean. From this, we show that the marginal variances of the reparameterization gradient estimator are smaller than those of the score function gradient estimator. We apply the result of our idealized analysis to real-world examples. ",Variance reduction properties of the reparameterization trick
12,1045292990967820288,111204749,Steven Silverberg,"['News from @diskdetective: we have a new paper up on the arXiv! <LINK>', ""We use the large number of website classifications (thanks to everyone who's made a website classification!) to estimate how many false positives we find. Most of the catalog (~90%) is false positives, and most of those are objects with multiple point sources in the W4 PSF. (2/n) https://t.co/B31hsZur37"", 'We also look at how these distribute on the sky. Most of our ""multiples"" fall in the Galactic plane, but some also appear in the LMC and SMC. (3/n) https://t.co/UKme33IxwW', 'We got high-resolution follow-up images of a selection of our targets, using Robo-AO in the north (laser-guided adaptive optics!) and duPont/RetroCam in the south, to look for point sources too faint for our website survey data to see but bright enough to affect the W4 excess. https://t.co/dJILNHvBRe', ""And high-resolution follow-up finds some false positives as well--we find that 6% of the objects we followed up are false positives. Unlike with the website data, we don't see a significant difference between in and out of the Galactic plane. https://t.co/OYRvnOszF3"", ""And this tells us what we can expect for false positives in other surveys. Searches that use visual inspection do a lot better than searches that don't, but there's still false positives in these surveys, too. https://t.co/TFJwjMLBuo"", 'And we have 244 disk candidates, 213 of which are new discoveries by @diskdetective! Many of these fall on the main sequence, but some could be examples of extreme debris disks, as well. https://t.co/cfLIqrmGYN', ""And there's a blog post up on the @diskdetective blog summarizing the paper as well. Take a read of both the paper and the blog post, if you're interested. https://t.co/qnFtcnqK3C https://t.co/52IEUKc0fH""]",https://arxiv.org/abs/1809.09663,"The Disk Detective citizen science project aims to find new stars with excess 22-$\mu$m emission from circumstellar dust in the AllWISE data release from the Wide-field Infrared Survey Explorer (WISE). We evaluated 261 Disk Detective objects of interest with imaging with the Robo-AO adaptive optics instrument on the 1.5m telescope at Palomar Observatory and with RetroCam on the 2.5m du Pont telescope at Las Campanas Observatory to search for background objects at 0.15''-12'' separations from each target. Our analysis of these data lead us to reject 7% of targets. Combining this result with statistics from our online image classification efforts implies that at most $7.9\% \pm 0.2\%$ of AllWISE-selected infrared excesses are good disk candidates. Applying our false positive rates to other surveys, we find that the infrared excess searches of McDonald et al. (2012), McDonald et al. (2017), and Marton et al. (2016) all have false positive rates $>70\%$. Moreover, we find that all thirteen disk candidates in Theissen & West (2014) with W4 signal-to-noise >3 are false positives. We present 244 disk candidates that have survived vetting by follow-up imaging. Of these, 213 are newly-identified disk systems. Twelve of these are candidate members of comoving pairs based on \textit{Gaia} astrometry, supporting the hypothesis that warm dust is associated with binary systems. We also note the discovery of 22 $\mu$m excess around two known members of the Scorpius-Centaurus association, and identify known disk host WISEA J164540.79-310226.6 as a likely Sco-Cen member. Thirty-one of these disk candidates are closer than $\sim 125$ pc (including 27 debris disks), making them good targets for direct imaging exoplanet searches. ","Follow-up Imaging of Disk Candidates from the Disk Detective Citizen
  Science Project: New Discoveries and False-Positives in WISE Circumstellar
  Disk Surveys"
13,1045240012240015360,131879500,John Ilee,"['Molecules that may be important for life show interesting behaviour in young, massive discs.  Check out our new paper on arXiv today: <LINK>\n\n(with David Quenard, Izas Jimenez-Serra, @dh4gan @cassidentprone @ExoKenRice) <LINK>']",https://arxiv.org/abs/1809.09900,"Recent high-sensitivity observations carried out with ALMA have revealed the presence of complex organic molecules (COMs) such as methyl cyanide (CH$_{\rm 3}$CN) and methanol (CH$_{\rm 3}$OH) in relatively evolved protoplanetary discs. The behaviour and abundance of COMs in earlier phases of disc evolution remains unclear. Here we combine a smoothed particle hydrodynamics simulation of a fragmenting, gravitationally unstable disc with a gas-grain chemical code. We use this to investigate the evolution of formamide (NH$_{\rm 2}$CHO), a pre-biotic species, in both the disc and in the fragments that form within it. Our results show that formamide remains frozen onto grains in the majority of the disc where the temperatures are $<$100 K, with a predicted solid-phase abundance that matches those observed in comets. Formamide is present in the gas-phase in three fragments as a result of the high temperatures ($\geq$200\,K), but remains in the solid-phase in one colder ($\leq$150 K) fragment. The timescale over which this occurs is comparable to the dust sedimentation timescales, suggesting that any rocky core which is formed would inherit their formamide content directly from the protosolar nebula. ",The fate of formamide in a fragmenting protoplanetary disc
14,1045118783743029249,702241209276829697,Cecilia Garraffo 💚,['Our new tool to generate synthetic stellar radio images is out! Check out the recently accepted paper led by @SofiaMoschou: <LINK>  @AstroRaikoh @cosmodrake'],https://arxiv.org/abs/1809.09750,"Radio observations grant access to a wide range of physical processes through different emission mechanisms. These processes range from thermal and quiescent to eruptive phenomena, such as shock waves and particle beams. We present a new synthetic radio imaging tool that calculates and visualizes the Bremsstrahlung radio emission. This tool works concurrently with state-of-the-art Magnetohydrodynamic (MHD) simulations of the solar corona using the code BATS-R-US. Our model produces results that are in good agreement with both high and low frequency observations of the solar disk. In this study, a ray-tracing algorithm is used and the radio intensity is computed along the actual curved ray trajectories. We illustrate the importance of refraction in locating the radio emitting source by comparison of the radio imaging illustrations when the line-of-sight instead of the refracted paths are considered. We are planning to incorporate non-thermal radio emission mechanisms in a future version of the radio imaging tool. ",Synthetic Radio Imaging for Quiescent and CME-flare Scenarios
15,1045009636175335425,759249,Dean Eckles,"['Want to ""seed"" a behavior in a network without observing the network? Our new paper studies how to evaluate stochastic seeding strategies, such as taking ""one-hop"" from random starting nodes.\n<LINK>\n@ajwchin @jugander <LINK>', 'The one-hop seeding strategy is designed to exploit a version of the friendship paradox (your friends have more friends than you do). It puts probability on many different seed sets, but more probability on seed sets with higher normalized in-degree. https://t.co/ogOkmf6f7P', 'Experiments studying this strategy have randomized villages to targeting with one-hop or (uniform) random seeding. But because these strategies are stochastic, the random selected seeds can have higher in-degree than the one-hop seeds! Here in 3/8 cases in https://t.co/nQ3yuwoLGN https://t.co/l1YB2EQAIh', 'The estimators we propose exploit that we know the probability of some seed set under one-hop seeding. They can dramatically increase precision and power compared with a simple difference-in-means. https://t.co/vspuCznv4o', 'You can also use these methods ""off-policy"" with existing field experiments that measure a network and randomize a few nodes to treatment. Our results so far are cautionary: one-step seeding does not seem to outperform random seeding, and might even be less effective. https://t.co/N8S29cuWyI', 'One exciting this about this work is making novel reuse of data from ambitious field experiments — enabled by public data from Cai et al. https://t.co/Nc0j14Xt6e and data sharing by @betsylevyp et al. https://t.co/Q023CUd87L\nKnow any other experiments we could apply this to?']",https://arxiv.org/abs/1809.09561,"When trying to maximize the adoption of a behavior in a population connected by a social network, it is common to strategize about where in the network to seed the behavior, often with an element of randomness. Selecting seeds uniformly at random is a basic but compelling strategy in that it distributes seeds broadly throughout the network. A more sophisticated stochastic strategy, one-hop targeting, is to select random network neighbors of random individuals; this exploits a version of the friendship paradox, whereby the friend of a random individual is expected to have more friends than a random individual, with the hope that seeding a behavior at more connected individuals leads to more adoption. Many seeding strategies have been proposed, but empirical evaluations have demanded large field experiments designed specifically for this purpose and have yielded relatively imprecise comparisons of strategies. Here we show how stochastic seeding strategies can be evaluated more efficiently in such experiments, how they can be evaluated ""off-policy"" using existing data arising from experiments designed for other purposes, and how to design more efficient experiments. In particular, we consider contrasts between stochastic seeding strategies and analyze nonparametric estimators adapted from policy evaluation and importance sampling. We use simulations on real networks to show that the proposed estimators and designs can increase precision while yielding valid inference. We then apply our proposed estimators to two field experiments, one that assigned households to an intensive marketing intervention and one that assigned students to an anti-bullying intervention. ",Evaluating stochastic seeding strategies in networks
16,1044979217681862656,91634245,Brad Marston,['New paper with @abigail_plummer and Steve Tobias on an idealized model of the solar tachocline shows emergent periodic transitions reminiscent of the solar cycle.  <LINK>'],https://arxiv.org/abs/1809.00921,"Global magnetohydrodynamic (MHD) instabilities are investigated in a computationally tractable two-dimensional model of the solar tachocline. The model's differential rotation yields stability in the absence of a magnetic field, but if a magnetic field is present, a joint instability is observed. We analyze the nonlinear development of the instability via fully nonlinear direct numerical simulation, the generalized quasilinear approximation (GQL), and direct statistical simulation (DSS) based upon low-order expansion in equal-time cumulants. As the magnetic diffusivity is decreased, the nonlinear development of the instability becomes more complicated until eventually a set of parameters are identified that produce a previously unidentified long-term cycle in which energy is transformed from kinetic energy to magnetic energy and back. We find that the periodic transitions, which mimic some aspects of solar variability -- for example, the quasiperiodic seasonal exchange of energy between toroidal field and waves or eddies -- are unable to be reproduced when eddy-scattering processes are excluded from the model. ","Joint Instability and Abrupt Nonlinear Transitions in a Differentially
  Rotating Plasma"
17,1044976138639069185,3238448948,Carl T. Bergstrom,"['Our new paper: Why scatter plots suggest causality, and what we can do about it\n\n<LINK>\n\ntl;dr — Rotate them 45 degrees. <LINK>', ""@thelonglab @hadleywickham If people don't convince us diamond plots are stupid, we'll figure out how to do this, brute force if necessary, and make code available."", ""@JessicaHullman We thought about moving axis labels or working with variants on parallel coordinates plots. Didn't have other good ideas, to be honest."", '@JessicaHullman A big part of the user testing will be to see what happens to error rates.']",https://arxiv.org/abs/1809.09328,"Scatter plots carry an implicit if subtle message about causality. Whether we look at functions of one variable in pure mathematics, plots of experimental measurements as a function of the experimental conditions, or scatter plots of predictor and response variables, the value plotted on the vertical axis is by convention assumed to be determined or influenced by the value on the horizontal axis. This is a problem for the public understanding of scientific results and perhaps also for professional scientists' interpretations of scatter plots. To avoid suggesting a causal relationship between the x and y values in a scatter plot, we propose a new type of data visualization, the diamond plot. Diamond plots are essentially 45 degree rotations of ordinary scatter plots; by visually jarring the viewer they clearly indicate that she should not draw the usual distinction between independent/predictor variable and dependent/response variable. Instead, she should see the relationship as purely correlative. ","Why scatter plots suggest causality, and what we can do about it"
18,1044945666877747200,194377912,Brian Keating,['New research post with @andyfriedman2:\n“Constraints on Lorentz Invariance and CPT Violation using Optical Photometry and Polarimetry of Active Galaxies BL Lacertae and S5 B0716+714”. Really fun to write my first paper in optical astronomy! <LINK> <LINK>'],http://arxiv.org/abs/1809.08356,"Various quantum gravity approaches that extend beyond the standard model predict Lorentz Invariance and Charge-Parity-Time Violation at energies approaching the Planck scale. These models frequently predict a wavelength dependent speed of light, which would result in time delays between promptly emitted photons at different energies, as well as a wavelength-dependent rotation of the plane of linear polarization for photons resulting from vacuum birefringence. Here, we describe a pilot program with an automated system of small telescopes that can simultaneously conduct high cadence optical photometry and polarimetry of Active Galactic Nuclei (AGN) in multiple passbands. We use these observations as a proof-of-principle to demonstrate how such data can be used to test various Lorentz Violation models, including special cases of the Standard Model Extension (SME). In our initial campaign with this system, the Array Photo Polarimeter, we observed two AGN sources, including BL Lacertae at redshift z = 0.069, and S5 B0716+714 at z = 0.31. We demonstrate that optical polarimetry with a broadband Luminance filter combined with simultaneous $I_c$-band observations yields SME parameter constraints that are up to ~10 and ~30 times more sensitive than with a standard $I_c$-band filter, for SME models with mass dimension d = 5 and d = 6, respectively. Using only a small system of telescopes with an effective 0.45-m aperture, we further demonstrate d = 5 constraints for individual lines of sight that are within a factor of ~1-10 in sensitivity to comparable constraints from optical polarimetry with a 3.6-m telescope. Such an approach could significantly improve existing SME constraints via a polarimetric all-sky survey of AGN with multiple 1-meter class telescopes. ","Constraints on Lorentz Invariance and CPT Violation using Optical
  Photometry and Polarimetry of Active Galaxies BL Lacertae and S5 B0716+714"
19,1044642375417696256,192826908,Jorge Lillo-Box,"['Nice result today in @arxiv_org about the characterization of a new rocky super-Earth, K2-265b, by Kristine Lam at @WarwickAstro. Ternary diagram with its composition in the plot below. See paper at <LINK> <LINK>']",https://arxiv.org/abs/1809.08869,"We report the discovery of the super-Earth K2-265 b detected with K2 photometry. The planet orbits a bright (V_mag = 11.1) star of spectral type G8V with a period of 2.37 days. We obtained high-precision follow-up radial velocity measurements from HARPS, and the joint Bayesian analysis showed that K2-265 b has a radius of 1.71 +/- 0.11 R_earth and a mass of 6.54 +/- 0.84 M_earth, corresponding to a bulk density of 7.1 +/- 1.8 g/cm^3 . Composition analysis of the planet reveals an Earth-like, rocky interior, with a rock mass fraction of 80%. The short orbital period and small radius of the planet puts it below the lower limit of the photoevaporation gap, where the envelope of the planet could have eroded due to strong stellar irradiation, leaving behind an exposed core. Knowledge of the planet core composition allows us to infer the possible formation and evolution mechanism responsible for its current physical parameters. ",K2-265 b: A Transiting Rocky Super-Earth
20,1044509570671017984,440891105,Vahid Moosavi,"['Our recent paper on Data-Driven Design: Exploring new Structural Forms using Machine Learning and Graphic Statics, using UMAP developed by @leland_mcinnes and Self Organizing Maps <LINK> <LINK>']",https://arxiv.org/abs/1809.08660,"The aim of this research is to introduce a novel structural design process that allows architects and engineers to extend their typical design space horizon and thereby promoting the idea of creativity in structural design. The theoretical base of this work builds on the combination of structural form-finding and state-of-the-art machine learning algorithms. In the first step of the process, Combinatorial Equilibrium Modelling (CEM) is used to generate a large variety of spatial networks in equilibrium for given input parameters. In the second step, these networks are clustered and represented in a form-map through the implementation of a Self Organizing Map (SOM) algorithm. In the third step, the solution space is interpreted with the help of a Uniform Manifold Approximation and Projection algorithm (UMAP). This allows gaining important insights in the structure of the solution space. A specific case study is used to illustrate how the infinite equilibrium states of a given topology can be defined and represented by clusters. Furthermore, three classes, related to the non-linear interaction between the input parameters and the form space, are verified and a statement about the entire manifold of the solution space of the case study is made. To conclude, this work presents an innovative approach on how the manifold of a solution space can be grasped with a minimum amount of data and how to operate within the manifold in order to increase the diversity of solutions. ","Data-Driven Design: Exploring new Structural Forms using Machine
  Learning and Graphic Statics"
21,1044477141197434880,780575699035840512,Dirk L.,"['New preprint ""Sarrus rules and dihedral groups"". I would have filed this under ""general mathematics"", but arxiv decided to put it in ""combinatorics"". This will probably stay my only paper dealing with groups in some reasonable way!\n<LINK>']",https://arxiv.org/abs/1809.08948,"This paper is devoted to the analysis of a false generalization of the rule of Sarrus and its properties that can be derived with the help of dihedral groups. Further, we discuss a Sarrus-like scheme that could be helpful for students to memorize the calculation of a $4\times 4$ determinant. ",Sarrus rules and dihedral groups
22,1044403197836591104,608502805,THOMAS Guillaume,['New paper on arxiv today where we trace the stellar halo profile up to 220 kpc with BHB detected in The Canada-France-imaging-survey (CFIS) <LINK>'],https://arxiv.org/abs/1809.08245,"We present the stellar density profile of the outer halo of the Galaxy traced over a range of Galactocentric radii from $15< R_{GC} < 220$ kpc by blue horizontal branch (BHB) stars. These stars are identified photometrically using deep $u-$band imaging from the new Canada-France-Imaging-Survey (CFIS) that reaches 24.5 mag. This is combined with $griz$ bands from Pan-STARRS 1 and covers a total of $\sim4000$ deg$^2$ of the northern sky. We present a new method to select BHB stars that has low contamination from blue stragglers and high completeness. We use this sample to measure and parameterize the three dimensional density profile of the outer stellar halo. We fit the profile using (i) a simple power-law with a constant flattening (ii) a flattening that varies as a function of Galactocentric radius (iii) a broken power law profile. We find that outer stellar halo traced by the BHB is well modelled by a broken power law with a constant flattening of $q=0.86 \pm 0.02$, with an inner slope of $\gamma=4.24 \pm 0.08$. This is much steeper than the preferred outer profile that has a slope of $\beta=3.21\pm 0.07$ after a break radius of $r_b=41.4^{+2.5}_{-2.4}$ kpc. The outer profile of the stellar halo trace by BHB stars is shallower than that recently measured using RR Lyrae, a surprising result given the broad similarity of the ages of these stellar populations. ","A-type stars in the Canada-France Imaging Survey I. The stellar halo of
  the Milky Way traced to large radius by blue horizontal branch stars"
23,1044395189362552832,11778512,Mason Porter,"['Our new paper includes a particularly cheesy illustration of simplicial complexes.\n\n""Topological Data Analysis of Task-Based fMRI Data from Experiments on Schizophrenia"": <LINK>\n\nAuthors: B. J. Stolz, T. Emerson, S. Nahkuri, M. A. Porter, H. A. Harrington <LINK>']",https://arxiv.org/abs/1809.08504,"We use methods from computational algebraic topology to study functional brain networks, in which nodes represent brain regions and weighted edges encode the similarity of fMRI time series from each region. With these tools, which allow one to characterize topological invariants such as loops in high-dimensional data, we are able to gain understanding into low-dimensional structures in networks in a way that complements traditional approaches that are based on pairwise interactions. In the present paper, we use persistent homology to analyze networks that we construct from task-based fMRI data from schizophrenia patients, healthy controls, and healthy siblings of schizophrenia patients. We thereby explore the persistence of topological structures such as loops at different scales in these networks. We use persistence landscapes and persistence images to create output summaries from our persistent-homology calculations, and we study the persistence landscapes and images using $k$-means clustering and community detection. Based on our analysis of persistence landscapes, we find that the members of the sibling cohort have topological features (specifically, their 1-dimensional loops) that are distinct from the other two cohorts. From the persistence images, we are able to distinguish all three subject groups and to determine the brain regions in the loops (with four or more edges) that allow us to make these distinctions. ","Topological Data Analysis of Task-Based fMRI Data from Experiments on
  Schizophrenia"
24,1044257031220326400,953616889,Justin Read,"['New paper out today! The relationship between globular cluster (GC) system mass and halo mass appears to extend to the very smallest galaxies. This gives us new clues as to how GCs form, and a new tool for estimating halo masses.\n\n<LINK> <LINK>']",https://arxiv.org/abs/1809.07831,"High mass galaxies, with halo masses $M_{200} \ge 10^{10} M_{\odot}$, reveal a remarkable near-linear relation between their globular cluster (GC) system mass and their host galaxy halo mass. Extending this relation to the mass range of dwarf galaxies has been problematic due to the difficulty in measuring independent halo masses. Here we derive new halo masses based on stellar and HI gas kinematics for a sample of nearby dwarf galaxies with GC systems. We find that the GC system mass--halo mass relation for galaxies populated by GCs holds from halo masses of $M_{200} \sim 10^{14} M_{\odot}$ down to below $M_{200}$ $\sim 10^9 M_{\odot}$, although there is a substantial increase in scatter towards low masses. In particular, three well-studied ultra diffuse galaxies, with dwarf-like stellar masses, reveal a wide range in their GC-to-halo mass ratios. We compare our GC system--halo mass relation to the recent model of El Badry et al., finding that their fiducial model does not reproduce our data in the low mass regime. This may suggest that GC formation needs to be more efficient than assumed in their model, or it may be due to the onset of stochastic GC occupation in low mass halos. Finally, we briefly discuss the stellar mass-halo mass relation for our low mass galaxies with GCs, and we suggest some nearby dwarf galaxies for which searches for GCs may be fruitful. ","Extending the Globular Cluster System-Halo Mass Relation to the Lowest
  Galaxy Masses"
25,1044031960543363072,725442056886345729,Sudhanva Lalit,"['Here it is, our new paper:\n\n<LINK>', '@abinashpun Thanks mate :)']",https://arxiv.org/abs/1809.08126,"In simulations of binary neutron star mergers, the dense matter equation of state (EOS) is required over wide ranges of density and temperature as well as under conditions in which neutrinos are trapped, and the effects of magnetic fields and rotation prevail. Here we assess the status of dense matter theory and point out the successes and limitations of approaches currently in use. A comparative study of the excluded volume (EV) and virial approaches for the $np\alpha$ system using the equation of state of Akmal, Pandharipande and Ravenhall for interacting nucleons is presented in the sub-nuclear density regime. Owing to the excluded volume of the $\alpha$-particles, their mass fraction vanishes in the EV approach below the baryon density 0.1 fm$^{-3}$, whereas it continues to rise due to the predominantly attractive interactions in the virial approach. The EV approach of Lattimer et al. is extended here to include clusters of light nuclei such as d, $^3$H and $^3$He in addition to $\alpha$-particles. Results of the relevant state variables from this development are presented and enable comparisons with related but slightly different approaches in the literature. We also comment on some of the sweet and sour aspects of the supra-nuclear EOS. The extent to which the neutron star gravitational and baryon masses vary due to thermal effects, neutrino trapping, magnetic fields and rotation are summarized from earlier studies in which the effects from each of these sources were considered separately. Increases of about $20\% (\gtrsim 50\%)$ occur for rigid (differential) rotation with comparable increases occurring in the presence of magnetic fields only for fields in excess of $10^{18}$ Gauss. Comparatively smaller changes occur due to thermal effects and neutrino trapping. Some future studies to gain further insight into the outcome of dynamical simulations are suggested. ",Dense matter equation of state for neutron star mergers
26,1043234192874602496,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper appeared on arxiv. It is about DE and DM isocurvature from the Swampland conjecture. Interestingly, vector dark matter stands out as it naturally avoids the isocurvature and the right DM abundance can be generated in high-scale inf.\n\n<LINK>']",https://arxiv.org/abs/1809.07286,"We point out that the recently proposed Swampland conjecture on the potential gradient can lead to isocurvature perturbations of dark energy, if the quintessence field acquires large quantum fluctuations during high-scale inflation preferred by the conjecture. Also, if the quintessence field is coupled to a dark sector that contains dark matter, isocurvature perturbation of dark matter is similarly induced. Both isocurvature perturbations can be suppressed if the quintessence potential allows a tracker solution in the early Universe. We find that a vector field of mass $\lesssim {\cal O}(1)$ meV is an excellent dark matter candidate in this context, not only because the right abundance is known to be produced by quantum fluctuations during high-scale inflation without running afoul of isocurvature bounds, but also because its coupling to the quintessence does not spoil the flatness of the potential. ","Isocurvature Perturbations of Dark Energy and Dark Matter from the
  Swampland Conjecture"
27,1043125184591548417,91420905,Alex Smith,['My new paper! Correcting for Fibre Assignment Incompleteness in the DESI Bright Galaxy Survey <LINK>'],https://arxiv.org/abs/1809.07355,"The Dark Energy Spectroscopic Instrument (DESI) Bright Galaxy Survey (BGS) will be a survey of bright, low redshift galaxies, which is planned to cover an area of ~14,000 sq deg in 3 passes. Each pass will cover the survey area with ~2000 pointings, each of area ~8 sq deg. The BGS is currently proposed to consist of a bright high priority sample to an r-band magnitude limit r ~ 19.5, with a fainter low priority sample to r ~ 20. The geometry of the DESI fibre positioners in the focal plane of the telescope affects the completeness of the survey, and has a non-trivial impact on clustering measurements. Using a BGS mock catalogue, we show that completeness due to fibre assignment primarily depends on the surface density of galaxies. Completeness is high (>95%) in low density regions, but very low (<10%) in the centre of massive clusters. We apply the pair inverse probability (PIP) weighting correction to clustering measurements from a BGS mock which has been through the fibre assignment algorithm. This method is only unbiased if it is possible to observe every galaxy pair. To facilitate this, we randomly promote a small fraction of the fainter sample to be high priority, and dither the set of tile positions by a small angle. We show that inverse pair weighting combined with angular upweighting provides an unbiased correction to galaxy clustering measurements for the complete 3 pass survey, and also after 1 pass, which is highly incomplete. ","Correcting for Fibre Assignment Incompleteness in the DESI Bright Galaxy
  Survey"
28,1043045911683899392,1662363468,Amélie Saintonge,"['Introducing JINGLE, a new @eao_jcmt legacy survey all about dust and gas in galaxies. The paper presents the survey and sample, and gives a sneak preview of the cool science coming your way very soon. Stay tuned! <LINK>']",https://arxiv.org/abs/1809.07336,"JINGLE is a new JCMT legacy survey designed to systematically study the cold interstellar medium of galaxies in the local Universe. As part of the survey we perform 850um continuum measurements with SCUBA-2 for a representative sample of 193 Herschel-selected galaxies with M*>10^9Msun, as well as integrated CO(2-1) line fluxes with RxA3m for a subset of 90 of these galaxies. The sample is selected from fields covered by the Herschel-ATLAS survey that are also targeted by the MaNGA optical integral-field spectroscopic survey. The new JCMT observations combined with the multi-wavelength ancillary data will allow for the robust characterization of the properties of dust in the nearby Universe, and the benchmarking of scaling relations between dust, gas, and global galaxy properties. In this paper we give an overview of the survey objectives and details about the sample selection and JCMT observations, present a consistent 30 band UV-to-FIR photometric catalog with derived properties, and introduce the JINGLE Main Data Release (MDR). Science highlights include the non-linearity of the relation between 850um luminosity and CO line luminosity, and the serendipitous discovery of candidate z>6 galaxies. ","JINGLE, a JCMT legacy survey of dust and gas for galaxy evolution
  studies: I. Survey overview and first results"
29,1043036765680889856,776765039726460929,Carlo Felice Manara,"['1/4 New paper based on @almaobs <LINK> @ESO VLT/X-Shooter #GaiaDR2 data: \n""Why do protoplanetary disks appear not massive enough to form the known exoplanet population?""\n<LINK>\nwith A LOT of help from Alessandro Morbidelli and Tristan Guillot <LINK>', '2/4 Take home: we do not have enough mass in disks at 1-3 Myr to explain exoplanetary systems, even when considering the mass in solids in disks vs the mass of solids in planets. \nThis is now true for a large range of stellar masses covered both in disks and exoplanet surveys https://t.co/ebtdU5l6eN', '3/4 Possible solutions:\n0) protoplanetary disk masses are underestimated (but I have reasons to believe this is not the case)\n1) cores of planets have formed very rapidly (&lt;0.1-1 Myr) \n2) disks are continuously replenished of fresh planet-forming material from the environment', '4/4 I want to also point to (some of the) relevant references for the various points:\n0) https://t.co/i1hlQnitOx https://t.co/bvRTlRGZzn\n1) https://t.co/mISVRLILyh https://t.co/nphETM5oYV https://t.co/mb87Cek9xN \n2) https://t.co/Zy6lHNfjhX https://t.co/3pZFSDZoJS', 'P.S. the right link is https://t.co/yElqOqW9EI.....', '@exohugh Assuming at least 100% efficiency, which is not expected by models?']",https://arxiv.org/abs/1809.07374,"When and how planets form in protoplanetary disks is still a topic of discussion. Exoplanet detection surveys and protoplanetary disk surveys are now providing results that allow us to have new insights. We collect the masses of confirmed exoplanets and compare their dependence with stellar mass with the same dependence for protoplanetary disk masses measured in ~1-3 Myr old star-forming regions. The latter are recalculated by us using the new estimates of their distances derived from Gaia DR2 parallaxes. We note that single and multiple exoplanetary systems form two different populations, probably pointing to a different formation mechanism for massive giant planets around very low mass stars. While expecting that the mass in exoplanetary systems is much lower than the measured disk masses, we instead find that exoplanetary systems masses are comparable or higher than the most massive disks. This same result is found also by converting the measured planet masses into heavy-element content (core masses for the giant planets and full masses for the super-Earth systems) and by comparing this value with the disk dust masses. Unless disk dust masses are heavily underestimated, this is a big conundrum. An extremely efficient recycling of dust particles in the disk cannot solve this conundrum. This implies that either the cores of planets have formed very rapidly (<0.1-1 Myr) and large amount of gas is expelled on the same timescales from the disk, or that disks are continuously replenished of fresh planet-forming material from the environment. These hypotheses can be tested by measuring disk masses in even younger targets and by better understanding if and how the disks are replenished by their surroundings. ","Why do protoplanetary disks appear not massive enough to form the known
  exoplanet population?"
30,1043011078840446976,31388740,arnab chatterjee,['New paper:\nAnalyzing behavioral trends in community driven discussion platforms like Reddit\n<LINK>'],https://arxiv.org/abs/1809.07087,"The aim of this paper is to present methods to systematically analyze individual and group behavioral patterns observed in community driven discussion platforms like Reddit where users exchange information and views on various topics of current interest. We conduct this study by analyzing the statistical behavior of posts and modeling user interactions around them. We have chosen Reddit as an example, since it has grown exponentially from a small community to one of the biggest social network platforms in the recent times. Due to its large user base and popularity, a variety of behavior is present among users in terms of their activity. Our study provides interesting insights about a large number of inactive posts which fail to gather attention despite their authors exhibiting Cyborg-like behavior to draw attention. We also present interesting insights about short-lived but extremely active posts emulating a phenomenon like Mayfly Buzz. Further, we present methods to find the nature of activity around highly active posts to determine the presence of Limelight hogging activity, if any. We analyzed over $2$ million posts and more than $7$ million user responses to them during entire 2008 and over $63$ million posts and over $608$ million user responses to them from August 2014 to July 2015 amounting to two one-year periods, in order to understand how social media space has evolved over the years. ","Analyzing behavioral trends in community driven discussion platforms
  like Reddit"
31,1042843163600216064,31388740,arnab chatterjee,['New paper: Prosocial or Selfish? Agents with different behaviors for Contract Negotiation using Reinforcement Learning\n<LINK> #DeepLearning #rl'],https://arxiv.org/abs/1809.07066,"We present an effective technique for training deep learning agents capable of negotiating on a set of clauses in a contract agreement using a simple communication protocol. We use Multi Agent Reinforcement Learning to train both agents simultaneously as they negotiate with each other in the training environment. We also model selfish and prosocial behavior to varying degrees in these agents. Empirical evidence is provided showing consistency in agent behaviors. We further train a meta agent with a mixture of behaviors by learning an ensemble of different models using reinforcement learning. Finally, to ascertain the deployability of the negotiating agents, we conducted experiments pitting the trained agents against human players. Results demonstrate that the agents are able to hold their own against human players, often emerging as winners in the negotiation. Our experiments demonstrate that the meta agent is able to reasonably emulate human behavior. ","Prosocial or Selfish? Agents with different behaviors for Contract
  Negotiation using Reinforcement Learning"
32,1042525613448298496,238054276,Xianyu Tan,"['This is a movie on the 1D atmospheric variability presented in Figure 1 and 2 of our recent brown dwarf +  directly imaged giant planet paper <LINK>.  This paper presents a brand new, very natural mechanism generating spontaneous variability for BDs and EGPs. <LINK>']",https://arxiv.org/abs/1809.06467,"Growing observational evidence has suggested active meteorology in atmospheres of brown dwarfs (BDs) and directly imaged extrasolar giant planets (EGPs). In particular, a number of surveys have shown that near-IR brightness variability is common among L and T dwarfs. Despite initial understandings of atmospheric dynamics which is the major cause of the variability by previous studies, the detailed mechanism of variability remains elusive, and we need to seek a natural, self-consistent mechanism. Clouds are important in shaping the thermal structure and spectral properties of these atmospheres via large opacity, and we expect the same for inducing atmospheric variability. In this work, using a time-dependent one-dimensional model that incorporates a self-consistent coupling between the thermal structure, convective mixing, cloud radiative heating/cooling and condensation/evaporation of clouds, we show that radiative cloud feedback can drive spontaneous atmospheric variability in both temperature and cloud structure in conditions appropriate for BDs and directly imaged EGPs. The typical periods of variability are one to tens of hours with typical amplitude of the variability up to hundreds of Kelvins in effective temperature. The existence of variability is robust over a wide range of parameter space, but the detailed evolution of variability is sensitive to model parameters. Our novel, self-consistent mechanism has important implications for the observed flux variability of BDs and directly imaged EGPs, especially those evolve in short timescales. It is also a promising mechanism for cloud breaking, which has been proposed to explain the L/T transition of brown dwarfs. ","Atmospheric variability driven by radiative cloud feedback in brown
  dwarfs and directly imaged extrasolar giant planets"
33,1042446396027219968,1004365363574902784,Kevin J. Kelly,"['New paper out today! We explored a new signal channel for beam-produced dark matter being detected at neutrino experiments, called ""dark tridents.""\n\n<LINK>\n\nThanks to my collaborators for an interesting project! <LINK>']",https://arxiv.org/abs/1809.06388,"We present dark tridents, a new channel for exploring dark sectors in short-baseline neutrino experiments. Dark tridents are clean, distinct events where, like neutrino tridents, the scattering of a very weakly coupled particle leads to the production of a lepton--antilepton pair. Dark trident production occurs in models where long-lived dark-sector particles are produced along with the neutrinos in a beam-dump environment and interact with neutrino detectors downstream, producing an on-shell boson which decays into a pair of charged leptons. We focus on a simple model where the dark matter particle interacts with the standard model exclusively through a dark photon, and concentrate on the region of parameter space where the dark photon mass is smaller than twice that of the dark matter particle and hence decays exclusively into standard-model particles. We compute event rates and discuss search strategies for dark tridents from dark matter at the current and upcoming liquid argon detectors aligned with the Booster beam at Fermilab -- MicroBooNE, SBND, and ICARUS -- assuming the dark sector particles are produced off-axis in the higher energy NuMI beam. We find that MicroBooNE has already recorded enough data to be competitive with existing bounds on this dark sector model, and that new regions of parameter space will be probed with future data and experiments. ",Dark Tridents at Off-Axis Liquid Argon Neutrino Detectors
34,1042417592189825025,2352151766,Thomas Klausch,['Our new paper on Bayesian Optimal Treatment Regimes is online on arXiv: <LINK> We describe machine learning of optimal treatment assignment strategies that avoid unnecessary toxicity/side-effects and we show interesting results for oropharynx cancer treatment.'],https://arxiv.org/abs/1809.06679,"Optimal treatment regimes (OTR) are individualised treatment assignment strategies that identify a medical treatment as optimal given all background information available on the individual. We discuss Bayes optimal treatment regimes estimated using a loss function defined on the bivariate distribution of dichotomous potential outcomes. The proposed approach allows considering more general objectives for the OTR than maximization of an expected outcome (e.g., survival probability) by taking into account, for example, unnecessary treatment burden. As a motivating example we consider the case of oropharynx cancer treatment where unnecessary burden due to chemotherapy is to be avoided while maximizing survival chances. Assuming ignorable treatment assignment we describe Bayesian inference about the OTR including a sensitivity analysis on the unobserved partial association of the potential outcomes. We evaluate the methodology by simulations that apply Bayesian parametric and more flexible non-parametric outcome models. The proposed OTR for oropharynx cancer reduces the frequency of the more burdensome chemotherapy assignment by approximately 75% without reducing the average survival probability. This regime thus offers a strong increase in expected quality of life of patients. ","Estimating Bayesian Optimal Treatment Regimes for Dichotomous Outcomes
  using Observational Data"
35,1042376850423590913,966155324293046272,Constantin Schrade,['Our new paper is on the arXiv! <LINK>  \nIt contributes to a clarification on the rather long-standing debate on the nature of zero-energy bound states in candidate systems for topological superconductors.'],https://arxiv.org/abs/1809.06370,"We study a Cooper pair transistor realized by a mesoscopic superconductor island that couples to a pair of $s$-wave superconducting leads. For a trivial island, the critical supercurrent between the leads exhibits a well-known $2e$-periodicity in the island-gate charge. Here, we show that for an island with spatially separated zero-energy Majorana or Andreev bound states the periodicity of the magnitude of the critical supercurrent transitions to $1e$ in the island-gate charge. Moreover, for Andreev bound states the current-phase relation displays a sign reversal when the parity of the charge ground state of the island changes between even and odd. Notably, for Majorana bound states the same sign reversal does not occur. Our results highlight the relevance of measuring the full current-phase relation of a Cooper pair transistor for clarifying the nature of zero-energy bound states in candidate systems for topological superconductors and provide an initial step towards integrating Majorana qubits in superconducting circuits. ","Andreev or Majorana, Cooper finds out"
36,1042370875792740352,340197502,David G. Cerdeño,"['The neutrino floor can be raised by new physics in the neutrino sector. In this paper <LINK> with @CelineBoehm1 @PedroANMachado @andres0l1 and Elliott Reid we find a large increase at low masses, relevant for future dark matter detectors @IPPP_Durham #neutrino <LINK>']",https://arxiv.org/abs/1809.06385,"In this paper, we compute the contribution to the coherent elastic neutrino-nucleus scattering cross section from new physics models in the neutrino sector. We use this information to calculate the maximum value of the so-called neutrino floor for direct dark matter detection experiments, which determines when these detectors are sensitive to the neutrino background. After including all relevant experimental constraints in different simplified neutrino models, we have found that the neutrino floor can increase by various orders of magnitude in the region of dark matter masses below 10 GeV in the case of scalar mediators, however, this spectacular enhancement is subject to the re-examination of supernovae bounds. The increase is approximately a factor of two for vector mediators. In the light of these results, future claims by direct detection experiments exploring the low-mass window must be carefully examined if a signal is found well above the expected Standard Model neutrino floor. ",How high is the neutrino floor?
37,1042216247885217793,60768243,Dr. Liz Munch,['My new paper with Jesse Berwald and Joel Gottlieb on computing Wasserstein distance for persistence diagrams using a D-Wave quantum computer is now up! <LINK>'],https://arxiv.org/abs/1809.06433,"Persistence diagrams are a useful tool from topological data analysis which can be used to provide a concise description of a filtered topological space. What makes them even more useful in practice is that they come with a notion of a metric, the Wasserstein distance (closely related to but not the same as the homonymous metric from probability theory). Further, this metric provides a notion of stability; that is, small noise in the input causes at worst small differences in the output. In this paper, we show that the Wasserstein distance for persistence diagrams can be computed through quantum annealing. We provide a formulation of the problem as a Quadratic Unconstrained Binary Optimization problem, or QUBO, and prove correctness. Finally, we test our algorithm, exploring parameter choices and problem size capabilities, using a D-Wave 2000Q quantum annealing computer. ","Computing Wasserstein Distance for Persistence Diagrams on a Quantum
  Computer"
38,1042054735539449857,112053784,Kaley Brauer 💫,"['My new paper with @alexanderpji is on arXiv! It’s about how we can likely use old stars we see today (in this case, r-II stars) to learn about the smallest galaxies that merged together to form the Milky Way. This is an example of  *stellar archaeology*!✨ <LINK>', 'The old stars in the extended outskirts of the Milky Way (called our “stellar halo”) preserve a ton of information about the early universe and the history of our galaxy. We don’t know how to decode that info, though, so I’ve been developing a model to help.', 'That’s why it’s called “stellar archaeology”: we’re looking at old stars to learn about the history of our galaxy/universe! 🌟', 'This paper shows the first result of the model I’ve been working on. We’ve found a likely connection between old r-process-enhanced stars and small, faint galaxies that were destroyed during the formation of the Milky Way, the smallest building blocks of our galaxy.', 'This is extra exciting because the r-process forms the heaviest elements in our galaxy (including gold 🥇), so this could also help us understand the origin of these elements.']",https://arxiv.org/abs/1809.05539,"The highly r-process enhanced (r-II) metal-poor halo stars we observe today could play a key role in understanding early ultra-faint dwarf galaxies, the smallest building blocks of the Milky Way. If a significant fraction of metal-poor r-II halo stars originated in the ultra-faint dwarf galaxies that merged to help form the Milky Way, observations of r-II stars could help us study these now-destroyed systems and probe the formation history of our Galaxy. To conduct our initial investigation into this possible connection, we use high-resolution cosmological simulations of Milky-Way-mass galaxies from the Caterpillar suite in combination with a simple, empirically motivated treatment of r-process enrichment. We determine the fraction of metal-poor halo stars that could have formed from highly r-process enhanced gas in now-destroyed low-mass ultra-faint dwarf galaxies, the simulated r-II fraction, and compare it to the ""as observed"" r-II fraction. We find that the simulated fraction, f_{r-II,sim} ~ 1-2%, can account for around half of the ""as observed"" fraction, f_{r-II,obs} ~ 2-4%. The ""as observed"" fraction likely overrepresents the fraction of r-II stars due to incomplete sampling, though, meaning f_{r-II,sim} likely accounts for more than half of the true f_{r-II,obs}. Further considering some parameter variations and scatter between individual simulations, the simulated fraction can account for around 20-80% of the ""as observed"" fraction. ","The Origin of r-process Enhanced Metal-Poor Halo Stars In Now-Destroyed
  Ultra-Faint Dwarf Galaxies"
39,1042044193387040769,702241209276829697,Cecilia Garraffo 💚,['Our paper on new radio signatures of close in exoplanets is out. Check it out! <LINK> @cosmodrake @SofiaMoschou @AstroRaikoh <LINK>'],https://arxiv.org/abs/1809.06279,"The search for exoplanets in the radio bands has been focused on detecting radio emissions produced by the interaction between magnetized planets and the stellar wind (auroral emission). Here we introduce a new tool, which is part of our MHD stellar corona model, to predict the ambient coronal radio emission and its modulations induced by a close planet. For simplicity, the present work assumes that the exoplanet is stationary in the frame rotating with the stellar rotation. We explore the radio flux modulations using a limited parameter space of idealized cases by changing the magnitude of the planetary field, its polarity, the planetary orbital separation, and the strength of the stellar field. We find that the modulations induced by the planet could be significant and observable in the case of hot Jupiter planets --- above 100% modulation with respect to the ambient flux in the $10-100~MHz$ range in some cases, and 2-10% in the frequency bands above $250~MHz$ for some cases. Thus, our work indicates that radio signature of exoplanets might not be limited to low-frequency radio range. We find that the intensity modulations are sensitive to the planetary magnetic field polarity for short-orbit planets, and to the stellar magnetic field strength for all cases. The new radio tool, when applied to real systems, could provide predictions for the frequency range at which the modulations can be observed by current facilities. ",Exoplanet Modulation of Stellar Coronal Radio Emission
40,1041964571307782145,2780063142,Sabine Kraml,"['New today: <LINK>. Our paper on the Simplified Likelihood Framework for communicating likelihood information from #LHC measurements @ATLASexperiment @CMSexperiment to the outside world.', 'Reference Python implementation and demo are publicly available at https://t.co/Rgyh9iTk5Q  #opensource #OpenData']",https://arxiv.org/abs/1809.05548,"We discuss the simplified likelihood framework as a systematic approximation scheme for experimental likelihoods such as those originating from LHC experiments. We develop the simplified likelihood from the Central Limit Theorem keeping the next-to-leading term in the large $N$ expansion to correctly account for asymmetries. Moreover, we present an efficient method to compute the parameters of the simplified likelihood from Monte Carlo simulations. The approach is validated using a realistic LHC-like analysis, and the limits of the approximation are explored. Finally, we discuss how the simplified likelihood data can be conveniently released in the HepData error source format and automatically built from it, making this framework a convenient tool to transmit realistic experimental likelihoods to the community. ",The Simplified Likelihood Framework
41,1041778144020242433,45105022,Riccardo Sapienza,"['New paper! Nanoscale design of the local density of optical states, or how to make an emitter shine 800 times more, from the creative mind of @sandromignuzzi, with Stefano Vezzoli &amp; Stefan Maier @ImperialPhysics and Bill Barnes &amp; Simon Horsley @UniofExeter <LINK> <LINK>']",https://arxiv.org/abs/1809.05514,"We propose a design concept for tailoring the local density of optical states (LDOS) in dielectric nanostructures, based on the phase distribution of the scattered optical fields induced by point-like emitters. First we demonstrate that the LDOS can be expressed in terms of a coherent summation of constructive and destructive contributions. By using an iterative approach, dielectric nanostructures can be designed to effectively remove the destructive terms. In this way dielectric Mie resonators, featuring low LDOS for electric dipoles, can be reshaped to enable enhancements of three orders of magnitude. To demonstrate the generality of the method, we also design nanocavities that enhance the radiated power of a circular dipole, a quadrupole and an arbitrary collection of coherent dipoles. Our concept provides a powerful tool for high-performance dielectric resonators, and affords fundamental insights into light-matter coupling at the nanoscale. ",Nanoscale design of the local density of optical states
42,1040622283616907265,628967454,Bruce Desmarais,"['New paper on arXiv with @B_W_Campbell and @SkylerCranmer on the regime-type configuration of triads in the international conflict network. Thanks for disseminating, @alexvespi! <LINK> <LINK>']",http://arxiv.org/abs/1809.04141,"Decades of research has found that democratic dyads rarely exhibit violent tendencies, making the democratic peace arguably the principal finding of Peace Science. However, the democratic peace rests upon a dyadic understanding of conflict. Conflict rarely reflects a purely dyadic phenomena---even if a conflict is not multi-party, multiple states may be engaged in distinct disputes with the same enemy. We postulate a network theory of conflict that treats the democratic peace as a function of the competing interests of mixed-regime dyads and the strategic inefficiencies of fighting with enemies' enemies. Specifically, we find that a state's decision to engage in conflict with a target state is conditioned by the other states in which the target state is in conflict. When accounting for this network effect, we are unable to find support for the democratic peace. This suggests that the major finding of three decades worth of conflict research is spurious. ",Triangulating War: Network Structure and the Democratic Peace
43,1040566123081678848,870695740359573506,Jan Gieseler,['check out our latest paper on @arxiv\n<LINK>\nWe propose a new architecture for quantum networks based on magnonic excitations in nanomagnets and superconducting loops. Looking forward to seeing this realized in an experiment!!\n#quantum #QuantumComputing #nanotech'],https://arxiv.org/abs/1809.04901,"We show theoretically that a network of superconducting loops and magnetic particles can be used to implement magnonic crystals with tunable magnonic band structures. In our approach, the loops mediate interactions between the particles and allow magnetic excitations to tunnel over long distances. As a result, different arrangements of loops and particles allow one to engineer the band structure for the magnonic excitations. Furthermore, we show how magnons in such crystals can serve as a quantum bus for long-distance magnetic coupling of spin qubits. The qubits are coupled to the magnets in the network by their local magnetic-dipole interaction and provide an integrated way to measure the state of the magnonic quantum network. ",Hybrid Architecture for Engineering Magnonic Quantum Networks
44,1040536109049151488,20703003,Peter B Denton,"['I have a new paper out today on the highest energy neutrinos in the universe! Next generation experiments like the Giant Radio Array for Neutrino Detection will be able to measure this guaranteed flux. But what happens after it has been measured? <LINK> [1/5]', ""This guaranteed flux comes from the highest energy *particles* of any kind in the universe: extreme energy cosmic rays. They scatter in the vacuum en route to the Earth and lose energy, creating the so-called cosmogenic neutrino flux. This flux hasn't been measured yet, ... [2/5]"", '...but ambitious experiments like GRAND should be able to. This paper answers the question, ""What next?"" It turns out that we can learn a great deal about which sources are accelerating particles and about the particles themselves, although not with neutrino data alone. [3/5]', 'Neutrino data alone cannot tell the difference between ""lots of sources"" (right side of the figure) and ""favorable composition"" (bottom side of the figure). With other information the degeneracy can be broken though, with great results.\n\n(terms and conditions apply) [4/5] https://t.co/yxMeOkblUi', '(Also this paper is exciting personally because it means that my inspire graph has hit double digits for the year!) [5/5]']",https://arxiv.org/abs/1809.04866,"The sources of cosmic rays with energies above 55 EeV are still mysterious. A guaranteed associated flux of ultra high energy neutrinos known as the cosmogenic neutrino flux will be measured by next generation radio facilities, such as the proposed Giant Radio Array for Neutrino Detection (GRAND). By using the orthogonal information provided by the cosmogenic neutrino flux, we here determine the prospects of GRAND to constrain the source redshift evolution and the chemical composition of the cosmic ray sources. If the redshift evolution is known, independently on GRAND's energy resolution, GRAND with 200,000 antennas will constrain the proton/iron fraction to the $\sim5-10\%$ level after one year of data taking; on the other hand, if hints on the average source composition are given, GRAND will measure the redshift evolution of the sources to a $\sim 10\%$ uncertainty. However, the foreseen configuration of GRAND alone will not be able to break the degeneracy between redshift evolution of the sources and their composition. Our findings underline the discriminating potential of next generation radio array detectors and motivate further efforts in this direction. ","Cosmogenic Neutrinos Through the GRAND Lens Unveil the Nature of Cosmic
  Accelerators"
45,1040477100581564418,430865632,Patrick Stöcker,"['New paper: ""Freeze-in production of decaying dark matter in five steps\n"" <LINK> .\nWe study the production and evolution of a light dark ""Higgs"" with a feeble coupling to the SM Higgs <LINK>']",https://arxiv.org/abs/1809.04849,"We study the cosmological evolution and phenomenological properties of scalar bosons in the keV to MeV range that have a tiny mixing with the Standard Model Higgs boson. The mixing determines both the abundance of light scalars produced via the freeze-in mechanism and their lifetime. Intriguingly, the parameters required for such scalars to account for all of the dark matter in the present Universe generically predict lifetimes comparable to the sensitivity of present and future indirect detection experiments. In order to accurately determine the relic abundance of light scalars, we calculate freeze-in yields including effects from finite temperatures and quantum statistics and develop a new approach for solving the Boltzmann equation for number-changing processes in the dark sector. We find that light scalars can potentially explain the anomalous x-ray emission at 3.5 keV, while evading constraints from structure formation and predicting potentially observable self-interaction cross sections. ",Freeze-in production of decaying dark matter in five steps
46,1040321373569261569,75248576,Ben Schuetz,['Have a look at my new paper regarding photometer advancements at the Boquete Optical SETI Observaotry.  <LINK>'],https://arxiv.org/abs/1809.01956,"Progress at the privately owned Boquete Optical SETI Observatory in Panama and the Owl Observatory in Michigan is reported. The Boquete Observatory has been dedicated to the development of innovative optical SETI detectors and observations since 2010. It is currently equipped with a 0.5 meter Newtonian main telescope and a piggybacked 0.35 meter Cassegrain for tracking. Although small, the observatory's telescope and detector system has capabilities that are equivalent to most other institutional optical SETI facilities (Schuetz, M. et al., 2016). The optical SETI detectors at Boquete have evolved through many stages from a three photomultiplier coincidence detector to the current single photomultiplier version capable of detecting pulse widths up to 50 ns and for coincidence detection against a wide range of stellar background counts. The Owl Observatory photometer has similarly been in continuous development to improve performance. These activities have sought to provide practical solutions to the needs of optical SETI. Over the past 5 years the search for laser like signals at Boquete has included over 5000 stellar objects. Yet, until early 2017 year there were large gaps in the search parameters that limited the thoroughness of those searches.Reported herein are developments that have filled many of those gaps further extending the search boundaries. With the new capabilities, the search through the list of stellar candidates out to 200 ly was begun anew. ","Recent Developments at the Boquete Optical SETI Observatory and Owl
  Observatory"
47,1040193415688974337,839104540985151490,IPPP Durham,"['New IPPP paper: ""Studies of Dimension-Six EFT effects in Vector Boson Scattering"" by Raquel Gomez-Ambrosio <LINK>']",http://arxiv.org/abs/1809.04189,"We discuss the implications of dimension-six operators of the Effective Field Theory (EFT) framework in the study of Vector Boson Scattering (VBS) in the $pp \to Z Z j j $ channel. We show that operators of dimension-six should not be neglected in favour of those of dimension-eight. We observe that this process is very sensitive to some of the operators commonly fit using LEP and Higgs data, and that it can be used to improve the bounds on the former. Further we show that other operators than the ones generating anomalous triple and quartic gauge couplings (aTGCs/aQCGs) can have a non-negligible impact on the total and differential rates and their shapes. For this reason, a correct interpretation of the experimental results can only be achieved by including all the relevant, bosonic and fermionic operators; we finally discuss how such an interpretation of experimental measurements can be done. ",Studies of Dimension-Six EFT effects in Vector Boson Scattering
48,1040106477359427584,2969696397,Ion Nechita,['New paper <LINK> with Andreas Bluhm: compatibility of quantum measurements is equivalent to inclusion of the matrix jewel. A new application of algebraic convexity (and of the theory of free spectrahedra) to quantum information - or is it the other way around ???'],https://arxiv.org/abs/1809.04514,"In this work, we establish the connection between the study of free spectrahedra and the compatibility of quantum measurements with an arbitrary number of outcomes. This generalizes previous results by the authors for measurements with two outcomes. Free spectrahedra arise from matricial relaxations of linear matrix inequalities. A particular free spectrahedron which we define in this work is the matrix jewel. We find that the compatibility of arbitrary measurements corresponds to the inclusion of the matrix jewel into a free spectrahedron defined by the effect operators of the measurements under study. We subsequently use this connection to bound the set of (asymmetric) inclusion constants for the matrix jewel using results from quantum information theory and symmetrization. The latter translate to new lower bounds on the compatibility of quantum measurements. Among the techniques we employ are approximate quantum cloning and mutually unbiased bases. ","Compatibility of quantum measurements and inclusion constants for the
  matrix jewel"
49,1040104569768828928,326915032,Conor A Nixon,"[""Titan molecules update: first paper by my research intern, Nick  Lombardo, gives a new estimate of how much propene (used to make  polypropylene on Earth) is floating in Titan's atmosphere. Article: <LINK> <LINK>"", '@rosaly_lopes Thanks!']",https://arxiv.org/abs/1809.04102,"Of the C$_{3}$H$_{x}$ hydrocarbons, propane (C${_3}$H$_{8}$) and propyne (methylacetylene, CH$_{3}$C$_{2}$H) were first detected in Titan's atmosphere during the Voyager 1 flyby in 1980. Propene (propylene, C$_{3}$H$_{6}$) was first detected in 2013 with data from the Composite InfraRed Spectrometer (CIRS) instrument on Cassini. We present the first measured abundance profiles of propene on Titan from radiative transfer modeling, and compare our measurements to predictions derived from several photochemical models. Near the equator, propene is observed to have a peak abundance of 10 ppbv at a pressure of 0.2 mbar. Several photochemical models predict the amount at this pressure to be in the range 0.3 - 1 ppbv and also show a local minimum near 0.2 mbar which we do not see in our measurements. We also see that propene follows a different latitudinal trend than the other C$_{3}$ molecules. While propane and propyne concentrate near the winter pole, transported via a global convective cell, propene is most abundant above the equator. We retrieve vertical abundances profiles between 125 km and 375 km for these gases for latitude averages between 60$^{\circ}$S to 20$^{\circ}$S, 20$^{\circ}$S to 20$^{\circ}$N, and 20$^{\circ}$N to 60$^{\circ}$N over two time periods, 2004 through 2009 representing Titan's atmosphere before the 2009 equinox, and 2012 through 2015 representing time after the equinox. Additionally, using newly corrected line data, we determined an updated upper limit for allene (propadiene, CH$_{2}$CCH$_{2}$, the isomer of propyne). We claim a 3-$\sigma$ upper limit mixing ratio of 2.5$\times$10$^{-9}$ within 30$^\circ$ of the equator. The measurements we present will further constrain photochemical models by refining reaction rates and the transport of these gases throughout Titan's atmosphere. ","Spatial and Seasonal Variations in C3Hx Hydrocarbon Abundance in Titan's
  Stratosphere from Cassini CIRS Observations"
50,1039826180583706624,7318592,Adrià Recasens,"['Happy to release paper and code for the Saliency Sampler, our new layer for improving spatial sampling of input images in CNNs. We just presented it at #ECCV2018. Getting closer to have our own Instagram filter. \nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1809.03355,"We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: this http URL ",Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks
51,1039767129611030530,41761979,Jhon W. González,"['My new paper ""Thermopower of graphene nanoribbons in the pseudodiffusive regime"" diffusive  behavior without impurities #graphene #impurities #Physics #arXiv \n<LINK>']",https://arxiv.org/abs/1809.02822,"Thermoelectric measurements for graphene ribbons are currently performed on samples that include atomic disorder via defects and irregular edges. In this work, we investigate the thermopower or Seebeck coefficient of graphene ribbons within the linear response theory and the Landauer formalism, and we consider the diffusive regime taken as a limit of the ribbon aspect ratio. We find that the thermopower and the electronic conductivity depend not only on the aspect ratio, but also on chemical potential and temperature, which are set here as key parameters. The obtained numerical results with temperature and doping are brought into contact with the thermoelectric measurements on disordered graphene ribbons with good agreement. ",Thermopower of Graphene Nanoribbons in the Pseudodiffusive Regime
52,1039759626596769792,2952300430,Aaron Jackson,['New paper on arXiv\n\n3D Human Body Reconstruction from a Single Image via Volumetric  Regression\n\n<LINK>'],https://arxiv.org/abs/1809.03770,"This paper proposes the use of an end-to-end Convolutional Neural Network for direct reconstruction of the 3D geometry of humans via volumetric regression. The proposed method does not require the fitting of a shape model and can be trained to work from a variety of input types, whether it be landmarks, images or segmentation masks. Additionally, non-visible parts, either self-occluded or otherwise, are still reconstructed, which is not the case with depth map regression. We present results that show that our method can handle both pose variation and detailed reconstruction given appropriate datasets for training. ","3D Human Body Reconstruction from a Single Image via Volumetric
  Regression"
53,1039686486881431552,19510090,Julian Togelius,"[""Say you have an AI method and don't have the resources to test it on all possible benchmark problems. How do you select which games to test it on? In our new paper, we use information theory to describe how to do this.\n<LINK> <LINK>"", 'The paper, ""A Continuous Information Gain Measure to Find the Most Discriminatory  Problems for AI Benchmarking"", is by @matthew_stephe @DamorinSolusar @Amidos2006 @john_levine Jochen Renz, I and @ChristophSalge', ""The core idea is that you want to test your algorithm on the problems that best separate between existing algorithms, because that's how you can find the most information about your algorithm. Thus information theory."", 'This paper is partly a response to the worrying trend where it is becoming very hard to do AI research without Google-scale resources. For example, you might have GPUs enough to test your new RL algorithm on some ALE games, but not all of them.', 'We tested the methods on the games (and many submitted algorithms) in the @gvgai benchmark set. As a side effect we derived some really nice tables of correlations between GVGAI games, and identified some games that make algorithms behave very differently.', 'But the core idea is applicable to many different types of problems and algorithms, including supervised learning (where there are famously a very large number of benchmark datasets to test on).', 'We hope that this measure can help researchers both doing good science without astronomic resources, and to stop cherry-picking benchmark problems that fit their methods. But for this to happen, there needs to be agreement on which are the most discriminatory problems in a set.', '@shyamal_chandra @github I guess @DamorinSolusar @matthew_stephe could tell you']",https://arxiv.org/abs/1809.02904,"This paper introduces an information-theoretic method for selecting a subset of problems which gives the most information about a group of problem-solving algorithms. This method was tested on the games in the General Video Game AI (GVGAI) framework, allowing us to identify a smaller set of games that still gives a large amount of information about the abilities of different game-playing agents. This approach can be used to make agent testing more efficient. We can achieve almost as good discriminatory accuracy when testing on only a handful of games as when testing on more than a hundred games, something which is often computationally infeasible. Furthermore, this method can be extended to study the dimensions of the effective variance in game design between these games, allowing us to identify which games differentiate between agents in the most complementary ways. ","A Continuous Information Gain Measure to Find the Most Discriminatory
  Problems for AI Benchmarking"
54,1039660004708499456,174734024,Danny Lange,['Pre-print of our new paper on ML-Agents --- Unity: A General Platform for Intelligent Agents <LINK> #artificialintelligence #ml #simulations #intelligentagents <LINK>'],https://arxiv.org/abs/1809.02627,"Recent advances in artificial intelligence have been driven by the presence of increasingly realistic and complex simulated environments. However, many of the existing environments provide either unrealistic visuals, inaccurate physics, low task complexity, restricted agent perspective, or a limited capacity for interaction among artificial agents. Furthermore, many platforms lack the ability to flexibly configure the simulation, making the simulated environment a black-box from the perspective of the learning system. In this work, we propose a novel taxonomy of existing simulation platforms and discuss the highest level class of general platforms which enable the development of learning environments that are rich in visual, physical, task, and social complexity. We argue that modern game engines are uniquely suited to act as general platforms and as a case study examine the Unity engine and open source Unity ML-Agents Toolkit. We then survey the research enabled by Unity and the Unity ML-Agents Toolkit, discussing the kinds of research a flexible, interactive and easily configurable general platform can facilitate. ",Unity: A General Platform for Intelligent Agents
55,1039553679873826816,174734024,Danny Lange,['Pre-print of our new paper on ML-Agents: Unity: A General Platform for Intelligent Agents <LINK> <LINK>'],https://arxiv.org/abs/1809.02627,"Recent advances in artificial intelligence have been driven by the presence of increasingly realistic and complex simulated environments. However, many of the existing environments provide either unrealistic visuals, inaccurate physics, low task complexity, restricted agent perspective, or a limited capacity for interaction among artificial agents. Furthermore, many platforms lack the ability to flexibly configure the simulation, making the simulated environment a black-box from the perspective of the learning system. In this work, we propose a novel taxonomy of existing simulation platforms and discuss the highest level class of general platforms which enable the development of learning environments that are rich in visual, physical, task, and social complexity. We argue that modern game engines are uniquely suited to act as general platforms and as a case study examine the Unity engine and open source Unity ML-Agents Toolkit. We then survey the research enabled by Unity and the Unity ML-Agents Toolkit, discussing the kinds of research a flexible, interactive and easily configurable general platform can facilitate. ",Unity: A General Platform for Intelligent Agents
56,1039373490112028672,914498162462646273,Miller Group Caltech,"['New paper by @millergroupcaltech’s Matt Welborn (@mattgwelborn) in collab with Fred Manby (University of Bristol) on even-handed subsystem selection in projection-based embedding, just accepted for publication in JCP! Preprint in the arXiv at <LINK>.']",https://arxiv.org/abs/1809.03004,"Projection-based embedding offers a simple framework for embedding correlated wavefunction methods in density functional theory. Partitioning between the correlated wavefunction and density functional subsystems is performed in the space of localized molecular orbitals. However, during a large geometry change--such as a chemical reaction--the nature of these localized molecular orbitals, as well as their partitioning into the two subsystems, can change dramatically. This can lead to unphysical cusps and even discontinuities in the potential energy surface. In this work, we present an even-handed framework for localized orbital partitioning that ensures consistent subsystems across a set of molecular geometries. We illustrate this problem and the even-handed solution with a simple example of an SN2 reaction. Applications to a nitrogen umbrella flip in a cobalt-based CO2 reduction catalyst and to the binding of CO to Cu clusters are presented. In both cases, we find that even-handed partitioning enables chemically accurate embedding with modestly-sized embedded regions for systems in which previous partitioning strategies are problematic. ",Even-handed subsystem selection in projection-based embedding
57,1039127863960764416,19989030,Luis Lamb,"['New paper: ""Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate"" with @marceloprates_ and Pedro Avelar. Please see <LINK>']",https://arxiv.org/abs/1809.02208,"Recently there has been a growing concern about machine bias, where trained statistical models grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of AI tools have recently been suggested to be harmfully biased towards some minority, with reports of racist criminal behavior predictors, Iphone X failing to differentiate between two Asian people and Google photos' mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it to build sentences in constructions like ""He/She is an Engineer"" in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender-neutral pronouns in the translated output. We show that GT exhibits a strong tendency towards male defaults, in particular for fields linked to unbalanced gender distribution such as STEM jobs. We ran these statistics against BLS' data for the frequency of female participation in each job position, showing that GT fails to reproduce a real-world distribution of female workers. We provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, GT yields male defaults much more frequently than what would be expected from demographic data alone. We are hopeful that this work will ignite a debate about the need to augment current statistical translation tools with debiasing techniques which can already be found in the scientific literature. ","Assessing Gender Bias in Machine Translation -- A Case Study with Google
  Translate"
58,1038966514030927878,930272306022223873,The Sokolov Lab,"['Our new paper titled ""Multi-reference algebraic diagrammatic construction theory for excited states: General formulation and first-order implementation"" is available on ArXiv: <LINK>']",https://arxiv.org/abs/1809.02556,"We present a multi-reference generalization of the algebraic diagrammatic construction theory (ADC) [J. Schirmer, Phys. Rev. A 26, 2395 (1982)] for excited electronic states. The resulting multi-reference ADC approach (MR-ADC) can be efficiently and reliably applied to systems, which exhibit strong electron correlation in the ground or excited electronic states. In contrast to conventional multi-reference perturbation theories, MR-ADC describes electronic transitions involving all orbitals (core, active, and external) and enables efficient computation of spectroscopic properties, such as transition amplitudes and spectral densities. Our derivation of MR-ADC is based on the effective Liouvillean formalism of Mukherjee and Kutzelnigg [D. Mukherjee, W. Kutzelnigg, in Many-Body Methods in Quantum Chemistry (1989), pp. 257--274], which we generalize to multi-determinant reference states. We discuss a general formulation of MR-ADC, perform its perturbative analysis, and present an implementation of the first-order MR-ADC approximation, termed MR-ADC(1), as a first step in defining the MR-ADC hierarchy of methods. We show results of MR-ADC(1) for the excitation energies of the Be atom, an avoided crossing in LiF, doubly excited states in C2, and outline directions for our future developments. ","Multi-reference algebraic diagrammatic construction theory for excited
  states: General formulation and first-order implementation"
59,1038021719917768704,2766925212,Andrew Childs,"['New paper with Chakrabarti, @tongyang93, @xiaodiwu gives a quantum speedup for general convex optimization <LINK> (similar results obtained independently by van Apeldoorn, Gilyén, Gribling, de Wolf @CWInl <LINK>)']",http://arxiv.org/abs/1809.01731,"While recent work suggests that quantum computers can speed up the solution of semidefinite programs, little is known about the quantum complexity of more general convex optimization. We present a quantum algorithm that can optimize a convex function over an $n$-dimensional convex body using $\tilde{O}(n)$ queries to oracles that evaluate the objective function and determine membership in the convex body. This represents a quadratic improvement over the best-known classical algorithm. We also study limitations on the power of quantum computers for general convex optimization, showing that it requires $\tilde{\Omega}(\sqrt n)$ evaluation queries and $\Omega(\sqrt{n})$ membership queries. ",Quantum algorithms and lower bounds for convex optimization
60,1037928280609832965,3389619551,Bhaskar Krishnamachari,"['Aditya Asgaonkar  and I have just posted on arXiv a pre-print of a paper we have been working on, titled ""Token Curated Registries - A Game Theoretic Approach.""  TCRs are a new mechanism proposed by Mike Goldin to enable decentrali…<LINK> <LINK>']",https://arxiv.org/abs/1809.01756,"Token curated registries (TCRs) have been proposed recently as an approach to create and maintain high quality lists of resources or recommendations in a decentralized manner. Applications range from maintaining registries of web domains for advertising purposes (e.g., adChain) or restaurants, consumer products, etc. The registry is maintained through a combination of candidate applications requiring a token deposit, challenges based on token staking and token-weighted votes with a redistribution of tokens occurring as a consequence of the vote. We present a simplified mathematical model of a TCR and its challenge and voting process analyze it from a game-theoretic perspective. We derive some insights into conditions with respect to the quality of a candidate under which challenges occur, and under which the outcome is reject or accept. We also show that there are conditions under which the outcome may not be entirely predictable in the sense that everyone voting for accept and everyone voting for reject could both be Nash Equilibria outcomes. For such conditions, we also explore when a particular strategy profile may be payoff dominant. We identify ways in which our modeling can be extended and also some implications of our model with respect to the composition of TCRs. ",Token Curated Registries - A Game Theoretic Approach
61,1037716852988887040,403609486,Tom J Wilson,"[""Wondering about Gaia DR2 external catalogue matches? Think you might be getting slightly fewer matches than you'd like? Step this way! In our new paper (<LINK>) we show how shifts in positions caused by star blends (WISE; g+) affect Gaia DR2 cross-matches (rx) <LINK>"", 'Accounting for these perturbations leads to, in crowded fields, recovery of almost twice as many matches, shown as the non-Gaussian tails to the Gaia-WISE cross-match distributions (solid line vs dotted, typically assumed Gaussian). https://t.co/yRddDbXCQy', 'We provide some estimates for the level of flux contamination these blended sources contribute to Galactic plane WISE sources, as well as 1/10% relative flux contaminant probabilities. Sources are ~10% too bright in W1 as bright as W1~13, even at the Galactic anti-centre. https://t.co/wXYb6PeAOv', 'Finally, we provide a catalogue of Gaia DR2-WISE matches in the Galactic plane |b| &lt; 10 (https://t.co/rHbrWmGaIa), including match probability, WISE contamination probability &amp; flux contamination level, and astrometric &amp; photometric likelihood ratios (see https://t.co/vMSiJIqTOl)', 'To conclude: please take care when matching high angular resolution datasets to deep, lower angular resolution datasets, especially at longer wavelengths, as blended sources significantly perturb the recorded positions of astrometric objects.', ""@fringetracker Yes, I think you can, I think that's perfectly allowed in polite society :D"", ""@fringetracker Unfortunately Vizier require MNRAS to filter Volume and Page numbers before they will make it available, so that is the only option for now. How terrible is it? I didn't actually try downloading my own data table, I just verified the link existed..."", ""@fringetracker Yeah that's pretty terrible. I get something similar, so perhaps it's not setup right, I will check. Not a particularly useful repository if you're limited to dialup speeds :/""]",http://arxiv.org/abs/1809.00018,"Faint, hidden contaminants in the point-spread functions (PSFs) of stars cause shifts to their measured positions. Wilson & Naylor (2017) showed failing to account for these shifts can lead to a drastic decrease in the number of returned catalogue matches in crowded fields. Here we highlight the effect these perturbations have on cross-matching, for matches between Gaia DR2 and WISE stars in a crowded Galactic plane region. Applying the uncertainties as quoted to Gaussian-based astrometric uncertainty functions (AUFs) can lead, in dense Galactic fields, to only matching 55% of the counterparts. We describe the construction of empirical descriptions for AUFs, building on the cross-matching method of Wilson & Naylor (2018), utilising the magnitudes of both catalogues to discriminate between true and false counterparts. We apply the improved cross-matching method to the Galactic plane |b| < 10. We provide the most likely counterpart matches and their respective probabilities. We also analyse several cases to verify the robustness of the results, highlighting some important caveats and considerations. Finally, we discuss the effect PSF resolution has by comparing the intra-catalogue nearest neighbour separation distributions of a sample of likely contaminated WISE objects and their corresponding Spitzer counterpart. We show that some WISE contaminants are resolved in Spitzer, with smaller intra-catalogue separations. We have highlighted the effect contaminant stars have on WISE, but it is important for all photometric catalogues, playing an important role in the next generation of surveys, such as LSST. ","A Contaminant-Free Catalogue of Gaia DR2-WISE Galactic Plane Matches:
  Including the Effects of Crowding in the Cross-Matching of Photometric
  Catalogues"
62,1037594928212312064,331750508,matteo ronchetti,['My latest paper where I present IKA a new method for low rank kernel approximation is now on arXiv <LINK>'],https://arxiv.org/abs/1809.01353,"This paper describes a new method for low rank kernel approximation called IKA. The main advantage of IKA is that it produces a function $\psi(x)$ defined as a linear combination of arbitrarily chosen functions. In contrast the approximation produced by Nystr\""om method is a linear combination of kernel evaluations. The proposed method consistently outperformed Nystr\""om method in a comparison on the STL-10 dataset. Numerical results are reproducible using the source code available at this https URL ",IKA: Independent Kernel Approximator
63,1037335300270432257,2956121356,Russ Salakhutdinov,"['New #EMNLP2018 paper on Open Domain Question Answering Using Early Fusion of KB and Text: Graph convolution based neural network for extracting answers from question-specific subgraphs containing text and KB entities &amp; relations, with Sun, Dhingra et al. \n<LINK> <LINK>']",https://arxiv.org/abs/1809.00782,"Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting. Source code is available at this https URL . ","Open Domain Question Answering Using Early Fusion of Knowledge Bases and
  Text"
64,1037311772104044544,1011309405273513986,NarangLab,"['New paper on arXiv by Nick Rivera &amp; Jenny Coulter, @doecsgf fellows in the group on ab initio phonon-polariton descriptions #photon #Optics #materials \n<LINK>']",https://arxiv.org/abs/1809.00058,"The ability to use photonic quasiparticles to control electromagnetic energy far below the diffraction limit is a defining paradigm in nanophotonics. An important recent development in this field is the measurement and manipulation of extremely confined phonon-polariton modes in polar dielectrics such as silicon carbide and hexagonal boron nitride, which pave the way for nanophotonics and extreme light-matter interactions in the mid-IR to THz frequency range. To further advance this promising field, it is of great interest to predict the optical response of recently discovered and yet-to-be-synthesized polaritonic materials alike. Here we develop a unified framework based on quantum linear response theory to calculate the spatially non-local dielectric function of a polar lattice in arbitrary dimensions. In the case of a three-dimensional bulk material, the spatially local limit of our calculation reproduces standard results for the dielectric response of a polar lattice. Using this framework, we provide ab initio calculations of the dielectric permittivity of important bulk polar dielectrics such as silicon carbide and hexagonal boron nitride in good agreement with experiments. From the ab initio theory, we are able to develop a microscopic understanding of which phonon modes contribute to each component of the dielectric function, as well as predict features in the dielectric function that are a result of weak TO phonons. This formalism also identifies regime(s) where quantum nonlocal effects may correct the phonon polariton dispersion, extremely relevant in recent atomic-scale experiments which confine electromagnetic fields to the scale of 1~nm. Finally, our work points the way towards first principles descriptions of the effect of interface phonons, phonon strong coupling, and chiral phonons on the properties of phonon polaritons. ","Ab initio calculation of phonon polaritons in silicon carbide and boron
  nitride"
65,1037252348702347264,946726588200218624,Laurent Bétermin,"['New preprint titled ""Minimal Soft Lattice Theta Functions"" is now available on @arxiv. In this paper, the optimality properties of a new d-dimensional lattice theta function for mass interaction (i.e. condensed matter interaction) are investigated.   <LINK>']",https://arxiv.org/abs/1809.00473,"We study the minimality properties of a new type of ""soft"" theta functions. For a lattice $L\subset \mathbb{R}^d$, a $L$-periodic distribution of mass $\mu_L$ and an other mass $\nu_z$ centred at $z\in \mathbb{R}^d$, we define, for all scaling parameter $\alpha>0$, the translated lattice theta function $\theta_{\mu_L+\nu_z}(\alpha)$ as the Gaussian interaction energy between $\nu_z$ and $\mu_L$. We show that any strict local or global minimality result that is true in the point case $\mu=\nu=\delta_0$ also holds for $L\mapsto \theta_{\mu_L+\nu_0}(\alpha)$ and $z\mapsto \theta_{\mu_L+\nu_z}(\alpha)$ when the measures are radially symmetric with respect to the points of $L\cup \{z\}$ and sufficiently rescaled around them (i.e. at a low scale). The minimality at all scales is also proved when the radially symmetric measures are generated by a completely monotone kernel. The method is based on a generalized Jacobi transformation formula, some standard integral representations for lattice energies and an approximation argument. Furthermore, for the honeycomb lattice $\mathsf{H}$, the center of any primitive honeycomb is shown to minimize $z\mapsto \theta_{\mu_{\mathsf{H}}+\nu_z}(\alpha)$ and many applications are stated for other particular physically relevant lattices including the triangular, square, cubic, orthorhombic, body-centred-cubic and face-centred-cubic lattices. ",Minimal Soft Lattice Theta Functions
66,1037228252232855552,2850683124,Paul Michel,"['Our new @emnlp2018  paper, ""MTNT: A Testbed for Machine Translation of Noisy Text"" is now on arxiv. New dataset for evaluating MT systems on non-standard input. Check it out! Paper: <LINK> Data: <LINK> Code: <LINK> <LINK>', 'Thanks to fantastic advisor and co-author @gneubig !']",https://arxiv.org/abs/1809.00388,"Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT. The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/. ",MTNT: A Testbed for Machine Translation of Noisy Text
67,1037210294672666624,10666172,Sabine Hossenfelder,"['Strong lensing with superfluid dark matter, new paper with my student Tobias Mistele <LINK>']",https://arxiv.org/abs/1809.00840,"In superfluid dark matter the exchange of phonons can create an additional force that has an effect similar to Modified Newtonian Dynamics (MOND). To test whether this hypothesis is compatible with observation, we study a set of strong gravitational lenses from the SLACS survey and check whether the measurements can be explained by a superfluid in the central region of galaxies. Concretely, we try to simultaneously fit each lens's Einstein radius and velocity dispersion with a spherically symmetric density profile of a fluid that has both a normal and a superfluid component. We demonstrate that we can successfully fit all galaxies except one, and that the fits have reasonable stellar mass-to-light-ratios. We conclude that strong gravitational lensing does not pose a challenge for the idea that superfluid dark matter mimics modified gravity. ",Strong lensing with superfluid dark matter
68,1047458239259062272,271601706,Virginia Dignum,"['Intelligent systems are among us: we need #AI to be *socially* intelligent,, not just intelligent. \nNew paper on the formalisation of Interactions as Social Practices  by Frank Dignum: <LINK>']",https://arxiv.org/abs/1809.08751,"Multi-agent models are a suitable starting point to model complex social interactions. However, as the complexity of the systems increase, we argue that novel modeling approaches are needed that can deal with inter-dependencies at different levels of society, where many heterogeneous parties (software agents, robots, humans) are interacting and reacting to each other. In this paper, we present a formalization of a social framework for agents based in the concept of Social Practices as high level specifications of normal (expected) behavior in a given social context. We argue that social practices facilitate the practical reasoning of agents in standard social interactions. ",Interactions as Social Practices: towards a formalization
69,1044489488028889088,869154646694264832,Hugh Salimbeni,"['Pleased to share our new paper on orthogonal decoupled GP inference, with Ching-An Cheng, Byron Boots and @mpd, appearing at NIPS 2018. Paper: <LINK> code: <LINK> Five tweet summary:👇', ""1/5 Gaussian process variational posteriors are a great idea (see @alexggmatthews's thesis (https://t.co/SR1MAEj2Sf) for details), but typically incur cubic scaling in the covariance parameterization, limiting how many ‘inducing points’ can be used. This is a shame."", '2/5 We can squeeze more flexibility into the mean, as shown in https://t.co/1vg3MOCyfE. This is the ‘decoupled’ approach, as it uses different parameterizations for the mean and covariance. The ‘coupled’ approach is the usual one where the mean/covariance share the same basis', '3/5 This seems like a good idea, but it unfortunately has two downsides: 1) it’s non-convex (unlike the coupled case) and 2) it doesn’t admit a natural gradient update (which has proved very useful in the coupled case, see e.g. https://t.co/gzE3fGudTz)', '4/5 We propose a new basis. It has all the advantages of the original decoupled basis, but with two additional properties: convexity, and a natural gradient update rule.', '5/5 The basis has an orthogonal projection for the additional component in the mean, which decouples the natural gradient update. Implementing a natural gradient step is just as easy as the coupled case. See code https://t.co/QaQCv79oHS']",https://arxiv.org/abs/1809.08820,"Gaussian processes (GPs) provide a powerful non-parametric framework for reasoning over functions. Despite appealing theory, its superlinear computational and memory complexities have presented a long-standing challenge. State-of-the-art sparse variational inference methods trade modeling accuracy against complexity. However, the complexities of these methods still scale superlinearly in the number of basis functions, implying that that sparse GP methods are able to learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed that removes the unnecessary coupling between the complexities of modeling the mean and the covariance functions of a GP. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments. ",Orthogonally Decoupled Variational Gaussian Processes
70,1043552710086340609,10834752,Arvind Narayanan,"['New draft paper: Formal Barriers to Longest-Chain Proof-of-Stake Protocols\n<LINK>\nMain result: in a game-theoretic model where all players are strategic, any PoS protocol allows one of two types of beneficial deviation\nTalk: <LINK>\nKey intuition: <LINK>', '@farkf Our view is that requiring external randomness merely shifts the consensus problem elsewhere, because you need a way to ensure all users measure the same randomness. In our textbook we explain why physical randomness sources are not useful for this (p 253) https://t.co/2Yv5VdHWAn', '@farkf Another idea is to use a randomness beacon derived from a different (PoW) blockchain. This avoids the measurement problem if each block in the PoS chain is linked to a block in the PoW chain (similar to merge mining). But this is unappealing for many reasons. CC @socrates1024']",https://arxiv.org/abs/1809.06528,"The security of most existing cryptocurrencies is based on a concept called Proof-of-Work, in which users must solve a computationally hard cryptopuzzle to authorize transactions (`one unit of computation, one vote'). This leads to enormous expenditure on hardware and electricity in order to collect the rewards associated with transaction authorization. Proof-of-Stake is an alternative concept that instead selects users to authorize transactions proportional to their wealth (`one coin, one vote'). Some aspects of the two paradigms are the same. For instance, obtaining voting power in Proof-of-Stake has a monetary cost just as in Proof-of-Work: a coin cannot be freely duplicated any more easily than a unit of computation. However some aspects are fundamentally different. In particular, exactly because Proof-of-Stake is wasteless, there is no inherent resource cost to deviating (commonly referred to as the `Nothing-at-Stake' problem). In contrast to prior work, we focus on incentive-driven deviations (any participant will deviate if doing so yields higher revenue) instead of adversarial corruption (an adversary may take over a significant fraction of the network, but the remaining players follow the protocol). The main results of this paper are several formal barriers to designing incentive-compatible proof-of-stake cryptocurrencies (that don't apply to proof-of-work). ",Formal Barriers to Longest-Chain Proof-of-Stake Protocols
71,1043499734709809152,48277460,Dr Peter Overbruy,['My new paper :-) Speedy way to find networks but working on expanding to more problems in the next paper. <LINK>'],https://arxiv.org/abs/1809.06293,"Methods that generate networks sharing a given degree distribution and global clustering can induce changes in structural properties other than that controlled for. Diversity in structural properties, in turn, can affect the outcomes of dynamical processes operating on those networks. Since exhaustive sampling is not possible, we propose a novel evolutionary framework for mapping this structural diversity. The three main features of this framework are: (a) subgraph-based encoding of networks, (b) exact mutations based on solving systems of Diophantine equations, and (c) heuristic diversity-driven mechanism to drive resolution changes in the MapElite algorithm. We show that our framework can elicit networks with diversity in their higher-order structure and that this diversity affects the behaviour of the complex contagion model. Through a comparison with state of the art clustered network generation methods, we demonstrate that our approach can uncover a comparably diverse range of networks without needing computationally unfeasible mixing times. Further, we suggest that the subgraph-based encoding provides greater confidence in the diversity of higher-order network structure for low numbers of samples and is the basis for explaining our results with complex contagion model. We believe that this framework could be applied to other complex landscapes that cannot be practically mapped via exhaustive sampling. ","Mapping structural diversity in networks sharing a given degree
  distribution and global clustering: Adaptive resolution grid search evolution
  with Diophantine equation-based mutations"
72,1043112386931113985,409006348,Shrimai,"[""New #emnlp2018 paper on A Datast for Document Grounded Conversations with Kangyan Zhou - a Master's student I worked with last year from @LTIatCMU and Alan W Black. Paper: <LINK>\nThe dataset is available here: <LINK>""]",https://arxiv.org/abs/1809.07358,"This paper introduces a document grounded dataset for text conversations. We define ""Document Grounded Conversations"" as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses. ",A Dataset for Document Grounded Conversations
73,1042215786968887296,1008831733406490624,Emily Martin,"['Excited to announce my last thesis paper was accepted for publication!! <LINK> We present parallaxes for 22 late-T and Y dwarfs using @NASAspitzer data. There are updated parallaxes for 18 targets and new parallaxes for 4 targets! Also 1 new Y dwarf!', 'and 6 new T dwarfs, all with Keck/NIRSPEC spectra! We found that the Y dwarf class spans a larger range in absolute magnitudes than expected-- implying a large range in effective temperatures. Bottom line: give us all the #JWST time please, because these dwarfs are COLD! 😁']",https://arxiv.org/abs/1809.06479,"Y dwarfs provide a unique opportunity to study free-floating objects with masses $<$30 M$_{Jup}$ and atmospheric temperatures approaching those of known Jupiter-like exoplanets. Obtaining distances to these objects is an essential step towards characterizing their absolute physical properties. Using Spitzer/IRAC [4.5] images taken over baselines of $\sim$2-7 years, we measure astrometric distances for 22 late-T and early Y dwarfs, including updated parallaxes for 18 objects and new parallax measurements for 4 objects. These parallaxes will make it possible to explore the physical parameter space occupied by the coldest brown dwarfs. We also present the discovery of 6 new late-T dwarfs, updated spectra of two T dwarfs, and the reclassification of a new Y dwarf, WISE J033605.04$-$014351.0, based on Keck/NIRSPEC $J$-band spectroscopy. Assuming that effective temperatures are inversely proportional to absolute magnitude, we examine trends in the evolution of the spectral energy distributions of brown dwarfs with decreasing effective temperature. Surprisingly, the Y dwarf class encompasses a large range in absolute magnitude in the near- to mid-infrared photometric bandpasses, demonstrating a larger range of effective temperatures than previously assumed. This sample will be ideal for obtaining mid-infrared spectra with the James Webb Space Telescope because their known distances will make it easier to measure absolute physical properties. ",Y dwarf Trigonometric Parallaxes from the Spitzer Space Telescope
74,1040574292449210368,806058672619212800,Guillaume Lample,"[""XNLI: Evaluating Cross-lingual Sentence Representations - @alex_conneau's #emnlp2018 new paper. Extends the NLI dataset to 15 languages, including low-resource ones such as Swahili and Urdu. <LINK> <LINK>"", 'If you had to classify Urdu sentences given English labeled data, what would you do between 1) translate the labeled data and train a Urdu classifier 2) translate Urdu sentences and feed them to the English classifier 3) use cross-lingual representations and a single classifier?', 'For now, 2) works best, as cross-lingual representations are not perfect yet, and XNLI is a great way to evaluate this :)']",https://arxiv.org/abs/1809.05053,"State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines. ",XNLI: Evaluating Cross-lingual Sentence Representations
75,1040283816491925505,554869994,Yonatan Belinkov,"['Always heard about the split between Classical and Modern Standard Arabic? Interested in periodization of historical languages? Check out our new paper on the history of Arabic: <LINK>\nJoint work with Alex Magidow, @_albarron_ , Avi Shmidman, and @maximromanov', 'We preprocess the amazing OpenITI corpus (https://t.co/y3gzrKYfxw), identify text reuse, and perform both automatic and expert-based periodization. We find evidence for familiar periods of change (e.g. the split between Classical and MSA), as well as new ones. https://t.co/cxO6Rv6qTy', '@marcos_zampieri @_albarron_ @maximromanov It remains to be seen...']",https://arxiv.org/abs/1809.03891,"Arabic is a widely-spoken language with a long and rich history, but existing corpora and language technology focus mostly on modern Arabic and its varieties. Therefore, studying the history of the language has so far been mostly limited to manual analyses on a small scale. In this work, we present a large-scale historical corpus of the written Arabic language, spanning 1400 years. We describe our efforts to clean and process this corpus using Arabic NLP tools, including the identification of reused text. We study the history of the Arabic language using a novel automatic periodization algorithm, as well as other techniques. Our findings confirm the established division of written Arabic into Modern Standard and Classical Arabic, and confirm other established periodizations, while suggesting that written Arabic may be divisible into still further periods of development. ","Studying the History of the Arabic Language: Language Technology and a
  Large-Scale Historical Corpus"
76,1039435719209611264,1446792746,Andrew Davison,"[""We will present new Dyson Robotics Lab work this week at ECCV. We show that we can learn a non-linear least squares optimiser for dense reconstruction problems which doesn't need hand-designed priors. @ronnieclark__ \n<LINK>\nPaper link: <LINK>""]",https://arxiv.org/abs/1809.02966,"Sum-of-squares objective functions are very popular in computer vision algorithms. However, these objective functions are not always easy to optimize. The underlying assumptions made by solvers are often not satisfied and many problems are inherently ill-posed. In this paper, we propose LS-Net, a neural nonlinear least squares optimization algorithm which learns to effectively optimize these cost functions even in the presence of adversities. Unlike traditional approaches, the proposed solver requires no hand-crafted regularizers or priors as these are implicitly learned from the data. We apply our method to the problem of motion stereo ie. jointly estimating the motion and scene geometry from pairs of images of a monocular sequence. We show that our learned optimizer is able to efficiently and effectively solve this challenging optimization problem. ",LS-Net: Learning to Solve Nonlinear Least Squares for Monocular Stereo
77,1038090755691175937,12131042,Jeremy Blackburn,"['WaPo article about our new paper (<LINK>) is now in print!\n\nAnyone know where you can buy newspapers in 2018?! I want to pick up a few copies :D <LINK>', '@benjamindhorne gotta find copies first... :(']",https://arxiv.org/abs/1809.01644,"A new wave of growing antisemitism, driven by fringe Web communities, is an increasingly worrying presence in the socio-political realm. The ubiquitous and global nature of the Web has provided tools used by these groups to spread their ideology to the rest of the Internet. Although the study of antisemitism and hate is not new, the scale and rate of change of online data has impacted the efficacy of traditional approaches to measure and understand these troubling trends. In this paper, we present a large-scale, quantitative study of online antisemitism. We collect hundreds of million posts and images from alt-right Web communities like 4chan's Politically Incorrect board (/pol/) and Gab. Using scientifically grounded methods, we quantify the escalation and spread of antisemitic memes and rhetoric across the Web. We find the frequency of antisemitic content greatly increases (in some cases more than doubling) after major political events such as the 2016 US Presidential Election and the ""Unite the Right"" rally in Charlottesville. We extract semantic embeddings from our corpus of posts and demonstrate how automated techniques can discover and categorize the use of antisemitic terminology. We additionally examine the prevalence and spread of the antisemitic ""Happy Merchant"" meme, and in particular how these fringe communities influence its propagation to more mainstream communities like Twitter and Reddit. Taken together, our results provide a data-driven, quantitative framework for understanding online antisemitism. Our methods serve as a framework to augment current qualitative efforts by anti-hate groups, providing new insights into the growth and spread of hate online. ",A Quantitative Approach to Understanding Online Antisemitism
78,1044083728882888704,1033351625702658049,Ken N. Okada,"['We numerically study noncentrosymmetric Kondo lattice models subject to Rashba and Dresselhaus spin-orbit couplings. We uncover versatile multiple-Q orderings under zero magnetic field, which originate in the instabilities of the spin-split Fermi surface.\n<LINK>']",http://arxiv.org/abs/1809.07582,"We study magnetic textures realized in noncentrosymmetric Kondo lattice models, in which localized magnetic moments weakly interact with itinerant electrons subject to Rashba and Dresselhaus spin-orbit couplings. By virtue of state-of-the-art numerical simulations as well as variational calculations, we uncover versatile multiple-$Q$ orderings under zero magnetic field, which are found to originate in the instabilities of the Fermi surface whose spin degeneracy is lifted by the spin-orbit couplings. In the case with equally-strong Rashba and Dresselhaus spin-orbit couplings, which is known to realize a persistent spin helix in semiconductor quantum wells, we discover a sextuple-$Q$ magnetic ordering with a checkerboard-like spatial pattern of the spin scalar chirality. In the presence of either Rashba or Dresselhaus spin-orbit coupling, we find out another multiple-$Q$ ordering, which is distinct from Skyrmion crystals discussed under the same symmetry. Our results indicate that the cooperation of the spin-charge and spin-orbit couplings brings about richer magnetic textures than those studied within effective spin models. The situations would be experimentally realized, e.g., in noncentrosymmetric heavy-fermion compounds and heterostructures of spin-orbit coupled metals and magnetic insulators. ",Multiple-$Q$ magnetic orders in Rashba-Dresselhaus metals
79,1042763663000649728,1021671924752048129,Anna McLeod,"[""Paper day! Check out Peter Zeidler's analysis of MUSE data on Westerlund 2: among other things, we find a bimodal radial velocity trend of the massive O- and B-type stars\n<LINK>""]",https://arxiv.org/abs/1809.06866,"Westerlund 2 (Wd2) is the central ionizing star cluster of the \ion{H}{2} region RCW~49 and the second most massive young star cluster (${\rm M} = (3.6 \pm 0.3)\times 10^4\,{\rm M}_\odot$) in the Milky Way. Its young age ($\sim2\,$Myr) and close proximity to the Sun ($\sim 4\,$kpc) makes it a perfect target to study stars emerging from their parental gas cloud, the large number of OB-stars and their feedback onto the gas, and the gas dynamics. We combine high-resolution multi-band photometry obtained in the optical and near-infrared with the \textit{Hubble} Space Telescope (HST), and VLT/MUSE integral field spectroscopy to study the gas, the stars, and their interactions, simultaneously. In this paper we focus on a small, $64\times64\,{\rm arcsec}^2$ region North of the main cluster center, which we call the Northern Bubble (NB), a circular cavity carved into the gas of the cluster region. Using MUSE data, we determined the spectral types of 17 stars in the NB from G9III to O7.5. With the estimation of these spectral types we add 2 O and 5 B-type stars to the previously published census of 37 OB-stars in Wd2. To measure radial velocities we extracted 72 stellar spectra throughout Wd2, including the 17 of the NB, and show that the cluster member stars follow a bimodal velocity distribution centered around $(8.10 \pm 1.53)\,{\rm km}\,{\rm s}^{-1}$ and $(25.41 \pm 1.57)\,{\rm km}\,{\rm s}^{-1}$ with a dispersion of $(4.52 \pm 1.78)\,{\rm km}\,{\rm s}^{-1}$ and $(3.46 \pm 1.29)\,{\rm km}\,{\rm s}^{-1}$, respectively. These are in agreement with CO($J=1$-2) studies of RCW~49 leaving cloud-cloud collision as a viable option for the formation scenario of Wd2. The bimodal distribution is also detected in the Gaia DR2 proper motions. ","The young massive star cluster Westerlund 2 observed with MUSE. I. First
  results on the cluster internal motion from stellar radial velocities"
80,1042760748542435328,2867153067,Luca Carenzo,['What happens between healthcare providers and patients in a(simulated) mass casualty event? Have a  look at our latest preprint <LINK>  with @ISI_Fondazione we used #proximity sensors to study interaction #networks &amp; #patient flow @ciro #SocioPatterns @CRIMEDIM <LINK>'],https://arxiv.org/abs/1809.06887,"Over the past several decades, naturally occurring and man-made mass casualty incidents (MCI) have increased in frequency and number, worldwide. To test the impact of such event on medical resources, simulations can provide a safe, controlled setting while replicating the chaotic environment typical of an actual disaster. A standardised method to collect and analyse data from mass casualty exercises is needed, in order to assess preparedness and performance of the healthcare staff involved. We report on the use of wearable proximity sensors to measure proximity events during a MCI simulation. We investigated the interactions between medical staff and patients, to evaluate the time dedicated by the medical staff with respect to the severity of the injury of the victims depending on the roles. We estimated the presence of the patients in the different spaces of the field hospital, in order to study the patients' flow. Data were obtained and collected through the deployment of wearable proximity sensors during a mass casualty incident functional exercise. The scenario included two areas: the accident site and the Advanced Medical Post (AMP), and the exercise lasted 3 hours. A total of 238 participants simulating medical staff and victims were involved. Each participant wore a proximity sensor and 30 fixed devices were placed in the field hospital. The contact networks show a heterogeneous distribution of the cumulative time spent in proximity by participants. We obtained contact matrices based on cumulative time spent in proximity between victims and the rescuers. Our results showed that the time spent in proximity by the healthcare teams with the victims is related to the severity of the patient's injury. The analysis of patients' flow showed that the presence of patients in the rooms of the hospital is consistent with triage code and diagnosis, and no obvious bottlenecks were found. ","Wearable proximity sensors for monitoring a mass casualty incident
  exercise: a feasibility study"
81,1042687761638191104,1032247786941698048,denis_gordeev_,['We have published a pre-print of our article <LINK> where we study unsupervised cross-lingual dictionary induction (MUSE <LINK>) for matching product classifications.'],https://arxiv.org/abs/1809.07234,Unsupervised cross-lingual embeddings mapping has provided a unique tool for completely unsupervised translation even for languages with different scripts. In this work we use this method for the task of unsupervised cross-lingual matching of product classifications. Our work also investigates limitations of unsupervised vector alignment and we also suggest two other techniques for aligning product classifications based on their descriptions: using hierarchical information and translations. ,Unsupervised cross-lingual matching of product classifications
82,1042631673144324096,526115229,Kevin Heng,"['Fisher &amp; Heng (2018, MNRAS): a comprehensive study led by @cfisher94, where we interpreted all 38 existing WFC3 transmission spectra and revisited the normalization degeneracy. It is Chloe Fisher’s first lead-author paper and second after a year of Ph.D. <LINK>']",http://arxiv.org/abs/1809.06894,"A comprehensive analysis of 38 previously published Wide Field Camera 3 (WFC3) transmission spectra is performed using a hierarchy of nested-sampling retrievals: with versus without clouds, grey versus non-grey clouds, isothermal versus non-isothermal transit chords and with water, hydrogen cyanide and/or ammonia. We revisit the ""normalisation degeneracy"": the relative abundances of molecules are degenerate at the order-of-magnitude level with the absolute normalisation of the transmission spectrum. Using a suite of mock retrievals, we demonstrate that the normalisation degeneracy may be partially broken using WFC3 data alone, even in the absence of optical/visible data and without appealing to the presence of patchy clouds, although lower limits to the mixing ratios may be prior-dominated depending on the measurement uncertainties. With James Webb Space Telescope-like spectral resolutions, the normalisation degeneracy may be completely broken from infrared spectra alone. We find no trend in the retrieved water abundances across nearly two orders of magnitude in exoplanet mass and a factor of 5 in retrieved temperature (about 500 to 2500 K). We further show that there is a general lack of strong Bayesian evidence to support interpretations of non-grey over grey clouds (only for WASP-69b and WASP-76b) and non-isothermal over isothermal atmospheres (no objects). 35 out of 38 WFC3 transmission spectra are well-fitted by an isothermal transit chord with grey clouds and water only, while 8 are adequately explained by flat lines. Generally, the cloud composition is unconstrained. ","Retrieval analysis of 38 WFC3 transmission spectra and resolution of the
  normalisation degeneracy"
83,1042370875792740352,340197502,David G. Cerdeño,"['The neutrino floor can be raised by new physics in the neutrino sector. In this paper <LINK> with @CelineBoehm1 @PedroANMachado @andres0l1 and Elliott Reid we find a large increase at low masses, relevant for future dark matter detectors @IPPP_Durham #neutrino <LINK>']",https://arxiv.org/abs/1809.06385,"In this paper, we compute the contribution to the coherent elastic neutrino-nucleus scattering cross section from new physics models in the neutrino sector. We use this information to calculate the maximum value of the so-called neutrino floor for direct dark matter detection experiments, which determines when these detectors are sensitive to the neutrino background. After including all relevant experimental constraints in different simplified neutrino models, we have found that the neutrino floor can increase by various orders of magnitude in the region of dark matter masses below 10 GeV in the case of scalar mediators, however, this spectacular enhancement is subject to the re-examination of supernovae bounds. The increase is approximately a factor of two for vector mediators. In the light of these results, future claims by direct detection experiments exploring the low-mass window must be carefully examined if a signal is found well above the expected Standard Model neutrino floor. ",How high is the neutrino floor?
84,1041970226827587584,211996267,Symeon Papadopoulos,"['We propose a new problem formulation and large dataset for reverse video search, catered for news videos, FIVR: Fine-grained Incident Video Retrieval, <LINK> @InVID_EU']",https://arxiv.org/abs/1809.04094,"This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. FIVR offers a single framework that contains several retrieval tasks as special cases. To address the benchmarking needs of all such tasks, we construct and present a large-scale annotated video dataset, which we call FIVR-200K, and it comprises 225,960 videos. To create the dataset, we devise a process for the collection of YouTube videos based on major news events from recent years crawled from Wikipedia and deploy a retrieval pipeline for the automatic selection of query videos based on their estimated suitability as benchmarks. We also devise a protocol for the annotation of the dataset with respect to the four types of video associations defined by FIVR. Finally, we report the results of an experimental study on the dataset comparing five state-of-the-art methods developed based on a variety of visual descriptors, highlighting the challenges of the current problem. ",FIVR: Fine-grained Incident Video Retrieval
85,1040575636211986435,334470578,Chris J. Maddison,"['Excited to share this paper. We generalize the momentum method for optimization, and in the process find that you can greatly extend the class of convex functions on which exponential convergence is possible using just gradients: Hamiltonian descent (<LINK>). <LINK>', 'It was magical when we discovered this months ago, and then lots of hard work to get it done. With Daniel Paulin, @yeewhye, @bodonoghue85, @ArnaudDoucet1.', '@gmravi2003 Yes, exponential convergence beyond strongly convex. There is no magic, these methods violate the assumptions of the lower bounds, despite using only gradient information. The ""trick"" is to use the gradients of two functions instead of one.', '@mena_gonzalo @yeewhye @bodonoghue85 @ArnaudDoucet1 Thanks, Gonzalo!']",https://arxiv.org/abs/1809.05042,"We propose a family of optimization methods that achieve linear convergence using first-order gradient information and constant step sizes on a class of convex functions much larger than the smooth and strongly convex ones. This larger class includes functions whose second derivatives may be singular or unbounded at their minima. Our methods are discretizations of conformal Hamiltonian dynamics, which generalize the classical momentum method to model the motion of a particle with non-standard kinetic energy exposed to a dissipative force and the gradient field of the function of interest. They are first-order in the sense that they require only gradient computation. Yet, crucially the kinetic gradient map can be designed to incorporate information about the convex conjugate in a fashion that allows for linear convergence on convex functions that may be non-smooth or non-strongly convex. We study in detail one implicit and two explicit methods. For one explicit method, we provide conditions under which it converges to stationary points of non-convex functions. For all, we provide conditions on the convex function and kinetic energy pair that guarantee linear convergence, and show that these conditions can be satisfied by functions with power growth. In sum, these methods expand the class of convex functions on which linear convergence is possible with first-order computation. ",Hamiltonian Descent Methods
86,1040566123081678848,870695740359573506,Jan Gieseler,['check out our latest paper on @arxiv\n<LINK>\nWe propose a new architecture for quantum networks based on magnonic excitations in nanomagnets and superconducting loops. Looking forward to seeing this realized in an experiment!!\n#quantum #QuantumComputing #nanotech'],https://arxiv.org/abs/1809.04901,"We show theoretically that a network of superconducting loops and magnetic particles can be used to implement magnonic crystals with tunable magnonic band structures. In our approach, the loops mediate interactions between the particles and allow magnetic excitations to tunnel over long distances. As a result, different arrangements of loops and particles allow one to engineer the band structure for the magnonic excitations. Furthermore, we show how magnons in such crystals can serve as a quantum bus for long-distance magnetic coupling of spin qubits. The qubits are coupled to the magnets in the network by their local magnetic-dipole interaction and provide an integrated way to measure the state of the magnonic quantum network. ",Hybrid Architecture for Engineering Magnonic Quantum Networks
87,1040477100581564418,430865632,Patrick Stöcker,"['New paper: ""Freeze-in production of decaying dark matter in five steps\n"" <LINK> .\nWe study the production and evolution of a light dark ""Higgs"" with a feeble coupling to the SM Higgs <LINK>']",https://arxiv.org/abs/1809.04849,"We study the cosmological evolution and phenomenological properties of scalar bosons in the keV to MeV range that have a tiny mixing with the Standard Model Higgs boson. The mixing determines both the abundance of light scalars produced via the freeze-in mechanism and their lifetime. Intriguingly, the parameters required for such scalars to account for all of the dark matter in the present Universe generically predict lifetimes comparable to the sensitivity of present and future indirect detection experiments. In order to accurately determine the relic abundance of light scalars, we calculate freeze-in yields including effects from finite temperatures and quantum statistics and develop a new approach for solving the Boltzmann equation for number-changing processes in the dark sector. We find that light scalars can potentially explain the anomalous x-ray emission at 3.5 keV, while evading constraints from structure formation and predicting potentially observable self-interaction cross sections. ",Freeze-in production of decaying dark matter in five steps
88,1040133126712832000,66175375,Jason Wang,['Our Gemini @PlanetImager orbits of the HR 8799 planets is out: <LINK>. We combined orbit fits with N-body simulations to find stable orbit solutions. Inside: 4-planet resonance lock is not necessary for stability; planet-disk interactions; and dynamical masses! <LINK>'],https://arxiv.org/abs/1809.04107,"The HR 8799 system uniquely harbors four young super-Jupiters whose orbits can provide insights into the system's dynamical history and constrain the masses of the planets themselves. Using the Gemini Planet Imager (GPI), we obtained down to one milliarcsecond precision on the astrometry of these planets. We assessed four-planet orbit models with different levels of constraints and found that assuming the planets are near 1:2:4:8 period commensurabilities, or are coplanar, does not worsen the fit. We added the prior that the planets must have been stable for the age of the system (40 Myr) by running orbit configurations from our posteriors through $N$-body simulations and varying the masses of the planets. We found that only assuming the planets are both coplanar and near 1:2:4:8 period commensurabilities produces dynamically stable orbits in large quantities. Our posterior of stable coplanar orbits tightly constrains the planets' orbits, and we discuss implications for the outermost planet b shaping the debris disk. A four-planet resonance lock is not necessary for stability up to now. However, planet pairs d and e, and c and d, are each likely locked in two-body resonances for stability if their component masses are above $6~M_{\rm{Jup}}$ and $7~M_{\rm{Jup}}$, respectively. Combining the dynamical and luminosity constraints on the masses using hot-start evolutionary models and a system age of $42 \pm 5$~Myr, we found the mass of planet b to be $5.8 \pm 0.5~M_{\rm{Jup}}$, and the masses of planets c, d, and e to be $7.2_{-0.7}^{+0.6}~M_{\rm{Jup}}$ each. ",Dynamical Constraints on the HR 8799 Planets with GPI
89,1039828253383512064,889777924991307778,Sebastian Meyer,"['""Forecasting Based on Surveillance Data"" (<LINK>) with @HeldLeonhard. We review statistical methods to quantify the accuracy of epidemic forecasts. A case study compares forecasts of influenza in Switzerland. Vignettes and #rstats code at <LINK>. <LINK>']",https://arxiv.org/abs/1809.03735,"Forecasting the future course of epidemics has always been one of the main goals of epidemic modelling. This chapter reviews statistical methods to quantify the accuracy of epidemic forecasts. We distinguish point and probabilistic forecasts and describe different methods to evaluate and compare the predictive performance across models. Two case studies demonstrate how to apply the different techniques to uni- and multivariate forecasts. We focus on forecasting count time series from routine public health surveillance: weekly counts of influenza-like illness in Switzerland, and age-stratified counts of norovirus gastroenteritis in Berlin, Germany. Data and code for all analyses are available in a supplementary R package. ",Forecasting Based on Surveillance Data
90,1039751045591826432,982515235285274624,R. Mor,"['Besançon Galaxy Model is ready for multi-parameter inference: BGM FASt (<LINK>). Here it is a Corner plot resultig to study the IMF, the SFH and the density at the Solar Neighbourhood with Tycho-2 data. We are working really hard to apply our method to #GaiaDR2!! <LINK>']",https://arxiv.org/abs/1809.03511,"We develop a new theoretical framework to generate Besan\c{c}on Galaxy Model fast approximate simulations (BGM FASt) to address fundamental questions of the Galactic structure and evolution performing multi-parameter inference. As a first application of our strategy we simultaneously infer the IMF, the star formation history and the stellar mass density in the Solar Neighbourhood. The BGM FASt strategy is based on a reweighing scheme, that uses a specific pre-sampled simulation, and on the assumption that the distribution function of the generated stars in the Galaxy can be described by an analytical expression. To validate BGM FASt we execute a set of tests. Finally, we use BGM FASt with an approximate Bayesian computation algorithm to obtain the posterior PDF of the inferred parameters, by comparing synthetic versus Tycho-2 colour-magnitude diagrams. Results: The validation shows a very good agreement between BGM FASt and the standard BGM, with BGM FASt being $\approx 10^4$ times faster. By analysing Tycho-2 data we obtain a thin disc star formation history decreasing in time and a present rate of $1.2 \pm 0.2 M_\odot/yr$. The resulting total stellar mass density in the Solar Neighbourhood is $0.051_{-0.005}^{+0.002} M_\odot/pc^3$ and the local dark matter density is $0.012 \pm 0.001 M_\odot/pc^3$. For the composite IMF we obtain a slope of $\alpha_2={2.1}_{-0.3}^{+0.1}$ in the mass range between $0.5 M_\odot$ and $1.53M_\odot$. The results of the slope at the high mass range are trustable up to $4M_\odot$ and highly depend on the choice of the extinction map (obtaining $\alpha_3={2.9}_{-0.2}^{+0.2}$ and $\alpha_3={3.7}_{-0.2}^{+0.2}$ respectively, for two different extinction maps). Systematic uncertainties are not included. Conclusions: The good performance of BGM FASt demonstrates that it is a very valuable tool to perform multi-parameter inference using Gaia data releases. ","BGM FASt: Besan\c{c}on Galaxy Model for Big Data. Simultaneous inference
  of the IMF, SFH and density in the Solar Neighbourhood"
91,1037251115832815616,4242812189,Adam Noel,"[""A common design choice for a diffusive mol comm system is for receiver to be passive or reactive. A big advantage for passive is that it's much faster to accurately simulate. We propose an algorithm that can speed up absorption by orders of magnitude <LINK>""]",https://arxiv.org/abs/1809.00808,"A novel a priori Monte Carlo (APMC) algorithm is proposed to accurately simulate the molecules absorbed at spherical receiver(s) with low computational complexity in diffusion-based molecular communication (MC) systems. It is demonstrated that the APMC algorithm achieves high simulation efficiency since by using this algorithm, the fraction of molecules absorbed for a relatively large time step length precisely matches the analytical result. Therefore, the APMC algorithm overcomes the shortcoming of the existing refined Monte Carlo (RMC) algorithm which enables accurate simulation for a relatively small time step length only. Moreover, for the RMC algorithm, an expression is proposed to quickly predict the simulation accuracy as a function of the time step length and system parameters, which facilitates the choice of simulation time step for a given system. Furthermore, a rejection threshold is proposed for both the RMC and APMC algorithms to significantly save computational complexity while causing an extremely small loss in accuracy. ","A Novel A Priori Simulation Algorithm for Absorbing Receivers in
  Diffusion-Based Molecular Communication Systems"
92,1044513143244955648,73090248,Prof. Danushka Bollegala,['If you train language models and worry about compounding errors consider neighbourhood sampling. <LINK> We propose a method that use nearest neighbours computed using word embeddings to make predictions early in the training when the LM is not very accurate.'],https://arxiv.org/abs/1809.05916,"The task of multi-step ahead prediction in language models is challenging considering the discrepancy between training and testing. At test time, a language model is required to make predictions given past predictions as input, instead of the past targets that are provided during training. This difference, known as exposure bias, can lead to the compounding of errors along a generated sequence at test time. In order to improve generalization in neural language models and address compounding errors, we propose a curriculum learning based method that gradually changes an initially deterministic teacher policy to a gradually more stochastic policy, which we refer to as \textit{Nearest-Neighbor Replacement Sampling}. A chosen input at a given timestep is replaced with a sampled nearest neighbor of the past target with a truncated probability proportional to the cosine similarity between the original word and its top $k$ most similar words. This allows the teacher to explore alternatives when the teacher provides a sub-optimal policy or when the initial policy is difficult for the learner to model. The proposed strategy is straightforward, online and requires little additional memory requirements. We report our main findings on two language modelling benchmarks and find that the proposed approach performs particularly well when used in conjunction with scheduled sampling, that too attempts to mitigate compounding errors in language models. ",Curriculum-Based Neighborhood Sampling For Sequence Prediction
