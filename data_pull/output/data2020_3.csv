,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1249779748961767425,2785337469,Sebastian Ruder,"[""I'm excited to announce XTREME, a new benchmark that covers 9 tasks and 40 typologically diverse languages.\n\nPaper: <LINK>\nBlog post: <LINK>\nCode: <LINK> <LINK>"", 'XTREME evaluates models on their capability to do zero-shot cross-lingual transfer when fine-tuned on English. In the paper, we conduct experiments with several state-of-the-art models that have been pre-trained on large multilingual corpora. https://t.co/9X59Ht8v7m', 'Overall, we find that there is a large gap to English and human performance and a lot of potential for improvement, particularly on syntactic tasks. See below for instance for the performance of XLM-R across tasks and languages. https://t.co/b8jUjW4ufF', 'This work would have not been possible without the contributions of many amazing people including @JunjieHu12 @orf_bnw @gneubig Melvin, Aditya @dhgarrette @JonClarkSeattle and others.', ""@gena_d Argh. The last letter of the website got lost during copy-pasting. 😅 Here's the correct link:\nhttps://t.co/S8RdKmhCsE"", '@boknilev Yes. For all but TyDiQA-GoldP, human performance is based on English. That comes with the obvious caveat that performance will differ across languages. IMO for datasets that are derived using translation (XNLI, PAWS-X, XQuAD), human perf should be very similar across languages.']",https://arxiv.org/abs/2003.11080,"Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks. ","XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating
  Cross-lingual Generalization"
1,1248493279521341440,161398503,Hao Tang,"['My new paper ""TensorFlow Solver for Quantum PageRank in Large-Scale Networks"" is available on arXiv:  <LINK>  It shows a very efficient way to solve Kronecker product using Runge-Kutta numerical method and GPU parallel computing.']",https://arxiv.org/abs/2003.04930,"Google PageRank is a prevalent and useful algorithm for ranking the significance of nodes or websites in a network, and a recent quantum counterpart for PageRank algorithm has been raised to suggest a higher accuracy of ranking comparing to Google PageRank. The quantum PageRank algorithm is essentially based on quantum stochastic walks and can be expressed using Lindblad master equation, which, however, needs to solve the Kronecker products of an O(N^4) dimension and requires severely large memory and time when the number of nodes N in a network increases above 150. Here, we present an efficient solver for quantum PageRank by using the Runge-Kutta method to reduce the matrix dimension to O(N^2) and employing TensorFlow to conduct GPU parallel computing. We demonstrate its performance in solving quantum PageRank for the USA major airline network with up to 922 nodes. Compared with the previous quantum PageRank solver, our solver dramatically reduces the required memory and time to only 1% and 0.2%, respectively, making it practical to work in a normal computer with a memory of 4-8 GB in no more than 100 seconds. This efficient solver for large-scale quantum PageRank and quantum stochastic walks would greatly facilitate studies of quantum information in real-life applications. ",TensorFlow Solver for Quantum PageRank in Large-Scale Networks
2,1248135650848550913,479249561,Makoto Morishita,['Our new paper is now available on arXiv.\nIt is time-consuming to manually recover an ICT system from unexpected errors.\nWe find the seq2seq model trained with error logs and recovery commands pairs could help to recover the server automatically.\n<LINK>'],https://arxiv.org/abs/2003.10784,"With the increase in scale and complexity of ICT systems, their operation increasingly requires automatic recovery from failures. Although it has become possible to automatically detect anomalies and analyze root causes of failures with current methods, making decisions on what commands should be executed to recover from failures still depends on manual operation, which is quite time-consuming. Toward automatic recovery, we propose a method of estimating recovery commands by using Seq2Seq, a neural network model. This model learns complex relationships between logs obtained from equipment and recovery commands that operators executed in the past. When a new failure occurs, our method estimates plausible commands that recover from the failure on the basis of collected logs. We conducted experiments using a synthetic dataset and realistic OpenStack dataset, demonstrating that our method can estimate recovery commands with high accuracy. ","Recovery command generation towards automatic recovery in ICT systems by
  Seq2Seq learning"
3,1247595780573200389,384900803,Shantanu Basu,"['Thrilled abt our new paper on star formation. The solutions with a counter-rotating disk, tiny disk, or nonexistent disk (direct collapse) are unique outcomes that are realized in collapse from magnetically-dominated clouds. <LINK> @westernuPhysAst @KyushuUniv_EN']",https://arxiv.org/abs/2003.03078,"The accretion phase of star formation is investigated in magnetically-dominated clouds that have an initial subcritical mass-to-flux ratio. We employ nonideal magnetohydrodynamic simulations that include ambipolar diffusion and ohmic dissipation. During the early prestellar phase the mass-to-flux ratio rises toward the critical value for collapse, and during this time the angular momentum of the cloud core is reduced significantly by magnetic braking. Once a protostar is formed in the core, the accretion phase is characterized by the presence of a small amount of angular momentum but a large amount of magnetic flux in the near-protostellar environment. The low angular momentum leads to a very small (or even nonexistent) disk and weak outflow, while the large magnetic flux can lead to an interchange instability that rapidly removes flux from the central region. The effective magnetic braking in the early collapse phase can even lead to a counter-rotating disk and outflow, in which the rotation direction of the disk and outflow is opposite to that of the infalling envelope. The solutions with a counter-rotating disk, tiny disk, or nonexistent disk (direct collapse) are unique outcomes that are realized in collapse from magnetically-dominated clouds with an initial subcritical mass-to-flux ratio. ","Different Modes of Star Formation II: Gas Accretion Phase of Initially
  Subcritical Star-Forming Clouds"
4,1247329534887821312,19355829,Jonathan McDowell,"['Final version of my Starlink paper now on Arxiv, includes cites to the new results that Darksat worked. <LINK> This is (almost) the same as the official version from ApJ Letters.']",https://arxiv.org/abs/2003.07446,"I discuss the current low Earth orbit artificial satellite population and show that the proposed `megaconstellation' of circa 12,000 Starlink internet satellites would dominate the lower part of Earth orbit, below 600 km, with a latitude-dependent areal number density of between 0.005 and 0.01 objects per square degree at airmass < 2. Such large, low altitude satellites appear visually bright to ground observers, and the initial Starlinks are naked eye objects. I model the expected number of illuminated satellites as a function of latitude, time of year, and time of night and summarize the range of possible consequences for ground-based astronomy. In winter at lower latitudes typical of major observatories, the satellites will not be illuminated for six hours in the middle of the night. However, at low elevations near twilight at intermediate latitudes (45-55 deg, e.g. much of Europe) hundreds of satellites may be visible at once to naked-eye observers at dark sites. ","The Low Earth Orbit Satellite Population and Impacts of the SpaceX
  Starlink Constellation"
5,1247301913336520704,1066522206614814720,"Arinc Ozturk, MD",['Our new paper is available online! Congratulations Jimmy for leading this paper and congratulations CURT team for your great efforts in this work. <LINK> \n@anthonysamir'],https://arxiv.org/abs/2003.09070,"Modern deep learning algorithms geared towards clinical adaption rely on a significant amount of high fidelity labeled data. Low-resource settings pose challenges like acquiring high fidelity data and becomes the bottleneck for developing artificial intelligence applications. Ultrasound images, stored in Digital Imaging and Communication in Medicine (DICOM) format, have additional metadata data corresponding to ultrasound image parameters and medical exams. In this work, we leverage DICOM metadata from ultrasound images to help learn representations of the ultrasound image. We demonstrate that the proposed method outperforms the non-metadata based approaches across different downstream tasks. ","Weakly Supervised Context Encoder using DICOM metadata in Ultrasound
  Imaging"
6,1247214844551421960,1245412923268198400,Kai-Hung Chang,['Our GNN research for Structural Engineering is now available!\n\nLearning to simulate and design for structural engineering\nPaper: <LINK>\n\n@chinyich and I believe structural engineering will be a new domain where GNN can shine! <LINK>'],https://arxiv.org/abs/2003.09103,"The structural design process for buildings is time-consuming and laborious. To automate this process, structural engineers combine optimization methods with simulation tools to find an optimal design with minimal building mass subject to building regulations. However, structural engineers in practice often avoid optimization and compromise on a suboptimal design for the majority of buildings, due to the large size of the design space, the iterative nature of the optimization methods, and the slow simulation tools. In this work, we formulate the building structures as graphs and create an end-to-end pipeline that can learn to propose the optimal cross-sections of columns and beams by training together with a pre-trained differentiable structural simulator. The performance of the proposed structural designs is comparable to the ones optimized by genetic algorithm (GA), with all the constraints satisfied. The optimal structural design with the reduced the building mass can not only lower the material cost, but also decrease the carbon footprint. ",Learning to simulate and design for structural engineering
7,1245671152124600320,302624082,Grushin,"['Amorphous solids are not as random! In our new work we studied, with Q. Marsal and @danielvarjas how to describe topological phases in amorphous lattices! Check this 🧵 and the paper on the arxiv: <LINK> <LINK>', 'So, solids are made by atoms. A gazillion of them. Many scientists usually like them better when they are nice an ordered in a periodic structure we call a crystal. They are nice clean and symmetric so we can apply nice mathematical theorems to study them.', 'Now, most of things around you are not nice in this sense. Most of the materials you see are not in their crystalline form, but rather in some amorphous or disordered version. So, are these outside nice theorems and mathematics?', 'The GOD of disorder was Phil Anderson, @NobelPrize who passed away last Sunday at 96 (still going to the office a few weeks ago!). From him we learned, among many things, that disorder can be studied, and can have beautiful mathematical structures.', 'In this work we follow this spirit, but now the disorder is in the lattice that forms the solid. The atoms are arranged aperiodially, but not random!', 'Amorphous solids actually have some order. The atoms in them see an environment that is actually quite similar to their crystal counterparts. It is only at long length scales (the whole  material) where we can see that something is different compared to a crystal!', 'So, what we did in this paper was to use this ‘short range order’ to find phases with beautiful mathematical structure, topological phase, that are typically associated to crystals, this time in amorphous solids!', 'We are not the first to study amorphous Topological phases but we found a way to use this hidden order to predict when they appear. We think it can eventually open a way to classify them, something that is quite challenging compared to the beautiful symmetric crystals.', 'I have discussed topological phases in the past. They could be useful for a bunch of technological progress, mostly because of their robustness, even to a completely disordered lattice like in our case. Basically: Nice new material property + topology = robust technology (maybe)', 'So, now is time to push this to its full potential! Amorphous materials are everywhere, so let’s make topology be everywhere too! 💪🏼🥳']",https://arxiv.org/abs/2003.13701,"Amorphous solids remain outside of the classification and systematic discovery of new topological materials, partially due to the lack of realistic models that are analytically tractable. Here we introduce the topological Weaire-Thorpe class of models, which are defined on amorphous lattices with fixed coordination number, a realistic feature of covalently bonded amorphous solids. Their short-range properties allow us to analytically predict spectral gaps. Their symmetry under permutation of orbitals allows us to analytically compute topological phase diagrams, which determine quantized observables like circular dichroism, by introducing symmetry indicators for the first time in amorphous systems. These models and our procedures to define invariants are generalizable to higher coordination number and dimensions, opening a route towards a complete classification of amorphous topological states in real space using quasilocal properties. ",Topological Weaire-Thorpe models of amorphous matter
8,1245385591824347137,171674815,Mark Marley,"[""Nice new paper by Phillips et al. presenting a new cloudless grid of substellar models. <LINK> \nIf you'd like to compare with our model photometry and evolution you can get them here\n<LINK>\nand spectra here\n<LINK>"", 'Differences: Phillips et al. employ the very latest alkali line profiles by Nikole Allard. Our online models used the previous iteration of her profiles.', 'We both use updated equations of state of the evolution. I need to dig deeper to see exactly the differences. Comparing our evolution Luminosity(time) will elucidate how much uncertainty remains.', 'Our disequilibrium models are essentially done but not yet online.', 'We both use rainout, not pure equilibrium, chemistry but with slightly different approaches. Will need to compare carefully to see how much that matters.', 'Having two independent groups make models is extremely valuable and I think we can learn a lot about where the field is by comparing/contrasting.']",https://arxiv.org/abs/2003.13717,"We present a new set of solar metallicity atmosphere and evolutionary models for very cool brown dwarfs and self-luminous giant exoplanets, which we term ATMO 2020. Atmosphere models are generated with our state-of-the-art 1D radiative-convective equilibrium code ATMO, and are used as surface boundary conditions to calculate the interior structure and evolution of $0.001-0.075\,\mathrm{M_{\odot}}$ objects. Our models include several key improvements to the input physics used in previous models available in the literature. Most notably, the use of a new H-He equation of state including ab initio quantum molecular dynamics calculations has raised the mass by $\sim1-2\%$ at the stellar-substellar boundary and has altered the cooling tracks around the hydrogen and deuterium burning minimum masses. A second key improvement concerns updated molecular opacities in our atmosphere model ATMO, which now contains significantly more line transitions required to accurately capture the opacity in these hot atmospheres. This leads to warmer atmospheric temperature structures, further changing the cooling curves and predicted emission spectra of substellar objects. We present significant improvement for the treatment of the collisionally broadened potassium resonance doublet, and highlight the importance of these lines in shaping the red-optical and near-infrared spectrum of brown dwarfs. We generate three different grids of model simulations, one using equilibrium chemistry and two using non-equilibrium chemistry due to vertical mixing, all three computed self-consistently with the pressure-temperature structure of the atmosphere. We show the impact of vertical mixing on emission spectra and in colour-magnitude diagrams, highlighting how the $3.5-5.5\,\mathrm{\mu m}$ flux window can be used to calibrate vertical mixing in cool T-Y spectral type objects. ","A new set of atmosphere and evolution models for cool T-Y brown dwarfs
  and giant exoplanets"
9,1245379641025736706,880052980938133505,Graham D Bruce,"['Need to know the polarisation of multiple overlapped laser beams simultaneously? Do it with #speckle! @StLeonards_PGs-funded PhD student @MorganFacchin has a new preprint (his first first-author paper!) on @arxiv today, explaining how. <LINK> #NotAnAprilFool <LINK>']",https://arxiv.org/abs/2003.14408,"Laser speckle is generated by the multiple interference of light through a disordered medium. Here we study the premise that the speckle pattern retains information about the polarisation state of the incident field. We analytically verify that a linear relation exists between the Stokes vector of the light and the resulting speckle pattern. As a result, the polarisation state of a beam can be measured from the speckle pattern using a transmission matrix approach. We perform a quantitative analysis of the accuracy of the transmission matrix method to measure randomly time-varying polarisation states. In experiment, we find that the Stokes parameters of light from a diode laser can be retrieved with an uncertainty of 0.05 using speckle images of 150$\times$150 pixels and 17 training states. We show both analytically and in experiment that this approach may be extended to the case of more than one laser field, demonstrating the measurement of the Stokes parameters of two laser beams simultaneously from a single speckle pattern and achieving the same uncertainty of 0.05. ","Speckle-based determination of the polarisation state of single and
  multiple laser beams"
10,1245351140386123782,988446705417904133,Mark Phillips,"[""I'm very excited that my first paper is out today - a new set of models for cool brown dwarfs and giant exoplanets - <LINK> The grid includes solar metallicity, chemical equilibrium and disequilibrium atmosphere models and substellar evolutionary tracks (1/4)"", 'The grid of models is publically available at https://t.co/YjoSsWPrRn, which includes #JWST photometric and coronagraphic filters for upcoming proposals! (2/4)', 'These new models include several key improvements to the input physics including a new H-He EOS, updated molecular opacities, new K resonance line profiles and self-consistent disequilibrium chemistry (3/4)', ""I also realise the unfortunate timing of releasing this on April fools day! It's all real science I promise! (4/4) https://t.co/vaeAPd4KHl""]",https://arxiv.org/abs/2003.13717,"We present a new set of solar metallicity atmosphere and evolutionary models for very cool brown dwarfs and self-luminous giant exoplanets, which we term ATMO 2020. Atmosphere models are generated with our state-of-the-art 1D radiative-convective equilibrium code ATMO, and are used as surface boundary conditions to calculate the interior structure and evolution of $0.001-0.075\,\mathrm{M_{\odot}}$ objects. Our models include several key improvements to the input physics used in previous models available in the literature. Most notably, the use of a new H-He equation of state including ab initio quantum molecular dynamics calculations has raised the mass by $\sim1-2\%$ at the stellar-substellar boundary and has altered the cooling tracks around the hydrogen and deuterium burning minimum masses. A second key improvement concerns updated molecular opacities in our atmosphere model ATMO, which now contains significantly more line transitions required to accurately capture the opacity in these hot atmospheres. This leads to warmer atmospheric temperature structures, further changing the cooling curves and predicted emission spectra of substellar objects. We present significant improvement for the treatment of the collisionally broadened potassium resonance doublet, and highlight the importance of these lines in shaping the red-optical and near-infrared spectrum of brown dwarfs. We generate three different grids of model simulations, one using equilibrium chemistry and two using non-equilibrium chemistry due to vertical mixing, all three computed self-consistently with the pressure-temperature structure of the atmosphere. We show the impact of vertical mixing on emission spectra and in colour-magnitude diagrams, highlighting how the $3.5-5.5\,\mathrm{\mu m}$ flux window can be used to calibrate vertical mixing in cool T-Y spectral type objects. ","A new set of atmosphere and evolution models for cool T-Y brown dwarfs
  and giant exoplanets"
11,1245324956281581568,225073245,Nikita,"['Doing few-shot learning across domains? \nGet some inspiration from our new work ""Selecting Relevant Features from a Universal Representation for Few-shot Learning"". A simple idea with SoTA results.\n\nPaper: <LINK>\n\nThe code is now online: <LINK>']",http://arxiv.org/abs/2003.09338,"Popular approaches for few-shot classification consist of first learning a generic data representation based on a large annotated dataset, before adapting the representation to new classes given only a few labeled samples. In this work, we propose a new strategy based on feature selection, which is both simpler and more effective than previous feature adaptation approaches. First, we obtain a multi-domain representation by training a set of semantically different feature extractors. Then, given a few-shot learning task, we use our multi-domain feature bank to automatically select the most relevant representations. We show that a simple non-parametric classifier built on top of such features produces high accuracy and generalizes to domains never seen during training, which leads to state-of-the-art results on MetaDataset and improved accuracy on mini-ImageNet. ","Selecting Relevant Features from a Multi-domain Representation for
  Few-shot Classification"
12,1245248781383274498,120285811,Rob Crain,"['Predictions from the EAGLE and IllustrisTNG simulations suggest that stacked @eROSITA_SRG observations of ~L*  galaxies will reveal the structure of the CGM: <LINK>, new paper with @nonstopbenopp et al.', 'For the avoidance of doubt, this is *not* an April Fools joke...']",https://arxiv.org/abs/2003.13889,"We simulate stacked observations of nearby hot X-ray coronae associated with galaxies in the EAGLE and Illustris-TNG hydrodynamic simulations. A forward modeling pipeline is developed to predict 4-year eROSITA observations and stacked image analysis, including the effects of instrumental and astrophysical backgrounds. We propose an experiment to stack z~0.01 galaxies separated by specific star-formation rate (sSFR) to examine how the hot (T>=10^6 K) circumgalactic medium (CGM) differs for high- and low-sSFR galaxies. The simulations indicate that the hot CGM of low-mass (M_*~10^{10.5} Msol), high-sSFR (defined as the top one-third ranked by sSFR) central galaxies will be detectable to a galactocentric radius r~30-50 kpc. Both simulations predict lower luminosities at fixed stellar mass for the low-sSFR galaxies (the lower third of sSFR) with Illustris-TNG predicting 3x brighter coronae around high-sSFR galaxies than EAGLE. Both simulations predict detectable emission out to r~150-200 kpc for stacks centered on high-mass (M_*~10^{11.0} Msol) galaxies, with EAGLE predicting brighter X-ray halos. The extended soft X-ray luminosity correlates strongly and positively with the mass of circumgalactic gas within the virial radius (f_{CGM}). Prior analyses of both simulations have established that f_{CGM} is reduced by expulsive feedback driven mainly by black hole growth, which quenches galaxy growth by inhibiting replenishment of the ISM. Both simulations predict that eROSITA stacks should not only conclusively detect and resolve the hot CGM around L^* galaxies for the first time, but provide a powerful probe of how the baryon cycle operates, for which there remains an absence of consensus between state-of-the-art simulations. ","EAGLE and Illustris-TNG predictions for resolved eROSITA X-ray
  observations of the circumgalactic medium around normal galaxies"
13,1245178787526041603,37213193,🇺🇦 David Lazer,"['New paper alert!\n\nSurvey Data and Human Computation for Improved Flu Tracking\n\nw/ Stefan Wojcik, Avleen Bijral, Richard Johnston, @BDataScientist , Ryan Kennedy, @kinggary @alexvespi \n\n<LINK>']",https://arxiv.org/abs/2003.13822,"While digital trace data from sources like search engines hold enormous potential for tracking and understanding human behavior, these streams of data lack information about the actual experiences of those individuals generating the data. Moreover, most current methods ignore or under-utilize human processing capabilities that allow humans to solve problems not yet solvable by computers (human computation). We demonstrate how behavioral research, linking digital and real-world behavior, along with human computation, can be utilized to improve the performance of studies using digital data streams. This study looks at the use of search data to track prevalence of Influenza-Like Illness (ILI). We build a behavioral model of flu search based on survey data linked to users online browsing data. We then utilize human computation for classifying search strings. Leveraging these resources, we construct a tracking model of ILI prevalence that outperforms strong historical benchmarks using only a limited stream of search data and lends itself to tracking ILI in smaller geographic units. While this paper only addresses searches related to ILI, the method we describe has potential for tracking a broad set of phenomena in near real-time. ",Survey Data and Human Computation for Improved Flu Tracking
14,1245156690485485568,58080978,Washington Ramos,"['Check out our new #CVPR2020 paper on fast-forwarding instructional videos! This joint work with @michelms, @edsonroteia, @leandrosmarc, and @ericksonrn_ is now available at:\n\n📝<LINK>\n🌎<LINK>\n▶ <LINK>\n\n#verlab #dccufmg <LINK>']",https://arxiv.org/abs/2003.14229,"The rapid increase in the amount of published visual data and the limited time of users bring the demand for processing untrimmed videos to produce shorter versions that convey the same information. Despite the remarkable progress that has been made by summarization methods, most of them can only select a few frames or skims, which creates visual gaps and breaks the video context. In this paper, we present a novel methodology based on a reinforcement learning formulation to accelerate instructional videos. Our approach can adaptively select frames that are not relevant to convey the information without creating gaps in the final video. Our agent is textually and visually oriented to select which frames to remove to shrink the input video. Additionally, we propose a novel network, called Visually-guided Document Attention Network (VDAN), able to generate a highly discriminative embedding space to represent both textual and visual data. Our experiments show that our method achieves the best performance in terms of F1 Score and coverage at the video segment level. ","Straight to the Point: Fast-forwarding Videos via Reinforcement Learning
  Using Textual Data"
15,1245152811672715265,556151596,Lawrence M. Krauss,"['Happy about this new research paper by the first large experimental collaboration I have been a member of, proposed to search for particle dark matter and an exotic process called double beta decay.  It is rather technical, but here is the link.  <LINK>', '@IVerboten meh', '@nadine_feiler This expt is designed to detect very rare events depositing small amounts of energy in an underground detector. Two exotic sources:  particle dark matter that collides with particles in the detector, or very rare radioactive decays inside the detector.  Both involve new physics.', '@FarrimondRobert yes.. but actually these types of things have already been used as deep underground observatories, looking for neutrinos and dark matter, for decades..  This device is bigger, and more sensitive...and has multiple uses.', ""@IVerboten meh, I have looked at it enough to feel it isn't interesting."", '@FarrimondRobert Gran Sasso, in Italy is a tunnel lab built for expts like this.']",https://arxiv.org/abs/2003.13407,"The DARWIN observatory is a proposed next-generation experiment to search for particle dark matter and for the neutrinoless double beta decay of $^{136}$Xe. Out of its 50$\,$t total natural xenon inventory, 40$\,$t will be the active target of a time projection chamber which thus contains about 3.6 t of $^{136}$Xe. Here, we show that its projected half-life sensitivity is $2.4\times10^{27}\,$yr, using a fiducial volume of 5t of natural xenon and 10$\,$yr of operation with a background rate of less than 0.2$~$events/(t$\cdot$yr) in the energy region of interest. This sensitivity is based on a detailed Monte Carlo simulation study of the background and event topologies in the large, homogeneous target. DARWIN will be comparable in its science reach to dedicated double beta decay experiments using xenon enriched in $^{136}$Xe. ","Sensitivity of the DARWIN observatory to the neutrinoless double beta
  decay of $^{136}$Xe"
16,1244979364153917442,4211253189,Rohit Bhattacharya,['Happy to share @raziehnabi and my paper on semiparametric inference in hidden variable causal graphical models! This is a rough time for all. Writing this paper was one way for us to stop obsessing over every new reported case of #Covid_19. Stay safe all!\n\n<LINK>'],https://arxiv.org/abs/2003.12659,"Identification theory for causal effects in causal models associated with hidden variable directed acyclic graphs (DAGs) is well studied. However, the corresponding algorithms are underused due to the complexity of estimating the identifying functionals they output. In this work, we bridge the gap between identification and estimation of population-level causal effects involving a single treatment and a single outcome. We derive influence function based estimators that exhibit double robustness for the identified effects in a large class of hidden variable DAGs where the treatment satisfies a simple graphical criterion; this class includes models yielding the adjustment and front-door functionals as special cases. We also provide necessary and sufficient conditions under which the statistical model of a hidden variable DAG is nonparametrically saturated and implies no equality constraints on the observed data distribution. Further, we derive an important class of hidden variable DAGs that imply observed data distributions observationally equivalent (up to equality constraints) to fully observed DAGs. In these classes of DAGs, we derive estimators that achieve the semiparametric efficiency bounds for the target of interest where the treatment satisfies our graphical criterion. Finally, we provide a sound and complete identification algorithm that directly yields a weight based estimation strategy for any identifiable effect in hidden variable causal models. ","Semiparametric Inference For Causal Effects In Graphical Models With
  Hidden Variables"
17,1244969215813201921,382269545,Jason Lyall,"[""In non-pandemic news, we have a new working paper about how to do causal inference when you have spatio-temporal data (which is nearly everyone doing microlevel studies). Application is to US airstrikes &amp; effects on insurgent violence in Iraq. It's here: <LINK> <LINK>"", '@fhollenbach Thanks! It’s the first in a series, so would love feedback', '@notanastronomer Ha! I’d be happy too, but I’m not sure this one’s a political science paper!', ""@notanastronomer It's a really cool method with lots and lots of applications. If you've got feedback, let us know. We're planning a series of papers on the topic.""]",https://arxiv.org/abs/2003.13555,"Many causal processes have spatial and temporal dimensions. Yet the classic causal inference framework is not directly applicable when the treatment and outcome variables are generated by spatio-temporal processes with an infinite number of possible event locations. We extend the potential outcomes framework to these settings by formulating the treatment point process as a stochastic intervention. Our causal estimands include the expected number of outcome events in a specified area under a particular stochastic treatment assignment strategy. We develop methodology that allows for arbitrary patterns of spatial spillover and temporal carryover effects. Using martingale theory, we show that the proposed estimator is consistent and asymptotically normal as the number of time periods increases, even when the propensity score is estimated. We propose a sensitivity analysis for the possible existence of unmeasured confounders, and extend it to the H\'ajek estimator. Simulation studies are conducted to examine the estimators' finite sample performance. Finally, we use the proposed methods to estimate the effects of American airstrikes on insurgent violence in Iraq from February 2007 to July 2008. We find that increasing the average number of daily airstrikes for up to one month results in more insurgent attacks across Iraq and within Baghdad. We also find evidence that airstrikes can displace attacks from Baghdad to new locations up to 400 kilometers away. ","Causal Inference with Spatio-temporal Data: Estimating the Effects of
  Airstrikes on Insurgent Violence in Iraq"
18,1244894052635590662,718384250270126081,Sven Krippendorf,"['Wondered how to learn about symmetries in your data with neural networks? Check out our new paper: <LINK>  1/2 <LINK>', 'Our way is to look at the embedding layer and analyse the orbits related by the symmetry. Re-identifying symmetries for Calabi-Yau classification, standard situations  with SO(n) and SU(2), and rotations in images. 2/2 https://t.co/hKkHJqwlvv']",https://arxiv.org/abs/2003.13679,"Identifying symmetries in data sets is generally difficult, but knowledge about them is crucial for efficient data handling. Here we present a method how neural networks can be used to identify symmetries. We make extensive use of the structure in the embedding layer of the neural network which allows us to identify whether a symmetry is present and to identify orbits of the symmetry in the input. To determine which continuous or discrete symmetry group is present we analyse the invariant orbits in the input. We present examples based on rotation groups $SO(n)$ and the unitary group $SU(2).$ Further we find that this method is useful for the classification of complete intersection Calabi-Yau manifolds where it is crucial to identify discrete symmetries on the input space. For this example we present a novel data representation in terms of graphs. ",Detecting Symmetries with Neural Networks
19,1244833975920513024,119013247,Aditya Vijaykumar,"['New (my first) preprint paper from the Astrophysical Relativity group at @ictstifr out on arXiv! It talks about constraining the variation of the gravitational constant using gravitational-wave observations. Find it at <LINK>, and read on for a summary 😃 [1/n] <LINK>', ""We all are taught that the Gravitational Constant G is a 'universal constant' and it's value doesn't change with time. In fact, if Einstein's General Relativity is the true theory of gravity, G should indeed be constant. [2/n] https://t.co/jV8ijRENNu"", ""Nonetheless, there are alternative theories of gravity such as the Brans-Dicke theory which aren't yet ruled out by experiments/observations. Such theories generically predict a time-varying G. [3/n] https://t.co/D0QWwPAKvh"", ""Okay, let's switch gears for a moment. Stars sustain themselves by burning mostly hydrogen and helium. When this nuclear fuel is exhausted, stars collapse onto themselves, forming denser stars like white dwarfs and neutron stars. [4/n]"", 'Subrahmanyan Chandrasekhar in the 1930s showed that if white dwarfs are to be stable and not collapse into BHs, the mass of white dwarfs cannot exceed 1.44 times the mass of our sun. This value is called the Chandrasekhar limit. [5/n] https://t.co/BZiiRDfkaX', 'Neutron stars have a similar maximum mass limit called the TOV limit. Contingent on certain assumptions, we can safely say that the mass of a neutron star cannot be more than 4 times the mass of the sun. [6/n]', 'From other fundamental considerations, we can also say that the neutron star mass has to be greater than 0.09 solar masses. That is, any neutron star mass M satisfies :-\n0.09 Msun &lt; M &lt; 4 Msun. [7/n]', 'Of course, one consideration is that all these bounds are derived assuming that the real value of G is the same for all times as it is today (ie G=Go). So what happens to the mass limits when we change the value of G? \n\nWe find that these limits scale as (G/Go)^{-3/2}. [8/n]', 'Two neutron stars orbiting each other emit energy via gravitational waves, causing their orbit to shrink! These GWs can be experimentally observed - in fact, we have two such observations - GW170817 and GW190425! This is what they sound like :) [9/n] https://t.co/uew1QxQalZ', 'These observations allow us to measure masses of the neutron stars that emitted gravitational waves. Again, estimated masses depend on the value of G. If M_obs is the estimated mass of the neutron star, we find that value of G at the time of merger (Gs) should satisfy [10/n] :- https://t.co/C5AuVPfMZi', 'So once we have M_obs, we can place bounds on how much G has varied from the time the neutron stars merged (hundreds of millions of years ago!) to the current day. Using GW170817, we find the following [11/n] :- https://t.co/4u25K0VumP', 'This plot pictorially describes our method. With future gravitational-wave observations, we can probe deeper into space and place tighter bounds on the value of Gs! https://t.co/OpwVVkKKNP', ""Thanks for reading so far! Here's a cute gif of two neutron stars playing merry-go-round 😃[12/12] :- https://t.co/CdBQ8D2Dm1"", 'Oh, and thanks @_sanskritea for help with the tweets!', ""Among lessons this one taught me - attend group meetings, deliver group seminars and don't doze off in either 😅"", '@aayushmaan_jain Thanks bruh!\n\nShall do a thread sometime but the summary is - one has real signal  from GW observations, and also has theoretical predictions of what the signal should look like as a function of M_obs. So we basically ""fit""  this function to the data and get an estimate of mass.', 'Erratum to this tweet - WDs need not always collapse to BHs, in fact they more likely collapse to neutron stars. The statement should be read as ""... if white dwarfs are to be stable and not collapse under their own gravity ...""\n\nThanks to Ajith for pointing the error!', '@threadreaderapp please unroll', '@ananya_tweeted @ictstifr Thanks Ananya for reading through!', '@gawwrgi @ictstifr Thanks Gargi! Means a lot :)']",https://arxiv.org/abs/2003.12832,"We propose a method to constrain the variation of the gravitational constant $G$ with cosmic time using gravitational-wave (GW) observations of merging binary neutron stars. The method essentially relies on the fact that the maximum and minimum allowed masses of neutron stars at a particular cosmic epoch has a simple dependence on the value of $G$ at that epoch. GWs carry an imprint of the value of $G$ at the time of the merger. Thus, if the value of $G$ at merger is significantly different from its current value, the masses of the neutron stars inferred from the GW observations will be inconsistent with the theoretically allowed range. This enables us to place bounds on the variation of $G$ between the merger epoch and the present epoch. Using the observation of the binary neutron star system GW170817, we constrain the fractional difference in $G$ between the merger and the current epoch to be in the range $-1 \lesssim \Delta G/G \lesssim 8$. Assuming a monotonic variation in $G$, this corresponds to a bound on the average rate of change of $-7 \times 10^{-9}~\mathrm{yr}^{-1} \le \dot{G}/G \le 5 \times 10^{-8}~\mathrm{yr}^{-1}$ between these epochs. Future observations will put tight constraints on the deviation of $G$ over vast cosmological epochs not probed by other observations. ","Constraints on the time variation of the gravitational constant using
  gravitational-wave observations of binary neutron stars"
20,1244710319491919873,103540992,Amanpreet Singh,['New dataset TextCaps (think of TextVQA but for captioning) now available at <LINK>. Participate in the challenge on EvalAI (<LINK>) and read the paper at <LINK>. <LINK>'],https://arxiv.org/abs/2003.12462,"Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets. ",TextCaps: a Dataset for Image Captioning with Reading Comprehension
21,1244678891874193408,1133088397273313282,francesco croce,"['Confused about which methods for adversarial robustness are effective? We evaluated most of the recently published defenses with our new AutoAttack. Adversarial training (plus extra data) still works best!\n\ncode <LINK>\npaper <LINK>\n1/n <LINK>', 'We plan to maintain a list of the proposed adversarial defenses and their evaluation with AutoAttack here https://t.co/11wV0qEiB6. Feel free to suggest missing defenses!\n2/n https://t.co/uQDVkKUiXt', 'To reliably evaluate adversarial robustness we propose an ensemble of diverse attacks, AutoAttack, including white- and black-box attacks. All the hyperparameters are fixed, so that no tuning is required to the user!\n3/n', 'As part of AutoAttack, we propose AutoPGD, a variant of PGD which automatically adapts the step size. It achieves higher loss and lower robust accuracy than usual PGD with fixed step size!\n4/n https://t.co/E31bbYyPy0']",https://arxiv.org/abs/2003.01690,"The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses. ","Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks"
22,1243921060287975429,384900803,Shantanu Basu,['New paper on Multiple Power Law Distribution for Initial Mass Functions is available. A very satisfying collaboration with my @westernuAPMaths colleague Chris Essex and @TUChemnitz colleagues Janett Prehl and Karl Heinz Hoffmann. @westernuPhysAst <LINK>'],http://arxiv.org/abs/2003.10544,"We introduce a new multi-power-law distribution for the Initial Mass Function (IMF) to explore its potential properties. It follows on prior work that introduced mechanisms accounting for mass accretion in star formation, developed within the framework of general evolution equations for the mass distribution of accreting and non-accreting (proto)stars. This paper uses the same fundamental framework to demonstrate that the interplay between a mass-dependent and a time-dependent step-like dropout rate from accretion leads to IMFs that exhibit multiple power laws for an exponential mass growth. While the mass-dependent accretion and its dropout is intrinsic to each star, the time-dependent dropout might be tied to a specific history such as the rapid consumption of nebular material by nearby stars or the sweeping away of some material by shock waves. The time-dependent dropout folded into the mass-dependent process of star formation is shown to have a significant influence on the IMFs. ",A Multiple Power Law Distribution for Initial Mass Functions
23,1243919228836761600,127070843,Michael Sentef,['New paper with Michael Schüler on how to detect light-induced topology: <LINK>\n@Stanford @SLAClab @MPSDHamburg &amp; Fribourg University'],https://arxiv.org/abs/2003.11621,"Pumping graphene with circularly polarized light is the archetype of light-tailoring topological bands. Realizing the induced Floquet-Chern insulator state and tracing clear experimental manifestions has been a challenge, and it has become clear that scattering effects play a crucial role. We tackle this gap between theory and experiment by employing microscopic quantum kinetic calculations including realistic electron-electron and electron-phonon scattering. Our theory provides a direct link to the build-up of the Floquet-Chern insulator state in light-driven graphene and its detection in time- and angle-resolved photoemission spectroscopy (ARPES). This allows us to study the stability of the Floquet features due to dephasing and thermalization effects. We also discuss the ultrafast Hall response in the laser-heated state. Furthermore, the induced pseudospin texture and the associated Berry curvature gives rise to momentum-dependent orbital magnetization, which is reflected in circular dichroism in ARPES (CD-ARPES). Combining our nonequilibrium calculations with an accurate one-step theory of photoemission allows us to establish a direct link between the build-up of the topological state and the dichroic pump-probe photoemission signal. The characteristic features in CD-ARPES are further corroborated to be stable against heating and dephasing effects. Thus, tracing circular dichroism in time-resolve photoemission provides new insights into transient topological properties. ","How Circular Dichroism in time- and angle-resolved photoemission can be
  used to spectroscopically detect transient topological states in graphene"
24,1243616253740232704,1010495601799323648,Jorrit Leenaarts,"[""Sepideh Kianfar's new paper, read and be enlightened about the properties of chromospheric fibrils: <LINK>""]",https://arxiv.org/abs/2003.11302,"Broad-band images of the solar chromosphere in the Ca II H&K line cores around active regions are covered with fine bright elongated structures called bright fibrils. The mechanisms that form these structures and cause them to appear bright are still unknown. We aim to investigate the physical properties, such as temperature, line-of-sight velocity, and microturbulence, in the atmosphere that produces bright fibrils and to compare those to the properties of their surrounding atmosphere. We used simultaneous observations of a plage region in Fe I 6301-2 \r{A}, Ca II 8542 \r{A}, Ca II K, and H$\alpha$ acquired by the CRISP and CHROMIS instruments on the Swedish 1-m Solar Telescope. We manually selected a sample of 282 Ca II K bright fibrils. We compared the appearance of the fibrils in our sample to the Ca II 8542 \r{A} and H$\alpha$ data. We performed non-local thermodynamic equilibrium (non-LTE) inversions using the inversion code STiC on the Fe I 6301-2 \r{A}, Ca II 8542 \r{A}, Ca II K lines to infer the physical properties of the atmosphere. The line profiles in bright fibrils have a higher intensity in their K$_2$ peaks compared to profiles formed in the surrounding atmosphere. The inversion results show that the atmosphere in fibrils is on average $100-200$~K hotter at an optical depth log$(\tau) = -4.3$ compared to their surroundings. The line-of-sight velocity at chromospheric heights in the fibrils does not show any preference towards upflows or downflows. The microturbulence in the fibrils is on average 0.5 km s$^{-1}$ higher compared to their surroundings. Our results suggest that the fibrils have a limited extent in height, and they should be viewed as hot threads pervading the chromosphere. ",Physical properties of bright Ca II K fibrils in the solar chromosphere
25,1243597765160120320,3132910891,Delia Milliron,"['Generalized depletion attraction-induced assembly of metal oxide nanocrystals. New collaborative paper led by @csaezcab, combining synthesis, SAXS characterization, and theory by the @TM_Truskett group @texasmrsec @TexasChE <LINK> <LINK>']",https://arxiv.org/abs/2003.11633,"Nanocrystal gelation provides a powerful framework to translate nanoscale properties into bulk materials and to engineer emergent properties through the assembled microstructure. However, many established gelation strategies rely on chemical reactions and specific interactions, e.g., stabilizing ligands or ions on the surface of the nanocrystals, and are therefore not easily transferrable. Here, we report a general gelation strategy via non-specific and purely entropic depletion attractions applied to three types of metal oxide nanocrystals. The gelation thresholds of two compositionally distinct spherical nanocrystals agree quantitatively, demonstrating the adaptability of the approach for different chemistries. Consistent with theoretical phase behavior predictions, nanocrystal cubes form gels at a lower polymer concentration than nanocrystal spheres, allowing shape to serve as a handle to control gelation. These results suggest that the fundamental underpinnings of depletion-driven assembly, traditionally associated with larger colloidal particles, are also applicable at the nanoscale. ",Universal Gelation of Metal Oxide Nanocrystals via Depletion Attractions
26,1243551404297342982,888216099757490176,Maithra Raghu,"['A Survey of Deep Learning for Scientific Discovery\n\nTo help facilitate using DL in science, we survey a broad range of deep learning methods, new research results, implementation tips &amp; many links to code/tutorials\n\nPaper <LINK>\n\nWork with @ericschmidt\n \nThread⬇️ <LINK>', 'We begin with some high level considerations: (i) template ways in which deep learning can be used in scientific problems (ii) overviews of the entire end-to-end deep learning design process (iii) highlights of (when to use) key alternate machine learning methods', 'We provide links to incredible resources developed by the community: software packages &amp; high level APIs, freely available DL tutorials, sites with summaries/discussions/code of new research, repositories of DL pipelines &amp; pretrained models, data curation &amp; analysis packages https://t.co/JloKcxtAO4', 'We then describe core models/tasks/methods, including CNNs (detection, segmentation, registration, many others), graph NNs, sequence models + tasks (RNNs, transformers, attention, embeddings, Q&amp;A, seq2seq, etc) with links to science use cases, tutorials and code throughout https://t.co/ADfMsZ0W0k', 'We overview other powerful methods for training neural networks, such as transfer learning, domain adaptation, multitask learning and weak supervision. https://t.co/QPdWPCOjgS', 'In many (scientific) use cases not much data may be available to train machine learning models. Requiring less (labelled) data is a very active research area, and we overview new advances in (i) self-supervision (ii) semi-supervised learning (iii) data augmentation (iv)denoising https://t.co/Ag028UbRI9', 'Central to many scientific problems is going from *predictions* to *understanding*: identifying underlying mechanisms &amp; key data features. We survey results in interpretability &amp; representation analysis enabling data feature attribution &amp; insights on model hidden representations https://t.co/oCIkJaqgE2', 'We also (i) highlight core ideas and possible use cases of deep generative models and deep reinforcement learning (ii) provide implementation tips for getting started (explore data, try simple methods, start with known models/algorithms) and for debugging/improving performance', 'The best part of writing this survey was learning even more about the incredible work being done by the community across research, teaching courses, developing/opensourcing code, in-depth tutorials. Was very hard to reference it all! We welcome pointers to other related work!']",https://arxiv.org/abs/2003.11755,"Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models --- two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains. ",A Survey of Deep Learning for Scientific Discovery
27,1243527258951888896,197323213,Tianfu Wu,['Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis\n\nPaper: <LINK>\nCode: <LINK> (new version coming soon)'],https://arxiv.org/abs/2003.11571v1,"With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable inputs. This paper focuses on a recent emerged task, layout-to-image, to learn generative models that are capable of synthesizing photo-realistic images from spatial layout (i.e., object bounding boxes configured in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, to learn to unfold object masks of given bounding boxes in an input layout to bridge the gap between the input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks for the proposed layout-to-mask-to-image with style control at both image and mask levels. Object masks are learned from the input layout and iteratively refined along stages in the generator network. Style control at the image level is the same as in vanilla GANs, while style control at the object mask level is realized by a proposed novel feature normalization scheme, Instance-Sensitive and Layout-Aware Normalization. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. ","] Learning Layout and Style Reconfigurable GANs for Controllable Image
  Synthesis"
28,1243457640639930368,1323321295,Nicolas Tessore,['new paper by @itrharrison and me: Source Distributions of Cosmic Shear Surveys in Efficiency Space <LINK>'],https://arxiv.org/abs/2003.11558,"We show that the lensing efficiency of cosmic shear generically has a simple shape, even in the case of a tomographic survey with badly behaved photometric redshifts. We argue that source distributions for cosmic shear can therefore be more effectively parametrised in ``efficiency space''. Using realistic simulations, we find that the true lensing efficiency of a current cosmic shear survey without disconnected outliers in the redshift distributions can be described to per cent accuracy with only two parameters, and the approach straightforwardly generalises to other parametric forms and surveys. The cosmic shear signal is thus largely insensitive to the details of the source distributions, and the features that matter can be summarised by a small number of suitable efficiency parameters. For the simulated survey, we show that prior knowledge at the 10% level, which is attainable e.g. from photometric redshifts, is enough to marginalise over the efficiency parameters without severely affecting the constraints on the cosmology parameters $\Omega_m$ and $\sigma_8$. ",Source Distributions of Cosmic Shear Surveys in Efficiency Space
29,1243235434672664576,80454546,Greg Gilbert,"['My new paper, “An information theoretic framework for classifying exoplanetary system architectures” is on arXiv today! A thread 👇🏼  (1/n)\n\n<LINK>', 'Headline result #1: planets really are “peas in a pod” - within a system, planets tend to be roughly the same size and roughly evenly spaced. (2/n) https://t.co/K1hevkYEDb', 'Method: we define quantities to describe the dynamical mass (µ), mass partitioning (Q), gap complexity (C), spacing scale (S), and monotonicity (M) of each system, finding Q and C both strongly peaked near 0 - equal sizes and even spacings. (3/n) https://t.co/fgBLJEDq3G', 'Comparing to forward models (SysSim, He+ 2019; EPOS, Mulders+ 2019), we find that these are real features, not selection effects. In fact, planets are even more evenly spaced than previously realized. (4/n) https://t.co/Aw3WU8Ha8B', 'Most planets are separated by ~20 mutual Hill radii, with evidence of an “echo peak” at ~30 mutual Hill radii. The same feature shows up in forward models; a natural explanation is missing planets. (5/n) https://t.co/Twi7EVTGdR', 'We apply unsupervised clustering on these quantities (µ, Q, C, S), plus a measure of flatness (f) and find two populations, the smaller of which has both wide spacing and uneven spacings, more evidence for missing planets. (6/n) https://t.co/e27imK6HBS', 'It’s worth repeating: the same systems which are widely spaced also tend to be unevenly spaced. (7/n)', 'We also find that the typical system-to-star mass ratio a bit less than ~10^4, reminiscent of the giant moon systems of Jupiter, Saturn, and Uranus. Taken together, these various observations lead us to conclude… (8/n) https://t.co/iCkRA6ftNo', 'Headline result #2: most of Kepler’s high-multiplicity (N≥3) systems belong to a single intrinsic population, with a subset of systems (~20%) hosting additional undetected planets intermediate in period between the known planets. (9/n)', 'So what’s next? We plan to apply this method to other populations (e.g. RV) and search for hypothetical missing planets. These statistics will also provide improved targets for population synthesis and planet formation models. (10/n)', 'Thanks for reading! Questions? Comments? Feel free to email or message me and I’m happy to chat!']",https://arxiv.org/abs/2003.11098,"We propose several descriptive measures to characterize the arrangements of planetary masses, periods, and mutual inclinations within exoplanetary systems. These measures are based in complexity theory and capture the global, system-level trends of each architecture. Our approach considers all planets in a system simultaneously, facilitating both intra-system and inter-system analysis. We find that based on these measures, Kepler's high-multiplicity ($N\geq3$) systems can be explained if most systems belong to a single intrinsic population, with a subset of high-multiplicity systems ($\sim20\%$) hosting additional, undetected planets intermediate in period between the known planets. We confirm prior findings that planets within a system tend to be roughly the same size and approximately coplanar. We find that forward modeling has not yet reproduced the high degree of spacing similarity (in log-period) actually seen in the Kepler data. Although our classification scheme was developed using compact Kepler multis as a test sample, our methods can be immediately applied to any other population of exoplanetary systems. We apply this classification scheme to (1) quantify the similarity between systems, (2) resolve observational biases from physical trends, and (3) identify which systems to search for additional planets and where to look for these planets. ","An information theoretic framework for classifying exoplanetary system
  architectures"
30,1243222083657551873,89781782,Ivan Nazarov,"['In a new paper <LINK> with @burnaevevgeny we extend (Sparse) Variational Dropout methods to deep Complex-valued networks and show that they also compress them reliably well. Methods are in <LINK> (pytorch), experiments -- <LINK>']",https://arxiv.org/abs/2003.11413,"With continual miniaturization ever more applications of deep learning can be found in embedded systems, where it is common to encounter data with natural complex domain representation. To this end we extend Sparse Variational Dropout to complex-valued neural networks and verify the proposed Bayesian technique by conducting a large numerical study of the performance-compression trade-off of C-valued networks on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription on MusicNet. We replicate the state-of-the-art result by Trabelsi et al. [2018] on MusicNet with a complex-valued network compressed by 50-100x at a small performance penalty. ",Bayesian Sparsification Methods for Deep Complex-valued Networks
31,1242919852634849280,989251872107085824,Quoc Le,"['New paper: Meta Pseudo Labels\n\nSelf-training has a pre-trained teacher to generate pseudo labels to train a student. Here we use the student’s performance to meta-train the teacher to generate better pseudo labels. Works well on ImageNet 10%.\n\nLink: <LINK> <LINK>', 'This work continues our efforts on semi-supervised learning such as\n\nUDA: https://t.co/J74Nn4i8no\nMixMatch: https://t.co/34ztIFttUQ\nFixMatch: https://t.co/3qTUVbPO0N\nNoisy Student: https://t.co/ZYDaef6sdp\netc.\n\nJoint work with @hieupham789 @QizheXie @ZihangDai', 'Some people pointed out some bugs with the figure. We fixed the comparisons and the updated figure is below. https://t.co/onP3mr9iS7']",https://arxiv.org/abs/2003.10580,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL ",Meta Pseudo Labels
32,1242839362041151488,778144844,"Dean Astumian, OFS",['See new paper out today on ArXiv.  Explains the importance of kinetic asymmetry and why all transitions are important for systems in contact with several reservoirs that are not in equilibrium with one another.\n\n<LINK>'],https://arxiv.org/abs/2003.10747,"In a recent perspective article, Horowitz and Gingrich discuss thermodynamic uncertainty relations that have been derived using ""stochastic thermodynamics"", a theory based on a hypothesis known as local detailed balance. The authors examined the foundations of this theory in their ""Box 1: A brief primer on local detailed balance and stochastic thermodynamics"", where a kinetic scheme for transport of particles between two reservoirs is presented. Horowitz and Gingrich arrive at a relationship between the probability to cycle through the states in one order vs. the probability to cycle in the reverse order. This relation bears on the extremely important question of what governs directionality when a system is driven away from equilibrium by contact with multiple reservoirs that are not in equilibrium with one another. In the original version of their paper the relation given for this ratio was obviously wrong and contrary to the second law of thermodynamics. Based on our private communications and on the recent paper authored by my colleagues and myself, Horowitz and Gingrich accepted the necessity of correcting the error in their Box 1. Unfortunately, in making the correction, the authors introduced an equally serious, if less transparent, mistake, and continue to base their theory on the thermodynamically impossible idea that transitions are mediated by only one of the two reservoirs. In this comment we illustrate how the principle of microscopic reversibility reveals that the true origin of directional cycling amongst a network of states is kinetic asymmetry. This understanding is important in guiding synthesis of molecular machines and other devices designed to exploit transport or catalysis to drive non-equilibrium processes. ","Comment on the perspective article ""Thermodynamic uncertainty relations
  constrain non-equilibrium fluctuations"""
33,1242634502377222149,87451094,Hotta Hideyuki,['Our new paper about solar flux emergence is accepted by MNRAS and you can access it in arxiV. Here I also upload two interesting movies related to the paper.\n\n<LINK>\n<LINK>'],http://arxiv.org/abs/2003.10583,"We investigate the rising flux tube and the formation of sunspots in an unprecedentedly deep computational domain that covers the whole convection zone with a radiative magnetohydrodynamics simulation. Previous calculations had shallow computational boxes (< 30 Mm) and convection zones at a depth of 200 Mm. By using our new numerical code R2D2, we succeed in covering the whole convection zone and reproduce the formation of the sunspot from a simple horizontal flux tube because of the turbulent thermal convection. The main findings are (1) The rising speed of the flux tube is larger than the upward convection velocity because of the low density caused by the magnetic pressure and the suppression of the mixing. (2) The rising speed of the flux tube exceeds 250 m/s at a depth of 18 Mm, while we do not see any clear evidence of the divergent flow 3 hr before the emergence at the solar surface. (3) Initially, the root of the flux tube is filled with the downflows and then the upflow fills the center of the flux tube during the formation of the sunspot. (4) The essential mechanisms for the formation of the sunspot are the coherent inflow and the turbulent transport. (5) The low-temperature region is extended to a depth of at least 40 Mm in the matured sunspot, with the high-temperature region in the center of the flux tube. Some of the findings indicate the importance of the deep computational domain for the flux emergence simulations. ",On rising magnetic flux tube and formation of sunspots in a deep domain
34,1242631473519136773,1976929051,Sayna Ebrahimi,"['Our new work on continually learning tasks without forgetting using adversarial learning!\nJoint work with @facebookai with @_kainoa_ , @RCalandra , Trevor Darrell, and @marcus_rohrbach \nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/2003.09553,Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at \url{this https URL}. ,Adversarial Continual Learning
35,1242616327866650625,1077995761487568896,Jon Miller,"['New paper day!  Student Mayura Balakrishnan (@mayuishungry) analyzed *15* @NASASwift spectra of the black hole GRO J1655-40.  The most disk wind spectra in an outburst, by a factor of several.  Magnetic pressure is expelling the gas at over 10,000 km/s.\n<LINK> <LINK>', '@ciropinto1982 @mayuishungry @NASASwift thank you!']",https://arxiv.org/abs/2003.10945,"Chandra obtained two High Energy Transmission Grating (HETG) spectra of the stellar-mass black hole GRO J1655-40 during its 2005 outburst, revealing a rich and complex disk wind. Soon after its launch, the Neil Gehrels Swift Observatory began monitoring the same outburst. Some X-ray Telescope (XRT) observations were obtained in a mode that makes it impossible to remove strong Mn calibration lines, so the Fe K-alpha line region in the spectra was previously neglected. However, these lines enable a precise calibration of the energy scale, facilitating studies of the absorption-dominated disk wind and its velocity shifts. Here, we present fits to 15 Swift/XRT spectra, revealing variability and evolution in the outflow. The data strongly point to a magnetically driven disk wind: both the higher velocity (e.g., v ~ 10^4 km/s) and lower velocity (e.g., v ~ 10^3 km/s) wind components are typically much faster than is possible for thermally driven outflows (v < 200 km/s), and photoionization modeling yields absorption radii that are two orders of magnitude below the Compton radius that defines the typical inner extent of thermal winds. Moreover, correlations between key wind parameters yield an average absorption measure distribution (AMD) that is consistent with magnetohydrodynamic wind models. We discuss our results in terms of recent observational and theoretical studies of black hole accretion disks and outflows, and future prospects. ","Swift Spectroscopy of the Accretion Disk Wind in the Black Hole GRO
  J1655-40"
36,1242505004524670981,2698179823,Yaron Lipman,['New paper: Universal Differentiable Renderer (UDR) - learning 3D shapes from 2D images under a  wide range of reflectance and lighting models. <LINK>\n\n@YarivLior @matanatzmon <LINK>'],https://arxiv.org/abs/2003.09852,"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail. ","Multiview Neural Surface Reconstruction by Disentangling Geometry and
  Appearance"
37,1242482510648373251,288623330,Aaron Hertzmann,"['Our new CVPR 2020 paper combines classic geometric algorithms with modern machine learning to produce state-of-the-art line drawings from 3D models. This is the work of Difan Liu, with Evangelos Kalogerakis, Mohamed Nabail, and myself. Link: <LINK> <LINK>', '@msurguy Unfortunately the current algorithm produces bitmap output; vector output is future work ...']",https://arxiv.org/abs/2003.10333,"This paper introduces a method for learning to generate line drawings from 3D models. Our architecture incorporates a differentiable module operating on geometric features of the 3D model, and an image-based module operating on view-based shape representations. At test time, geometric and view-based reasoning are combined with the help of a neural module to create a line drawing. The model is trained on a large number of crowdsourced comparisons of line drawings. Experiments demonstrate that our method achieves significant improvements in line drawing over the state-of-the-art when evaluated on standard benchmarks, resulting in drawings that are comparable to those produced by experienced human artists. ",Neural Contours: Learning to Draw Lines from 3D Shapes
38,1242479062746116096,1169068112177745922,Alexis Plascencia,"['The paper with Pavel @fileviez, Clara Murgui and Elliot Golias is out 😀 We studied constraints from SM Higgs decays into new leptophobic gauge bosons ZB and the associated production of the Higgs with a ZB at the LHC \n\n<LINK> <LINK>']",https://arxiv.org/abs/2003.09426,"The Higgs boson could provide the key to discover new physics at the Large Hadron Collider. We investigate novel decays of the Standard Model (SM) Higgs boson into leptophobic gauge bosons which can be light in agreement with all experimental constraints. We study the associated production of the SM Higgs and the leptophobic gauge boson that could be crucial to test the existence of a leptophobic force. Our results demonstrate that it is possible to have a simple gauge extension of the SM at the low scale, without assuming very small couplings and in agreement with all the experimental bounds that can be probed at the LHC. ",The Higgs and Leptophobic Force at the LHC
39,1242477984671563777,1070756639127556097,Ryan Beal,"['A pre-print version of our new paper ""Optimising Game Tactics for Football"" is now available, we use game theory and machine learning to help with pre and in-match decisions. \n\nYou can read it here ➡️ <LINK>', 'The final version of the paper will be published in the proceedings of  AAMAS 2020 later this year!']",https://arxiv.org/abs/2003.10294,"In this paper we present a novel approach to optimise tactical and strategic decision making in football (soccer). We model the game of football as a multi-stage game which is made up from a Bayesian game to model the pre-match decisions and a stochastic game to model the in-match state transitions and decisions. Using this formulation, we propose a method to predict the probability of game outcomes and the payoffs of team actions. Building upon this, we develop algorithms to optimise team formation and in-game tactics with different objectives. Empirical evaluation of our approach on real-world datasets from 760 matches shows that by using optimised tactics from our Bayesian and stochastic games, we can increase a team chances of winning by up to 16.1\% and 3.4\% respectively. ",Optimising Game Tactics for Football
40,1242463821953773574,1182901422200934400,Yohan Dubois,"['Today on the arXiv is our new paper led by my former PhD student Gohar Dashyan on cosmic ray feedback from supernovae in dwarf galaxies. Take home message: accelerated cosmic rays in supernova remnants are key to produce massive large-scale galactic winds.\n<LINK> <LINK>', 'A bit of context first: on top of galaxy formation problems, there is the long-standing issue of regulating the mass in galaxies. Observations teach us that both low-mass and high-mass galaxies have a mass content so low that ``something’’ has to eject gas from galaxies. https://t.co/5wTISRXf6S', 'This ``something’’ is coined feedback and is very likely to be powered by dying massive stars, aka supernovae, in low-mass galaxies. (And supermassive black holes for massive galaxies, but that is a different story) https://t.co/7pIDtsnGSu', 'Supernovae release large amount of energy, which is large enough to drive the observed large-scale galactic winds. They are supposed to propel large-scale winds by producing strong shocks through the thermal pressure built in the shell that is sweeping up the surrounding gas. https://t.co/chGEC00QK7', 'The problem is that galactic gas has such properties that supernova-driven shocks lose their thermal pressure through radiation, such that, quickly, there is not sufficient energy left to keep accelerating the gas to velocities large enough to drive the large-scale galactic wind.', 'This is where cosmic rays are trying to save this scenario of supernova feedback. Cosmic rays, here mostly made of charged protons, are produced by bouncing off protons across the shock back and forth until they have supra-thermal energies and escape the shock discontinuity.', 'Now comes the two key features of cosmic rays:', '1) in typical interstellar medium densities, cosmic rays radiate their energy at a much slower rate than that of thermal particles, hence, they provide a pressure support to the expanding supernova bubble for much longer time, allowing for faster terminal velocities;', '2) cosmic rays scatter off the perturbed magnetic field lines so that, collectively as a fluid, they diffuse in space along magnetic field lines.', 'Our work, based on numerical simulations, models low-mass disc galaxies with star formation, supernova feedback and cosmic rays. These simulations have a resolution large enough to separate the dense and cold star forming regions from the hot and diffuse gas in galaxies. https://t.co/JR03RvuFim', 'These simulations show that, as long as there is a moderate amount of diffusion, cosmic rays injected in supernova remnants always enhance the strength of large-scale galactic winds as opposed to simulations without cosmic rays. https://t.co/Avh3YSCFxE', 'Diffusion is the key process to allow for cosmic rays to have a large-scale effect, because, otherwise, cosmic rays are trapped in star forming clumps where they quench the star formation, and where there is too much mass for it to be launched above the galaxy escape velocity.', 'With diffusion, cosmic rays leak out of the dense star forming clumps, where most supernovae are released.', 'It is then easier to launch outflows at velocities faster than the escape velocity of the galaxy, ii) there is less radiative losses, iii) and, at the same time, the star forming regions continue to moderately power star formation and the release of supernovae.', 'This cosmic ray-enhanced supernova feedback provides wind mass outflow rates in much better agreement with observations (but still lying on the weak side), and winds are now faster, denser and colder. https://t.co/SsqYNMQ9K7', 'In addition, we tested cosmic ray streaming, that is the process by which cosmic rays propagate (stream) along their own gradient of pressure at velocities close to that of Alfvén velocities (the velocity at which a magnetic perturbation propagates into the fluid).', 'Cosmic ray streaming alone, without diffusion, do not allow for efficient large-scale galactic winds, as Alfvén velocities are small compared to typical supernova wind velocities.', ""That's the end! I hope you will like reading this paper as much as I suffered typing my first ever Twitter thread.""]",https://arxiv.org/abs/2003.09900,"The regulation of the baryonic content in dwarf galaxies is a long-standing problem. Supernovae (SNe) are supposed to play a key role in forming large-scale galactic winds by removing important amounts of gas from galaxies. SNe are efficient accelerators of non-thermal particles, so-called cosmic rays (CRs), which can substantially modify the dynamics of the gas and conditions to form large-scale galactic winds. We investigate how CR injection by SNe impacts the star formation and the formation of large-scale winds in dwarf galaxies, and whether it can produce galaxy star-formation rates (SFR) and wind properties closer to observations. We ran CR magneto-hydrodynamical simulations of dwarf galaxies at high resolution (9 pc) with the adaptive mesh refinement code ramses. Disc galaxies are embedded in isolated halos of mass of $10^{10}$ and $10^{11} \, \rm M_{\odot}$, and CRs are injected by SNe. We included CR isotropic and anisotropic diffusion with various diffusion coefficients, CR radiative losses, and CR streaming. The injection of CR energy into the interstellar medium smooths out the highest gas densities, which reduces the SFR by a factor of 2-3. Mass outflow rates are significantly greater with CR diffusion, by 2 orders of magnitudes for the higher diffusion coefficients. Without diffusion and streaming, CRs are inefficient at generating winds. CR streaming alone allows for the formation of winds but which are too weak to match observations. The formation of galactic winds strongly depends on the diffusion coefficient: for low coefficients, CR energy stays confined in high density regions where CR energy losses are highest, and higher coefficients, which allow for a more efficient leaking of CRs out of dense gas, produce stronger winds. CR diffusion leads to colder and denser winds than without CRs, and brings outflow rates and mass loading factors much closer to observations. ",Cosmic ray feedback from supernovae in dwarf galaxies
41,1242455162444222464,198283832,Lefteris Spyromitros,"['We got a new paper accepted at #ijcnn2020, part of #wcci2020! And we just made it available at arXiv: <LINK>']",https://arxiv.org/abs/2003.09896,"Multi-target regression is concerned with the prediction of multiple continuous target variables using a shared set of predictors. Two key challenges in multi-target regression are: (a) modelling target dependencies and (b) scalability to large output spaces. In this paper, a new multi-target regression method is proposed that tries to jointly address these challenges via a novel problem transformation approach. The proposed method, called MRQ, is based on the idea of quantizing the output space in order to transform the multiple continuous targets into one or more discrete ones. Learning on the transformed output space naturally enables modeling of target dependencies while the quantization strategy can be flexibly parameterized to control the trade-off between prediction accuracy and computational efficiency. Experiments on a large collection of benchmark datasets show that MRQ is both highly scalable and also competitive with the state-of-the-art in terms of accuracy. In particular, an ensemble version of MRQ obtains the best overall accuracy, while being an order of magnitude faster than the runner up method. ",Multi-target regression via output space quantization
42,1242432976794931200,4039347852,Dimitry Ayzenberg,"['New paper on extending our non-Kerr black hole X-ray reflection model RELXILL_NK to include a finite thickness disk first studied by Taylor &amp; Reynolds (2018), that are more realistic than the commonly used infinitesimally thin disks.\n\n<LINK>']",https://arxiv.org/abs/2003.09663,"X-ray reflection spectroscopy is a powerful tool for probing the strong gravity region of black holes and can be used for testing general relativity in the strong field regime. Simplifications of the available relativistic reflection models limit the capability of performing accurate measurements of the properties of black holes. In this paper, we present an extension of the model RELXILL_NK in which the accretion disk has a finite thickness rather than being infinitesimally thin. We employ the accretion disk geometry proposed by Taylor & Reynolds (2018) and we construct relativistic reflection models for different values of the mass accretion rate of the black hole. We apply the new model to high quality Suzaku data of the X-ray binary GRS 1915+105 to explore the impact of the thickness of the disk on tests of the Kerr metric. ","Testing the Kerr black hole hypothesis using X-ray reflection
  spectroscopy and a thin disk model with finite thickness"
43,1242398961647988736,61623544,Dr./Prof. Renée Hložek,"['Some good news in the shut down - the hard work of the past few months paid off, congrats Jurek on the paper, and nice to have some new collaborators too! <LINK> #axion #cosmology']",https://arxiv.org/abs/2003.09655,"Intensity mapping (IM) of spectral lines has the potential to revolutionize cosmology by increasing the total number of observed modes by several orders of magnitude compared to the cosmic microwave background (CMB) anisotropies. In this paper, we consider IM of neutral hydrogen (HI) in the redshift range $0 \lesssim z \lesssim 3$ employing a halo model approach where HI is assumed to follow the distribution of dark matter (DM) halos. If a portion of the DM is composed of ultralight axions then the abundance of halos is changed compared to cold dark matter below the axion Jeans mass. With fixed total HI density, $\Omega_{\rm HI}$, assumed to reside entirely in halos, this effect introduces a scale-independent increase in the HI power spectrum on scales above the axion Jeans scale, which our model predicts consistent with N-body simulations. Lighter axions introduce a scale-dependent feature even on linear scales due to its suppression of the matter power spectrum near the Jeans scale. We use the Fisher matrix formalism to forecast the ability of future HI surveys to constrain the axion fraction of DM and marginalize over astrophysical and model uncertainties. We find that a HIRAX-like survey is a very reliable IM survey configuration, being affected minimally by uncertainties due to non-linear scales, while the SKA1MID configuration is the most constraining as it is sensitive to non-linear scales. Including non-linear scales and combining a SKA1MID-like IM survey with the Simons Observatory CMB, the benchmark ""fuzzy DM"" model with $m_a = 10^{-22}\text{ eV}$ can be constrained at the 10% level. For lighter ULAs this limit improves below 1%, and allows the possibility to test the connection between axion models and the grand unification scale across a wide range of masses. ",Intensity Mapping as a Probe of Axion Dark Matter
44,1242375218062114816,1177063549606203394,Tommi Tenkanen,"['A new paper out! <LINK> 1/n', 'We showed that a ""non-minimal"" coupling between the field that drives cosmic inflation (the ""inflaton"" field) and gravity can not only make some inflationary models consistent with cosmological data but can also realize graceful exit from inflation. 2/n', 'In particular, this is the case in models where a destabilizing mechanism that ends inflation should be assumed when the model is only ""minimally"" coupled to gravity. 3/n', 'As explicit examples, we considered the power-law and inverse monomial inflation models with a non-minimal coupling to gravity. While these models are excluded in the minimally coupled case, we showed that they can become viable again in non-minimally coupled scenarios. 4/n', 'We also argue that in most scenarios we considered reheating of the universe after inflation can be naturally realized via gravitational particle production but that this depends on the underlying theory of gravity in a non-trivial - and therefore very interesting - way. 5/5']",https://arxiv.org/abs/2003.10203,"We show that a non-minimal coupling to gravity can not only make some inflationary models consistent with cosmological data, similar to the case of Higgs inflation, but can also invoke slow-roll violation to realize graceful exit from inflation. In particular, this is the case in models where a destabilizing mechanism that ends inflation should be assumed when the model is minimally coupled to gravity. As explicit examples, we consider the power-law and inverse monomial inflation models with a non-minimal coupling to gravity. While these models are excluded in the minimally coupled case, we show that they can become viable again in non-minimally coupled scenarios. In most scenarios we considered, reheating can be naturally realized via gravitational particle production but that this depends on the underlying theory of gravity in a non-trivial way. ",Violation of slow-roll in non-minimal inflation
45,1242364582808686592,1198546068713201664,Alejandro Cuetos Menéndez,['We have uploaded to arXiv a new paper about dynamic properties of  nematic phase of cuboidal particles. This collaboration  with Alessandro Patti group from University of Manchester is now under revision in Soft Matter. <LINK>'],https://arxiv.org/abs/2003.09824,"Colloidal cuboids have the potential to self-assemble into biaxial liquid crystal phases, which exhibit two independent optical axes. Over the last few decades, several theoretical works predicted the existence of a wide region of the phase diagram where the biaxial nematic phase would be stable, but imposed rather strong constraints on the particle rotational degrees of freedom. In this work, we employ molecular simulation to investigate the impact of size dispersity on the phase behaviour of freely-rotating hard cuboids, here modelled as self-dual-shaped nanoboards. This peculiar anisotropy, exactly in between oblate and prolate geometry, has been proposed as the most appropriate to promote phase biaxiality. We observe that size dispersity radically changes the phase behaviour of monodisperse systems and leads to the formation of the elusive biaxial nematic phase, being found in an large region of the packing fraction vs polydispersity phase diagram. Although our results confirm the tendencies reported in past experimental observations on colloidal dispersions of slightly prolate goethite particles, they cannot reproduce the direct isotropic-to-biaxial nematic phase transition observed in these experiments. ","Self-assembly of Freely-rotating Polydisperse Cuboids: Unveiling the
  Boundaries of the Biaxial Nematic Phase"
46,1242301851619307521,258996336,Sandeep K. Goyal,['<LINK>\n\nCheck out our new paper on the #Quantum k-nearest neighbor #MachineLearning algorithm.  #QuantumComputing'],Https://arxiv.org/abs/2003.09187,"One of the simplest and most effective classical machine learning algorithms is the $k$-nearest neighbors algorithm ($k$NN) which classifies an unknown test state by finding the $k$ nearest neighbors from a set of $M$ train states. Here we present a quantum analog of classical $k$NN $-$ quantum $k$NN (Q$k$NN) $-$ based on fidelity as the similarity measure. We show that Q$k$NN algorithm can be reduced to an instance of the quantum $k$-maxima algorithm, hence the query complexity of Q$k$NN is $O(\sqrt{kM})$. The non-trivial task in this reduction is to encode the fidelity information between the test state and all the train states as amplitudes of a quantum state. Converting this amplitude encoded information to a digital format enables us to compare them efficiently, thus completing the reduction. Unlike classical $k$NN and existing quantum $k$NN algorithms, the proposed algorithm can be directly used on quantum data thereby bypassing expensive processes such as quantum state tomography. As an example, we show the applicability of this algorithm in entanglement classification and quantum state discrimination. ",Quantum $k$-nearest neighbors algorithm
47,1240845694421721089,92182169,Ian Manchester,['New paper (with Ray Wang and Roland Toth) introducing virtual control contraction metrics:\n<LINK>\nExtends CCM to a wider class of problems. Generalises both local (gain scheduling) and global (exact linearisation) LPV control with stronger theoretical guarantees'],https://arxiv.org/abs/2003.08513,"This paper proposes a novel approach to nonlinear state-feedback control design that has three main advantages: (i) it ensures exponential stability and $ \mathcal{L}_2 $-gain performance with respect to a user-defined set of reference trajectories, and (ii) it provides constructive conditions based on convex optimization and a path-integral-based control realization, and (iii) it is less restrictive than previous similar approaches. In the proposed approach, first a virtual representation of the nonlinear dynamics is constructed for which a behavioral (parameter-varying) embedding is generated. Then, by introducing a virtual control contraction metric, a convex control synthesis formulation is derived. Finally, a control realization with a virtual reference generator is computed, which is guaranteed to achieve exponential stability and $ \mathcal{L}_2 $-gain performance for all trajectories of the targeted reference behavior. Connections with the linear-parameter-varying (LPV) theory are also explored showing that the proposed methodology is a generalization of LPV state-feedback control in two aspects. First, it is a unified generalization of the two distinct categories of LPV control approaches: global and local methods. Second, it provides rigorous stability and performance guarantees when applied to the true nonlinear system, while such properties are not guaranteed for tracking control using LPV approaches. ","Virtual Control Contraction Metrics: Convex Nonlinear Feedback Design
  via Behavioral Embedding"
48,1240825236053831680,767659609,Yoshihiko Hasegawa,"['My new paper entitled ""Thermodynamic uncertainty relation for open quantum systems"" appeared in arXiv. <LINK>']",https://arxiv.org/abs/2003.08557,"We derive a thermodynamic uncertainty relation for general open quantum dynamics, described by a joint unitary evolution on a composite system comprising a system and an environment. By measuring the environmental state after the system-environment interaction, we bound the counting observables in the environment by the survival activity, which reduces to the dynamical activity in classical Markov processes. Remarkably, the relation derived herein holds for general open quantum systems with any counting observable and any initial state. Therefore, our relation is satisfied for classical Markov processes with arbitrary time-dependent transition rates and initial states. We apply our relation to continuous measurement and the quantum walk to find that the quantum nature of the system can enhance the precision. Moreover, we can make the lower bound arbitrarily small by employing appropriate continuous measurement. ",Thermodynamic uncertainty relation for general open quantum systems
49,1240803383776350208,47768825,Ramesh Raskar,['Apps Gone Rogue: Unintended Consequences of Digital Solutions in an Epidemic (Our new paper)\n<LINK>'],https://arxiv.org/abs/2003.08567,"Containment, the key strategy in quickly halting an epidemic, requires rapid identification and quarantine of the infected individuals, determination of whom they have had close contact with in the previous days and weeks, and decontamination of locations the infected individual has visited. Achieving containment demands accurate and timely collection of the infected individual's location and contact history. Traditionally, this process is labor intensive, susceptible to memory errors, and fraught with privacy concerns. With the recent almost ubiquitous availability of smart phones, many people carry a tool which can be utilized to quickly identify an infected individual's contacts during an epidemic, such as the current 2019 novel Coronavirus crisis. Unfortunately, the very same first-generation contact tracing tools have been used to expand mass surveillance, limit individual freedoms and expose the most private details about individuals. We seek to outline the different technological approaches to mobile-phone based contact-tracing to date and elaborate on the opportunities and the risks that these technologies pose to individuals and societies. We describe advanced security enhancing approaches that can mitigate these risks and describe trade-offs one must make when developing and deploying any mass contact-tracing technology. With this paper, our aim is to continue to grow the conversation regarding contact-tracing for epidemic and pandemic containment and discuss opportunities to advance this space. We invite feedback and discussion. ",Apps Gone Rogue: Maintaining Personal Privacy in an Epidemic
50,1240676987577888768,1240425108621025280,danielgordon10,"[""Are you stuck inside and don't know what to do? Read my new paper. Watching the World Go By: Representation Learning from Unlabeled Videos <LINK>""]",https://arxiv.org/abs/2003.07990,"Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at this https URL ",Watching the World Go By: Representation Learning from Unlabeled Videos
51,1240554228399005697,923132721760649216,kosukekurosawa,['Our new paper has been accepted for publication in JGR-Planets. We investigated the required spatial resolution in numerical models for reproducing ejecta velocity distribution by comparing ultra-high-speed images of impact ejecta. <LINK>'],https://arxiv.org/abs/2003.08103,"High-speed impact ejecta at velocities comparable to the impact velocity are expected to contribute to material transport between planetary bodies and deposition of ejecta far from the impact crater. We investigated the behavior of high-speed ejecta produced at angles of 45 and 90 degrees, using both experimental and numerical methods. The experimental system developed at the Planetary Exploration Research Center of Chiba Institute of Technology (Japan) allowed us to observe the initial growth of the ejecta. We succeeded in imaging high-speed ejecta at 0.2 $\mathrm{{\mu}}$s intervals for impacts of polycarbonate projectiles of 4.8 mm diameter onto a polycarbonate plate at an impact velocity of ~4 km s$^{-1}$. Smoothed particle hydrodynamics (SPH) simulations of various numerical resolutions were conducted for the same impact conditions as pertaining to the experiments. We compared the morphology and velocities of the ejecta for the experiments and simulations, and we confirmed a close match for high-resolution simulations (with $\geq10^6$ SPH particles representing the projectile). According to the ejecta velocity distributions obtained from our high-resolution simulations, the ejection velocities of the high-speed ejecta for oblique impacts are much greater than those for vertical impacts. The translational motion of penetrating projectiles parallel to the target surface in oblique impacts could cause long-term, sustained acceleration at the root of the ejecta. ","Impact Ejecta near the Impact Point Observed using Ultra-high-speed
  Imaging and SPH Simulations, and a Comparison of the Two Methods"
52,1240363475412475904,92182169,Ian Manchester,"['New paper (with Vera Somers) on sparse resource allocation for control of spreading processes in networks:\n<LINK>\nWe transform the problem to an exponential cone program, with l1 (or reweighted l1) resource constraints and a cost on discounted future impact/risk.', 'Dynamics are a linear upper-bound on an SIS/cellular-automata model (accurately modelling exponential growth phase). Code available here:\nhttps://t.co/E9OroeDCpJ\nusing Matlab, Yalmip, @mosektw', 'Note that we do not claim that this work is directly helpful or relevant to the current COVID-19 outbreak (been working on it for a while, just coincidence).\n\nIn fact, until a few weeks ago we thought the most topical application in Australia) would be bushfires!', 'Also, open post-doc position to work on related problems:\nhttps://t.co/7koFS1K1f5\nCloses 13th of April, Sydney Time (12th of April most other places).']",https://arxiv.org/abs/2003.07555,"In this letter we propose a method for sparse allocation of resources to control spreading processes -- such as epidemics and wildfires -- using convex optimization, in particular exponential cone programming. Sparsity of allocation has advantages in situations where resources cannot easily be distributed over a large area. In addition, we introduce a model of risk to optimize the product of the likelihood and the future impact of an outbreak. We demonstrate with a simplified wildfire example that our method can provide more targeted resource allocation compared to previous approaches based on geometric programming. ","Sparse Resource Allocation for Control of Spreading Processes via Convex
  Optimization"
53,1240216622390001664,589532859,Sylvain Lobry,"['Our new paper on VQA for remote sensing is on #arxiv!\n<LINK>\n\nOur goal with it is to make information extraction from RS images accessible to everyone, and faster for experts.\nIt comes with two datasets online, accessible here: <LINK> <LINK>']",https://arxiv.org/abs/2003.07333,"This paper introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information which can be useful for a wide range of tasks including land cover classification, object counting or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two datasets (using low and high resolution data) of image/question/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The datasets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on Convolutional Neural Networks (CNNs) for the visual part and on a Recurrent Neural Network (RNN) for the natural language part to this task. The model is trained on the two datasets, yielding promising results in both cases. ",RSVQA: Visual Question Answering for Remote Sensing Data
54,1239967777084952577,501534807,Yuhao Zhang,"[""Very excited to announce Stanza v1.0.0, our latest #NLProc library that provides full state-of-the-art Universal Dependencies support and very accurate NER. We've also compared its performance against similar toolkits in our new system paper: <LINK> Check it out! <LINK>""]",https://arxiv.org/abs/2003.07082,"We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at this https URL ","Stanza: A Python Natural Language Processing Toolkit for Many Human
  Languages"
55,1239959495230418945,45105022,Riccardo Sapienza,['New paper form the group!Electrical control of single-photon emission in highly-charged individual colloidal quantum dots <LINK>'],https://arxiv.org/abs/2003.06382,"Electron transfer to an individual quantum dot promotes the formation of charged excitons with enhanced recombination pathways and reduced lifetimes. Excitons with only one or two extra charges have been observed and exploited for very efficient lasing or single quantum dot LEDs. Here, by room-temperature time-resolved experiments on individual giant-shell CdSe/CdS quantum dots, we show the electrochemical formation of highly charged excitons containing more than twelve electrons and one hole. We report the control over intensity blinking, along with a deterministic manipulation of quantum dot photodynamics, with an observed 210-fold increase of the decay rate, accompanied by 12-fold decrease of the emission intensity, while preserving single-photon emission characteristics. These results pave the way for deterministic control over the charge state, and room-temperature decay-rate engineering for colloidal quantum dot-based classical and quantum communication technologies. ","Electrical control of single-photon emission in highly-charged
  individual colloidal quantum dots"
56,1239848498033385473,38700246,Sascha Trippe,"['New paper led by my student Dae-Won Kim: From observations of quasar 3C273 by @NASAFermi, @almaobs, and @TheNRAO VLBA, we found 2 γ-ray flares in 2016 and 2017 and were able to pin down the emission region in the parsec-scale jet. For more, please see <LINK> <LINK>']",https://arxiv.org/abs/2003.05659,"Due to its powerful radiation over the entire electromagnetic spectrum and its radio jet activity, the blazar 3C273 offers the opportunity to study the physics of $\gamma$-ray emission from active galactic nuclei. Since a historically strong outburst in 2009, 3C273 showed relatively weak emission in the gamma-ray band over multiple years. However, recent Fermi-Large Area Telescope observations indicate elevated activity during 2015-2019. We aim at constraining the origin of the gamma-ray outbursts towards 3C273 and investigate their connection to the parsec-scale jet. We generate Fermi-LAT gamma-ray light curves with multiple binning intervals and study the spectral properties of the gamma-ray emission. Using a 3-mm ALMA light curve, we study the correlation between radio and gamma-ray emission. Relevant activity in the parsec-scale jet of 3C273 is investigated with 7-mm VLBA observations obtained close in time to notable gamma-ray outbursts. We find two prominent gamma-ray outbursts in 2016 (MJD 57382) and 2017 (MJD 57883) accompanied by mm-wavelength flaring activity. The gamma-ray photon index time series show a weak hump-like feature around the gamma-ray outbursts. The monthly gamma-ray flux-index plot indicates a transition from softer-when-brighter to harder-when-brighter at $1.03\times10^{-7}\rm\,ph\,cm^{-2}\,s^{-1}$. A significant correlation between the gamma-ray and mm-wavelength emission is found, with the radio lagging the gamma-rays by about 105-112 days. The 43-GHz jet images reveal the known stationary features (i.e., the core, S1, and S2) in a region upstream of the jet. We find indication for a propagating disturbance and a polarized knot between the stationary components around the times of both gamma-ray outbursts. ","Investigating the connection between gamma-ray activity and relativistic
  jet in 3C273 during 2015-2019"
57,1239801951061229569,10666172,Sabine Hossenfelder,"[""New paper!\n\nThe Milky Way's rotation curve with superfluid dark matter\n\n<LINK>""]",https://arxiv.org/abs/2003.07324,"Recent studies have shown that dark matter with a superfluid phase in which phonons mediate a long-distance force gives rise to the phenomenologically well-established regularities of Modified Newtonian Dynamics (MOND). Superfluid dark matter, therefore, has emerged as a promising explanation for astrophysical observations by combining the benefits of both particle dark matter and MOND, or its relativistic completions, respectively. We here investigate whether superfluid dark matter can reproduce the observed Milky Way rotation curve for $ R < 25\,\rm{kpc}$ and are able to answer this question in the affirmative. Our analysis demonstrates that superfluid dark matter fits the data well with parameters in reasonable ranges. The most notable difference between superfluid dark matter and MOND is that superfluid dark matter requires about $ 20\% $ less total baryonic mass (with a suitable interpolation function). The total baryonic mass is then $5.96 \cdot 10^{10}\,M_\odot$, of which $1.03\cdot10^{10}\,M_\odot$ are from the bulge, $3.95\cdot10^{10}\,M_\odot$ are from the stellar disk, and $0.98\cdot10^{10}\,M_\odot$ are from the gas disk. Our analysis further allows us to estimate the radius of the Milky Way's superfluid core (concretely, the so-called NFW and thermal radii) and the total mass of dark matter in both the superfluid and the normal phase. By varying the boundary conditions of the superfluid to give virial masses $M_{200}^{\rm{DM}}$ in the range $0.5-3.0\cdot10^{12}\,M_\odot$, we find that the NFW radius $R_{\rm{NFW}}$ varies between $65\,\rm{kpc}$ and $73\,\rm{kpc}$, while the thermal radius $R_T$ varies between about $67\,\rm{kpc}$ and $105\,\rm{kpc}$. This is the first such treatment of a non-spherically-symmetric system in superfluid dark matter. ",The Milky Way's rotation curve with superfluid dark matter
58,1239547436948996101,769142140765167616,Siamak F. Shahandashti,"[""In our work on the #security of 5 top password managers, @MikeyJonCarr and I found\n- many reported vulnerabilities persisting \n- one new attack allowing a malicious app to steal another app's saved password\n- 3 other issues \nPaper at @IFIP_SEC_2020\n<LINK> <LINK>"", 'All vulnerabilities already reported to @dashlane @lastpass @keepersecurity @1Password and @roboform']",https://arxiv.org/abs/2003.01985,"In this work we analyse five popular commercial password managers for security vulnerabilities. Our analysis is twofold. First, we compile a list of previously disclosed vulnerabilities through a comprehensive review of the academic and non-academic sources and test each password manager against all the previously disclosed vulnerabilities. We find a mixed picture of fixed and persisting vulnerabilities. Then we carry out systematic functionality tests on the considered password managers and find four new vulnerabilities. Notably, one of the new vulnerabilities we identified allows a malicious app to impersonate a legitimate app to two out of five widely-used password managers we tested and as a result steal the user's password for the targeted service. We implement a proof-of-concept attack to show the feasibility of this vulnerability in a real-life scenario. Finally, we report and reflect on our experience of responsible disclosure of the newly discovered vulnerabilities to the corresponding password manager vendors. ",Revisiting Security Vulnerabilities in Commercial Password Managers
59,1239497859147456518,1066288106,Fabian Dablander,"['New paper! We derive default Bayes factors for testing hypotheses on independent population variances: <LINK>\n\n@donvdbergh @AlexanderLyNL @EJWagenmakers (1/6) <LINK>', 'We choose a prior so that the resulting Bayes factor fulfills a number of desiderata. This leads to a closed-form expression for the K = 1 and K = 2 group case. (2/6) https://t.co/5wpxZ167k9', 'It is arguably more intuitive to reason about standard deviations than about variances, and we provide practical examples from psychology and paleoanthropology (cool, right?). One may also elicit an informative prior rather than rely on defaults. https://t.co/Ml5PwJAPyq', ""We extend out Bayes factor to K &gt; 2 groups and allow testing 'mixed' or 'informative' hypothesis (see left image). We provide practical examples from archeology (chupa pots!) and education. https://t.co/ritexKthmG"", 'Our default Bayes factor generalizes a recently proposed automatic fractional Bayes factor (Böing-Messing &amp; Mulder, 2018), yielding the exact same results when testing hypotheses about (in)equality when choosing a rather wide prior \\alpha = 0.50. https://t.co/b0PcNKYhxz', ""All of this is implemented in the R package 'bfvartest', which is available from Github (https://t.co/jHuVZdQPaE). Give it a go! Of course, this will also be implemented in @JASPStats for ease of use :-) https://t.co/8pk63VkbkW""]",https://arxiv.org/abs/2003.06278,"Testing the (in)equality of variances is an important problem in many statistical applications. We develop default Bayes factor tests to assess the (in)equality of two or more population variances, as well as a test for whether the population variance equals a specific value. The resulting test can be used to check assumptions for commonly used procedures such as the $t$-test or ANOVA, or test substantive hypotheses concerning variances directly. We further extend the Bayes factor to allow $\mathcal{H}_0$ to have a null-region. Researchers may have directed hypotheses such as $\sigma_1^2 > \sigma_2^2$, or want to combine hypotheses about equality with hypotheses about inequality, for example $\sigma_1^2 = \sigma_2^2 > (\sigma_3^2, \sigma_4^2)$. We generalize our Bayes factor to accommodate such hypotheses for $K > 2$ groups. We show that our Bayes factor fulfills a number of desiderata, provide practical examples illustrating the method, and compare it to a recently proposed fractional Bayes factor procedure by B\""oing-Messing & Mulder (2018). Our procedure is implemented in the R package $bfvartest$. ","Default Bayes Factors for Testing the (In)equality of Several Population
  Variances"
60,1239488080106700800,214488634,Florian Kräutli,['New paper from our collaboration with #BZML. How to detect similarities in historical tables? – Building and Interpreting Deep Similarity Models #Sphaera @mpiwg <LINK> <LINK>'],https://arxiv.org/abs/2003.05431,"Many learning algorithms such as kernel machines, nearest neighbors, clustering, or anomaly detection, are based on the concept of 'distance' or 'similarity'. Before similarities are used for training an actual machine learning model, we would like to verify that they are bound to meaningful patterns in the data. In this paper, we propose to make similarities interpretable by augmenting them with an explanation in terms of input features. We develop BiLRP, a scalable and theoretically founded method to systematically decompose similarity scores on pairs of input features. Our method can be expressed as a composition of LRP explanations, which were shown in previous works to scale to highly nonlinear functions. Through an extensive set of experiments, we demonstrate that BiLRP robustly explains complex similarity models, e.g. built on VGG-16 deep neural network features. Additionally, we apply our method to an open problem in digital humanities: detailed assessment of similarity between historical documents such as astronomical tables. Here again, BiLRP provides insight and brings verifiability into a highly engineered and problem-specific similarity model. ",Building and Interpreting Deep Similarity Models
61,1239352070802718720,1191056593476915200,Ray Bai,"['New preprint for a paper I co-authored! ""VC-BART: Bayesian trees meet varying coefficients.""  We introduce a new approach for varying coefficient models with Bayesian additive regression tree priors. <LINK> @skdeshpande91', 'We aim to build flexible but interpretable regression models, where the additive effect of each covariate on the outcome *varies* as a function of effect modifiers (e.g. the predictors could be functions of time and space). BART is ideal for this purpose.']",https://arxiv.org/abs/2003.06416,"Many studies have reported associations between later-life cognition and socioeconomic position in childhood, young adulthood, and mid-life. However, the vast majority of these studies are unable to quantify how these associations vary over time and with respect to several demographic factors. Varying coefficient (VC) models, which treat the covariate effects in a linear model as nonparametric functions of additional effect modifiers, offer an appealing way to overcome these limitations. Unfortunately, state-of-the-art VC modeling methods require computationally prohibitive parameter tuning or make restrictive assumptions about the functional form of the covariate effects. In response, we propose VCBART, which estimates the covariate effects in a VC model using Bayesian Additive Regression Trees. With simple default hyperparameter settings, VCBART outperforms existing methods in terms of covariate effect estimation and prediction. Using VCBART, we predict the cognitive trajectories of 4,167 subjects from the Health and Retirement Study using multiple measures of socioeconomic position and physical health. We find that socioeconomic position in childhood and young adulthood have small effects that do not vary with age. In contrast, the effects of measures of mid-life physical health tend to vary with respect to age, race, and marital status. An R package implementing VCBART is available at this https URL ",VCBART: Bayesian trees for varying coefficients
62,1238442953284235270,1068545181576773632,Kenneth Brown,"['Natalie Brown (@GTPhys), Andrew Cross (#IBMQ), and I  posted a new paper on leakage errors in the surface code to the arXiv today. <LINK>  @DukeEngineering  #DukeQuantum', 'Natalie recently defended her thesis on leakage errors. First, she compared whether it was better to have a magnetic field sensitive qubit or a leaky qubit using the standard depolarizing error model for the leaky qubit interaction. https://t.co/4pbNQzJeHN', 'Then Natalie, Mike Newman ,and I realized the physical model of how leaked states and qubit states interact for trapped ion gates is nicer than the standard depolarizing error model. https://t.co/8GtMy7jM05 https://t.co/Dc88XKpG6c', 'What about the leakage interaction models for other physical systems?  Natalie was visiting Andrew at IBM as part of the @NSF QISE-Net triplet program. @jaygambetta mentioned to Natalie that for cross-resonance two-qubit gates, one qubit is more likely to leak than the other.', 'From her work on ions, Natalie knew that leakage on syndrome qubits was more damaging than leakage on data qubits because of how error spread.  Taking advantage of the asymmetry of leakage after a cross-resonance gate, she could design improved gate compilations for the code.', 'Again we see that the physical errors of the system inform the best choice for circuit compilation.', 'For those allergic to device specific error models, Natalie went one step further and designed a new leakage reduction circuit that works for the standard leakage error model and saves gates by fixing leakage on syndrome and data qubits separately.']",https://arxiv.org/abs/2003.05843,"Leakage is a particularly damaging error that occurs when a qubit leaves the defined computational subspace. Leakage errors limit the effectiveness of quantum error correcting codes by spreading additional errors to other qubits and corrupting syndrome measurements. The effects of leakage errors on the surface code has been studied in various contexts. However, the effects of a leaked data qubit versus a leaked ancilla qubit can be quite different. Here, we study the effects of data leakage and ancilla leakage separately. We show that data leakage is much less damaging. We show that the surface code maintains its distance in the presence of leakage by either confining leakage to data qubits or eliminating aniclla qubit leakage at the critical fault location. We also introduce new techniques for handling leakage by using gates with one-sided leakage and by mixing two types of leakage reducing circuits: one to handle data leakage and one to handle ancilla leakage. ",Critical faults of leakage errors on the surface code
63,1238392964226965504,3021399517,Jean-Baptiste Mouret,"['New paper:  We combine embeddings and meta-learning to adapt dynamical models of robots learned initially in simulation. This allows robots to adapt to novel situations in a few minutes and cross the reality gap (@sim2realAIorg). <LINK> [with @RR_Kaushik ] <LINK>', 'Full video: https://t.co/1rezhLjCGv', '@jrieffel I am late on the announcements... I am trying to catch up.']",https://arxiv.org/abs/2003.04663,"Meta-learning algorithms can accelerate the model-based reinforcement learning (MBRL) algorithms by finding an initial set of parameters for the dynamical model such that the model can be trained to match the actual dynamics of the system with only a few data-points. However, in the real world, a robot might encounter any situation starting from motor failures to finding itself in a rocky terrain where the dynamics of the robot can be significantly different from one another. In this paper, first, we show that when meta-training situations (the prior situations) have such diverse dynamics, using a single set of meta-trained parameters as a starting point still requires a large number of observations from the real system to learn a useful model of the dynamics. Second, we propose an algorithm called FAMLE that mitigates this limitation by meta-training several initial starting points (i.e., initial parameters) for training the model and allows the robot to select the most suitable starting point to adapt the model to the current situation with only a few gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model with MAML, and model-free policy search algorithm PPO for various simulated and real robotic tasks, and show that FAMLE allows the robots to adapt to novel damages in significantly fewer time-steps than the baselines. ","Fast Online Adaptation in Robotics through Meta-Learning Embeddings of
  Simulated Priors"
64,1238390910846742528,3021399517,Jean-Baptiste Mouret,"['New paper: we designed an algorithm that allows miniature underground UAVs to establish communication relays in tunnels. Paper: <LINK> We use the nice Crazyflies by @Bitcraze_se. <LINK>', '@Bitcraze_se Thanks. If you are interested, we could write a blog post about this (in particular, we have actual p2p communication and everyting runs on the crazyflies)']",https://arxiv.org/abs/2003.04409,"Miniature multi-rotors are promising robots for navigating subterranean networks, but maintaining a radio connection underground is challenging. In this paper, we introduce a distributed algorithm, called U-Chain (for Underground-chain), that coordinates a chain of flying robots between an exploration drone and an operator. Our algorithm only uses the measurement of the signal quality between two successive robots as well as an estimate of the ground speed based on an optic flow sensor. We evaluate our approach formally and in simulation, and we describe experimental results with a chain of 3 real miniature quadrotors (12 by 12 cm) and a base station. ","Signal-based self-organization of a chain of UAVs for subterranean
  exploration"
65,1238361803186470912,617492880,Benoit Famaey,"['New paper with Tariq Hilmi, @iminchev1 , @honestjago , @cfl2126 , @ReadDark , @GalacticRAVE et al. today: <LINK>', 'Bottom line: spiral-bar interactions can make galactic bars ‘pulsate‘ on relatively short timescales (60-200 Myr periods in MW-like galaxies). Combination of bias (when the bar and a spiral arm are connected, the bar appears longer) and true pulsations related to mode coupling.']",https://arxiv.org/abs/2003.05457,"We study the late-time evolution of the central regions of two Milky Way-like simulations of galaxies formed in a cosmological context, one hosting a fast bar and the other a slow one. We find that bar length, R_b, measurements fluctuate on a dynamical timescale by up to 100%, depending on the spiral structure strength and measurement threshold. The bar amplitude oscillates by about 15%, correlating with R_b. The Tremaine-Weinberg-method estimates of the bars' instantaneous pattern speeds show variations around the mean of up to ~20%, typically anti-correlating with the bar length and strength. Through power spectrum analyses, we establish that these bar pulsations, with a period in the range ~60-200 Myr, result from its interaction with multiple spiral modes, which are coupled with the bar. Because of the presence of odd spiral modes, the two bar halves typically do not connect at exactly the same time to a spiral arm, and their individual lengths can be significantly offset. We estimated that in about 50% of bar measurements in Milky Way-mass external galaxies, the bar lengths of SBab type galaxies are overestimated by ~15% and those of SBbc types by ~55%. Consequently, bars longer than their corotation radius reported in the literature, dubbed ""ultra-fast bars"", may simply correspond to the largest biases. Given that the Scutum-Centaurus arm is likely connected to the near half of the Milky Way bar, recent direct measurements may be overestimating its length by 1-1.5 kpc, while its present pattern speed may be 5-10 km/s/kpc smaller than its time-averaged value. ",Fluctuations in galactic bar parameters due to bar-spiral interaction
66,1238114557815664640,740997024,Mike Byrne,"[""I know not the highest priority right now, but we have a new paper that’s highly relevant to some current issues in voting technology:\n\nVoter Verification of BMD Ballots Is a Two-Part Question: Can They? Mostly, They Can. Do They? Mostly, They Don't\n\n<LINK>""]",https://arxiv.org/abs/2003.04997,"The question of whether or not voters actually verify ballots produced by ballot marking devices (BMDs) is presently the subject of some controversy. Recent studies (e.g., Bernhard, et al. 2020) suggest the verification rate is low. What is not clear from previous research is whether this is more a result of voters being unable to do so accurately or whether this is because voters simply choose not to attempt verification in the first place. In order to understand this problem, we conducted an experiment in which 108 participants participated in a mock election where the BMD displayed the voters' true choices, but then changed a subset of those choices on the printed ballot. The design of the printed ballot, the length of the ballot, the number of changes that were made to the ballot, the location of those changes, and the instructions provided to the voters were manipulated as part of the experiment. Results indicated that of those voters who chose to examine the printed ballot, 76% detected anomalies, indicating that voters can reliably detect errors on their ballot if they will simply review it. This suggests that administrative remedies, rather than attempts to alter fundamental human perceptual capabilities, could be employed to encourage voters to check their ballots, which could prove as an effective countermeasure. ","Voter Verification of BMD Ballots Is a Two-Part Question: Can They?
  Mostly, They Can. Do They? Mostly, They Don't"
67,1238016759988355072,1151085970432937984,Antonio Manesco,"['Check our new paper (<LINK>)!\n\nYou can also access our code and data @ZENODO_ORG (<LINK>).\n\nWork with @JLado_Phys @AaltoUniversity; @rib_eduardo_, @gbrlwbr and Durval Rodrigues Jr. (and me @QuantumTinkerer @tudelft). <LINK>', 'I missed @usponline btw!']",http://arxiv.org/abs/2003.05163,"Electronic correlations stemming from nearly flat bands in van der Waals materials have demonstrated to be a powerful playground to engineer artificial quantum matter, including superconductors, correlated insulators and topological matter. This phenomenology has been experimentally observed in a variety of twisted van der Waals materials, such as graphene and dichalcogenide multilayers. Here we show that spontaneously buckled graphene can yield a correlated state, emerging from an elastic pseudo Landau level. Our results build on top of recent experimental findings reporting that, when placed on top of hBN or NbSe$_2$ substrates, wrinkled graphene sheets relax forming a periodic, long-range buckling pattern. The low-energy physics can be accurately described by electrons in the presence of a pseudo-axial gauge field, leading to the formation of sublattice-polarized Landau levels. Moreover, we verify that the high density of states at the zeroth Landau level leads to the formation of a periodically modulated ferrimagnetic groundstate, which can be controlled by the application of external electric fields. Our results indicate that periodically strained graphene is a versatile platform to explore emergent electronic states arising from correlated elastic Landau levels. ","Correlations in the elastic Landau level of spontaneously buckled
  graphene"
68,1237896875061067776,373525906,Weijie Su,"['How to accurately track the overall #privacy cost under composition of many private algorithms? Our new paper (<LINK>) offers a new approach using the Edgeworth expansion in the f-#differentialprivacy framework. w/ Qinqing Zheng, @JinshuoD, and Qi Long.', 'This improves on our earlier central limit theorem-based approach to handling composition of differentially private algorithms. The secret sauce is *Edgeworth expansion*. In short, Edgeworth expansion is to central limit theorem as linear expansion is to Taylor expansion! 2/2']",https://arxiv.org/abs/2003.04493,"Datasets containing sensitive information are often sequentially analyzed by many algorithms. This raises a fundamental question in differential privacy regarding how the overall privacy bound degrades under composition. To address this question, we introduce a family of analytical and sharp privacy bounds under composition using the Edgeworth expansion in the framework of the recently proposed f-differential privacy. In contrast to the existing composition theorems using the central limit theorem, our new privacy bounds under composition gain improved tightness by leveraging the refined approximation accuracy of the Edgeworth expansion. Our approach is easy to implement and computationally efficient for any number of compositions. The superiority of these new bounds is confirmed by an asymptotic error analysis and an application to quantifying the overall privacy guarantees of noisy stochastic gradient descent used in training private deep neural networks. ","Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth
  Expansion"
69,1237691838141739008,1194301384872615939,Trevor Searight,"['The AM version of my paper ""Completing the dark matter solutions in degenerate Kaluza-Klein theory"" is now available on arXiv, predicting two new particles: a massive photon and a massive graviton: <LINK>']",https://arxiv.org/abs/2003.04725,"A complete set of wave solutions is given for the weak field in a Kaluza-Klein theory with degenerate metric. In the five-dimensional version of this theory electromagnetism is described by two vector fields, and there is a reflection symmetry between them which unifies them with gravitation; wave behaviour in the extra dimension has been interpreted as dark matter. Here three independent dark matter solutions are found, and for two of them it is shown how they must be combined into a single solution in order to obey the reflection symmetry. The unification is also expanded to six dimensions to prepare the way to include further forces. ",Completing the dark matter solutions in degenerate Kaluza-Klein theory
70,1237691225437868032,1032283657015316480,Josh Dorrington,"['Very happy to say my new paper with Kristian Strommen on circulation regimes is now submitted to GRL and up on the arxiv: <LINK>\n\nWe find removing the influence of jet speed on geopotential makes regimes far easier to find and more robust!', ""@AndrewIWilliams Aha it's from an admittedly rather old book... ;D \n\nhttps://t.co/8UYPIpJnlg""]",https://arxiv.org/abs/2003.04871,"Euro-Atlantic regimes are typically identified using either the latitude of the eddy-driven jet, or clustering algorithms in the phase space of 500hPa geopotential height (Z500). However, while robust trimodality is visibly apparent in jet latitude indices, Z500 clusters require highly sensitive significance tests to distinguish them from autocorrelated noise. As a result, even small shifts in the time-period considered can notably alter the diagnosed regimes. Fixing the optimal regime number is also hard to justify. We argue that the jet speed, a near-Gaussian distribution projecting strongly onto the Z500 field, is the source of this lack of robustness. Once its influence is removed, the Z500 phase space becomes visibly non-Gaussian, and clustering algorithms easily recover three extremely stable regimes, corresponding to the jet latitude regimes. Further analysis supports the existence of two additional regimes, corresponding to a tilted and split jet. This framework therefore naturally unifies the two regime perspectives. ","Through a Jet Speed Darkly: The Emergence of Robust Euro-Atlantic
  Regimes in the Absence of Jet Speed Variability"
71,1237676184244293632,1051234106,Corentin Cadiou,"['Today my new paper showed up on the arXiv (<LINK>). So... what is it all about? [spoiler: this may be quite technical]\n\nIn theoretical cosmology, one open question is how to predict, form the initial conditions of the Universe, the fate of the large-scale... 1/n', 'structure of the Universe (galaxies, dark matter halos but also cosmic filaments, walls and voids). In particular, we want to know how many of each structure are found at a given time, but also how they evolve (via accretion or mergers). 2/n', 'We already know that halos are preferentially found around peaks in the initial density field (peak theory), so we roughly know how to count them at fixed mass and time... 3/n', 'and if we study how the count changes with mass and time, we know how much mass halos acquire... but not how! Was it via accretion or mergers?\n\nIn our paper, we propose an extension of the peak theory to find ""critical events"", which aims at finding all merger events 4/n', 'in the initial conditions (including halo mergers but also filament and wall mergers)! For a 1D field, we would have maxima (peaks, below in red) or minima (in blue) which we can track as a function of smoothing (a.k.a. mass). Eventually, an extremum will disappear... 5/n https://t.co/nTPzOW60Rb', 'by merging into its neighbor... we just found our first merger in the initial conditions! You can generalize this to 2+1D (2D + smoothing dimension), where you now have peaks (red), saddle points (green) and minima (blue) or 3D where you have filament- and wall-type saddles. 6/n https://t.co/hvBMo2YfZO', 'So we now know how to count critical points (we ~know how many halos, filament and walls exist) and when they merge, all of this from the initial conditions, 14Gyr ago!\n\nAnd we can do many cool things with this new theory (the critical event theory). 7/n', 'We can compute the correlation function (= excess probability) of having a filament merger near a peak merger (solid red line), as a function of distance and compare to the peak-peak merger (dashed red line).\nThe result is that peak mergers are surrounded preferentially by .. 8/n https://t.co/RNGaB2CpJz', 'filament mergers, i.e. whenever two halos merge, you can expect one of their surrounding filament to merger shortly after.\n\nWe can also compute the number of objects destroyed in a merger per unit time per unit mass (destruction rate). Here we see that ... 9/n https://t.co/6PDb8jiphT', 'halo destructions  (red curves) happen all the way up to 10¹⁵Msun at z=0 (today), i.e. clusters may still be destroyed in a merger event! We also have predictions for void destruction rates (blue curves).\nBut we can do more! We have applied it to the assembly bias problem...10/n', 'i.e. how the secondary properties of halos and galaxies assemble are influenced by their environment (figure below, see paper for details 😁), study connectivity (how many filaments are connected to a given halo) and laid the first stones to use it ... 11/n https://t.co/1JgXTf6gbt', 'as a cosmological probe at high redshift (by extending our theory to take into account the first steps of gravitational collapse).\n\nThanks to all my collaborators C.Pichon, @CodisSandrine, M.Musso, D.Pogosyan, @DuboisYohan3, S.Prunet, JF.Cardoso \n12/12', 'Bonus for those still there, the 2+1D figure has an interactive version here https://t.co/0K0M4lVciS']",https://arxiv.org/abs/2003.04413,"The merging rate of cosmic structures is computed, relying on the Ansatz that they can be predicted in the initial linear density field from the coalescence of critical points with increasing smoothing scale, used here as a proxy for cosmic time. Beyond the mergers of peaks with saddle points (a proxy for halo mergers), we consider the coalescence and nucleation of all sets of critical points, including wall-saddle to filament-saddle and wall-saddle to minima (a proxy for filament and void mergers respectively), as they impact the geometry of galactic infall, and in particular filament disconnection. Analytical predictions of the one-point statistics are validated against multiscale measurements in 2D and 3D realisations of Gaussian random fields (the corresponding code being available upon request) and compared qualitatively to cosmological $N$-body simulations at early times ($z\geq 10$) and large scales ($\geq 5\, \mathrm{Mpc}/h$). The rate of filament coalescence is compared to the merger rate of haloes and the two-point clustering of these events is computed, along with their cross-correlations with critical points. These correlations are qualitatively consistent with the preservation of the connectivity of dark matter haloes, and the impact of the large scale structures on assembly bias. The destruction rate of haloes and voids as a function of mass and redshift is quantified down to $z=0$ for a $\Lambda$CDM cosmology. The one-point statistics in higher dimensions are also presented, together with consistency relations between critical point and critical event counts. ","When do cosmic peaks, filaments or walls merge? A theory of critical
  events in a multi-scale landscape"
72,1237565440970248194,1077995761487568896,Jon Miller,"['New paper day!  X-ray spectral-timing of the inner regions around black holes continues to surprise.  Abdu Zoghbi invented this kind of work and reinvents it regularly.  The lag spectrum in NGC 5506 needs more reflection than the time-averaged spectrum!  \n<LINK>', 'I am also very happy about this paper for the excellent contributions from Sihem Kalli, a student in Algeria.  Abdu works hard to create opportunities for students there.']",https://arxiv.org/abs/2003.04322,"The lamp-post geometry is often used to model X-ray data of accreting black holes. Despite its simple assumptions, it has proven to be powerful in inferring fundamental black hole properties such as the spin. Early results of X-ray reverberations showed support for such a simple picture, though wind-reverberation models have also been shown to explain the observed delays. Here, we analyze new and old XMM-Newton observations of the variable Seyfert-1 galaxy NGC 5506 to test these models. The source shows an emission line feature around 6.7 keV that is delayed relative to harder and softer energy bands. The spectral feature can be modeled with either a weakly relativistic disk line or by scattering in distant material. By modeling both the spectral and timing signatures, we find that the reflection fraction needed to explain the lags is larger than observed in the time-averaged spectrum, ruling out both a static lamp-post and simple wind reverberation models. ","Testing The Lamp-Post and Wind Reverberation Models with XMM-Newton
  Observations of NGC 5506"
73,1237561405001224194,185312331,Ryan Shaffer,"['and now for some shameless self-promotion: we just posted a new arXiv paper! ""Benchmarking protocols for analog quantum simulators""... it\'s a thrilling read. thanks to my @Berkeley_ions co-authors! <LINK>']",https://arxiv.org/abs/2003.04500,"Analog quantum simulation is expected to be a significant application of near-term quantum devices. Verification of these devices without comparison to known simulation results will be an important task as the system size grows beyond the regime that can be simulated classically. We introduce a set of experimentally-motivated verification protocols for analog quantum simulators, discussing their sensitivity to a variety of error sources and their scalability to larger system sizes. We demonstrate these protocols experimentally using a two-qubit trapped-ion analog quantum simulator and numerically using models of up to five qubits. ",Practical verification protocols for analog quantum simulators
74,1237543047430156289,1658162341,Narayanan Rengaswamy,"['New paper out! See <LINK> for a detailed analysis of a Belief-Propagation with *Quantum* Messages (BPQM) algorithm that Joseph Renes from ETH came up with a few years back. We show that it suggests a potentially new application for a photonic quantum computer.', 'The algorithm is designed for decoding classical binary codes over the pure-state classical-quantum channel. This channel manifests in the binary phase shift keying (BPSK) modulated pure-loss bosonic channel in deep-space laser communications.', 'We analyze the algorithm thoroughly for a simple 5 bit code in order to understand the workings of the algorithm, and even suggest one prominent tweak after measuring bit 1. We also provide detailed circuit decomposition in terms of standard gates.', 'Most importantly, we show that the block error rate of BPQM on this code matches the quantum optimal Yuen-Kennedy-Lax (YKL) limit in deciding which codeword was transmitted. We show this via analysis and also verify by Monte-Carlo simulations.', 'Current receivers in optical communications *do not* attain this limit because they measure each output qubit and run a classical decoder on the bit string produced by the measurements. So this may be an exciting new application for a special-purpose photonic quantum computer!', 'If you have any comments, please let me know! @kenbrownquantum @QuantumChambs @dabacon @ShrutiPuri11 @preskill @S_Flammia @markwilde @kunal_phy @CVuillot @earltcampbell @JoshKoomz @krishnanirudh @DriptoDebroy @JarrodMcclean', 'This was a fun collaboration with one of my advisors Henry Pfister, and Saikat Guha and Kaushik Seshadreesan from University of Arizona! I started looking at the algorithm back in mid 2017 and worked on it on and off. It feels great to have finally put it out!']",https://arxiv.org/abs/2003.04356,"For space-based laser communications, when the mean photon number per received optical pulse is much smaller than one, there is a large gap between communications capacity achievable with a receiver that performs individual pulse-by-pulse detection, and the quantum-optimal ""joint-detection receiver"" that acts collectively on long codeword-blocks of modulated pulses; an effect often termed ""superadditive capacity"". In this paper, we consider the simplest scenario where a large superadditive capacity is known: a pure-loss channel with a coherent-state binary phase-shift keyed (BPSK) modulation. The two BPSK states can be mapped conceptually to two non-orthogonal states of a qubit, described by an inner product that is a function of the mean photon number per pulse. Using this map, we derive an explicit construction of the quantum circuit of a joint-detection receiver based on a recent idea of ""belief-propagation with quantum messages"" (BPQM) (arXiv:1607.04833). We quantify its performance improvement over the Dolinar receiver that performs optimal pulse-by-pulse detection, which represents the best ""classical"" approach. We analyze the scheme rigorously and show that it achieves the quantum limit of minimum average error probability in discriminating 8 (BPSK) codewords of a length-5 binary linear code with a tree factor graph. Our result suggests that a BPQM-receiver might attain the Holevo capacity of this BPSK-modulated pure-loss channel. Moreover, our receiver circuit provides an alternative proposal for a quantum supremacy experiment, targeted at a specific application that can potentially be implemented on a small, special-purpose, photonic quantum computer capable of performing cat-basis universal qubit logic. ","Belief Propagation with Quantum Messages for Quantum-Enhanced Classical
  Communications"
75,1237387884929388544,3433220662,Anthony Bonato,"[""Our new paper on @arxiv on Cops and Eternal Robbers, generalizing eternal domination. This was fun work that began with a discussion with Richard Nowakowski at GRASCan'19 at the @FieldsInstitute \n \n<LINK> <LINK>""]",https://arxiv.org/abs/2003.03791,"We introduce the game of Cops and Eternal Robbers played on graphs, where there are infinitely many robbers that appear sequentially over distinct plays of the game. A positive integer $t$ is fixed, and the cops are required to capture the robber in at most $t$ time-steps in each play. The associated optimization parameter is the eternal cop number, denoted by $c_t^{\infty},$ which equals the eternal domination number in the case $t=1,$ and the cop number for sufficiently large $t.$ We study the complexity of Cops and Eternal Robbers, and show that game is NP-hard when $t$ is a fixed constant and EXPTIME-complete for large values of $t$. We determine precise values of $c_t^{\infty}$ for paths and cycles. The eternal cop number is studied for retracts, and this approach is applied to give bounds for trees, as well as for strong and Cartesian grids. ",The Game of Cops and Eternal Robbers
76,1237305769109999616,1060176917813477376,Nikhil Iyer,"['New paper out !!!\n\nWide-minima Density Hypothesis &amp; the Explore-Exploit Learning Rate Schedule\n<LINK>\n\n1/n', 'We show how our hypothesis on the density of wide vs. narrow minima affects generalization and devise a 2 step learning rate schedule (Explore - Exploit) based on our hypothesis.\n\n2/n', 'We show performance gains on all experiments over the full training budget. Motivated by our hypothesis, we also experiment with reduced training budget and match the baseline numbers (44% reduction in time for ImageNet training with ResNet50).\n\n3/n', 'We outperform multiple hand-tuned LR schedules across full training budget and short training budget and show that our method is insensitive to the seed learning rate.\n\n4/n', 'Joint work with Thejas Venkatesh, @kwatra , @ramaramjee and Dr. Muthian Sivathanu at @MSFTResearch-India']",http://arxiv.org/abs/2003.03977,"Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) dataset by just modifying the learning rate schedule of a high performing model. ","Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate
  Schedule"
77,1237305554369986561,885067890755600384,Eric Savin,"['New paper: Multiscale analysis of spectral broadening of acoustic waves by a turbulent shear layer, with Josselin Garnier and Etienne Gay.\n<LINK>']",https://arxiv.org/abs/2003.04017,"We consider the scattering of acoustic waves emitted by an active source above a plane turbulent shear layer. The layer is modeled by a moving random medium with small spatial and temporal fluctuations of its mean velocity, and constant density and speed of sound. We develop a multi-scale perturbative analysis for the acoustic pressure field transmitted by the layer and derive its power spectral density when the correlation function of the velocity fluctuations is known. Our aim is to compare the proposed analytical model with some experimental results obtained for jet flows in open wind tunnels. We start with the Euler equations for an ideal fluid flow and linearize them about an ambient, unsteady inhomogeneous flow. We study the transmitted pressure field without fluctuations of the ambient flow velocity to obtain the Green's function of the unperturbed medium with constant characteristics. Then we use a Lippmann-Schwinger equation to derive an analytical expression of the transmitted pressure field, as a function of the velocity fluctuations within the layer. Its power spectral density is subsequently computed invoking a stationary-phase argument, assuming in addition that the source is time-harmonic and the layer is thin. We finally study the influence of the source tone frequency and ambient flow velocity on the power spectral density of the transmitted pressure field and compare our results with other analytical models and experimental data. ","Multiscale analysis of spectral broadening of acoustic waves by a
  turbulent shear layer"
78,1237302029166534656,561899047,Aki Vehtari,"['New paper ""When are Bayesian model probabilities overconfident?"" by Oscar Oelrich, Shutong Ding, @MansMeg, me, and @matvil <LINK> (I had just a minor role, others did awesome work on this) <LINK>', 'We demonstrate overconfidence in two high-profile applications in economics and neuroscience and derive the sampling variance in univariate and multivariate linear regression. https://t.co/jjRBaMcRxf', 'Overconfidence is likely when 1) the compared models give very different approximations of the data-gener. process, 2) the models are very flexible with large degrees of freedom that are not shared between the models, 3) the models underestimate the true variability in the data. https://t.co/jD7zRbluxK', ""In the end we mention possible alternatives for model comparison and combination, but I realize we should have mentioned also that instead of trying to compare or combine bad models it's better to think harder and construct better models.""]",https://arxiv.org/abs/2003.04026,"Bayesian model comparison is often based on the posterior distribution over the set of compared models. This distribution is often observed to concentrate on a single model even when other measures of model fit or forecasting ability indicate no strong preference. Furthermore, a moderate change in the data sample can easily shift the posterior model probabilities to concentrate on another model. We document overconfidence in two high-profile applications in economics and neuroscience. To shed more light on the sources of overconfidence we derive the sampling variance of the Bayes factor in univariate and multivariate linear regression. The results show that overconfidence is likely to happen when i) the compared models give very different approximations of the data-generating process, ii) the models are very flexible with large degrees of freedom that are not shared between the models, and iii) the models underestimate the true variability in the data. ",When are Bayesian model probabilities overconfident?
79,1237277579150753792,882303076505456642,Timon Emken,"['Nobody likes rejection. However, in #science rejecting a hypothesis is basically ""learning something about nature"". Our new #paper (💯% from @ChalmersPhysics) can be found on the @arxiv. We study the question ""Is #darkmatter its own anti-particle?"".\n<LINK>\n\n(1/7) <LINK>', 'We focus on a scenario where a sub-GeV DM particle has been discovered via direct detection through electron recoils mediated by higher-order photon interactions (anapole, magnetic dipole, and/or electric dipole moments). \n\n(2/7) https://t.co/JpGOK0fvs1', 'The interactions allowed by symmetry depend on whether the DM is a Dirac or Majorana fermion. \n\nFurthermore, the two hypotheses (Dirac vs Majorana) generally give rise to different predictions, in our case distinct energy spectra of ionization events, as seen here. \n\n(3/7) https://t.co/tiIq973aPw', 'Using #MonteCarlo simulations and the likelihood ratio test, we quantitatively determine the statistical significance Z (or p-value) to reject the Majorana hypothesis (our ""null"" hypothesis) for a number of benchmark scenarios as a function of observed signal events. \n\n(4/7) https://t.co/Rshhw1Ozd8', 'In the most favourable(for us at least) benchmark point denoted here as T3, the detection of only 45(120) DM-induced ionizations in a xenon target would suffice to reject the Majorana nature of DM in favour of Dirac fermions with 3(5) sigma. Other benchmarks require more. \n\n(5/7) https://t.co/pP2Gl6vs31', 'A successful rejection of the Majorana nature of sub-GeV DM would tell us that DM is not its own anti-particle. \n\nAs usual, the code used for the statistical analysis is publicly available on @github:  https://t.co/XNg7LMErd2 . #OpenScience (6/7) https://t.co/NGcBJL9QBB', 'Calculating ionization spectra for general, non-relativistic DM-electron interactions requires the knowledge of 4 atomic response functions, 3 of which we computed last year for the 1st time for isolated xenon and argon atoms: \n\nhttps://t.co/tbqOcbQ1go\n\n(7/7)', '@rittenhousewest @ChalmersPhysics @arxiv Thanks! 😃', '@saniaheba @ChalmersPhysics @arxiv Thank you 🙃', '@MagdaKersting @ChalmersPhysics @arxiv Thanks so much 😀']",https://arxiv.org/abs/2003.04039,"Assuming that Dark Matter (DM) is made of fermions in the sub-GeV mass range with interactions dominated by electromagnetic moments of higher order, such as the electric and magnetic dipoles or the anapole moment, we show that direct detection experiments searching for atomic ionisation events in xenon targets can shed light on whether DM is a Dirac or Majorana particle. Specifically, we find that between about 45 (120) and 610 (1700) signal events are required to reject Majorana DM in favour of Dirac DM with a statistical significance corresponding to 3 (5) standard deviations. The exact number of DM signal events corresponding to a given significance depends on the relative size of the anapole, magnetic dipole and electric dipole contributions to the expected rate of DM-induced atomic ionisations under the Dirac hypothesis. Our conclusions are based on Monte Carlo simulations and the likelihood ratio test. While the use of asymptotic formulae for the latter is standard in many applications, here it requires a non-trivial extension to the case where one of the hypotheses lies on the boundary of the parameter space. Our results constitute a solid proof of concept about the possibility of using direct detection experiments to reject the Majorana DM hypothesis when the DM interactions are dominated by higher-order electromagnetic moments. ","Rejecting the Majorana nature of dark matter with electron scattering
  experiments"
80,1237277384539279360,2862127121,ioana ciucă,"['Now that the figures look nicer, check out our new paper  @ <LINK>. We employ a simple ML approach on the brilliant ages of A. Miglio and the amazing group at Birmingham to get stellar age for 17,305 stars in the local neighbourhood.', 'We then compare our results with predictions from the superb, high-resolution cosmological simulations Auriga (amazing work done by Rob Grand @ MPA). We find that the thick disc forms only in the inner disc, and that the inner and outer disc follow a distinct formation pathway.', '@sadie_lb Thanks Sadie ❤️❤️ ❤️.']",https://arxiv.org/abs/2003.03316,"We develop a Bayesian Machine Learning framework called BINGO (Bayesian INference for Galactic archaeOlogy) centred around a Bayesian neural network. After being trained on the APOGEE and \emph{Kepler} asteroseismic age data, BINGO is used to obtain precise relative stellar age estimates with uncertainties for the APOGEE stars. We carefully construct a training set to minimise bias and apply BINGO to a stellar population that is similar to our training set. We then select the 17,305 stars with ages from BINGO and reliable kinematic properties obtained from \textit{Gaia} DR2. By combining the age and chemo-kinematical information, we dissect the Galactic disc stars into three components, namely, the thick disc (old, high-[$\alpha$/Fe], [$\alpha$/Fe] $\gtrsim$ 0.12), the thin disc (young, low-[$\alpha$/Fe]) and the Bridge, which is a region between the thick and thin discs. Our results indicate that the thick disc formed at an early epoch only in the inner region, and the inner disc smoothly transforms to the thin disc. We found that the outer disc follows a different chemical evolution pathway from the inner disc. The outer metal-poor stars only start forming after the compact thick disc phase has completed and the star-forming gas disc extended outwardly with metal-poor gas accretion. We found that in the Bridge region the range of [Fe/H] becomes wider with decreasing age, which suggests that the Bridge region corresponds to the transition phase from the smaller chemically well-mixed thick to a larger thin disc with a metallicity gradient. ","Unveiling the Distinct Formation Pathways of the Inner and Outer Discs
  of the Milky Way with Bayesian Machine Learning"
81,1237255803213578241,274444499,Richard Gill,"['My new paper ""Anna Karenina and the Two Envelopes"" is on arXiv. At the last the definitive solution to this famous paradox. <LINK> <LINK>']",https://arxiv.org/abs/2003.04008,"The Anna Karenina principle is named after the opening sentence in the eponymous novel: Happy families are all alike; every unhappy family is unhappy in its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in probability theory, mathematical economics, logic, and philosophy. Time and again a new analysis is published in which an author claims finally to explain what actually goes wrong in this paradox. Each author (the present author included) emphasizes what is new in their approach and concludes that earlier approaches did not get to the root of the matter. We observe that though a logical argument is only correct if every step is correct, an apparently logical argument which goes astray can be thought of as going astray at different places. This leads to a comparison between the literature on TEP and a successful movie franchise: it generates a succession of sequels, and even prequels, each with a different director who approaches the same basic premise in a personal way. We survey resolutions in the literature with a view to synthesis, correct common errors, and give a new theorem on order properties of an exchangeable pair of random variables, at the heart of most TEP variants and interpretations. A theorem on asymptotic independence between the amount in your envelope and the question whether it is smaller or larger shows that the pathological situation of improper priors or infinite expectation values has consequences as we merely approach such a situation. ",Anna Karenina and The Two Envelopes Problem
82,1237255581808852993,23303892,Richard Gill,"['My new paper ""Anna Karenina and the Two Envelopes"" is on arXiv. At the last the definitive solution to this famous paradox. <LINK> <LINK>']",https://arxiv.org/abs/2003.04008,"The Anna Karenina principle is named after the opening sentence in the eponymous novel: Happy families are all alike; every unhappy family is unhappy in its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in probability theory, mathematical economics, logic, and philosophy. Time and again a new analysis is published in which an author claims finally to explain what actually goes wrong in this paradox. Each author (the present author included) emphasizes what is new in their approach and concludes that earlier approaches did not get to the root of the matter. We observe that though a logical argument is only correct if every step is correct, an apparently logical argument which goes astray can be thought of as going astray at different places. This leads to a comparison between the literature on TEP and a successful movie franchise: it generates a succession of sequels, and even prequels, each with a different director who approaches the same basic premise in a personal way. We survey resolutions in the literature with a view to synthesis, correct common errors, and give a new theorem on order properties of an exchangeable pair of random variables, at the heart of most TEP variants and interpretations. A theorem on asymptotic independence between the amount in your envelope and the question whether it is smaller or larger shows that the pathological situation of improper priors or infinite expectation values has consequences as we merely approach such a situation. ",Anna Karenina and The Two Envelopes Problem
83,1237204046185811968,701797730750898176,Triet Le,"['Glad to share that our paper PUMiner: Mining #security posts from Q&amp;A websites using PU learning with @davidhin1, @CroftRoland and Prof. @alibabar @CREST_center  accepted in @msrconf. A new #ML way to retrieve #informationsecurity with no negative class. <LINK>']",http://arxiv.org/abs/2003.03741,"Security is an increasing concern in software development. Developer Question and Answer (Q&A) websites provide a large amount of security discussion. Existing studies have used human-defined rules to mine security discussions, but these works still miss many posts, which may lead to an incomplete analysis of the security practices reported on Q&A websites. Traditional supervised Machine Learning methods can automate the mining process; however, the required negative (non-security) class is too expensive to obtain. We propose a novel learning framework, PUMiner, to automatically mine security posts from Q&A websites. PUMiner builds a context-aware embedding model to extract features of the posts, and then develops a two-stage PU model to identify security content using the labelled Positive and Unlabelled posts. We evaluate PUMiner on more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange. We show that PUMiner is effective with the validation performance of at least 0.85 across all model configurations. Moreover, Matthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner also performs well with an MCC of 0.745 for scenarios where string matching totally fails. Even when the ratio of the labelled positive posts to the unlabelled ones is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security content on Q&A websites for practitioners and researchers. ","PUMiner: Mining Security Posts from Developer Question and Answer
  Websites with PU Learning"
84,1237179236210728961,779148981804806144,Lantao Yu,['Excited to share our new paper on how to train deep energy-based models effectively using any desired f-Divergence:\n<LINK>\njoint work with @YSongStanford @baaadas @StefanoErmon'],https://arxiv.org/abs/2003.03463,"Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL. ",Training Deep Energy-Based Models with f-Divergence Minimization
85,1237100449250992128,788397395608412160,Julia Mendelsohn,"['My paper with @jurafsky and Yulia Tsvetkov is on arXiv! We develop a computational linguistic framework for analyzing dehumanization, which we use to study changing representations of LGBTQ groups in the New York Times over 30 years: <LINK>']",https://arxiv.org/abs/2003.03014,"Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online. ",A Framework for the Computational Linguistic Analysis of Dehumanization
86,1237011878125211648,101810581,Animesh Garg,"[""CNNs are biased towards high-frequency textural information.\n\nNew work on fixing CNN's over-reliance on texture through a curriculum that exposes texture slowly. \n\nResults in better features that generalize both to new datasets and to new tasks. \n\nPaper: <LINK> <LINK>""]",https://arxiv.org/abs/2003.01367,"Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classification, detection, and segmentation. Moreover, recent work in Generative Adversarial Networks (GANs) has highlighted the importance of learning by progressively increasing the difficulty of a learning task [26]. When learning a network from scratch, the information propagated within the network during the earlier stages of training can contain distortion artifacts due to noise which can be detrimental to training. In this paper, we propose an elegant curriculum based scheme that smoothes the feature embedding of a CNN using anti-aliasing or low-pass filters. We propose to augment the train-ing of CNNs by controlling the amount of high frequency information propagated within the CNNs as training progresses, by convolving the output of a CNN feature map of each layer with a Gaussian kernel. By decreasing the variance of the Gaussian kernel, we gradually increase the amount of high-frequency information available within the network for inference. As the amount of information in the feature maps increases during training, the network is able to progressively learn better representations of the data. Our proposed augmented training scheme significantly improves the performance of CNNs on various vision tasks without either adding additional trainable parameters or an auxiliary regularization objective. The generality of our method is demonstrated through empirical performance gains in CNN architectures across four different tasks: transfer learning, cross-task transfer learning, and generative models. ",Curriculum By Smoothing
87,1236968390574694400,4890000754,Debora Nozza,"['Do you want to know why language-specific BERT models obtain better results wrt multilingual BERT? Read our new paper ""What the [MASK]? Making Sense of Language-Specific BERT Models"" and visit Bert Lang Street (<LINK>)\n#NLProc\n\n<LINK> <LINK> <LINK>']",https://arxiv.org/abs/2003.02912,"Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at this https URL ",What the [MASK]? Making Sense of Language-Specific BERT Models
88,1236924605476069377,1114209043114024967,Michael Schomaker,"['Economics, DAGs, and causal inference with #TMLE. Have a look at our new working paper where we estimate the effect of central bank independence on inflation: <LINK> Comments welcome!']",https://arxiv.org/abs/2003.02208,"The notion that an independent central bank reduces a country's inflation is a controversial hypothesis. To date, it has not been possible to satisfactorily answer this question because the complex macroeconomic structure that gives rise to the data has not been adequately incorporated into statistical analyses. We develop a causal model that summarizes the economic process of inflation. Based on this causal model and recent data, we discuss and identify the assumptions under which the effect of central bank independence on inflation can be identified and estimated. Given these and alternative assumptions, we estimate this effect using modern doubly robust effect estimators, i.e., longitudinal targeted maximum likelihood estimators. The estimation procedure incorporates machine learning algorithms and is tailored to address the challenges associated with complex longitudinal macroeconomic data. We do not find strong support for the hypothesis that having an independent central bank for a long period of time necessarily lowers inflation. Simulation studies evaluate the sensitivity of the proposed methods in complex settings when certain assumptions are violated and highlight the importance of working with appropriate learning algorithms for estimation. ","Estimating the Effect of Central Bank Independence on Inflation Using
  Longitudinal Targeted Maximum Likelihood Estimation"
89,1236896810645495809,1608190106,Jouni Helske,['New paper regarding Bayesian estimation of causal effects in the presence of a trapdoor variable: <LINK>. We consider practical issues with a certain type of causal formulas where frontdoor and backdoor adjustments are not applicable. #Causalinference #Statistics'],https://arxiv.org/abs/2003.03187,"We consider the problem of estimating causal effects of interventions from observational data when well-known back-door and front-door adjustments are not applicable. We show that when an identifiable causal effect is subject to an implicit functional constraint that is not deducible from conditional independence relations, the estimator of the causal effect can exhibit bias in small samples. This bias is related to variables that we call trapdoor variables. We use simulated data to study different strategies to account for trapdoor variables and suggest how the related trapdoor bias might be minimized. The importance of trapdoor variables in causal effect estimation is illustrated with real data from the Life Course 1971-2002 study. Using this dataset, we estimate the causal effect of education on income in the Finnish context. Bayesian modelling allows us to take the parameter uncertainty into account and to present the estimated causal effects as posterior distributions. ","Estimation of causal effects with small data in the presence of trapdoor
  variables"
90,1235895065895145472,1212040530592333826,Sunny Vagnozzi,"['Shout-out to Steffen Hagstotz (@TheOKC) for tirelessly leading our new paper on light #sterileneutrinos combining cosmology and lab searches, together w @PFdeSalas, @ktfreese, &amp; @spastorcarpi: <LINK>. Also love our very international affiliation list: 🇸🇪🇪🇸🇺🇸🇮🇹🇬🇧! <LINK>']",https://arxiv.org/abs/2003.02289,"We provide a consistent framework to set limits on properties of light sterile neutrinos coupled to all three active neutrinos using a combination of the latest cosmological data and terrestrial measurements from oscillations, $\beta$-decay and neutrinoless double-$\beta$ decay ($0\nu\beta\beta$) experiments. We directly constrain the full $3+1$ active-sterile mixing matrix elements $|U_{\alpha4}|^2$, with $\alpha \in ( e,\mu ,\tau )$, and the mass-squared splitting $\Delta m^2_{41} \equiv m_4^2-m_1^2$. We find that results for a $3+1$ case differ from previously studied $1+1$ scenarios where the sterile is only coupled to one of the neutrinos, which is largely explained by parameter space volume effects. Limits on the mass splitting and the mixing matrix elements are currently dominated by the cosmological data sets. The exact results are slightly prior dependent, but we reliably find all matrix elements to be constrained below $|U_{\alpha4}|^2 \lesssim 10^{-3}$. Short-baseline neutrino oscillation hints in favor of eV-scale sterile neutrinos are in serious tension with these bounds, irrespective of prior assumptions. We also translate the bounds from the cosmological analysis into constraints on the parameters probed by laboratory searches, such as $m_\beta$ or $m_{\beta \beta}$, the effective mass parameters probed by $\beta$-decay and $0\nu\beta\beta$ searches, respectively. When allowing for mixing with a light sterile neutrino, cosmology leads to upper bounds of $m_\beta < 0.09$ eV and $m_{\beta \beta} < 0.07$ eV at 95\% C.L, more stringent than the limits from current laboratory experiments. ","Bounds on light sterile neutrino mass and mixing from cosmology and
  laboratory searches"
91,1235794722683146240,117917587,Tatsuro KAWAMOTO,"['New detectability paper (mainly done by Chihiro Noguchi) is on arxiv. \n“Fragility of spectral clustering for networks with an overlapping structure” \n<LINK> \n\n(continued) <LINK>', 'Whereas the spectral clustering is known to be nearly optimal for some random graph models, it’s also known to be fragile against noise. (2/3)', 'We investigated how an overlapping structure in the stochastic block model affects the spectrum (isolated eigenvalue and spectral band). Interestingly, the effects are qualitatively different depending on the way the groups are overlapped. (3/3)']",https://arxiv.org/abs/2003.02463,"Communities commonly overlap in real-world networks. This is a motivation to develop overlapping community detection methods, because methods for non-overlapping communities may not perform well. However, deterioration mechanism of the detection methods used for non-overlapping communities have rarely been investigated theoretically. Here, we analyze an accuracy of spectral clustering, which does not consider overlapping structures, by using the replica method from statistical physics. Our analysis on an overlapping stochastic block model reveals how the structural information is lost from the leading eigenvector because of the overlapping structure. ","Fragility of spectral clustering for networks with an overlapping
  structure"
92,1235742749313269760,1075649842955866114,Luca Cortese,['New #xGASS paper by @ICRAR @UWAresearch @ARC_ASTRO3D PhD student @AstroRobbo looking at the role of bulges on star formation. Bulges appear to have little or no influence at all on the position of galaxies on the star-forming main sequence at z~0.  <LINK> <LINK>'],https://arxiv.org/abs/2003.02464,"We use our catalogue of structural decomposition measurements for the extended GALEX Arecibo SDSS Survey (xGASS) to study the role of bulges both along and across the galaxy star-forming main sequence (SFMS). We show that the slope in the $sSFR$-$M_{\star}$ relation flattens by $\sim$0.1 dex per decade in $M_{\star}$ when re-normalising $sSFR$ by disc stellar mass instead of total stellar mass. However, recasting the $sSFR$-$M_{\star}$ relation into the framework of only disc-specific quantities shows that a residual trend remains against disc stellar mass with equivalent slope and comparable scatter to that of the total galaxy relation. This suggests that the residual declining slope of the SFMS is intrinsic to the disc components of galaxies. We further investigate the distribution of bulge-to-total ratios ($B/T$) as a function of distance from the SFMS ($\Delta SFR_{MS}$). At all stellar masses, the average $B/T$ of local galaxies decreases monotonically with increasing $\Delta SFR_{MS}$. Contrary to previous works, we find that the upper-envelope of the SFMS is not dominated by objects with a significant bulge component. This rules out a scenario in which, in the local Universe, objects with increased star formation activity are simultaneously experiencing a significant bulge growth. We suggest that much of the discrepancies between different works studying the role of bulges originates from differences in the methodology of structurally decomposing galaxies. ","xGASS: The Role of Bulges Along and Across the Local Star-Forming Main
  Sequence"
93,1235590132679553024,1011308091328028672,Vashisht Madhavan,"['New paper! Scaling MAP-Elites to Deep Neuroevolution. We enable agents to recover from damage and explore in high-D control tasks, where traditional QD algorithms fail. Arxiv <LINK>. Work led by @cedcolas during his internship with me @UberAILabs 1/3 <LINK>', 'Work done with a great team including @Joost_Huizinga and @jeffclune 2/3', 'In this hard task, the agent must learn to 1) navigate and 2) reach the goal w/ a strongly deceptive reward signal directly leading the agent to the trap on the right.\nPrevious attempts rely on hierarchical RL, yet our method solves the task directly with a single policy. 3/3']",https://arxiv.org/abs/2003.01825,"Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have proven very useful for a broad range of applications including enabling real robots to recover quickly from joint damage, solving strongly deceptive maze tasks or evolving robot morphologies to discover new gaits. However, present implementations of MAP-Elites and other QD algorithms seem to be limited to low-dimensional controllers with far fewer parameters than modern deep neural network models. In this paper, we propose to leverage the efficiency of Evolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers parameterized by large neural networks. We design and evaluate a new hybrid algorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage recovery in a difficult high-dimensional control task where traditional ME fails. Additionally, we show that ME-ES performs efficient exploration, on par with state-of-the-art exploration algorithms in high-dimensional control tasks with strongly deceptive rewards. ",Scaling MAP-Elites to Deep Neuroevolution
94,1235377114976849923,2800204849,Andrew Gordon Wilson,"['Overparameterization isn\'t mysterious, if we stop parameter counting as a proxy for complexity. Our new paper ""Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited"": <LINK>. With Wesley Maddox, @g_benton_. 1/7 <LINK>', 'We want our models to be as flexible as possible; moreover, a high dimensional parameter space actually makes it easier to find a good but simple solution. We look at model selection and generalization through the lens of effective dimensionality. 2/7', 'From this perspective, models with zero training loss can be viewed as providing lossless compressions of data. The models with the best compression, the lowest effective dimensionality (ED), will typically generalize best. In this regime, ED closely tracks double descent. 3/7 https://t.co/oHSi5jMhnv', 'The effective dimensionality is defined by the eigenspectrum of the Hessian. We can compute the spectrum efficiently using GPU accelerated Lanczos methods in GPyTorch. 4/7 https://t.co/By4gJkXzre', 'Research has lately focused on increasing width, cf. double descent, NTK, etc. But it is *depth* that makes deep learning interesting. We see ED tracks generalization as a function of width *and* depth in the regime with zero training loss. Deep+narrow &amp; shallow+wide overfit. 5/7 https://t.co/3IxqKa152O', 'We also relate effective dimensionality to posterior contraction in Bayesian deep learning, and look at the properties of function space as we move in directions of determined and undetermined parameters. 6/7 https://t.co/YlwYwxbijv', 'Many more results, figures, discussions, experiments, in the paper! Code is available at: https://t.co/6Ot3r9iYee\n7/7', '@chhaviyadav_ Yes, hard to contain the excitement :)']",https://arxiv.org/abs/2003.02139,"Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, width-depth tradeoffs, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models. We also show that effective dimensionality compares favourably to alternative norm- and flatness- based generalization measures. ","Rethinking Parameter Counting in Deep Models: Effective Dimensionality
  Revisited"
95,1235209875006926849,376494033,Alex Nichol,"['Super excited to release VQ-DRAW, a new kind of discrete VAE!\n\nPaper: <LINK>\nCode: <LINK>\nBlog: <LINK>', 'Some samples https://t.co/tMk1k4QRTt', 'Decoding in action for a few CIFAR images. Decoding happens in stages, and each stage adds a few more bits. https://t.co/txIPOnoLe1']",https://arxiv.org/abs/2003.01599,"In this paper, I present VQ-DRAW, an algorithm for learning compact discrete representations of data. VQ-DRAW leverages a vector quantization effect to adapt the sequential generation scheme of DRAW to discrete latent variables. I show that VQ-DRAW can effectively learn to compress images from a variety of common datasets, as well as generate realistic samples from these datasets with no help from an autoregressive prior. ",VQ-DRAW: A Sequential Discrete VAE
96,1235205105722576902,356223194,Pieter Claeys,"[""1/ Our latest paper on quantum circuits with maximal butterfly velocity just appeared on arXiv! It's a new topic for me, so all comments are more than welcome. Also: a short thread.\n\n<LINK> <LINK>"", ""2/ If we perturb a physical system at a given point in space and time, how does this perturbation influence the system at a different point in space and time? If the points are far away, it generally doesn't."", '3/ Perturbations tend to grow with a finite ""butterfly velocity"" (think chaos and the butterfly effect), and it takes a finite time for any local effect to become noticeable at a different location.', ""4/ In the measures we usually consider, this is not a very 'sharp' effect: the butterfly velocity itself is spread out in a diffusively-broadening front."", '5/ Here we calculate so-called ""out-of-time-order correlators"" —a measure for chaos and the scrambling of quantum information—, in systems where this butterfly velocity is maximal and dynamics are governed by so-called quantum circuits.', '6/ No broadening is possible due to geometric constraints and everything becomes more clearly delineated: all effects are focused on a ""light cone"" (so-called since the butterfly velocity is an effective speed of light), and either decay away from this light cone or don\'t.', '7/ In the special case of ""dual-unitary circuits"", we explicitly relate how quickly such a system relaxes to thermal equilibrium with the rate at which OTOCs decay away from this light cone — scrambling here neatly connects with thermalization.']",https://arxiv.org/abs/2003.01133,"We consider the long-time limit of out-of-time-order correlators (OTOCs) in two classes of quantum lattice models with time evolution governed by local unitary quantum circuits and maximal butterfly velocity $v_{B} = 1$. Using a transfer matrix approach, we present analytic results for the long-time value of the OTOC on and inside the light cone. First, we consider `dual-unitary' circuits with various levels of ergodicity, including the integrable and non-integrable kicked Ising model, where we show exponential decay away from the light cone and relate both the decay rate and the long-time value to those of the correlation functions. Second, we consider a class of kicked XY models similar to the integrable kicked Ising model, again satisfying $v_{B}=1$, highlighting that maximal butterfly velocity is not exclusive to dual-unitary circuits. ",Maximum velocity quantum circuits
97,1235022661660508161,839913287240278020,Joey Rodriguez,['NEW PAPER!: LTT 3780: Two Small planets that span the super Earth Radius valley orbiting a mid-M dwarf! Great work by Ryan! @TESSatMIT @NASAExoplanets <LINK>'],https://arxiv.org/abs/2003.01136,"We present the confirmation of two new planets transiting the nearby mid-M dwarf LTT 3780 (TIC 36724087, TOI-732, $V=13.07$, $K_s=8.204$, $R_s$=0.374 R$_{\odot}$, $M_s$=0.401 M$_{\odot}$, d=22 pc). The two planet candidates are identified in a single TESS sector and are validated with reconnaissance spectroscopy, ground-based photometric follow-up, and high-resolution imaging. With measured orbital periods of $P_b=0.77$ days, $P_c=12.25$ days and sizes $r_{p,b}=1.33\pm 0.07$ R$_{\oplus}$, $r_{p,c}=2.30\pm 0.16$ R$_{\oplus}$, the two planets span the radius valley in period-radius space around low mass stars thus making the system a laboratory to test competing theories of the emergence of the radius valley in that stellar mass regime. By combining 63 precise radial-velocity measurements from HARPS and HARPS-N, we measure planet masses of $m_{p,b}=2.62^{+0.48}_{-0.46}$ M$_{\oplus}$ and $m_{p,c}=8.6^{+1.6}_{-1.3}$ M$_{\oplus}$, which indicates that LTT 3780b has a bulk composition consistent with being Earth-like, while LTT 3780c likely hosts an extended H/He envelope. We show that the recovered planetary masses are consistent with predictions from both photoevaporation and from core-powered mass loss models. The brightness and small size of LTT 3780, along with the measured planetary parameters, render LTT 3780b and c as accessible targets for atmospheric characterization of planets within the same planetary system and spanning the radius valley. ","A pair of TESS planets spanning the radius valley around the nearby
  mid-M dwarf LTT 3780"
98,1234927029193187331,1405259995,Adam Elmachtoub,"['Excited about our new paper where we use decision trees for predicting cost vectors to optimization problems! We show our SPO Trees lead to higher quality decisions than CART, and require significantly fewer leaves to do so(more interpretable)!  <LINK>', 'Work with former PhD student Ryan McNellis (now at @amazon) and former Columbia undergrad Jason Liang (now at @ORCenter)']",https://arxiv.org/abs/2003.00360,"We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error. ","Decision Trees for Decision-Making under the Predict-then-Optimize
  Framework"
99,1234833191770501121,633176250,Dr. Larry R. Nittler 🚀🎹🌖☄️,['New paper with @SZWeider and @efrankplanetary\n\nGlobal Major-Element Maps of Mercury from Four Years of MESSENGER X-Ray Spectrometer Observations\n\n<LINK> <LINK>'],https://arxiv.org/abs/2003.00650,"The X-Ray Spectrometer (XRS) on the MESSENGER spacecraft provided measurements of major-element ratios across Mercury's surface. We present global maps of Mg/Si, Al/Si, S/Si, Ca/Si, and Fe/Si derived from XRS data collected throughout MESSENGER's orbital mission. We describe the procedures we used to select and filter data and to combine them to make the final maps, which are archived in NASA's Planetary Data System. Areal coverage is variable for the different element-ratio maps, with 100% coverage for Mg/Si and Al/Si, but only 18% coverage for Fe/Si north of 30 $^{\circ}$ N, where the spatial resolution is highest. The spatial resolution is improved over previous maps by 10-15% because of the inclusion of higher-resolution data from late in the mission when the spacecraft periapsis altitude was low. Unlike typical planetary data maps, however, the spatial resolution of the XRS maps can vary from pixel to pixel, and thus care must be taken in interpreting small-scale features. We provide several examples of how the XRS maps can be used to investigate elemental variations in the context of geological features on Mercury, which range in size from single $\sim$100-km-diameter craters to large impact basins. We expect that these maps will provide the basis for and/or contribute to studies of Mercury's origin and geological history for many years to come. ","Global Major-Element Maps of Mercury from Four Years of MESSENGER X-Ray
  Spectrometer Observations"
100,1234767337653440512,378228706,Hannah Wakeford,"[""New Paper: Presenting the 1st peer-reviewed science application of Hubble's WFC3/UVIS G280 spectroscopic grism. We used it to measure the atmosphere of HAT-P-41b from 200-800nm in one shot with a precision on the transit of 30ppm! \n<LINK> <LINK>"", 'The UV through optical (200–800nm) spectra of planets hold rich information about the chemistry and physics at work. In the solar system, UV spectroscopy has been critical in identifying and measuring the abundances of a variety of hydrocarbon and sulfur-bearing species. https://t.co/rs6dgBmkGY', 'G280 is a great mode to use as it has a really high throughput compared to STIS/COS, that we normally use -&gt; 25x more sensitive than COS &amp; 4x higher than STIS G430L at 350nm. You also get both the +ve &amp; -ve spectral orders = more photons https://t.co/D4kFLikc8y', 'This is what a G280 spectrum looks like. The 0th order is the center, the +1 (left) &amp; -1 (right) spectra. You can also see the +/-2 orders making as the mustache whiskers\nAs you see the trace is curved - makes things hard. https://t.co/7yKIFuMAJz', 'A complete extraction and reduction of the provided data requires the following steps: a) cosmic ray removal b) background subtraction c) aperture determination, and d) trace fitting. We provided details on all of these in the paper', ""In the paper we used @spitzer to accurately measure the planets period, inclination and a/R* to better fit our data. We then used two different methods to correct for Hubble's timeseries systematics - https://t.co/syoKdeCdZp &amp; https://t.co/13m2sZwsje https://t.co/UmjWC1ZKYK"", 'We measured 2 consecutive transits, both giving us the +1 &amp; -1 spectral orders. The -ve order is ~60% of the +1 counts. We then had effectively 4 transits to measure! We can also use the 1st orbit, unlike STIS/WFC3-IR! https://t.co/206ikXL09D', 'We show that the transmission spectra derived using different extraction and analysis techniques are the same with only 1.7sigma discrepancy. This shows that the instrument is stable and repeatable. Still obtaining high precision data which is what we need for #exoplanets https://t.co/voymZ3xxI0', 'We can also compare these measurements to other of the same planet taken with the STIS G430L and G750L gratings. We show that the UVIS G280 grism is higher precision, and can reach higher resolution, over a wider wavelength range in less observing time! https://t.co/lfph1PyGpH', 'We do a quick preliminary fit to the measured spectrum with forward models and find it can be described by a high temperature atmosphere with clouds and high metallicity. But I stress this is very preliminary and you should stay tuned for more on this soon. https://t.co/XWfouNhYRf', 'This work could not have been done without @ExoSing @NikoleKLewis @kevinbstevenson @Onoddil and Nor Pirzkal who are the best partners to have when trying to use a whole new instrument for the first time. https://t.co/LK5iokr5KO', ""But in short. Hubble's WFC3 UVIS G280 grism is an excellent instrument to use for #exoplanet spectra. A little quirky and tricky to analyse, but it is stable, efficient, and precise. \n\nHappy #HSTcycle28 proposing and please feel free contact me for UVIS fun.""]",https://arxiv.org/abs/2003.00536,"The ultraviolet-visible wavelength range holds critical spectral diagnostics for the chemistry and physics at work in planetary atmospheres. To date, exoplanet time-series atmospheric characterization studies have relied on several combinations of modes on Hubble's STIS/COS instruments to access this wavelength regime. Here for the first time, we apply the Hubble WFC3/UVIS G280 grism mode to obtain exoplanet spectroscopy from 200-800 nm in a single observation. We test the G280 grism mode on the hot Jupiter HAT-P-41b over two consecutive transits to determine its viability for exoplanet atmospheric characterization. We obtain a broadband transit depth precision of 29-33ppm and a precision of on average 200ppm in 10nm spectroscopic bins. Spectral information from the G280 grism can be extracted from both the positive and negative first order spectra, resulting in a 60% increase in the measurable flux. Additionally, the first HST orbit can be fully utilized in the time-series analysis. We present detailed extraction and reduction methods for use by future investigations with this mode, testing multiple techniques. We find the results fully consistent with STIS measurements of HAT-P-41b from 310-800 nm, with the G280 results representing a more observationally efficient and precise spectrum. We fit HAT-P-41b's transmission spectrum with a forward model at Teq=2091K, high metallicity, and significant scattering and cloud opacity. With these first of their kind observations, we demonstrate that WFC3/UVIS G280 is a powerful new tool to obtain UV-optical spectra of exoplanet atmospheres, adding to the UV legacy of Hubble and complementing future observations with the James Webb Space Telescope. ","Into the UV: A precise transmission spectrum of HAT-P-41b using Hubble's
  WFC3/UVIS G280 grism"
101,1234671558854995971,383494771,Ian Williamson,"['Our new paper and software package (led by @momchilmm) for inverse design of photonic crystals (or any periodic optical structure) via automatic differentiation are now online! 🔴🟢🔵 \nGitHub: <LINK> \nDocs: <LINK>\nPaper: <LINK>', '@momchilmm That sweet sweet logo was designed by @NadineGilmer']",https://arxiv.org/abs/2003.00379,"Gradient-based inverse design in photonics has already achieved remarkable results in designing small-footprint, high-performance optical devices. The adjoint variable method, which allows for the efficient computation of gradients, has played a major role in this success. However, gradient-based optimization has not yet been applied to the mode-expansion methods that are the most common approach to studying periodic optical structures like photonic crystals. This is because, in such simulations, the adjoint variable method cannot be defined as explicitly as in standard finite-difference or finite-element time- or frequency-domain methods. Here, we overcome this through the use of automatic differentiation, which is a generalization of the adjoint variable method to arbitrary computational graphs. We implement the plane-wave expansion and the guided-mode expansion methods using an automatic differentiation library, and show that the gradient of any simulation output can be computed efficiently and in parallel with respect to all input parameters. We then use this implementation to optimize the dispersion of a photonic crystal waveguide, and the quality factor of an ultra-small cavity in a lithium niobate slab. This extends photonic inverse design to a whole new class of simulations, and more broadly highlights the importance that automatic differentiation could play in the future for tracking and optimizing complicated physical models. ",Inverse design of photonic crystals through automatic differentiation
102,1249722383700766720,702098312963477504,Xinshuo Weng,"['Posted our new paper ""Joint 3D Tracking and Forecasting with Graph Neural Network and Diversity Sampling"" on arXiv. The first unified 3D MOT and trajectory forecasting method with object interaction modeling and improved sample efficiency\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2003.07847,"Multi-object tracking (MOT) and trajectory prediction are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneficial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. Furthermore, instead of performing tracking and prediction sequentially which can propagate errors from tracking to prediction, we propose a parallelized framework to mitigate the issue. Also, our parallel track-forecast framework incorporates two additional novel computational units. First, we use a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which agents interact with one another. The GNN is able to improve discriminative feature learning for MOT association and provide socially-aware contexts for trajectory prediction. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating duplicate trajectory samples. We evaluate on KITTI and nuScenes datasets showing that our method with socially-aware feature learning and diversity sampling achieves new state-of-the-art performance on 3D MOT and trajectory prediction. Project website is: this https URL ","PTP: Parallelized Tracking and Prediction with Graph Neural Networks and
  Diversity Sampling"
103,1248524395988930560,414324523,andrea madotto,"['We are excited to announce ""XPersona: Evaluating Multilingual Personalized Chatbots"" a new evaluation dataset and baselines for personalized mulitilingual chatbots.   #NLProc #crosslingual #multilingual (1/5)\nPaper: <LINK>\nCode/Data: <LINK>', 'Big effort from multiple authors @zlinao_lin @johanliu96 @gentaiscool @BuatNgodingAja @AndreaMadotto in finding annotators, for both the dataset annotation and human evaluation, and for running a massive number of experiments. (2/5)', 'XPersona dataset is an extension of the persona-chat dataset (Thanks to @facebookai). Specifically, we extend the #ConvAI2 to other six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese.  (3/5)', 'We provide both cross-lingual and multilingual baselines and compared them with monolingual approaches and two-stage translation over SOTA models. Always thanks to @huggingface for the great codebase. (4/5)', 'We run an extensive automatic and human evaluation in all 6 languages using self-chat AcuteEVAL @facebookai.  The results show that the cross-lingual setting is challenging and even XNLG (SOTA) fails, which suggests a new interesting and challenging research direction. (5/5)', 'https://t.co/EXDpRYzznM', 'more examples in the paper. Stay tuned for a demo link :)']",https://arxiv.org/abs/2003.07568,"Personalized dialogue systems are an essential step toward better human-machine interaction. Existing personalized dialogue agents rely on properly designed conversational datasets, which are mostly monolingual (e.g., English), which greatly limits the usage of conversational agents in other languages. In this paper, we propose a multi-lingual extension of Persona-Chat, namely XPersona. Our dataset includes persona conversations in six different languages other than English for building and evaluating multilingual personalized agents. We experiment with both multilingual and cross-lingual trained baselines, and evaluate them against monolingual and translation-pipeline models using both automatic and human evaluation. Experimental results show that the multilingual trained models outperform the translation-pipeline and that they are on par with the monolingual models, with the advantage of having a single model across multiple languages. On the other hand, the state-of-the-art cross-lingual trained models achieve inferior performance to the other models, showing that cross-lingual conversation modeling is a challenging task. We hope that our dataset and baselines will accelerate research in multilingual dialogue systems. ",XPersona: Evaluating Multilingual Personalized Chatbot
104,1246759353060716544,931224122335653888,Idan Achituve,"['New paper! Domain-Adaptation (DA) for point-cloud data using Self-Supervised Learning (SSL). SSL was recently shown to be a powerful approach for DA on image data. We demonstrate its effectiveness on point clouds and obtain a new SoTA. \xa0<LINK> (1/2) <LINK>', 'We introduce (i) a novel SSL reconstruction task and (ii) a new mixup based training procedure tailored for point clouds. Both help to bridge the gap in sim-to-real adaptations by learning meaningful representations. Joint work with @haggaimaron, @galchechik and @NVIDIAAI (2/2)']",https://arxiv.org/abs/2003.12641,"Self-supervised learning (SSL) is a technique for learning useful representations from unlabeled data. It has been applied effectively to domain adaptation (DA) on images and videos. It is still unknown if and how it can be leveraged for domain adaptation in 3D perception problems. Here we describe the first study of SSL for DA on point clouds. We introduce a new family of pretext tasks, Deformation Reconstruction, inspired by the deformations encountered in sim-to-real transformations. In addition, we propose a novel training procedure for labeled point cloud data motivated by the MixUp method called Point cloud Mixup (PCM). Evaluations on domain adaptations datasets for classification and segmentation, demonstrate a large improvement over existing and baseline methods. ",Self-Supervised Learning for Domain Adaptation on Point-Clouds
105,1245788817145110528,3064612637,Ruben van Bergen,"['New perspective paper “Going in circles is the way forward: the role of recurrence in visual inference” with @KriegeskorteLab. <LINK> 1/12', 'Feedforward neural networks have dominated models of vision in engineering and computational neuroscience. Yet, biological visual systems are highly recurrent. This raises the question whether recurrence is fundamentally important to vision, or just a peculiarity of biology. 2/12', 'We make the case that understanding biological vision and building better computer vision both require engaging recurrent models. 3/12', 'Any finite-time RNN can be unrolled into a deep feedforward NN. This induces a network architecture with many connections that skip levels in the hierarchy, similar to the successful “ResNet” DNN architecture. 4/12 https://t.co/A95NMQr8qm', 'The fact that any finite-time RNN can be unrolled into a feedforward NN may suggest that feedforward NNs are a superset of RNNs. On the other hand, any feedforward NN can be interpreted as a special case of RNN and could be enhanced by adding recurrent connections. 5/12', 'Although any RNN can theoretically be unrolled, this is only realistic for a small number of time steps. So there are realistic RNNs that correspond to unrealistic feedforward NNs. Realism here could refer either to what fits in a brain or what fits in a GPU or smartphone. 6/12 https://t.co/5FUJpDm7Bf', 'Recurrent models of visual inference have a range of computational advantages. RNNs can (1) achieve greater and more flexible computational depth, (2) compress complex computations into limited hardware, … 7/12', 'RNNs can (3) integrate priors and priorities into visual inference through expectation and attention, (4) exploit sequential dependencies in their data for better inference and prediction, and (5) leverage the power of iterative computation. 7.5/12', 'Biological vision is active in that animals explore the scene with their eyes, targeting particularly informative and important regions. Active vision is a recurrent process where the cycle runs through the environment. 8/12', 'Overt active vision uses eye movements to selectively deploy a limited computational resource (fovea) to important parts of the scene. Covert active vision can similarly selectively deploy limited computational resources (attention) in processing of each glimpse. 9/12', 'Visual inference is like a game of twenty questions, where the answer to each question (result of some inference on the visual evidence) determines what question should be posed next (what further inference to pursue next). 10/12', 'We briefly review the evidence that recurrent vision models work better. Recent studies show that recurrence can render recognition more flexible in terms of the required time and energy and more robust to challenging conditions, such as when objects are partially occluded. 11/12', 'We also briefly review the evidence that recurrent models better explain brain activity. A range of older and recent studies show that cortical activity dynamics are better predicted by recurrent models. 12/12']",https://arxiv.org/abs/2003.12128,"Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth, (2) compress complex computations into limited hardware, (3) integrate priors and priorities into visual inference through expectation and attention, (4) exploit sequential dependencies in their data for better inference and prediction, and (5) leverage the power of iterative computation. ","Going in circles is the way forward: the role of recurrence in visual
  inference"
106,1245422735758483456,1618178317,Alex Berke,"['Spent the last few weeks focused on location data, COVID-19, individual privacy\n\nPut out a new paper \n\nAssessing Disease Exposure Risk With Location Histories And Protecting Privacy: \nA Cryptographic Approach In Response To A Global Pandemic\n\n<LINK>', ""with @bakkermichiel @alex_pentland @MITCities and others @medialab \n\nhttps://t.co/xXjZp4xeq4\n\nalong the way have met so many more amazing people in the math/CS/privacy/security space who are actively working on privacy-preserving solutions to contact  tracing.  We'll keep on..."", 'Governments and researchers around the world are implementing digital contact tracing solutions to stem the spread of infectious disease, namely COVID-19. Many of these solutions threaten individual rights and privacy.', 'Our goal is to break past the false dichotomy of effective\nversus privacy-preserving contact tracing. We offer an alternative approach to assess and communicate users’ risk of exposure to an infectious disease while preserving individual privacy.', 'This is a proposed design for how a system can identify and notify users who may have come in contact\nwith diagnosed disease carriers about their risk while respecting individuals’ privacy.\n\nhttps://t.co/xXjZp4xeq4', 'Our proposal uses recent GPS location histories, which are transformed and encrypted, and a private\nset intersection protocol.\n\nWe intend to show that effective contact tracing is possible without further forfeiting privacy.\n\nhttps://t.co/xXjZp4xeq4']",https://arxiv.org/abs/2003.14412,"Governments and researchers around the world are implementing digital contact tracing solutions to stem the spread of infectious disease, namely COVID-19. Many of these solutions threaten individual rights and privacy. Our goal is to break past the false dichotomy of effective versus privacy-preserving contact tracing. We offer an alternative approach to assess and communicate users' risk of exposure to an infectious disease while preserving individual privacy. Our proposal uses recent GPS location histories, which are transformed and encrypted, and a private set intersection protocol to interface with a semi-trusted authority. There have been other recent proposals for privacy-preserving contact tracing, based on Bluetooth and decentralization, that could further eliminate the need for trust in authority. However, solutions with Bluetooth are currently limited to certain devices and contexts while decentralization adds complexity. The goal of this work is two-fold: we aim to propose a location-based system that is more privacy-preserving than what is currently being adopted by governments around the world, and that is also practical to implement with the immediacy needed to stem a viral outbreak. ","Assessing Disease Exposure Risk with Location Data: A Proposal for
  Cryptographic Preservation of Privacy"
107,1245400744381087746,1201449637,"David Fink, PhD","['Great new paper by @Lizstuartdc and others showing the limitations of common statistical approaches to evaluating effect of opioid policy. Much needed work!\n\n<LINK>', ""@GloriaMiele @Lizstuartdc Doubtful. This pre-print was just published on March 26th. However, this is a very respected group of researchers, so I would expect this paper to be nearly publication ready. \n\nIf you're interested, @Lizstuartdc recently publish this great policy eval: https://t.co/AJz916dUmu""]",https://arxiv.org/abs/2003.12008,"State-level policy evaluations commonly employ a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. Motivated by applied state-level opioid policy evaluations, this simulation study compares statistical performance of multiple variations of two-way fixed effect models traditionally used for DID under a range of simulation conditions. While most linear models resulted in minimal bias, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160%). Further, root mean square error is minimized by linear AR models when examining crude mortality rates and by negative binomial models when examining raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis (< 10%), raising concerns of spurious conclusions about policy effectiveness. When considering performance across models, the linear autoregressive models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. These findings highlight notable limitations of traditional statistical models commonly used for DID designs, designs widely used in opioid policy studies and in state policy evaluations more broadly. ","Moving beyond the classic difference-in-differences model: A simulation
  study comparing statistical methods for estimating effectiveness of
  state-level policies"
108,1245370187567816708,2983164057,Mohsen Fayyaz,"['#CVPR2020 Learning To Temporally Segment Untrimmed Videos from Set of Actions:\n""SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation""\nA new paper by me and my supervisor Juergen Gall.\n<LINK> <LINK>', 'In a set supervised action segmentation problem, for each training video, only the list of actions is given that\noccur in the video, but not when, how often, and in which order they occur.', 'Our network divides a video into smaller temporal regions. For each region, the network estimates its length and the corresponding action label. Then the upsampling module uses the lengths and the action probabilities of all regions to estimate the framewise probabilities.', 'Although it is possible to do the upsampling by linear interpolation this operation is not differentiable w.r.t predicted lengths. Therefore, we use our novel upsampling module which is differentiable w.r.t. predicted lengths.', 'Since we do not know the ground-truth lengths and orders but only the set of present actions, we cannot directly use the predicted frame-wise probabilities of the network.', 'Therefore,  we introduce a novel temporal transformation method that transforms a temporal sequence to the set of action probabilities w.r.t predicted lengths and temporal locations of actions. Using the set of action probabilities we can use the GT to train the model end-to-end.', 'We will release the source code as soon as possible.\nThanks @yassersouri for his valuable comments on our work.']",https://arxiv.org/abs/2003.14266,"Temporal action segmentation is a topic of increasing interest, however, annotating each frame in a video is cumbersome and costly. Weakly supervised approaches therefore aim at learning temporal action segmentation from videos that are only weakly labeled. In this work, we assume that for each training video only the list of actions is given that occur in the video, but not when, how often, and in which order they occur. In order to address this task, we propose an approach that can be trained end-to-end on such data. The approach divides the video into smaller temporal regions and predicts for each region the action label and its length. In addition, the network estimates the action labels for each frame. By measuring how consistent the frame-wise predictions are with respect to the temporal regions and the annotated action labels, the network learns to divide a video into class-consistent regions. We evaluate our approach on three datasets where the approach achieves state-of-the-art results. ","SCT: Set Constrained Temporal Transformer for Set Supervised Action
  Segmentation"
109,1245169109446213632,1146573400908926981,Mariko Isogawa,"['Excited to share our new paper accepted at #CVPR2020! ""Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation"" by M. Isogawa, Y. Yuan, M. O’Toole, K. Kitani. \n\nProject page: <LINK>\nPaper: <LINK>\nFull video: <LINK> <LINK>']",https://arxiv.org/abs/2003.14414,"We describe a method for 3D human pose estimation from transient images (i.e., a 3D spatio-temporal histogram of photons) acquired by an optical non-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose by `looking around corners' through the use of light indirectly reflected by the environment. We bring together a diverse set of technologies from NLOS imaging, human pose estimation and deep reinforcement learning to construct an end-to-end data processing pipeline that converts a raw stream of photon measurements into a full 3D human pose sequence estimate. Our contributions are the design of data representation process which includes (1) a learnable inverse point spread function (PSF) to convert raw transient images into a deep feature vector; (2) a neural humanoid control policy conditioned on the transient image feature and learned from interactions with a physics simulator; and (3) a data synthesis and augmentation strategy based on depth data that can be transferred to a real-world NLOS imaging system. Our preliminary experiments suggest that our method is able to generalize to real-world NLOS measurement to estimate physically-valid 3D human poses. ",Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation
110,1244969395455176704,2885797996,Prof Jesse Capecelatro,"[""Check out @CompPhysGirl's new paper using sparse regression as an alternative to #NeuralNetworks for #turbulence modeling on arXiv: <LINK>""]",https://arxiv.org/abs/2003.12884,"A data-driven framework for formulation of closures of the Reynolds-Average Navier--Stokes (RANS) equations is presented. In recent years, the scientific community has turned to machine learning techniques to distill a wealth of highly resolved data into improved RANS closures. While the body of work in this area has primarily leveraged Neural Networks (NNs), we alternately leverage a sparse regression framework. This methodology has two important properties: (1) The resultant model is in a closed, algebraic form, allowing for direct physical inferences to be drawn and naive integration into existing computational fluid dynamics solvers, and (2) Galilean invariance can be guaranteed by thoughtful tailoring of the feature space. Our approach is demonstrated for two classes of flows: homogeneous free shear turbulence and turbulent flow over a wavy wall. This work demonstrates equivalent performance to that of modern NNs but with the added benefits of interpretability, increased ease-of-use and dissemination, and robustness to sparse training datasets. ","Formulating turbulence closures using sparse regression with embedded
  form invariance"
111,1244653450287292417,1135416938,Tommaso Rigon,"['Our new paper ""Enriched Pitman-Yor processes"", with Bruno Scarpa and Sonia Petrone, is online!\n\n<LINK>']",https://arxiv.org/abs/2003.12200,"In Bayesian nonparametrics there exists a rich variety of discrete priors, including the Dirichlet process and its generalizations, which are nowadays well-established tools. Despite the remarkable advances, few proposals are tailored for modeling observations lying on product spaces, such as $\mathbb{R}^p$. Indeed, for multivariate random measures, most available priors lack flexibility and do not allow for separate partition structures among the spaces. We introduce a discrete nonparametric prior, termed enriched Pitman-Yor process (EPY), aimed at addressing these issues. Theoretical properties of this novel prior are extensively investigated. We discuss its formal link with the enriched Dirichlet process and normalized random measures, we describe a square-breaking representation and we obtain closed-form expressions for the posterior law and the involved urn schemes. In second place, we show that several existing approaches, including Dirichlet processes with a spike and slab base measure and mixture of mixtures models, implicitly rely on special cases of the EPY, which therefore constitutes a unified probabilistic framework for many Bayesian nonparametric priors. Interestingly, our unifying formulation will allow us to naturally extend these models while preserving their analytical tractability. As an illustration, we employ the EPY for a species sampling problem in ecology and for functional clustering in an e-commerce application. ",Enriched Pitman-Yor processes
112,1244549888186241024,1141772501472727040,Lena Voita,"['[1/3] Information-Theoretic Probing with Minimum Description Length: a new paper with @iatitov, and a blog post!\n\nWe propose an alternative to the standard probes, which is more informative, stable, and sensible.\n\nBlog: <LINK>\nPaper: <LINK> <LINK>', '[2/3] We measure minimum description length (MDL) of labels given representations. MDL naturally characterizes not only probe quality, but also *the amount of effort* needed to achieve it (or, intuitively, strength of the regularity in representations with respect to the labels). https://t.co/OxPfCuQ4Sq', '[3/3] Our MDL probing is very simple: you do not need to change much in the standard probe-training pipelines! We measure MDL while still training probing classifiers. \n\nIntrigued? Take a look at the blog or the paper :)', '@akashkm99 @EdinburghNLP @iatitov Thank you! For non-python graphs, I use Corel Draw']",https://arxiv.org/abs/2003.12298,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates ""the amount of effort"" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes. ",Information-Theoretic Probing with Minimum Description Length
113,1242582208793247744,1209641938342690816,Taro Makino,"['It is my pleasure to announce that our new paper “Understanding the robustness of deep neural network classifiers for breast cancer screening” <LINK> was accepted at the #ICLR2020 workshop on AI for Affordable Healthcare!', 'We show that DNNs for mammogram image classification are sensitive to several commonly studied perturbations that natural image classifiers are sensitive to. This suggests that robustness research for mammogram image classification can build upon extensive existing literature. https://t.co/qUyx6icBDu', 'Natural image classifiers are vulnerable to many perturbations concentrated in high frequencies, such as additive noise and blurring. The fact that humans are invariant to these perturbations has spurred efforts to make natural image classifiers invariant to them as well.', 'In contrast, we argue that mammogram image classifiers should not be made invariant to high frequency perturbations, since it would degrade their ability to detect clinically significant features called microcalcifications. https://t.co/01XAoIAXyE', 'This work is a collaboration between @cai2r, @NYUDataScience, and @PW_edu. It was led by Witold Oleszkiewicz, myself, and @kudkudakpl, and supported by @tomasztrzcinsk1, @DrLindaMoy, @kchonyc, Laura Heacock, and @kjgeras.']",https://arxiv.org/abs/2003.10041,"Deep neural networks (DNNs) show promise in breast cancer screening, but their robustness to input perturbations must be better understood before they can be clinically implemented. There exists extensive literature on this subject in the context of natural images that can potentially be built upon. However, it cannot be assumed that conclusions about robustness will transfer from natural images to mammogram images, due to significant differences between the two image modalities. In order to determine whether conclusions will transfer, we measure the sensitivity of a radiologist-level screening mammogram image classifier to four commonly studied input perturbations that natural image classifiers are sensitive to. We find that mammogram image classifiers are also sensitive to these perturbations, which suggests that we can build on the existing literature. We also perform a detailed analysis on the effects of low-pass filtering, and find that it degrades the visibility of clinically meaningful features called microcalcifications. Since low-pass filtering removes semantically meaningful information that is predictive of breast cancer, we argue that it is undesirable for mammogram image classifiers to be invariant to it. This is in contrast to natural images, where we do not want DNNs to be sensitive to low-pass filtering due to its tendency to remove information that is human-incomprehensible. ","Understanding the robustness of deep neural network classifiers for
  breast cancer screening"
114,1242431814687436801,97238962,Dr Jonathan Nichols,"['New paper klaxon! We calculate the power of auroral radio bursts from ‘hot Jupiter’ exoplanets using a 3D MHD model of the planet’s magnetosphere and its interaction with the stellar wind. By @zazen2357111317, accepted into MNRAS <LINK>', ""@zazen2357111317 The MHD model shows the same power saturation effect I've talked about previously, and possibly explains the lack of unambiguous detection to date. But detection certainly not beyond realms of possibility, and very likely with SKA."", '@zazen2357111317 https://t.co/x8wFGe4kBH']",https://arxiv.org/abs/2003.09991,"We present calculations of auroral radio powers of magnetised hot Jupiters orbiting Sun-like stars, computed using global magnetohydrodynamic (MHD) modelling of the magnetospheric and ionospheric convection arising from the interaction between the magnetosphere and the stellar wind. Exoplanetary auroral radio powers are traditionally estimated using empirical or analytically-derived relations, such as the Radiometric Bode's Law (RBL), which relates radio power to the magnetic or kinetic energy dissipated in the stellar wind-planet interaction. Such methods risk an oversimplification of the magnetospheric electrodynamics giving rise to radio emission. As the next step toward a self-consistent picture, we model the stellar wind-magnetosphere-ionosphere coupling currents using a 3D MHD model. We compute electron-cyclotron maser instability-driven emission from the calculated ionospheric field-aligned current density. We show that the auroral radio power is highly sensitive to interplanetary magnetic field (IMF) strength, and that the emission is saturated for plausible hot Jupiter Pedersen conductances, indicating that radio power may be largely independent of ionospheric conductance. We estimate peak radio powers of $10^{14}$ W from a planet exposed to an IMF strength of $10^3$ nT, implying flux densities at a distance of 15 pc from Earth potentially detectable with current and future radio telescopes. We also find a relation between radio power and planetary orbital distance that is broadly consistent with results from previous analytic models of magnetosphere-ionosphere coupling at hot Jupiters, and indicates that the RBL likely overestimates the radio powers by up to two orders of magnitude in the hot Jupiter regime ","Magnetohydrodynamic modelling of star-planet interaction and associated
  auroral radio emission"
115,1242071277415870470,1216061729131696128,Hugo Touvron,['Our FixEfficientNet-L2 obtains a new state-of-the-art performance on ImageNet!\nYou can find all our new results on the FixRes additional note (<LINK>) and also on @paperswithcode and @sotabench\n(In case you missed the FixRes paper : <LINK>) <LINK>'],https://arxiv.org/abs/2003.08237,"This paper provides an extensive analysis of the performance of the EfficientNet image classifiers with several recent training procedures, in particular one that corrects the discrepancy between train and test images. The resulting network, called FixEfficientNet, significantly outperforms the initial architecture with the same number of parameters. For instance, our FixEfficientNet-B0 trained without additional training data achieves 79.3% top-1 accuracy on ImageNet with 5.3M parameters. This is a +0.5% absolute improvement over the Noisy student EfficientNet-B0 trained with 300M unlabeled images. An EfficientNet-L2 pre-trained with weak supervision on 300M unlabeled images and further optimized with FixRes achieves 88.5% top-1 accuracy (top-5: 98.7%), which establishes the new state of the art for ImageNet with a single crop. These improvements are thoroughly evaluated with cleaner protocols than the one usually employed for Imagenet, and particular we show that our improvement remains in the experimental setting of ImageNet-v2, that is less prone to overfitting, and with ImageNet Real Labels. In both cases we also establish the new state of the art. ",Fixing the train-test resolution discrepancy: FixEfficientNet
116,1241132241197109345,1241116652013596674,Cyrille Combettes,"['Glad to share our new paper ""Boosting Frank-Wolfe by Chasing Gradients"" with @spokutta We propose to speed-up FW by moving in directions better aligned with the gradients. Turns out the progress obtained overcomes the cost of multiple oracle calls! <LINK>']",https://arxiv.org/abs/2003.06369,"The Frank-Wolfe algorithm has become a popular first-order optimization algorithm for it is simple and projection-free, and it has been successfully applied to a variety of real-world problems. Its main drawback however lies in its convergence rate, which can be excessively slow due to naive descent directions. We propose to speed up the Frank-Wolfe algorithm by better aligning the descent direction with that of the negative gradient via a subroutine. This subroutine chases the negative gradient direction in a matching pursuit-style while still preserving the projection-free property. Although the approach is reasonably natural, it produces very significant results. We derive convergence rates $\mathcal{O}(1/t)$ to $\mathcal{O}(e^{-\omega t})$ of our method and we demonstrate its competitive advantage both per iteration and in CPU time over the state-of-the-art in a series of computational experiments. ",Boosting Frank-Wolfe by Chasing Gradients
117,1239928547369799681,2332157006,Federico Bianchi,"['New paper about e-commerce in NLP (workshop at @TheWebConf)! See how we combine product representations, language modeling and images to support type-ahead personalization in e-commerce! With @christineyyuu,  @jacopotagliabue and @GreCo_CiRo from @coveo. <LINK> <LINK> <LINK>', '@vibhavagarwal5 @debora_nozza @TheWebConf @christineyyuu @jacopotagliabue @GreCo_CiRo @coveo Thanks a lot :) :) :)']",https://arxiv.org/abs/2003.07160,"We address the problem of personalizing query completion in a digital commerce setting, in which the bounce rate is typically high and recurring users are rare. We focus on in-session personalization and improve a standard noisy channel model by injecting dense vectors computed from product images at query time. We argue that image-based personalization displays several advantages over alternative proposals (from data availability to business scalability), and provide quantitative evidence and qualitative support on the effectiveness of the proposed methods. Finally, we show how a shared vector space between similar shops can be used to improve the experience of users browsing across sites, opening up the possibility of applying zero-shot unsupervised personalization to increase conversions. This will prove to be particularly relevant to retail groups that manage multiple brands and/or websites and to multi-tenant SaaS providers that serve multiple clients in the same space. ","""An Image is Worth a Thousand Features"": Scalable Product
  Representations for In-Session Type-Ahead Personalization"
118,1239569276421443589,2279967715,Michael Tremmel,"['New paper day! Angelo Ricarte leads this work studying the connection between ram pressure and active galactic nuclei (i.e. growing supermassive black holes) in the RomulusC high resolution galaxy cluster simulation\n\n<LINK>', 'Galaxy clusters, some of the most massive gravitationally bound structures in the Universe, are host to lots of hot, dense gas. As galaxies move through this hot gas, their own gas supply gets blown away. Sort of like walking in the wind and having your open umbrella fly away', 'The physical mechanism is ram pressure (RP) and sometimes we are lucky enough to see it in action in what are referred to as ""jellyfish"" galaxies. This is an impressive example imaged by Yale grad student William Cramer. The red tail is gas removed by RP\n\nhttps://t.co/S9LdRtbVqZ https://t.co/KJ8UvmYLkX', 'We see this same thing happening in the simulation. Galaxies experience ram pressure and have their gas removed, subsequently shutting down both star formation and supermassive black hole (SMBH) growth, both of which require gas!', 'Here are three examples from RomulusC. Top: images of gas density, with tails of gas removed via RP. Middle: the decline in the growth rate of stars (blue) and central SMBH (red). Yellow is the amount of RP. Bottom: mass in stars (blue), SMBH (red), gas (grey), and total (purple) https://t.co/H1PFUtzY5M', 'The first peak in RP coincides with the galaxy\'s first pericenter passage where it is closest to the center of the cluster (denser gas) and moving fast (more ""wind""). After this, we see a drastic decline in gas mass, star formation, and SMBH growth. But also something else...', 'If you look closely at the red line, prior to a drastic fall in star formation and SMBH growth, the SMBH growth rate peaks. Turns out this a general trend in the simulation.', 'Top: SMBH growth at various times just before (left), at (middle) and just after (right) pericenter as a function of the mass of the galaxy. Bottom: same but for star formation rate (SFR) . Values are relative to a baseline calculated for each galaxy well before pericenter https://t.co/v9xN9rcOyq', 'While star formation rate (SFR) is in decline the whole time, the SMBH accretion rate increases on average by a factor of 2.2 as the galaxy goes through pericenter. Diamond shapes on the bottom refer to galaxies without central SMBHs (not all galaxies have them in the simulation)', 'Examining only galaxies that have not yet lost all of their gas to RP, there is a positive correlation between the magnitude of RP and both the SMBH growth rate (top left) and SFR (bottom left). Subsequently, this leads to anti-correlations with distance to cluster center (right) https://t.co/TrXrgK49FO', 'This occurs because, while RP acts to remove gas, it also compresses it, making it more dense and susceptible not only to form stars but also to trigger SMBH growth. At very high values, there is a drop off as RP begins to remove gas more rapidly.', 'The grey region marked on the figure is from theoretical predictions for an AGN-triggering ""goldilocks"" region of RP. Our results disagree with these models, finding that SMBH growth continues to increase to higher values of RP (so long as their galaxy holds onto its gas)', 'The idea that RP can lead to SMBH activity has been thought about recently in light of some exquisite observations by the GASP collaboration. This is the first time it has been studied in a cosmological simulation.\n\nhttps://t.co/n7At0fxbUZ\n\nhttps://t.co/OWfEMDtAxB', 'These are observations from George et al (2019) that show a jellyfish galaxy with a growing SMBH. The lefthand image shows UV emission (notice the gap in the center). The right shows regions of star formation (blue) and AGN (red)\n\nhttps://t.co/OWfEMDtAxB https://t.co/Glou5g9hb1', 'In these observations, the AGN is suppressing star formation in the center of the galaxy, while in the outskirts the gas that is compressed by RP is driving enhanced star formation. The AGN are also driving outflows of gas. We see something similar in Romulus.', 'Star formation histories for two similar mass galaxies as they fall into the RomulusC cluster. Time is normalized by  pericenter passages. The red represents a galaxy with a central SMBH and rapidly falling SFR. The blue follows a galaxy without a SMBH and a more gradual decline https://t.co/CEs7VymDGP', 'Top: edge-on images of gas temperature and velocity maps. The blue case is on the left, with a nice stable disk. The red (right) is disturbed and experiencing powerful outflows, evidence that feedback from SMBHs can lead to rapid quenching of star formation in cluster galaxies', 'Summary: we use the RomulusC simulation to make the first self-consistent prediction for AGN triggering due to ram pressure within a cosmological simulation. We find evidence that feedback from SMBHs triggered in this way can affect how long it takes cluster galaxies to quench.', ""This work was possible b/c of the unprecedented resolution of RomulusC. Read about it (https://t.co/ka2ZzrZ0Yu) and how we've used it to study AGN feedback (https://t.co/JCZlmoYhdi), ultra diffuse dwarf galaxies (https://t.co/njbVCNmYRJ) and cluster gas (https://t.co/dxxChGscMT)""]",https://arxiv.org/abs/2003.05950,"The dense environment of a galaxy cluster can radically transform the content of in-falling galaxies. Recent observations have found a significant population of active galactic nuclei (AGN) within ""jellyfish galaxies,"" galaxies with trailing tails of gas and stars that indicate significant ram pressure stripping. The relationship between AGN and ram pressure stripping is not well understood. In this letter, we investigate the connection between AGN activity and ram pressure in a fully cosmological setting for the first time using the RomulusC simulation, one of the highest resolution simulations of a galaxy cluster to date. We find unambiguous morphological evidence for ram pressure stripping. For lower mass galaxies (with stellar masses < 10^9.5 solar masses) both star formation and black hole accretion are suppressed by ram pressure before they reach pericenter, whereas for more massive galaxies accretion onto the black hole is enhanced during pericentric passage. Our analysis also indicates that as long as the galaxy retains gas, AGN with higher Eddington ratios are more likely to be the found in galaxies experiencing higher ram pressure. We conclude that prior to quenching star formation, ram pressure triggers enhanced accretion onto the black hole, which then produces heating and outflows due to AGN feedback. AGN feedback may in turn serve to aid in the quenching of star formation in tandem with ram pressure. ",A Link Between Ram Pressure Stripping and Active Galactic Nuclei
119,1238194818507038720,1024452387816255488,Ali DB,"['Recently, I found #MixUp and #CutMix. They blindly combine multiple images to produce new training sample. Then I thought why not supervising the mixing augmentation? So, I have developed #SuperMix. \nPaper: <LINK>\nCode: <LINK>\n#augmentation']",https://arxiv.org/abs/2003.05034,"This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples. SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To enhance the efficiency of the algorithm, we develop a variant of the Newton iterative method, $65\times$ faster than gradient descent on this problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides comparable performance to the advanced augmentation methods, such as AutoAugment and RandAugment. In particular, combining SuperMix with RandAugment achieves 78.2\% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacher's knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4\% and 3.1\% on CIFAR-100 and ImageNet, respectively. {\it The code is available at this https URL}. ",SuperMix: Supervising the Mixing Data Augmentation
120,1237366952789712897,62849728,Thiemo Fetzer 🇪🇺🇺🇦,"['How does fear of #coronavirus affect economic sentiment? May uncertainty about key features of #Covid19 exacerbate economic anxieties and contribute to panic reactions? We study this in a new paper <LINK>', '1) We exploit the rapid global spread of the #Coronavirus combined with high-frequency daily Google search activity data to explore how search patterns for four broad topics changed: #Recession, Stock market crash, Survivalism, and conspiracy theories. https://t.co/qMOLuC38wI', '2) Across the globe there has been a rise in Google searches related to these topics. We see that the arrival of #Coronavirus in a country measured as the first confirmed case or measured as the first inter-community transmission is associated with a sizable jump in such searches https://t.co/Fs0HGhBLp5', '3) Upon arrival of #Corona #Covid19, searches for these four broad topics jump up by between 18 – 58% in a country relative to the pre-Corona country-specific average search activity on these topics. https://t.co/XhWaEsd5Wo', '4) Why does this matter from a economic perspective? Over the past 5 years using quarterly data, we find that sharp increases in search activity on recession-topics is a leading indicator predicting economic slowdowns in subsequent quarters. In particular, due to lower C. https://t.co/Q4hDCK6CQc', '5) We find that a 100% increase in recession search intensity in a quarter vis-à-vis the country-specific average intensity is associated with a 1.5 percentage points lower growth in consumption or a 1 percentage point lower growth in real GDP in the subsequent quarter.', '6) While the #coronavirus has so far been primarily considered a supply shock, aggregate demand is likely affected in response to changing human behaviour to avoid infections: we expect less service consumption, much less service sector imports (travel and tourism) etc.', '7) So what may lie behind these economic anxieties? And how can the economic fallout be limited? To shed some light on this, we conduct an online survey experiment in a nationally representative sample with around 1000 participants from the US on March 4th.', '8) We elicit peoples’ beliefs about both, #Covid19’s mortality as well as its contagiousness (R0) and contrast this with what the medical literature has documented so far (WHO recently suggested mortality of 3.4%, earlier estimates were closer to 2%). R0 was estimated around 2', '9) So what do these beliefs about mortality look like in our representative sample? They are heterogenous with 50% of respondents thinking mortality is higher than 5% (average at 19%). This seems higher than what the medical literature on #Covid19 suggests to date. https://t.co/ukFPMcZOtD', '10) Beliefs about contagiousness are also heterogenous. 50% of respondents think that every infected person infects at least 10 others. The average is substantially higher with 43. Early medical research suggest an R0 around 2 (https://t.co/1QOGIxg5lC &amp; https://t.co/dezoYAHIjp ) https://t.co/izjlNC6ltI', '11) When asking people about their economic anxieties the group of people that seem to be overestimating the contagiousness and the mortality seem to exhibit significantly higher degrees of anxiety. https://t.co/UKYg0le4cg', '12) Naturally there are many things we do not yet know about #coronavirus – including the true population value of R0 and mortality. But if the knowledge to date is any guide mortality rates around 20% (as is the average) seem off.', '13)When #coronavirus is discussed in the media, nuances around what we know and what we do not know yet are often not conveyed. Yet, often times #Coronavirus mortality gets compared to that of the flu or that of SARS. How do people respond to this information in terms of anxiety?', '14) We administer two information treatments on randomly selected subsets. In one case we inform people about estimates of the relative mortality (#covid19 has an xx times higher mortality than the flu vis-à-vis #covid19 has a xx lower mortality than #SARS).', '15) People respond to these information treatments with the group exposed to the “high mortality” frame as is commonly used in the media exacerbating higher degrees of anxieties about the #coronavirus. https://t.co/mdSyaAWGie', '16) This in turn creates higher anxieties about the economic impact of #coronavirus. We do something similar about contagiousness, informing a subset of people about the present estimates of R0 – this seems to have an effect on lowering anxieties. https://t.co/yKIbWbFc6e', '17) There are many things we do not know yet about #coronavirus, and beliefs seem very heterogenous resulting in potentially exacerbated anxieties. Making sure that new findings are transparently and clearly communicated, explaining the rationale behind decisions etc is key', '18) My own personal worry is that the last decade has seen a steady erosion of trust in institutions, science, experts and the media -- often times wilfully politically engineered. This could exacerbate the social and economic fall out from #Coronavirus.', '19) The whole paper is available here: https://t.co/uq0gcWnb8E', '@chris_breu The five percent is the median , so 50% think it’s higher.  Don’t get hung up about the five percent, it’s the fact that 50% of respondents think that it’s higher than that - this is what we refer to as overestimation.']",https://arxiv.org/abs/2003.03848,"We provide one of the first systematic assessments of the development and determinants of economic anxiety at the onset of the coronavirus pandemic. Using a global dataset on internet searches and two representative surveys from the US, we document a substantial increase in economic anxiety during and after the arrival of the coronavirus. We also document a large dispersion in beliefs about the pandemic risk factors of the coronavirus, and demonstrate that these beliefs causally affect individuals' economic anxieties. Finally, we show that individuals' mental models of infectious disease spread understate non-linear growth and shape the extent of economic anxiety. ",Coronavirus Perceptions And Economic Anxiety
121,1235527694730899457,192826908,Jorge Lillo-Box,"['New @CARMENES_exopl paper by Nowak et al. on a very interesting system of two planets around the nearby M-dwarf LP729-54, on both sides of the radius valley. Very interesting to understand planet formation and evolution scenarios:\n<LINK> <LINK>', 'Also, it is the first paper using the official release of tpfplotter! (https://t.co/NIWpYDeDcW) A python package to plot @NASA_TESS TPF and @ESAGaia  sources on top in an automatic and easy way. The figures are paper-ready and very nice! Check it out! https://t.co/D7OEua9qSA', 'Note that another paper on the same system was published simultaneously by Cloutier et al. (https://t.co/TmmDYF1LyI) reaching similar although not fully compatible parameters.']",https://arxiv.org/abs/2003.01140,"We present the discovery and characterisation of two transiting planets observed by the Transiting Exoplanet Survey Satellite (TESS) orbiting the nearby (d ~ 22 pc), bright (J ~ 9 mag) M3.5 dwarf LTT 3780 (TOI-732). We confirm both planets and their association with LTT 3780 via ground-based photometry and determine their masses using precise radial velocities measured with the CARMENES spectrograph. Precise stellar parameters determined from CARMENES high resolution spectra confirm that LTT 3780 is a mid-M dwarf with an effective temperature of T_eff = 3360 +\- 51 K, a surface gravity of log(g) = 4.81 +/- 0.04 (cgs), and an iron abundance of [Fe/H] = 0.09 +/- 0.16 dex, with an inferred mass of M_star = 0.379 +/- 0.016 M_sun and a radius of R_star = 0.382 +/- 0.012 R_sun. The ultra-short-period planet LTT 3780 b (P_b = 0.77 d) with a radius of 1.35^{+0.06}_{-0.06} R_earth, a mass of 2.34^{+0.24}_{-0.23} M_earth, and a bulk density of 5.24^{+0.94}_{-0.81} g cm^{-3} joins the population of Earth-size planets with rocky, terrestrial composition. The outer planet, LTT 3780 c, with an orbital period of 12.25 d, radius of 2.42^{+0.10}_{-0.10} R_earth, mass of 6.29^{+0.63}_{-0.61} M_earth, and mean density of 2.45^{+0.44}_{-0.37} g cm^{-3} belongs to the population of dense sub-Neptunes. With the two planets located on opposite sides of the radius gap, this planetary system is an excellent target for testing planetary formation, evolution and atmospheric models. In particular, LTT 3780 c is an ideal object for atmospheric studies with the James Webb Space Telescope. ","The CARMENES search for exoplanets around M dwarfs. Two planets on the
  opposite sides of the radius gap transiting the nearby M dwarf LTT 3780"
122,1234691629815984129,713389493076758528,Xavier Bresson,"['New paper on benchmarking graph neural networks w/ @vijaypradwi @chaitjo T. Laurent and Y. Bengio\n\n<LINK>\n\nOur goal was to identify trends and good building blocks for GNNs. <LINK>', '@vijaypradwi @chaitjo Such analysis was not possible with small CORA and TU datasets (all GNNs perform the same as well as non-graph NNs).\n\nHence we introduced 6 medium-size datasets (w/ 12k-70k graphs and 8-500 nodes) for node/edge/graph classification and graph regression.', '@vijaypradwi @chaitjo As expected, graph NNs outperform non-graph NNs for larger datasets. \n\nSo nothing new under the sun but it was important to show this experimentally.', '@vijaypradwi @chaitjo We released a GitHub repo for full reproducibility:\nhttps://t.co/zWawrI70Or\n\nWe made as easy as possible for a new user to add her/his GNN model (for future comparison).\n\nEnjoy!']",https://arxiv.org/abs/2003.00982,"In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of May 2022, the GitHub repository has reached 1,800 stars and 339 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting. ",Benchmarking Graph Neural Networks
123,1248135650848550913,479249561,Makoto Morishita,['Our new paper is now available on arXiv.\nIt is time-consuming to manually recover an ICT system from unexpected errors.\nWe find the seq2seq model trained with error logs and recovery commands pairs could help to recover the server automatically.\n<LINK>'],https://arxiv.org/abs/2003.10784,"With the increase in scale and complexity of ICT systems, their operation increasingly requires automatic recovery from failures. Although it has become possible to automatically detect anomalies and analyze root causes of failures with current methods, making decisions on what commands should be executed to recover from failures still depends on manual operation, which is quite time-consuming. Toward automatic recovery, we propose a method of estimating recovery commands by using Seq2Seq, a neural network model. This model learns complex relationships between logs obtained from equipment and recovery commands that operators executed in the past. When a new failure occurs, our method estimates plausible commands that recover from the failure on the basis of collected logs. We conducted experiments using a synthetic dataset and realistic OpenStack dataset, demonstrating that our method can estimate recovery commands with high accuracy. ","Recovery command generation towards automatic recovery in ICT systems by
  Seq2Seq learning"
124,1245671152124600320,302624082,Grushin,"['Amorphous solids are not as random! In our new work we studied, with Q. Marsal and @danielvarjas how to describe topological phases in amorphous lattices! Check this 🧵 and the paper on the arxiv: <LINK> <LINK>', 'So, solids are made by atoms. A gazillion of them. Many scientists usually like them better when they are nice an ordered in a periodic structure we call a crystal. They are nice clean and symmetric so we can apply nice mathematical theorems to study them.', 'Now, most of things around you are not nice in this sense. Most of the materials you see are not in their crystalline form, but rather in some amorphous or disordered version. So, are these outside nice theorems and mathematics?', 'The GOD of disorder was Phil Anderson, @NobelPrize who passed away last Sunday at 96 (still going to the office a few weeks ago!). From him we learned, among many things, that disorder can be studied, and can have beautiful mathematical structures.', 'In this work we follow this spirit, but now the disorder is in the lattice that forms the solid. The atoms are arranged aperiodially, but not random!', 'Amorphous solids actually have some order. The atoms in them see an environment that is actually quite similar to their crystal counterparts. It is only at long length scales (the whole  material) where we can see that something is different compared to a crystal!', 'So, what we did in this paper was to use this ‘short range order’ to find phases with beautiful mathematical structure, topological phase, that are typically associated to crystals, this time in amorphous solids!', 'We are not the first to study amorphous Topological phases but we found a way to use this hidden order to predict when they appear. We think it can eventually open a way to classify them, something that is quite challenging compared to the beautiful symmetric crystals.', 'I have discussed topological phases in the past. They could be useful for a bunch of technological progress, mostly because of their robustness, even to a completely disordered lattice like in our case. Basically: Nice new material property + topology = robust technology (maybe)', 'So, now is time to push this to its full potential! Amorphous materials are everywhere, so let’s make topology be everywhere too! 💪🏼🥳']",https://arxiv.org/abs/2003.13701,"Amorphous solids remain outside of the classification and systematic discovery of new topological materials, partially due to the lack of realistic models that are analytically tractable. Here we introduce the topological Weaire-Thorpe class of models, which are defined on amorphous lattices with fixed coordination number, a realistic feature of covalently bonded amorphous solids. Their short-range properties allow us to analytically predict spectral gaps. Their symmetry under permutation of orbitals allows us to analytically compute topological phase diagrams, which determine quantized observables like circular dichroism, by introducing symmetry indicators for the first time in amorphous systems. These models and our procedures to define invariants are generalizable to higher coordination number and dimensions, opening a route towards a complete classification of amorphous topological states in real space using quasilocal properties. ",Topological Weaire-Thorpe models of amorphous matter
125,1245171087744344064,2774186330,Lana Garmire 🌶,"['Nevertheless, we persisted! arXiv accepted the preprint of My wonderful postdoc Dr Bing He. He made #drug reposition predictions to treat #lung injury due to COVID, by targetting #ACE2. We propose 6 drugs, 5 of 6 have prior clinical/experiment record. 1/n\n<LINK>']",https://arxiv.org/abs/2003.14333,"Coronavirus disease (COVID-19) is an infectious disease discovered in 2019 and currently in outbreak across the world. Lung injury with severe respiratory failure is the leading cause of death in COVID-19, brought by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). However, there still lacks efficient treatment for COVID-19 induced lung injury and acute respiratory failure. Inhibition of Angiotensin-converting enzyme 2 (ACE2) caused by spike protein of SARS-CoV-2 is the most plausible mechanism of lung injury in COVID-19. We propose two candidate drugs, COL-3 (a chemically modified tetracycline) and CGP-60474 (a cyclin-dependent kinase inhibitor), for treating lung injuries in COVID-19, based on their abilities to reverse the gene expression patterns in HCC515 cells treated with ACE2 inhibitor and in human COVID-19 patient lung tissues. Further bioinformatics analysis shows that twelve significantly enriched pathways (P-value <0.05) overlap between HCC515 cells treated with ACE2 inhibitor and human COVID-19 patient lung tissues, including signaling pathways known to be associated with lung injury such as TNF signaling, MAPK signaling and Chemokine signaling pathways. All these twelve pathways are targeted in COL-3 treated HCC515 cells, in which genes such as RHOA, RAC2, FAS, CDC42 have reduced expression. CGP-60474 shares eleven of twelve pathways with COL-3 with common target genes such as RHOA. It also uniquely targets genes related to lung injury, such as CALR and MMP14. In summary, this study shows that ACE2 inhibition is likely part of the mechanisms leading to lung injury in COVID-19, and that compounds such as COL-3 and CGP-60474 have the potential as repurposed drugs for its treatment. ",Prediction of repurposed drugs for treating lung injury in COVID-19
126,1244541694298775552,279730366,David Montes,['Today in this paper (Klutsch et al. <LINK>) the results of our detailed study of the Cepheus association (including the CO Cepheus void) where we identify two distinct populations of young stars that are spatially and kinematically separated. <LINK>'],https://arxiv.org/abs/2003.12155,"Young field stars are hardly distinguishable from older ones because their space motion rapidly mixes them with the stellar population of the Galactic plane. Nevertheless, a careful target selection allows for young stars to be spotted throughout the sky. We aim to identify additional sources associated with the four young comoving stars that we discovered towards the CO Cepheus void and to provide a comprehensive view of the Cepheus association. Based on multivariate analysis methods, we have built an extended sample of 193 young star candidates, which are the optical and infrared counterparts of ROSAT All-Sky Survey and XMM-Newton X-ray sources. From optical spectroscopic observations, we measured their radial velocity with the cross-correlation technique. We derived their atmospheric parameters and projected rotational velocity with the code ROTFIT. We applied the subtraction of inactive templates to measure the lithium equivalent width, from which we infer their lithium abundance and age. Finally, we studied their kinematics using the second Gaia data release. Our sample is mainly composed of young or active stars and multiple systems. We identify two distinct populations of young stars that are spatially and kinematically separated. Those with an age between 100 and 300 Myr are mostly projected towards the Galactic plane. In contrast, 23 of the 37 sources younger than 30 Myr are located in the CO Cepheus void, and 21 of them belong to the stellar kinematic group that we previously reported in this sky area. We report a total of 32 bona fide members and nine candidates for this nearby (distance = 157$\pm$10 pc) young (age = 10-20 Myr) stellar association. According to the spatial distribution of its members, the original cluster is already dispersed and partially mixed with the local population of the Galactic plane. ","Discovery of new members of the nearby young stellar association in
  Cepheus"
127,1243273243450097665,869532833764831232,Thibaut Vidal,"['Random forests made simple: 👉""...we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble"" #explainableAI\n<LINK> <LINK>']",https://arxiv.org/abs/2003.11132,"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise. ",Born-Again Tree Ensembles
128,1243238142397812736,572479189,Manlio De Domenico,"['Break from #COVID19 to share this great work led by @GiuliaTtt in coll. w/ @ricgallotti \nWe propose a physically grounded way to calculate efficiency of network flows, and show its relevance for functional percolation as opposed to structural percolation.\n\n<LINK> <LINK>', 'In parallel, our model w/ @egaltmann for bursts in collective attention just published! https://t.co/FE399RzTwm\n\nWe make publicly available the multiplex networks: 26M+ social links among 10M+ users during 9 exceptional events. Enjoy!\nGithub: https://t.co/fZBoZ1Ky4g https://t.co/Qm77r63w2G']",https://arxiv.org/abs/2003.11374,"Network science enables the effective analysis of real interconnected systems, characterized by a complex interplay between topology and interconnections strength. It is well-known that the topology of a network affects its resilience to failures or attacks, as well as its functions. Exchanging information is crucial for many real systems: the internet, transportation networks and the brain are key examples. Despite the introduction of measures of efficiency to analyze network flows, i.e. topologies characterized by weighted connectivity, here we show that they fail to capture combined information of link existence and link weight. In this letter we propose a physically-grounded estimator of flow efficiency which can be computed for every weighted network, regardless from the scale and nature of weights and from any (missing) metadata. Remarkably, results show that our estimator captures the heterogeneity of flows along with topological differences and its complement information obtained from percolation analysis of several empirical systems, including transportation, trade, migrations, and brain networks. We show that cutting the heaviest connections may increase the average communication efficiency of the system and hence, counterintuively, a sparser network is not necessarily less efficient. Remarkably, our estimator enables the comparison of communication efficiency of networks arising from different fields, without the possible pitfalls deriving from the scale of flow. ",Quantifying efficient information exchange in real network flows
129,1243171122293727234,461122309,Ali Senhaji,"['🔥 Check out our recent paper: \n""Not all domains are equally complex: Adaptive Multi-Domain Learning""\nWe propose an adaptive parameterization approach to deep neural networks for multi-domain learning.\n📝 <LINK> <LINK>']",https://arxiv.org/abs/2003.11504,"Deep learning approaches are highly specialized and require training separate models for different tasks. Multi-domain learning looks at ways to learn a multitude of different tasks, each coming from a different domain, at once. The most common approach in multi-domain learning is to form a domain agnostic model, the parameters of which are shared among all domains, and learn a small number of extra domain-specific parameters for each individual new domain. However, different domains come with different levels of difficulty; parameterizing the models of all domains using an augmented version of the domain agnostic model leads to unnecessarily inefficient solutions, especially for easy to solve tasks. We propose an adaptive parameterization approach to deep neural networks for multi-domain learning. The proposed approach performs on par with the original approach while reducing by far the number of parameters, leading to efficient multi-domain learning solutions. ",Not all domains are equally complex: Adaptive Multi-Domain Learning
130,1242479062746116096,1169068112177745922,Alexis Plascencia,"['The paper with Pavel @fileviez, Clara Murgui and Elliot Golias is out 😀 We studied constraints from SM Higgs decays into new leptophobic gauge bosons ZB and the associated production of the Higgs with a ZB at the LHC \n\n<LINK> <LINK>']",https://arxiv.org/abs/2003.09426,"The Higgs boson could provide the key to discover new physics at the Large Hadron Collider. We investigate novel decays of the Standard Model (SM) Higgs boson into leptophobic gauge bosons which can be light in agreement with all experimental constraints. We study the associated production of the SM Higgs and the leptophobic gauge boson that could be crucial to test the existence of a leptophobic force. Our results demonstrate that it is possible to have a simple gauge extension of the SM at the low scale, without assuming very small couplings and in agreement with all the experimental bounds that can be probed at the LHC. ",The Higgs and Leptophobic Force at the LHC
131,1242456057714216962,1406622804,Luis M. A. Bettencourt,['Perhaps not surprising: we find that the attack rate of #COVID19  is larger in bigger cities in the US: <LINK> : you see in NYC already.\nThis requires place-appropriate measures and nuance. \nMuch at stake in the next few days to #bendthecurve'],https://arxiv.org/abs/2003.10376,"The current outbreak of novel coronavirus disease 2019 (COVID-19) poses an unprecedented global health and economic threat to interconnected human societies. Until a vaccine is developed, strategies for controlling the outbreak rely on aggressive social distancing. These measures largely disconnect the social network fabric of human societies, especially in urban areas. Here, we estimate the growth rates and reproductive numbers of COVID-19 in US cities from March 14th through March 19th to reveal a power-law scaling relationship to city population size. This means that COVID-19 is spreading faster on average in larger cities with the additional implication that, in an uncontrolled outbreak, larger fractions of the population are expected to become infected in more populous urban areas. We discuss the implications of these observations for controlling the COVID-19 outbreak, emphasizing the need to implement more aggressive distancing policies in larger cities while also preserving socioeconomic activity. ",COVID-19 attack rate increases with city size
132,1242386785969278976,1223595907326926850,Daniela Galarraga-Espinosa,['My first paper is now on the arXiv! We studied filaments in numerical simulations and we discovered 2 different populations.\nFind out more here 👉<LINK> <LINK>'],https://arxiv.org/abs/2003.09697,"We present a statistical study of the filamentary structures of the cosmic web in the large hydro-dynamical simulations Illustris-TNG, Illustris, and Magneticum at redshift z=0. We focus on the radial distribution of the galaxy density around filaments detected using the Discrete Persistent Structure Extractor (DisPerSE). We show that the average profile of filaments presents an excess of galaxy density (> 5 sigma) up to radial distances of 27 Mpc from the core. The relation between galaxy density and the length of filaments is further investigated showing that short (L_f < 9 Mpc) and long (L_f > 20 Mpc) filaments are two statistically different populations. Short filaments are puffier, denser, and more connected to massive objects, whereas long filaments are thinner, less dense, and more connected to less massive structures. These two populations trace different environments and may correspond to bridges of matter between over-dense structures (short filaments), and to cosmic filaments shaping the skeleton of the cosmic web (long filaments). Through Markov Chain Monte Carlo (MCMC) explorations, we find that the density profiles of both short and long filaments can be described by the same empirical models (generalised Navarro, Frenk and White, beta-model, a single and a double power law) with different and distinct sets of parameters. ","Populations of filaments from the distribution of galaxies in numerical
  simulations"
133,1240749604221489154,625899636,Amir Gholami,"['Ever wondered why BN is not used in NLP? \nWe found that NLP batch statistics exhibit *large variance* throughout training, which leads to poor BN performance. To address this, we propose Power Norm that achieves SOTA vs. LN/BN.\n[1/5]\n<LINK> <LINK>', 'Transformers use *Layer Norm* instead of *Batch Norm*, as the latter leads to low performance.\nHowever, Batch Norm gives the best results in CV vs. Layer Norm.  Why is this the case?\n[2/5]', 'We performed a systematic study of normalization, and found subtle issues that had not been observed before. Importantly, input/activation distribution statistics play a major role. A naive use of BN in NLP leads to diverging statistics.\n[3/5] https://t.co/VgPryUQayq', 'Based on our observations, we propose PN which outperforms both LN and BN, exceeding state-of-the-art for a wide range of tasks.\n[4/5]', 'So how does PN address the problem? It reduces the variance of statistics in forward/backward pass by:\n(i) Relaxing zero-mean normalization and replacing variance with quadratic mean,\n(ii) Using running statistics for the quadratic mean in FP and a novel approximated BP. \n[5/5] https://t.co/6edFSgMBEk']",https://arxiv.org/abs/2003.07845,"The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \url{this https URL}. ",PowerNorm: Rethinking Batch Normalization in Transformers
134,1240730615344844801,1103577756909023232,Sheng Shen,"['Ever wondered why BN is not used in NLP? \nWe found that NLP batch statistics exhibit *large variance* throughout training, which leads to poor BN performance. To address this, we propose Power Norm that achieves SOTA vs. LN/BN. 👇\n[1/5]\n<LINK>', 'Transformers use *Layer Norm* instead of *Batch Norm*, as the latter leads to low performance.\nHowever, Batch Norm gives the best results in CV vs. Layer Norm.  Why is this the case?\n[2/5] https://t.co/rQaYIJrH3t', 'We performed a systematic study of normalization, and found subtle issues that had not been observed before. Importantly, input/activation distribution statistics play a major role. A naive\nuse of BN in NLP leads to diverging statistics.\n[3/5] https://t.co/cf6OgCNyC3', 'Based on our observations, we propose PN which outperforms both LN and BN, exceeding state-of-the-art for a wide range of tasks.\n[4/5] https://t.co/gpypEJCiTk', 'So how does PN address the problem? It reduces the variance of statistics in forward/backward pass by:\n(i) Relaxing zero-mean normalization and replacing variance with quadratic mean,\n(ii) Using running statistics for the quadratic mean in FP and a novel approximated BP. \n[5/5] https://t.co/er9OZ3j5Rs', 'See all this in:\n\n""Rethinking Batch Normalization in Transformers""\n\nBy @shengs1123, @ZheweiYao, @a__gholami, Michael Mahoney, Kurt Keutzer\n\nPaper: https://t.co/lpSJker4wV', '@arankomatsuzaki Thanks for reaching out and kind words! We will definitely opensource our code once it get accepted. But several points may lead to a better implements are (i) exclude the padding when computing the statistics. (ii) have a EMA for gz with ema_gz += g*z - (1-alpha_bwd)*ema_gz*z*z;']",https://arxiv.org/abs/2003.07845,"The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \url{this https URL}. ",PowerNorm: Rethinking Batch Normalization in Transformers
135,1240160968312840198,14422882,Eduardo ⭔,"['«Kummer surfaces for primality testing», my latest paper to find primes geometrically. \n\nHere we construct &amp; do arithmetic explicitly with the Kummer surface associated to the Jacobian of a genus 2 curve.\nAlgorithms &amp; source via my github. #math #mathchat\n<LINK> <LINK>']",http://arxiv.org/abs/2003.06490,"We use the arithmetic of the Kummer surface associated to the Jacobian of a hyperelliptic curve to study the primality of integers of the form $4m^2 5^n-1$. We provide an algorithm capable of proving the primality or compositeness of most of the integers in these families and discuss in detail the necessary steps to implement this algorithm in a computer. Although an indetermination is possible, in which case another choice of initial parameters should be used, we prove that the probability of reaching this situation is exceedingly low and decreases exponentially with $n$. ",Kummer surfaces for primality testing
136,1238538288501317635,500364862,Sergey Dorogovtsev,"['Our new work: G. Timár, R. A. da Costa, S. N. Dorogovtsev, J. F. F. Mendes, ""Choosing among alternative histories of a tree""\n<LINK>\nWe propose a fast algorithm for finding the most likely histories of very large growing trees\n&lt;ln N_histories&gt; \\cong N ln N − c N']",https://arxiv.org/abs/2003.04378,"The structure of an evolving network contains information about its past. Extracting this information efficiently, however, is, in general, a difficult challenge. We formulate a fast and efficient method to estimate the most likely history of growing trees, based on exact results on root finding. We show that our linear-time algorithm produces the exact stepwise most probable history in a broad class of tree growth models. Our formulation is able to treat very large trees and therefore allows us to make reliable numerical observations regarding the possibility of root inference and history reconstruction in growing trees. We obtain the general formula $\langle \ln \mathcal{N} \rangle \cong N \ln N - cN$ for the size-dependence of the mean logarithmic number of possible histories of a given tree, a quantity that largely determines the reconstructability of tree histories. We also reveal an uncertainty principle: a relationship between the inferrability of the root and that of the complete history, indicating that there is a tradeoff between the two tasks; the root and the complete history cannot both be inferred with high accuracy at the same time. ",Choosing among alternative histories of a tree
137,1238036772661202944,468764094,W. Quattrociocchi,"['Comparing different social media (Instagram, YT, Twitter, Gab and Reddit) we find specific amplification dynamics and that misinformation spreading is peculiar of the interaction patterns of the medium\n\nThe #COVID19 Social Media Infodemic\n<LINK> <LINK>']",https://arxiv.org/abs/2003.05004,"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction numbers $R_0$ for each social media platform. Moreover, we characterize information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification. ",The COVID-19 Social Media Infodemic
138,1237705388495159296,631176159,Till Sawala,"['Where exactly does the information for a particular #darkmatter halo exist in the primordial density field? \n\nTogether with colleagues from @helsinkiuni,  @DarkerMatters, @TheOKC &amp; @astroIAP, we went to find out at <LINK> <LINK>']",https://arxiv.org/abs/2003.04321,"We study structure formation in a set of cosmological simulations to uncover the scales in the initial density field that gave rise to the formation of present-day structures. Our simulations share a common primordial power spectrum (here Lambda-CDM), but the introduction of hierarchical variations of the phase information allows us to systematically study the scales that determine the formation of structure at later times. We consider the variance in z=0 statistics such as the matter power spectrum and halo mass function. We also define a criterion for the existence of individual haloes across simulations, and determine what scales in the initial density field contain sufficient information for the non-linear formation of unique haloes. We study how the characteristics of individual haloes such as the mass and concentration, as well as the position and velocity, are affected by variations on different scales, and give scaling relations for haloes of different mass. Finally, we use the example of a cluster-mass halo to show how our hierarchical parametrisation of the initial density field can be used to create variants of particular objects. With properties such as mass, concentration, kinematics and substructure of haloes set on distinct and well-determined scales, and its unique ability to introduce variations localised in real space, our method is a powerful tool to study structure formation in cosmological simulations. ",Setting the Stage: Structures from Gaussian Random Fields
139,1237691225437868032,1032283657015316480,Josh Dorrington,"['Very happy to say my new paper with Kristian Strommen on circulation regimes is now submitted to GRL and up on the arxiv: <LINK>\n\nWe find removing the influence of jet speed on geopotential makes regimes far easier to find and more robust!', ""@AndrewIWilliams Aha it's from an admittedly rather old book... ;D \n\nhttps://t.co/8UYPIpJnlg""]",https://arxiv.org/abs/2003.04871,"Euro-Atlantic regimes are typically identified using either the latitude of the eddy-driven jet, or clustering algorithms in the phase space of 500hPa geopotential height (Z500). However, while robust trimodality is visibly apparent in jet latitude indices, Z500 clusters require highly sensitive significance tests to distinguish them from autocorrelated noise. As a result, even small shifts in the time-period considered can notably alter the diagnosed regimes. Fixing the optimal regime number is also hard to justify. We argue that the jet speed, a near-Gaussian distribution projecting strongly onto the Z500 field, is the source of this lack of robustness. Once its influence is removed, the Z500 phase space becomes visibly non-Gaussian, and clustering algorithms easily recover three extremely stable regimes, corresponding to the jet latitude regimes. Further analysis supports the existence of two additional regimes, corresponding to a tilted and split jet. This framework therefore naturally unifies the two regime perspectives. ","Through a Jet Speed Darkly: The Emergence of Robust Euro-Atlantic
  Regimes in the Absence of Jet Speed Variability"
140,1237644677828022274,3232627976,Khyati Malhan,"['""Measuring the Matter Density of the Galactic Disk Using Stellar Streams"" today on arXiv by our ""Galaxy &amp; Gaia"" group @TheOKC . <LINK>\nWith A. Widmark, @PFdeSalas  and S. Sivertsson, we propose a new technique to infer disk surface density using star streams. <LINK>']",https://arxiv.org/abs/2003.04318,"We present a novel method for determining the total matter surface density of the Galactic disk by analysing the kinematics of a dynamically cold stellar stream that passes through or close to the Galactic plane. The method relies on the fact that the vertical component of energy for such stream stars is approximately constant, such that their vertical positions and vertical velocities are interrelated via the matter density of the Galactic disk. By testing our method on mock data stellar streams, with realistic phase-space dispersions and Gaia uncertainties, we demonstrate that it is applicable to small streams out to a distance of a few kilo-parsec, and that the surface density of the disk can be determined to a precision of 6 %. This method is complementary to other mass measurements. In particular, it does not rely on any equilibrium assumption for stars in the Galactic disk, and also makes it possible to measure the surface density to good precision at large distances from the Sun. Such measurements would inform us of the matter composition of the Galactic disk and its spatial variation, place stronger constraints on dark disk sub-structure, and even diagnose possible non-equilibrium effects that bias other types of dynamical mass measurements. ",Measuring the Matter Density of the Galactic Disk Using Stellar Streams
141,1237389095640293378,918263452698841088,KemperLab,['Preprint alert!  @Gcoslo led a new study on charge density wave dynamics in the high-Tc superconductor YBCO using time-resolved resonant X-ray scattering.  They find intriguing dynamics at the CDW peak (and we contributed a little theory).\n\n<LINK> <LINK>'],https://arxiv.org/abs/2003.04224,"In high-T$_{C}$ cuprates, superconductivity and charge density waves (CDW) are competitive, yet coexisting orders. To understand their microscopic interdependence a probe capable of discerning their interaction on its natural length and time scales is necessary. Here we use ultrafast resonant soft x-ray scattering to track the transient evolution of CDW correlations in YBa$_{2}$Cu$_{3}$O$_{6+x}$ following the quench of superconductivity by an infrared laser pulse. We observe a picosecond non-thermal response of the CDW order, characterized by a large enhancement of spatial coherence, nearly doubling the CDW correlation length, while only marginally affecting its amplitude. This ultrafast snapshot of the interaction between order parameters demonstrates that their competition manifests inhomogeneously through disruption of spatial coherence, and indicates the role of superconductivity in stabilizing topological defects within CDW domains. ","Light-enhanced Charge Density Wave Coherence in a High-Temperature
  Superconductor"
142,1237327422044979200,2656744506,Kaspar Märtens 💙💛,"['Excited to share our #AISTATS2020 paper with @cwcyau, where we propose the BasisVAE: a VAE framework for\xa0clustering features (allowing for scale and/or translation invariance).\xa0\n\nPaper: <LINK>\n\nPyTorch implementation: <LINK> <LINK>', 'Our goal is to have a joint model for \n1) dimensionality reduction (inferring latent z) and \n2) clustering features. \nSpecifically, based on the mappings z -&gt; data, we want to find features that have a similar shape (scale invariance) and those that are shifted (translation inv). https://t.co/zrHD7O49T4', 'We propose to achieve this within the VAE framework. Specifically, we modify the decoder network by embedding a probabilistic clustering prior within the last layer of the decoder. This lets us learn a one-hot basis function representation as part of the decoder network. https://t.co/p7Juamjh4S', 'Training illustrated on a toy example: (A) scale-invariant BasisVAE on the left, and (B) scale-and-translation-invariant on the right. https://t.co/U8hUtJIf2c', 'Due to the scalability of VAEs, we can use this on large-scale data sets, e.g. single-cell gene expression. On a mouse spermatogenesis data set (Ernst et al 2019) we show how BasisVAE lets us jointly learn a one-dimensional representation and cluster genes with similar shapes. https://t.co/75hUQVOqwk']",https://arxiv.org/abs/2003.03462,"Variational Autoencoders (VAEs) provide a flexible and scalable framework for non-linear dimensionality reduction. However, in application domains such as genomics where data sets are typically tabular and high-dimensional, a black-box approach to dimensionality reduction does not provide sufficient insights. Common data analysis workflows additionally use clustering techniques to identify groups of similar features. This usually leads to a two-stage process, however, it would be desirable to construct a joint modelling framework for simultaneous dimensionality reduction and clustering of features. In this paper, we propose to achieve this through the BasisVAE: a combination of the VAE and a probabilistic clustering prior, which lets us learn a one-hot basis function representation as part of the decoder network. Furthermore, for scenarios where not all features are aligned, we develop an extension to handle translation-invariant basis functions. We show how a collapsed variational inference scheme leads to scalable and efficient inference for BasisVAE, demonstrated on various toy examples as well as on single-cell gene expression data. ","BasisVAE: Translation-invariant feature-level clustering with
  Variational Autoencoders"
143,1237304435497459712,948240843911155717,Simone Rossi,"['A new #preprint  is online! In ""Rethinking Sparse Gaussian Processes: Bayesian Approaches to Inducing-Variable Approximations"" we propose to fully embrace the Bayesian philosophy for sparse #DeepGP.  \n@edvbon @MaurizioFilip19 @zheyangshen @HeinonenMarkus \n\n<LINK> <LINK>']",https://arxiv.org/abs/2003.03080,"Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems. ","Sparse Gaussian Processes Revisited: Bayesian Approaches to
  Inducing-Variable Approximations"
144,1237277579150753792,882303076505456642,Timon Emken,"['Nobody likes rejection. However, in #science rejecting a hypothesis is basically ""learning something about nature"". Our new #paper (💯% from @ChalmersPhysics) can be found on the @arxiv. We study the question ""Is #darkmatter its own anti-particle?"".\n<LINK>\n\n(1/7) <LINK>', 'We focus on a scenario where a sub-GeV DM particle has been discovered via direct detection through electron recoils mediated by higher-order photon interactions (anapole, magnetic dipole, and/or electric dipole moments). \n\n(2/7) https://t.co/JpGOK0fvs1', 'The interactions allowed by symmetry depend on whether the DM is a Dirac or Majorana fermion. \n\nFurthermore, the two hypotheses (Dirac vs Majorana) generally give rise to different predictions, in our case distinct energy spectra of ionization events, as seen here. \n\n(3/7) https://t.co/tiIq973aPw', 'Using #MonteCarlo simulations and the likelihood ratio test, we quantitatively determine the statistical significance Z (or p-value) to reject the Majorana hypothesis (our ""null"" hypothesis) for a number of benchmark scenarios as a function of observed signal events. \n\n(4/7) https://t.co/Rshhw1Ozd8', 'In the most favourable(for us at least) benchmark point denoted here as T3, the detection of only 45(120) DM-induced ionizations in a xenon target would suffice to reject the Majorana nature of DM in favour of Dirac fermions with 3(5) sigma. Other benchmarks require more. \n\n(5/7) https://t.co/pP2Gl6vs31', 'A successful rejection of the Majorana nature of sub-GeV DM would tell us that DM is not its own anti-particle. \n\nAs usual, the code used for the statistical analysis is publicly available on @github:  https://t.co/XNg7LMErd2 . #OpenScience (6/7) https://t.co/NGcBJL9QBB', 'Calculating ionization spectra for general, non-relativistic DM-electron interactions requires the knowledge of 4 atomic response functions, 3 of which we computed last year for the 1st time for isolated xenon and argon atoms: \n\nhttps://t.co/tbqOcbQ1go\n\n(7/7)', '@rittenhousewest @ChalmersPhysics @arxiv Thanks! 😃', '@saniaheba @ChalmersPhysics @arxiv Thank you 🙃', '@MagdaKersting @ChalmersPhysics @arxiv Thanks so much 😀']",https://arxiv.org/abs/2003.04039,"Assuming that Dark Matter (DM) is made of fermions in the sub-GeV mass range with interactions dominated by electromagnetic moments of higher order, such as the electric and magnetic dipoles or the anapole moment, we show that direct detection experiments searching for atomic ionisation events in xenon targets can shed light on whether DM is a Dirac or Majorana particle. Specifically, we find that between about 45 (120) and 610 (1700) signal events are required to reject Majorana DM in favour of Dirac DM with a statistical significance corresponding to 3 (5) standard deviations. The exact number of DM signal events corresponding to a given significance depends on the relative size of the anapole, magnetic dipole and electric dipole contributions to the expected rate of DM-induced atomic ionisations under the Dirac hypothesis. Our conclusions are based on Monte Carlo simulations and the likelihood ratio test. While the use of asymptotic formulae for the latter is standard in many applications, here it requires a non-trivial extension to the case where one of the hypotheses lies on the boundary of the parameter space. Our results constitute a solid proof of concept about the possibility of using direct detection experiments to reject the Majorana DM hypothesis when the DM interactions are dominated by higher-order electromagnetic moments. ","Rejecting the Majorana nature of dark matter with electron scattering
  experiments"
145,1237100449250992128,788397395608412160,Julia Mendelsohn,"['My paper with @jurafsky and Yulia Tsvetkov is on arXiv! We develop a computational linguistic framework for analyzing dehumanization, which we use to study changing representations of LGBTQ groups in the New York Times over 30 years: <LINK>']",https://arxiv.org/abs/2003.03014,"Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online. ",A Framework for the Computational Linguistic Analysis of Dehumanization
146,1236966511341899776,1030804177855959042,Michael Scherer,"['The coupling between matter and gauge fields is of fundamental importance in nature, but very challenging for theoretical modelling. In our new preprint we combine QMC and field theory to study a confinement transition of U(1)-charged Dirac fermions: <LINK>. <LINK>']",https://arxiv.org/abs/2003.01722,"The coupling between fermionic matter and gauge fields plays a fundamental role in our understanding of nature, while at the same time posing a challenging problem for theoretical modeling. In this situation, controlled information can be gained by combining different complementary approaches. Here, we study a confinement transition in a system of $N_f$ flavors of interacting Dirac fermions charged under a U(1) gauge field in 2+1 dimensions. Using Quantum Monte Carlo simulations, we investigate a lattice model that exhibits a continuous transition at zero temperature between a gapless deconfined phase, described by three-dimensional quantum electrodynamics, and a gapped confined phase, in which the system develops valence-bond-solid order. We argue that the quantum critical point is in the universality class of the QED$_3$-Gross-Neveu-XY model. We study this field theory within a $1/N_f$ expansion in fixed dimension as well as a renormalization group analysis in $4-\epsilon$ space-time dimensions. The consistency between numerical and analytical results is revealed from large to intermediate flavor number. ",Confinement transition in the QED$_3$-Gross-Neveu-XY universality class
147,1236873011073912833,49792291,Yan Zhang,"[""Combining GHOST and Casper out on arXiv: <LINK> We study an idealized version of the Ethereum 2.0 Beacon Chain. Collaboration between SJSU and the EF through SJSU's the CAMCOS program. Extra shoutout to the impressive contributions of our SJSU students!""]",https://arxiv.org/abs/2003.03052,"We present ""Gasper,"" a proof-of-stake-based consensus protocol, which is an idealized version of the proposed Ethereum 2.0 beacon chain. The protocol combines Casper FFG, a finality tool, with LMD GHOST, a fork-choice rule. We prove safety, plausible liveness, and probabilistic liveness under different sets of assumptions. ",Combining GHOST and Casper
148,1235900260133855234,1057966638748876800,Joey O'Brien,"[""New on arXiv 'A Generalization of the Classical Kelly Betting\nFormula to the Case of Temporal Correlation' - where we study how a Kelly bettor should wager if bets are no longer i.i.d. but rather exhibit temporal correlations. <LINK> <LINK>"", 'We find an analytical formula for the equivalent Kelly optimum in this case and show that in some cases the classical theory actually results in losing  money! The temporal optimum however always results in positive expected growth. https://t.co/KWNuHbcMgH', 'Really happy with this piece of work which started during a week long research visit from Prof. Bob Barmish to @MACSIMaths last November.']",https://arxiv.org/abs/2003.02743,"For sequential betting games, Kelly's theory, aimed at maximization of the logarithmic growth of one's account value, involves optimization of the so-called betting fraction $K$. In this Letter, we extend the classical formulation to allow for temporal correlation among bets. To demonstrate the potential of this new paradigm, for simplicity of exposition, we mainly address the case of a coin-flipping game with even-money payoff. To this end, we solve a problem with memory depth $m$. By this, we mean that the outcomes of coin flips are no longer assumed to be i.i.d.random variables. Instead, the probability of heads on flip $k$ depends on previous flips $k-1,k-2,...,k-m$. For the simplest case of $n$ flips, with $m = 1$, we obtain a closed form solution $K_n$ for the optimal betting fraction. This generalizes the classical result for the memoryless case. That is, instead of fraction $K^* = 2p-1$ which pervades the literature for a coin with probability of heads $p\geq 1/2$, our new fraction $K_n$ depends on both $n$ and the parameters associated with the temporal correlation. Generalizations of these results for $m > 1$ and numerical simulations are also included. Finally, we indicate how the theory extends to time-varying feedback and alternative payoff distributions. ","A Generalization of the Classical Kelly Betting Formula to the Case of
  Temporal Correlation"
149,1235405916268158976,1015190157845258240,Yuji Matsumoto,['We studied the orbital stability of planets in resonant chains again!\n<LINK>'],https://arxiv.org/abs/2003.01965,"Recent exoplanet observations reported a large number of multiple-planet systems, in which some of the planets are in a chain of resonances. The fraction of resonant systems to non-resonant systems provides clues about their formation history. We investigated the orbital stability of planets in resonant chains by considering the long-term evolution of planetary mass and stellar mass and using orbital calculations. We found that while resonant chains were stable, they can be destabilized by a change of $\sim$10% in planetary mass. Such a mass evolution can occur by atmospheric escape due to photoevaporation. We also found that resonant chains can be broken by a stellar mass loss of $\lesssim1$%, which would be explained by stellar winds or coronal mass ejections. The long-term mass change of planets and stars plays an important role in the orbital evolutions of planetary systems including super-Earths. ","Breaking Resonant Chains: Destabilization of Resonant Planets due to
  Long-term Mass Evolution"
150,1234662387724062722,830120476282408960,Nienke van der Marel,"['A new study on transition disks by @uvic @PHASTatUVIC @UVicScience PhD student Logan Francis and myself, with several new insights! In this work we focus on inner disks, detected and resolved by ALMA continuum observations. 1/5\n\n<LINK> <LINK>', '@uvic @PHASTatUVIC @UVicScience In a sample of 38 targets, 18 inner disks are detected inside the cavities and 14 of those are resolved. 9 of those are definitely misaligned with the outer disk, and the other 5 might be as well. In fact, it is possible that ALL inner disks are misaligned w.r.t. the outer disk. https://t.co/vRB8PVvtha', '@uvic @PHASTatUVIC @UVicScience Furthermore, the dust emission of the inner disks is shown to be optically thin, so we can derived dust masses. They are NOT correlated with NIR excess, which was the old method of determining inner disks. The size-luminosity relation of these disks suggest radial drift. 3/5', '@uvic @PHASTatUVIC @UVicScience With the stellar accretion rate we compute a GDR of 10^4 for most inner disks, except PDS70, the only known disk with a detected planet! We formulate a hypothesis that planets can only be detected in a disk when still accreting, and material is flowing through the gap. 4/5 https://t.co/y9MjQTlzTF', 'This amazing work was entirely done with @almaobs  ALMA archival data, with no fewer than 38 datasets or 30 Tb of data, which was downloaded and reduced with the use of @ComputeCanada 5/5']",https://arxiv.org/abs/2003.00079,"Transition disks with large inner dust cavities are thought to host massive companions. However, the disk structure inside the companion orbit and how material flows toward an actively accreting star remain unclear. We present a high resolution continuum study of inner disks in the cavities of 38 transition disks. Measurements of the dust mass from archival Atacama Large Millimeter/Submillimeter Array observations are combined with stellar properties and spectral energy distributions to assemble a detailed picture of the inner disk. An inner dust disk is detected in 18 of 38 disks in our sample. Of the 14 resolved disks, 9 are significantly misaligned with the outer disk. The near-infrared excess is uncorrelated with the mm dust mass of the inner disk. The size-luminosity correlation known for protoplanetary disks is recovered for the inner disks as well, consistent with radial drift. The inner disks are depleted in dust relative to the outer disk and their dust mass is uncorrelated with the accretion rates. This is interpreted as the result of radial drift and trapping by planets in a low $\alpha$ ($\sim 10^{-3}$) disk, or a failure of the $\alpha$-disk model to describe angular momentum transport and accretion. The only disk in our sample with confirmed planets in the gap, PDS 70, has an inner disk with a significantly larger radius and lower inferred gas-to-dust ratio than other disks in the sample. We hypothesize that these inner disk properties and the detection of planets are due to the gap having only been opened recently by young, actively accreting planets. ","Dust depleted inner disks in a large sample of transition disks through
  long-baseline ALMA observations"
151,1234654911083827200,29149902,Isaac Tamblyn,"['Reinforcement learning is COOL\n\nHere we show how PPO can rapidly find the ground state of spin-Hamiltonians, solving difficult optimization problems\n\n<LINK>']",https://arxiv.org/abs/2003.00011,"Reinforcement learning (RL) has become a proven method for optimizing a procedure for which success has been defined, but the specific actions needed to achieve it have not. We apply the so-called ""black box"" method of RL to what has been referred as the ""black art"" of simulated annealing (SA), demonstrating that an RL agent based on proximal policy optimization can, through experience alone, arrive at a temperature schedule that surpasses the performance of standard heuristic temperature schedules for two classes of Hamiltonians. When the system is initialized at a cool temperature, the RL agent learns to heat the system to ""melt"" it, and then slowly cool it in an effort to anneal to the ground state; if the system is initialized at a high temperature, the algorithm immediately cools the system. We investigate the performance of our RL-driven SA agent in generalizing to all Hamiltonians of a specific class; when trained on random Hamiltonians of nearest-neighbour spin glasses, the RL agent is able to control the SA process for other Hamiltonians, reaching the ground state with a higher probability than a simple linear annealing schedule. Furthermore, the scaling performance (with respect to system size) of the RL approach is far more favourable, achieving a performance improvement of one order of magnitude on L=14x14 systems. We demonstrate the robustness of the RL approach when the system operates in a ""destructive observation"" mode, an allusion to a quantum system where measurements destroy the state of the system. The success of the RL agent could have far-reaching impact, from classical optimization, to quantum annealing, to the simulation of physical systems. ","Controlled Online Optimization Learning (COOL): Finding the ground state
  of spin Hamiltonians with reinforcement learning"
152,1244549888186241024,1141772501472727040,Lena Voita,"['[1/3] Information-Theoretic Probing with Minimum Description Length: a new paper with @iatitov, and a blog post!\n\nWe propose an alternative to the standard probes, which is more informative, stable, and sensible.\n\nBlog: <LINK>\nPaper: <LINK> <LINK>', '[2/3] We measure minimum description length (MDL) of labels given representations. MDL naturally characterizes not only probe quality, but also *the amount of effort* needed to achieve it (or, intuitively, strength of the regularity in representations with respect to the labels). https://t.co/OxPfCuQ4Sq', '[3/3] Our MDL probing is very simple: you do not need to change much in the standard probe-training pipelines! We measure MDL while still training probing classifiers. \n\nIntrigued? Take a look at the blog or the paper :)', '@akashkm99 @EdinburghNLP @iatitov Thank you! For non-python graphs, I use Corel Draw']",https://arxiv.org/abs/2003.12298,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates ""the amount of effort"" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes. ",Information-Theoretic Probing with Minimum Description Length
153,1244548993511763969,3511309816,Bram Siebert,"['Delighted to be part of this work <LINK> where we study how modularity in complex systems can enable pattern formations. Together with @AsllaniMalbor @gleesonj @ultracrepidam funded by @IrishResearch @scienceirel <LINK>', 'In particular, we show that macroscopic spatial patterns at the modular scale can develop, which may explain how spontaneous order in biological networks follows their modular structural organisation.', 'Now displaying the full figure ;) https://t.co/gTH4w9iECu']",https://arxiv.org/abs/2003.12311,"Interconnected ensembles of biological entities are perhaps some of the most complex systems that modern science has encountered so far. In particular, scientists have concentrated on understanding how the complexity of the interacting structure between different neurons, proteins or species influences the functioning of their respective systems. It is well-established that many biological networks are constructed in a highly hierarchical way with two main properties: short average paths that join two apparently distant nodes (neuronal, species, or protein patches) and a high proportion of nodes in modular aggregations. Although several hypotheses have been proposed so far, still little is known about the relation of the modules with the dynamical activity in such biological systems. Here we show that network modularity is a key ingredient for the formation of self-organising patterns of functional activity, independently of the topological peculiarities of the structure of the modules. In particular, we propose a self-organising mechanism which explains the formation of macroscopic spatial patterns, which are homogeneous within modules. This may explain how spontaneous order in biological networks follows their modular structural organisation. We test our results on real-world networks to confirm the important role of modularity in creating macro-scale patterns. ","The role of modularity in self-organisation dynamics in biological
  networks"
154,1243650232501456896,713117884239753216,Xiaolong Wang,"['We propose simple baselines for few-shot learning, achieving much better results than fancy meta-learning approaches (<LINK>). Beyond good performance, we also conduct a detailed analysis of when meta-learning can help or hurt. \n\nCode: <LINK>']",https://arxiv.org/abs/2003.04390,"Meta-learning has been the most common framework for few-shot learning in recent years. It learns the model from collections of few-shot classification tasks, which is believed to have a key advantage of making the training objective consistent with the testing objective. However, some recent works report that by training for whole-classification, i.e. classification on the whole label-set, it can get comparable or even better embedding than many meta-learning algorithms. The edge between these two lines of works has yet been underexplored, and the effectiveness of meta-learning in few-shot learning remains unclear. In this paper, we explore a simple process: meta-learning over a whole-classification pre-trained model on its evaluation metric. We observe this simple method achieves competitive performance to state-of-the-art methods on standard benchmarks. Our further analysis shed some light on understanding the trade-offs between the meta-learning objective and the whole-classification objective in few-shot learning. ",Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning
155,1242308445828608001,321783625,Oliver Cliff,"['Our latest on modelling transmission and control of the #COVID19 pandemic in Aus. We adapt ou agent-based flu model to study #COVID19Aus intervention strategies. Results: without 80%+ compliance of social distancing, the epidemic will not be controlled.\n<LINK> <LINK>', 'We model COVID-19 with ACEMod (the Australian Census-based Epidemic Model):\nhttps://t.co/5YpzARZKec\nOur prior work showed uncontrolled epidemics in Aus spread hierarchically - from major cities to smaller towns - mainly driven by urbanisation:\nhttps://t.co/88ngma1Oxc', 'To calibrate ACEMod to COVID-19, we matched reported reproductive number (R0 1.6-2.8), length of incubation (5d) and generation (6-10d) periods, age-dependent attack rates, and growth rate of cumulative incidence during sustained, unmitigated local transmission (20-30%) https://t.co/tf2FyQVxLx', 'Results:\n- Travel ban (TB) (blue): ~50% infected\n- TB+Isolate symptomatic cases (CI) (orange): delays peak 1 week\n- TB+CI+School closures (SC) (yellow): delays peak 2 weeks\n- TB+CI+Social distancing for 91d w/ 70% compliance (purple): delays peak 12 weeks, still high infections https://t.co/O4kMNEW5rc', 'So, 70% social distancing for 91d (13 weeks) has little effect. Higher compliance:\n- 80%: &lt;1k active (b) and ~5k total (c) cases\n- 90%: &lt;&lt;1k active and ~2.5k total cases\n- 100% (lockdown): zero active cases after 49d\n\nAll SD strategies w/ compliance &lt; 100% resurge https://t.co/NSLF2KeBbI', 'Take away: There is a trade-off between social distancing (SD) compliance and the duration of the SD strategy but we require a compliance of at least 80% for effective control of #COVID19. Even with 90%+ compliance for 91d, the pandemic resurges after relaxing SD.', '@EngBakir The number of cases is low at the start of the epidemic, so any small fluctuations mean a large oscillation in growth rate. Cf. with real rates in Fig. 10 in our paper or here:\nhttps://t.co/gyKUoXG0eE\nThe grey zone is when intervention is in place - so growth immediately slows', ""@DrBreaky Thanks, Breaky. Yes, as we discuss, school closures seem to have a similar effect to ~10% higher SD.\n\nUnfortunately, the original ACEMod code is under an IP agreement with USyd. I'm checking what that means for sharing it for non-commercial use - will updated when I know""]",https://arxiv.org/abs/2003.10218,"There is a continuing debate on relative benefits of various mitigation and suppression strategies aimed to control the spread of COVID-19. Here we report the results of agent-based modelling using a fine-grained computational simulation of the ongoing COVID-19 pandemic in Australia. This model is calibrated to match key characteristics of COVID-19 transmission. An important calibration outcome is the age-dependent fraction of symptomatic cases, with this fraction for children found to be one-fifth of such fraction for adults. We apply the model to compare several intervention strategies, including restrictions on international air travel, case isolation, home quarantine, social distancing with varying levels of compliance, and school closures. School closures are not found to bring decisive benefits, unless coupled with high level of social distancing compliance. We report several trade-offs, and an important transition across the levels of social distancing compliance, in the range between 70% and 80% levels, with compliance at the 90% level found to control the disease within 13--14 weeks, when coupled with effective case isolation and international travel restrictions. ",Modelling transmission and control of the COVID-19 pandemic in Australia
156,1242272780000931840,164696005,Sai Vikneshwar Mani Jayaraman,"['We just uploaded our paper ""Covering the Relational Join"" on arxiv -- <LINK>. \n\nWe initiate a study of what we call the join covering problem, where in addition to the join query, we are given a Hamming distance parameter \\Delta.', 'Our goal is to find the smallest subset S of the join output J such that every tuple in J has Hamming distance at most \\Delta - 1 from some tuple in S.\n\nOur problem captures both the natural join and constructing covering codes as special cases.', 'We mainly focus on the combinatorial problem of bounding the smallest subset S for a given query and value of \\Delta.\n \nOur upper bounds are based on worst-case output size bounds for joins and exploit a well-known Hamming distance property.', 'For lower bounds, we turn to standard error-correcting codes like Reed-Solomon codes. \n\nOur main result is tight bounds* on the join cover for any query with arity at most two and any value of \\Delta. For general arity, we prove a slightly weaker result.', 'We can directly turn our combinatorial results into algorithms for computing the join cover.']",https://arxiv.org/abs/2003.09537,"In this paper, we initiate a theoretical study of what we call the join covering problem. We are given a natural join query instance $Q$ on $n$ attributes and $m$ relations $(R_i)_{i \in [m]}$. Let $J_{Q} = \ \Join_{i=1}^m R_i$ denote the join output of $Q$. In addition to $Q$, we are given a parameter $\Delta: 1\le \Delta\le n$ and our goal is to compute the smallest subset $\mathcal{T}_{Q, \Delta} \subseteq J_{Q}$ such that every tuple in $J_{Q}$ is within Hamming distance $\Delta - 1$ from some tuple in $\mathcal{T}_{Q, \Delta}$. The join covering problem captures both computing the natural join from database theory and constructing a covering code with covering radius $\Delta - 1$ from coding theory, as special cases. We consider the combinatorial version of the join covering problem, where our goal is to determine the worst-case $|\mathcal{T}_{Q, \Delta}|$ in terms of the structure of $Q$ and value of $\Delta$. One obvious approach to upper bound $|\mathcal{T}_{Q, \Delta}|$ is to exploit a distance property (of Hamming distance) from coding theory and combine it with the worst-case bounds on output size of natural joins (AGM bound hereon) due to Atserias, Grohe and Marx [SIAM J. of Computing'13]. Somewhat surprisingly, this approach is not tight even for the case when the input relations have arity at most two. Instead, we show that using the polymatroid degree-based bound of Abo Khamis, Ngo and Suciu [PODS'17] in place of the AGM bound gives us a tight bound (up to constant factors) on the $|\mathcal{T}_{Q, \Delta}|$ for the arity two case. We prove lower bounds for $|\mathcal{T}_{Q, \Delta}|$ using well-known classes of error-correcting codes e.g, Reed-Solomon codes. We can extend our results for the arity two case to general arity with a polynomial gap between our upper and lower bounds. ",Covering the Relational Join
157,1241132241197109345,1241116652013596674,Cyrille Combettes,"['Glad to share our new paper ""Boosting Frank-Wolfe by Chasing Gradients"" with @spokutta We propose to speed-up FW by moving in directions better aligned with the gradients. Turns out the progress obtained overcomes the cost of multiple oracle calls! <LINK>']",https://arxiv.org/abs/2003.06369,"The Frank-Wolfe algorithm has become a popular first-order optimization algorithm for it is simple and projection-free, and it has been successfully applied to a variety of real-world problems. Its main drawback however lies in its convergence rate, which can be excessively slow due to naive descent directions. We propose to speed up the Frank-Wolfe algorithm by better aligning the descent direction with that of the negative gradient via a subroutine. This subroutine chases the negative gradient direction in a matching pursuit-style while still preserving the projection-free property. Although the approach is reasonably natural, it produces very significant results. We derive convergence rates $\mathcal{O}(1/t)$ to $\mathcal{O}(e^{-\omega t})$ of our method and we demonstrate its competitive advantage both per iteration and in CPU time over the state-of-the-art in a series of computational experiments. ",Boosting Frank-Wolfe by Chasing Gradients
158,1237366952789712897,62849728,Thiemo Fetzer 🇪🇺🇺🇦,"['How does fear of #coronavirus affect economic sentiment? May uncertainty about key features of #Covid19 exacerbate economic anxieties and contribute to panic reactions? We study this in a new paper <LINK>', '1) We exploit the rapid global spread of the #Coronavirus combined with high-frequency daily Google search activity data to explore how search patterns for four broad topics changed: #Recession, Stock market crash, Survivalism, and conspiracy theories. https://t.co/qMOLuC38wI', '2) Across the globe there has been a rise in Google searches related to these topics. We see that the arrival of #Coronavirus in a country measured as the first confirmed case or measured as the first inter-community transmission is associated with a sizable jump in such searches https://t.co/Fs0HGhBLp5', '3) Upon arrival of #Corona #Covid19, searches for these four broad topics jump up by between 18 – 58% in a country relative to the pre-Corona country-specific average search activity on these topics. https://t.co/XhWaEsd5Wo', '4) Why does this matter from a economic perspective? Over the past 5 years using quarterly data, we find that sharp increases in search activity on recession-topics is a leading indicator predicting economic slowdowns in subsequent quarters. In particular, due to lower C. https://t.co/Q4hDCK6CQc', '5) We find that a 100% increase in recession search intensity in a quarter vis-à-vis the country-specific average intensity is associated with a 1.5 percentage points lower growth in consumption or a 1 percentage point lower growth in real GDP in the subsequent quarter.', '6) While the #coronavirus has so far been primarily considered a supply shock, aggregate demand is likely affected in response to changing human behaviour to avoid infections: we expect less service consumption, much less service sector imports (travel and tourism) etc.', '7) So what may lie behind these economic anxieties? And how can the economic fallout be limited? To shed some light on this, we conduct an online survey experiment in a nationally representative sample with around 1000 participants from the US on March 4th.', '8) We elicit peoples’ beliefs about both, #Covid19’s mortality as well as its contagiousness (R0) and contrast this with what the medical literature has documented so far (WHO recently suggested mortality of 3.4%, earlier estimates were closer to 2%). R0 was estimated around 2', '9) So what do these beliefs about mortality look like in our representative sample? They are heterogenous with 50% of respondents thinking mortality is higher than 5% (average at 19%). This seems higher than what the medical literature on #Covid19 suggests to date. https://t.co/ukFPMcZOtD', '10) Beliefs about contagiousness are also heterogenous. 50% of respondents think that every infected person infects at least 10 others. The average is substantially higher with 43. Early medical research suggest an R0 around 2 (https://t.co/1QOGIxg5lC &amp; https://t.co/dezoYAHIjp ) https://t.co/izjlNC6ltI', '11) When asking people about their economic anxieties the group of people that seem to be overestimating the contagiousness and the mortality seem to exhibit significantly higher degrees of anxiety. https://t.co/UKYg0le4cg', '12) Naturally there are many things we do not yet know about #coronavirus – including the true population value of R0 and mortality. But if the knowledge to date is any guide mortality rates around 20% (as is the average) seem off.', '13)When #coronavirus is discussed in the media, nuances around what we know and what we do not know yet are often not conveyed. Yet, often times #Coronavirus mortality gets compared to that of the flu or that of SARS. How do people respond to this information in terms of anxiety?', '14) We administer two information treatments on randomly selected subsets. In one case we inform people about estimates of the relative mortality (#covid19 has an xx times higher mortality than the flu vis-à-vis #covid19 has a xx lower mortality than #SARS).', '15) People respond to these information treatments with the group exposed to the “high mortality” frame as is commonly used in the media exacerbating higher degrees of anxieties about the #coronavirus. https://t.co/mdSyaAWGie', '16) This in turn creates higher anxieties about the economic impact of #coronavirus. We do something similar about contagiousness, informing a subset of people about the present estimates of R0 – this seems to have an effect on lowering anxieties. https://t.co/yKIbWbFc6e', '17) There are many things we do not know yet about #coronavirus, and beliefs seem very heterogenous resulting in potentially exacerbated anxieties. Making sure that new findings are transparently and clearly communicated, explaining the rationale behind decisions etc is key', '18) My own personal worry is that the last decade has seen a steady erosion of trust in institutions, science, experts and the media -- often times wilfully politically engineered. This could exacerbate the social and economic fall out from #Coronavirus.', '19) The whole paper is available here: https://t.co/uq0gcWnb8E', '@chris_breu The five percent is the median , so 50% think it’s higher.  Don’t get hung up about the five percent, it’s the fact that 50% of respondents think that it’s higher than that - this is what we refer to as overestimation.']",https://arxiv.org/abs/2003.03848,"We provide one of the first systematic assessments of the development and determinants of economic anxiety at the onset of the coronavirus pandemic. Using a global dataset on internet searches and two representative surveys from the US, we document a substantial increase in economic anxiety during and after the arrival of the coronavirus. We also document a large dispersion in beliefs about the pandemic risk factors of the coronavirus, and demonstrate that these beliefs causally affect individuals' economic anxieties. Finally, we show that individuals' mental models of infectious disease spread understate non-linear growth and shape the extent of economic anxiety. ",Coronavirus Perceptions And Economic Anxiety
