,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1194939153181487106,92839676,Reza Zadeh,['Two 3D deep learning papers from our team that had to build basic blocks for 3D  DL. Happy to see NVIDIA reducing overlaps with a library.\n\nPaper 1: <LINK>\nPaper 2: <LINK>\nNew 3DDL library: <LINK>'],https://arxiv.org/abs/1910.06302,"We describe a new approach to automated Glaucoma detection in 3D Spectral Domain Optical Coherence Tomography (OCT) optic nerve scans. First, we gathered a unique and diverse multi-ethnic dataset of OCT scans consisting of glaucoma and non-glaucomatous cases obtained from four tertiary care eye hospitals located in four different countries. Using this longitudinal data, we achieved state-of-the-art results for automatically detecting Glaucoma from a single raw OCT using a 3D Deep Learning system. These results are close to human doctors in a variety of settings across heterogeneous datasets and scanning environments. To verify correctness and interpretability of the automated categorization, we used saliency maps to find areas of focus for the model. Matching human doctor behavior, the model predictions indeed correlated with the conventional diagnostic parameters in the OCT printouts, such as the retinal nerve fiber layer. We further used our model to find new areas in the 3D data that are presently not being identified as a diagnostic parameter to detect glaucoma by human doctors. Namely, we found that the Lamina Cribrosa (LC) region can be a valuable source of helpful diagnostic information previously unavailable to doctors during routine clinical care because it lacks a quantitative printout. Our model provides such volumetric quantification of this region. We found that even when a majority of the RNFL is removed, the LC region can distinguish glaucoma. This is clinically relevant in high myopes, when the RNFL is already reduced, and thus the LC region may help differentiate glaucoma in this confounding situation. We further generalize this approach to create a new algorithm called DiagFind that provides a recipe for finding new diagnostic information in medical imagery that may have been previously unusable by doctors. ","Finding New Diagnostic Information for Detecting Glaucoma using Neural
  Networks"
1,1194123094043922433,141440459,Rod Van Meter üåª,"['Okay, v2 of our paper on Quantum Network Coding on the IBM machines is up! But be careful, not all mirrors and caches are up to date yet.\n<LINK>\nIf you see this figure, you have the new version. Accept no substitutes! <LINK>', 'Weirdly, I see only the web page for v1, but get PDF for v2, while Michal is the opposite: sees the v2 page but gets v1 PDF. Local caches or mirroring problem?', 'Changes in this version: completely rewritten abstract, intro, discussion. New data analysis (quantum concurrence), but no new raw data (machine we need has been decommissioned!). Much better read in v2, IMO, and I *liked* v1.']",https://arxiv.org/abs/1910.00815,"Quantum network coding has been proposed to improve resource utilization to support distributed computation but has not yet been put in to practice. We investigate a particular implementation of quantum network coding using measurement-based quantum computation on IBM Q processors. We compare the performance of quantum network coding with entanglement swapping and entanglement distribution via linear cluster states. These protocols outperform quantum network coding in terms of the final Bell pair fidelities but are unsuitable for optimal resource utilization in complex networks with contention present. We demonstrate the suitability of noisy intermediate-scale quantum (NISQ) devices such as IBM Q for the study of quantum networks. We also identify the factors that limit the performance of quantum network coding on these processors and provide estimates or error rates required to boost the final Bell pair fidelities to a point where they can be used for generation of genuinely random cryptographic keys among other useful tasks. Surprisingly, the required error rates are only around a factor of 2 smaller than the current status and we expect they will be achieved in the near future. ","Modeling of Measurement-based Quantum Network Coding on IBM Q Experience
  Devices"
2,1193227699956748288,66175375,Jason Wang,"['A new exoplanet orbit movie just in time for #ExoCup2019. A 4-year time-lapse of 51 Eri b, the coolest directly imaged exoplanet to date! We just published the updated orbit from the Gemini @PlanetImager in a paper led by Rob De Rosa (<LINK>). <LINK> <LINK>', ""@jonnymetts @PlanetImager This is real images! We don't have images continuously so we do have to do motion interpolation to fill in the gaps to make it look this smooth"", '@cal_promo @PlanetImager Roughly 30 years, so long but not super long', ""@Kat_Black @PlanetImager This planet is about 13 au from its star, so it's 2-3 times further away than Jupiter is from our Sun (Jupiter is at 5 au). It'll take 30 or so years to complete a orbit."", ""@gummih @PlanetImager Yep, that's the residual glare of the star that we couldn't remove in data processing. And we work very hard to remove the glare (the planet is almost a million times fainter than the star)"", '@PlanetImager I just realized the scalebar at the bottom is not 5 au, but 12 auüò≥']",https://arxiv.org/abs/1910.10169,"We present a revision to the visual orbit of the young, directly-imaged exoplanet 51 Eridani b using four years of observations with the Gemini Planet Imager. The relative astrometry is consistent with an eccentric ($e=0.53_{-0.13}^{+0.09}$) orbit at an intermediate inclination ($i=136_{-11}^{+10}$\,deg), although circular orbits cannot be excluded due to the complex shape of the multidimensional posterior distribution. We find a semi-major axis of $11.1_{-1.3}^{+4.2}$\,au and a period of $28.1_{-4.9}^{+17.2}$\,yr, assuming a mass of 1.75\,M$_{\odot}$ for the host star. We find consistent values with a recent analysis of VLT/SPHERE data covering a similar baseline. We investigated the potential of using absolute astrometry of the host star to obtain a dynamical mass constraint for the planet. The astrometric acceleration of 51~Eri derived from a comparison of the {\it Hipparcos} and {\it Gaia} catalogues was found to be inconsistent at the 2--3$\sigma$ level with the predicted reflex motion induced by the orbiting planet. Potential sources of this inconsistency include a combination of random and systematic errors between the two astrometric catalogs or the signature of an additional companion within the system interior to current detection limits. We also explored the potential of using {\it Gaia} astrometry alone for a dynamical mass measurement of the planet by simulating {\it Gaia} measurements of the motion of the photocenter of the system over the course of the extended eight-year mission. We find that such a measurement is only possible ($>98$\% probability) given the most optimistic predictions for the {\it Gaia} scan astrometric uncertainties for bright stars, and a high mass for the planet ($\gtrsim3.6$\,M$_{\rm Jup}$). ","An updated visual orbit of the directly-imaged exoplanet 51 Eridani b
  and prospects for a dynamical mass measurement with Gaia"
3,1192204258814373888,65528671,G.-A. Bilodeau,"['New paper in Expert systems with applications:\nDeep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait, \nImanne El Maachi, Guillaume-A. Bilodeau, Wassim Bouachir\n\nRead it on arXiv: <LINK>']",https://arxiv.org/abs/1910.11509,"Diagnosing Parkinson's disease is a complex task that requires the evaluation of several motor and non-motor symptoms. During diagnosis, gait abnormalities are among the important symptoms that physicians should consider. However, gait evaluation is challenging and relies on the expertise and subjectivity of clinicians. In this context, the use of an intelligent gait analysis algorithm may assist physicians in order to facilitate the diagnosis process. This paper proposes a novel intelligent Parkinson detection system based on deep learning techniques to analyze gait information. We used 1D convolutional neural network (1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed model processes 18 1D-signals coming from foot sensors measuring the vertical ground reaction force (VGRF). The first part of the network consists of 18 parallel 1D-Convnet corresponding to system inputs. The second part is a fully connected network that connects the concatenated outputs of the 1D-Convnets to obtain a final classification. We tested our algorithm in Parkinson's detection and in the prediction of the severity of the disease with the Unified Parkinson's Disease Rating Scale (UPDRS). Our experiments demonstrate the high efficiency of the proposed method in the detection of Parkinson disease based on gait data. The proposed algorithm achieved an accuracy of 98.7 %. To our knowledge, this is the state-of-the-start performance in Parkinson's gait recognition. Furthermore, we achieved an accuracy of 85.3 % in Parkinson's severity prediction. To the best of our knowledge, this is the first algorithm to perform a severity prediction based on the UPDRS. Our results show that the model is able to learn intrinsic characteristics from gait data and to generalize to unseen subjects, which could be helpful in a clinical diagnosis. ","Deep 1D-Convnet for accurate Parkinson disease detection and severity
  prediction from gait"
4,1190294691931136007,158106469,Hal Daum√© III,"[""New #nlproc working paper w/ @Triiiiiista on moving toward more gender inclusive coreference. Fundamental question: how gender biases, especially cis-normativity, are embedded in this technology.\n\n<LINK>\n\nWe're truly looking forward to feedback; please reach out! <LINK>"", '@evanmiltenburg @Triiiiiista It also uses they/their. While ""they"" is common, it\'s not the only pronoun people use. The goal was to rotate btw different pronouns in common (English) usage to be inclusive of the fact that just as not everyone identifies with he/she, neither does everyone identify with they.', ""@evanmiltenburg @Triiiiiista That makes sense, and not just for non-native English speakers. We'll think about whether we can find improved ways to thread this needle :)"", '@evanmiltenburg @Triiiiiista Maybe a good solution is to have a footnote explanation of the *choice*?', '@McCaffeteria @evanmiltenburg @Triiiiiista I think there are 2 things: ""they"" as a specific pronoun a person uses for themself, vs as a generic 3s reference\n\nWhat I was trying to express (badly) is that from a visibility perspective, using only the latter ignore that there are people who use something else for the former', '@McCaffeteria @evanmiltenburg @Triiiiiista In particular (and it\'s my bad for saying ""identify"" unclearly): as a specific reference to (in the case of the examples on p2) a hypothetical person, this hypothetical person may use any of many pronouns, just like a hyp person may use he or she.\n\n(Not sure if this is clear)', '@McCaffeteria @evanmiltenburg @Triiiiiista I think there\'s 2 schools of thought:\n1) Make everything generic -&gt; ""they"" always\n2) Rotate around, similar to how you may use ethically/geographically/etc. diverse names in examples; specific but broad coverage\nWe opted for 2, but I don\'t think it\'s the only reasonable choice']",https://arxiv.org/abs/1910.13913,"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms. ",Toward Gender-Inclusive Coreference Resolution
5,1190253424471937024,50343115,Thomas Kipf,"['The Toulouse Road Network dataset, a new benchmark for image-conditioned graph generation. Great work by (former) MSc student @davide__belli  \n\nBlog: <LINK>\nCode: <LINK>\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1910.14388,"Deep generative models for graphs have shown great promise in the area of drug design, but have so far found little application beyond generating graph-structured molecules. In this work, we demonstrate a proof of concept for the challenging task of road network extraction from image data. This task can be framed as image-conditioned graph generation, for which we develop the Generative Graph Transformer (GGT), a deep autoregressive model that makes use of attention mechanisms for image conditioning and the recurrent generation of graphs. We benchmark GGT on the application of road network extraction from semantic segmentation data. For this, we introduce the Toulouse Road Network dataset, based on real-world publicly-available data. We further propose the StreetMover distance: a metric based on the Sinkhorn distance for effectively evaluating the quality of road network generation. The code and dataset are publicly available. ",Image-Conditioned Graph Generation for Road Network Extraction
6,1190232282851946497,936008568263860224,Claudia Ratti,['Our new paper on correlators of conserved charges from lattice QCD and how to measure them in experiment is out today! Check it out: <LINK>'],https://arxiv.org/abs/1910.14592,"Like fluctuations, non-diagonal correlators of conserved charges provide a tool for the study of chemical freeze-out in heavy ion collisions. They can be calculated in thermal equilibrium using lattice simulations, and be connected to moments of event-by-event net-particle multiplicity distributions. We calculate them from continuum extrapolated lattice simulations at $\mu_B=0$, and present a finite-$\mu_B$ extrapolation, comparing two different methods. In order to relate the grand canonical observables to the experimentally available net-particle fluctuations and correlations, we perform a Hadron Resonance Gas (HRG) model analysis, which allows us to completely break down the contributions from different hadrons. We then construct suitable hadronic proxies for fluctuations ratios, and study their behavior at finite chemical potentials. We also study the effect of introducing acceptance cuts, and argue that the small dependence of certain ratios on the latter allows for a direct comparison with lattice QCD results, provided that the same cuts are applied to all hadronic species. Finally, we perform a comparison for the constructed quantities for experimentally available measurements from the STAR Collaboration. Thus, we estimate the chemical freeze-out temperature to 165 MeV using a strangeness-related proxy. This is a rather high temperature for the use of the Hadron Resonance Gas, thus, further lattice studies are necessary to provide first principle results at intermediate $\mu_B$. ","Off-diagonal correlators of conserved charges from lattice QCD and how
  to relate them to experiment"
7,1190067652254982145,1376287471,Tharindu Jayasinghe,"['New paper day!\nThe latest paper in the ASAS-SN (@SuperASASSN) series of variable stars (Paper VI) is on astro-ph tonight : <LINK>\nHere, we study an all-sky catalog of ~8,400 Delta Scuti stars, out of which ~3,300 (~40%) are new discoveries! <LINK>', 'Delta Scuti stars are pulsating variable stars (kappa mechanism), much like the classifical pulsators (RR Lyrae and Cepheids), and they follow a period-luminosity relationship (PLR). We derive PLRs for fundamental mode and overtone Delta Scuti stars using an all-sky sample! https://t.co/dwLYxHUUxK', '@NASA_TESS has observed the southern hemisphere last year, so we obtained 20 TESS light curves of new ASAS-SN Delta Scuti stars using the pipeline curated by @AstronomerPat. TESS light curves are awesome! We found out that two of these variables were in eclipsing binaries! https://t.co/BVXyCCJDA3', 'We also find that the overtone pulsators that are analyzed in this work have a dominant overtone pulsation, with most of them pulsating in the second overtone or higher modes. We also find evidence of Delta Scuti stars pulsating in a dominant fourth overtone. https://t.co/81wBb2NMw0', 'We were in for something exciting when we looked at the Galactic distribution of these stars. Through cross-matches to large spectroscopic catalogs, we see a clear trend in period-metallicity space for these stars. Longer period Delta Scuti are more metal rich! https://t.co/NgFfAjZXKM', 'Looking at the distribution of these stars above the Galactic mid-plane (Z), we see that the median periods of these variables are longer as we approach the Galactic disk, and their median periods gradually get shorter as we move away from the mid-plane... https://t.co/gaxRPzccoD', 'When we look at the same distribution for fundamental mode RR Lyrae stars, we see the exact opposite trend. However, we know that shorter period RR Lyrae have lower metallicities than those with longer periods.... https://t.co/WmzG4UTEe1', 'Thus, these observations are what we would expect of a vertical metallicity gradient in the Galactic disk! I found this quite fascinating. Shown below is an all-sky map of the Delta Scuti, showing the trend in period. https://t.co/X7MTSITdgf', '@AstronomerPat @NASA_TESS This is perfect!']",http://arxiv.org/abs/1910.14187,"We characterize an all-sky catalog of ${\sim} 8,400$ $\delta$ Scuti variables in ASAS-SN, which includes ${\sim} 3,300$ new discoveries. Using distances from \textit{Gaia} DR2, we derive period-luminosity relationships (PLRs) for both the fundamental mode and overtone pulsators in the $W_{JK}$, $V$, Gaia DR2 $G$, $J$, $H$, $K_s$ and $W_1$ bands. We find that the overtone pulsators have a dominant overtone mode, with many sources pulsating in the second overtone or higher order modes. The fundamental mode pulsators have metallicity dependent periods, with $\log_{10} (\rm P){\sim}-1.1$ for $\rm [Fe/H]<-0.3$ and $\log_{10} (\rm P){\sim}-0.9$ for $\rm [Fe/H]>0$ which leads to a period-dependant scale height. Stars with $\rm P>0.100\,d$ are predominantly located close to the Galactic disk ($\rm |Z|<0.5\,kpc$). The median period at a scale height of $\rm Z{\sim}0\, kpc$ also increases with the Galactocentric radius $\rm R$, from $\log_{10} (\rm P){\sim}-0.94$ for sources with $\rm R>9\, kpc$ to $\log_{10} (\rm P){\sim}-0.85$ for sources with $\rm R<7\, kpc$, which is indicative of a radial metallicity gradient. To illustrate potential applications of this all-sky catalog, we obtained 30 min cadence, image subtraction TESS light curves for a sample of 10 fundamental mode and 10 overtone $\delta$ Scuti stars discovered by ASAS-SN. From this sample, we identified two new $\delta$ Scuti eclipsing binaries, ASASSN-V J071855.62$-$434247.3 and ASASSN-V J170344.20$-$615941.2 with short orbital periods of $\rm P_{orb}=2.6096$ d and $\rm P_{orb}=2.5347$ d respectively. ","The ASAS-SN Catalog of Variable Stars VI: An All-Sky Sample of $\delta$
  Scuti Stars"
8,1190003479412129797,2800204849,Andrew Gordon Wilson,"['With a function-space approach to kernel learning, we can incorporate interpretable inductive biases, manage uncertainty, and discover rich representations of data. Our new #NeurIPS2019 paper, with G. Benton, W. Maddox, J. Salkey &amp; J. Albinati: <LINK> <LINK>']",https://arxiv.org/abs/1910.13565,"Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modelling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments. ",Function-Space Distributions over Kernels
9,1189918515358314502,1016991777407029249,hlflannery,"['Excited for feedback on new paper: <LINK> @GYNOBioethicist, @bertcmiller, @billgleim, @ethereumJoseph, @ConsenSys, @bihgieee_isto, @BlockchainIEEE, @EntEthAlliance, @HIMSS, @zkCapital, @HashedHealth <LINK>']",https://arxiv.org/abs/1910.12603,"We propose a novel architecture for federated learning within healthcare consortia. At the heart of the solution is a unique integration of privacy preserving technologies, built upon native enterprise blockchain components available in the Ethereum ecosystem. We show how the specific characteristics and challenges of healthcare consortia informed our design choices, notably the conception of a new Secure Aggregation protocol assembled with a protected hardware component and an encryption toolkit native to Ethereum. Our architecture also brings in a privacy preserving audit trail that logs events in the network without revealing identities. ","A blockchain-orchestrated Federated Learning architecture for healthcare
  consortia"
10,1189917209600184321,416132515,Louis Kirsch,['Meta Reinforcement Learning is good at adaptation to very similar environments. But can we meta-learn general RL algorithms?\nOur new approach MetaGenRL is able to.\nWith @vansteenkiste_s and @SchmidhuberAI \n\nPaper: <LINK>\nBlog: <LINK>'],https://arxiv.org/abs/1910.04098,"Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency. ","Improving Generalization in Meta Reinforcement Learning using Learned
  Objectives"
11,1189703223780630528,930764003277643777,Matias Quiroz,"[""New paper on Subsampling MCMC, extending our methods beyond the case of conditionally iid observations. The key is the Whittle likelihood - it's a sum in the frequency domain so we can subsample!  <LINK> @SalomoneRob @robertjk59 @matvil @ngocmath1"", 'Conditionally independent I mean (we have never required identically distributed).']",http://arxiv.org/abs/1910.13627,"Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings where the data have specific forms of independence. We propose a novel technique for speeding up MCMC for time series data by efficient data subsampling in the frequency domain. For several challenging time series models, we demonstrate a speedup of up to two orders of magnitude while incurring negligible bias compared to MCMC on the full dataset. We also propose alternative control variates for variance reduction based on data grouping and coreset constructions. ",Spectral Subsampling MCMC for Stationary Time Series
12,1189577273906728963,1103767531314180104,Yuan Zhou,"['Excited to share our recent work on a new inference strategy, ùóóùó∂ùòÉùó∂ùó±ùó≤, ùóñùóºùóªùóæùòÇùó≤ùóø, ùóÆùóªùó± ùóñùóºùó∫ùóØùó∂ùóªùó≤ (ùóóùóñùóñ), for probabilistic programs with stochastic support.\n\nWith @hyang144, @yeewhye, and @tom_rainforth. \n\nPaper <LINK>.']",https://arxiv.org/abs/1910.13324,"Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich probabilistic models. They further attempt to automate the process of drawing inferences from these models, but doing this successfully is severely hampered by the wide range of non--standard models they can express. As a result, although one can specify complex models in a universal PPS, the provided inference engines often fall far short of what is required. In particular, we show that they produce surprisingly unsatisfactory performance for models where the support varies between executions, often doing no better than importance sampling from the prior. To address this, we introduce a new inference framework: Divide, Conquer, and Combine, which remains efficient for such models, and show how it can be implemented as an automated and generic PPS inference engine. We empirically demonstrate substantial performance improvements over existing approaches on three examples. ","Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic
  Programs with Stochastic Support"
13,1189553352583061505,1041466458843230209,Mason Hargrave | Neuroscientist,['Just dropped my new paper on Monday! Check it out here:\n<LINK>'],https://arxiv.org/abs/1910.11484,"Implicit in the RNA world hypothesis is that prebiotic RNA synthesis, despite occurring in an environment without biochemical catalysts, produced the long RNA polymers which are essential to the formation of life. In order to investigate the prebiotic formation of long RNA polymers, we consider a general solution of functionally identical monomer units that are capable of bonding to form linear polymers by a step-growth process. Under the assumptions that (1) the solution is well-mixed and (2) bonding/unbonding rates are independent of polymerization state, the concentration of each length of polymer follows the geometric Flory-Schulz distribution. We consider the rate dynamics that produce this equilibrium; connect the rate dynamics, Gibbs free energy of bond formation, and the bonding probability; solve the dynamics in closed form for the representative special case of a Flory-Schulz initial condition; and demonstrate the effects of imposing a maximum polymer length. Afterwards, we derive a lower bound on the error introduced by truncation and compare this lower bound to the actual error found in our simulation. Finally, we suggest methods to connect these theoretical predictions to experimental results. ","A Polyaddition Model for the Prebiotic Polymerization of RNA and
  RNA-like Polymers"
14,1189546512491134977,1141772501472727040,Lena Voita,"['[1/3] BPE-dropout: our new paper by Ivan Provilkov and Dmitrii Emelianenko! <LINK>\n\nIn training, we corrupt segmentation procedure of BPE to produce different segmentations of the same word. In inference, we use standard BPE and outperform BPE and sentencepiece. <LINK>', '[2/3] Usually, models have a pathological behavior of token embeddings: a vast majority of closest neighbors of rare tokens are rare tokens. \n\nBut not with BPE-dropout! The embeddings are more sensible, and a model is more robust to misspellings (despite not being exposed to any) https://t.co/eQZqAzbiFI', '[3/3] Takeaway message: we show how to train models with BPE and make them more robust and up to 3 BLEU better.\n\nAnd yes, I do want to make one of my research parents a bit happy :) @RicoSennrich', '@cathalhoran @RicoSennrich  already implemented it in his subword-nmt repo (thank you, Rico!): https://t.co/CkwLHwVEuK \nYou just have to apply it with p=0.1 (or 0.6 for Ja and Zh) for each batch in training.\n\nMost probably, a bit later we will release the code we experimented with.', '@BZhangGo @cathalhoran @RicoSennrich Yes, exactly :)']",https://arxiv.org/abs/1910.13267,"Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization. ",BPE-Dropout: Simple and Effective Subword Regularization
15,1189536300338044929,4639078397,John Wise,"['New paper day! Focusing on the growth of a massive black hole at z=7.5, we find that UV radiation from the SMBH and nuclear stellar cluster regulates runaway star formation within 30 pc, promoting early rapid growth of the SMBH. Led by Ji-hoon Kim (SNU) <LINK> <LINK>', 'Our work also demonstrates the need to match the input physical models with the simulation mass/spatial resolution. Co-authors: @TomAbelStanford, Y. Jo (SNU), @JoelPrimack, @PFHopkins_Astro']",https://arxiv.org/abs/1910.12888,"As computational resolution of modern cosmological simulations reach ever so close to resolving individual star-forming clumps in a galaxy, a need for ""resolution-appropriate"" physics for a galaxy-scale simulation has never been greater. To this end, we introduce a self-consistent numerical framework that includes explicit treatments of feedback from star-forming molecular clouds (SFMCs) and massive black holes (MBHs). In addition to the thermal supernovae feedback from SFMC particles, photoionizing radiation from both SFMCs and MBHs is tracked through full 3-dimensional ray tracing. A mechanical feedback channel from MBHs is also considered. Using our framework, we perform a state-of-the-art cosmological simulation of a quasar-host galaxy at z~7.5 for ~25 Myrs with all relevant galactic components such as dark matter, gas, SFMCs, and an embedded MBH seed of ~> 1e6 Ms. We find that feedback from SFMCs and an accreting MBH suppresses runaway star formation locally in the galactic core region. Newly included radiation feedback from SFMCs, combined with feedback from the MBH, helps the MBH grow faster by retaining gas that eventually accretes on to the MBH. Our experiment demonstrates that previously undiscussed types of interplay between gas, SFMCs, and a MBH may hold important clues about the growth and feedback of quasars and their host galaxies in the high-redshift Universe. ","High-redshift Galaxy Formation with Self-consistently Modeled Stars and
  Massive Black Holes: Stellar Feedback and Quasar Growth"
16,1189460017935781888,954396146461564928,Kuznetsov Maxim,"['I‚Äôm excited to share our new paper at #Neurips2019 with @d_polykovskiy,  @alex_zhebrak, and Dmitry Vetrov made in @InSilicoMeds! \n\nA Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models: <LINK> <LINK>']",https://arxiv.org/abs/1910.13148,"Generative models produce realistic objects in many domains, including text, image, video, and audio synthesis. Most popular models---Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)---usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions---Tensor Ring Induced Prior (TRIP)---that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Fr\'echet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether, we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures. ","A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for
  Generative Models"
17,1189276793360048129,1442906958,Richard Socher,"['Summarization is one of the most important &amp; least solved tasks in #NLProc\nProblem with all #DeepLearning models: they are not optimized for factual correctness\nWe introduce a new task, dataset and model. Work by @iam_wkr @BMarcusMcCann @CaimingXiong\nPaper <LINK> <LINK>']",https://arxiv.org/abs/1910.12840,"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. ",Evaluating the Factual Consistency of Abstractive Text Summarization
18,1189260528897671169,835680883,Jason Wright,"['OK very cool new paper on the @arXiv by @sharonxuesong:\n<LINK>\n\nThis was a lot of fun. Frustrated with not understanding why we could not model our spectra, we started to suspect the FTS spectra of our iodine cell. How to check it?', '@arxiv @sharonxuesong When I first visited @mcdonaldobs I was introduced to the Tull spectrograph, including its long unused ""double pass"" arm with *many* times the resolving power of the main arm.  It was old, it only did 2 Angstroms at a time, no one used it.', '@arxiv @sharonxuesong @mcdonaldobs BUT it made a ""real"" echelle spectra and we understood how it worked. @sharonxuesong, Ming Zhao, and @AstroKimCartier spent a *long* time coaxing it back into service, and slowly ""scanning"" the iodine cell. Then reducing the data and comparing it to the FTS scan.', '.@sharonxuesong also used code that theoretically models iodine spectra and examined a new FTS scan by Gillian Nave and Stephen Redman to really understand what the iodine molecule does to starlight.', 'So if you want to understand your iodine cell, its limitations, and its temperature sensitivity, or if you have an ""ordinary"" hi-res spectrograph you want to use for EPRV work: this is the paper for you!\n\nhttps://t.co/CplKzveWSI', ""Oooh, one more tweet. We do *not* resolve iodine lines with our astronomical spectrographs. It's just a forest.  Behold, what an echellogram of 2 Angstroms of iodine looks like at resolution &gt; 400,000: https://t.co/kbGe9OOrcg"", 'Also, by ""lot of fun"" I mean ""a huge amount of drudgery and work plus ingenuity and novel methods to come to an important and unexpected conclusion of general importance to a broad community."" \n\nYou know, ""fun""!']",https://arxiv.org/abs/1910.10756,"High fidelity iodine spectra provide the wavelength and instrument calibration needed to extract precise radial velocities (RVs) from stellar spectral observations taken through iodine cells. Such iodine spectra are usually taken by a Fourier Transform Spectrometer (FTS). In this work, we investigated the reason behind the discrepancy between two FTS spectra of the iodine cell used for precise RV work with the High Resolution Spectrograph (HRS) at the Hobby-Eberly Telescope. We concluded that the discrepancy between the two HRS FTS spectra was due to temperature changes of the iodine cell. Our work demonstrated that the ultra-high resolution spectra taken by the TS12 arm of the Tull Spectrograph One at McDonald Observatory are of similar quality to the FTS spectra and thus can be used to validate the FTS spectra. Using the software IodineSpec5, which computes the iodine absorption lines at different temperatures, we concluded that the HET/HRS cell was most likely not at its nominal operating temperature of 70 degree Celsius during its FTS scan at NIST or at the TS12 measurement. We found that extremely high resolution echelle spectra (R>200,000) can validate and diagnose deficiencies in FTS spectra. We also recommend best practices for temperature control and nightly calibration of iodine cells. ",Calibrating Iodine Cells for Precise Radial Velocities
19,1189246252267003907,374233623,Shane Barratt,"['Excited to release our new paper ""Minimizing a Sum of Clipped Convex Functions"", joint work w/ @GuilleAngeris and Stephen Boyd.\n\nPaper: <LINK>\nCode: <LINK>', '@GuilleAngeris The paper provides a good heuristic for minimizing sums of clipped convex functions, as well as a computational lower bound based on the perspective transform. The figure below gives a simple 1-d example of a sum of clipped convex functions. https://t.co/Xm6NF4BSUX', '@GuilleAngeris Applications include clipped empirical risk minimization, for example, clipped regression https://t.co/dT9PbGj3ZE', '@GuilleAngeris And clipped control, where, for example, the cost encourages us to be in one of the two lanes https://t.co/Dc5L8iTpkv', '@GuilleAngeris Our algorithm requires solving roughly 5-20 convex optimization problems, and we have implemented it as a CVXPY extension, making it easy to (approximately) solve such problems https://t.co/n3azp19CWs', '@GuilleAngeris To get a lower bound, we first convert the problem into a mixed-integer convex program using the perspective formulation (https://t.co/JKaDSTMzJT) and relax the integral constraint to get a lower bound.\n(See the paper for more details.) Here is an example of the lower bound. https://t.co/y4moDiDO4N']",https://arxiv.org/abs/1910.12342,"We consider the problem of minimizing a sum of clipped convex functions; applications include clipped empirical risk minimization and clipped control. While the problem of minimizing the sum of clipped convex functions is NP-hard, we present some heuristics for approximately solving instances of these problems. These heuristics can be used to find good, if not global, solutions and appear to work well in practice. We also describe an alternative formulation, based on the perspective transformation, which makes the problem amenable to mixed-integer convex programming and yields computationally tractable lower bounds. We illustrate one of our heuristic methods by applying it to various examples and use the perspective transformation to certify that the solutions are relatively close to the global optimum. This paper is accompanied by an open-source implementation. ",Minimizing a Sum of Clipped Convex Functions
20,1189201238857879552,94412971,Wojciech Kry≈õci≈Ñski,"['New work in which we approach the problem of evaluating the factual consistency of abstractive summarization models is out! üì´üìë\n\nWork w/ @BMarcusMcCann @CaimingXiong @RichardSocher \nPaper: <LINK>\n\nKey points in thread (1/6): <LINK>', 'We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between a source document and a generated summary. (2/6) https://t.co/W3tCe2t4vn', 'Training data is generated by applying a series of rule-based transformations to sentences sampled from an unlabeled corpus of source documents. (3/6) https://t.co/DUeJDvhiLu', 'Our model is trained jointly to: \n1) identify whether sentences remain factually consistent after the transformations,\n2) extract a span in the source documents to support the consistency prediction, \n3) extract a span in the sentence that is inconsistent, if one exists. (4/6) https://t.co/rktlT9Chqd', 'Transferring our model to summaries generated by state-of-the art summarization models shows that our approach substantially outperforms other models trained with strong supervision using standard datasets for NLI and fact checking. (5/6) https://t.co/PXTcKOLOpb', 'Data and code will be available soon! (6/6)', 'This work is a follow up to our #emnlp2019 paper in which we critically evaluate the state of neural text summarization. #NLProc \n\nPaper: https://t.co/1Mgk4R6M4b']",https://arxiv.org/abs/1910.12840,"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. ",Evaluating the Factual Consistency of Abstractive Text Summarization
21,1189148821823074304,3226232932,Gianluca Detommaso,"['My new paper is out!\n""Multilevel Dimension-Independent Likelihood-Informed MCMC for Large-Scale Inverse Problems""\n<LINK>']",https://arxiv.org/abs/1910.12431,"We present a non-trivial integration of dimension-independent likelihood-informed (DILI) MCMC (Cui, Law, Marzouk, 2016) and the multilevel MCMC (Dodwell et al., 2015) to explore the hierarchy of posterior distributions. This integration offers several advantages: First, DILI-MCMC employs an intrinsic likelihood-informed subspace (LIS) (Cui et al., 2014) -- which involves a number of forward and adjoint model simulations -- to design accelerated operator-weighted proposals. By exploiting the multilevel structure of the discretised parameters and discretised forward models, we design a Rayleigh-Ritz procedure to significantly reduce the computational effort in building the LIS and operating with DILI proposals. Second, the resulting DILI-MCMC can drastically improve the sampling efficiency of MCMC at each level, and hence reduce the integration error of the multilevel algorithm for fixed CPU time. To be able to fully exploit the power of multilevel MCMC and to reduce the dependencies of samples on different levels for a parallel implementation, we also suggest a new pooling strategy for allocating computational resources across different levels and constructing Markov chains at higher levels conditioned on those simulated on lower levels. Numerical results confirm the improved computational efficiency of the multilevel DILI approach. ","Multilevel Dimension-Independent Likelihood-Informed MCMC for
  Large-Scale Inverse Problems"
22,1189005221583704064,1249492212,Karol Hausman üíôüíõ,"['New paper: simple hierarchical policy structure\xa0+ play data\xa0+ relabeling = goal-conditioned RL agent that can solve very long-horizon tasks!\n\nWebsite: <LINK>\nArxiv: <LINK>\n\nw/ A. Gupta, @Vikashplus, @coreylynch, @svlevine \n\nspotlight @ #CoRL2019 <LINK>', ""@Vikashplus @coreylynch @svlevine More videos of successful tasks below.\n\nTbh, I was quite surprised with our *sim* results for long-horizon tasks. \n\nWhat are the most impressive deep RL *sim* policies that you've ever seen? https://t.co/pi7THkOUhO""]",https://arxiv.org/abs/1910.11956,"We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to challenging long-horizon tasks. We simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of every specific tasks that is being solved, and instead leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos are available at this https URL ","Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and
  Reinforcement Learning"
23,1188924971302735879,1078559471402663936,Stefania Carpano,"[""My new paper on the luminous Super Soft Source in NGC300 is now on arXiv/astro-ph! The paper is already in press, but I'm still very interested to continue to work on the source, especially if I get new data. So feedback is welcome!\n<LINK>""]",https://arxiv.org/abs/1910.08314,"The nearby galaxy NGC 300 is hosting two luminous transient supersoft X-ray sources with bolometric luminosities above 3x10^38 erg/s, assuming simple black-body spectra with temperatures around 60-70 eV. For one of these, SSS1, a periodic modulation of 5.4h was observed in an XMM-Newton observation from 1st of January 2001 lasting 47 ks, but not visible 6 days earlier when the luminosity was higher. We report here the detection of a new outburst from this source, which occurred during two more recent XMM-Newton observations performed on 17 to 20 December 2016 lasting for 310 ks. The luminosity was similar as in December 2000, and the 0.2-2.0 keV light curve revealed again a periodic modulation, with a period of 4.68+-0.26h, significant only in the first of the two observations. Taking into account the large uncertainties (the 2001 period was re-estimated at 5.7+-1.1h), the two values could be marginally compatible, and maybe associated with an orbital period, although the signal strength is highly variable. Thanks to the new long exposures, an additional absorption feature is now visible in the spectra, that we modelled with an absorption edge. This component decreases the bolometric luminosity below 3x10^38 erg/s and would therefore allow the presence of a white dwarf with a mass close to the Chandrasekhar limit. The system was found in outburst in 1992, 2000, 2008, and 2016 suggesting a possible recurrence period of about 8 years. We discuss viable models involving white dwarfs, neutron stars or black holes. ","New outburst from the luminous supersoft source SSS1 in NGC 300 with
  periodic modulation"
24,1188874161562636288,1138901263439949824,Colin White,['BANANAS: Bayesian optimization with neural architectures for neural architecture search.\n\nPaper: <LINK>\nGithub: <LINK>\nBlog: <LINK>\n\nWith @willieneis and Yash Savani @realityengines'],https://arxiv.org/abs/1910.11858,"Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the ""BO + neural predictor"" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at this https URL ","BANANAS: Bayesian Optimization with Neural Architectures for Neural
  Architecture Search"
25,1188843496339197955,84822240,Luca Bertinetto üáÆüáπ üá™üá∫ üåê,"['New paper! ""Anchor Diffusion for Unsupervised Video Object Segmentation""\n<LINK>\n\nCode: <LINK>\n\nIf you are at #iccv2019, check our \nposter tomorrow at 10:30, Hall B, stand 97']",https://arxiv.org/abs/1910.10895,"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approaches tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators of [70], we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the ViSal video saliency dataset, showing results competitive with the state of the art. ",Anchor Diffusion for Unsupervised Video Object Segmentation
26,1188794232833740800,963779351333523456,Dr. Chloe Fisher,"['Our new paper is out on the arxiv today (<LINK>) using a combination of cross-correlations and our random forest retrieval method to perform atmospheric retrieval on high-resolution, ground-based spectra of exoplanets', 'The number of points in a HARPS-N dataset (~10^6) and the high level of noise are problems for retrieval techniques, and something very new to those used to dealing with HST data (i.e. me). We needed a way to reduce the noise and the dimensionality, without losing the information', 'Traditionally, this is where cross-correlations come in. They are usually used to detect molecules, but we wanted to see if there was a link between the shapes of the CCFs and the atmospheric properties.\n\nCCF gif from @HoeijmakersJens: https://t.co/bIdIK9oG8U', '@HoeijmakersJens We perform 64 CCFs on each model spectrum (using templates of different species, temperature, and metallicity) to create a ‚ÄúCCF-sequence‚Äù (pic). We created a grid of ~1000 noise-free spectra, mapped them to CCF-sequences, and then added noise. https://t.co/XEGTu3KI0W', '@HoeijmakersJens We trained our forest on these CCF-sequences, and then used the forest to predict on mock data and real data. Picture shows the predictions using the spectra themselves, and our CCF-sequences. https://t.co/GQMiQ2W7Ob', ""@HoeijmakersJens We then used this to retrieve on the real KELT-9b data (previous #exocup winner and obvs everyone's favourite planet). https://t.co/mHpTZXQHXU"", '@HoeijmakersJens Extremely high temperature is driven by the high Fe+ CCFs, which lie outside the training set (see pic). [Machine learning warning! Do not believe results from stuff outside your training set.] This indicates missing physics in the model. Winds? Outflow? Ideas welcome! https://t.co/KaG2XEyVFy', '@HoeijmakersJens We then removed the ion CCFs and performed the retrieval in the same way. We obtained a metallicity consistent with solar, but still a very high temperature (5600K). https://t.co/yVch0MUoAB', '@HoeijmakersJens We also investigated a degeneracy in metallicity, possible other machine learning methods, and alternative error bar distributions. https://t.co/xCc87R9dIF', '@HoeijmakersJens Finally, we also went back to our previous use of the random forest (on HST data) and showed that it is just as dependent on the model as nested-sampling/MCMC.', '@HoeijmakersJens Moral: machine learning does not replace physics. It is a helpful tool, but should be used with caution, and preferably, the assistance of some experts üôÇ\n\nhttps://t.co/7OWT9HtDNf', '@HoeijmakersJens On a different note, I just want to say how much I enjoyed this project. I learnt a huge amount, especially from @HoeijmakersJens (who also did a lot of the work) about the high-res data and observations in general. Looking forward to more projects like it!', ""@HoeijmakersJens This is also the first paper I've put on the arxiv before acceptance. It feels a little exposing but hopefully a positive experience. Looking forward to helpful feedback!""]",https://arxiv.org/abs/1910.11627,"We present a new method for performing atmospheric retrieval on ground-based, high-resolution data of exoplanets. Our method combines cross-correlation functions with a random forest, a supervised machine learning technique, to overcome challenges associated with high-resolution data. A series of cross-correlation functions are concatenated to give a ""CCF-sequence"" for each model atmosphere, which reduces the dimensionality by a factor of ~100. The random forest, trained on our grid of ~65,000 models, provides a likelihood-free method of retrieval. The pre-computed grid spans 31 values of both temperature and metallicity, and incorporates a realistic noise model. We apply our method to HARPS-N observations of the ultra-hot Jupiter KELT-9b, and obtain a metallicity consistent with solar (logM = $-0.2\pm0.2$). Our retrieved transit chord temperature (T = $6000^{+0}_{-200}$K) is unreliable as the ion cross-correlations lie outside of the training set, which we interpret as being indicative of missing physics in our atmospheric model. We compare our method to traditional nested-sampling, as well as other machine learning techniques, such as Bayesian neural networks. We demonstrate that the likelihood-free aspect of the random forest makes it more robust than nested-sampling to different error distributions, and that the Bayesian neural network we tested is unable to reproduce complex posteriors. We also address the claim in Cobb et al. (2019) that our random forest retrieval technique can be over-confident but incorrect. We show that this is an artefact of the training set, rather than the machine learning method, and that the posteriors agree with those obtained using nested-sampling. ","Interpreting High-Resolution Spectroscopy of Exoplanets Using
  Cross-Correlations and Supervised Machine Learning"
27,1188788015969255425,892059194240532480,Mikel Artetxe,"['Check out our new paper ""On the Cross-lingual Transferability of Monolingual Representations"" (w/ @seb_ruder &amp; @DaniYogatama)\n\nWe challenge common beliefs of why mBERT works by showing that a monolingual BERT can also be transferred to new languages\n\n<LINK> <LINK>']",https://arxiv.org/abs/1910.11856,"State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators. ",On the Cross-lingual Transferability of Monolingual Representations
28,1188752446350336001,549460404,ÂêâÁî∞ Á¥Ö (Beni Yoshida),"['New paper, construction of interior operator for a black hole perturbed by an infalling observer. \nÊñ∞„Åó„ÅÑË´ñÊñá„Åß„Åô„ÄÇ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„ÅÆÂÜÖÈÉ®„ÅÆÊºîÁÆóÂ≠ê„ÇíÂÖ∑‰ΩìÁöÑ„Å´Êõ∏„Åç„Åæ„Åó„Åü„ÄÇ\n<LINK>', '„Åì„Çå„Åæ„Åß„ÅÆÁ†îÁ©∂„Åß„ÅØ„ÄÅÂÜÖÈÉ®„ÅÆÊºîÁÆóÂ≠ê„ÅØÊó¢„Å´ÊîæÂá∫„Åï„Çå„ÅüÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞ÑÂÜÖ„Å´ÊßãÁØâ„Åï„Çå„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åó„Åã„Åó„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„Å´ËêΩ„Å°„Å¶„ÅÑ„ÅèË¶≥Ê∏¨ËÄÖ„ÅåÊ∏¨ÂÆö„Åô„ÇãÊºîÁÆóÂ≠ê„Åå„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„Åã„ÇâÈÅ†„ÅÑÂ†¥ÊâÄ„Å´„ÅÇ„Çã„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞ÑÂÜÖ„Å´ÂØæÂøú„Åô„Çã„ÅÆ„ÅØ„ÄÅÊéß„Åà„ÇÅ„Å´Ë®Ä„Å£„Å¶ÊÑèÂë≥‰∏çÊòé„Åß„Åô„ÄÇ', '„Åì„ÅÆÁüõÁõæ„Åã„Çâ„ÄÅ„Éï„Ç°„Ç§„É§„Éº„Ç¶„Ç©„Éº„É´ÂïèÈ°å„ÇÑÈùûÂ±ÄÊâÄÂïèÈ°åÁ≠â„ÄÅËâ≤„ÄÖ„Å™ÊÇ©„Åø„ÅÆÁ®Æ„ÅåÁîü„Åæ„Çå„Çã„ÅÆ„Åß„Åô„Åå„ÄÅÈÅéÂéª„ÅÆÁ†îÁ©∂„ÅØ„ÄÅ„Åù„ÅÜ„ÅÑ„Å£„ÅüÁüõÁõæ„ÅØË¶≥Ê∏¨„Åß„Åç„Å™„ÅÑÁâ©„Å†„Å®„Åã„ÄÅÂ∞ë„Åó„Åè„Çâ„ÅÑÈùûÂ±ÄÊâÄ„Åß„ÇÇ„ÅÑ„ÅÑ„Åò„ÇÉ„Å™„ÅÑ„Åã„ÄÅetc„Å®Ë®Ä„Å£„Åü„ÄÅÊéß„Åà„ÇÅ„Å´Ë®Ä„Å£„Å¶„ÇÇ‰Ωï„ÅÆËß£Ê±∫„Å´„ÇÇ„Å™„Å£„Å¶„ÅÑ„Å™„ÅÑÁâ©„Åß„Åó„Åü„ÄÇ', '„Åó„Åã„ÅóÈÅéÂéª„ÅÆÁ†îÁ©∂„Åß„ÅØ„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„Å´ËêΩ„Å°„Å¶„ÅÑ„ÅèË¶≥Ê∏¨ËÄÖ„Åã„Çâ„ÅÆbackreaction„Åå‰∏ÄÂàáËÄÉ„Åà„Çâ„Çå„Å¶„Åä„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ', '„Åó„Åã„Åó„Åç„Å°„Çì„Å®backreaction„ÇíËÄÉ„Åà„Çã„Å®„ÄÅÂÜÖÈÉ®ÊºîÁÆóÂ≠ê„Åå„Å°„ÇÉ„Çì„Å®ÂÜÖÈÉ®Ëá™Áî±Â∫¶„Å†„Åë„ÅßÊõ∏„Åë„Å¶ÁüõÁõæ„Åå„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ', '„Åì„Åì„Åß„ÄÅbackreaction„ÅØ„ÅÑ„Çè„ÇÜ„Çã„Çπ„ÇØ„É©„É≥„Éñ„É™„É≥„Ç∞„ÇíÁîü„ÅøÂá∫„Åó„Å¶„ÅÑ„Çã„Å®‰ªÆÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂé≥ÂØÜ„Å´„ÅØ„ÄÅout-of-time order correlator„ÅåÂ∞è„Åï„Åè„Å™„Çã„Å®„ÅÑ„ÅÜ‰∫ã„Åß„Åô„ÄÇ', '„Åù„ÅÆÊï∞Â≠¶ÁöÑ„Å™Â∏∞Áµê„Å®„Åó„Å¶„ÄÅÊñ∞„Åó„ÅèÂá∫„Å¶„Åç„Åü„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„ÅØ„ÄÅÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Å®‰∏ÄÂàá„Ç®„É≥„Çø„É≥„Ç∞„É´„Åó„Å¶„ÅÑ„Å™„ÅÑ„Åì„Å®„ÅåÂé≥ÂØÜ„Å´Ë®ºÊòé„Åß„Åç„Åæ„Åô„ÄÇ„Å™„ÅÆ„Åß„ÄÅÂÜÖÈÉ®ÊºîÁÆóÂ≠ê„ÇÇ„ÄÅÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Çí‰Ωø„Çè„Åö„Å´ÊßãÊàê„Åß„Åç„Åæ„Åô„ÄÇ', '„Å°„Å™„Åø„Å´ÊßãÊàêÊñπÊ≥ï„ÅØ„ÄÅ„Éò„Ç§„Éá„É≥„Éó„É¨„Çπ„Ç≠„É´ÊÄùËÄÉÂÆüÈ®ì„Çí‰Ωø„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅ‰Ωø„ÅÑÊñπ„Å´Â∑•Â§´„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ', '„Äå„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„ÅåÂá∫„Å¶„Åè„Çã„Äç„Å®„ÅÑ„ÅÜÁâ©ÁêÜÈÅéÁ®ã„ÇíÈÄÜÂõû„Åó„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ„Åô„Çã„Å®„ÄÅ„Äå„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Åå„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„Å´ËêΩ„Å°„Å¶„ÅÑ„Åè„Äç„Åì„Å®„Å´„Å™„Çä„Åæ„Åô„ÄÇ', '„Åì„ÅÆ„ÄÅÊôÇÈñìÈÄÜÂõû„Åó„ÅßËêΩ„Å°„Å¶„ÅÑ„Åè„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Çí„ÄÅ„Éò„Ç§„Éá„É≥„Éó„É¨„Çπ„Ç≠„É´„Å´„Åä„Åë„Çã„ÄÅ„Äå„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„Å´Âê∏„ÅÑËæº„Åæ„Çå„ÅüÈáèÂ≠êÊÉÖÂ†±„Äç„Å®Êçâ„Åà„Çå„Å∞„ÄÅ„Åù„Çå„Çí„É™„Ç´„Éê„Éº„Åô„ÇãÊñπÊ≥ï„ÅåËá™ÁÑ∂„Å®ÂØæ„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÂÜÖÈÉ®ÊºîÁÆóÂ≠ê„Çí‰∏é„Åà„Å¶„Åè„Çå„Åæ„Åô„ÄÇ', '„Åï„Å¶„ÄÅ„Åì„Çå„Çâ„ÅÆÊßãÊàê„ÅØÂΩìÁÑ∂„ÄÅ„Éï„Ç°„Ç§„É§„Éº„Ç¶„Ç©„Éº„É´ÂïèÈ°å„ÅÆËß£Ê±∫ÊñπÊ≥ï„ÇÇÁ§∫ÂîÜ„Åó„Åæ„Åô„ÄÇ', 'ÂÖÉ„ÄÖ„ÅÆË≠∞Ë´ñ„ÅØ„ÄÅÊñ∞„Åó„ÅèÂá∫„Å¶„Åç„Åü„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„ÅØ„ÄÅÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Å®„Ç®„É≥„Çø„É≥„Ç∞„É´„Åó„Å¶„ÅÑ„Çã„ÅÆ„Åß„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´ÂÜÖÈÉ®„ÅÆÊºîÁÆóÂ≠ê„Å®„ÅØ„Ç®„É≥„Çø„É≥„Ç∞„É´„Åß„Åç„Å™„ÅÑ„ÅØ„Åö„ÄÇ„Å™„ÅÆ„Åß„ÄÅHorizon‰∏ä„Å´„Éï„Ç°„Ç§„É§„Éº„Ç¶„Ç©„Éº„É´„Åå„ÅÇ„Çã„ÅØ„Åö„Å†„ÄÅ„Åß„Åó„Åü„ÄÇ', '„Åó„Åã„Åó„ÄÅË¶≥Ê∏¨ËÄÖ„ÅÆbackreaction„ÇíËÄÉ„Åà„Çå„Å∞„ÄÅÊñ∞„Åó„ÅèÂá∫„Å¶„Åç„Åü„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Å®ÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„ÅÆ„Ç®„É≥„Çø„É≥„Ç∞„É´„É°„É≥„Éà„ÅØ„ÄÅÂãïÁöÑ„Å´Âàá„Çå„Å¶„Åó„Åæ„ÅÜ„ÅÆ„Åß„ÄÅÂÜÖÈÉ®„ÅÆÊºîÁÆóÂ≠ê„Å®„ÅØ„Ç®„É≥„Çø„É≥„Ç∞„É´„Åó„Å¶„ÇÇÁüõÁõæ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ', '„Å°„Å™„Åø„Å´„Åì„ÅÆË≠∞Ë´ñ„ÅØ„ÄÅ„ÅÑ„Çè„ÇÜ„ÇãER=EPRÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅ„Åå„Å™„ÅúÈñìÈÅï„Å£„Å¶„ÅÑ„Çã„Åã„ÇíÁ§∫ÂîÜ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇË¶≥Ê∏¨ËÄÖ„ÅÆbackreaction„ÇíËÄÉ„Åà„Çå„Å∞„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„ÅåÈÅéÂéª„ÅÆ„Éõ„Éº„Ç≠„É≥„Ç∞ËºªÂ∞Ñ„Å®„Ç®„É≥„Çø„É≥„Ç∞„É´„Åó„Å¶„ÅÑ„Çã„Åã„Å©„ÅÜ„Åã„ÅØ„ÄÅË¶≥Ê∏¨ËÄÖ„ÅÆË¶ã„Çã„ÇÇ„ÅÆ„Å´ÂΩ±Èüø„Çí‰∏é„Åà„Å™„ÅÑ‰∫ã„Åå„Çè„Åã„Çã„ÅÆ„Åß„ÄÇ', 'Ëâ≤„ÄÖÊõ∏„Åç„Åæ„Åó„Åü„Åå„ÄÅ„Åì„ÅÆË´ñÊñá„ÅÆ„É°„Ç§„É≥„É°„ÉÉ„Çª„Éº„Ç∏„ÅØ„ÄÅÂÜÖÈÉ®ÊºîÁÆóÂ≠ê„ÅÆË®òËø∞„ÅØ„ÄÅË¶≥Ê∏¨ËÄÖ„ÅÆÁä∂ÊÖã„ÅÆ„Åø„Å´‰æùÂ≠ò„Åô„Çã„ÅÆ„Åß„ÄÅ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„ÅÆÂÜÖÈÉ®„ÅØË¶≥Ê∏¨ËÄÖ„Å´‰æùÂ≠ò„Åó„ÅüÊ¶ÇÂøµ„Å†„Å®„ÅÑ„ÅÜ„Åì„Å®„Åß„Åô„ÄÇ', '„Åì„ÅÆ„ÅÇ„Åü„Çä„ÅØ„ÄÅÈáèÂ≠êÂäõÂ≠¶„Å´„Åä„Åë„ÇãOperator Growth„Å®„ÅÑ„ÅÜÁèæË±°„Å´Ê∑±„ÅèÈñ¢„Çè„Å£„Å¶„Åä„Çä„ÄÅË¶≥Ê∏¨ËÄÖ‰æùÂ≠ò„Åß„ÅÇ„Çã„Åì„Å®„ÇíÂº∑„ÅèÁ§∫ÂîÜ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ', 'Ë¶ÅÁ¥Ñ„Åô„Çã„Å®„ÄÅÔºëÔºâ„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„ÅÆÂÜÖÈÉ®ÊºîÁÆóÂ≠ê„ÅØ„ÄÅË¶≥Ê∏¨ËÄÖ„Åã„Çâ„ÅÆbackreaction„ÇíËÄÉ„Åà„Çå„Å∞‰Ωï„ÅÆÁüõÁõæ„ÇÇ„Å™„ÅÑÊßãÊàê„ÅåÂá∫Êù•„Çã„ÄÅÔºíÔºâÊßãÊàê„ÅØË¶≥Ê∏¨ËÄÖ„ÅÆ„Åø„Å´‰æùÂ≠ò„Åó„Å¶„ÄÅÂÖÉ„ÄÖ„ÅÇ„Å£„Åü„Éñ„É©„ÉÉ„ÇØ„Éõ„Éº„É´„ÅÆÁä∂ÊÖã„ÇÑ„Ç®„É≥„Çø„É≥„Ç∞„É´„É°„É≥„Éà„Å´„ÅØ‰∏ÄÂàá‰æùÂ≠ò„Åó„Å™„ÅÑ„ÄÇ']",http://arxiv.org/abs/1910.11346,"We present concrete construction of interior operators for a black hole which is perturbed by an infalling observer. The construction is independent from the initial states of the black hole while dependent only on the quantum state of the infalling observer. The construction has a natural interpretation from the perspective of the boundary operator's growth, resulting from the collision between operators accounting for the infalling and outgoing modes. The interior partner modes are created once the infalling observer measures the outgoing mode, suggesting that the black hole interior is observer-dependent. Implications of our results on various conceptual puzzles, including the firewall puzzle and the information problem, are also discussed. ",Observer-dependent black hole interior from operator collision
29,1188655759657062400,208048785,Kazuhiro Nakamura,['Our new paper:\n‚ÄúFast and High-Quality Singing Voice Synthesis System based on Convolutional Neural Networks‚Äù\n\n<LINK>'],https://arxiv.org/abs/1910.11690,"The present paper describes singing voice synthesis based on convolutional neural networks (CNNs). Singing voice synthesis systems based on deep neural networks (DNNs) are currently being proposed and are improving the naturalness of synthesized singing voices. As singing voices represent a rich form of expression, a powerful technique to model them accurately is required. In the proposed technique, long-term dependencies of singing voices are modeled by CNNs. An acoustic feature sequence is generated for each segment that consists of long-term frames, and a natural trajectory is obtained without the parameter generation algorithm. Furthermore, a computational complexity reduction technique, which drives the DNNs in different time units depending on type of musical score features, is proposed. Experimental results show that the proposed method can synthesize natural sounding singing voices much faster than the conventional method. ","Fast and High-Quality Singing Voice Synthesis System based on
  Convolutional Neural Networks"
30,1188469688579350533,2423179856,Edward Raff,"['Would a File by Any Other Name Seem as Malicious? New @BoozAllen  paper with @AndreNguyen16 &amp; @AaronSantMiller accepted to @IEEEorg #BigData conference! There are a number of cases where you might not have an EXE file, but want to know if it was malicious. <LINK> <LINK>', 'Turns out, you can get surprisingly far by just looking at file names, with an AUC=0.98 on the @EndgameInc  Ember2017 corpus! LightGBM is domain knowledge features, CharCNN our approach just using the file name. Why does this even work? https://t.co/s1QlMTgBCQ', 'Malware authors are generally lazy. If we look at clusters in CharCNN file name space, we can find a number of naming patterns used: pretending to be a backup, random tokens, copying common app name (e.g, iTunes), or even lazily just calling themselves Ransomware. https://t.co/oE822eoVFP', ""We did a lot of checking to try and avoid as much potential label leakage, and to check that we are more than just a naive black list on file names - which would fail to classify most files. \n\nFile name based classification isn't a panacea, but a helpful approach. Check it out! https://t.co/xHir6O5WxQ""]",https://arxiv.org/abs/1910.04753,"Successful malware attacks on information technology systems can cause millions of dollars in damage, the exposure of sensitive and private information, and the irreversible destruction of data. Anti-virus systems that analyze a file's contents use a combination of static and dynamic analysis to detect and remove/remediate such malware. However, examining a file's entire contents is not always possible in practice, as the volume and velocity of incoming data may be too high, or access to the underlying file contents may be restricted or unavailable. If it were possible to obtain estimates of a file's relative likelihood of being malicious without looking at the file contents, we could better prioritize file processing order and aid analysts in situations where a file is unavailable. In this work, we demonstrate that file names can contain information predictive of the presence of malware in a file. In particular, we show the effectiveness of a character-level convolutional neural network at predicting malware status using file names on Endgame's EMBER malware detection benchmark dataset. ",Would a File by Any Other Name Seem as Malicious?
31,1188205077703675905,1079592050973061122,Pieter van Dokkum,"['A new paper by my excellent student Lamiya Mowla! In <LINK> she obtained ALMA CO kinematics to confirm that compact, massive galaxies at z~2.3 are, in fact, compact and massive. There had been some doubt, as their HŒ± lines often are strangely narrow.', 'In 2015 we had hypothesized that these narrow HŒ± lines were caused by dust obscuration of the central regions of the galaxies. However, the CO lines of the three target galaxies are narrow as well! Only FWHM~165 km/s, whereas the expectation was 500-700 km/s. https://t.co/EDkoG2JxJY', 'This means we need other explanations. One possibility is that the CO is in nearly face-on disks. Somewhat worryingly, Lamiya shows that there may be an inclination bias in the size-mass plane: dusty galaxies could appear much larger when viewed edge-on than when viewed face-on. https://t.co/sUkIvwWWzt', ""The only way to settle this definitively is with rest-frame 3 micron imaging from JWST - that will show us where the stars really are. Whatever the cause, the implication is that these objects do not evolve in a simple way into the cores of today's massive elliptical galaxies!""]",https://arxiv.org/abs/1910.10722v1,"Compact, massive star forming galaxies at $z\sim2.5$ are thought to be building the central regions of giant elliptical galaxies today. However, a significant fraction of these objects were previously shown to have much smaller H$\alpha$ line widths than expected. A possible interpretation is that H$\alpha$ emission from their central regions, where the highest velocities are expected, is typically obscured by dust. Here we present ALMA observations of the CO(3-2) emission line of three compact, massive galaxies with H$\alpha$ line widths of FWHM(H$\alpha$)$\sim$125-260 km s$^{-1}$ to test this hypothesis. Surprisingly, in all three galaxies, the CO line width is similar to the H$\alpha$ line width: we find FWHM(CO)$\sim$165 km s$^{-1}$ for all three galaxies whereas FWHM(CO)$\sim$450-700 km s$^{-1}$ was expected from a simple virial estimator. These results show that the narrow H$\alpha$ linewidths of many compact massive star-forming galaxies are not due to preferential obscuration of the highest velocity gas. An alternative explanation for the narrow line widths is that the galaxies are disks that are viewed nearly face-on. We suggest that there may be an inclination bias in the size-mass plane, such that the apparent rest-frame optical sizes of face-on galaxies are smaller than those of edge-on galaxies. Although not conclusive, this hypothesis is supported by an observed anti-correlation between the size and axis ratio of massive galaxies. ","] Anomalously Narrow Linewidths of Compact Massive Star-Forming Galaxies
  at z~2.3: A Possible Inclination Bias in the Size-Mass Plane"
32,1187930403954278400,321648912,Vishnu Boddeti,"['New paper led by my student Bashir Sadeghi. \n\nOn the Global Optima of Kernelized Adversarial Representation Learning (#iccv2019)\n<LINK>\n\nWe consider Adversarial Representation Learning (ARL) from an  optimization perspective under linear/kernel methods.\n\nThread <LINK>', 'Existing DNN based ARL methods are optimized through simultaneous SGD. They provide no guarantees of reaching global optima, nor provable bounds on achievable trade-off between utility and invariance/fairness. \n\nWe consider linear/kernel methods and obtain global optima. https://t.co/57FEakYk4c', 'Even under linear/kernel methods, the objective is non-convex and non-differentiable. We obtain closed-form solution by reducing it to a set of trace minimization problems on a Stiefel manifold. We also provide a geometric interpretation. https://t.co/XvtoKT5b2Q', 'We apply our solution for Fair Classification tasks on the UCI dataset. Kernel-ARL with global optima outperforms neural network based solutions with local optima. https://t.co/M4LKFuvqny', 'We define a new task on CIFAR-100 dataset. The 100 fine classes are divided into 20 coarse superclasses. Learn embedding to predict coarse label but prevent leakage of fine label. We derive bounds on provably achievable target accuracy and leakage of sensitive information. https://t.co/MNShH73jGj', 'What should be the optimal dimensionality of fair/invariant embeddings? Existing method choose this arbitrarily. Our approach determines the optimal embedding  dimensionality as part of the solution. For CIFAR-100 example, optimal dimensionality transitions from 19 to 1. https://t.co/np0p0AA0dH']",https://arxiv.org/abs/1910.07423,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches. Code is available at this https URL ",On the Global Optima of Kernelized Adversarial Representation Learning
33,1187689034669350912,3295041857,Dr. Dan Diaper,['New #research #paper \n<LINK> \n@guerillacricket @abdulhayemehta @NorthStandGang @ilegally_indian @AnnieChave @Sofa_Katie @palfreyman1414 @Prof_Cooper @Edgaralanpoe48 @andydurrant75 @cbar2323 @chriswaller1 @Mog7734 @JeffPerkins12 @evilscootus @kinobe911 @mattbob84 <LINK>'],https://arxiv.org/abs/1910.10481,"An entirely novel synthesis combines the applied cognitive psychology of a task analytic approach with a neural cell assembly perspective that models both brain and mind function during task performance; similar cell assemblies could be implemented as an artificially intelligent neural network. A simplified cell assembly model is introduced and this leads to several new representational formats that, in combination, are demonstrated as suitable for analysing tasks. The advantages of using neural models are exposed and compared with previous research that has used symbolic artificial intelligence production systems, which make no attempt to model neurophysiology. For cognitive scientists, the approach provides an easy and practical introduction to thinking about brains, minds and artificial intelligence in terms of cell assemblies. In the future, subsequent developments have the potential to lead to a new, general theory of psychology and neurophysiology, supported by cell assembly based artificial intelligences. ",The Task Analysis Cell Assembly Perspective
34,1187628272277831680,168097862,Â±±Êú¨ Èæç‰∏Ä / Ryuichi Yamamoto,"['ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit <LINK> Check our our new paper for the open-source speech processing toolkit for end-to-end TTS. Audio samples are available at <LINK> üòä', 'Code and reproducible recipes are all available on Github! https://t.co/MJ7am9A4bd Happy speech hacking :)', ""@heiga_zen Ah, thanks for pointing it out! We'll fix it as soon as possible.""]",https://arxiv.org/abs/1910.10909,"This paper introduces a new end-to-end text-to-speech (E2E-TTS) toolkit named ESPnet-TTS, which is an extension of the open-source speech processing toolkit ESPnet. The toolkit supports state-of-the-art E2E-TTS models, including Tacotron~2, Transformer TTS, and FastSpeech, and also provides recipes inspired by the Kaldi automatic speech recognition (ASR) toolkit. The recipes are based on the design unified with the ESPnet ASR recipe, providing high reproducibility. The toolkit also provides pre-trained models and samples of all of the recipes so that users can use it as a baseline. Furthermore, the unified design enables the integration of ASR functions with TTS, e.g., ASR-based objective evaluation and semi-supervised learning with both ASR and TTS models. This paper describes the design of the toolkit and experimental evaluation in comparison with other toolkits. The experimental results show that our models can achieve state-of-the-art performance comparable to the other latest toolkits, resulting in a mean opinion score (MOS) of 4.25 on the LJSpeech dataset. The toolkit is publicly available at this https URL ","ESPnet-TTS: Unified, Reproducible, and Integratable Open Source
  End-to-End Text-to-Speech Toolkit"
35,1187569997633048577,1249492212,Karol Hausman üíôüíõ,"[""The Meta-World paper is out! <LINK>\n\nWe evaluated different multi-task and meta-RL algorithms and there's a lot of room for improvement. Try your new algorithms on it! <LINK>""]",https://arxiv.org/abs/1910.10897,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods. ","Meta-World: A Benchmark and Evaluation for Multi-Task and Meta
  Reinforcement Learning"
36,1187565577667170304,990433714948661250,Sergey Levine,['The paper on meta-world -- a new benchmark for meta-learning with 50 distinct robotic manipulation tasks -- is now out!\n\nCode &amp; documentation here: <LINK>\nFull paper: <LINK> <LINK>'],https://arxiv.org/abs/1910.10897,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods. ","Meta-World: A Benchmark and Evaluation for Multi-Task and Meta
  Reinforcement Learning"
37,1187551195461804033,3290526443,Jia-Bin Huang,"['Badour will be presenting her work on guided image-to-image translation in the upcoming #iccv2019! A new way of incorporating auxiliary input for various problems. \n\nPaper: <LINK>\nProject/code: <LINK> <LINK>', ""Badour pulls off impressive work while raising two kids. Read her story in @VT_DAC's student spotlight last month! https://t.co/4Q3EoOJ4Yj\n\nNote: She will be looking for an internship next summer. Please come talk to her at #iccv2019""]",https://arxiv.org/abs/1910.11328,"We address the problem of guided image-to-image translation where we translate an input image into another while respecting the constraints provided by an external, user-provided guidance image. Various conditioning methods for leveraging the given guidance image have been explored, including input concatenation , feature concatenation, and conditional affine transformation of feature activations. All these conditioning mechanisms, however, are uni-directional, i.e., no information flow from the input image back to the guidance. To better utilize the constraints of the guidance image, we present a bi-directional feature transformation (bFT) scheme. We show that our bFT scheme outperforms other conditioning schemes and has comparable results to state-of-the-art methods on different tasks. ","Guided Image-to-Image Translation with Bi-Directional Feature
  Transformation"
38,1187549620366393346,1077995761487568896,Jon Miller,"['New Paper Day!\n<LINK>\nExcellent analysis by grad student @NicolasTrueba. \nStrong evidence of magnetically driven winds from accretion onto the black hole in 4U 1630-472.  \nMade possible by @chandraxray.  Prime science for XRISM and @AthenaXIFU. <LINK>', '@tracijo32 @NicolasTrueba @chandraxray @AthenaXIFU The power-law will be with you, always.']",https://arxiv.org/abs/1910.11243,"The mechanisms that drive disk winds are a window into the physical processes that underlie the disk. Stellar-mass black holes are an ideal setting in which to explore these mechanisms, in part because their outbursts span a broad range in mass accretion rate. We performed a spectral analysis of the disk wind found in six Chandra/HETG observations of the black hole candidate 4U~1630$-$472, covering a range of luminosities over two distinct spectral states. We modeled both wind absorption and extended wind re-emission components using PION, a self-consistent photoionized absorption model. In all but one case, two photoionization zones were required in order to obtain acceptable fits. Two independent constraints on launching radii, obtained via the ionization parameter formalism and the dynamical broadening of the re-emission, helped characterize the geometry of the wind. The innermost wind components ($r \simeq {10}^{2-3}$ $GM/{c}^{2}$) tend towards small volume filling factors, high ionization, densities up to $n \simeq {10}^{15-16} {\text{cm}}^{-3}$, and outflow velocities of $\sim 0.003c$. These small launching radii and large densities require magnetic driving, as they are inconsistent with numerical and analytical treatments of thermally driven winds. Outer wind components ($r \simeq {10}^{5}$ $GM/{c}^{2}$) are significantly less ionized and have filling factors near unity. Their larger launching radii, lower densities ($n \simeq {10}^{12} {\text{cm}}^{-3}$), and outflow velocities ($\sim 0.0007c$) are nominally consistent with thermally driven winds. The overall wind structure suggests that these components may also be part of a broader MHD outflow and perhaps better described as magneto-thermal hybrid winds. ","A Comprehensive Chandra Study of the Disk Wind in the Black Hole
  Candidate 4U 1630-472"
39,1187535911082483712,1108746946695622658,Kazu Akiyama,['Here is our new theory paper led by Kotaro Moriyama at \n@MITHaystack. The black hole spacetime around Sgr A* (and M87) may be measured by capturing time-domain signatures of general relativistic echos of lights from infalling gas clouds with @ehtelescope.\n<LINK>'],https://arxiv.org/abs/1910.10713,"The black hole spacetime is described by general relativity and characterized by two quantities: the black hole mass and spin. Black hole spin measurement requires information from the vicinity of the event horizon, which is spatially resolved for the Galactic center SagittariusA* (SgrA*) and nearby radio galaxy M87 by means of very long baseline interferometry (VLBI) observations with the Event Horizon Telescope (EHT). In this paper, we simulate EHT observations for a gas cloud intermittently falling onto a black hole, and construct a method for spin measurement based on its relativistic flux variation. The light curve of the infalling gas cloud is composed of peaks formed by photons which directly reach a distant observer and by secondary ones reaching the observer after more than one rotation around the black hole. The time interval between the peaks is determined by a period of photon rotation near the photon circular orbit which uniquely depends on the spin. We perform synthetic EHT observations for SgrA* under a more realistic situation that a number of gas clouds intermittently fall towards the black hole with various initial parameters. Even for this case, the black hole spin dependence is detectable in correlated flux densities which are accurately calibrated by baselines between sites with redundant stations. The synthetic observations indicate that our methodology can be applied to EHT observations of Sgr A* since April 2017. ","Black hole Spin Measurement Based on Time-domain VLBI Observations of
  Infalling Gas Cloud"
40,1187529975903080448,2577596593,Chelsea Finn,['The Meta-World paper is now out! Includes an eval of 8 methods &amp; 5 eval modes.\n<LINK>\n\nWe look forward to seeing how your new algorithms fare on the suite of 50 tasks. <LINK> <LINK>'],https://arxiv.org/abs/1910.10897,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods. ","Meta-World: A Benchmark and Evaluation for Multi-Task and Meta
  Reinforcement Learning"
41,1187507675258486784,1601615208,Rui Shu,"['New paper on disentanglement c: Given the recent impossibility results in unsupervised disentanglement, we decided to be optimistic and instead provide guarantees (unimpossibility results?) via weak supervision (1/13)\n<LINK>\n<LINK> <LINK>', 'We provide definitions of disentanglement that can be measured in a weakly supervised fashion. In particular, we decompose disentanglement into two concepts: consistency and restrictiveness. (2/13) https://t.co/rqU1yHlMf3', 'Since these two concepts operate over sets of factors, we build a set-based calculus of disentanglement to facilitate abstract reasoning about the relationships between consistency (C), restrictiveness (R), and disentanglement (D). (3/13) https://t.co/9ovdjCzEiQ', 'We then analyze the disentanglement guarantees of three notable forms of weak supervision: restricted labeling, match pairing, and rank pairing. (4/13) https://t.co/FBXLfvMZH4', 'We show how weak supervision can be applied in a targeted manner to achieve either consistency or restrictiveness on a particular factor of interest. (5/13) https://t.co/gLGJaGWTLG', 'And demonstrate that combining weak supervision techniques leads to full disentanglement both in terms of various performance metrics and visual inspection. (6/13) https://t.co/DjOvWGZonX', 'We also show a simple but important set of impossibility results in some weak supervision settings: for example, did you know that style-content disentanglement is not actually guaranteed when you only have access to the sample x and the content label y? üòÆ (7/13)', 'We show that despite the impossibility result for style-content disentanglement when you only have content labels, there is a strong inductive bias by the neural network to achieve disentanglement anyway. Still an open problem as to why this is the case ü§î (8/13) https://t.co/BWZ0BDYvMd', 'All of our results are collected over a range of hyperparameter settings. We were too lazy to cherrypick c: (9/13)', 'Also, my favorite plot is hidden all the way in the appendix in Figure 11, showing a neat little experiment we did on consistency vs restrictiveness. *cough* please read the appendix üòÖ *cough* (10/13) https://t.co/cE4dQCDpyN', 'We hope this paper encourages further analysis of disentanglement in the weakly supervised regime. Lots of science left to be done to understand disentanglement! üôÇ (11/13)', 'This joint work with @cynnjjs, @studentofml, @ermonste and @poolio. Work done primarily in the Google MTV-40 microkitchen c: (12/13)', 'Also, what do you get when a PyTorch user interns at Google? Introducing: Tensorsketch, designed for all the PyTorch users thinking about playing with TensorFlow 2.0 üôÉ\nhttps://t.co/nNuCobRUCB (13/13) https://t.co/MUqwqNB7rV', '@tsauri_eecs Our analysis assumes successful distribution-matching in the augmented space. How to achieve that in practice, especially in the context of discrete latents vars, is certainly something we should think more about! My only answer at the moment is: try both? :p']",http://arxiv.org/abs/1910.09772,"Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework. ",Weakly Supervised Disentanglement with Guarantees
42,1187498239551037450,373525906,Weijie Su,"['Learning rate is recognized as ‚Äúthe single most important hyper-parameter‚Äù in training deep learning. Inspired by statistical insights, our new paper (<LINK>) proposes a dynamic learning rate schedule by splitting SGD for stationarity detection.']",https://arxiv.org/abs/1910.08597,"This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, yields better generalization performance than other adaptive gradient methods such as Adam. ","Robust Learning Rate Selection for Stochastic Optimization via Splitting
  Diagnostic"
43,1187483750608728064,19332603,Eric Battenberg,"['Check out our new paper on location-relative attention mechanisms for Tacotron TTS. We compare existing mechanism and introduce Dynamic Convolution Attention (DCA), which enables Tacotron to generalize to very long utterances. <LINK> <LINK>', 'We also compare various tweaks to GMM-based attention mechanisms that improve alignment consistency and speed during training, and show that GMM mechanisms can  generalize to very long utterances as well. https://t.co/E6k6jurLoY', ""Audio examples are available at: https://t.co/UNxqJITP9u\n(We're most excited for you to hear some of the failure cases.) #Attenchilada"", 'Reddit post for additional discussion: https://t.co/AH8tUF0cIS', ""@log_pie We should add you to the acknowledgements.  You're the one who introduced GMM attention to our Tacotron models. (Gonna have to add some negative vspace to the ICASSP template to make it fit). :)""]",https://arxiv.org/abs/1910.10288,"Despite the ability to produce human-level speech for in-domain text, attention-based end-to-end text-to-speech (TTS) systems suffer from text alignment failures that increase in frequency for out-of-domain text. We show that these failures can be addressed using simple location-relative attention mechanisms that do away with content-based query/key comparisons. We compare two families of attention mechanisms: location-relative GMM-based mechanisms and additive energy-based mechanisms. We suggest simple modifications to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dynamic Convolution Attention (DCA). We compare the various mechanisms in terms of alignment speed and consistency during training, naturalness, and ability to generalize to long utterances, and conclude that GMM attention and DCA can generalize to very long utterances, while preserving naturalness for shorter, in-domain utterances. ","Location-Relative Attention Mechanisms For Robust Long-Form Speech
  Synthesis"
44,1187457921816432642,2585907650,Leslie Smith,['I just posted a new paper to arXiv on robustness to adversarial examples.  I think some people might find it useful.\n<LINK>'],https://arxiv.org/abs/1910.10679,"Adversarial attacks and defenses are currently active areas of research for the deep learning community. A recent review paper divided the defense approaches into three categories; gradient masking, robust optimization, and adversarial example detection. We divide gradient masking and robust optimization differently: (1) increasing intra-class compactness and inter-class separation of the feature vectors improves adversarial robustness, and (2) marginalization or removal of non-robust image features also improves adversarial robustness. By reframing these topics differently, we provide a fresh perspective that provides insight into the underlying factors that enable training more robust networks and can help inspire novel solutions. In addition, there are several papers in the literature of adversarial defenses that claim there is a cost for adversarial robustness, or a trade-off between robustness and accuracy but, under this proposed taxonomy, we hypothesis that this is not universal. We follow up on our taxonomy with several challenges to the deep learning research community that builds on the connections and insights in this paper. ",A Useful Taxonomy for Adversarial Robustness of Neural Networks
45,1187292243121655808,259597339,Krishna Murthy,"['Excited to announce our new work!\n""gradSLAM: Dense SLAM meets automatic differentiation""\n\nWe leverage the power of autodiff frameworks to make dense SLAM fully differentiable.\n\nPaper: <LINK>\nProject page: <LINK>\nVideo: <LINK> <LINK>', 'We can now backprop all the way from 3D maps to 2D pixels! This opens up new avenues in deep learning for SLAM.\n\nFor example, by computing the gradient wrt every input pixel, it is possible to know how much (error) each pixel in an image contributed to the 3D map. https://t.co/xzzf5a2ads', 'gradSLAM proposes modifications to existing SLAM components to make them differentiable, without sacrificing performance. We implement differentiable versions of KinectFusion, PointFusion, and ICP-SLAM. https://t.co/6kD1d5oJf6', ""We've tried gradSLAM out on a few different datasets. Here's examples from an ICL-NUIM synthetic sequence: https://t.co/J5p1unwNf2  (Thanks @ankurhandos for creating this dataset) https://t.co/1ARB6O4Dmw"", 'ScanNet: https://t.co/lgvTcKdTZw https://t.co/TkrKlnL3W8', 'TUM RGB-D benchmark: https://t.co/osd9T4AVUm https://t.co/7lIJxEFGsH', 'And, a ""real"" real sequence scanned using an @IntelRealSense https://t.co/nniE7Z7SqH']",https://arxiv.org/abs/1910.10672,"Blending representation learning approaches with simultaneous localization and mapping (SLAM) systems is an open question, because of their highly modular and complex nature. Functionally, SLAM is an operation that transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals to learn representations that optimize task performance. However, several components of a typical dense SLAM system are non-differentiable. In this work, we propose gradSLAM, a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM. TL;DR: We leverage the power of automatic differentiation frameworks to make dense SLAM differentiable. ",gradSLAM: Automagically differentiable SLAM
46,1187161460033458177,837133583558987776,Colin Raffel,"['New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the limits to achieve SoTA on GLUE, SuperGLUE, CNN/DM, and SQuAD.\nPaper: <LINK>\nCode/models/data/etc: <LINK>\nSummary ‚¨áÔ∏è (1/14) <LINK>', 'Our approach casts *every* language problem as a text-to-text task. For example, English-to-German translation -- input: ""translate English to German: That is good."" target: ""Das ist gut."" or sentiment ID -- input: ""sentiment: This movie is terrible!"", target: ""negative"" (2/14)', 'The text-to-text approach allows us to use the same model, loss function, decoding process, training procedure, etc. across every task we study. It also provides a standard testbed for the many ideas we evaluate in our empirical survey. (3/14)', 'Transfer learning for NLP usually uses unlabeled data for pre-training, so we assembled the ""Colossal Clean Crawled Corpus"" (C4), ~750GB of cleaned text from Common Crawl. The code for generating C4 is already available in TensorFlow datasets: https://t.co/Vm35EUhuMG (4/14)', 'For most of the experiments in the paper, we use a basic encoder-decoder Transformer architecture. We found this worked well both on generative and classification tasks in the text-to-text framework. We call our model the ""Text-to-Text Transfer Transformer"" (T5). (5/14)', 'For our empirical survey, we first compared different architectural variants including encoder-decoder models and language models in various configurations and with various objectives. The encoder-decoder architecture performed best in our text-to-text setting. (6/14) https://t.co/RNNLGGT1zH', 'Then, we explored the space of different pre-training objectives. We found that BERT-style denoising objectives generally outperformed other approaches and that a SpanBERT-style (Joshi et al. 2019) objective had the best combination of performance and training speed. (7/14) https://t.co/sznm2tKKBe', 'Next, we compared various unlabeled datasets and found that in some cases in-domain pre-training data boosted performance on downstream tasks. Our diverse C4 dataset, however, is large enough that you can avoid repeating any examples, which we showed can be detrimental. (8/14) https://t.co/8Lop6BtHMh', 'Unsupervised pre-training is standard practice, but an alternative is to pre-train on a mixture of supervised and unsupervised data as in the MT-DNN (Liu et al. 2019). We found both approaches can achieve similar performance once you get the mixing proportions right. (9/14)', 'Scaling up is a powerful way to improve performance, but how should you scale? We compared training on more data, training a longer model, and ensembling given a specific computational budget. tl;dr: A bigger model is a necessity, but everything helps. (10/14)', 'Finally, we combine the insights from our study to train five models of varying sizes (up to 11 billion parameters) on 1 trillion tokens of data. We obtained state-of-the-art on GLUE, SuperGLUE, SQuAD, and CNN/Daily Mail, but not WMT translation. (11/14)', ""I'm particularly happy that we beat the SoTA on SuperGLUE by 4.3% and are within spitting distance of human performance (88.9 vs 89.8). SuperGLUE was designed to only include tasks that were easy for humans but hard for machines. (12/14)"", 'This work was a collaboration between an incredible team including Noam Shazeer, @ada_rob, @katherine1ee, @sharan0909, Michael Matena, @zhouyanqi30, @kongkonglli, and @peterjliu. (13/14)', 'All of our code, pre-trained models, and datasets are already online, see https://t.co/uaViIYyHR7 for more details. Please reach out if you have any questions or suggestions! (14/14)', '@hardmaru Nah, we are saving that for future work. ;)']",https://arxiv.org/abs/1910.10683,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. ","Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer"
47,1187107907688529920,348637346,Diana Powell,"['Inhomogeneous clouds can be observed in transmission even with limited wavelength coverage, uncertainty on limb darkening coefficients, and imprecise transit times in a way that is statistically robust! See my new paper to learn why and how: <LINK>', 'Both the east and west limb often form clouds, but that the different properties of these clouds enhances the limb to limb differences compared to the clear atmosphere case. https://t.co/x3XRvqZnDx', 'Using JWST it should be possible to detect the presence of cloud inhomogeneities by comparing the shape of the transit lightcurve at multiple wavelengths because inhomogeneous clouds impart a characteristic, wavelength dependent signature https://t.co/MUIodCPjrZ', 'Probing limb inhomogeneity on hot Jupiters due to clouds can be statistically robust even with limited wavelength coverage, uncertainty on limb darkening coefficients, and imprecise transit times! https://t.co/gQW2OUDYg2', 'The signatures due to clouds in these models are key in observing inhomogeneity! Check out the paper for predictions about cloud properties and their impact on the observed spectra!', ""Also don't forget to consider cloud particle size distributions!  Using the area or mass weighted particle size significantly alters the relative strength of the cloud spectral features compared to using the predicted size distribution. https://t.co/R0MTSaVyW1"", 'In short, we can use cloud physics to probe fundamental atmospheric properties like limb inhomogeneity! This work was done in collaboration with @TomLouden_b, @lkreidberg, Xi Zhang, @PlanetaryGao, and @V_Parmentier \n\n#dreamteam #cloudsarefriends\nhttps://t.co/GqurZ0jzHb', '@henrifdrake thank you!!!', '@Of_FallingStars @TomLouden_b @lkreidberg @PlanetaryGao @V_Parmentier thank you!!!']",https://arxiv.org/abs/1910.07527,"We determine the observability in transmission of inhomogeneous cloud cover on the limbs of hot Jupiters through post processing a general circulation model to include cloud distributions computed using a cloud microphysics model. We find that both the east and west limb often form clouds, but that the different properties of these clouds enhances the limb to limb differences compared to the clear case. Using JWST it should be possible to detect the presence of cloud inhomogeneities by comparing the shape of the transit lightcurve at multiple wavelengths because inhomogeneous clouds impart a characteristic, wavelength dependent signature. This method is statistically robust even with limited wavelength coverage, uncertainty on limb darkening coefficients, and imprecise transit times. We predict that the short wavelength slope varies strongly with temperature. The hot limb of the hottest planets form higher altitude clouds composed of smaller particles leading to a strong rayleigh slope. The near infrared spectral features of clouds are almost always detectable, even when no spectral slope is visible in the optical. In some of our models a spectral window between 5 and 9 microns can be used to probe through the clouds and detect chemical spectral features. Our cloud particle size distributions are not log-normal and differ from species to species. Using the area or mass weighted particle size significantly alters the relative strength of the cloud spectral features compared to using the predicted size distribution. Finally, the cloud content of a given planet is sensitive to a species' desorption energy and contact angle, two parameters that could be constrained experimentally in the future. ","Transit Signatures of Inhomogeneous Clouds on Hot Jupiters: Insights
  From Microphysical Cloud Modeling"
48,1187101122206388224,4639078397,John Wise,"['New paper day(+1)! Aycin Aykutalp (LANL) led our study to see how a massive black hole seed affects its surroundings. We find that a metal-free star cluster forms around this newly formed black hole, which would be unique to the high-z universe! <LINK>', 'The X-rays from the BH partially ionizes the gas outside of the HII region, which then catalyzes H2 formation. The nearby dense clouds can then cool and collapse to form ~1000 Msun of Pop III stars within ~10 pc']",https://arxiv.org/abs/1910.08554,"The direct formation of a massive black hole is a potential seeding mechanism of the earliest observed supermassive black holes. We investigate how the existence of a massive black hole seed impacts the ionization and thermal state of its pre-galactic host halo and subsequent star formation. We show that its X-ray radiation ionizes and heats the medium, enhancing $\rm{H}_2$ formation in shielded regions, within the nuclear region in the span of a million years. The enhanced molecular cooling triggers the formation of a $\sim 10^4~{\rm M}_\odot$ metal-free stellar cluster at a star formation efficiency of $\sim 0.1\%$ in a single event. Star formation occurs near the edges of the H II region that is partially ionized by X-rays, thus the initial size depends on the black hole properties and surrounding environment. The simulated metal-free galaxy has an initial half-light radius of $\sim 10$ pc but expands to $\sim 50$ pc after 10 million years because of the outward velocities of their birth clouds. Supernova feedback then quenches any further star formation for tens of millions of years, allowing the massive black hole to dominate the spectrum once the massive metal-free stars die. ",Induced metal-free star formation around a massive black hole seed
49,1187007570432679936,876274407995527169,David Madras,"['New paper on ArXiv! ‚ÄúDetecting Extrapolation with Local Ensembles‚Äù (w/ @alexdamour and @james_c_atwood)! <LINK>\n\nTL;DR: We present a post-hoc method for approximating the variance of an ensemble, which is useful for estimating prediction reliability.', 'Ensemble training (many models, same architecture &amp; training set) is a good way to estimate predictive reliability. If many models in ensemble disagree, prediction is probably unreliable. However, sometimes you can‚Äôt train an ensemble (pre-trained models, resource constraints). https://t.co/iGtPXXV2Jd', 'We present Local Ensembles, a method for estimating prediction reliability in a *pre-trained* model, which approximates the variance of an ensemble (from that model class and training set), using only local second-order information.', ""Intuition: some directions around model parameters are flat (small Hessian eigenvalues), so perturbing in those directions won't raise training loss. We call this the ensemble subspace. https://t.co/KvnGctFrqf"", 'The models in the ensemble subspace are all similarly good with respect to the training loss; if their predictions at an input disagree, the model class is underdetermined by the training data (and loss function) at that input, and the prediction is probably unreliable.', 'For a test input, we can approximate the predictive variance of the models lying locally in the ensemble subspace by projecting the prediction gradient into it. We show empirically the norm of this projection (our extrapolation score) approximates true ensemble variance well. https://t.co/xlAZsB8FXz', 'See the paper for experimental results. For instance, active learning: we show that our method provides useful signal for selecting the next points to train on (higher scores mean more extrapolation means receiving a label would more useful) https://t.co/rzXCkHttep', 'Lots more technical details in the paper: how do we find the ensemble subspace (the Lanczos iteration and some tricks), relationships to other second-order methods (e.g. influence functions, Laplace approximation) and more!']",https://arxiv.org/abs/1910.09573,"We present local ensembles, a method for detecting underspecification -- when many possible predictors are consistent with the training data and model class -- at test time in a pre-trained model. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, we show that our method is capable of detecting when a pre-trained model is underspecified on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning. ",Detecting Underspecification with Local Ensembles
50,1186919099932299265,92004169,Yerai Doval,"[""Want to ensure a good integration of 2+ monolingual embedding spaces? Meemi's got you covered! In our new article (<LINK>), we extend our #emnlp2018 paper on meeting in the middle with new languages, a new downstream task, and many new insights!"", 'By ""we"" I mean the usual suspects: @CamachoCollados, @luisanke, Steven Schockaert, and yours truly.', 'Bonus track: https://t.co/95vLTLZm2h']",https://arxiv.org/abs/1910.07221,"Word embeddings have become a standard resource in the toolset of any Natural Language Processing practitioner. While monolingual word embeddings encode information about words in the context of a particular language, cross-lingual embeddings define a multilingual space where word embeddings from two or more languages are integrated together. Current state-of-the-art approaches learn these embeddings by aligning two disjoint monolingual vector spaces through an orthogonal transformation which preserves the structure of the monolingual counterparts. In this work, we propose to apply an additional transformation after this initial alignment step, which aims to bring the vector representations of a given word and its translations closer to their average. Since this additional transformation is non-orthogonal, it also affects the structure of the monolingual spaces. We show that our approach both improves the integration of the monolingual spaces as well as the quality of the monolingual spaces themselves. Furthermore, because our transformation can be applied to an arbitrary number of languages, we are able to effectively obtain a truly multilingual space. The resulting (monolingual and multilingual) spaces show consistent gains over the current state-of-the-art in standard intrinsic tasks, namely dictionary induction and word similarity, as well as in extrinsic tasks such as cross-lingual hypernym discovery and cross-lingual natural language inference. ","Meemi: A Simple Method for Post-processing and Integrating Cross-lingual
  Word Embeddings"
51,1186904346035970053,1141345406594560005,Dimitra Atri,['NEW paper! Accepted in MNRAS Letters.\n\nI blasted terrestrial #exoplanets in #habitable zones with stellar radiation and studied scenarios with different atm sizes and magnetic fields to understand how these factors contribute to #planetary #habitability.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1910.09871,"The discovery of terrestrial exoplanets orbiting in habitable zones around nearby stars has been one of the significant developments in modern astronomy. More than a dozen such planets, like Proxima Centauri b and TRAPPIST-1 e, are in close-in configurations and their proximity to the host star makes them highly sensitive to stellar activity. Episodic events such as flares have the potential to cause severe damage to close-in planets, adversely impacting their habitability. Flares on fast rotating young M stars occur up to 100 times more frequently than on G-type stars which makes their planets even more susceptible to stellar activity. Stellar Energetic Particles (SEPs) emanating from Stellar Proton Events (SPEs) cause atmospheric damage (erosion and photochemical changes), and produce secondary particles, which in turn results in enhanced radiation dosage on planetary surfaces. We explore the role of SPEs and planetary factors in determining planetary surface radiation doses. These factors include SPE fluence and spectra, and planetary column density and magnetic field strength. Taking particle spectra from 70 major solar events (observed between 1956 and 2012) as proxy, we use the GEANT4 Monte Carlo model to simulate SPE interactions with exoplanetary atmospheres, and we compute surface radiation dose. We demonstrate that in addition to fluence, SPE spectrum is also a crucial factor in determining the surface radiation dose. We discuss the implications of these findings in constraining the habitability of terrestrial exoplanets. ","Stellar Proton Event-induced surface radiation dose as a constraint on
  the habitability of terrestrial exoplanets"
52,1186814794344665088,472628395,Peter Melchior,"['New paper: constrained #optimization using Adam (the optimizer, not the biblical figure) in a proximal quasi-Newton scheme. This is not just better (as in lower loss) but also faster than the traditional proximal gradient method (PGM) <LINK> <LINK>', ""Why does that matter? Because we ran into a major problem with PGM. It needs to *know* the step sizes for gradient descent. For data fusion, like joint processing of @LSST and @NASAWFIRST, there's no closed form for the step sizes. ‚ùå"", '@LSST @NASAWFIRST Now the step sizes are learned on-the-fly, regardless of how complicated our loss function from multiple surveys, varying observational conditions, different instruments, etc is.', '@LSST @NASAWFIRST In addition, we can now use constraints in problems that are normally only optimized without. I have ideas for constrained #NeuralNetworks']",https://arxiv.org/abs/1910.10094,"We implement the adaptive step size scheme from the optimization methods AdaGrad and Adam in a novel variant of the Proximal Gradient Method (PGM). Our algorithm, dubbed AdaProx, avoids the need for explicit computation of the Lipschitz constants or additional line searches and thus reduces per-iteration cost. In test cases for Constrained Matrix Factorization we demonstrate the advantages of AdaProx in fidelity and performance over PGM, while still allowing for arbitrary penalty functions. The python implementation of the algorithm presented here is available as an open-source package at this https URL ","Proximal Adam: Robust Adaptive Update Scheme for Constrained
  Optimization"
53,1186667501985390592,65434005,Bai Li,['My new paper: learn the tones shapes of Mandarin and Cantonese using a list of spoken words! <LINK>'],https://arxiv.org/abs/1910.08987,"Tone is a prosodic feature used to distinguish words in many languages, some of which are endangered and scarcely documented. In this work, we use unsupervised representation learning to identify probable clusters of syllables that share the same phonemic tone. Our method extracts the pitch for each syllable, then trains a convolutional autoencoder to learn a low dimensional representation for each contour. We then apply the mean shift algorithm to cluster tones in high-density regions of the latent space. Furthermore, by feeding the centers of each cluster into the decoder, we produce a prototypical contour that represents each cluster. We apply this method to spoken multi-syllable words in Mandarin Chinese and Cantonese and evaluate how closely our clusters match the ground truth tone categories. Finally, we discuss some difficulties with our approach, including contextual tone variation and allophony effects. ",Representation Learning for Discovering Phonemic Tone Contours
54,1186636945650278401,716958933039054848,Senellart's QD Group,['PhD student H√©l√®ne Ollivier presenting our exciting new results on the reproducibility of quantum dot single photon sources at #singlephotonworkshop. New paper on the arxiv today! <LINK> <LINK>'],https://arxiv.org/abs/1910.08863,"Single-photon sources based on semiconductor quantum dots have emerged as an excellent platform for high efficiency quantum light generation. However, scalability remains a challenge since quantum dots generally present inhomogeneous characteristics. Here we benchmark the performance of fifteen deterministically fabricated single-photon sources. They display an average indistinguishability of 90.6 +/- 2.8 % with a single-photon purity of 95.4 +/- 1.5 % and high homogeneity in operation wavelength and temporal profile. Each source also has state-of-the-art brightness with an average first lens brightness value of 13.6 +/- 4.4 %. Whilst the highest brightness is obtained with a charged quantum dot, the highest quantum purity is obtained with neutral ones. We also introduce various techniques to identify the nature of the emitting state. Our study sets the groundwork for large-scale fabrication of identical sources by identifying the remaining challenges and outlining solutions. ",Reproducibility of high-performance quantum dot single-photon sources
55,1186627818039529474,57793813,Teppei Katori (È¶ôÂèñÂì≤Âπ≥),"['The latest #T2K paper is about the cross-section measurements of NCQE-like, where NCQE is a new NC process of single nucleon emission + de-excitation photon! (add in your textbook please) #nuxsec\n<LINK> <LINK>']",https://arxiv.org/abs/1910.09439,"Neutrino- and antineutrino-oxygen neutral-current quasielastic-like interactions are measured at Super-Kamiokande using nuclear de-excitation $\gamma$-rays to identify signal-like interactions in data from a $14.94 \ (16.35)\times 10^{20}$ protons-on-target exposure of the T2K neutrino (antineutrino) beam. The measured flux-averaged cross sections on oxygen nuclei are $\langle \sigma_{\nu {\rm -NCQE}} \rangle = 1.70 \pm 0.17 ({\rm stat.}) ^{+ {\rm 0.51}}_{- {\rm 0.38}} ({\rm syst.}) \times 10^{-38} \ {\rm cm^2/oxygen}$ with a flux-averaged energy of 0.82 GeV and $\langle \sigma_{\bar{\nu} {\rm -NCQE}} \rangle = 0.98 \pm 0.16 ({\rm stat.}) ^{+ {\rm 0.26}}_{- {\rm 0.19}} ({\rm syst.}) \times 10^{-38} \ {\rm cm^2/oxygen}$ with a flux-averaged energy of 0.68 GeV, for neutrinos and antineutrinos, respectively. These results are the most precise to date, and the antineutrino result is the first cross section measurement of this channel. They are compared with various theoretical predictions. The impact on evaluation of backgrounds to searches for supernova relic neutrinos at present and future water Cherenkov detectors is also discussed. ","Measurement of neutrino and antineutrino neutral-current
  quasielastic-like interactions on oxygen by detecting nuclear de-excitation
  $\gamma$-rays"
56,1186568420495708161,228380217,James Grist,"['Interested in childhood brain #tumours? We‚Äôve put together a new study looking at telling the difference between three common cancer types, using #AI. Give us your comments, as we‚Äôd love to improve the paper! \n<LINK>']",https://arxiv.org/abs/1910.09247,"The imaging and subsequent accurate diagnosis of paediatric brain tumours presents a radiological challenge, with magnetic resonance imaging playing a key role in providing tumour specific imaging information. Diffusion weighted and perfusion imaging are commonly used to aid the non invasive diagnosis of paediatric brain tumours, but are usually evaluated by expert qualitative review. Quantitative studies are mainly single centre and single modality. The aim of this work was to combine multi centre diffusion and perfusion imaging, with machine learning, to develop machine learning based classifiers to discriminate between three common paediatric tumour types. The results show that diffusion and perfusion weighted imaging of both the tumour and whole brain provide significant features which differ between tumour types, and that combining these features gives the optimal machine learning classifier with greater than 80 percent predictive precision. This work represents a step forward to aid in the non invasive diagnosis of paediatric brain tumours, using advanced clinical imaging. ","Distinguishing between paediatric brain tumour types using
  multi-parametric magnetic resonance imaging and machine learning: a
  multi-site study"
57,1186535693767626753,1049714859548598272,Farsane Tabataba-Vakili,['Check out our new paper on critical coupling in active III-nitride photonic circuits in the blue on ArXiv: <LINK>'],https://arxiv.org/abs/1910.09236,"On-chip microlaser sources in the blue constitute an important building block for complex integrated photonic circuits on silicon. We have developed photonic circuits operating in the blue spectral range based on microdisks and bus waveguides in III-nitride on silicon. We report on the interplay between microdisk-waveguide coupling and its optical properties. We observe critical coupling and phase matching, i.e. the most efficient energy transfer scheme, for very short gap sizes and thin waveguides (g = 45 nm and w = 170 nm) in the spontaneous emission regime. Whispering gallery mode lasing is demonstrated for a wide range of parameters with a strong dependence of the threshold on the loaded quality factor. We show the dependence and high sensitivity of the output signal on the coupling. Lastly, we observe the impact of processing on the tuning of mode resonances due to the very short coupling distances. Such small footprint on-chip integrated microlasers providing maximum energy transfer into a photonic circuit have important potential applications for visible-light communication and lab-on-chip bio-sensors. ","Demonstration of critical coupling in an active III-nitride microdisk
  photonic circuit on silicon"
58,1186473370784800768,1658162341,Narayanan Rengaswamy,"['New paper out just now! <LINK>\nWe rigorously characterize the most general stabilizer codes that support physical transversal T and T^{-1}. @kenbrownquantum @JoshKoomz @dabacon @earltcampbell @CVuillot @QuantumChambs @JarrodMcclean @QuantumGosset', 'As a corollary we prove that CSS codes are optimal for transversal T among non-degenerate stabilizer codes.', 'Personally I think this is my best so far! This paper has been one fun journey that started with two questions I had in mind around October last year:', ""1. Does the symplectic formalism have to be restricted to Cliffords? This lead to https://t.co/ItErm0KxZT\n2. If non-Clifford gates don't map all Paulis to Paulis, then how do some stabilizer codes support physical T gates? At first I didn't get why the codespace is preserved."", ""Conversations with Michael Newman from @kenbrownquantum's group were very helpful in understanding several things! This is the first official collaboration between the Calderbank/Pfister lab and @kenbrownquantum's lab. I look forward to more!"", ""We also show that Bravyi and Haah's triorthogonal codes is essentially the most general family of CSS codes that realize logical transversal T via physical transversal T."", ""As I'm originally trained in classical information and coding theory, I really enjoy that this work produces exciting classical coding problems to work on that are well motivated by quantum constraints, e.g. transversal T. (See CSS-T codes!)""]",http://arxiv.org/abs/1910.09333,"In order to perform universal fault-tolerant quantum computation, one needs to implement a logical non-Clifford gate. Consequently, it is important to understand codes that implement such gates transversally. In this paper, we adopt an algebraic approach to characterize all stabilizer codes for which transversal $T$ and $T^{-1}$ gates preserve the codespace. Our Heisenberg perspective reduces this to a finite geometry problem that translates to the design of certain classical codes. We prove three corollaries: (a) For any non-degenerate $[[ n,k,d ]]$ stabilizer code supporting a physical transversal $T$, there exists an $[[ n,k,d ]]$ CSS code with the same property; (b) Triorthogonal codes are the most general CSS codes that realize logical transversal $T$ via physical transversal $T$; (c) Triorthogonality is necessary for physical transversal $T$ on a CSS code to realize the logical identity. The main tool we use is a recent efficient characterization of certain diagonal gates in the Clifford hierarchy (arXiv:1902.04022). We refer to these gates as Quadratic Form Diagonal (QFD) gates. Our framework generalizes all existing code constructions that realize logical gates via transversal $T$. We provide several examples and briefly discuss connections to decreasing monomial codes, pin codes, generalized triorthogonality and quasitransversality. We partially extend these results towards characterizing all stabilizer codes that support transversal $\pi/2^{\ell}$ $Z$-rotations. In particular, using Ax's theorem on residue weights of polynomials, we provide an alternate characterization of logical gates induced by transversal $\pi/2^{\ell}$ $Z$-rotations on a family of quantum Reed-Muller codes. We also briefly discuss a general approach to analyze QFD gates that might lead to a characterization of all stabilizer codes that support any given physical transversal $1$- or $2$-local diagonal gate. ",On Optimality of CSS Codes for Transversal $T$
59,1186464812508667904,3458090593,Wenhao Yang,['Our new paper on Federated Learning(<LINK>) is available on arXiv. We propose an algorithm(PDSGD) to reduce the communication cost in a decentralized framework. We theoretically prove the convergence rate and empirically show the performance.'],http://arxiv.org/abs/1910.09126,"Recently, the technique of local updates is a powerful tool in centralized settings to improve communication efficiency via periodical communication. For decentralized settings, it is still unclear how to efficiently combine local updates and decentralized communication. In this work, we propose an algorithm named as LD-SGD, which incorporates arbitrary update schemes that alternate between multiple Local updates and multiple Decentralized SGDs, and provide an analytical framework for LD-SGD. Under the framework, we present a sufficient condition to guarantee the convergence. We show that LD-SGD converges to a critical point for a wide range of update schemes when the objective is non-convex and the training data are non-identically independent distributed. Moreover, our framework brings many insights into the design of update schemes for decentralized optimization. As examples, we specify two update schemes and show how they help improve communication efficiency. Specifically, the first scheme alternates the number of local and global update steps. From our analysis, the ratio of the number of local updates to that of decentralized SGD trades off communication and computation. The second scheme is to periodically shrink the length of local updates. We show that the decaying strategy helps improve communication efficiency both theoretically and empirically. ",Communication-Efficient Local Decentralized SGD Methods
60,1186462785317654529,19089454,Dr. Teddy Kareta,"['In other news, our new paper on the active Centaur 174P/Echeclus has been accepted for publication in the Astronomical Journal! We detail observations of its huge December 2017 outburst and find some strange stuff.\n<LINK> <LINK>', ""We combined near-infrared spectra with visible imaging to infer a debris-ejection/mini-fragmentation event smaller than Echeclus's massive 2005 outburst but with many of the same properties. If Echeclus is typical of the active centaurs, other centaurs should be doing this too."", ""(Thankfully, they totally are -- after we submitted the paper, Astronomer's Telegram 13179 was submitted finding something similar-ish at 29P/SW1. Phew.) https://t.co/t7hcVkFln4"", ""We also did some fun (and new for me) dynamical work to show that Echeclus's orbit hasn't changed nearly as much as many other active Centaurs, so the origin of Echeclus's modern strong activity is, for now, unclear. https://t.co/ijsJgxDjli"", 'The co-authors include the (by now) normal cast of characters of : @benjaminsharkey @J_Noons @kat_volk @moonyguy @WaltHarris2 and Dr. Richard Miles.']",https://arxiv.org/abs/1910.09490,"The Centaurs are the small solar system bodies intermediate between the active inner solar system Jupiter Family Comets and their inactive progenitors in the trans-Neptunian region. Among the fraction of Centaurs which show comet-like activity, 174P/Echeclus is best known for its massive 2005 outburst in which a large apparently active fragment was ejected above the escape velocity from the primary nucleus. We present visible imaging and near-infrared spectroscopy of Echeclus during the first week after its December 2017 outburst taken at the Faulkes North & South Telescopes and the NASA IRTF, the largest outburst since 2005. The coma was seen to be highly asymmetric. A secondary peak was seen in the near-infrared 2D spectra, which is strongly hinted at in the visible images, moving hyperbolically with respect to the nucleus. The retrieved reflectance spectrum of Echelcus is consistent with the unobscured nucleus but becomes bluer when a wider extraction aperture is used. We find that Echeclus's coma is best explained as dominated by large blue dust grains, which agrees with previous work. We also conducted a high-resolution orbital integration of Echeclus's recent evolution and found no large orbital changes that could drive its modern evolution. We interpret the second peak in the visible and near-infrared datasets as a large cloud of larger-than-dust debris ejected at the time of outburst. If Echeclus is typical of the Centaurs, there may be several debris ejection or fragmentation events per year on other Centaurs that are going unnoticed. ","Physical Characterization of the December 2017 Outburst of the Centaur
  174P/Echeclus"
61,1186460974791806976,369569444,Takahiro TERADA (ÂØ∫Áî∞ ÈöÜÂ∫É),"['Our new paper on the #Swampland conjecture #TCC and single-field slow-roll #inflation is available on <LINK>\nFor smooth potentials, the maximally possible e-folding number has a tight lower bound. A sizable running parameter is expected to be compatible with TCC. <LINK>']",https://arxiv.org/abs/1910.09460,"It was recently proposed that a field theory cannot be consistent with quantum gravity if it allows a mode shorter than the Planck length to exit the Hubble horizon. This is called the Trans-Planckian Censorship Conjecture (TCC). We discuss the implications of the TCC on the possible shape of the inflaton potential in single-field slow-roll inflation. We point out that (1) there is generically an initial condition in which the total e-folding number $N_\text{total}$ is doubled or more compared to the e-folds necessary for the cosmic microwave background fluctuations, and (2) a sizable negative running of spectral index is generically expected to make $N_\text{total}$ small. In concrete setups, we find a stringent constraint on the inflationary energy scale, $V_\text{inf}^{1/4} < \mathcal{O}(10) \, \text{TeV}$ with $r < \mathcal{O}(10^{-50})$, and the running parameter is bounded above as $\alpha_\text{s} \lesssim - 4 \times 10^{-3}$. ",Trans-Planckian censorship and single-field inflaton potential
62,1186403966881615874,314395154,Tengyu Ma,['A new paper on improving the generalization of deep models (w.r.t clean or robust accuracy) by theory-inspired explicit regularizers. <LINK>'],https://arxiv.org/abs/1910.04284,"For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound -- a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the ""all-layer margin."" Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice. ","Improved Sample Complexities for Deep Networks and Robust Classification
  via an All-Layer Margin"
63,1186296492287332352,1048984881131401217,Cora Dvorkin,"['Very proud of our new CMB component separation paper, with fantastic former Harvard undergraduate students Sebastian Wagner-Carena and Max Hopkins and my Harvard PhD student Ana Diaz Rivero: <LINK> @harvardphysics @Stanford @UCSanDiego']",https://arxiv.org/abs/1910.08077,"We present a novel technique for Cosmic Microwave Background (CMB) foreground subtraction based on the framework of blind source separation. Inspired by previous work incorporating local variation to Generalized Morphological Component Analysis (GMCA), we introduce Hierarchical GMCA (HGMCA), a Bayesian hierarchical graphical model for source separation. We test our method on $N_{\rm side}=256$ simulated sky maps that include dust, synchrotron, free-free and anomalous microwave emission, and show that HGMCA reduces foreground contamination by $25\%$ over GMCA in both the regions included and excluded by the Planck UT78 mask, decreases the error in the measurement of the CMB temperature power spectrum to the $0.02-0.03\%$ level at $\ell>200$ (and $<0.26\%$ for all $\ell$), and reduces correlation to all the foregrounds. We find equivalent or improved performance when compared to state-of-the-art Internal Linear Combination (ILC)-type algorithms on these simulations, suggesting that HGMCA may be a competitive alternative to foreground separation techniques previously applied to observed CMB data. Additionally, we show that our performance does not suffer when we perturb model parameters or alter the CMB realization, which suggests that our algorithm generalizes well beyond our simplified simulations. Our results open a new avenue for constructing CMB maps through Bayesian hierarchical analysis. ","A Novel CMB Component Separation Method: Hierarchical Generalized
  Morphological Component Analysis"
64,1186252152588701699,245373685,Javier Nogales,['Our new paper finished: massive time-series forecasting with recurrent neural networks <LINK> #datascience <LINK>'],https://arxiv.org/abs/1910.06640,"Most electricity systems worldwide are deploying advanced metering infrastructures to collect relevant operational data. In particular, smart meters allow tracking electricity load consumption at a very disaggregated level and at high frequency rates. This data opens the possibility of developing new forecasting models with a potential positive impact in electricity systems. We present a general methodology that is able to process and forecast a large number of smart meter time series. Instead of using traditional and univariate approaches, we develop a single but complex recurrent neural-network model with long short-term memory that can capture individual consumption patterns and also consumptions from different households. The resulting model can accurately predict future loads (short-term) of individual consumers, even if these were not included in the original training set. This entails a great potential for large scale applications as once the single network is trained, accurate individual forecast for new consumers can be obtained at almost no computational cost. The proposed model is tested under a large set of numerical experiments by using a real-world dataset with thousands of disaggregated electricity consumption time series. Furthermore, we explore how geo-demographic segmentation of consumers may impact the forecasting accuracy of the model. ","A Single Scalable LSTM Model for Short-Term Forecasting of Disaggregated
  Electricity Loads"
65,1186198353148858368,280083723,Yoh Tanimoto,['new paper~ <LINK> we prove unitarity of the vacuum modules of the W_3 algebra with c &gt;= 2. Next we will construct conformal nets for them'],https://arxiv.org/abs/1910.08334,"We prove unitarity of the vacuum representation of the $\mathcal{W}_3$-algebra for all values of the central charge $c\geq 2$. We do it by modifying the free field realization of Fateev and Zamolodchikov resulting in a representation which, by a nontrivial argument, can be shown to be unitary on a certain invariant subspace, although it is not unitary on the full space of the two currents needed for the construction. These vacuum representations give rise to simple unitary vertex operator algebras. We also construct explicitly unitary representations for many positive lowest weight values. Taking into account the known form of the Kac determinants, we then completely clarify the question of unitarity of the irreducible lowest weight representations of the $\mathcal{W}_3$-algebra in the $2\leq c\leq 98$ region. ",Unitary representations of the $\mathcal{W}_3$-algebra with $c\geq 2$
66,1186161085637943296,2191249633,Sebastiano Carpi,"['New paper on the arXiv: ""Unitary representations of the W_3 algebra with c \\geq 2"" (with Yoh Tanimoto and Mihaly Weiner)         <LINK>']",https://arxiv.org/abs/1910.08334,"We prove unitarity of the vacuum representation of the $\mathcal{W}_3$-algebra for all values of the central charge $c\geq 2$. We do it by modifying the free field realization of Fateev and Zamolodchikov resulting in a representation which, by a nontrivial argument, can be shown to be unitary on a certain invariant subspace, although it is not unitary on the full space of the two currents needed for the construction. These vacuum representations give rise to simple unitary vertex operator algebras. We also construct explicitly unitary representations for many positive lowest weight values. Taking into account the known form of the Kac determinants, we then completely clarify the question of unitarity of the irreducible lowest weight representations of the $\mathcal{W}_3$-algebra in the $2\leq c\leq 98$ region. ",Unitary representations of the $\mathcal{W}_3$-algebra with $c\geq 2$
67,1185945398541725698,2750994354,Tai-Danae Bradley,"['I‚Äôm happy to share a new paper with @MStoudenmire and John Terilla: <LINK> We share a tensor network generative model, a deterministic training algo &amp; estimate generalization error, all with the clarity of linear algebra. Now on the blog! <LINK> <LINK>', '@MStoudenmire I‚Äôll explain some of the ideas here, sticking to a ""lite"" version. (Check out the paper for the full version!)\n\nIf you‚Äôve been following the posts on Math3ma for the past 6-or-so months, you‚Äôll be delighted to know the content is all related. But more on that later‚Ä¶', '@MStoudenmire Alright, before jumping in, let‚Äôs warm up with a question: https://t.co/Pvo1rQ07Qt', '@MStoudenmire Example? Take bitstrings‚Äîsequences of 0s and 1s, all of length 16, say. There are 65,536 of them but suppose we only have a fraction, drawn from some probability distribution. We may/may not know what it is, but perhaps we‚Äôd like to use the samples to infer it and sample from it.', '@MStoudenmire As another example, suppose our data points are meaningful sentences‚Äîsequences of words. Natural language has a probability distribution on it (e.g. ‚Äúcute little dog‚Äù has higher probability of being spoken than ‚Äútomato rickety blue‚Äù), though we don‚Äôt have access to it.', '@MStoudenmire But we might like to use the data we *do* have to estimate the probability distribution on language with the goal of generating new, plausible text. (Hello language models!) https://t.co/sTHC1YImPk', '@MStoudenmire These examples, I hope, help put the paper in context. We‚Äôre interested modeling probability distributions given samples from some dataset. \n\nMore specifically, we‚Äôre interested in modeling probability distributions on *sequences*‚Äîstrings of symbols from some finite alphabet.', '@MStoudenmire So the goal is two-fold:\n\n1. Find a model that can *infer* a probability distribution if you show it a few examples.\n\n2. Understand the theory well enough to *predict* how good the model will perform based the number of examples used.', '@MStoudenmire And that‚Äôs what we do! We present a theoretical model and share a training algorithm, and also include a roadmap for theoretical analysis, with experimental results to illustrate. https://t.co/5euDyXSkHn', '@MStoudenmire Specifically, we work with the even-parity dataset: We start with some examples of even parity bitstrings of length 16 (a bitstring has ‚Äòeven parity‚Äô if it contains an even number of 1s), and then aim to predict model‚Äôs performance based on the fraction of examples used. https://t.co/JsdirWl1MD', '@MStoudenmire I won‚Äôt get into details (they‚Äôre in the paper!) but I will share a few words about the model since I‚Äôve blogged/tweeted about it before. \n\nIt‚Äôs very simple: We model probability distributions by density operators, a special kind of linear transformation. https://t.co/1YkYegMNGs', '@MStoudenmire I‚Äôve written about this idea before. Check out ‚ÄúA First Look at Quantum Probability‚Äù for all the details: how to get a density operator from a probability distribution, and why this is a useful thing to do! https://t.co/nESiCTbaly https://t.co/0YbvzArPCP', '@MStoudenmire Or for a bite-sized intro to these ideas on quantum probability theory, check out this old thread: https://t.co/ep1DsTqbOD', '@MStoudenmire In any case, the main idea is very simple and worth sharing again:\n\nGiven an empirical probability distribution on a training set of sequences, we define a unit vector which is essentially the sum of the samples weighted by the *square roots* of their probabilities. https://t.co/uQyP3uAD62', '@MStoudenmire By passing from a probability distribution to a unit vector, we‚Äôve moved from the world of probability theory to the world of linear algebra.\n\nAnd since the coordinates of the vector are *square roots* of probabilities, we are in the realm of ‚Äú2-norm probability.‚Äù', ""@MStoudenmire ‚Äú2-norm probability‚Äù usually goes by the name of quantum probability‚Äîand a density operator (obtained by orthogonal projection onto that unit vector) is called a quantum state‚Äîbut it's really just another way to think about probability. \n\nIt's just linear algebra!"", '@MStoudenmire Alright, so what do we do with that unit vector? (i.e. the sum of training samples, weighted by the square roots of their probabilities.) \n\nWe tweak it to get a *new* vector.\n\nThis is all happening in a tensor product of Hilbert spaces, so there are nice tensor network diagrams: https://t.co/tYlquPDwHQ', ""@MStoudenmire The new vector is a special kind called a matrix product state (MPS). \n\nMPS are nice bc far fewer parameters are needed to describe them than a generic vector in a tensor product.\xa0This is especially useful if you're working in ultra-large-dimensional spaces. MPS are efficient!"", '@MStoudenmire Anyways, the punchline: We reach our goal! The probability distribution defined by (the squares of the entries of) the MPS is very close to the *true* distribution on bitstrings that we wanted to infer from the dataset.', ""@MStoudenmire Even better, we can estimate the model's performance, based on the fraction of training examples used:\n\nüî∂ = experimental average\nüî∑ = theoretical prediction\n\nThe results are produced using the @ITensorLib library, https://t.co/uWHDgO1uL7; details are in Section 6 of the paper. https://t.co/Ez7qYDxvK5"", '@MStoudenmire @ITensorLib Section 4 presents the training algorithm to get the MPS. It consists of successive applications of SVD on reduced density operators. The reason why we do this‚Äîand why an MPS\xa0is a natural choice of model‚Äîis inside this thread on quantum probability theory:\nhttps://t.co/wdSEAXnbJQ', '@MStoudenmire @ITensorLib In short, the success boils down to ONE crucial fact:\n\n‚ú®Eigenvectors of the reduced densities of pure, entangled quantum states contain conditional probabilistic information about your data.‚ú®\n\nI won‚Äôt unwind this here. It‚Äôs all laid out on Math3ma:\nhttps://t.co/nESiCTbaly', '@MStoudenmire @ITensorLib So we capitalize on this. Each tensor‚Äîblue node‚Äîin our MPS model *is* an array of eigenvectors of some reduced density operator obtained from that original vector (the sum of the training samples, weighted by the square roots of their probabilities). https://t.co/hPCTUMSQ4z', '@MStoudenmire @ITensorLib Result? The model *knows* what ""words"" go together to form ""meaningful sentences"" just based on the statistics of the training data. And it\'s all just simple linear algebra, so we understand when/why/how the model succeeds or fails, given the number of training examples.', '@MStoudenmire @ITensorLib Mission accomplished!', '@MStoudenmire @ITensorLib By the way, the heart of the algorithm is really very simple, but it goes by a sophisticated name in physics literature: the ""density matrix renormalization group"" (DMRG) procedure, introduced in 1992 by physicist Steven White. \n\nhttps://t.co/qWsBJOLmk8', '@MStoudenmire @ITensorLib DMRG is usually used to find a particular eigenvector of a linear operator (the Hamiltonian of a quantum many-body system), but we instead apply it to our special vector: the sum of the training examples.', '@MStoudenmire @ITensorLib In short, the results are clear and interpretable since it\'s all just linear algebra, so we\'re able to give an ""under the hood"" analysis of the theory in Section 5, which is what allows us to make those theoretical predictions about the model\'s ability to generalize.', ""@MStoudenmire @ITensorLib (There's a bit of combinatorics involved, and we end up representing matrices as bipartite graphs for bookkeeping purposes. This is what inspired the article ‚ÄòViewing Matrices and Probability as Graphs‚Äô from earlier this year.)\n\nhttps://t.co/VP8FGi1gwX"", ""@MStoudenmire @ITensorLib There's lots more to say, but I'll stop here. \n\nI might share a deeper look into the mathematics later in the future. In the mean time, feel free to dive in!  https://t.co/FVdTGhETMP\n\n&lt;/thread&gt; https://t.co/Ud2VMsGk0q""]",http://arxiv.org/abs/1910.07425,"Classical probability distributions on sets of sequences can be modeled using quantum states. Here, we do so with a quantum state that is pure and entangled. Because it is entangled, the reduced densities that describe subsystems also carry information about the complementary subsystem. This is in contrast to the classical marginal distributions on a subsystem in which information about the complementary system has been integrated out and lost. A training algorithm based on the density matrix renormalization group (DMRG) procedure uses the extra information contained in the reduced densities and organizes it into a tensor network model. An understanding of the extra information contained in the reduced densities allow us to examine the mechanics of this DMRG algorithm and study the generalization error of the resulting model. As an illustration, we work with the even-parity dataset and produce an estimate for the generalization error as a function of the fraction of the dataset used in training. ",Modeling Sequences with Quantum States: A Look Under the Hood
68,1185220669430218752,21306343,"Prof. Barry O'Sullivan, MRIA","['Guillaume Escamocher and I have just posted a new paper of ours entitled ""Solving Logic Grid Puzzles with an Algorithm that Imitates Human Behavior"" on arXiv. #AI #ArtificialIntelligence <LINK> <LINK>']",https://arxiv.org/abs/1910.06636,"We present in this paper our solver for logic grid puzzles. The approach used by our algorithm mimics the way a human would try to solve the same problem. Every progress made during the solving process is accompanied by a detailed explanation of our program's reasoning. Since this reasoning is based on the same heuristics that a human would employ, the user can easily follow the given explanation. ","Solving Logic Grid Puzzles with an Algorithm that Imitates Human
  Behavior"
69,1185013095686643714,301341538,Renjie Liao,['Predicting the future behaviour of pedestrians with uncertainty is important for self-driving. We propose a Discrete Residual Flow Network which leverages map information and captures the multi-modality. Check our new #CoRL2019 paper: <LINK> <LINK>'],https://arxiv.org/abs/1910.08041,"Self-driving vehicles plan around both static and dynamic objects, applying predictive models of behavior to estimate future locations of the objects in the environment. However, future behavior is inherently uncertain, and models of motion that produce deterministic outputs are limited to short timescales. Particularly difficult is the prediction of human behavior. In this work, we propose the discrete residual flow network (DRF-Net), a convolutional neural network for human motion prediction that captures the uncertainty inherent in long-range motion forecasting. In particular, our learned network effectively captures multimodal posteriors over future human motion by predicting and updating a discretized distribution over spatial locations. We compare our model against several strong competitors and show that our model outperforms all baselines. ",Discrete Residual Flow for Probabilistic Pedestrian Behavior Prediction
70,1184868134676828161,763033119356256256,Rishad Shafik,['Temporal encoding offers a novel #PervasiveAI hardware design approach and a *significant leap* towards circuits with minimal power management/regulation. Read our new Royal Soc. Philo. Trans. A paper here: <LINK>\n#PervasiveAI #RealPowerCircuits @nclmicrosystems'],https://arxiv.org/abs/1910.07492,"Neural Networks (NNs) are steering a new generation of artificial intelligence (AI) applications at the micro-edge. Examples include wireless sensors, wearables and cybernetic systems that collect data and process them to support real-world decisions and controls. For energy autonomy, these applications are typically powered by energy harvesters. As harvesters and other power sources which provide energy autonomy inevitably have power variations, the circuits need to robustly operate over a dynamic power envelope. In other words, the NN hardware needs to be able to function correctly under unpredictable and variable supply voltages. In this paper, we propose a novel NN design approach using the principle of pulse width modulation (PWM). PWM signals represent information with their duty cycle values which may be made independent of the voltages and frequencies of the carrier signals. We design a PWM-based perceptron which can serve as the fundamental building block for NNs, by using an entirely new method of realising arithmetic in the PWM domain. We analyse the proposed approach building from a 3x3 perceptron circuit to a complex multi-layer NN. Using handwritten character recognition as an exemplar of AI applications, we demonstrate the power elasticity, resilience and efficiency of the proposed NN design in the presence of functional and parametric variations including large voltage variations in the power supply. ","Neural Network Design for Energy-Autonomous AI Applications using
  Temporal Encoding"
71,1184781421036163072,958312958593064961,Mikayel Samvelyan,"['Our new ""#MAVEN: Multi-Agent Variational Exploration"" paper for committed exploration in multi-agent #RL, which has been accepted at @NeurIPSConf , is now out! \n\nCheck it out - <LINK>\n\n#NeurIPS2019 <LINK>']",https://arxiv.org/abs/1910.07483,"Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43]. ",MAVEN: Multi-Agent Variational Exploration
72,1184775673233190912,927216144620183553,Anuj Mahajan üáÆüá≥,"['Checkout our new paper about tackling suboptimality with committed exploration in multi agent systems  <LINK>. Nice working with coauthors Tabish, @samveIyan and @shimon8282. See you at #NeurIPS19! <LINK>']",https://arxiv.org/abs/1910.07483,"Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43]. ",MAVEN: Multi-Agent Variational Exploration
73,1184653166916390912,930272306022223873,The Sokolov Lab,"['A preprint of our new paper is out on arXiv. We describe implementation of algebraic diagrammatic construction theory for electron attachment and ionization and its applications to small molecules, DNA/RNA nucleobases, X-ray ionization, and hydrogen chains\n<LINK>']",https://arxiv.org/abs/1910.07116,"We present implementation of second- and third-order algebraic diagrammatic construction theory for efficient and accurate computations of molecular electron affinities (EA), ionization potentials (IP), and densities of states (EA-/IP-ADC(n), n = 2, 3). Our work utilizes the non-Dyson formulation of ADC for the single-particle propagator and reports working equations and benchmark results for the EA-ADC(2) and EA-ADC(3) approximations. We describe two algorithms for solving EA-/IP-ADC equations: (i) conventional algorithm that uses iterative diagonalization techniques to compute low-energy EA, IP, and density of states, and (ii) Green's function algorithm (GF-ADC) that solves a system of linear equations to compute density of states directly for a specified spectral region. To assess accuracy of EA-ADC(2) and EA-ADC(3), we benchmark their performance for a set of atoms, small molecules, and five DNA/RNA nucleobases. As our next step, we demonstrate efficiency of our GF-ADC implementation by computing core-level K-, L-, and M-shell ionization energies of a zinc atom without introducing core-valence separation approximation. Finally, we use EA- and IP-ADC methods to compute band gaps of equally-spaced hydrogen chains Hn with n up to 150, providing their estimates near thermodynamic limit. Our results demonstrate that EA-/IP-ADC(n) (n = 2, 3) methods are efficient and accurate alternatives to widely used electronic structure methods for simulations of electron attachment and ionization properties. ","Third-order algebraic diagrammatic construction theory for electron
  attachment and ionization energies: Conventional and Green's function
  implementation"
74,1184490296853315589,7352832,Theo Weber,"['Really excited to share this new paper with Lars Buesing and Nicolas Heess: <LINK>\nBy following connections between RL and inference, we inspire ourselves from alpha-zero style MCTS (for optimization) and develop a tree-search based alg. for inference.']",https://arxiv.org/abs/1910.06862,"A plethora of problems in AI, engineering and the sciences are naturally formalized as inference in discrete probabilistic models. Exact inference is often prohibitively expensive, as it may require evaluating the (unnormalized) target density on its entire domain. Here we consider the setting where only a limited budget of calls to the unnormalized density oracle is available, raising the challenge of where in the domain to allocate these function calls in order to construct a good approximate solution. We formulate this problem as an instance of sequential decision-making under uncertainty and leverage methods from reinforcement learning for probabilistic inference with budget constraints. In particular, we propose the TreeSample algorithm, an adaptation of Monte Carlo Tree Search to approximate inference. This algorithm caches all previous queries to the density oracle in an explicit search tree, and dynamically allocates new queries based on a ""best-first"" heuristic for exploration, using existing upper confidence bound methods. Our non-parametric inference method can be effectively combined with neural networks that compile approximate conditionals of the target, which are then used to guide the inference search and enable generalization across multiple target distributions. We show empirically that TreeSample outperforms standard approximate inference methods on synthetic factor graphs. ","Approximate Inference in Discrete Distributions with Monte Carlo Tree
  Search and Value Functions"
75,1184429680746815488,839820726777544705,Alexia Jolicoeur-Martineau,"['My new paper is out! We show a framework in which we can both derive #SVMs and gradient penalized #GANs! We also show how to make better gradient penalties!\n\n<LINK>\n<LINK> <LINK>', 'Researchers who might be interested in this: @__ishaan @dumoulinv @goodfellow_ian @martinarjovsky @soumithchintala', '@ionicdevil Thank you for the compliment! I worked very hard on this. I try to generalize things as much as possible and present the big picture.', 'I would also like to thank @BorealisAI who helped me through their fellowship. Make sure to apply to this year Fellowship https://t.co/gj7Qcgx4jf, they extended their deadline to October 20.']",https://arxiv.org/abs/1910.06922,"A popular heuristic for improved performance in Generative adversarial networks (GANs) is to use some form of gradient penalty on the discriminator. This gradient penalty was originally motivated by a Wasserstein distance formulation. However, the use of gradient penalty in other GAN formulations is not well motivated. We present a unifying framework of expected margin maximization and show that a wide range of gradient-penalized GANs (e.g., Wasserstein, Standard, Least-Squares, and Hinge GANs) can be derived from this framework. Our results imply that employing gradient penalties induces a large-margin classifier (thus, a large-margin discriminator in GANs). We describe how expected margin maximization helps reduce vanishing gradients at fake (generated) samples, a known problem in GANs. From this framework, we derive a new $L^\infty$ gradient norm penalty with Hinge loss which generally produces equally good (or better) generated output in GANs than $L^2$-norm penalties (based on the Fr\'echet Inception Distance). ",Gradient penalty from a maximum margin perspective
76,1184375048951480321,13879522,Giulio Rossetti,"['Here\'s the preprint of the second paper by our new PhD student @SalvatoreCitra3: ""EVA:  Attribute-Aware Network segmentation""\n <LINK>\n\nLooking forward for the presentation at @complex_nets this December!\n\n@kdd_lab @DI_Unipisa @SoBigData\n#networkscience']",https://arxiv.org/abs/1910.06599,"Identifying topologically well-defined communities that are also homogeneous w.r.t. attributes carried by the nodes that compose them is a challenging social network analysis task. We address such a problem by introducing Eva, a bottom-up low complexity algorithm designed to identify network hidden mesoscale topologies by optimizing structural and attribute-homophilic clustering criteria. We evaluate the proposed approach on heterogeneous real-world labeled network datasets, such as co-citation, linguistic, and social networks, and compare it with state-of-art community discovery competitors. Experimental results underline that Eva ensures that network nodes are grouped into communities according to their attribute similarity without considerably degrading partition modularity, both in single and multi node-attribute scenarios ",Eva: Attribute-Aware Network Segmentation
77,1184186671043989504,63211254,Jorgen Thelin,"['New research paper available: ""Blink - Fast and Generic Collectives for Distributed ML"" <LINK>']",https://arxiv.org/abs/1910.04940,"Model parameter synchronization across GPUs introduces high overheads for data-parallel training at scale. Existing parameter synchronization protocols cannot effectively leverage available network resources in the face of ever increasing hardware heterogeneity. To address this, we propose Blink, a collective communication library that dynamically generates optimal communication primitives by packing spanning trees. We propose techniques to minimize the number of trees generated and extend Blink to leverage heterogeneous communication channels for faster data transfers. Evaluations show that compared to the state-of-the-art (NCCL), Blink can achieve up to 8x faster model synchronization, and reduce end-to-end training time for image classification tasks by up to 40%. ",Blink: Fast and Generic Collectives for Distributed ML
78,1184176079432499200,40285266,Stanislav Fort at EAGx Prague ¬¨(üî•üìéüî•üìé),['Excited to announce our new paper Emergent properties of the local geometry of neural loss landscapes <LINK> with my great advisor @SuryaGanguli! We used a simple model to explain 4 surprising effects of local geometry of neural network landscapes. <LINK>'],https://arxiv.org/abs/1910.05929,"The local geometry of high dimensional neural network loss landscapes can both challenge our cherished theoretical intuitions as well as dramatically impact the practical success of neural network training. Indeed recent works have observed 4 striking local properties of neural loss landscapes on classification tasks: (1) the landscape exhibits exactly $C$ directions of high positive curvature, where $C$ is the number of classes; (2) gradient directions are largely confined to this extremely low dimensional subspace of positive Hessian curvature, leaving the vast majority of directions in weight space unexplored; (3) gradient descent transiently explores intermediate regions of higher positive curvature before eventually finding flatter minima; (4) training can be successful even when confined to low dimensional {\it random} affine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of higher than average curvature. We develop a simple theoretical model of gradients and Hessians, justified by numerical experiments on architectures and datasets used in practice, that {\it simultaneously} accounts for all $4$ of these surprising and seemingly unrelated properties. Our unified model provides conceptual insights into the emergence of these properties and makes connections with diverse topics in neural networks, random matrix theory, and spin glasses, including the neural tangent kernel, BBP phase transitions, and Derrida's random energy model. ",Emergent properties of the local geometry of neural loss landscapes
79,1184117174652325888,3885912072,Marshall Johnson,"['New paper alert! <LINK> ""A Binary of Ice and Fire"" We discovered with the KELT survey what we initially thought could be the first hot Jupiter around a B star, but the transiting object turned out to be an M star instead.', 'The lead authors, Dan Stevens and George Zhou (neither of whom are on Twitter AFAIK) did a great job characterizing this extreme mass ratio EB, including the @NASA_TESS  light curve and Doppler tomography which showed the orbit is aligned with the host star rotation.']",https://arxiv.org/abs/1910.06212,"We present the discovery of \thisstar\ (HD 58730), a very low mass ratio ($q \equiv M_2/M_1 \approx 0.07$) eclipsing binary (EB) identified by the Kilodegree Extremely Little Telescope (KELT) survey. We present the discovery light curve and perform a global analysis of four high-precision ground-based light curves, the Transiting Exoplanets Survey Satellite (TESS) light curve, radial velocity (RV) measurements, Doppler Tomography (DT) measurements, and the broad-band spectral energy distribution (SED). Results from the global analysis are consistent with a fully convective ($M_2 = 0.22 \pm 0.02\ M_{\odot})$ M star transiting a late-B primary ($M_1 = 3.34^{+0.07}_{-0.09}\ M_{\odot};\ T_{\rm eff,1} = 11960^{+430}_{-520}\ {\rm K}$). We infer that the primary star is $183_{-30}^{+33}$ Myr old and that the companion star's radius is inflated by $26 \pm 8\%$ relative to the predicted value from a low-mass isochrone of similar age. We separately and analytically fit for the variability in the out-of-eclipse TESS phase curve, finding good agreement between the resulting stellar parameters and those from the global fit. Such systems are valuable for testing theories of binary star formation and understanding how the environment of a star in a close-but-detached binary affects its physical properties. In particular, we examine how a star's properties in such a binary might differ from the properties it would have in isolation. ","An Extreme-mass Ratio, Short-period Eclipsing Binary Consisting of a B
  Dwarf Primary and a Pre-main Sequence M Star Companion Discovered by KELT"
80,1184107699803262983,3150787230,Xiaofan Liang ,"['My paper with Santa Fe Institute Researchers on ""The Scalability, Efficiency and Complexity of Universities and Colleges: A New Lens for Assessing the Higher Educational System"" is now on Arxiv! We applied the urban scaling framework to institutions. <LINK>']",https://arxiv.org/abs/1910.05470,"The growing need for affordable and accessible higher education is a major global challenge for the 21st century. Consequently, there is a need to develop a deeper understanding of the functionality and taxonomy of universities and colleges and, in particular, how their various characteristics change with size. Scaling has been a powerful tool for revealing systematic regularities in systems across a range of topics from physics and biology to cities, and for understanding the underlying principles of their organization and growth. Here, we apply this framework to institutions of higher learning in the United States and show that, like organisms, ecosystems and cities, they scale in a surprisingly systematic fashion following simple power law behavior. We analyze the entire spectrum encompassing 5,802 institutions ranging from large research universities to small professional schools, organized in seven commonly used sectors, which reveal distinct regimes of institutional scaling behavior. Metrics include variation in expenditures, revenues, graduation rates and estimated economic added value, expressed as functions of total enrollment, our fundamental measure of size. Our results quantify how each regime of institution leverages specific economies of scale to address distinct priorities. Taken together, the scaling of features within a sector and shifts in scaling across sectors implies that there are generic mechanisms and constraints shared by all sectors which lead to tradeoffs between their different societal functions and roles. We particularly highlight the strong complementarity between public and private research universities, and community and state colleges, four sectors that display superlinear returns to scale. ","The Scalability, Efficiency and Complexity of Universities and Colleges:
  A New Lens for Assessing the Higher Educational System"
81,1183952735705554944,731516236962496512,Ching-Yao Chuang,"['Ever wonder why domain-invariant representations work even without common support? In our new paper, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. <LINK> <LINK>', 'The error of DANN on simple benchmark MNIST-&gt;MNIST-M increases by 19.8% when 4 more CNN layers are added to the encoder!\n\nJoint work with Antonio Torralba and Stefanie Jegelka.']",https://arxiv.org/abs/1910.05804,"Unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain. One popular approach to this problem is to learn domain-invariant embeddings for both domains. In this work, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. In particular, this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Next, we specify our theoretical framework to multilayer neural networks. As a result, we develop a strategy that mitigates sensitivity to the embedding complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff. ",The Role of Embedding Complexity in Domain-invariant Representations
82,1183945822578167809,75498067,Dr. Dennis Foren üé®,"[""We've got a new paper up on the arXiv! ü§© Check out...\n\n‚öõÔ∏è SUM RULES FOR MASSIVE SPIN-2 KALUZA-KLEIN\nELASTIC SCATTERING AMPLITUDES ‚öõÔ∏è\n\n... right here: <LINK> <LINK>"", ""@InertialObservr It's nice to finally have these results in a paper! üòÅ""]",https://arxiv.org/abs/1910.06159,"It has recently been shown explicitly that the high-energy scattering amplitude of the longitudinal modes of massive spin-2 Kaluza Klein states in compactified 5-dimensional gravity, which would naively grow like O(s^5), grow only like O(s). Since the individual contributions to these amplitudes do grow like O(s^5), the required cancellations between these individual contributions result from intricate relationships between the masses of these states and their couplings. Here we report the explicit form of these sum-rule relationships which ensure the necessary cancellations for elastic scattering of spin-2 Kaluza Klein states in a Randall-Sundrum model. We consider an Anti-de-Sitter space of arbitrary curvature, including the special case of a toroidal compactification in which the curvature vanishes. The sum rules demonstrate that the cancellations at O(s^5) and O(s^4) are generic for a compact extra dimension, and arise from the Sturm-Liouville structure of the eigenmode system in the internal space. Separately, the sum rules at O(s^3) and O(s^2) illustrate the essential role of the radion mode of the extra-dimensional metric, which is the dynamical mode related to the size of the internal space. ",Sum Rules for Massive Spin-2 Kaluza-Klein Elastic Scattering Amplitudes
83,1183939545169121280,140287694,Anowar J Shajib,"['Our paper measuring H_0 from a new strong lens is out on arXiv: <LINK>.\n\nThis very rare double-source lens‚Äîdiscovered in @theDESurvey data‚Äîgives us the most precise single-system measurement (3.9%) to date, H_0 = 74.2+2.7-3.0 km/s/Mpc.\n\nImage from @NASAHubble. <LINK>']",https://arxiv.org/abs/1910.06306,"We present a blind time-delay cosmographic analysis for the lens system DES J0408$-$5354. This system is extraordinary for the presence of two sets of multiple images at different redshifts, which provide the opportunity to obtain more information at the cost of increased modelling complexity with respect to previously analyzed systems. We perform detailed modelling of the mass distribution for this lens system using three band Hubble Space Telescope imaging. We combine the measured time delays, line-of-sight central velocity dispersion of the deflector, and statistically constrained external convergence with our lens models to estimate two cosmological distances. We measure the ""effective"" time-delay distance corresponding to the redshifts of the deflector and the lensed quasar $D_{\Delta t}^{\rm eff}=3382^{+146}_{-115}$ Mpc and the angular diameter distance to the deflector $D_{\rm d}=1711^{+376}_{-280}$ Mpc, with covariance between the two distances. From these constraints on the cosmological distances, we infer the Hubble constant $H_0 = 74.2^{+2.7}_{-3.0}$ km s$^{-1}$ Mpc$^{-1}$ assuming a flat $\Lambda$CDM cosmology and a uniform prior for $\Omega_{\rm m}$ as $\Omega_{\rm m} \sim \mathcal{U}(0.05, 0.5)$. This measurement gives the most precise constraint on $H_0$ to date from a single lens. Our measurement is consistent with that obtained from the previous sample of six lenses analyzed by the $H_0$ Lenses in COSMOGRAIL's Wellspring (H0LiCOW) collaboration. It is also consistent with measurements of $H_0$ based on the local distance ladder, reinforcing the tension with the inference from early Universe probes, for example, with 2.2$\sigma$ discrepancy from the cosmic microwave background measurement. ","STRIDES: a 3.9 per cent measurement of the Hubble constant from the
  strong lens system DES J0408-5354"
84,1183914066613592065,3053138382,Russell Zhang Kunes,['Our paper on improving the prediction accuracy and coherence of supervised topic models just went up on Arxiv: <LINK>. We introduce a new approach for fitting topic models that leverages supervisory signal to only retain terms that are task-relevant'],https://arxiv.org/abs/1910.05495,"Supervised topic models are often sought to balance prediction quality and interpretability. However, when models are (inevitably) misspecified, standard approaches rarely deliver on both. We introduce a novel approach, the prediction-focused topic model, that uses the supervisory signal to retain only vocabulary terms that improve, or at least do not hinder, prediction performance. By removing terms with irrelevant signal, the topic model is able to learn task-relevant, coherent topics. We demonstrate on several data sets that compared to existing approaches, prediction-focused topic models learn much more coherent topics while maintaining competitive predictions. ",Prediction Focused Topic Models via Feature Selection
85,1183802647779794944,2873891847,Shirley Ho,"['Accepted for #NeurIPS2019 ML for Physical Sciences workshop! <LINK> led by @ElenaGiusarma\nNew paper in simulating the universe with neutrinos 10,000 times faster! <LINK>']",http://arxiv.org/abs/1910.04255,"Measuring the sum of the three active neutrino masses, $M_\nu$, is one of the most important challenges in modern cosmology. Massive neutrinos imprint characteristic signatures on several cosmological observables in particular on the large-scale structure of the Universe. In order to maximize the information that can be retrieved from galaxy surveys, accurate theoretical predictions in the non-linear regime are needed. Currently, one way to achieve those predictions is by running cosmological numerical simulations. Unfortunately, producing those simulations requires high computational resources -- seven hundred CPU hours for each neutrino mass case. In this work, we propose a new method, based on a deep learning network (U-Net), to quickly generate simulations with massive neutrinos from standard $\Lambda$CDM simulations without neutrinos. We computed multiple relevant statistical measures of deep-learning generated simulations, and conclude that our method accurately reproduces the 3-dimensional spatial distribution of matter down to non-linear scales: $k < 0.7$ h/Mpc. Finally, our method allows us to generate massive neutrino simulations 10,000 times faster than the traditional methods. ","Learning neutrino effects in Cosmology with Convolutional Neural
  Networks"
86,1183794108856459264,3279117936,Denis Yarats üá∫üá¶,"['Excited to share our new work lead by @rjerryma, where we attempt to demystify RAdam (Liu et al. 2019) and its automatic learning rate warmup schedule. \n\nPaper: <LINK>\n\n[1/4] <LINK>', ""We point out several shortcomings of RAdam's analysis and propose an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability.\n\n[2/4] https://t.co/D3awMEipLc"", 'We then notice that RAdam, in its essence, is just 4 iterations of momentum SGD, followed by Adam with a fixed warmup schedule.  Thus, as a ""rule-of-thumb"", we suggest to use a simple untuned warmup schedule as it performs more-or-less identical to RAdam in many settings.\n\n[3/4] https://t.co/XQNx5SKAHu', 'Finally, our advice to practitioners is to stick to the linear warmup with Adam, where the learning rate is increased over the first 2 / (1-\\beta_2) training iterations.\n\n[4/4] https://t.co/tpOvaOf4SE']",https://arxiv.org/abs/1910.04209,"Adaptive optimization algorithms such as Adam are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, recent work proposes automatic variance rectification of Adam's adaptive learning rate, claiming that this rectified approach (""RAdam"") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we refute this analysis and provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. We then provide some ""rule-of-thumb"" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over $2 / (1 - \beta_2)$ training iterations. ",On the adequacy of untuned warmup for adaptive optimization
87,1183695437586452480,1107358308,Jack Turner,"['Excited to release new paper w. @mpatacch on Gaussian Processes (GPs) for few-shot learning (with deep kernel transfer). \n\n   üìùpaper: <LINK>\n   üíæ code: <LINK> (in @PyTorch and GPyTorch)  \n\n(1/3)', 'GPs are a natural fit for few-shot because they work well in low data regime and have built-in uncertainty estimation. We apply this on standard few-shot benchmarks via deep kernel learning (@andrewgwils), using output feature maps of NN as input to GP\n\n(2/3)', 'E.g. my favourite plot. Trained on random periodic head pose trajectories,Feature Transfer (FT) and GP are tested on flat rotation with Cutout noise. FT overfits, GP predicts correctly &amp; acknowledges uncertainty on the noisy point.  Results on std few-shot benchmarks in paper. https://t.co/UtPoX1k4yB']",https://arxiv.org/abs/1910.05199,"Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy. ",Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels
88,1183687438209822720,22473658,Chris Norval,"[""Using machine learning to predict participants' research consent decisions? ‚Äì what could go wrong?!\n\nNew paper by myself and @tnhh now available:\n\nAutomating dynamic consent decisions for the processing of social media data in health research\n\n<LINK>"", ""@tnhh In this paper, we conduct a study with 67 participants, finding that while we could predict the types of social media data they would consent to sharing with different medical audiences, we probably wouldn't want to."", '@tnhh We deconstruct our models to identify a number of considerations for those interested in dynamic consent automation may wish to think about before attempting to deploy in the real world.']",https://arxiv.org/abs/1910.05265,"Social media have become a rich source of data, particularly in health research. Yet, the use of such data raises significant ethical questions about the need for the informed consent of those being studied. Consent mechanisms, if even obtained, are typically broad and inflexible, or place a significant burden on the participant. Machine learning algorithms show much promise for facilitating a 'middle ground' approach: using trained models to predict and automate granular consent decisions. Such techniques, however, raise a myriad of follow-on ethical and technical considerations. In this paper, we present an exploratory user study (n = 67) in which we find that we can predict the appropriate flow of health-related social media data with reasonable accuracy, while minimising undesired data leaks. We then attempt to deconstruct the findings of this study, identifying and discussing a number of real-world implications if such a technique were put into practice. ","Automating dynamic consent decisions for the processing of social media
  data in health research"
89,1183674831176458242,547776192,Chris Lovell,"[""New (accepted) paper on the arXiv!üö® <LINK>\n'Recalibrating the Cosmic Star Formation History' with @stewilkins and Elizabeth Stanway\n\nWe use BPASS to determine new SFR calibrations with the ultraviolet, thermal-infrared, and hydrogen recombination lines"", '@stewilkins the take away üåØ: \n\nusing these new calibrations we find that the tension between the integrated cosmic star formation history and the measured evolution of the stellar mass density is (mostly) resolved']",https://arxiv.org/abs/1910.05220v1,"The calibrations linking observed luminosities to the star formation rate depend on the assumed stellar population synthesis model, initial mass function, star formation and metal enrichment history, and whether reprocessing by dust and gas is included. Consequently the shape and normalisation of the inferred cosmic star formation history is sensitive to these assumptions. Using v2.2.1 of the Binary Population and Spectral Synthesis (\bpass) model we determine a new set of calibration coefficients for the ultraviolet, thermal-infrared, and, hydrogen recombination lines. These ultraviolet and thermal infrared coefficients are 0.15-0.2 dex higher than those widely utilised in the literature while the H$\alpha$ coefficient is $\sim 0.35$ dex larger. These differences arise in part due to the inclusion binary evolution pathways but predominantly reflect an extension in the IMF to 300 $M_{\odot}$ and a change in the choice of reference metallicity. We use these new coefficients to recalibrate the cosmic star formation history, and find improved agreement between the integrated cosmic star formation history and the in-situ measured stellar mass density as a function of redshift. However, these coefficients produce new tension between star formation rate densities inferred from the ultraviolet and thermal-infrared and those from H$\alpha$. ",] Recalibrating the Cosmic Star Formation History
90,1183599203781664768,401811181,Walter Tangarife,"['New paper: ""The Dodelson-Widrow Mechanism In the Presence of Self-Interacting Neutrinos"". Active neutrinos + self-interactions -&gt; sterile-neutrino dark matter.\nFirst article with my collaborators at Northwestern. \n#SterileNeutrinos #DarkMatter #Neutrinos\n<LINK>']",https://arxiv.org/abs/1910.04901,"keV-scale gauge-singlet fermions, allowed to mix with the active neutrinos, are elegant dark matter (DM) candidates. They are produced in the early universe via the Dodelson-Widrow mechanism and can be detected as they decay very slowly, emitting X-rays. In the absence of new physics, this hypothesis is virtually ruled out by astrophysical observations. Here, we show that new interactions among the active neutrinos allow these sterile neutrinos to make up all the DM while safely evading all current experimental bounds. The existence of these new neutrino interactions may manifest itself in next-generation experiments, including DUNE. ",Dodelson-Widrow Mechanism In the Presence of Self-Interacting Neutrinos
91,1183363937955397632,9316452,Xavier Amatriain,"['""Classification as Decoder"" - new paper where we explore how to introduce domain expertise into seq2seq generative language models <LINK> with @pnderthevstnes @ManishChablani et al.']",https://arxiv.org/abs/1910.03476,"Generative seq2seq dialogue systems are trained to predict the next word in dialogues that have already occurred. They can learn from large unlabeled conversation datasets, build a deep understanding of conversational context, and generate a wide variety of responses. This flexibility comes at the cost of control. Undesirable responses in the training data will be reproduced by the model at inference time, and longer generations often don't make sense. Instead of generating responses one word at a time, we train a classifier to choose from a predefined list of full responses. The classifier is trained on (conversation context, response class) pairs, where each response class is a noisily labeled group of interchangeable responses. At inference, we generate the exemplar response associated with the predicted response class. Experts can edit and improve these exemplar responses over time without retraining the classifier or invalidating old training data. Human evaluation of 775 unseen doctor/patient conversations shows that this tradeoff improves responses. Only 12% of our discriminative approach's responses are worse than the doctor's response in the same conversational context, compared to 18% for the generative model. A discriminative model trained without any manual labeling of response classes achieves equal performance to the generative model. ","Classification As Decoder: Trading Flexibility For Control In Neural
  Dialogue"
92,1183084231267885057,2780460254,Dr Deanna C. Hooper,"['I had a new paper out yesterday, where we looked at spectral distortions and how they complement CMB anisotropies: <LINK>\nSo what are spectral distortions (SDs)? Let me tell you about them! 1/n', 'The CMB - the background light of the universe - is the most perfect blackbody we have seen. This means that it absorbs (and emits) all wavelengths, without directly reflecting any. Here you can see how the CMB data points align with the predicted blackbody spectrum. 2/n https://t.co/BMEl6LytQE', 'However, it is not a perfect blackbody: there are tiny deviations to the blackbody spectrum. So tiny we can‚Äôt hope to see them by eye in the previous plot. These deviations are called spectral distortions. 3/n', 'Spectral distortions are caused by any process that injects energy into the photons in the early universe. A lot of things within our standard cosmological model can inject energy: things like inflation, structure formation, reionisation, even the cooling of normal matter. 4/n', 'But also more exotic models, such as decaying dark matter or evaporation of primordial black holes, can cause energy injection. This means that spectral distortions can help us study all of these things. 5/n', 'There are different types of spectral distortions, depending on when the energy was injected: mu-distortions, y-distortions, g-distortions‚Ä¶ Here‚Äôs a cool plot from our paper showing the total distortions as a function of redshift z (higher redshift = earlier time). 6/n https://t.co/gDTgs04Vin', 'One of the really cool things about SD is the physics is well understood and can be calculated very precisely. Different models make different predictions. Within LCDM we expect to measure mu-distortions of 10^-8 (that‚Äôs 0.00000001). For now I‚Äôll focus only on mu-distortions. 7/n', 'This means that if we do not measure mu-distortions at 10^-8, LCDM is wrong. Exotic models such as decaying dark matter predict mu-distortions~10^-9. If we were sensitive to such tiny distortions, we could test this model. 8/n', 'So now the big question: have we ever measured these spectral distortions? Well, the latest measurement we have is from the COBE/FIRAS mission from 1995 (nearly 25 years ago). FIRAS showed that mu-distortions have to be smaller than 10^-5. 9/n', 'This measurement was invaluable for testing a lot of models. But the sensitivity was three orders of magnitude off of what we would need to test the prediction of LCDM. For spectral distortions, the latest measurement is very far behind the theory. 10/n', 'But there is hope for a future mission! There are proposals such as PIXIE, which could measure mu-distortions at the level of 10^-8, or PRISM, which could push this to 10^-9. None are approved yet, but they might be in the future. 11/n', 'So what did we do with spectral distortions? First, we have implemented the full calculation of spectral distortions in our cosmology code CLASS. Before, this calculation was done in different codes, now we can calculate SDs together with other cosmological observables. 12/n', 'We have also created mock likelihoods for PIXIE and PRISM. These allow us to create ‚Äúmock‚Äù data: an approximation of the real data we could obtain in the future if one of these missions goes ahead, based on the experimental setup. 13/n', 'We then use these likelihoods to do forecasts: we assume a future mission has measured mu-distortions (as a starting point - or fiducial - we assume the prediction for LCDM), and see what else we could learn from this measurement. 14/n', 'As an example: if we measure the mu-distortions predicted by LCDM with PRISM, we could also put constraints on models where a fraction of dark matter can decay. We could constrain both the lifetime and fraction of this decaying dark matter. 15/n', 'And for some combinations of parameters, the constraints we obtain are several orders of magnitude better than the ones we can obtain with CMB temperature anisotropy measurements (like those from Planck). In other regions, CMB anisotropy constraints are better. 16/n', 'This really means the two are complementary: each is optimum for a different set of parameter combinations. Together, they can cover a much broader set of possible parameters. The same is true for constraining primordial black hole evaporation. 17/n', 'With the framework we have developed for this paper, we can make predictions simultaneously for CMB temperature anisotropies, spectral distortions, and matter power spectrum, among others. We tested it on a few models, but there are countless models we could apply this to. 18/n', 'In the next few months we are going to make the new code and likelihoods public, so others can test this synergy on different models. We hope all of this will help us one day have a dedicated spectral distortions mission. 19/n', 'I‚Äôve learnt so much working on this project. I‚Äôve wanted to work on spectral distortions for years, and I‚Äôm really happy with how this paper turned out! 20/20']",https://arxiv.org/abs/1910.04619,"Spectral distortions and anisotropies of the CMB provide independent and complementary probes to study energy injection processes in the early universe. Here we discuss the synergy between these observables, and show the promising future of spectral distortion missions to constrain both exotic and non-exotic energy injections. We show that conventional probes such as Big Bang Nucleosynthesis and CMB anisotropies can benefit from and even be surpassed by future spectral distortion experiments. For this, we have implemented a unified framework within the Boltzmann code CLASS to consistently treat the thermal evolution of photons and baryons. Furthermore, we give an extensive and pedagogical introduction into the topic of spectral distortions and energy injections throughout the thermal history of the universe, highlighting some of their unique features and potential as a novel probe for cosmology and particle physics. ",The synergy between CMB spectral distortions and anisotropies
93,1182644586113589248,1075413776709623808,Charles Stapleford,['New paper with @FrohlichCarla and @kneller_jim examining the feedback of neutrino flavor mixing on supernova dynamics: <LINK>'],https://arxiv.org/abs/1910.04172,"At the present time even the most sophisticated, multi-dimensional simulations of core-collapse supernovae do not (self-consistently) include neutrino flavor transformation. This physics is missing despite the importance of neutrinos in the core-collapse explosion paradigm. Because of this dependence, any flavor transformation that occurs in the region between the proto-neutron star and the shock could result in major effects upon the dynamics of the explosion. We present the first hydrodynamic core-collapse supernova simulation which simultaneously includes flavor transformation of the free-streaming neutrinos in the neutrino transport. These oscillation calculations are dynamically updated and evolve self-consistently alongside the hydrodynamics. Using a $M=20\;{\rm M_{\odot}}$ progenitor, we find that while the oscillations have an effect on the neutrino emission and the heating rates, flavor transformation alone does not lead to a successful explosion of this progenitor in spherical symmetry. ","Coupling Neutrino Oscillations and Simulations of Core-Collapse
  Supernovae"
94,1182620584779366401,1702174146,Janis Keuper,['We have a new paper out: <LINK> \n#DeepLearning <LINK>'],https://arxiv.org/abs/1910.03240,"Recent studies have shown remarkable success in image-to-image translation for attribute transfer applications. However, most of existing approaches are based on deep learning and require an abundant amount of labeled data to produce good results, therefore limiting their applicability. In the same vein, recent advances in meta-learning have led to successful implementations with limited available data, allowing so-called few-shot learning. In this paper, we address this limitation of supervised methods, by proposing a novel approach based on GANs. These are trained in a meta-training manner, which allows them to perform image-to-image translations using just a few labeled samples from a new target class. This work empirically demonstrates the potential of training a GAN for few shot image-to-image translation on hair color attribute synthesis tasks, opening the door to further research on generative transfer learning. ",Semi Few-Shot Attribute Translation
95,1182549693722759168,718195928335822853,Alexander Buchholz,['How do you achieve #Bayesian Model Choice in the massive data era? \nCheck out our new paper using distributed computing!\n<LINK>\njoint work with D. Ahfock and @srichardson962 at the @MRC_BSU @Cambridge_Uni'],https://arxiv.org/abs/1910.04672,"We propose a general method for distributed Bayesian model choice, using the marginal likelihood, where a data set is split in non-overlapping subsets. These subsets are only accessed locally by individual workers and no data is shared between the workers. We approximate the model evidence for the full data set through Monte Carlo sampling from the posterior on every subset generating a model evidence per subset. The results are combined using a novel approach which corrects for the splitting using summary statistics of the generated samples. Our divide-and-conquer approach enables Bayesian model choice in the large data setting, exploiting all available information but limiting communication between workers. We derive theoretical error bounds that quantify the resulting trade-off between computational gain and loss in precision. The embarrassingly parallel nature yields important speed-ups when used on massive data sets as illustrated by our real world experiments. In addition, we show how the suggested approach can be extended to model choice within a reversible jump setting that explores multiple feature combinations within one run. ",Distributed Computation for Marginal Likelihood based Model Choice
96,1182458040018038784,145986026,Erdem Bƒ±yƒ±k,"['Our new work shows: 1) Optimizing for info gain leads to easier questions for humans in active reward learning, and 2) easier questions improve performance by decreasing wrong responses, has been accepted at #CoRL2019 . The paper is also on arXiv now: <LINK>', 'We have also open-sourced the code: https://t.co/HouE0J15Lg ,\nand there is a video at https://t.co/JQHLYBwRR9 .\nI thank my collaborators on this work: Malayandi Palan, Nicholas C. Landolfi, Dylan P. Losey and @DorsaSadigh .']",https://arxiv.org/abs/1910.04365,"Robots can learn the right reward function by querying a human expert. Existing approaches attempt to choose questions where the robot is most uncertain about the human's response; however, they do not consider how easy it will be for the human to answer! In this paper we explore an information gain formulation for optimally selecting questions that naturally account for the human's ability to answer. Our approach identifies questions that optimize the trade-off between robot and human uncertainty, and determines when these questions become redundant or costly. Simulations and a user study show our method not only produces easy questions, but also ultimately results in faster reward learning. ","Asking Easy Questions: A User-Friendly Approach to Active Reward
  Learning"
97,1182334116777193472,1069106254927273984,Sham Kakade,"[""What actually constitutes a good representation for reinforcement learning? Lots of sufficient conditions. But what's necessary? New paper: <LINK>. Surprisingly, good value (or policy) based representations just don't cut it! w/ @SimonShaoleiDu @RuosongW @lyang36"", ""@roydanroy @SimonShaoleiDu @RuosongW @lyang36 Nope. i'll be talking about recent work in policy gradient methods in RL and controls. for the following  IAS workshop, I will! the rep. paper is pretty cool, in that it is still a bit puzzling to me!""]",https://arxiv.org/abs/1910.03016,"Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value-based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning. ","Is a Good Representation Sufficient for Sample Efficient Reinforcement
  Learning?"
98,1182307359663775745,2427184074,Christopher Berry,"['New paper led by @NUCIERA grad student @spacedontwait modelling the binary neutron star progenitors of short gamma-ray bursts GRB 070809 &amp; GRB 090515. We find evidence for neutron stars receiving large natal kicks of &gt; 200 km/s!\nüí•<LINK> <LINK>', '@NUCIERA @spacedontwait @DilithiumMatrix @matsoulina Following detection of GW170817 and GRB 170817A we know that short gamma-ray bursts can be generated by binary neutron star collisions. We have lots of of gamma-ray burst observations, so what can we learn about binary neutron stars from them?', 'GRB 070809 and GRB 090515 are interesting short gamma-ray bursts as they are a long way from their host galaxies. Neutron stars form from dying stars so they need to come from somewhere with stars. They can become offset thanks to the kicks they get during their natal supernova', ""We don't know how big a kick neutron stars get during supernova. This has a big impact on the number of merging neutron stars. We model the evolution of a binary neutron star population including the variation of kicks and other supernova properties to reproduce GRB observations"", '@mcnees @NUCIERA @spacedontwait @DilithiumMatrix @matsoulina Thank you Grave Robbert', 'How do you get a binary neutron star merging far out from a galaxy? It could start far out, but not many stars are there. It could get a small kick a long time ago or a big kick recently. We model the stellar population across the history of the galaxy to figure out the odds https://t.co/ID6E3A1Xua', '@spacedontwait @DilithiumMatrix By modelling of the galaxies of GRB 070809 and GRB 090515 we infer probable supernova natal kicks. This plots show results from five different stellar mass/halo mass relations used our galaxy modelling. GRB 090515 needs kicks above ~ 200 km/s\n\nüìähttps://t.co/87MV9HIhc2 https://t.co/QkFdM4u4mM', ""@profjsb @spacedontwait @DilithiumMatrix Thank you! I see we're not currently citing that paper, so the pointer is appreciated!""]",https://arxiv.org/abs/1910.03598,"We present a detailed analysis of two well-localized, highly offset short gamma-ray bursts---GRB~070809 and GRB~090515---investigating the kinematic evolution of their progenitors from compact object formation until merger. Calibrating to observations of their most probable host galaxies, we construct semi-analytic galactic models that account for star formation history and galaxy growth over time. We pair detailed kinematic evolution with compact binary population modeling to infer viable post-supernova velocities and inspiral times. By populating binary tracers according to the star formation history of the host and kinematically evolving their post-supernova trajectories through the time-dependent galactic potential, we find that systems matching the observed offsets of the bursts require post-supernova systemic velocities of hundreds of kilometers per second. Marginalizing over uncertainties in the stellar mass--halo mass relation, we find that the second-born neutron star in the GRB~070809 and GRB~090515 progenitor systems received a natal kick of $\gtrsim 200~\mathrm{km\,s}^{-1}$ at the 78\% and 91\% credible levels, respectively. Applying our analysis to the full catalog of localized short gamma-ray bursts will provide unique constraints on their progenitors and help unravel the selection effects inherent to observing transients that are highly offset with respect to their hosts. ","Forward Modeling of Double Neutron Stars: Insights from Highly-Offset
  Short Gamma-Ray Bursts"
99,1182274628049424385,1138255660750127108,Berthold Jaeck,['I propose a novel #microscopy technique based on superconducting #qubits to visualize the properties of #quantum materials. Check out my new paper on #arXiv! <LINK>\nMany thanks to @AvHStiftung for funding my research.'],https://arxiv.org/abs/1910.03583,"The investigation of novel electronic phases in low-dimensional quantum materials demands for the concurrent development of new measurement techniques that combine surface sensitivity with high spatial resolution and high measurement accuracy. We propose a new quantum sensing imaging modality based on superconducting charge qubits to study dissipative charge carrier dynamics with nanometer spatial and high temporal resolution. Using analytical and numerical calculations we show that superconducting charge qubit microscopy (SCQM) has the potential to resolve temperature and resistivity changes in a sample as small as $\Delta T\leq0.1\;$mK and $\Delta\rho\leq1\cdot10^{4} \,\Omega\cdot$cm, respectively. Among other applications, SCQM will be especially suited to study the microscopic mechanisms underlying interaction driven quantum phase transitions, to investigate the boundary modes found in novel topological insulators and, in a broader context, to visualize the dissiaptive charge carrier dynamics occurring in mesoscopic and nanoscale devices. ","Visualizing dissipative charge carrier dynamics at the nanoscale with
  superconducting charge qubit microscopy"
100,1182214682356174849,2352149191,Mimmo Nardiello,"['Project PATHOS (""A Psf-based Approach to @NASA_TESS High quality data Of Stellar clusters"") has officially started! Check out my paper <LINK> and give a look to the first light curves of 47Tuc: <LINK> .We also report a new candidate exoplanet!ü•≥ <LINK>', 'Many thanks to the co-authors @valentingranata @borsatoluca83 @Malavolta_Exo @ValerioNascim https://t.co/HbJtWGHR91', '... and also many thanks to @ScottWFleming for the amazing work done to upload the light curves on the MAST archive :)']",https://arxiv.org/abs/1910.03592,"The TESS mission will survey ~85 % of the sky, giving us the opportunity of extracting high-precision light curves of millions of stars, including stellar cluster members. In this work, we present our project ""A PSF-based Approach to TESS High quality data Of Stellar clusters"" (PATHOS), aimed at searching and characterise candidate exoplanets and variable stars in stellar clusters using our innovative method for the extraction of high-precision light curves of stars located in crowded environments. Our technique of light-curve extraction involves the use of empirical Point Spread Functions (PSFs), an input catalogue and neighbour-subtraction. The PSF-based approach allows us to minimise the dilution effects in crowded environments and to extract high-precision photometry for stars in the faint regime (G>13). For this pilot project, we extracted, corrected, and analysed the light curves of 16641 stars located in a dense region centred on the globular cluster 47 Tuc. We were able to reach the TESS magnitude T~16.5 with a photometric precision of ~1 % on the 6.5-hour timescale; in the bright regime we were able to detect transits with depth of ~34 parts per million. We searched for variables and candidate transiting exoplanets. Our pipeline detected one planetary candidate orbiting a main sequence star in the Galactic field. We analysed the period-luminosity distribution for red-giant stars of 47 Tuc and the eclipsing binaries in the field. Light curves are uploaded on the Mikulski Archive for Space Telescopes under the project PATHOS. ","A PSF-based Approach to TESS High quality data Of Stellar clusters
  (PATHOS) -- I. Search for exoplanets and variable stars in the field of 47
  Tuc"
101,1182093061704183808,1015053310603284480,Stephen Kane,"['New paper by myself and @SarahCBlunt is out! Not only does the 75 year eccentric orbit of HR 5183b allow stable orbits in the HZ, but at closest approach the giant planet would be 3 magnitudes brighter than Venus (V   ~ -7.3) and 2.2 arcmins in diameter!  <LINK>']",https://arxiv.org/abs/1910.03626,"Discoveries of exoplanets using the radial velocity method are progressively reaching out to increasingly longer orbital periods as the duration of surveys continues to climb. The improving sensitivity to potential Jupiter analogs is revealing a diversity of orbital architectures that are substantially different from that found in our solar system. An excellent example of this is the recent discovery of HR 5183b; a giant planet on a highly eccentric ($e = 0.84$) $\sim$75~year orbit. The presence of such giant planet orbits are intrinsically interesting from the perspective of the dynamical history of planetary systems, and also for examining the implications of on-going dynamical stability and habitability of these systems. In this work, we examine the latter, providing results of dynamical simulations that explore the stable regions that the eccentric orbit of the HR 5183 giant planet allows to exist within the Habitable Zone of the host star. Our results show that, despite the incredible perturbing influence of the giant planet, there remain a narrow range of locations within the Habitable Zone where terrestrial planets may reside in long-term stable orbits. We discuss the effects of the giant planet on the potential habitability of a stable terrestrial planet, including the modulation of terrestrial planet eccentricities and the periodically spectacular view of the giant planet from the terrestrial planet location. ","In the Presence of a Wrecking Ball: Orbital Stability in the HR 5183
  System"
102,1182010216314880000,3743366715,Aaron Mueller,"['new #emnlp2019 paper with @aryamccarthy, @amuuueller, David Yarowsky, and others --- we quantify what it means to be a basic color term cross-lingually, finding that Berlin &amp; Kay (1969) were *mostly* right, if a little Indo-Euro-centric: <LINK> <LINK>', '@aryamccarthy we also find that we can quantitatively recover the exact order of color term acquisition proposed in B&amp;K using their criteria! Interestingly, none of the criteria alone are enough to recover these; we need all of them https://t.co/WPZsHaEpZf']",https://arxiv.org/abs/1910.01531,"There is an extensive history of scholarship into what constitutes a ""basic"" color term, as well as a broadly attested acquisition sequence of basic color terms across many languages, as articulated in the seminal work of Berlin and Kay (1969). This paper employs a set of diverse measures on massively cross-linguistic data to operationalize and critique the Berlin and Kay color term hypotheses. Collectively, the 14 empirically-grounded computational linguistic metrics we design---as well as their aggregation---correlate strongly with both the Berlin and Kay basic/secondary color term partition (gamma=0.96) and their hypothesized universal acquisition sequence. The measures and result provide further empirical evidence from computational linguistics in support of their claims, as well as additional nuance: they suggest treating the partition as a spectrum instead of a dichotomy. ",Modeling Color Terminology Across Thousands of Languages
103,1181930208217071616,1398894936,Flip Tanedo,"[""Dark sectors can produce exotic long-range potentials between nucleons that generalize the Yukawa potential. New work with @UCRiverside grad @LexiCostantino (&amp; @NSFGRFP fellow) &amp; postdoc Sylvain Fichet (@Caltech/@ictpSA).  \n\n<LINK>\nLexi's 1st paper! #proudadviser <LINK>""]",https://arxiv.org/abs/1910.02972,"New dynamics from hidden sectors may manifest as long-range forces between visible matter particles. The well-known case of Yukawa-like potentials occurs via the exchange of a single virtual particle. However, more exotic behavior is also possible. We present three classes of exotic potentials that are generated by relativistic theories: (i) quantum forces from the loop-level exchange of two virtual particles, (ii) conformal forces from a conformal sector, and (iii) emergent forces from degrees of freedom that only exist in the infrared regime of the theory. We discuss the complementarity of spin-dependent force searches in an effective field theory framework. We identify well-motivated directions to search for exotic spin-dependent forces. ",Exotic Spin-Dependent Forces from a Hidden Sector
104,1181929973654917120,608502805,THOMAS Guillaume,"['Hey my new CFIS paper is on arrive today, which include among other collaborator @benfamaey and @nfmartin1980 : <LINK>. It present a new method to classify dwarfs/giants stars get their [Fe/H] and their distance. This method could be use in the future with @LSST']",https://arxiv.org/abs/1910.03076,"We present a new fully data-driven algorithm that uses photometric data from the Canada-France-Imaging-Survey (CFIS; $u$), Pan-STARRS 1 (PS1; $griz$), and Gaia ($G$) to discriminate between dwarf and giant stars and to estimate their distances and metallicities. The algorithm is trained and tested using the SDSS/SEGUE spectroscopic dataset and Gaia photometric/astrometric dataset. At [Fe/H]$<-1.2$, the algorithm succeeds in identifying more than 70% of the giants in the training/test set, with a dwarf contamination fraction below 30% (with respect to the SDSS/SEGUE dataset). The photometric metallicity estimates have uncertainties better than 0.2 dex when compared with the spectroscopic measurements. The distances estimated by the algorithm are valid out to a distance of at least $\sim 80$ kpc without requiring any prior on the stellar distribution, and have fully independent uncertainities that take into account both random and systematic errors. These advances allow us to estimate these stellar parameters for approximately 12 million stars in the photometric dataset. This will enable studies involving the chemical mapping of the distant outer disc and the stellar halo, including their kinematics using the Gaia proper motions. This type of algorithm can be applied in the Southern hemisphere to the first release of LSST data, thus providing an almost complete view of the external components of our Galaxy out to at least $\sim 80$ kpc. Critical to the success of these efforts will be ensuring well-defined spectroscopic training sets that sample a broad range of stellar parameters with minimal biases. A catalogue containing the training/test set and all relevant parameters within the public footprint of CFIS is available online. ","Dwarfs or giants? Stellar metallicities and distances in the
  Canada-France-Imaging-Survey from $ugrizG$ multi-band photometry"
105,1181877830382182400,109255123,Danny Caballero üá≤üáΩ,"['New paper alert: Computational literacy in physics. <LINK>', '@brianwfrank üòÇ']",https://arxiv.org/abs/1910.03316,"Computation is becoming an increasingly important part of physics education. However, there are currently few theories of learning that can be used to help explain and predict the unique challenges and affordances associated with computation in physics. In this study, we adapt the existing theory of computational literacy, which posits that computational learning can be divided into material, cognitive, and social aspects, to the context of undergraduate physics. Based on an exploratory study of undergraduate physics computational literacy, using a newly-developed teaching tool known as a computational essay, we have identified a variety of student practices, knowledge, and beliefs across these three aspects of computational literacy. We illustrate these categories with data collected from students who engaged in an initial implementation of computational essays in an introductory electricity and magnetism class. We conclude by arguing that this framework can be used to theoretically diagnose student difficulties with computation, distinguish educational approaches that focus on material vs. cognitive aspects of computational literacy, and highlight the benefits and limitations of open-ended projects like computational essays to student learning. ","Physics Computational Literacy: An Exploratory Case Study Using
  Computational Essays"
106,1181837945935990784,2999702157,Anton Ilderton,"['I have a new paper on the @arXiv: #quantum interference and enhancement of #pair_production rates, beyond the semiclassical approximation. In fact, exact!\n\n<LINK>\n\n#physics #lasers #qft #sfqed #strongfieldQED #pair_production @plym_math @PlymUni #plymuni <LINK>']",https://arxiv.org/abs/1910.03012,"We present an exactly solvable example of coherent quantum interference effects in the creation of electron-positron pairs from the collision of a photon with ultra-short laser pulses. Being characterised entirely by null, or lightlike, directions, this setup realises an all-optical double-slit in the ""null"" domain, and exhibits features both in common with, and distinct from, a time domain double-slit (Ramsey interferometer). We show that by tailoring the order and amplitude of the pulses one can control signatures of both quantum and classical physics in the produced positron spectrum. ",Coherent quantum enhancement of pair production in the null domain
107,1181780166533697538,398987599,Andrea Rapisarda,['New paper online #talent #luck #success #competition <LINK>'],https://arxiv.org/abs/1910.03400,"We present a new way of estimation of the role of chance in achieving success, by comparing the empirical data from 100-meter dash competitions (one of the sports disciplines with the most stringent controls of external randomness), with the results of an agent-based computer model, which assumes that success depends jointly on the intrinsic talent of the agent and on unpredictable luck. We find a small, but non-zero contribution of random luck to the performance of the best sprinters, which may serve as a lower bound for the randomness role in other, less stringently controlled competitive domains. Additionally we discuss the perception of the payoff differences among the top participants, and the role of random luck in the resulting inequality. ","Inequalities, chance and success in sport competitions: simulations vs
  empirical data"
108,1181605499873087488,16252640,Matthew FL,['New paper and code online: Exact and/or Fast Nearest Neighbors\n\nA new approach to identify the _true_ nearest neighbor in a high dimensional vector space without having to perform an expensive scan of the entire data set.\n<LINK>\n<LINK> <LINK>'],https://arxiv.org/abs/1910.02478,"Prior methods for retrieval of nearest neighbors in high dimensions are fast and approximate--providing probabilistic guarantees of returning the correct answer--or slow and exact performing an exhaustive search. We present Certified Cosine, a novel approach to nearest-neighbors which takes advantage of structure present in the cosine similarity distance metric to offer certificates. When a certificate is constructed, it guarantees that the nearest neighbor set is correct, possibly avoiding an exhaustive search. Certified Cosine's certificates work with high dimensional data and outperform previous exact nearest neighbor methods on these datasets. ",Exact and/or Fast Nearest Neighbors
109,1181572035304140801,3279117936,Denis Yarats üá∫üá¶,"['Excited to announce our new paper on Improving Sample Efficiency in Model-Free Reinforcement Learning from Images with @yayitsamyzhang, @ikostrikov, @brandondamos, Joelle Pineau, and Rob Fergus \n\nPaper: <LINK>\nCode: <LINK>\n\n[1/6] <LINK>', 'We augment Soft Actor-Critic with a simple pixels reconstruction loss of the current state (via a regularized autoencoder) and propose an extremely straightforward and sample-efficient model-free algorithm (SAC+AE):\n\n[2/6] https://t.co/bfAogNs8p6', ""Prior work has explored such auxiliary supervision extensively, yet it hasn't been possible to achieve a stable end-to-end training in the off-policy regime so far. Previous methods either used iterative representation pretraining, or trained it end-to-end but on-policy.\n\n[3/6]"", 'We are able to stabilize end-to-end learning in the off-policy setting and match both the model-free and model-based SOTA results on challenging continuous control tasks from dm_control:\n \n[4/6] https://t.co/TRVOjQSS1E', 'We also verify that our latent representation of image observations is conducive to recover corresponding proprioceptive states of the environment:\n\n[5/6] https://t.co/EOTaLeoy69', 'Finally, we successfully transfer our pretrained representation to unseen tasks that have different rewards and states distribution, but similar pixel observations. Which suggests that the representation is generalizable:\n\n[6/6] https://t.co/zgDTvO7xJX']",https://arxiv.org/abs/1910.01741,"Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at this https URL ","Improving Sample Efficiency in Model-Free Reinforcement Learning from
  Images"
110,1181548538645401601,1601296094,David Bowler,"['New paper with my student Jack Baker studying lattice dynamics in PZT, showing how the VCA has serious shortcomings #CompChem\n\n<LINK>']",https://arxiv.org/abs/1910.01685,"A comparative study between PbTiO$_3$, PbZrO$_3$, and the solid solution PbZr$_{0.5}$Ti$_{0.5}$O$_3$ is performed on the soft mode lattice dynamics within the first Brillouin Zone. We consider the six unique B-site orderings for PbZr$_{0.5}$Ti$_{0.5}$O$_3$ representable within the 2$\times$2$\times$2 primitive perovskite supercell as well as the virtual crystal approximation (VCA) to extract the phonon dispersion relations of a high-symmetry cubic-constrained form using density functional perturbation theory. We find that the most unstable modes in the rock-salt ordered structure and the VCA, like pure PbZrO$_3$, are antiferrodistortive (AFD) whilst lower symmetry arrangements are dominated by $\Gamma$-point ferroelectric (FE) instabilities like pure PbTiO$_3$. Despite similarities in the phonon dispersion relations between the rock-salt ordered supercell and the VCA, the character of modes at high symmetry points are found to be different. In particular, the a$^{0}$a$^{0}$c$^{-}$ & a$^{0}$a$^{0}$c$^{+}$ AFD instabilities of the rock-salt ordering are replaced with a$^{-}$b$^{-}$c$^{-}$ & a$^{+}$b$^{+}$c$^{+}$ instabilities within the VCA. Such a rotation pattern is not seen in any of the supercell-based calculations thus serving as a quantitative example of the inability of the method to represent accurately local structural distortions. Single modes are found exhibiting dual order parameters. At the zone centre, some arrangements show mixed FE & antipolar soft modes (due to Pb motion tansverse to the polar axis) and at long wavelengths all arrangements have soft modes of a mixed antipolar & AFD character. These are described with direct analysis of the eigendisplacements. ","First principles soft mode lattice dynamics of
  PbZr$_{0.5}$Ti$_{0.5}$O$_3$ and shortcomings of the virtual crystal
  approximation"
111,1181546422778372099,838292815,Ofir Nachum,"[""[new paper] Adding a simple linear constraint to a loss function (commonly used in ML fairness) can have strange and unintuitive effects on the resulting model. While such constraints seem natural/harmless, we don't fully understand their consequences yet <LINK>""]",https://arxiv.org/abs/1910.02097,"A number of machine learning (ML) methods have been proposed recently to maximize model predictive accuracy while enforcing notions of group parity or fairness across sub-populations. We propose a desirable property for these procedures, slack-consistency: For any individual, the predictions of the model should be monotonic with respect to allowed slack (i.e., maximum allowed group-parity violation). Such monotonicity can be useful for individuals to understand the impact of enforcing fairness on their predictions. Surprisingly, we find that standard ML methods for enforcing fairness violate this basic property. Moreover, this undesirable behavior arises in situations agnostic to the complexity of the underlying model or approximate optimizations, suggesting that the simple act of incorporating a constraint can lead to drastically unintended behavior in ML. We present a simple theoretical method for enforcing slack-consistency, while encouraging further discussions on the unintended behaviors potentially induced when enforcing group-based parity. ",Group-based Fair Learning Leads to Counter-intuitive Predictions
112,1181398654453784576,2337598033,Geraint F. Lewis,"['New paper on the arXiv with Brendon Brewer and PhD student, Zhen Wan ‚ÄúThe Globular Cluster Population of NGC 1052-DF2: Evidence for Rotation‚Äù <LINK> <LINK>']",https://arxiv.org/abs/1910.02778,"Based upon the kinematics of ten globular clusters, it has recently been claimed that the ultra-diffuse galaxy, NCD 1052-DF2, lacks a significant quantity of dark matter. Dynamical analyses have generally assumed that this galaxy is pressure supported, with the relatively small velocity dispersion of the globular cluster population indicating the deficit of dark matter. However, the presence of a significant rotation of the globular cluster population could substantially modify this conclusion. Here we present the discovery of such a signature of rotation in the kinematics of NGC 1052-DF2's globular clusters, with a velocity amplitude of $\sim12.44^{+4.40}_{-5.16}$ km/s, which, through Bayesian model comparison, represents a marginally better fit to the available kinematic data; note that this rotation is distinct from, and approximately perpendicular to, the recently identified rotation of the stellar component of NGC 1052-DF2. Assuming this truly represents an underlying rotation, it is shown that the determined mass depends upon the inclination of the rotational component and, with a moderate inclination, the resultant mass to light ratio can exceed $M/L\sim10$. ",The Globular Cluster Population of NGC 1052-DF2: Evidence for Rotation
113,1181381037303177216,174052756,Thiago Serra,['New paper on combining classical and quantum computing for #orms with @Ivey_Huang:\n\n- We show that IP can be used as a preprocessing step for adiabatic quantum optimization\n\n- We review adiabatic quantum computing &amp; related work on minor embeddings\n\nLink: <LINK>'],http://arxiv.org/abs/1910.02179,"Quantum Annealing (QA) can be used to quickly obtain near-optimal solutions for Quadratic Unconstrained Binary Optimization (QUBO) problems. In QA hardware, each decision variable of a QUBO should be mapped to one or more adjacent qubits in such a way that pairs of variables defining a quadratic term in the objective function are mapped to some pair of adjacent qubits. However, qubits have limited connectivity in existing QA hardware. This has spurred work on preprocessing algorithms for embedding the graph representing problem variables with quadratic terms into the hardware graph representing qubits adjacencies, such as the Chimera graph in hardware produced by D-Wave Systems. In this paper, we use integer linear programming to search for an embedding of the problem graph into certain classes of minors of the Chimera graph, which we call template embeddings. One of these classes corresponds to complete bipartite graphs, for which we show the limitation of the existing approach based on minimum Odd Cycle Transversals (OCTs). One of the formulations presented is exact, and thus can be used to certify the absence of a minor embedding using that template. On an extensive test set consisting of random graphs from five different classes of varying size and sparsity, we can embed more graphs than a state-of-the-art OCT-based approach, our approach scales better with the hardware size, and the runtime is generally orders of magnitude smaller. ",Template-based Minor Embedding for Adiabatic Quantum Optimization
114,1181213082091175937,882257115863187457,Sanjeev Arora,"['Conventional wisdom: ""Not enough data? Use classic learners (Random Forests, RBF SVM, ..), not deep nets."" New paper: infinitely wide nets beat these and also beat finite nets. Infinite nets train faster than finite nets here (hint: Neural Tangent Kernel)! <LINK>', 'Interestingly, in some settings one just uses representations computed by infinite nets (NTK or related notions) and trains a kernel SVM.', '@gilberttamrobot Yes we plan to release code. We think these kernels are powerful off-the-shelf methods.', '@OkbaLeftHanded For now only for small-data sets. SVMs usually have quadratic running time in # of training examples.']",https://arxiv.org/abs/1910.01663,"Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 - 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK's efficacy may trace to lower variance of output. ",Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks
115,1181123991999791104,978886471389151235,Toby Opferkuch,"['New paper sees the light of the arXiv, ""Gravitational Imprints of Flavour Hierarchies"" (<LINK>) with @BenStefanek\n@admir_greljo. Featuring the ""Triglav signature"", a three peaked GW spectrum arising naturally from models explaining the fermion mass hierarchies. <LINK>', 'All good things apparently come in multiples of three! https://t.co/1t9iEiAIzE']",https://arxiv.org/abs/1910.02014,"The mass hierarchy among the three generations of quarks and charged leptons is one of the greatest mysteries in particle physics. In various flavor models, the origin of this phenomenon is attributed to a series of hierarchical spontaneous symmetry breakings, most of which are beyond the reach of particle colliders. We point out that the observation of a multi-peaked stochastic gravitational wave signal from a series of cosmological phase transitions could well be a unique probe of the mechanism behind flavor hierarchies. To illustrate this point, we show how near future ground- and space-based gravitational wave observatories could detect up to three peaks in the recently proposed $PS^3$ model. ",Gravitational Imprints of Flavor Hierarchies
116,1181014693793501185,15751831,Yuke Zhu,['New work: we built a meta-learning algorithm for an agent to discover the causal and effect relations from its visual observations and to use such causal knowledge to perform goal-directed tasks. Paper: <LINK>\nJoint work w/ @SurajNair_1 @drfeifei @silviocinguetta <LINK>'],https://arxiv.org/abs/1910.01751,"Causal reasoning has been an indispensable capability for humans and other intelligent animals to interact with the physical world. In this work, we propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. We develop learning-based approaches to inducing causal knowledge in the form of directed acyclic graphs, which can be used to contextualize a learned goal-conditional policy to perform tasks in novel environments with latent causal structures. We leverage attention mechanisms in our causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions. Our experiments show that our method effectively generalizes towards completing new tasks in novel environments with previously unseen causal structures. ",Causal Induction from Visual Observations for Goal Directed Tasks
117,1181007168855662594,48144742,Gill Verdon ‚öõÔ∏èüé≤ü§ñ,"['New breakthrough paper with @theteamatX!\n\nQuantum Hamiltonian-Based Models &amp; \nthe Variational Quantum Thermalizer Algorithm \n\nA double feature of a paper - big leaps for both Quantum Machine Learning and Quantum Simulation\n\n<LINK>\n\nSee thread below for insightsüëá <LINK>', '@Theteamatx We set out to create a new generative model for QML on quantum data -- more specifically, we wanted to learn to represent *mixed* quantum distributions; probabilistic mixtures of quantum states. We thus opted to hybridize quantum and probabilistic machine learning models into one https://t.co/9H60OPTZDe', '@Theteamatx We use the quantum computer mainly to add non-classical correlations to the model, and delegate as much as possible to classical ML.\n\nYou can now readily combine Quantum Neural Networks with your favourite probabilistic deep learning model in a meaningful and rigorous fashion https://t.co/g6hTATO9jz', ""@Theteamatx We took inspiration from recent work in AI (Energy-Based Models; EBMs) where a neural net learns an energy function for which thermal samples replicate the data distribution. These EBMs are even competitive with GAN's and VAE's at scale, see work @OpenAI :\nhttps://t.co/gTv76TVoLp"", '@Theteamatx @OpenAI We directly generalize this approach to using hybrid quantum-classical neural networks, where the HQCNN learns a quantum *Hamiltonian operator* for which the quantum thermal state of this Hamiltonian replicates the given dataset https://t.co/UmjuhvBnRu', '@Theteamatx @OpenAI A huge advantage of unitary is the conservation of entropy / partition fnc. This allows us to tractably train directly on quantum relative entropy -- the gold standard of quantum statistical measures  -- for both generative learning and VQT tasks, no extra overhead on the QC side https://t.co/IFhKURbwA8', '@Theteamatx @OpenAI As the model itself is constructed as a parameterized class of quantum thermal states, we found them naturally suited to learn to model quantum systems at finite temperature -- a super important generalization of the famous VQE algorithm! We even show it recovers VQE at 0 temp! https://t.co/eNY8EnDJKP', '@Theteamatx @OpenAI A new variational principle: Free Energy is minimized exactly when the thermal state is achieved, akin to how the ground state is achieved when one finds the optimum of the VQE. In practical scenarios, depending on the ansatz, one gets a solid approximation to the true state.', '@Theteamatx @OpenAI A first set of systems we tested this VQT approach on was Heisenberg spin systems, we see we can reconstruct the thermal state density matrix very accurately via our free energy minimization approach https://t.co/zWbXgfOusX', '@Theteamatx @OpenAI As a dual task, given an unknown quantum state, we can learn an effective Hamiltonian and temperature to model it and then be able to reproduce it -- we call this Quantum Modular Hamiltonian Learning, we tested it on Heisenberg spins at various temperature to see if it can learn https://t.co/QRxb2mnWSd', '@Theteamatx @OpenAI As a further test, we checked if the QHML could learn a disentangled (factorized) latent representation for a Bosonic lattice subsystem -- we then used this representation to show it can learn to compress the quantum data with good fidelity https://t.co/F0g0Askdq3', '@Theteamatx @OpenAI As a hint of future industrial applications of the VQT approach, we learn to model thermal states of a certain type of superconductor (d-wave), given a target temperature. Modelling at finite temp will be crucial for future QC-aided superconducting materials discoveries! https://t.co/oj9kZCVgj3', ""@Theteamatx @OpenAI Overall, the power of this representation comes from the fact that we are using the quantum computer wisely - to add the quantum correlations to our model. The classical computer is used for latent space representation, but couldn't model complex quantum correlations on its own"", '@Theteamatx @OpenAI TL;DR :\n\n-New framework for hybrid probabilistic + quantum ML\n-Ability to learn to represent any quantum mixed state accurately (given right choice of ansatz)\n- Direct generalization of the VQE to thermal states\n\nExcited for future works exploring this new direction for QML! ü•≥üòÅ', '@Theteamatx @OpenAI Finally, I want to give a huge shoutout to the @theteamatX and particularly to @quantummarks who absolutely crushed it on his first QML paper. \n\nHuge thanks to @jackhidary for being the mastermind of this new awesome Quantum@X team! Very happy here', 'An important comment given the recent events: from our conversation a few months back, @peter_wittek told me he was very excited about the hybridization of probabilistic ML and Quantum ML. I really really hope he gets found and gets to see this &amp; keep contributing to our field üò¢', '@Theteamatx Here is the @scirate3 link for those who use the site: https://t.co/AqpiMdJVrQ', '@jenseisert @delft_circuits @Theteamatx Thanks Jens! Looking forward to seeing how your new QSGD algorithms fare on this new set of tasks üòâ']",https://arxiv.org/abs/1910.02071,"We introduce a new class of generative quantum-neural-network-based models called Quantum Hamiltonian-Based Models (QHBMs). In doing so, we establish a paradigmatic approach for quantum-probabilistic hybrid variational learning, where we efficiently decompose the tasks of learning classical and quantum correlations in a way which maximizes the utility of both classical and quantum processors. In addition, we introduce the Variational Quantum Thermalizer (VQT) for generating the thermal state of a given Hamiltonian and target temperature, a task for which QHBMs are naturally well-suited. The VQT can be seen as a generalization of the Variational Quantum Eigensolver (VQE) to thermal states: we show that the VQT converges to the VQE in the zero temperature limit. We provide numerical results demonstrating the efficacy of these techniques in illustrative examples. We use QHBMs and the VQT on Heisenberg spin systems, we apply QHBMs to learn entanglement Hamiltonians and compression codes in simulated free Bosonic systems, and finally we use the VQT to prepare thermal Fermionic Gaussian states for quantum simulation. ","Quantum Hamiltonian-Based Models and the Variational Quantum Thermalizer
  Algorithm"
118,1180084741770551296,1735170890,Professor Eiman Kanjo,['Our new eprint paper (#LabelSens) for labeling sensor data on #EdgeComputing  at the point of collection.\n<LINK>\n#ntu #AI #datacollection #deeplearning #algorithms #multimodel #pervasivesensing  #embeddedsystems #datascience <LINK>'],https://arxiv.org/abs/1910.01400,"In recent years, machine learning has developed rapidly, enabling the development of applications with high levels of recognition accuracy relating to the use of speech and images. However, other types of data to which these models can be applied have not yet been explored as thoroughly. Labelling is an indispensable stage of data pre-processing that can be particularly challenging, especially when applied to single or multi-model real-time sensor data collection approaches. Currently, real-time sensor data labelling is an unwieldy process, with a limited range of tools available and poor performance characteristics, which can lead to the performance of the machine learning models being compromised. In this paper, we introduce new techniques for labelling at the point of collection coupled with a pilot study and a systematic performance comparison of two popular types of deep neural networks running on five custom built devices and a comparative mobile app (68.5-89% accuracy within-device GRU model, 92.8% highest LSTM model accuracy). These devices are designed to enable real-time labelling with various buttons, slide potentiometer and force sensors. This exploratory work illustrates several key features that inform the design of data collection tools that can help researchers select and apply appropriate labelling techniques to their work. We also identify common bottlenecks in each architecture and provide field tested guidelines to assist in building adaptive, high-performance edge solutions. ","LabelSens: Enabling Real-time Sensor Data Labelling at the point of
  Collection on Edge Computing"
119,1179851974033969152,4527505582,Scott Reed,"['New paper w/ @konradzolna:  \nTask-Relevant Adversarial Imitation Learning\n\nTL;DR prevent discriminator from distinguishing agent/expert using task-irrelevant information, such as arm appearance or initial prop positions.\n\nPaper <LINK>\nDemo <LINK> <LINK>', '@konradzolna @ziyuwang @SashaVNovikov @serkancabi @NandoDF', '@konradzolna @ziyuwang @SashaVNovikov @serkancabi @NandoDF @notmisha @davidmbudden']",https://arxiv.org/abs/1910.01077,"We show that a critical vulnerability in adversarial imitation is the tendency of discriminator networks to learn spurious associations between visual features and expert labels. When the discriminator focuses on task-irrelevant features, it does not provide an informative reward signal, leading to poor task performance. We analyze this problem in detail and propose a solution that outperforms standard Generative Adversarial Imitation Learning (GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning (TRAIL), uses constrained discriminator optimization to learn informative rewards. In comprehensive experiments, we show that TRAIL can solve challenging robotic manipulation tasks from pixels by imitating human operators without access to any task rewards, and clearly outperforms comparable baseline imitation agents, including those trained via behaviour cloning and conventional GAIL. ",Task-Relevant Adversarial Imitation Learning
120,1179789580985352192,263265637,Dennis Prangle,"[""New paper <LINK>, with Andy Golightly+@ccbdcdt's Tom Ryder and Isaac Matthews.\n\nIt's on extending fast mini-batch variational inference to time series. We use a generative approx model which lets us cheaply sample a subsequence from the middle of a time series. <LINK>""]",https://arxiv.org/abs/1910.00879,"Variational inference has had great success in scaling approximate Bayesian inference to big data by exploiting mini-batch training. To date, however, this strategy has been most applicable to models of independent data. We propose an extension to state space models of time series data based on a novel generative model for latent temporal states: the neural moving average model. This permits a subsequence to be sampled without drawing from the entire distribution, enabling training iterations to use mini-batches of the time series at low computational cost. We illustrate our method on autoregressive, Lotka-Volterra, FitzHugh-Nagumo and stochastic volatility models, achieving accurate parameter estimation in a short time. ","The Neural Moving Average Model for Scalable Variational Inference of
  State Space Models"
121,1179771697433579524,350800734,Yujia Li,['New paper at #NeurIPS2019 on generative models of graphs.  We explored many quality-efficiency trade-offs in this work and came up with a new model that gets good graph generation quality with much better efficiency.\n\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1910.00760,"We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: this https URL ",Efficient Graph Generation with Graph Recurrent Attention Networks
122,1179616570873503749,175921010,Earl Patrick Bellinger,"['Does stellar evolution theory predict the correct inner structure of stars? In our new ApJ paper, we use asteroseismic data to measure the internal structure of a star, and we contrast it with that predicted by stellar evolution theory. The answer? No!\n<LINK>', ""Normally, researchers fit stellar evolution models to asteroseismic data, and then assume the best-fitting model to be the structure of the star... nevermind the fact that the frequencies don't actually fit! https://t.co/z4nF8Oqwel"", ""In this paper, we used seismic data to measure the speed of sound (the ratio of pressure to density, here denoted u') at various locations in the deep interior of a star. We then compared those measurements to the best-fitting stellar evolution model. They don't match! https://t.co/WyHPdDvDfP"", 'We then tried to create models using different assumptions on the physics governing the evolution of the star. No matter what we tried, we could not get a matching structure.', 'Now, this is only one star, so it may just be an odd-ball. We will need to apply this kind of analysis to many stars before we can draw firmer conclusions. Ultimately though this may indicate that there are important processes missing from our understanding of stellar evolution!']",https://arxiv.org/abs/1910.00603,"The goal of stellar evolution theory is to predict the structure of stars throughout their lifetimes. Usually, these predictions can be assessed only indirectly, for example by comparing predicted and observed effective temperatures and luminosities. Thanks now to asteroseismology, which can reveal the internal structure of stars, it becomes possible to compare the predictions from stellar evolution theory to actual stellar structures. In this work, we present an inverse analysis of the oscillation data from the solar-type star KIC 6225718, which was observed by the \emph{Kepler} space observatory during its nominal mission. As its mass is about 20% greater than solar, this star is predicted to transport energy by convection in its nuclear-burning core. We find significant differences between the predicted and actual structure of the star in the radiative interior near to the convective core. In particular, the predicted sound speed is higher than observed in the deep interior of the star, and too low at a fractional radius of 0.25 and beyond. The cause of these discrepancies is unknown, and is not remedied by known physics in the form of convective overshooting or elemental diffusion. ","Testing stellar evolution with asteroseismic inversions of a main
  sequence star harboring a small convective core"
123,1179579092112265217,1034666951258193920,Tim Waters,"['The starting point of accretion theory is the famous Bondi solution.  In my new paper I show this solution is unattainable - even in an idealized universe. The tiniest density inhomogeneity introduced at ""infinity"" creates outflows <LINK>', 'I posted movies of these simulations here https://t.co/P4KC4Uokkl']",https://arxiv.org/abs/1910.01106,"The classic Bondi solution remains a common starting point both for studying black hole growth across cosmic time in cosmological simulations and for smaller scale simulations of AGN feedback. In nature, however, there will be inhomogenous distributions of rotational velocity and density along the outer radius ($R_o$) marking the sphere of influence of a black hole. While there have been many studies of how the Bondi solution changes with a prescribed angular momentum boundary condition, they have all assumed a constant density at $R_o$. In this Letter, we show that a non-uniform density at $R_o$ causes a meridional flow and due to conservation of angular momentum, the Bondi solution qualitatively changes into an inflow-outflow solution. Using physical arguments, we analytically identify the critical logarithmic density gradient $|\partial{\ln{\rho}}/\partial{\theta}|$ above which this change of the solution occurs. For realistic $R_o$, this critical gradient is less than 0.01 and tends to 0 as $R_o \rightarrow \infty$. We show using numerical simulations that, unlike for solutions with an imposed rotational velocity, the accretion rate for solutions under an inhomogenous density boundary condition remains constant at nearly the Bondi rate $\dot{M}_B$, while the outflow rate can greatly exceed $\dot{M}_B$. ",Outflows from inflows: the nature of Bondi-like accretion
124,1179563744885649409,141440459,Rod Van Meter üåª,"['New paper dance!\n""Modeling of Measurement-based Quantum Network Coding on IBMQ Devices""\n<LINK>', '@MeAllainYann Of course! But with small ion traps there is all-to-all connectivity, so you can simply have the qubits talk to either other directly, without the need for such complicated switching.']",https://arxiv.org/abs/1910.00815,"Quantum network coding has been proposed to improve resource utilization to support distributed computation but has not yet been put in to practice. We investigate a particular implementation of quantum network coding using measurement-based quantum computation on IBM Q processors. We compare the performance of quantum network coding with entanglement swapping and entanglement distribution via linear cluster states. These protocols outperform quantum network coding in terms of the final Bell pair fidelities but are unsuitable for optimal resource utilization in complex networks with contention present. We demonstrate the suitability of noisy intermediate-scale quantum (NISQ) devices such as IBM Q for the study of quantum networks. We also identify the factors that limit the performance of quantum network coding on these processors and provide estimates or error rates required to boost the final Bell pair fidelities to a point where they can be used for generation of genuinely random cryptographic keys among other useful tasks. Surprisingly, the required error rates are only around a factor of 2 smaller than the current status and we expect they will be achieved in the near future. ","Modeling of Measurement-based Quantum Network Coding on IBM Q Experience
  Devices"
125,1179552879625084930,3004166806,David Rolnick,"['In new work with @KordingLab, we reconstruct the architecture, weights, and biases of unknown deep ReLU networks by observing their output.  ReLU nets are piecewise linear functions - a lot of info is in the boundaries between linear regions.\nFull paper: <LINK> <LINK>']",https://arxiv.org/abs/1910.00744,"It has been widely assumed that a neural network cannot be recovered from its outputs, as the network depends on its parameters in a highly nonlinear way. Here, we prove that in fact it is often possible to identify the architecture, weights, and biases of an unknown deep ReLU network by observing only its output. Every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism. ",Reverse-Engineering Deep ReLU Networks
126,1179420175365558274,1726662073,Jean-Loup Baudino,"['[Sci] New paper online! This time the main idea is effective temperature, i.e. the basic parameter that we try to infer using the observation: the reference temperature of an exoplanet. <LINK> 1/9 <LINK>', 'You have three methods in the literature: 1. compare the observations with models (each model has a given temperature) 2. Retrieve the temperature profile of the exoplanet (not easy to linked to the effective temperature it-self) 2/9', '3. Extrapolate the spectrum from the observations (possible if good wavelength coverage and good quality) \n3/9', 'We proposed a new method to give the effective temperature using a retrieval approach: 1. retrieval of the atmosphere structure using the observations 2. computation of the full spectrum using this structure 3. Integration to compute the effective temperature.\n4/9', 'As mentioned the effective temperature can be extrapolated directly from the data; but currently this requires to combine multiple instruments from multiple observatories with quite big error bars! 5/9', 'JWST alone will give at a same time a huge wavelength coverage + a good accuracy on the measurement. We show in the paper that we will be able to use the data from JWST directly, without extrapolation, to determine the effective temperature. \n6/9', 'Being able to determine the effective temperature without models will help to improve our models as the data directly will be use to benchmark our approaches. 7/9', 'We will be able to use fully this if all the models that we are using in the community are properly benchmarked between each other, to be able to determine the similarities and the differences between them. 8/9', 'To go further away, if you are an exoplanet modeller, please take a look on this benchmark protocol: https://t.co/Mke1YFTRYZ and/or come to have a chat ! 9/9']",https://arxiv.org/abs/1910.00329,"The current sparse wavelength range coverage of exoplanet direct imaging observations, and the fact that models are defined using a finite wavelength range, lead both to uncertainties on effective temperature determination.We study these effects using black-bodies and atmospheric models and we detail how to infer this parameter. Through highlighting the key wavelength coverage that allows for a more accurate representation of the effective temperature, our analysis can be used to mitigate or manage extra uncertainties being added in the analysis from the models. We find that the wavelength range coverage will soon no longer be a problem. An effective temperature computed by integrating the spectroscopic observations of the James Webb Space Telescope (JWST) will give uncertainties similar to, or better than, the current state-of-the-art, which is to fit models to data. Accurately calculating the effective temperature will help to improve current modelling approaches. Obtaining an independent and precise estimation of this crucial parameter will help the benchmarking process to identify the best practice to model exoplanet atmospheres. ","Toward the Analysis of JWST Exoplanet Spectra: the effective temperature
  in the context of direct imaging"
127,1179409574815830016,1177063549606203394,Tommi Tenkanen,"['A new paper out: <LINK> \nIf cosmic inflation lasted for a long time, then the observed length scales originate from modes smaller than the Planck length. This has serious consequences on dark matter which has its origins in quantum fluctuations during inflation.']",https://arxiv.org/abs/1910.00521,"If the inflationary phase lasted longer than the minimal period, the length scales observed today originate from modes that were smaller than the Planck length during inflation. It was recently argued that this ""trans-Planckian problem"" can never arise in a consistent string theory framework, which places a stringent constraint on the energy scale of inflation, $V^{1/4}\lesssim 10^9$ GeV. In this paper, we show that this requirement corresponds to a very small Hubble scale during inflation, $H_{\rm inf}\lesssim 1$ GeV, and therefore has serious consequences on scenarios where the dark matter density was generated by amplification of quantum fluctuations during inflation. We also present a class of inflationary models which both satisfy the above limit for the scale of inflation and are in perfect agreement with observational data. ","Trans-Planckian Censorship, Inflation and Dark Matter"
128,1179405517665587201,37180215,eleftherios,['Want to fit IVIM? Here is how you do it!!! New paper! ü§©ü§©ü§© <LINK>'],https://arxiv.org/abs/1910.00095,"Fitting multi-exponential models to Diffusion MRI (dMRI) data has always been challenging due to various underlying complexities. In this work, we introduce a novel and robust fitting framework for the standard two-compartment IVIM microstructural model. This framework provides a significant improvement over the existing methods and helps estimate the associated diffusion and perfusion parameters of IVIM in an automatic manner. As a part of this work we provide capabilities to switch between more advanced global optimization methods such as simplicial homology (SH) and differential evolution (DE). Our experiments show that the results obtained from this simultaneous fitting procedure disentangle the model parameters in a reduced subspace. The proposed framework extends the seminal work originated in the MIX framework, with improved procedures for multi-stage fitting. This framework has been made available as an open-source Python implementation and disseminated to the community through the DIPY project. ",Fitting IVIM with Variable Projection and Simplicial Optimization
129,1179194534313394177,2337598033,Geraint F. Lewis,"['A new, and rather cool, paper on the arXiv - ‚ÄúA black box for dark sector physics: Predicting dark matter annihilation feedback with conditional GANs‚Äù <LINK>\n\nAnd that‚Äôs not the end of this week‚Äôs results! <LINK>']",https://arxiv.org/abs/1910.00291,"Traditionally, incorporating additional physics into existing cosmological simulations requires re-running the cosmological simulation code, which can be computationally expensive. We show that conditional Generative Adversarial Networks (cGANs) can be harnessed to predict how changing the underlying physics alters the simulation results. To illustrate this, we train a cGAN to learn the impact of dark matter annihilation feedback (DMAF) on the gas density distribution. The predicted gas density slices are visually difficult to distinguish from their real brethren and the peak counts differ by less than 10 per cent for all test samples (the average deviation is < 3 per cent). Finally, we invert the problem and show that cGANs are capable of endowing smooth density distributions with realistic substructure. The cGAN does however have difficulty generating new knots as well as creating/eliminating bubble-like structures. We conclude that trained cGANs can be an effective approach to provide mock samples of cosmological simulations incorporating DMAF physics from existing samples of standard cosmological simulations of the evolution of cosmic structure. ","A black box for dark sector physics: Predicting dark matter annihilation
  feedback with conditional GANs"
130,1194043091063365633,925800751628279808,Nan Jiang,"['The densest paper I\'ve ever written for a while: <LINK>. My fav part is how a new world pops up when you swap the roles of importance weights &amp; value functions in the ""breaking the curse of horizon"" method (Liu, @LihongLi20 et al) üò≤ (1/x)', 'also, if you\'ve been curious about ""breaking the curse"" but the papers look complicated to you, I\'ve written a 2-page note which derives the method in a few lines https://t.co/Wq5HH0NdQN. The idea behind it is really simple! Hope this helps demystify these cool methods! (2/x)', 'Besides the methods covered in the note, we also show that we now have a framework that unifies many old &amp; new algs in RL: (tabular) model-based, importance sampling (!), marginalized IS, LSTDQ (!), DualDice, KernelLoss, etc... (3/x)', 'All insightful results are by visiting PhD student Masatoshi Uehara, and all stupid examples and weird observations are by me :) (4/4)']",https://arxiv.org/abs/1910.12809,"We provide theoretical investigations into off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner. (3) Several additional results that offer further insights into these methods, including the sample complexity analyses of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a unified view of some old and new algorithms in RL. ",Minimax Weight and Q-Function Learning for Off-Policy Evaluation
131,1192012326855163904,22037262,Stuart James,"['Ever wondered how we #Sketch effectively in #VR? Pure virtual? Physical devices? Check our new interaction study paper by @dannox75, Donald Degraen and @anthony_steed on arXiv  <LINK>']",https://arxiv.org/abs/1910.11637,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from within the virtual environment itself. These tools employ sketching and sculpting techniques known from desktop-based interfaces and apply them to hand-based controller interaction. While these techniques allow for mid-air sketching of basic shapes, it remains difficult for users to create detailed and comprehensive 3D models. In our work, we focus on supporting the user in designing the virtual environment around them by enhancing sketch-based interfaces with a supporting system for interactive model retrieval. Through sketching, an immersed user can query a database containing detailed 3D models and replace them into the virtual environment. To understand supportive sketching within a virtual environment, we compare different methods of sketch interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet. %using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and 3D mid-air sketching. Our results show that 3D mid-air sketching is considered to be a more intuitive method to search a collection of models while the addition of physical devices creates confusion due to the complications of their inclusion within a virtual environment. While we pose our work as a retrieval problem for 3D models of chairs, our results can be extrapolated to other sketching tasks for virtual environments. ",Mixing realities for sketch retrieval in Virtual Reality
132,1190843657429102592,923231130383536128,Eduardo Fonseca,['Do you have a sound event dataset with noisy labels &amp; a deep network? Would you like to boost recognition performance with minimal changes in your pipeline?\nCheck some things to (not) do in our new WASPAA paper! <LINK> #DCASE #machinelistening <LINK>'],https://arxiv.org/abs/1910.12004,"Label noise is emerging as a pressing issue in sound event classification. This arises as we move towards larger datasets that are difficult to annotate manually, but it is even more severe if datasets are collected automatically from online repositories, where labels are inferred through automated heuristics applied to the audio content or metadata. While learning from noisy labels has been an active area of research in computer vision, it has received little attention in sound event classification. Most recent computer vision approaches against label noise are relatively complex, requiring complex networks or extra data resources. In this work, we evaluate simple and efficient model-agnostic approaches to handling noisy labels when training sound event classifiers, namely label smoothing regularization, mixup and noise-robust loss functions. The main advantage of these methods is that they can be easily incorporated to existing deep learning pipelines without need for network modifications or extra resources. We report results from experiments conducted with the FSDnoisy18k dataset. We show that these simple methods can be effective in mitigating the effect of label noise, providing up to 2.5\% of accuracy boost when incorporated to two different CNNs, while requiring minimal intervention and computational overhead. ","Model-agnostic Approaches to Handling Noisy Labels When Training Sound
  Event Classifiers"
133,1190333497451450369,317422543,Ricard Sol√©,"[""Can parasites be central to the evolution of complexity? They are often seen as byproducts of evolutionary change, but they can actually be crucial to expand the computational landscape of complexity. Here's our new paper with @brigan_raman on <LINK> @sfiscience <LINK>""]",https://arxiv.org/abs/1910.14339,"Why are living systems complex? Why does the biosphere contain living beings with complexity features beyond those of the simplest replicators? What kind of evolutionary pressures result in more complex life forms? These are key questions that pervade the problem of how complexity arises in evolution. One particular way of tackling this is grounded in an algorithmic description of life: living organisms can be seen as systems that extract and process information from their surroundings in order to reduce uncertainty. Here we take this computational approach using a simple bit string model of coevolving agents and their parasites. While agents try to predict their worlds, parasites do the same with their hosts. The result of this process is that, in order to escape their parasites, the host agents expand their computational complexity despite the cost of maintaining it. This, in turn, is followed by increasingly complex parasitic counterparts. Such arms races display several qualitative phases, from monotonous to punctuated evolution or even ecological collapse. Our minimal model illustrates the relevance of parasites in providing an active mechanism for expanding living complexity beyond simple replicators, suggesting that parasitic agents are likely to be a major evolutionary driver for biological complexity. ",How Turing parasites expand the computational landscape of digital life
134,1190267490598711297,1169373448193236992,Pam Vervoort,"[""üåéüåûüö®Hey... wait a minute... this is not a paleoclimate paper! Venturing into new scientific territories and exploring how sensitive Earth's orbital cycles are to the presence of a giant planet like Jupiter. @JontiHorner @ExoCytherean @thealmashow \n\n<LINK>""]",https://arxiv.org/abs/1910.14250,"A wealth of Earth-sized exoplanets will be discovered in the coming years, proving a large pool of candidates from which the targets for the search for life beyond the Solar system will be chosen. The target selection process will require the leveraging of all available information in order to maximise the robustness of the target list and make the most productive use of follow-up resources. Here, we present the results of a suite of $n$-body simulations that demonstrate the degree to which the orbital architecture of the Solar system impacts the variability of Earth's orbital elements. By varying the orbit of Jupiter and keeping the initial orbits of the other planets constant, we demonstrate how subtle changes in Solar system architecture could alter the Earth's orbital evolution -- a key factor in the Milankovitch cycles that alter the amount and distribution of solar insolation, thereby driving periodic climate change on our planet. The amplitudes and frequencies of Earth's modern orbital cycles fall in the middle of the range seen in our runs for all parameters considered -- neither unusually fast nor slow, nor large nor small. This finding runs counter to the `Rare Earth' hypothesis, which suggests that conditions on Earth are so unusual that life elsewhere is essentially impossible. Our results highlight how dynamical simulations of newly discovered exoplanetary systems could be used as an additional means to assess the potential targets of biosignature searches, and thereby help focus the search for life to the most promising targets. ",Quantifying the Influence of Jupiter on the Earth's Orbital Cycles
135,1190093890319110144,4692890148,Dr. Ryan Alimo,['our new paper #alphaDOGS (w/ Pooriya Beyhaghi and Thomas Bewley): \n- <LINK> \nMuhan Zhao and I implemented the python version: \n- <LINK> (code is available in MATLAB too)\n#alphaDOGS #ai #deltaDOGS #optimization'],https://arxiv.org/abs/1910.12393,"This paper considers the efficient minimization of the infinite time average of a stationary ergodic process in the space of a handful of design parameters which affect it. Problems of this class, derived from physical or numerical experiments which are sometimes expensive to perform, are ubiquitous in engineering applications. In such problems, any given function evaluation, determined with finite sampling, is associated with a quantifiable amount of uncertainty, which may be reduced via additional sampling. The present paper proposes a new optimization algorithm to adjust the amount of sampling associated with each function evaluation, making function evaluations more accurate (and, thus, more expensive), as required, as convergence is approached. The work builds on our algorithm for Delaunay-based Derivative-free Optimization via Global Surrogates ($\Delta$-DOGS). The new algorithm, dubbed $\alpha$-DOGS, substantially reduces the overall cost of the optimization process for problems of this important class. Further, under certain well-defined conditions, rigorous proof of convergence to the global minimum of the problem considered is established. ","A derivative-free optimization algorithm for the efficient minimization
  of functions obtained via statistical averaging"
136,1189127347821256704,922917707808497669,Ali Shahin Shamsabadi,"['Our new paper, EdgeFool: An Adversarial Image Enhancement Filter, is on arXiv now.\nEdgeFool generates #adversarial images with #perturbations that #enhance image, as opposed to the SOA adversarial methods that may distort the image.\n<LINK>', '(Takeaway message: Images are special data, do not look at them as they are like financial data or ... )']",https://arxiv.org/abs/1910.12227,"Adversarial examples are intentionally perturbed images that mislead classifiers. These images can, however, be easily detected using denoising algorithms, when high-frequency spatial perturbations are used, or can be noticed by humans, when perturbations are large. In this paper, we propose EdgeFool, an adversarial image enhancement filter that learns structure-aware adversarial perturbations. EdgeFool generates adversarial images with perturbations that enhance image details via training a fully convolutional neural network end-to-end with a multi-task loss function. This loss function accounts for both image detail enhancement and class misleading objectives. We evaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using two datasets (ImageNet and Private-Places365) and compare it with six adversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv, Non-targeted and Private Fast Gradient Sign Methods). Code is available at this https URL ",EdgeFool: An Adversarial Image Enhancement Filter
137,1189053661974073349,2776073211,Tomasz Kusmierczyk,"['Our new paper ""Prior specification via prior predictive matching: Poisson matrix factorization and beyond"" is already available on arXiv (<LINK>). Accompanying code can be found on GitHub (<LINK>).']",https://arxiv.org/abs/1910.12263,"Hyperparameter optimization for machine learning models is typically carried out by some sort of cross-validation procedure or global optimization, both of which require running the learning algorithm numerous times. We show that for Bayesian hierarchical models there is an appealing alternative that allows selecting good hyperparameters without learning the model parameters during the process at all, facilitated by the prior predictive distribution that marginalizes out the model parameters. We propose an approach that matches suitable statistics of the prior predictive distribution with ones provided by an expert and apply the general concept for matrix factorization models. For some Poisson matrix factorization models we can analytically obtain exact hyperparameters, including the number of factors, and for more complex models we propose a model-independent optimization procedure. ","Prior specification via prior predictive matching: Poisson matrix
  factorization and beyond"
138,1187049345675223040,465023908,Ben Bartlett,"['My new paper on photonic quantum programmable gate arrays is now on arXiv! üçæ‚öõÔ∏èüéâ We describe an architecture for a nanophotonic integrated circuit which can be reprogrammed to perform any quantum computation. <LINK> <LINK>', '1/ Photonic systems have many unique advantages for quantum information processing, but deterministic multi-photon gates are difficult to implement, and complex quantum circuits can be prohibitively large to do with free-space optics since processing is done along the photon path', '2/ In this theoretical paper, we present an architecture for a photonic integrated circuit which can be dynamically programmed to implement any quantum operation, in principle deterministically and with perfect fidelity.', '3/ Our architecture consists of a lattice of beamsplitters and phase shifters, which perform rotations on path-encoded photonic qubits, and embedded quantum emitters, which use a two-photon scattering process to implement two-qubit controlled gates deterministically. https://t.co/WHpiBaeuz9', '4/ By appropriately setting phase shifts, the device can be programmed to implement any quantum circuit without hardware modifications.', ""5/ We show how to exactly prepare arbitrary quantum states and operators on the device, and we apply machine learning techniques to automatically implement highly compact approximations to important quantum circuits. Here's it learning to implement a quantum Fourier transform: https://t.co/dMMx2WcDJk"", '6/ Our design is the first (to our knowledge) to extend programmable integrated optics to the quantum domain in a manner which is both deterministic and spatially efficient (an N qubit state is encoded in O(N) waveguides).', '7/7 There has been tremendous recent experimental progress in both of the key technologies required to realize this design: nanophotonic processors and waveguide-coupled quantum emitters. Ongoing advancements may allow for feasible near-future implementation of this device.', 'Supplementary materials and the #TensorFlow code for the circuit optimization section can be found at https://t.co/0Z9mBDKRqG', ""Thanks to @sunilkpai and some of my labmates who aren't on Twitter for being helpful brainstorming buddies / rubber ducks for debugging, and shoutout to @InertialObservr for helping me debug the Mathematica animation in the first Tweet üòÇ"", '@ixfoduap thank you! your gate synthesis paper was a big inspiration for the last part of this paper!', '@brewstronomy This was most of the Mathematica code I had to write for the animation https://t.co/WMQW3KQP66', ""@brewstronomy it's actually not as complex as you might initially guess: I took a transparent png of the figure, clicked points along path I wanted to follow, and made an interpolating path from them. Then I just made a generic gaussian wavepacket follow that path and put the image in xz plane"", ""@brewstronomy so it's not 100% accurate (you wouldn't be able to see the photon pulse if it were to scale), but it's mainly meant as a conceptual aid to understand the scattering process"", '@alfcnz the animation would look weird if it were directly from above and would be harder to program', ""@alfcnz if you view directly from above, you need to have the pulse oscillate sideways to see it, otherwise it just looks like a blue line. when it goes around the curves you'd need to transform it to look like it is still traveling along the curved waveguides. this isn't a problem in 3D"", ""@alfcnz they're animated that way, yes"", '@alfcnz Mathematica! üòÄ https://t.co/KBxphIkAlQ', '@alfcnz @brewstronomy @matplotlib You could definitely do this with @matplotlib, although it might take a bit more code']",https://arxiv.org/abs/1910.10141,"We present a photonic integrated circuit architecture for a quantum programmable gate array (QPGA) capable of preparing arbitrary quantum states and operators. The architecture consists of a lattice of phase-modulated Mach-Zehnder interferometers, which perform rotations on path-encoded photonic qubits, and embedded quantum emitters, which use a two-photon scattering process to implement a deterministic controlled-$\sigma_z$ operation between adjacent qubits. By appropriately setting phase shifts within the lattice, the device can be programmed to implement any quantum circuit without hardware modifications. We provide algorithms for exactly preparing arbitrary quantum states and operators on the device and we show that gradient-based optimization can train a simulated QPGA to automatically implement highly compact approximations to important quantum circuits with near-unity fidelity. ","Universal programmable photonic architecture for quantum information
  processing"
139,1186836840327892993,48712353,Sungjin Ahn üá∫üá¶,"['Check out our new paper with Fei Deng and Zhuo Zhi on  ""Generative Hierarchical Modeling of Parts, Objects, and Scenes"" <LINK>\n\nWe learn compositional and interpretable probabilistic scene graphs from images. <LINK>']",https://arxiv.org/abs/1910.09119,"Compositional structures between parts and objects are inherent in natural scenes. Modeling such compositional hierarchies via unsupervised learning can bring various benefits such as interpretability and transferability, which are important in many downstream tasks. In this paper, we propose the first deep latent variable model, called RICH, for learning Representation of Interpretable Compositional Hierarchies. At the core of RICH is a latent scene graph representation that organizes the entities of a scene into a tree structure according to their compositional relationships. During inference, taking top-down approach, RICH is able to use higher-level representation to guide lower-level decomposition. This avoids the difficult problem of routing between parts and objects that is faced by bottom-up approaches. In experiments on images containing multiple objects with different part compositions, we demonstrate that RICH is able to learn the latent compositional hierarchy and generate imaginary scenes. ","Generative Hierarchical Models for Parts, Objects, and Scenes"
140,1186710627655413761,633942876,Kevin L. Keys,"['Feeling both proud and nepotistic today as I announce a new paper\n\nüéâ from my little brother! üéâ\n\nIf you love stochastic differential equations, quantum mechanics, and Sooper Precise Notation, then take a peek here:\n\n<LINK>', 'This is the same baby bro whose first publication landed the cover of the (paywalled) journal Solar Physics.\n\nref: Keys, D., Kholikov, S., Pevtsov, A.A. 2015, Solar Physics, 290, 659\n\nblog: https://t.co/E8BS00mcCu\n\npreprint: https://t.co/DD3YUpAY9B', 'I cheekily informed him that it‚Äôs hard to read the discussion in his recent paper about Fock spaces and Wiener processes without giggling like a teenager. üòá']",https://arxiv.org/abs/1910.08649,"The paper studies a class of quantum stochastic differential equations, modeling an interaction of a system with its environment in the quantum noise approximation. The space representing quantum noise is the symmetric Fock space over L^2(R_+). Using the isomorphism of this space with the space of square-integrable functionals of the Poisson process, the equations can be represented as classical stochastic differential equations, driven by Poisson processes. This leads to a discontinuous dynamical state reduction which we compare to the Ghirardi-Rimini-Weber model. A purely quantum object, the norm process, is found which plays the role of an observer (in the sense of Everett [H. Everett III, Reviews of modern physics, 29.3, 454, (1957)]), encoding all events occurring in the system space. An algorithm introduced by Dalibard et al [J. Dalibard, Y. Castin, and K. M{\o}lmer, Physical review letters, 68.5, 580 (1992)] to numerically solve quantum master equations is interpreted in the context of unravellings and the trajectories of expected values of system observables are calculated. ","Poisson stochastic master equation unravellings and the measurement
  problem: a quantum stochastic calculus perspective"
141,1186445773807992832,247800333,Ahmad ÿ∑Ÿá,"['New paper accepted for publication on dynamic state estimation (DSE) in power systems for nonlinear machine models *with performance guarantees*: <LINK>\nThis paper deserves a blogpost but a Twitter thread should do it.', ""DSE is really an old research problem---not only in power systems. There's virtually tens of methods in the literature that are based on Kalman filter and its variants of stochastic DSE methods. This work deviates from the literature's approach and follows a new, simpler one."", ""The problem is difficult due to: \ni) non-Gaussian noise of sensors (PMUs);\nii) the nonlinearity in both process and measurement models\niii) need to perform DSE so quickly to keep up with PMU's sampling rate."", 'In this work, we show how simple, highly-efficient, and robust nonlinear DSE, based on 100+ year old Lyapunov theory &amp; new convex relaxations, can be designed to guarantee upper and lower bounds on the state estimation error norm--the difference between system &amp; estimated states.', ""We also compare our approach to three mainstream methods in the literature of power systems DSE and showcase that the theoretical bounds aren't only theoretically useful, but they all hold under various scenarios."", ""This paper took so much of my student's energy (and mine too) over the summer, and I'm so happy that it's finally accepted for publication. It's a shame Sebastian isn't on Twitter. This man deserves so much good."", '@theEnergyMads Thanks Mads. By the way, this same approach in the paper can be extended to control for nonlinear dynamic models, without resorting to any linearization.']",https://arxiv.org/abs/1910.09487,"A robust observer for performing power system dynamic state estimation (DSE) of a synchronous generator is proposed. The observer is developed using the concept of $\mathcal{L}_{\infty}$ stability for uncertain, nonlinear dynamic generator models. We use this concept to (i) design a simple, scalable, and robust dynamic state estimator and (ii) obtain a performance guarantee on the state estimation error norm relative to the magnitude of uncertainty from unknown generator inputs, and process and measurement noises. Theoretical methods to obtain upper and lower bounds on the estimation error are also provided. Numerical tests validate the performance of the $\mathcal{L}_{\infty}$-based estimator in performing DSE under various scenarios. The case studies reveal that the derived theoretical bounds are valid for a variety of case studies and operating conditions, while yielding better performance than existing power system DSE methods. ","Robust Dynamic State Estimation of Synchronous Machines with Asymptotic
  State Estimation Error Performance Guarantees"
142,1185173733775949824,1171357907574824961,≈Åukasz Tychoniec,['Jets in Serpens cloud! Check out my new paper on arxiv today: <LINK> #astronomy #PhDresearch #phdlife <LINK>'],https://arxiv.org/abs/1910.07857,"The fastest molecular component to the protostellar outflows -- extremely high-velocity (EHV) molecular jets -- are still puzzling since they are seen only rarely. The first aim is to analyze the interaction between the EHV jet and the slow outflow by comparing their outflow force content. The second aim is to analyze the chemical composition of the different outflow velocity components and to reveal the spatial location of molecules. ALMA 3 mm and 1.3 mm observations of five outflow sources at 130 -- 260 au resolution in the Serpens Main cloud are presented. Observations of CO, SiO, H$_2$CO and HCN reveal the kinematic and chemical structure of those flows. Three velocity components are distinguished: the slow and the fast wing, and the EHV jet. Out of five sources, three have the EHV component. Comparison of outflow forces reveals that only the EHV jet in the youngest source Ser-emb 8 (N) has enough momentum to power the slow outflow. The SiO abundance is generally enhanced with velocity, while HCN is present in the slow and the fast wing, but disappears in the EHV jet. For Ser-emb 8 (N), HCN and SiO show a bow-shock shaped structure surrounding one of the EHV peaks suggesting sideways ejection creating secondary shocks upon interaction with the surroundings. Also, the SiO abundance in the EHV gas decreases with distance from this protostar, whereas that in the fast wing increases. H$_2$CO is mostly associated with low-velocity gas but also appears surprisingly in one of the bullets in the Ser-emb~8~(N) EHV jet. The high detection rate suggests that the presence of the EHV jet may be more common than previously expected. The origin and temporal evolution of the abundances of SiO, HCN and H$_2$CO through high-temperature chemistry are discussed. The data are consistent with a low C/O ratio in the EHV gas versus high C/O ratio in the fast and slow wings. ","Chemical and kinematic structure of extremely high-velocity molecular
  jets in the Serpens Main star-forming region"
143,1184856036076863488,1030500158159695872,Patrick Lewis,"[""QA Models should work in any language. So, we're releasing MLQA, a new cross-lingual QA evaluation dataset! \n\nCheck out the paper and dataset:\n<LINK>\n<LINK>\nWith Barlas Oguz, Ruty Rinott, @riedelcastro, @SchwenkHolger\n\n@facebookai @ucl_nlp üöÄ <LINK>"", '@riedelcastro @SchwenkHolger @facebookai @ucl_nlp MLQA is a highly parallel Extractive QA dataset in 7 diverse languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. \n\nBecause its highly parallel, we can even evaluate unusual language combinations like questions in Arabic and documents in Hindi', '@riedelcastro @SchwenkHolger @facebookai @ucl_nlp We train QA models using SQuAD, and test *zero-shot* in target languages. Even powerful models like XLM and MT approaches struggle to transfer without significant performance drops.\n\nWe hope that MLQA will be a useful testbed going forward for cross-lingual research.', ""@riedelcastro @SchwenkHolger @facebookai @ucl_nlp I've always loved chocolate..."", '@sameer_ @riedelcastro @SchwenkHolger @facebookai @ucl_nlp I pronounce it ""emm eeel kwah"".....']",https://arxiv.org/abs/1910.07475,"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance. ",MLQA: Evaluating Cross-lingual Extractive Question Answering
144,1184845891036954624,734677275216470016,Guodong Zhang,"['New work on solving minimax optimization locally. With @YuanhaoWang3 Jimmy Ba.\n\nWe propose a novel algorithm which converges to and only converges to local minimax. The main innovation is a correction term on top of gradient descent-ascent.\n\nPaper link: <LINK>', '@YuanhaoWang3 By adding the correction term to gradient descent-ascent (GDA), we guarantee that the whole system would get closer to the `ridge` every iteration while GDA fails to do so. https://t.co/8nQ00YLXjW']",https://arxiv.org/abs/1910.07512,"Many tasks in modern machine learning can be formulated as finding equilibria in \emph{sequential} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose \emph{Follow-the-Ridge} (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and \emph{positive} momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms. ",On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach
145,1183752705174626314,48712353,Sungjin Ahn üá∫üá¶,"['Check out our new paper with Jindong Jiang, Sepehr Janghorbani, and @gdm3000.  SCALOR for scalable unsupervised sequential object-oriented representation learning via generation. <LINK>\n<LINK> <LINK>', '@gdm3000 With moving background https://t.co/cK7mWuF8yw', '@gdm3000 MNIST https://t.co/raD70vYXG7', '@gdm3000 Very High Density https://t.co/yqH4woPRfF']",https://arxiv.org/abs/1910.02384,"Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the first unsupervised object representation model shown to work for natural scenes containing several tens of moving objects. ",SCALOR: Generative World Models with Scalable Object Representations
146,1182102744032858112,907232486735958018,Jaki Noronha-Hostler,"['New paper on the arxiv and my undergrad is the lead author! We make (baseline) predictions for integrated flow harmonics at the beam energy scan using linear+cubic response.   \n\n<LINK>', ""The idea was the following: we know at the lowest beam energies that the physics has to change but it's complicated since both the initial conditions and medium properties are changing at the same time. We wanted a way to separate those differences."", ""Also, we wanted to be able to test when the physics breaks down (using the assumptions of high energy collisions).  Basically, if we use the high-energy paradigm we expect that the initial conditions shouldn't change with beam energy (nor the flow fluctuations)."", 'This is the paper I was talking about here: \nhttps://t.co/BHlydrSurO']",https://arxiv.org/abs/1910.03677,"Currently the RHIC Beam Energy Scan is exploring a new region of the Quantum Chromodynamic phase diagram at large baryon densities that approaches nuclear astrophysics regimes. This provides an opportunity to study relativistic hydrodynamics in a regime where the net conserved charges of baryon number, strangeness, and electric charge play a role, which will significantly change the theoretical approach to simulating the baryon-dense Quark-Gluon Plasma. Here we detail many of the important changes needed to adapt both initial conditions and the medium to baryon-rich matter. Then, we make baseline predictions for the elliptical flow and fluctuations based on extrapolating the physics at LHC and top RHIC energies to support future analyses of where and how the new baryon-dense physics causes these extrapolations to break down. First we compare eccentricities across beam energies, exploring their underlying assumptions; we find the the extrapolated initial state is predicted to be nearly identical to that at AuAu $\sqrt{s_{NN}}=200$ GeV. Then the final flow harmonic predictions are based on linear+cubic response. We discuss preliminary STAR results in order to determine the implications that they have for linear+cubic response coefficients at the lowest beam energy of AuAu $\sqrt{s_{NN}}=7$ GeV. ","Baseline predictions of elliptic flow and fluctuations at the RHIC Beam
  Energy Scan using response coefficients"
147,1181577526805696516,101209471,Vini Netto üáßüá∑üá¨üáßüá∫üá≤üáµüáπ,"['SCALING IN CITIES:\n""On the Relation between Transversal and Longitudinal Scaling in Cities""\n\nNEW PAPER by Ribeiro, @joaovmeirelles  et al\n\n<LINK> <LINK>']",https://arxiv.org/abs/1910.02113,"Given that a group of cities follows a scaling law connecting urban population with socio-economic or infrastructural metrics (transversal scaling), should we expect that each city would follow the same behavior over time (longitudinal scaling)? This assumption has important policy implications, although rigorous empirical tests have been so far hindered by the lack of suitable data. Here, we advance the debate by looking into the temporal evolution of the scaling laws for 5507 municipalities in Brazil. We focus on the relationship between population size and two urban variables, GDP and water network length, analyzing the time evolution of the system of cities as well as their individual trajectory. We find that longitudinal (individual) scaling exponents are city-specific, but they are distributed around an average value that approaches to the transversal scaling exponent when the data are decomposed to eliminate external factors, and when we only consider cities with a sufficiently large growth rate. Such results give support to the idea that the longitudinal dynamics is a micro-scaling version of the transversal dynamics of the entire urban system. Finally, we propose a mathematical framework that connects the microscopic level to global behavior, and, in all analyzed cases, we find good agreement between theoretical prediction and empirical evidence. ",On the relation between Transversal and Longitudinal Scaling in Cities
148,1181041019615186944,19955483,Martin Roetteler,"['Doing the new paper dance tonight: happy to announce resource analysis of post-quantum schemes and Q# implementations: <LINK>  #Qsharp, #AES, #LowMC, @MSFTResearch @MSFTQuantum', '@JasonDCFord1 @MSFTResearch @MSFTQuantum Thanks!']",https://arxiv.org/abs/1910.01700,"Grover's search algorithm gives a quantum attack against block ciphers by searching for a key that matches a small number of plaintext-ciphertext pairs. This attack uses $O(\sqrt{N})$ calls to the cipher to search a key space of size $N$. Previous work in the specific case of AES derived the full gate cost by analyzing quantum circuits for the cipher, but focused on minimizing the number of qubits. In contrast, we study the cost of quantum key search attacks under a depth restriction and introduce techniques that reduce the oracle depth, even if it requires more qubits. As cases in point, we design quantum circuits for the block ciphers AES and LowMC. Our circuits give a lower overall attack cost in both the gate count and depth-times-width cost models. In NIST's post-quantum cryptography standardization process, security categories are defined based on the concrete cost of quantum key search against AES. We present new, lower cost estimates for each category, so our work has immediate implications for the security assessment of post-quantum cryptography. As part of this work, we release Q# implementations of the full Grover oracle for AES-128, -192, -256 and for the three LowMC instantiations used in Picnic, including unit tests and code to reproduce our quantum resource estimates. To the best of our knowledge, these are the first two such full implementations and automatic resource estimations. ",Implementing Grover oracles for quantum key search on AES and LowMC
149,1191066698683748353,780828430325600256,Artjoms ≈†eƒºa,"['You may have heard from an article in @NatureHumBehav that there is strong stylometric evidence for Beowulf being a creation of a single author.  In our replication study, led by Petr Plech√°ƒç @versotym we show it is quite unlikely. Pre-print here: <LINK> /1', ""Features that were used in the original paper turned to be poor predictors of authorship or haven't handled well (or both) with significant errors in data and code  /2"", 'Sense-pauses and compounds were left untested for their attribution value; claims based on hierarchical clustering of Beowulf parts to each other (25 top 3-grams used) without any other ""real"" candidate available + clear heterogeneity of stylistic signal between the parts /3', 'This paper reminds us of usual methodological caution in stylometry and, probably, hints that major journals would need to update their reviewer pool in computational lit text analysis. There will be much more of those studies! /4', ""Can't find their Twitter handles, but aside from Petr this work would be impossible without Andrew Cooper &amp; Benjamin Nagy !"", ""P.S. absolutely missed a link to the original paper because I'm clumsy: https://t.co/zpPOQq3mLF"", 'Ok, I found one! @rantyben redesigned original meter analysis to show actual difference in use of metrical types across Beowulf-A and Beowulf-B']",https://arxiv.org/abs/1910.12927,"In Nature Human Behaviour 3/2019, an article was published entitled ""Large-scale quantitative profiling of the Old English verse tradition"" dealing with (besides other things) the question of the authorship of the Old English poem Beowulf. The authors provide various textual measurements that they claim present ""serious obstacles to those who would advocate for composite authorship or scribal recomposition"" (p. 565). In what follows we raise doubts about their methods and address serious errors in both their data and their code. We show that reliable stylometric methods actually identify significant stylistic heterogeneity in Beowulf. In what follows we discuss each method separately following the order of the original article. ","Reply to: Large-scale quantitative profiling of the Old English verse
  tradition"
150,1190067652254982145,1376287471,Tharindu Jayasinghe,"['New paper day!\nThe latest paper in the ASAS-SN (@SuperASASSN) series of variable stars (Paper VI) is on astro-ph tonight : <LINK>\nHere, we study an all-sky catalog of ~8,400 Delta Scuti stars, out of which ~3,300 (~40%) are new discoveries! <LINK>', 'Delta Scuti stars are pulsating variable stars (kappa mechanism), much like the classifical pulsators (RR Lyrae and Cepheids), and they follow a period-luminosity relationship (PLR). We derive PLRs for fundamental mode and overtone Delta Scuti stars using an all-sky sample! https://t.co/dwLYxHUUxK', '@NASA_TESS has observed the southern hemisphere last year, so we obtained 20 TESS light curves of new ASAS-SN Delta Scuti stars using the pipeline curated by @AstronomerPat. TESS light curves are awesome! We found out that two of these variables were in eclipsing binaries! https://t.co/BVXyCCJDA3', 'We also find that the overtone pulsators that are analyzed in this work have a dominant overtone pulsation, with most of them pulsating in the second overtone or higher modes. We also find evidence of Delta Scuti stars pulsating in a dominant fourth overtone. https://t.co/81wBb2NMw0', 'We were in for something exciting when we looked at the Galactic distribution of these stars. Through cross-matches to large spectroscopic catalogs, we see a clear trend in period-metallicity space for these stars. Longer period Delta Scuti are more metal rich! https://t.co/NgFfAjZXKM', 'Looking at the distribution of these stars above the Galactic mid-plane (Z), we see that the median periods of these variables are longer as we approach the Galactic disk, and their median periods gradually get shorter as we move away from the mid-plane... https://t.co/gaxRPzccoD', 'When we look at the same distribution for fundamental mode RR Lyrae stars, we see the exact opposite trend. However, we know that shorter period RR Lyrae have lower metallicities than those with longer periods.... https://t.co/WmzG4UTEe1', 'Thus, these observations are what we would expect of a vertical metallicity gradient in the Galactic disk! I found this quite fascinating. Shown below is an all-sky map of the Delta Scuti, showing the trend in period. https://t.co/X7MTSITdgf', '@AstronomerPat @NASA_TESS This is perfect!']",http://arxiv.org/abs/1910.14187,"We characterize an all-sky catalog of ${\sim} 8,400$ $\delta$ Scuti variables in ASAS-SN, which includes ${\sim} 3,300$ new discoveries. Using distances from \textit{Gaia} DR2, we derive period-luminosity relationships (PLRs) for both the fundamental mode and overtone pulsators in the $W_{JK}$, $V$, Gaia DR2 $G$, $J$, $H$, $K_s$ and $W_1$ bands. We find that the overtone pulsators have a dominant overtone mode, with many sources pulsating in the second overtone or higher order modes. The fundamental mode pulsators have metallicity dependent periods, with $\log_{10} (\rm P){\sim}-1.1$ for $\rm [Fe/H]<-0.3$ and $\log_{10} (\rm P){\sim}-0.9$ for $\rm [Fe/H]>0$ which leads to a period-dependant scale height. Stars with $\rm P>0.100\,d$ are predominantly located close to the Galactic disk ($\rm |Z|<0.5\,kpc$). The median period at a scale height of $\rm Z{\sim}0\, kpc$ also increases with the Galactocentric radius $\rm R$, from $\log_{10} (\rm P){\sim}-0.94$ for sources with $\rm R>9\, kpc$ to $\log_{10} (\rm P){\sim}-0.85$ for sources with $\rm R<7\, kpc$, which is indicative of a radial metallicity gradient. To illustrate potential applications of this all-sky catalog, we obtained 30 min cadence, image subtraction TESS light curves for a sample of 10 fundamental mode and 10 overtone $\delta$ Scuti stars discovered by ASAS-SN. From this sample, we identified two new $\delta$ Scuti eclipsing binaries, ASASSN-V J071855.62$-$434247.3 and ASASSN-V J170344.20$-$615941.2 with short orbital periods of $\rm P_{orb}=2.6096$ d and $\rm P_{orb}=2.5347$ d respectively. ","The ASAS-SN Catalog of Variable Stars VI: An All-Sky Sample of $\delta$
  Scuti Stars"
151,1189664516830900231,206818334,Ivan Oseledets,"['There is problem when trying to use Deep ReLU networks for function approximation: there are many recent papers proving the approximability of classes functions, but most of them do not have any numerics. In <LINK> we propose a greedy way to solve this problem.']",https://arxiv.org/abs/1910.12686,"We propose a new method for learning deep neural network models that is based on a greedy learning approach: we add one basis function at a time, and a new basis function is generated as a non-linear activation function applied to a linear combination of the previous basis functions. Such a method (growing deep neural network by one neuron at a time) allows us to compute much more accurate approximants for several model problems in function approximation. ","Growing axons: greedy learning of neural networks with application to
  function approximation"
152,1189638348643549184,773986416,Nicholas Boardman,"['How might our Galaxy look to an external observer? And is our Galaxy ordinary amongst others of its kind? In a paper out on arXiv today, we employ @MaNGASurvey data to study the properties of nearby Milky Way Analogues. <LINK>']",http://arxiv.org/abs/1910.12896v1,"The Milky Way provides an ideal laboratory to test our understanding of galaxy evolution, owing to our ability to observe our Galaxy over fine scales. However, connecting the Galaxy to the wider galaxy population remains difficult, due to the challenges posed by our internal perspective and to the different observational techniques employed. Here, we present a sample of galaxies identified as Milky Way Analogs (MWAs) on the basis of their stellar masses and bulge-to-total ratios, observed as part of the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey. We analyse the galaxies in terms of their stellar kinematics and populations as well as their ionised gas contents. We find our sample to contain generally young stellar populations in their outskirts. However, we find a wide range of stellar ages in their central regions, and we detect central AGN-like or composite-like activity in roughly half of the sample galaxies, with the other half consisting of galaxies with central star-forming emission or emission consistent with old stars. We measure gradients in gas metallicity and stellar metallicity that are generally flatter in physical units than those measured for the Milky Way; however, we find far better agreement with the Milky Way when scaling gradients by galaxies' disc scale lengths. From this, we argue much of the discrepancy in metallicity gradients to be due to the relative compactness of the Milky Way, with differences in observing perspective also likely to be a factor. ","] Milky Way Analogues in MaNGA: Multi-Parameter Homogeneity and Comparison
  to the Milky Way"
153,1189564049899438080,14544467,Daniel Apai,"[""Identifying exo-earths among other planets in images of LUVOIR-like space telescopes won't be easy. In a new study w Alex Bixel we show how Bayesian classification and contextual info can increase mission yield: <LINK>\n@EOSNExSS @azstewobs @luvoirtelescope""]",https://arxiv.org/abs/1910.13440,"Future space telescopes may be able to directly image $\sim$10 - 100 planets with sizes and orbits consistent with habitable surface conditions (""exo-Earth candidates"" or EECs), but observers will face difficulty in distinguishing these from the potentially hundreds of non-habitable ""false positives"" which will also be detected. To maximize the efficiency of follow-up observations, a prioritization scheme must be developed to determine which planets are most likely to be EECs. In this paper, we present a Bayesian method for estimating the likelihood that any directly imaged extrasolar planet is a true exo-Earth candidate by interpreting the planet's apparent magnitude and separation in light of existing exoplanet statistics. As a specific application of this general framework, we use published estimates of the discovery yield of future space-based direct imaging mission concepts to conduct ""mock surveys"" in which we compute the likelihood that each detected planet is an EEC. We find that it will be difficult to determine which planets are EECs with $>50\%$ confidence using single-band photometry immediately upon their detection. The best way to reduce this ambiguity would be to constrain the planet's orbit by revisiting the system multiple times or through a radial velocity precursor survey. Astrometric or radial velocity constraints on the planet's mass would offer a lesser benefit. Finally, we show that a Bayesian approach to prioritizing targets would improve the follow-up efficiency of a direct imaging survey versus a blind approach using the same data. For example, the prioritized approach could reduce the amount of integration time required for the spectral detection (or rejection) of water absorption in most EECs by a factor of two. ","Identifying Exo-Earth Candidates in Direct Imaging Data through Bayesian
  Classification"
154,1189536300338044929,4639078397,John Wise,"['New paper day! Focusing on the growth of a massive black hole at z=7.5, we find that UV radiation from the SMBH and nuclear stellar cluster regulates runaway star formation within 30 pc, promoting early rapid growth of the SMBH. Led by Ji-hoon Kim (SNU) <LINK> <LINK>', 'Our work also demonstrates the need to match the input physical models with the simulation mass/spatial resolution. Co-authors: @TomAbelStanford, Y. Jo (SNU), @JoelPrimack, @PFHopkins_Astro']",https://arxiv.org/abs/1910.12888,"As computational resolution of modern cosmological simulations reach ever so close to resolving individual star-forming clumps in a galaxy, a need for ""resolution-appropriate"" physics for a galaxy-scale simulation has never been greater. To this end, we introduce a self-consistent numerical framework that includes explicit treatments of feedback from star-forming molecular clouds (SFMCs) and massive black holes (MBHs). In addition to the thermal supernovae feedback from SFMC particles, photoionizing radiation from both SFMCs and MBHs is tracked through full 3-dimensional ray tracing. A mechanical feedback channel from MBHs is also considered. Using our framework, we perform a state-of-the-art cosmological simulation of a quasar-host galaxy at z~7.5 for ~25 Myrs with all relevant galactic components such as dark matter, gas, SFMCs, and an embedded MBH seed of ~> 1e6 Ms. We find that feedback from SFMCs and an accreting MBH suppresses runaway star formation locally in the galactic core region. Newly included radiation feedback from SFMCs, combined with feedback from the MBH, helps the MBH grow faster by retaining gas that eventually accretes on to the MBH. Our experiment demonstrates that previously undiscussed types of interplay between gas, SFMCs, and a MBH may hold important clues about the growth and feedback of quasars and their host galaxies in the high-redshift Universe. ","High-redshift Galaxy Formation with Self-consistently Modeled Stars and
  Massive Black Holes: Stellar Feedback and Quasar Growth"
155,1189008636338692096,2302304521,Dr Andra Stroe üè≥Ô∏è‚Äçüåàüá∑üá¥,"['Harwood,  Vernstrom &amp; Stroe 2019 (or how I like to call it HarVeSt) is accepted for publication! We study how hybrid morphology radio galaxies with different types of jets and plumes on either side can come to be! Fantastic work lead by @AskAstroUK! <LINK>']",https://arxiv.org/abs/1910.12857,"Hybrid morphology radio sources (HyMoRS) are a rare group of radio galaxies in which differing Fanaroff & Riley morphologies (FR I/II) are observed for each of the two lobes. While they potentially provide insights into the formation of lobe structure, particle acceleration, and the FR dichotomy, previous work on HyMoRS has mainly been limited to low-resolution studies, searches for new candidates, and milliarcsecond-scale VLBI observations of the core region. In this paper, we use new multi-array configuration Very Large Array (VLA) observations between 1 and 8 GHz to determine the morphology of HyMoRS on arcsecond scales and perform the first well-resolved spectral study of these unusual sources. We find that while the apparent FR I lobe is centre-brightened, this is the result of a compact acceleration region resembling a hotspot with a spectrum more consistent with an FR II (""strong-flavour"") jet. We find that the spectra of the apparent FR I lobes are not similar to their classical counterparts and are likely the result of line-of-sight mixing of plasma across a range of spectral ages. We consider possible mechanisms that could lead to the formation of HyMoRS under such conditions, including environment asymmetry and restarted sources, concluding through the use of simple modelling that HyMoRS are the result of orientation effects on intrinsically FR II sources with lobes non-parallel to the inner jet. ",Unveiling the cause of hybrid morphology radio sources (HyMoRS)
156,1188730869210595328,59595964,Nic Ross,"['After years of study, we finally understand magnesium in quasars better. Key result: magnesium *is* seen to vary, often, but definitely not always, in a similar way to Hbeta:\n\n<LINK>\n\nMassive h/t to David Homan for leading this important project! üëèüëçüî≠üåå']",https://arxiv.org/abs/1910.11364v1,"We investigate the responsiveness of the 2798AA MgII broad emission line in AGN on timescales of several years. Our study is based on a sample of extremely variable AGN as well as a broad population sample. The observed response of the line in previous studies has been mixed. By focussing on extreme variability ($|\Delta g|>$ 1) we find that MgII clearly does respond to the continuum. However, the degree of responsivity varies strikingly from one object to another; we see cases of MgII changing by as much as the continuum, more than the continuum, or very little at all. In 72% of the highly variable sample the behaviour of MgII corresponds with that of H$\beta$, with 26% of the objects showing large variations in both lines. We do not detect any change in the line width that would correspond to Broad Line Region `breathing', in accordance with results from literature. Some of the objects in our highly variable sample show a clear asymmetry in the MgII profile. This skewness can be both to the blue and the red of the line centre. Results from our broad population sample show that highly variable quasars have lower Eddington ratios. This result holds for the variability of the continuum, but the correlation is significantly reduced for the variability of the MgII line. For the first time, we present an overview of the value of the intrinsic Baldwin Effect for MgII in a large sample. ",] Behaviour of the MgII 2798AA Line Over the Full Range of AGN Variability
157,1187161460033458177,837133583558987776,Colin Raffel,"['New paper! We perform a systematic study of transfer learning for NLP using a unified text-to-text model, then push the limits to achieve SoTA on GLUE, SuperGLUE, CNN/DM, and SQuAD.\nPaper: <LINK>\nCode/models/data/etc: <LINK>\nSummary ‚¨áÔ∏è (1/14) <LINK>', 'Our approach casts *every* language problem as a text-to-text task. For example, English-to-German translation -- input: ""translate English to German: That is good."" target: ""Das ist gut."" or sentiment ID -- input: ""sentiment: This movie is terrible!"", target: ""negative"" (2/14)', 'The text-to-text approach allows us to use the same model, loss function, decoding process, training procedure, etc. across every task we study. It also provides a standard testbed for the many ideas we evaluate in our empirical survey. (3/14)', 'Transfer learning for NLP usually uses unlabeled data for pre-training, so we assembled the ""Colossal Clean Crawled Corpus"" (C4), ~750GB of cleaned text from Common Crawl. The code for generating C4 is already available in TensorFlow datasets: https://t.co/Vm35EUhuMG (4/14)', 'For most of the experiments in the paper, we use a basic encoder-decoder Transformer architecture. We found this worked well both on generative and classification tasks in the text-to-text framework. We call our model the ""Text-to-Text Transfer Transformer"" (T5). (5/14)', 'For our empirical survey, we first compared different architectural variants including encoder-decoder models and language models in various configurations and with various objectives. The encoder-decoder architecture performed best in our text-to-text setting. (6/14) https://t.co/RNNLGGT1zH', 'Then, we explored the space of different pre-training objectives. We found that BERT-style denoising objectives generally outperformed other approaches and that a SpanBERT-style (Joshi et al. 2019) objective had the best combination of performance and training speed. (7/14) https://t.co/sznm2tKKBe', 'Next, we compared various unlabeled datasets and found that in some cases in-domain pre-training data boosted performance on downstream tasks. Our diverse C4 dataset, however, is large enough that you can avoid repeating any examples, which we showed can be detrimental. (8/14) https://t.co/8Lop6BtHMh', 'Unsupervised pre-training is standard practice, but an alternative is to pre-train on a mixture of supervised and unsupervised data as in the MT-DNN (Liu et al. 2019). We found both approaches can achieve similar performance once you get the mixing proportions right. (9/14)', 'Scaling up is a powerful way to improve performance, but how should you scale? We compared training on more data, training a longer model, and ensembling given a specific computational budget. tl;dr: A bigger model is a necessity, but everything helps. (10/14)', 'Finally, we combine the insights from our study to train five models of varying sizes (up to 11 billion parameters) on 1 trillion tokens of data. We obtained state-of-the-art on GLUE, SuperGLUE, SQuAD, and CNN/Daily Mail, but not WMT translation. (11/14)', ""I'm particularly happy that we beat the SoTA on SuperGLUE by 4.3% and are within spitting distance of human performance (88.9 vs 89.8). SuperGLUE was designed to only include tasks that were easy for humans but hard for machines. (12/14)"", 'This work was a collaboration between an incredible team including Noam Shazeer, @ada_rob, @katherine1ee, @sharan0909, Michael Matena, @zhouyanqi30, @kongkonglli, and @peterjliu. (13/14)', 'All of our code, pre-trained models, and datasets are already online, see https://t.co/uaViIYyHR7 for more details. Please reach out if you have any questions or suggestions! (14/14)', '@hardmaru Nah, we are saving that for future work. ;)']",https://arxiv.org/abs/1910.10683,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. ","Exploring the Limits of Transfer Learning with a Unified Text-to-Text
  Transformer"
158,1187108136899039235,3351632566,Mandana,['Preprint of our recent work is now on arXiv. \nWe propose a Meta-Dataset for clinical tasks. This dataset can be used as a testbed for developing algorithms for genomics applications which can leverage a few-shot learning regime. \n<LINK>'],http://arxiv.org/abs/1910.08636,"Machine learning is bringing a paradigm shift to healthcare by changing the process of disease diagnosis and prognosis in clinics and hospitals. This development equips doctors and medical staff with tools to evaluate their hypotheses and hence make more precise decisions. Although most current research in the literature seeks to develop techniques and methods for predicting one particular clinical outcome, this approach is far from the reality of clinical decision making in which you have to consider several factors simultaneously. In addition, it is difficult to follow the recent progress concretely as there is a lack of consistency in benchmark datasets and task definitions in the field of Genomics. To address the aforementioned issues, we provide a clinical Meta-Dataset derived from the publicly available data hub called The Cancer Genome Atlas Program (TCGA) that contains 174 tasks. We believe those tasks could be good proxy tasks to develop methods which can work on a few samples of gene expression data. Also, learning to predict multiple clinical variables using gene-expression data is an important task due to the variety of phenotypes in clinical problems and lack of samples for some of the rare variables. The defined tasks cover a wide range of clinical problems including predicting tumor tissue site, white cell count, histological type, family history of cancer, gender, and many others which we explain later in the paper. Each task represents an independent dataset. We use regression and neural network baselines for all the tasks using only 150 samples and compare their performance. ",The TCGA Meta-Dataset Clinical Benchmark
159,1187101122206388224,4639078397,John Wise,"['New paper day(+1)! Aycin Aykutalp (LANL) led our study to see how a massive black hole seed affects its surroundings. We find that a metal-free star cluster forms around this newly formed black hole, which would be unique to the high-z universe! <LINK>', 'The X-rays from the BH partially ionizes the gas outside of the HII region, which then catalyzes H2 formation. The nearby dense clouds can then cool and collapse to form ~1000 Msun of Pop III stars within ~10 pc']",https://arxiv.org/abs/1910.08554,"The direct formation of a massive black hole is a potential seeding mechanism of the earliest observed supermassive black holes. We investigate how the existence of a massive black hole seed impacts the ionization and thermal state of its pre-galactic host halo and subsequent star formation. We show that its X-ray radiation ionizes and heats the medium, enhancing $\rm{H}_2$ formation in shielded regions, within the nuclear region in the span of a million years. The enhanced molecular cooling triggers the formation of a $\sim 10^4~{\rm M}_\odot$ metal-free stellar cluster at a star formation efficiency of $\sim 0.1\%$ in a single event. Star formation occurs near the edges of the H II region that is partially ionized by X-rays, thus the initial size depends on the black hole properties and surrounding environment. The simulated metal-free galaxy has an initial half-light radius of $\sim 10$ pc but expands to $\sim 50$ pc after 10 million years because of the outward velocities of their birth clouds. Supernova feedback then quenches any further star formation for tens of millions of years, allowing the massive black hole to dominate the spectrum once the massive metal-free stars die. ",Induced metal-free star formation around a massive black hole seed
160,1186899795853660160,1100049062429163523,Eliska Greplova,"['Our work with Agnes Valenti,Gregor Boschung,Frank Sch√§fer,Niels L√∂rch,@hubersebi is up <LINK> We analyze performance of predictive models to find topological order. I‚Äôll talk about it at #odcqs @univdeevora today. @ETH_en @ETH_physics @NCCR_QSIT @DPhysics_UniBas']",https://arxiv.org/abs/1910.10124,"Machine-learning driven models have proven to be powerful tools for the identification of phases of matter. In particular, unsupervised methods hold the promise to help discover new phases of matter without the need for any prior theoretical knowledge. While for phases characterized by a broken symmetry, the use of unsupervised methods has proven to be successful, topological phases without a local order parameter seem to be much harder to identify without supervision. Here, we use an unsupervised approach to identify topological phases and transitions out of them. We train artificial neural nets to relate configurational data or measurement outcomes to quantities like temperature or tuning parameters in the Hamiltonian. The accuracy of these predictive models can then serve as an indicator for phase transitions. We successfully illustrate this approach on both the classical Ising gauge theory as well as on the quantum ground state of a generalized toric code. ",Unsupervised identification of topological order using predictive models
161,1186852474679783425,869862586610851840,Jeannette Bohg,"['Understanding dynamic 3D environment is crucial for robotic agents. \n\nWe propose MeteorNet for learning representations of dynamic 3D point cloud sequences (Oral @ICCV19).\n\nProject Page: <LINK>\n\nArxiv: <LINK> <LINK>', 'We achieve SOTA on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation.\n\nKudos to Xingyu Liu @xing_yu_liu and Mengyuan Yan @StanfordIPRL']",https://arxiv.org/abs/1910.09165,"Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called $MeteorNet$ for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences. ",MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences
162,1186815173396500485,1055320747,Shahab Aslani,['Preprint of our recent work on arXiv. \nWe propose a regularization network including an auxiliary loss function that is designed to encourage the baseline models to ignore domain-specific information.\nThanks to @ghamarneh and other co-authors.\nLink: <LINK>'],http://arxiv.org/abs/1910.10035,"This paper presents a simple and effective generalization method for magnetic resonance imaging (MRI) segmentation when data is collected from multiple MRI scanning sites and as a consequence is affected by (site-)domain shifts. We propose to integrate a traditional encoder-decoder network with a regularization network. This added network includes an auxiliary loss term which is responsible for the reduction of the domain shift problem and for the resulting improved generalization. The proposed method was evaluated on multiple sclerosis lesion segmentation from MRI data. We tested the proposed model on an in-house clinical dataset including 117 patients from 56 different scanning sites. In the experiments, our method showed better generalization performance than other baseline networks. ",Scanner Invariant Multiple Sclerosis Lesion Segmentation from MRI
163,1186752805211693057,325846699,Thaddeus Komacek,"['Is weather in the atmospheres of hot exoplanets observationally detectable? In <LINK>, Adam Showman and I determine a baseline for how time-variable hot Jupiter atmospheres should be. We find that JWST should be able to detect time-variability in emitted flux.', 'The figure below shows an emitted flux map and the change in emitted flux over 150 and 75 days. We find that the variability in flux, temperature, and wind speed is concentrated in equatorial regions. The local variability in emitted flux can be as large as ~10%! https://t.co/iLYCYqsltp', 'The global variability in the secondary eclipse depth can be as large as 2%. This is smaller than previous upper limits with Spitzer, but should be detectable with JWST. We also find that the phase curve offset and amplitude can shift significantly.', 'Lastly, we find that the wind speeds at the terminator can be strongly time-variable. In some cases, the wind speed at the western limb can reverse from eastward to westward! This variability might be detectable with many high-resolution spectroscopic transit observations. https://t.co/F2EoaG2zuQ', 'Our simulations did not include many other effects that can lead to time-variability, including vertical shear instabilities, hydrogen dissociation, magnetic effects, and clouds. We find that these effects are needed to explain previous observations of time-variability.']",https://arxiv.org/abs/1910.09523,"Hot Jupiters receive intense incident stellar light on their daysides, which drives vigorous atmospheric circulation that attempts to erase their large dayside-to-nightside flux contrasts. Propagating waves and instabilities in hot Jupiter atmospheres can cause emergent properties of the atmosphere to be time-variable. In this work, we study such weather in hot Jupiter atmospheres using idealized cloud-free general circulation models with double-grey radiative transfer. We find that hot Jupiter atmospheres can be time-variable at the $\sim 0.1-1\%$ level in globally averaged temperature and at the $\sim 1-10\%$ level in globally averaged wind speeds. As a result, we find that observable quantities are also time variable: the secondary eclipse depth can be variable at the $\lesssim 2\%$ level, the phase curve amplitude can change by $\lesssim 1\%$, the phase curve offset can shift by $\lesssim 5^{\circ}$, and terminator-averaged wind speeds can vary by $\lesssim 2~ \mathrm{km}~\mathrm{s}^{-1}$. Additionally, we calculate how the eastern and western limb-averaged wind speeds vary with incident stellar flux and the strength of an imposed drag that parameterizes Lorentz forces in partially ionized atmospheres. We find that the eastern limb is blueshifted in models over a wide range of equilibrium temperature and drag strength, while the western limb is only redshifted if equilibrium temperatures are $\lesssim1500~\mathrm{K}$ and drag is weak. Lastly, we show that temporal variability may be observationally detectable in the infrared through secondary eclipse observations with JWST, phase curve observations with future space telescopes (e.g., ARIEL), and/or Doppler wind speed measurements with high-resolution spectrographs. ",Temporal Variability in Hot Jupiter Atmospheres
164,1186568420495708161,228380217,James Grist,"['Interested in childhood brain #tumours? We‚Äôve put together a new study looking at telling the difference between three common cancer types, using #AI. Give us your comments, as we‚Äôd love to improve the paper! \n<LINK>']",https://arxiv.org/abs/1910.09247,"The imaging and subsequent accurate diagnosis of paediatric brain tumours presents a radiological challenge, with magnetic resonance imaging playing a key role in providing tumour specific imaging information. Diffusion weighted and perfusion imaging are commonly used to aid the non invasive diagnosis of paediatric brain tumours, but are usually evaluated by expert qualitative review. Quantitative studies are mainly single centre and single modality. The aim of this work was to combine multi centre diffusion and perfusion imaging, with machine learning, to develop machine learning based classifiers to discriminate between three common paediatric tumour types. The results show that diffusion and perfusion weighted imaging of both the tumour and whole brain provide significant features which differ between tumour types, and that combining these features gives the optimal machine learning classifier with greater than 80 percent predictive precision. This work represents a step forward to aid in the non invasive diagnosis of paediatric brain tumours, using advanced clinical imaging. ","Distinguishing between paediatric brain tumour types using
  multi-parametric magnetic resonance imaging and machine learning: a
  multi-site study"
165,1186464812508667904,3458090593,Wenhao Yang,['Our new paper on Federated Learning(<LINK>) is available on arXiv. We propose an algorithm(PDSGD) to reduce the communication cost in a decentralized framework. We theoretically prove the convergence rate and empirically show the performance.'],http://arxiv.org/abs/1910.09126,"Recently, the technique of local updates is a powerful tool in centralized settings to improve communication efficiency via periodical communication. For decentralized settings, it is still unclear how to efficiently combine local updates and decentralized communication. In this work, we propose an algorithm named as LD-SGD, which incorporates arbitrary update schemes that alternate between multiple Local updates and multiple Decentralized SGDs, and provide an analytical framework for LD-SGD. Under the framework, we present a sufficient condition to guarantee the convergence. We show that LD-SGD converges to a critical point for a wide range of update schemes when the objective is non-convex and the training data are non-identically independent distributed. Moreover, our framework brings many insights into the design of update schemes for decentralized optimization. As examples, we specify two update schemes and show how they help improve communication efficiency. Specifically, the first scheme alternates the number of local and global update steps. From our analysis, the ratio of the number of local updates to that of decentralized SGD trades off communication and computation. The second scheme is to periodically shrink the length of local updates. We show that the decaying strategy helps improve communication efficiency both theoretically and empirically. ",Communication-Efficient Local Decentralized SGD Methods
166,1186462785317654529,19089454,Dr. Teddy Kareta,"['In other news, our new paper on the active Centaur 174P/Echeclus has been accepted for publication in the Astronomical Journal! We detail observations of its huge December 2017 outburst and find some strange stuff.\n<LINK> <LINK>', ""We combined near-infrared spectra with visible imaging to infer a debris-ejection/mini-fragmentation event smaller than Echeclus's massive 2005 outburst but with many of the same properties. If Echeclus is typical of the active centaurs, other centaurs should be doing this too."", ""(Thankfully, they totally are -- after we submitted the paper, Astronomer's Telegram 13179 was submitted finding something similar-ish at 29P/SW1. Phew.) https://t.co/t7hcVkFln4"", ""We also did some fun (and new for me) dynamical work to show that Echeclus's orbit hasn't changed nearly as much as many other active Centaurs, so the origin of Echeclus's modern strong activity is, for now, unclear. https://t.co/ijsJgxDjli"", 'The co-authors include the (by now) normal cast of characters of : @benjaminsharkey @J_Noons @kat_volk @moonyguy @WaltHarris2 and Dr. Richard Miles.']",https://arxiv.org/abs/1910.09490,"The Centaurs are the small solar system bodies intermediate between the active inner solar system Jupiter Family Comets and their inactive progenitors in the trans-Neptunian region. Among the fraction of Centaurs which show comet-like activity, 174P/Echeclus is best known for its massive 2005 outburst in which a large apparently active fragment was ejected above the escape velocity from the primary nucleus. We present visible imaging and near-infrared spectroscopy of Echeclus during the first week after its December 2017 outburst taken at the Faulkes North & South Telescopes and the NASA IRTF, the largest outburst since 2005. The coma was seen to be highly asymmetric. A secondary peak was seen in the near-infrared 2D spectra, which is strongly hinted at in the visible images, moving hyperbolically with respect to the nucleus. The retrieved reflectance spectrum of Echelcus is consistent with the unobscured nucleus but becomes bluer when a wider extraction aperture is used. We find that Echeclus's coma is best explained as dominated by large blue dust grains, which agrees with previous work. We also conducted a high-resolution orbital integration of Echeclus's recent evolution and found no large orbital changes that could drive its modern evolution. We interpret the second peak in the visible and near-infrared datasets as a large cloud of larger-than-dust debris ejected at the time of outburst. If Echeclus is typical of the Centaurs, there may be several debris ejection or fragmentation events per year on other Centaurs that are going unnoticed. ","Physical Characterization of the December 2017 Outburst of the Centaur
  174P/Echeclus"
167,1185013095686643714,301341538,Renjie Liao,['Predicting the future behaviour of pedestrians with uncertainty is important for self-driving. We propose a Discrete Residual Flow Network which leverages map information and captures the multi-modality. Check our new #CoRL2019 paper: <LINK> <LINK>'],https://arxiv.org/abs/1910.08041,"Self-driving vehicles plan around both static and dynamic objects, applying predictive models of behavior to estimate future locations of the objects in the environment. However, future behavior is inherently uncertain, and models of motion that produce deterministic outputs are limited to short timescales. Particularly difficult is the prediction of human behavior. In this work, we propose the discrete residual flow network (DRF-Net), a convolutional neural network for human motion prediction that captures the uncertainty inherent in long-range motion forecasting. In particular, our learned network effectively captures multimodal posteriors over future human motion by predicting and updating a discretized distribution over spatial locations. We compare our model against several strong competitors and show that our model outperforms all baselines. ",Discrete Residual Flow for Probabilistic Pedestrian Behavior Prediction
168,1184470340866658305,225895485,Andreas Madsen,"['Extrapolation on math is hard for neural networks. We (@AlexRoseJo and I) propose new benchmark criteria and find that NALU is fragile. So how should we build networks that can learn arithmetics? More data does not solve the problem! <LINK> /cc @DeepMind <LINK>', '@iamtrask @AlexRoseJo @Deepmind @FelixHill84 @scott_e_reed @redpony Thanks, that means a lot :) Yes, I think a key insight here is that MSE and cross-entropy are not always great metrics when the problems are of a logical nature.', '@AlexRoseJo @Deepmind @iamtrask @FelixHill84 @scott_e_reed @redpony I would love if this reached a wide audience, such that we can improve in this field, so please consider retweeting.']",https://arxiv.org/abs/1910.01888,"The Neural Arithmetic Logic Unit (NALU) is a neural network layer that can learn exact arithmetic operations between the elements of a hidden state. The goal of NALU is to learn perfect extrapolation, which requires learning the exact underlying logic of an unknown arithmetic problem. Evaluating the performance of the NALU is non-trivial as one arithmetic problem might have many solutions. As a consequence, single-instance MSE has been used to evaluate and compare performance between models. However, it can be hard to interpret what magnitude of MSE represents a correct solution and models sensitivity to initialization. We propose using a success-criterion to measure if and when a model converges. Using a success-criterion we can summarize success-rate over many initialization seeds and calculate confidence intervals. We contribute a generalized version of the previous arithmetic benchmark to measure models sensitivity under different conditions. This is, to our knowledge, the first extensive evaluation with respect to convergence of the NALU and its sub-units. Using a success-criterion to summarize 4800 experiments we find that consistently learning arithmetic extrapolation is challenging, in particular for multiplication. ",Measuring Arithmetic Extrapolation Performance
169,1184379591047163904,89709541,Anke Arentsen,"['I am proud to present: the first Pristine Inner Galaxy Survey (PIGS) paper! <LINK> üéâ In it, we study the kinematics of metal-poor stars in the Galactic bulge region. A summary follows: [1/7] <LINK>', 'We used the anti-metal detector powers of CaHK photometry obtained at @CFHTelescope to find rare metal-poor stars in the metal-rich needle stack that is the Galactic bulge. [2/7]', ""The kinematics of the metal-poor ([Fe/H] &lt; -1) component of the bulge hasn't really been studied in detail yet. We use a sample of thousands of stars observed with AAT/AAOmega+2dF to look at how this old component behaves! [3/7]"", 'We find that the rotation of stars in the inner Galaxy decreases with decreasing metallicity, until it completely disappears for the most metal-poor stars ([Fe/H] &lt; -2), see this figure (the line is a bar model): [4/7] https://t.co/ucWKllIU0F', 'We also found a strikingly strong and continuous relation between the velocity dispersion (how hot the population is) and the metallicity: [5/7] https://t.co/0jN7F4NANR', 'We propose a few interpretations: a density transition of components of different kinematics &amp; metallicities, a different mapping of stars onto the boxy/peanut bulge and/or the influence of the bar on a pressure-supported component. Probably all of them play a role! [6/7]', 'So this is clearly not the last PIGS paper, since a lot is still unknown about this exciting population of stars..! :) [7/7] \n\n(with Twitter people @nfmartin1980, @Nico_Longeard, @kmalhan07, @Astro_Sestitof, @Thomas_gft, @Cosmic_Horizons and @FadAstra)', '@nfmartin1980 @CFHTelescope I guess I shouldn‚Äôt have put that - there... but yes, this paper is secretly about how we detected anti-matter in metal-poor stars! Should have submitted to Nature...', '@arm2armtweet Do you recognise the giraffe plot?!']",http://arxiv.org/abs/1910.06337,"Our Galaxy is known to contain a central boxy/peanut-shaped bulge, yet the importance of a classical, pressure-supported component within the central part of the Milky Way is still being debated. It should be most visible at low metallicity, a regime that has not yet been studied in detail. Using metallicity-sensitive narrow-band photometry, the Pristine Inner Galaxy Survey (PIGS) has collected a large sample of metal-poor ([Fe/H] < -1.0) stars in the inner Galaxy to address this open question. We use PIGS to trace the metal-poor inner Galaxy kinematics as function of metallicity for the first time. We find that the rotational signal decreases with decreasing [Fe/H], until it becomes negligible for the most metal-poor stars. Additionally, the velocity dispersion increases with decreasing metallicity for -3.0 < [Fe/H] < -0.5, with a gradient of -44 $\pm$ 4 km$\,$s$^{-1}\,$dex$^{-1}$. These observations may signal a transition between Galactic components of different metallicities and kinematics, a different mapping onto the boxy/peanut-shaped bulge for former disk stars of different metallicities and/or the secular dynamical and gravitational influence of the bar on the pressure-supported component. Our results provide strong constraints on models that attempt to explain the properties of the inner Galaxy. ","The Pristine Inner Galaxy Survey (PIGS) I: Tracing the kinematics of
  metal-poor stars in the Galactic bulge"
170,1183962172860358658,729364194840203265,Toan Nguyen,"['Our paper <LINK> on how to train good Transformer for low-resource NMT. We also propose ScaleNorm which uses l2-normalization with a single scale parameter, no centering, no per-unit scaling or shifting as in LayerNormalization.', 'ScaleNorm is similar to RMSNorm (https://t.co/KcNVI5Docw), but it uses only a single scale parameter and also no bias.', 'code: https://t.co/gCXOAaeqMh', 'work was collaboration with @JulianSlzr . Thanks for a wonderful internship. I owe you a macallan.', '@JulianSlzr we also want to thank Katrin Kirchhoff and @davidweichiang for their kind support of this work.', '@JulianSlzr I think we have to submit it :))']",https://arxiv.org/abs/1910.05895,"We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose $\ell_2$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance. ","Transformers without Tears: Improving the Normalization of
  Self-Attention"
171,1183952735705554944,731516236962496512,Ching-Yao Chuang,"['Ever wonder why domain-invariant representations work even without common support? In our new paper, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. <LINK> <LINK>', 'The error of DANN on simple benchmark MNIST-&gt;MNIST-M increases by 19.8% when 4 more CNN layers are added to the encoder!\n\nJoint work with Antonio Torralba and Stefanie Jegelka.']",https://arxiv.org/abs/1910.05804,"Unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain. One popular approach to this problem is to learn domain-invariant embeddings for both domains. In this work, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. In particular, this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Next, we specify our theoretical framework to multilayer neural networks. As a result, we develop a strategy that mitigates sensitivity to the embedding complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff. ",The Role of Embedding Complexity in Domain-invariant Representations
172,1182592355091779584,261865146,Dr Sofia Qvarfort,"['New article out on the #arXiv today! We study the optimal estimation bounds on parameters that can be encoded into a nonlinear quantum optomechanical Hamiltonian: <LINK> The results include time-dependent forces and properties of the system itself.\n@UCLQuantum <LINK>', ""@jimmy_jammy @UCLQuantum Thank you! I spent lots of time on the figures haha so I appreciate that a lot! If you happen to read it and have any questions/feedback for us, I'd be really interested in hearing about it (always looking to improve exposition and explanations). üòä""]",https://arxiv.org/abs/1910.04485,"We study the fundamental bounds on precision measurements of parameters contained in a time-dependent nonlinear optomechanical Hamiltonian, which includes the nonlinear light-matter coupling, a mechanical displacement term, and a single-mode mechanical squeezing term. By using a recently developed method to solve the dynamics of this system, we derive a general expression for the quantum Fisher information and demonstrate its applicability through three concrete examples: estimation of the strength of a nonlinear light-matter coupling, the strength of a time-modulated mechanical displacement, and a single-mode mechanical squeezing parameter, all of which are modulated at resonance. Our results can be used to compute the sensitivity of a nonlinear optomechanical system to a number of external and internal effects, such as forces acting on the system or modulations of the light--matter coupling. ","Optimal estimation with quantum optomechanical systems in the nonlinear
  regime"
173,1182307359663775745,2427184074,Christopher Berry,"['New paper led by @NUCIERA grad student @spacedontwait modelling the binary neutron star progenitors of short gamma-ray bursts GRB 070809 &amp; GRB 090515. We find evidence for neutron stars receiving large natal kicks of &gt; 200 km/s!\nüí•<LINK> <LINK>', '@NUCIERA @spacedontwait @DilithiumMatrix @matsoulina Following detection of GW170817 and GRB 170817A we know that short gamma-ray bursts can be generated by binary neutron star collisions. We have lots of of gamma-ray burst observations, so what can we learn about binary neutron stars from them?', 'GRB 070809 and GRB 090515 are interesting short gamma-ray bursts as they are a long way from their host galaxies. Neutron stars form from dying stars so they need to come from somewhere with stars. They can become offset thanks to the kicks they get during their natal supernova', ""We don't know how big a kick neutron stars get during supernova. This has a big impact on the number of merging neutron stars. We model the evolution of a binary neutron star population including the variation of kicks and other supernova properties to reproduce GRB observations"", '@mcnees @NUCIERA @spacedontwait @DilithiumMatrix @matsoulina Thank you Grave Robbert', 'How do you get a binary neutron star merging far out from a galaxy? It could start far out, but not many stars are there. It could get a small kick a long time ago or a big kick recently. We model the stellar population across the history of the galaxy to figure out the odds https://t.co/ID6E3A1Xua', '@spacedontwait @DilithiumMatrix By modelling of the galaxies of GRB 070809 and GRB 090515 we infer probable supernova natal kicks. This plots show results from five different stellar mass/halo mass relations used our galaxy modelling. GRB 090515 needs kicks above ~ 200 km/s\n\nüìähttps://t.co/87MV9HIhc2 https://t.co/QkFdM4u4mM', ""@profjsb @spacedontwait @DilithiumMatrix Thank you! I see we're not currently citing that paper, so the pointer is appreciated!""]",https://arxiv.org/abs/1910.03598,"We present a detailed analysis of two well-localized, highly offset short gamma-ray bursts---GRB~070809 and GRB~090515---investigating the kinematic evolution of their progenitors from compact object formation until merger. Calibrating to observations of their most probable host galaxies, we construct semi-analytic galactic models that account for star formation history and galaxy growth over time. We pair detailed kinematic evolution with compact binary population modeling to infer viable post-supernova velocities and inspiral times. By populating binary tracers according to the star formation history of the host and kinematically evolving their post-supernova trajectories through the time-dependent galactic potential, we find that systems matching the observed offsets of the bursts require post-supernova systemic velocities of hundreds of kilometers per second. Marginalizing over uncertainties in the stellar mass--halo mass relation, we find that the second-born neutron star in the GRB~070809 and GRB~090515 progenitor systems received a natal kick of $\gtrsim 200~\mathrm{km\,s}^{-1}$ at the 78\% and 91\% credible levels, respectively. Applying our analysis to the full catalog of localized short gamma-ray bursts will provide unique constraints on their progenitors and help unravel the selection effects inherent to observing transients that are highly offset with respect to their hosts. ","Forward Modeling of Double Neutron Stars: Insights from Highly-Offset
  Short Gamma-Ray Bursts"
174,1182276264104775680,865433623,Cris Mantilla,"['Constraining the Higgs width with #boosted #Higgs bosons @ #LHC! <LINK>. With P.Harris and D.Rankin we propose a catch-all method sensitive to all SM Higgs decays:', 'We used some cool (although rough) mass reconstruction for the Higgs  - that also aims to include final states with neutrinos! https://t.co/xxU7Z92KMh', ""And some #MachineLearning techniques to improve the ID of boosted Higgs bosons (that's the GRU line in this plot) https://t.co/BvZr3kBI3S"", 'And then we fit the mass distribution to get the ""inclusive"" cross section (ok the word ""inclusive"" has caveats) https://t.co/Cs7Qtr0FNe', 'and we translate the uncertainty of this measurement to an uncertainty on the width (&lt; 1.4 MeV)! @ HL-LHC of course...', 'OK this study is MODEL DEPENDENT and has some caveats - which we extensively discuss - BUT is also the FIRST ATTEMPT to identify challenging final states of the #Higgs!', ""Including H=&gt;gluons (the Higgs is a color singlet and that might be the only handle we can exploit)! (N.B. there's better handles for this like the jet pull or jet isolation) https://t.co/iL6xDSK7CD"", 'So take a look and let us know comments! You can also check out the talk at BOOST~ https://t.co/8JWLbpEvkm']",https://arxiv.org/abs/1910.02082,"Despite the discovery of the Higgs boson decay in five separate channels many parameters of the Higgs boson remain largely unconstrained. In this paper, we present a new approach to constraining the Higgs total width by requiring the Higgs to be resolved as a single high p$_T$ jet and measuring the inclusive Higgs boson cross section. To measure the inclusive Higgs boson cross section, we rely on new approaches from machine learning and a modified jet reconstruction. This approach is found to be complementary to the existing off-shell width measurement and, with the full HL-LHC luminosity, is capable of yielding similar sensitivity to the off-shell projections. We outline the theoretical and experimental limitations and present a path towards making this approach a truly model-independent measurement of the Higgs boson total width. ",An approach to constraining the Higgs width at the LHC and HL-LHC
175,1182136919616098306,627090309,Jo Bovy,"[""Check out @ted_mackereth and my paper on the Milky Way's stellar halo with @APOGEEsurvey and @ESAGaia: We find a 1.3(3)e9 Msun halo dominated by accretion, but not by a single (Gaia-Enceladus/Sausage) merger; just 1% of the disk was kicked up into the halo\n<LINK> <LINK>""]",https://arxiv.org/abs/1910.03590,"The stellar mass in the halo of the Milky Way is notoriously difficult to determine, owing to the paucity of its stars in the solar neighbourhood. With tentative evidence from $\it{Gaia}$ that the nearby stellar halo is dominated by a massive accretion event -- referred to as $\it{Gaia-Enceladus}$ or Sausage -- these constraints are now increasingly urgent. We measure the mass in kinematically selected mono-abundance populations (MAPs) of the stellar halo between $-3 <$ [Fe/H] $< -1$ and $0.0 <$ [Mg/Fe] $< 0.4$ using red giant star counts from APOGEE DR14. We find that MAPs are well fit by single power laws on triaxial ellipsoidal surfaces, and we show that that the power law slope $\alpha$ changes such that high $\mathrm{[Mg/Fe]}$ populations have $\alpha \sim 4$, whereas low [Mg/Fe] MAPs are more extended with shallow slopes, $\alpha \sim 2$. We estimate the total stellar mass to be $M_{*,\mathrm{tot}} = 1.3^{+0.3}_{-0.2}\times10^{9}\ \mathrm{M_{\odot}}$, of which we estimate $\sim 0.9^{+0.2}_{-0.1} \times 10^{9} \mathrm{M_{\odot}}$ to be accreted. We estimate that the mass of accreted stars with $e > 0.7$ is $M_{*,\mathrm{accreted},e>0.7} = 3 \pm 1\ \mathrm{(stat.)}\pm 1\ \mathrm{(syst.)}\times10^{8}\ \mathrm{M_\odot}$, or $\sim 30-50\%$ of the accreted halo mass. If the majority of these stars $\it{are}$ the progeny of a massive accreted dwarf, this places an upper limit on its stellar mass, and implies a halo mass for the progenitor of $\sim 10^{10.2\pm0.2}\ \mathrm{M_\odot}$. This constraint not only shows that the $\it{Gaia-Enceladus}$/Sausage progenitor may not be as massive as originally suggested, but that the majority of the Milky Way stellar halo was accreted. These measurements are an important step towards fully reconstructing the assembly history of the Milky Way. ","Weighing the stellar constituents of the Galactic halo with APOGEE red
  giant stars"
176,1181819225947676672,2466345433,Lutz Bornmann,['Indicators based on cited references have been proposed to measure the novelty of papers. We tested their convergent validity: do these indicators measure what they propose to measure? <LINK>'],https://arxiv.org/abs/1910.03233,"Lee, Walsh, and Wang (2015) - based on Uzzi, Mukherjee, Stringer, and Jones (2013) - and Wang, Veugelers, and Stephan (2017) proposed scores based on cited references (cited journals) data which can be used to measure the novelty of papers (named as novelty scores U and W in this study). Although previous research has used novelty scores in various empirical analyses, no study has been published up to now - to the best of our knowledge - which quantitatively tested the convergent validity of novelty scores: do these scores measure what they propose to measure? Using novelty assessments by faculty members (FMs) at F1000Prime for comparison, we tested the convergent validity of the two novelty scores (U and W). FMs' assessments not only refer to the quality of biomedical papers, but also to their characteristics (by assigning certain tags to the papers): for example, are the presented findings or formulated hypotheses novel (tags ""new findings"" and ""hypothesis"")? We used these and other tags to investigate the convergent validity of both novelty scores. Our study reveals different results for the novelty scores: the results for novelty score U are mostly in agreement with previously formulated expectations. We found, for instance, that for a standard deviation (one unit) increase in novelty score U, the expected number of assignments of the ""new finding"" tag increase by 7.47%. The results for novelty score W, however, do not reflect convergent validity with the FMs' assessments: only the results for some tags are in agreement with the expectations. Thus, we propose - based on our results - the use of novelty score U for measuring novelty quantitatively, but question the use of novelty score W. ","Do we measure novelty when we analyze unusual combinations of cited
  references? A validation study of bibliometric novelty indicators based on
  F1000Prime data"
177,1181547494913843201,322636963,Jonathan Berant,"[""CoNLL paper w/ student @ko_omri, @GabiStanovsky @viveksrikumar, Yichu Zhou. We examine learning an active learning policy for QA-SRL and find that an *oracle* policy selecting examples that maximize perf. on test set isn't much better than random sampling! <LINK>""]",https://arxiv.org/abs/1910.02228,"One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However, training such models requires expensive annotation of complex structures, which hinders their adoption. Learning to actively-learn (LTAL) is a recent paradigm for reducing the amount of labeled data by learning a policy that selects which samples should be labeled. In this work, we examine LTAL for learning semantic representations, such as QA-SRL. We show that even an oracle policy that is allowed to pick examples that maximize performance on the test set (and constitutes an upper bound on the potential of LTAL), does not substantially improve performance compared to a random policy. We investigate factors that could explain this finding and show that a distinguishing characteristic of successful applications of LTAL is the interaction between optimization and the oracle policy selection process. In successful applications of LTAL, the examples selected by the oracle policy do not substantially depend on the optimization procedure, while in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. ",On the Limits of Learning to Actively Learn Semantic Representations
178,1181452388118474752,734484164070772736,Guy Tennenholtz,"['In a recent work on ""Natural Language State Representation for Reinforcement Learning"" we experiment with various semantic state representations on the performance and robustness of RL agents. We find NLP representations to be a favorable candidate. Link: <LINK>']",http://arxiv.org/abs/1910.02789,"Recent advances in reinforcement learning have shown its potential to tackle complex real-life tasks. However, as the dimensionality of the task increases, reinforcement learning methods tend to struggle. To overcome this, we explore methods for representing the semantic information embedded in the state. While previous methods focused on information in its raw form (e.g., raw visual input), we propose to represent the state using natural language. Language can represent complex scenarios and concepts, making it a favorable candidate for representation. Empirical evidence, within the domain of ViZDoom, suggests that natural language based agents are more robust, converge faster and perform better than vision based agents, showing the benefit of using natural language representations for reinforcement learning. ","Language is Power: Representing States Using Natural Language in
  Reinforcement Learning"
179,1181292564869242880,50020689,Thomas Rademaker,"['Our paper with @kseniakolosova, Ben Dringoli, Juliann Wray and Michael Hilke on content sequencing in freshman E&amp;M classes is out <LINK>. We find that the order of presenting content types (theory, concept, example) has a large effect on student learning.', '@kseniakolosova Showing concept first is best for solving E&amp;M problems. Example first results in ""cheap learning"", i.e. being able to reproduce solutions, but not come up with your own, and theory first sucks (only works in advanced courses when the concepts are considered prerequisites).', '@kseniakolosova We proposed the Content Cube as a model of learning from sequences of contents. For optimal learning students will have to reach (=being exposed to) the concept plane first, before they can use example and theory based content to further their understanding https://t.co/ShMUru4bCn']",https://arxiv.org/abs/1910.00145,"We investigate the impact of content sequencing on student learning outcomes in a first-year university electromagnetism course. Using a custom-built online system, the McGill Learning Platform (McLEAP), we test student problem-solving performance as a function of the sequence in which the students are presented aspects of new material. New material was divided into the three categories of conceptual, theoretical and example-based content. Here, we present findings from a two-year study with over 1000 students participating. We find that content sequencing has a significant impact on learning outcomes in our study: students presented with conceptual content first perform significantly better on our assessment than those presented with theoretical content. To explain these results, we propose the Content Cube as an extension to the the mental model frameworks. Additionally, we find that instructors' preferences for content sequencing differ significantly from that of students. We discuss how this information can be used to improve course instruction and student learning, and motivate future work building upon our presented results to study the impact of additional factors on student performance. ","Content Sequencing and its Impact on Student Learning in
  Electromagnetism: Theory and Experiment"
180,1181209622931070977,14138114,Casey Law üí•,"['We found a way to do two different kinds of science with the same VLA observation.\n‚úÖ Test model for magnetar-powered supernovae\n‚úÖ Search for FRBs from young superluminous supernovae.\n<LINK>\n\nDespite the title ""A Search for..."", we did find something!']",https://arxiv.org/abs/1910.02036,"We present results of a search for late-time radio emission and Fast Radio Bursts (FRBs) from a sample of type-I superluminous supernovae (SLSNe-I). We used the Karl G. Jansky Very Large Array to observe ten SLSN-I more than 5 years old at a frequency of 3 GHz. We searched fast-sampled visibilities for FRBs and used the same data to perform a deep imaging search for late-time radio emission expected in models of magnetar-powered supernovae. No FRBs were found. One SLSN-I, PTF10hgi, is detected in deep imaging, corresponding to a luminosity of $1.2\times10^{28}$ erg s$^{-1}$. This luminosity, considered with the recent 6 GHz detection of PTF10hgi in Eftekhari et al (2019), supports the interpretation that it is powered by a young, fast-spinning ($\sim$ ms spin period) magnetar with $\sim$ 15 Msun of partially ionized ejecta. Broadly, our observations are most consistent with SLSNe-I being powered by neutron stars with fast spin periods, although most require more free-free absorption than is inferred for PTF10hgi. We predict that radio observations at higher frequencies or in the near future will detect these systems and begin constraining properties of the young pulsars and their birth environments. ","A Search for Late-Time Radio Emission and Fast Radio Bursts from
  Superluminous Supernovae"
181,1180128605344546817,893669858151284736,Mike Grudiƒá,"['<LINK> paper day! We studied the properties of giant molecular clouds in the FIRE simulations, to get some idea of how their masses, sizes, and other properties evolved over cosmic time. The results were a surprise! 1/n', ""When everybody thinks about the ISM and GMCs at high redshift, they think about the extreme, very high-pressure conditions that people observe in lensed galaxies. We found that in the galaxies we studied, we found that they really didn't evolve that much at all! 2/n"", 'The trick here is that the high-redshift ISM observations we have are probably not Milky Way progenitors - they were already nearly Milky-Way mass billions of years ago, so of course these had to have some extreme star formation activity. 3/n', ""When we look at something that actually does form a Milky Way, and something that forms an LMC, we find that the cloud properties can have wide fluctuations (we're going to explore why in future work), but on average, they're pretty steady over 13 billion years."", 'I think this paper ultimately opens more questions than it answers. Why do two completely different galaxies have a favorite GMC mass and surface density that says constant over time??? There‚Äôs gotta be some juicy physics there', '@brewstronomy At a given point in time the variations seem driven by gradients in the bulk galactic ISM properties. Those can change in turn, so e.g. in a major merger you can get really gas rich and make a big heckin chonker of a GMC. More on that in the next paper!']",https://arxiv.org/abs/1910.01163v1,"Giant molecular clouds (GMCs) are well-studied in the local Universe, however, exactly how their properties vary during galaxy evolution is poorly understood due to challenging resolution requirements, both observational and computational. We present the first time-dependent analysis of giant molecular clouds in a Milky Way-like galaxy and an LMC-like dwarf galaxy of the FIRE-2 (Feedback In Realistic Environments) simulation suite, which have sufficient resolution to predict the bulk properties of GMCs in cosmological galaxy formation self-consistently. We show explicitly that the majority of star formation outside the galactic center occurs within self-gravitating gas structures that have properties consistent with observed bound GMCs. We find that the typical cloud bulk properties such as mass and surface density do not vary more than a factor of 2 in any systematic way after the first Gyr of cosmic evolution within a given galaxy from its progenitor. While the median properties are constant, the tails of the distributions can briefly undergo drastic changes, which can produce very massive and dense self-gravitating gas clouds. Once the galaxy forms, we identify only two systematic trends in bulk properties over cosmic time: a steady increase in metallicity produced by previous stellar populations and a weak decrease in bulk cloud temperatures. With the exception of metallicity we find no significant differences in cloud properties between the Milky Way-like and dwarf galaxies. These results have important implications for cosmological star and star cluster formation and put especially strong constraints on theories relating the stellar initial mass function to cloud properties. ",] Evolution of giant molecular clouds across cosmic time
182,1180037680752713729,113350360,Gabriele Franch,"['Ever tried to find similar sequences in a huge database of temporal images? Check out our paper: <LINK>\nWe find sequences of ANY length in hundred thousands of images in &lt;5 seconds! Thanks to @lucoviello, @Giuseppe_jurman, @mpbalab and our friends @Meteotrentino', '@lucoviello @Giuseppe_Jurman @mpbalab @Meteotrentino And a huge thanks for this work goes to the ever-supportive @furlanello and to @Microsoft_Green for the #AIforEarth initiative for environmental innovation!']",https://arxiv.org/abs/1910.01211,"The use of analogs - similar weather patterns - for weather forecasting and analysis is an established method in meteorology. The most challenging aspect of using this approach in the context of operational radar applications is to be able to perform a fast and accurate search for similar spatiotemporal precipitation patterns in a large archive of historical records. In this context, sequential pairwise search is too slow and computationally expensive. Here we propose an architecture to significantly speed-up spatiotemporal analog retrieval by combining nonlinear geometric dimensionality reduction (UMAP) with the fastest known Euclidean search algorithm for time series (MASS) to find radar analogs in constant time, independently of the desired temporal length to match and the number of extracted analogs. We compare UMAP with Principal component analysis (PCA) and show that UMAP outperforms PCA for spatial MSE analog search with proper settings. Moreover, we show that MASS is 20 times faster than brute force search on the UMAP embeddings space. We test the architecture on a real dataset and show that it enables precise and fast operational analog ensemble search through more than 2 years of radar archive in less than 5 seconds on a single workstation. ","MASS-UMAP: Fast and accurate analog ensemble search in weather radar
  archive"
183,1192012326855163904,22037262,Stuart James,"['Ever wondered how we #Sketch effectively in #VR? Pure virtual? Physical devices? Check our new interaction study paper by @dannox75, Donald Degraen and @anthony_steed on arXiv  <LINK>']",https://arxiv.org/abs/1910.11637,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from within the virtual environment itself. These tools employ sketching and sculpting techniques known from desktop-based interfaces and apply them to hand-based controller interaction. While these techniques allow for mid-air sketching of basic shapes, it remains difficult for users to create detailed and comprehensive 3D models. In our work, we focus on supporting the user in designing the virtual environment around them by enhancing sketch-based interfaces with a supporting system for interactive model retrieval. Through sketching, an immersed user can query a database containing detailed 3D models and replace them into the virtual environment. To understand supportive sketching within a virtual environment, we compare different methods of sketch interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet. %using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and 3D mid-air sketching. Our results show that 3D mid-air sketching is considered to be a more intuitive method to search a collection of models while the addition of physical devices creates confusion due to the complications of their inclusion within a virtual environment. While we pose our work as a retrieval problem for 3D models of chairs, our results can be extrapolated to other sketching tasks for virtual environments. ",Mixing realities for sketch retrieval in Virtual Reality
184,1189482560121921536,41117987,Dr Michelle Collins,"['Today on the arXiv: My White Whale! ""A detailed study of Andromeda XIX, an extreme local analogue of ultra diffuse galaxies"" with @eteq @nfmartin1980 @Janpreston256. This interesting M31 dwarf galaxy has been hard to study, so what have we learned? <LINK> <LINK>', ""First, we've studied 4x the number of stars in And XIX compared with our 2013 work. We find a higher velocity dispersion, making And XIX more massive than before, although still very low density, like the unusual Milky Way dwarf, Antlia 2 (red=M31 dwarf, blue=MW, cyan= UDGs) https://t.co/CvFFQqIhui"", ""The change in dispersion is probably due to survey coverage. Our previous work was only on 1/2 of And XIX. There is some substructure on that side which would have biased us. An excellent example of why a single velocity dispersion doesn't always give a good mass estimate https://t.co/QJopU6Pk1Y"", ""And XIX has a higher mass-to-light ratio than dwarf galaxies of a similar luminosity. We see similar behaviour in galaxies that have been affected by tides. Could that be the case here? Maybe. But it's hard to be sure. https://t.co/5XvyFrK0B6"", ""From imaging, we've suspected that And XIX could be disrupting. It has an unusual shape, and is near a stream like feature that could be associated to it. To check up on this latter assumption, we took spectra of stars in this stream too. It is more metal rich, so probably not. https://t.co/eGeTKX0Aoy"", ""Given And XIX's complexity, it's still hard to be sure what has caused it to look the way it does. But we think the evidence favours a tidal interaction scenario. We have some new HST data, and some plans that should let us test this idea further. So stay tuned :D"", 'Finally, we note that And XIX appears like a UDG. And we note that caution should be taken when inferring masses of galaxies from their velocity dispersions when the tracers are few. Yay for dwarf galaxies :D']",https://arxiv.org/abs/1910.12879,"With a central surface brightness of $\mu_0=29.3$ mag. per sq. arcsec, and half-light radius of $r_{\rm half}=3.1^{+0.9}_{-1.1}$~kpc, Andromeda XIX (And XIX) is an extremely diffuse satellite of Andromeda. We present spectra for $\sim100$ red giant branch stars in this galaxy, plus 16 stars in a nearby stellar stream. With this exquisite dataset, we re-derive the properties of And XIX, measuring a systemic velocity of $v_r=-109.0\pm1.6$ km/s and a velocity dispersion of $\sigma_v = 7.8^{+1.7}_{-1.5}$ km/s (higher than derived in our previous work). We marginally detect a velocity gradient along the major axis of ${\rm d}v/{\rm d}r = -2.1\pm1.8$ km/s kpc$^{-1}$. We find its mass-to-light ratio is higher than galaxies of comparable stellar mass ($[M/L]_{\rm half} = 278^{+146}_{-198}M_\odot/L_\odot$), but its dynamics place it in a halo with a similar total mass to these galaxies. This could suggest that And XIX is a ""puffed up"" dwarf galaxy, whose properties have been altered by tidal processes, similar to its Milky Way counterpart, Antlia II. For the nearby stream, we measure $v_r=-279.2\pm3.7$ km/s, and $\sigma_v=13.8^{+3.5}_{-2.6}$ km/s. We measure its metallicity, and find it to be more metal rich than And XIX, implying that the two features are unrelated. Finally, And XIX's dynamical and structural properties imply it is a local analogue to ultra diffuse galaxies (UDGs). Its complex dynamics suggest that the masses of distant UDGs measured from velocity dispersions alone should be carefully interpreted ","A detailed study of Andromeda XIX, an extreme local analogue of ultra
  diffuse galaxies"
185,1184845891036954624,734677275216470016,Guodong Zhang,"['New work on solving minimax optimization locally. With @YuanhaoWang3 Jimmy Ba.\n\nWe propose a novel algorithm which converges to and only converges to local minimax. The main innovation is a correction term on top of gradient descent-ascent.\n\nPaper link: <LINK>', '@YuanhaoWang3 By adding the correction term to gradient descent-ascent (GDA), we guarantee that the whole system would get closer to the `ridge` every iteration while GDA fails to do so. https://t.co/8nQ00YLXjW']",https://arxiv.org/abs/1910.07512,"Many tasks in modern machine learning can be formulated as finding equilibria in \emph{sequential} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose \emph{Follow-the-Ridge} (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and \emph{positive} momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms. ",On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach
186,1184654282869493761,3223517814,Himan Abdollahpouri,"['In our most recent work, we show how popularity bias in recommender systems is closely connected to  fairness and calibration \n#recsys #fatrecsys #fairness \n\nThe full paper can be found here:\n\n<LINK> <LINK>']",https://arxiv.org/abs/1910.05755,"Recently there has been a growing interest in fairness-aware recommender systems, including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users' true preferences and we consider how various algorithms may result in different degrees of miscalibration. A well-known type of bias in recommendation is popularity bias where few popular items are over-represented in recommendations, while the majority of other items do not get significant exposure. We conjecture that popularity bias is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a strong correlation between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show algorithms with greater popularity bias amplification tend to have greater miscalibration. ","The Impact of Popularity Bias on Fairness and Calibration in
  Recommendation"
187,1182427765963816960,22368984,Jessica Schonhut-Stasik,['I did a new science. The second in a series of papers looking at the binarity of asteroseismic stars that were observed by the Kepler mission.\nWhat did we find? NO SPOILERS YOU GOTS TO READ IT. #stem #apj #steminist #starsarerad #kepler\n<LINK>'],https://arxiv.org/abs/1910.03803,"The Kepler space telescope observed over 15,000 stars for asteroseismic studies. Of these, 75% of dwarfs (and 8% of giants) were found to show anomalous behavior: such as suppressed oscillations (low amplitude) or no oscillations at all. The lack of solar-like oscillations may be a consequence of multiplicity, due to physical interactions with spectroscopic companions or due to the dilution of oscillation amplitudes from ""wide'"" (AO detected; visual) or spectroscopic companions introducing contaminating flux. We present a search for stellar companions to 327 of the Kepler asteroseismic sample, which were expected to display solar-like oscillations. We used direct imaging with Robo-AO, which can resolve secondary sources at ~0.""15, and followed up detected companions with Keck AO. Directly imaged companion systems with both separations of $\leq$0.""5 and amplitude dilutions >10% all have anomalous primaries, suggesting these oscillation signals are diluted by a sufficient amount of excess flux. We also used the high-resolution spectrometer ESPaDOnS at CFHT to search for spectroscopic binaries. We find tentative evidence for a higher fraction of spectroscopic binaries with high radial velocity scatter in anomalous systems, which would be consistent with previous results suggesting that oscillations are suppressed by tidal interactions in close eclipsing binaries. ","Robo-AO Kepler Asteroseismic Survey. II. Do Stellar Companions Inhibit
  Stellar Oscillations?"
188,1182268650960670721,471102898,Kilian Fatras,"['Challenged by the optimal transport computational cost ? Wanna rely on minibatches? In this new work, we present a deeper study of learning with minibatch Wasserstein! We argue that it acts like an implicit regularization on the OT plan. Check it out ! <LINK> <LINK>', 'The minibatch Wasserstein losses has desirable properties. From an unbiased estimator, we proved concentration bounds and unbiased gradient properties, unlike the Wasserstein distance. Hence its use with SGD and for large scale! But also defects as they are no metrics anymore.', 'But for large enough batch sizes, you can still recover good results as shown in our experiments ! We show its behaviour for generative modelling, gradient flow and color transfer (full images). Joint work with Younes Zine, R√©mi Flamary, @RemiGribonval  &amp; @nicolas_courty ! https://t.co/Xy8igvWyy1']",https://arxiv.org/abs/1910.04091,"Optimal transport distances are powerful tools to compare probability distributions and have found many applications in machine learning. Yet their algorithmic complexity prevents their direct use on large scale datasets. To overcome this challenge, practitioners compute these distances on minibatches {\em i.e.} they average the outcome of several smaller optimal transport problems. We propose in this paper an analysis of this practice, which effects are not well understood so far. We notably argue that it is equivalent to an implicit regularization of the original problem, with appealing properties such as unbiased estimators, gradients and a concentration bound around the expectation, but also with defects such as loss of distance property. Along with this theoretical analysis, we also conduct empirical experiments on gradient flows, GANs or color transfer that highlight the practical interest of this strategy. ",Learning with minibatch Wasserstein : asymptotic and gradient properties
