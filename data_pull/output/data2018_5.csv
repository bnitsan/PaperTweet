,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1004817892339265537,3059466984,Ananda Theertha Suresh,"['New paper! In <LINK> , we propose a new mechanism for communication-efficient and differentially-private distributed SGD.']",https://arxiv.org/abs/1805.10559,"Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For $d$ variables and $n \approx d$ clients, the proposed method uses $O(\log \log(nd))$ bits of communication per client per coordinate and ensures constant privacy. We also extend and improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest. ","cpSGD: Communication-efficient and differentially-private distributed
  SGD"
1,1004650352413675520,1900087399,Warwick Astro Group,['ICYMI New research from our own Elizabeth Stanway from @warwickuni and @astro_jje from @UoA_Physics!  Reevaluating Old Stellar Populations with the @astroBPASS models - Read the press release here <LINK>  and the paper on arxiv here <LINK> <LINK>'],https://arxiv.org/abs/1805.08784,"Determining the properties of old stellar populations (those with age >1 Gyr) has long involved the comparison of their integrated light, either in the form of photometry or spectroscopic indexes, with empirical or synthetic templates. Here we reevaluate the properties of old stellar populations using a new set of stellar population synthesis models, designed to incorporate the effects of binary stellar evolution pathways as a function of stellar mass and age. We find that single-aged stellar population models incorporating binary stars, as well as new stellar evolution and atmosphere models, can reproduce the colours and spectral indices observed in both globular clusters and quiescent galaxies. The best fitting model populations are often younger than those derived from older spectral synthesis models, and may also lie at slightly higher metallicities. ",Reevaluating Old Stellar Populations
2,1003936697682448390,24603962,Eneko Agirre,"['New paper accepted at #NLPOSS @acl2018 workshop on ""The risk of  sub-optimal use of Open Source NLP Software"" with lessons for releasing #NLProc  research software <LINK>  (thread 1/5)', 'UKB is an open source collection of programs for performing, among other  tasks, knowledge-based Word Sense Disambiguation (WSD).  https://t.co/GGePy40ZLs (thread 2/5)', 'Since it was released in 2009 it has been often used by third parties  out-of-the-box in sub-optimal settings. We show that nine years later it  is the state-of-the-art on knowledge-based WSD. (thread 3/5)', 'This case shows the pitfalls of releasing open source NLP software  without optimal default settings and precise instructions for  reproducibility.  Authors should not rely on other researchers reading  the papers with care. (thread 4/5)', 'It is in the interest  of authors to include end-to-end scripts that  download all resources, perform any necessary pre-processing and  reproduce the results. We fixed this for UKB in version 3.1  https://t.co/bvXvV1FwJ6 (thread 5/5)']",https://arxiv.org/abs/1805.04277,"UKB is an open source collection of programs for performing, among other tasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility. ","The risk of sub-optimal use of Open Source NLP Software: UKB is
  inadvertently state-of-the-art in knowledge-based WSD"
3,1003698175042048001,965383857775300608,Yichuan Zhang,"['In a few days, I will release a new version of my paper on variational measure preserving flows <LINK> and the demo code on my github <LINK>', '@viettran86 Yes, it is very likely to reach global optimum. check out the experiment section for Guassian and half moon target example. Soon, you can verify that with my code.', '@viettran86 I am not sure what do you mean by mixture model. But in my experiment, my methods achieved much better results than current SOTA on mnist with deep convolutional decoder. I think that is more challenging than most mixture models']",https://arxiv.org/abs/1805.10377,"Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI. ",Ergodic Inference: Accelerate Convergence by Optimisation
4,1003625222812131329,1900087399,Warwick Astro Group,"['New research from our own Elizabeth Stanway from @warwickuni and @astro_jje from @UoA_Physics!  Reevaluating Old Stellar Populations with the @astroBPASS models - Read the press release here <LINK> and the paper on arxiv here <LINK> <LINK>', '(Image credit:  Mark Garlick/University of Warwick)']",https://arxiv.org/abs/1805.08784,"Determining the properties of old stellar populations (those with age >1 Gyr) has long involved the comparison of their integrated light, either in the form of photometry or spectroscopic indexes, with empirical or synthetic templates. Here we reevaluate the properties of old stellar populations using a new set of stellar population synthesis models, designed to incorporate the effects of binary stellar evolution pathways as a function of stellar mass and age. We find that single-aged stellar population models incorporating binary stars, as well as new stellar evolution and atmosphere models, can reproduce the colours and spectral indices observed in both globular clusters and quiescent galaxies. The best fitting model populations are often younger than those derived from older spectral synthesis models, and may also lie at slightly higher metallicities. ",Reevaluating Old Stellar Populations
5,1003222274684661760,57793813,Teppei Katori (香取哲平),"['People asked me what I think about new #MiniBooNE paper (<LINK>). I think we need new simulation. 2 benefits; 1st, we can confirm there is really excess. 2nd, we can produce covariance matrices for all #nuxsec data for global fit #BananaIsDead #ZombieBanana', '@ClareBurrage Geometry and neutrino interaction. We (I am one of authors) need to check all materials around the detector are correctly defined, then make sure all interactions (especially pi0 productions) are simulated correctly. Background simulation need to be checked more carefully.']",https://arxiv.org/abs/1805.12028,"The MiniBooNE experiment at Fermilab reports results from an analysis of $\nu_e$ appearance data from $12.84 \times 10^{20}$ protons on target in neutrino mode, an increase of approximately a factor of two over previously reported results. A $\nu_e$ charged-current quasielastic event excess of $381.2 \pm 85.2$ events ($4.5 \sigma$) is observed in the energy range $200<E_\nu^{QE}<1250$~MeV. Combining these data with the $\bar \nu_e$ appearance data from $11.27 \times 10^{20}$ protons on target in antineutrino mode, a total $\nu_e$ plus $\bar \nu_e$ charged-current quasielastic event excess of $460.5 \pm 99.0$ events ($4.7 \sigma$) is observed. If interpreted in a two-neutrino oscillation model, ${\nu}_{\mu} \rightarrow {\nu}_e$, the best oscillation fit to the excess has a probability of $21.1\%$, while the background-only fit has a $\chi^2$ probability of $6 \times 10^{-7}$ relative to the best fit. The MiniBooNE data are consistent in energy and magnitude with the excess of events reported by the Liquid Scintillator Neutrino Detector (LSND), and the significance of the combined LSND and MiniBooNE excesses is $6.0 \sigma$. A two-neutrino oscillation interpretation of the data would require at least four neutrino types and indicate physics beyond the three neutrino paradigm.Although the data are fit with a two-neutrino oscillation model, other models may provide better fits to the data. ","Significant Excess of ElectronLike Events in the MiniBooNE
  Short-Baseline Neutrino Experiment"
6,1003017137823219713,1281653876,Siddhartha Brahma,"['Our new paper on Suffix BiLSTMs leading to better performance on several NLP tasks compared to vanilla BiLSTMs, including SOTA on some.\n<LINK>']",https://arxiv.org/abs/1805.07340,"Recurrent neural networks have become ubiquitous in computing representations of sequential data, especially textual data in natural language processing. In particular, Bidirectional LSTMs are at the heart of several neural models achieving state-of-the-art performance in a wide variety of tasks in NLP. However, BiLSTMs are known to suffer from sequential bias - the contextual representation of a token is heavily influenced by tokens close to it in a sentence. We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions. We call our model Suffix Bidirectional LSTM or SuBiLSTM. This introduces an alternate bias that favors long range dependencies. We apply SuBiLSTMs to several tasks that require sentence modeling. We demonstrate that using SuBiLSTM instead of a BiLSTM in existing models leads to improvements in performance in learning general sentence representations, text classification, textual entailment and paraphrase detection. Using SuBiLSTM we achieve new state-of-the-art results for fine-grained sentiment classification and question classification. ",Improved Sentence Modeling using Suffix Bidirectional LSTM
7,1002897625497026560,53464710,Eric Wong,"['New followup paper on scalable provable adversarial defenses with Frank Schmidt, Jan Metzen, and @zicokolter: verified CIFAR10 performance on residual networks with 100k hidden units and millions of parameters. \n\nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/1805.12514,"Recent work has developed methods for learning deep network classifiers that are provably robust to norm-bounded adversarial perturbation; however, these methods are currently only possible for relatively small feedforward networks. In this paper, in an effort to scale these approaches to substantially larger models, we extend previous work in three main directions. First, we present a technique for extending these training procedures to much more general networks, with skip connections (such as ResNets) and general nonlinearities; the approach is fully modular, and can be implemented automatically (analogous to automatic differentiation). Second, in the specific case of $\ell_\infty$ adversarial perturbations and networks with ReLU nonlinearities, we adopt a nonlinear random projection for training, which scales linearly in the number of hidden units (previous approaches scaled quadratically). Third, we show how to further improve robust error through cascade models. On both MNIST and CIFAR data sets, we train classifiers that improve substantially on the state of the art in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST (with $\ell_\infty$ perturbations of $\epsilon=0.1$), and from 80% to 36.4% on CIFAR (with $\ell_\infty$ perturbations of $\epsilon=2/255$). Code for all experiments in the paper is available at this https URL ",Scaling provable adversarial defenses
8,1002598053658972160,976476400248147968,Hagan Lab,"['New paper up on arXiv, ""Nanoparticles Binding to Lipid Membranes: from Vesicle-Based Gels to Vesicle Inversion and Destruction"", in collaboration with @RodalLab here at Brandeis, and @RotelloGroup and the Dinsmore group at UMass Amherst!\n<LINK>']",https://arxiv.org/abs/1805.04214,"Cells offer numerous inspiring examples where proteins and membranes combine to form complex structures that are key to intracellular compartmentalization, cargo transport, and specialization of cell morphology. Despite this wealth of examples, we still lack the design principles to control membrane morphology in synthetic systems. Here we show that even the relatively simple case of spherical nanoparticles binding to lipid-bilayer membrane vesicles results in a remarkably rich set of morphologies that can be controlled quantitatively via the particle binding energy. We find that when the binding energy is weak relative to a characteristic membrane-bending energy, the vesicles adhere to one another and form a soft solid, which could be used as a useful platform for controlled release. When the binding energy is larger, the vesicles undergo a remarkable destruction process consisting first of invaginated tubules, followed by vesicles turning inside-out, yielding a network of nanoparticle-membrane tubules. We propose that the crossover from one behavior to the other is triggered by the transition from partial to complete wrapping of nanoparticles. This model is confirmed by computer simulations and by quantitative estimates of the binding energy. These findings open the door to a new class of vesicle-based, closed-cell gels that are more than 99% water and can encapsulate and release on demand. Our results also show how to intentionally drive dramatic shape changes in vesicles as a step toward shape-responsive particles. Finally, they help us to unify the wide range of previously observed responses of vesicles and cells to added nanoparticles. ","Nanoparticles Binding to Lipid Membranes: from Vesicle-Based Gels to
  Vesicle Inversion and Destruction"
9,1002572739255812096,759118366468481024,Decker French,"['New paper out today! \n<LINK>', 'We used ALMA to look at galaxies with recently-ended starbursts. These are good test cases for studying processes which shut down star formation in galaxies, but there was a problem: some of these galaxies have way to much CO-traced gas for their low star formation rates.', 'In the new paper, we looked for denser gas as traced by HCN and HCO+', 'We found no signal, with limits deep enough to imply (1) the absence of dense gas is consistent with the low star formation rates, and (2) the dense gas ratio (traced by HCN/CO) is low!', 'This means we understand *why* the post-starbursts can no longer form stars at high rates: they are out of *dense* gas. It also means something is preventing the gas we saw traced by CO from collapsing into denser states.', 'These results pair in an interesting way to another paper I have submitted, so stay tuned!']",https://arxiv.org/abs/1805.12132,"Post-starburst or ""E+A"" galaxies are rapidly transitioning from star-forming to quiescence. While the current star formation rate of post-starbursts is already at the level of early type galaxies, we recently discovered that many have large CO-traced molecular gas reservoirs consistent with normal star forming galaxies. These observations raise the question of why these galaxies have such low star formation rates. Here we present an ALMA search for the denser gas traced by HCN (1--0) and HCO+ (1--0) in two CO-luminous, quiescent post-starburst galaxies. Intriguingly, we fail to detect either molecule. The upper limits are consistent with the low star formation rates and with early-type galaxies. The HCN/CO luminosity ratio upper limits are low compared to star-forming and even many early type galaxies. This implied low dense gas mass fraction explains the low star formation rates relative to the CO-traced molecular gas and suggests the state of the gas in post-starburst galaxies is unusual, with some mechanism inhibiting its collapse to denser states. We conclude that post-starbursts galaxies are now quiescent because little dense gas is available, in contrast to the significant CO-traced lower density gas reservoirs that still remain. ",Why Post-Starburst Galaxies are Now Quiescent
10,1002532250964119552,252608121,Yaneer Bar-Yam,"['new paper: Technical discussion of when local properties are important for the system as a whole. ""Renormalization of Sparse Disorder in the Ising Model""  <LINK>']",https://arxiv.org/abs/1805.12556,"We consider the renormalization of quenched bond disorder in the Ising model in the limit that it is sparse -- highly localized and vanishing in the thermodynamic limit. We begin in 1D with arbitrary disorder assigned to a finite number of bonds and study how the system renormalizes, finding non-trivial fixed point structure for any given bond with a separatrix at zero bond strength, equivalent to inserting a break in the chain. Either side of this critical line, renormalization group (RG) trajectories flow towards one of two attractors on which the disordered bonds settle onto ferromagnetic or anti-ferromagnetic (AF) couplings of equal and opposite magnitude. Bonds that settle on an AF attractor are equivalent to inserting a twist in the chain at the location of the bond, implying a multi-kink ground state solution. Qualitatively different behavior emerges at the RG step when bonds start to coalesce, with the chain `untwisting' whenever two AF bonds coalesce. Our findings generalize to higher dimensions for codimension one defects that are sparse from the perspective of the orthogonal complement lattice. In 2D, the $\mathbb Z_2$ symmetry of the model has an IR manifestation where one can construct field strengths and Wilson loops (which characterize frustration) from fundamental plaquette variables. In the non-sparse limit where the disorder parameter is drawn from an arbitrary but homogeneously assigned probability distribution function, we recover previously found fixed distributions as special cases, but find only the trivial paramagnetic distribution to be an attractor. For non-homogeneously assigned disorder that is sufficiently dilute however, we find the Edwards-Anderson model with equal probability $\pm J$ bonds to also be an IR attractor. ",Renormalization of Sparse Disorder in the Ising Model
11,1002497801685753856,157973000,Michael Pfarrhofer,['New working paper on regularization/shrinkage in spatial econometric models <LINK>'],https://arxiv.org/abs/1805.10822,"This article introduces two absolutely continuous global-local shrinkage priors to enable stochastic variable selection in the context of high-dimensional matrix exponential spatial specifications. Existing approaches as a means to dealing with overparameterization problems in spatial autoregressive specifications typically rely on computationally demanding Bayesian model-averaging techniques. The proposed shrinkage priors can be implemented using Markov chain Monte Carlo methods in a flexible and efficient way. A simulation study is conducted to evaluate the performance of each of the shrinkage priors. Results suggest that they perform particularly well in high-dimensional environments, especially when the number of parameters to estimate exceeds the number of observations. For an empirical illustration we use pan-European regional economic growth data. ","Flexible shrinkage in high-dimensional Bayesian spatial autoregressive
  models"
12,1002445280803999745,75249390,Axel Maas,['An interesting new paper on neutrino oscillations by MiniBooNE today: <LINK> - confirms the LSND results.'],https://arxiv.org/abs/1805.12028,"The MiniBooNE experiment at Fermilab reports results from an analysis of $\nu_e$ appearance data from $12.84 \times 10^{20}$ protons on target in neutrino mode, an increase of approximately a factor of two over previously reported results. A $\nu_e$ charged-current quasielastic event excess of $381.2 \pm 85.2$ events ($4.5 \sigma$) is observed in the energy range $200<E_\nu^{QE}<1250$~MeV. Combining these data with the $\bar \nu_e$ appearance data from $11.27 \times 10^{20}$ protons on target in antineutrino mode, a total $\nu_e$ plus $\bar \nu_e$ charged-current quasielastic event excess of $460.5 \pm 99.0$ events ($4.7 \sigma$) is observed. If interpreted in a two-neutrino oscillation model, ${\nu}_{\mu} \rightarrow {\nu}_e$, the best oscillation fit to the excess has a probability of $21.1\%$, while the background-only fit has a $\chi^2$ probability of $6 \times 10^{-7}$ relative to the best fit. The MiniBooNE data are consistent in energy and magnitude with the excess of events reported by the Liquid Scintillator Neutrino Detector (LSND), and the significance of the combined LSND and MiniBooNE excesses is $6.0 \sigma$. A two-neutrino oscillation interpretation of the data would require at least four neutrino types and indicate physics beyond the three neutrino paradigm.Although the data are fit with a two-neutrino oscillation model, other models may provide better fits to the data. ","Significant Excess of ElectronLike Events in the MiniBooNE
  Short-Baseline Neutrino Experiment"
13,1002348104945930242,1648277749,Greg Warrington,"['New gerrymandering paper comparing metrics: efficiency gap, declination (differential responsiveness), mean-median, partisan bias, .... Lots of historical and hypothetical elections considered individually. <LINK> <LINK>', '@mathzorro Many good mathematical questions still to be explored :)']",https://arxiv.org/abs/1805.12572,"We compare and contrast fourteen measures that have been proposed for the purpose of quantifying partisan gerrymandering. We consider measures that, rather than examining the shapes of districts, utilize only the partisan vote distribution among districts. The measures considered are two versions of partisan bias; the efficiency gap and several of its variants; the mean-median difference and the equal vote weight standard; the declination and one variant; and the lopsided-means test. Our primary means of evaluating these measures is a suite of hypothetical elections we classify from the start as fair or unfair. We conclude that the declination is the most successful measure in terms of avoiding false positives and false negatives on the elections considered. We include in an appendix the most extreme outliers for each measure among historical congressional and state legislative elections. ",A comparison of partisan-gerrymandering measures
14,1002346578353557511,1556664198,Kyle Cranmer,"['Happy to share a new paper ""Mining gold from implicit models to improve likelihood-free inference"", which dramatically improves sample efficiency. \nWe introduce a suite of new methods that go #BeyondABC &amp; the #LikelihoodRatioTrick\n<LINK>\n<LINK> <LINK>', 'This paper describes a generalization of the techniques used for our particle physics use-case to a machine learning / stats / non-physics audience.\nhttps://t.co/adJKEFw0Ko', 'And you can reproduce plots in the paper by running it in Binder thanks to @mybinderteam ! https://t.co/eK7nhs6qiQ']",https://arxiv.org/abs/1805.12244,"Simulators often provide the best description of real-world phenomena. However, they also lead to challenging inverse problems because the density they implicitly define is often intractable. We present a new suite of simulation-based inference techniques that go beyond the traditional Approximate Bayesian Computation approach, which struggles in a high-dimensional setting, and extend methods that use surrogate models based on neural networks. We show that additional information, such as the joint likelihood ratio and the joint score, can often be extracted from simulators and used to augment the training data for these surrogate models. Finally, we demonstrate that these new techniques are more sample efficient and provide higher-fidelity inference than traditional methods. ",Mining gold from implicit models to improve likelihood-free inference
15,1002337442291056641,16714100,Cayman Unterborn,['New paper alert! @diogo_souto and coauthors measure some really hard-to-get abundances for Ross-128 and I talk myself in circles trying to explain how this stellar stoichiometry affects the potential planetary interior composition of Ross-128b! <LINK>'],https://arxiv.org/abs/1805.11633,"The first detailed chemical abundance analysis of the M dwarf (M4.0) exoplanet-hosting star Ross 128 is presented here, based upon near-infrared (1.5--1.7 \micron) high-resolution ($R$$\sim$22,500) spectra from the SDSS-APOGEE survey. We determined precise atmospheric parameters $T_{\rm eff}$=3231$\pm$100K, log$g$=4.96$\pm$0.11 dex and chemical abundances of eight elements (C, O, Mg, Al, K, Ca, Ti, and Fe), finding Ross 128 to have near solar metallicity ([Fe/H] = +0.03$\pm$0.09 dex). The derived results were obtained via spectral synthesis (1-D LTE) adopting both MARCS and PHOENIX model atmospheres; stellar parameters and chemical abundances derived from the different adopted models do not show significant offsets. Mass-radius modeling of Ross 128b indicate that it lies below the pure rock composition curve, suggesting that it contains a mixture of rock and iron, with the relative amounts of each set by the ratio of Fe/Mg. If Ross 128b formed with a sub-solar Si/Mg ratio, and assuming the planet's composition matches that of the host-star, it likely has a larger core size relative to the Earth. The derived planetary parameters -- insolation flux (S$_{\rm Earth}$=1.79$\pm$0.26) and equilibrium temperature ($T_{\rm eq}$=294$\pm$10K) -- support previous findings that Ross 128b is a temperate exoplanet in the inner edge of the habitable zone. ","Stellar and Planetary Characterization of the Ross 128 Exoplanetary
  System from APOGEE Spectra"
16,1002220171124248577,2162589301,Marc Brockschmidt,"['New paper on learning to generate semantically meaningful graphs is out: <LINK> - work done by our amazing intern Qi Liu, with Alex Gaunt and @miltos1. Trains generative graph model in VAE setting, works super-well to generate somewhat believable new molecules.']",https://arxiv.org/abs/1805.09076,"Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties. ",Constrained Graph Variational Autoencoders for Molecule Design
17,1002165356633485312,792359461,Michael Hind,"[""I'm excited to announce a new approach to AI Explainability (#XAI), called TED.   This initial paper is with my wonderful colleagues at @IBMResearch, @NoelCodella, @nrkarthikeyan, @murraycampbell, Amit Dhundhar, @krvarshney, Dennis Wei, @datapriestess  <LINK>"", '@vdignum @IBMResearch @NoelCodella @nrkarthikeyan @murraycampbell @krvarshney @datapriestess Thanks for your interest.  Consider a loan approval system: the training explanations would be reasons why a loan is denied (or approved) something that other researchers have argued is relatively simple to articulate once the loan expert has decided the approve/deny decision ...', '@vdignum @IBMResearch @NoelCodella @nrkarthikeyan @murraycampbell @krvarshney @datapriestess ... Since these explanations are constructed by a domain expert, rather than the AI system, there is hope one could avoid the issues you raise.  However, there are several interesting research questions to be answered regarding the format and content of the training explanations.']",https://arxiv.org/abs/1805.11648,"The adoption of machine learning in high-stakes applications such as healthcare and law has lagged in part because predictions are not accompanied by explanations comprehensible to the domain user, who often holds the ultimate responsibility for decisions and outcomes. In this paper, we propose an approach to generate such explanations in which training data is augmented to include, in addition to features and labels, explanations elicited from domain users. A joint model is then learned to produce both labels and explanations from the input features. This simple idea ensures that explanations are tailored to the complexity expectations and domain knowledge of the consumer. Evaluation spans multiple modeling techniques on a game dataset, a (visual) aesthetics dataset, a chemical odor dataset and a Melanoma dataset showing that our approach is generalizable across domains and algorithms. Results demonstrate that meaningful explanations can be reliably taught to machine learning algorithms, and in some cases, also improve modeling accuracy. ",Teaching Meaningful Explanations
18,1002111606195736576,57793813,Teppei Katori (香取哲平),"['Dr Jordi Salvadó #Barcelona talks future of 1eV sterile #neutrino search. A new result from #MiniBooNE is an additional discussion topic today (paper <LINK>) at #ICTP atmospheric neutrino workshop, #Trieste @ICC_UB @DUNEScience <LINK>']",https://arxiv.org/abs/1805.12028,"The MiniBooNE experiment at Fermilab reports results from an analysis of $\nu_e$ appearance data from $12.84 \times 10^{20}$ protons on target in neutrino mode, an increase of approximately a factor of two over previously reported results. A $\nu_e$ charged-current quasielastic event excess of $381.2 \pm 85.2$ events ($4.5 \sigma$) is observed in the energy range $200<E_\nu^{QE}<1250$~MeV. Combining these data with the $\bar \nu_e$ appearance data from $11.27 \times 10^{20}$ protons on target in antineutrino mode, a total $\nu_e$ plus $\bar \nu_e$ charged-current quasielastic event excess of $460.5 \pm 99.0$ events ($4.7 \sigma$) is observed. If interpreted in a two-neutrino oscillation model, ${\nu}_{\mu} \rightarrow {\nu}_e$, the best oscillation fit to the excess has a probability of $21.1\%$, while the background-only fit has a $\chi^2$ probability of $6 \times 10^{-7}$ relative to the best fit. The MiniBooNE data are consistent in energy and magnitude with the excess of events reported by the Liquid Scintillator Neutrino Detector (LSND), and the significance of the combined LSND and MiniBooNE excesses is $6.0 \sigma$. A two-neutrino oscillation interpretation of the data would require at least four neutrino types and indicate physics beyond the three neutrino paradigm.Although the data are fit with a two-neutrino oscillation model, other models may provide better fits to the data. ","Significant Excess of ElectronLike Events in the MiniBooNE
  Short-Baseline Neutrino Experiment"
19,1001994216648691713,334628959,Roberto Calandra,"['Tired of RL algorithms that take forever to converge? Read our new paper on ""Deep Reinforcement Learning in a Handful of Trials using Probabilistic  Dynamics Models"" <LINK>\n@svlevine <LINK>']",http://arxiv.org/abs/1805.12114,"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task). ","Deep Reinforcement Learning in a Handful of Trials using Probabilistic
  Dynamics Models"
20,1001994190325276674,334628959,Roberto Calandra,"['Tired of RL algorithms that take forever to converge? Read our new paper on ""Deep Reinforcement Learning in a Handful of Trials using Probabilistic  Dynamics Models"" <LINK>\n@svlevine']",http://arxiv.org/abs/1805.12114,"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task). ","Deep Reinforcement Learning in a Handful of Trials using Probabilistic
  Dynamics Models"
21,1001990634973159425,617213120,Daniel Murfet,"['My new paper with James Clift: Derivatives of Turing machines in Linear Logic (<LINK>). We make connections between derivatives of algorithms and propagation of uncertainty, and apply this to the problem of synthesising Turing machines by gradient descent.']",https://arxiv.org/abs/1805.11813,We calculate denotations under the Sweedler semantics of the Ehrhard-Regnier derivatives of various encodings of Turing machines into linear logic. We show that these derivatives calculate the rate of change of probabilities naturally arising in the Sweedler semantics of linear logic proofs. The resulting theory is applied to the problem of synthesising Turing machines by gradient descent. ,Derivatives of Turing machines in Linear Logic
22,1001896436194213894,2932678322,Keaton Bell,"['Our new paper uses nearly 200 hours of @mcdonaldobs 2.1m time to find and characterize pulsating stars among the enigmatic ""sdAs""---stars with spectroscopic gravities too strong for the main sequence and temperatures too low for normal subdwarfs. <LINK>', 'Seven out of 23 targets show clear variability in their time series photometry. With dominant pulsation periods spanning 4.6 minutes to 12.3 hours, the asteroseismic evidence supports that the ""sdAs"" comprise multiple subpopulations. https://t.co/6RPdQQEcN8', 'An orbital period of 6.4 hours from follow-up spectroscopy establishes that SDSS J1618+3854 is a pulsating extremely low-mass white dwarf, while SDSS J0756+5027 may be a new low-mass RR Lyrae variable like OGLE-BLG-RRLYR-02792. https://t.co/HQVxGqEUXx https://t.co/bT6f0ntFKd', 'We are releasing all of the light curves of the new variable stars to facilitate better follow-up analyses.']",https://arxiv.org/abs/1805.11129,"Context. The nature of the recently identified ""sdA"" spectroscopic class of star is not well understood. The thousands of known sdAs have H-dominated spectra, spectroscopic surface gravities intermediate to main sequence stars and isolated white dwarfs, and effective temperatures below the lower limit for He-burning subdwarfs. Most are likely products of binary stellar evolution, whether extremely low-mass white dwarfs and their precursors, or blue stragglers in the halo. Aims. Stellar eigenfrequencies revealed through time series photometry of pulsating stars sensitively probe stellar structural properties. The properties of pulsations exhibited by any sdA stars would contribute importantly to our developing understanding of this class. Methods. We extend our photometric campaign to discover pulsating extremely low-mass white dwarfs from McDonald Observatory to target sdA stars classified from SDSS spectra. We also obtain follow-up time series spectroscopy to search for binary signatures from four new pulsators. Results. Out of 23 sdA stars observed, we clearly detect stellar pulsations in seven. Dominant pulsation periods range from 4.6 minutes to 12.3 hours, with most on ~hour timescales. We argue specific classifications for some of the new variables, identifying both compact and likely main sequence dwarf pulsators, along with a candidate low-mass RR Lyrae star. Conclusions. With dominant pulsation periods spanning orders of magnitude, the pulsational evidence supports the emerging narrative that the sdA class consists of multiple stellar populations. Since multiple types of sdA exhibit stellar pulsations, follow-up asteroseismic analysis can be used to probe the precise evolutionary natures and stellar structures of these individual subpopulations. ","The McDonald Observatory search for pulsating sdA stars: asteroseismic
  support for multiple populations"
23,1001869533286682624,893109085,Edouard Grave,"['New paper on unsupervised mapping of word vectors, using Procrustes in Wasserstein distance, available on arxiv: <LINK>. With @armandjoulin and Q. Berthet. More resources to come soon!']",https://arxiv.org/abs/1805.11222,"We consider the task of aligning two sets of points in high dimension, which has many applications in natural language processing and computer vision. As an example, it was recently shown that it is possible to infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data. These recent advances are based on adversarial training to learn the mapping between the two embeddings. In this paper, we propose to use an alternative formulation, based on the joint estimation of an orthogonal matrix and a permutation matrix. While this problem is not convex, we propose to initialize our optimization algorithm by using a convex relaxation, traditionally considered for the graph isomorphism problem. We propose a stochastic algorithm to minimize our cost function on large scale problems. Finally, we evaluate our method on the problem of unsupervised word translation, by aligning word embeddings trained on monolingual data. On this task, our method obtains state of the art results, while requiring less computational resources than competing approaches. ",Unsupervised Alignment of Embeddings with Wasserstein Procrustes
24,1001824722340990978,830355049,Mohit Bansal,"['New @acl2018 paper by @YenChunChen4 on ""Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"" is out on arxiv now: <LINK> \n\nCODE+MODELS all publicly available at: <LINK>\n\n#NLProc #UNCNLP #ACL2018 <LINK>']",https://arxiv.org/abs/1805.11080,"Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model. ","Fast Abstractive Summarization with Reinforce-Selected Sentence
  Rewriting"
25,1001546442413035521,334628959,Roberto Calandra,['Our new paper on learning to grasp and re-grasp using vision and touch is now on arxiv: <LINK>\n@svlevine'],https://arxiv.org/abs/1805.11085,"For humans, the process of grasping an object relies heavily on rich tactile feedback. Most recent robotic grasping work, however, has been based only on visual input, and thus cannot easily benefit from feedback after initiating contact. In this paper, we investigate how a robot can learn to use tactile information to iteratively and efficiently adjust its grasp. To this end, we propose an end-to-end action-conditional model that learns regrasping policies from raw visuo-tactile data. This model -- a deep, multimodal convolutional network -- predicts the outcome of a candidate grasp adjustment, and then executes a grasp by iteratively selecting the most promising actions. Our approach requires neither calibration of the tactile sensors, nor any analytical modeling of contact forces, thus reducing the engineering effort required to obtain efficient grasping policies. We train our model with data from about 6,450 grasping trials on a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger. Across extensive experiments, our approach outperforms a variety of baselines at (i) estimating grasp adjustment outcomes, (ii) selecting efficient grasp adjustments for quick grasping, and (iii) reducing the amount of force applied at the fingers, while maintaining competitive performance. Finally, we study the choices made by our model and show that it has successfully acquired useful and interpretable grasping behaviors. ","More Than a Feeling: Learning to Grasp and Regrasp using Vision and
  Touch"
26,1001428653832630272,772809603046334464,Jonathan Mackey,"['New paper on the expansion of photoionized bubbles around hot stars, with analogy to underwater bubbles :-)\n<LINK>\nGrew out of a small residential workshop @DunsinkObs @DIAS_Dublin in 2016!']",https://arxiv.org/abs/1805.09273,"Recent numerical and analytic work has highlighted some shortcomings in our understanding of the dynamics of H II region expansion, especially at late times, when the H II region approaches pressure equilibrium with the ambient medium. Here we reconsider the idealized case of a constant radiation source in a uniform and spherically symmetric ambient medium, with an isothermal equation of state. A thick-shell solution is developed which captures the stalling of the ionization front and the decay of the leading shock to a weak compression wave as it escapes to large radii. An acoustic approximation is introduced to capture the late-time damped oscillations of the H II region about the stagnation radius. Putting these together, a matched asymptotic equation is derived for the radius of the ionization front which accounts for both the inertia of the expanding shell and the finite temperature of the ambient medium. The solution to this equation is shown to agree very well with the numerical solution at all times, and is superior to all previously published solutions. The matched asymptotic solution can also accurately model the variation of H II region radius for a time-varying radiation source. ",The classical D-type expansion of spherical H II regions
27,1001423993310760960,721931072,Shimon Whiteson,"[""I'm pleased to share our new paper, Contextual Policy Optimisation: using Bayesian optimisation to tune environment variables in a simulator so as to maximise the improvement of each step of a policy gradient method.  <LINK> @maosbot""]",https://arxiv.org/abs/1805.10662,"Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies. ",Fingerprint Policy Optimisation for Robust Reinforcement Learning
28,1001356657711026176,2886658437,Sean Raymond,"['New paper out with a simple message: the migration model produces a diversity in super-Earth compositions, from pure rocks to ice-rich. Bonus: a simulation that matches the different densities of the Kepler-36 planets (one icy, one rocky)  <LINK> <LINK>', ""A key corollary: finding purely rocky planets -- especially the closest-in ones that are easiest to characterize -- shouldn't be used to discount the migration model."", 'However, if *all* super-Earths turn out to be rocky then we are missing something important.', '@bmac_astro @mattkenworthy Absolutely.  Most exoplanets are massive enough that, embedded in a gaseous disk, they must migrate. Models suggest the big cores form fast past the snow line. Without something to block them (like Jupiter) they should migrate in and drench the HZ. https://t.co/mDVejSxyp2', ""While I'm at it, here is a MOJO video about the origins of super-Earths.  Not specifically their compositions, just a description of the migration model. https://t.co/y7vLnzbn2O""]",https://arxiv.org/abs/1805.10345,"A leading model for the origin of super-Earths proposes that planetary embryos migrate inward and pile up on close-in orbits. As large embryos are thought to preferentially form beyond the snow line, this naively predicts that most super-Earths should be very water-rich. Here we show that the shortest-period planets formed in the migration model are often purely rocky. The inward migration of icy embryos through the terrestrial zone accelerates the growth of rocky planets via resonant shepherding. We illustrate this process with a simulation that provided a match to the Kepler-36 system of two planets on close orbits with very different densities. In the simulation, two super-Earths formed in a Kepler-36-like configuration; the inner planet was pure rock while the outer one was ice-rich. We conclude from a suite of simulations that the feeding zones of close-in super-Earths are likely to be broad and disconnected from their final orbital radii. ",Migration-driven diversity of super-Earth compositions
29,1001263035896475649,3245949691,Rebecca Leane,"['New paper! ""GeV-Scale Thermal WIMPs: Not Even Slightly Dead""\n<LINK>\nWe construct a model-indep limit on the WIMP total annihilation cross section, showing thermal WIMPs w/ 2-&gt;2 s-wave annihilation to visible states are only known to have mass above about 20 GeV. <LINK>']",https://arxiv.org/abs/1805.10305,"Weakly Interacting Massive Particles (WIMPs) have long reigned as one of the leading classes of dark matter candidates. The observed dark matter abundance can be naturally obtained by freezeout of weak-scale dark matter annihilations in the early universe. This ""thermal WIMP"" scenario makes direct predictions for the total annihilation cross section that can be tested in present-day experiments. While the dark matter mass constraint can be as high as $m_\chi\gtrsim100$ GeV for particular annihilation channels, the constraint on the total cross section has not been determined. We construct the first model-independent limit on the WIMP total annihilation cross section, showing that allowed combinations of the annihilation-channel branching ratios considerably weaken the sensitivity. For thermal WIMPs with s-wave $2\rightarrow2$ annihilation to visible final states, we find the dark matter mass is only known to be $m_\chi\gtrsim20$ GeV. This is the strongest largely model-independent lower limit on the mass of thermal-relic WIMPs, together with the upper limit on the mass from the unitarity bound ($m_\chi\lesssim 100$ TeV), it defines what we call the ""WIMP window"". To probe the remaining mass range, we outline ways forward. ",GeV-Scale Thermal WIMPs: Not Even Slightly Dead
30,1001259821692342272,113216942,Blake Camp,"['New paper, Scalable approach to Multi-Context Continual Learning via Lifelong Skill-Encoding, <LINK>']",https://arxiv.org/abs/1805.10354,"Learning a set of tasks over time, also known as continual learning (CL), is one of the most challenging problems in artificial intelligence. While recent approaches achieve some degree of CL in deep neural networks, they either (1) grow the network parameters linearly with the number of tasks, (2) require storing training data from previous tasks, or (3) restrict the network's ability to learn new tasks. To address these issues, we propose a novel framework, Self-Net, that uses an autoencoder to learn a set of low-dimensional representations of the weights learned for different tasks. We demonstrate that these low-dimensional vectors can then be used to generate high-fidelity recollections of the original weights. Self-Net can incorporate new tasks over time with little retraining and with minimal loss in performance for older tasks. Our system does not require storing prior training data and its parameters grow only logarithmically with the number of tasks. We show that our technique outperforms current state-of-the-art approaches on numerous datasets---including continual versions of MNIST, CIFAR10, CIFAR100, and Atari---and we demonstrate that our method can achieve over 10X storage compression in a continual fashion. To the best of our knowledge, we are the first to use autoencoders to sequentially encode sets of network weights to enable continual learning. ",Self-Net: Lifelong Learning via Continual Self-Modeling
31,1001186740298702848,529438517,Aaron Clauset,"['Excited to share a new paper, which investigates how faculty hiring is a mechanism for how ""Prestige drives epistemic inequality in the diffusion of scientific ideas,"" with @alliecmorgan D. Economou, and @samfway <LINK> <LINK>', '@boydgraber @alliecmorgan @samfway We did consider using topic models on titles (like we did here https://t.co/bIIuEfGzps). Expert-generated keywords worked well for the subject areas we chose. Some careful topic modeling would be a great extension of our empirical analysis.']",https://arxiv.org/abs/1805.09966,"The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact. However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads. Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates. We consider comprehensive data on the hiring events of 5032 faculty at all 205 Ph.D.-granting departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications. Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science. Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its longterm diffusion across the network. We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation. Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science. ","Prestige drives epistemic inequality in the diffusion of scientific
  ideas"
32,1001049570443431938,735386827578875904,siegfried Vanaverbek,['Our new paper on the formation of brown dwarfs and low-mass stars is on arxiv today:\n<LINK>\nThis is very much a work in progress.  I have finished the implementation of radiative transfer while the combination with MHD can now be done too.'],http://arxiv.org/abs/1805.09881,"The origin of very low-mass stars (VLMS) and brown dwarfs (BDs) is still an unresolved topic of star formation. We here present numerical simulations of the formation of VLMS, BDs, and planet mass objects (planemos) resulting from the gravitational collapse and fragmentation of solar mass molecular cores with varying rotation rates and initial density perturbations. Our simulations yield various types of binary systems including the combinations VLMS-VLMS, BD-BD, planemo-planemo, VLMS-BD, VLMS-planemos, BD-planemo. Our scheme successfully addresses the formation of wide VLMS and BD binaries with semi-major axis up to 441 AU and produces a spectrum of mass ratios closer to the observed mass ratio distribution (q > 0.5). Molecular cores with moderate values of the ratio of kinetic to gravitational potential energy (0.16 <= beta <= 0.21) produce planemos. Solar mass cores with rotational parameters beta outside of this range yield either VLMS/BDs or a combination of both. With regard to the mass ratios we find that for both types of binary systems the mass ratio distribution varies in the range 0.31 <= q <= 0.74. We note that in the presence of radiative feedback, the length scale of fragmentation would increase by approximately two orders of magnitude, implying that the formation of binaries may be efficient for wide orbits, while being suppressed for short-orbit systems. ","Formation of multiple low mass stars, brown dwarfs and planemos via
  gravitational collapse"
33,1001028089067995136,841031248839618560,Relja Arandjelović,"['Our new paper on training networks which are verifiably robust against all possible adversarial attacks. SOTA on MNIST, first network with some guarantees on CIFAR. @pushmeet <LINK> <LINK>', '@DeepMindAI']",https://arxiv.org/abs/1805.10265,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10. ",Training verified learners with learned verifiers
34,1001021159343837185,523241142,Juste Raimbault,['New paper : Reconciling complexities: for a stronger integration of approaches to  complex socio-technical systems\n<LINK>'],https://arxiv.org/abs/1805.10195,"Systems engineering has developed a mature knowledge on how to design, integrate and manage complex industrial systems, whereas disciplines studying complex systems in nature or society also propose numerous tools for their understanding. Socio-technical systems, that situate at their intersection, could benefit from a higher integration between these. This position paper advocates for such integrated approaches. A bibliometric study through citation networks first illustrates the respective isolation of some of these approaches. We then produce a proof-of-concept of how the transfer of concepts from biology can be useful for the design of complex systems, in the particular case of transportation networks, using a biological network growth model to produce various optimal networks in terms of cost and efficiency. We finally discuss possible disciplinary positioning of such hybrid approaches. ","Reconciling complexities: for a stronger integration of approaches to
  complex socio-technical systems"
35,1000508958518468608,811644390963748864,Mark Mace,['Very excited that our new paper on the system size dependence on correlations observed at @RHIC_PHENIX @BrookhavenLab is now out! Hope our timely study encourages further discussion <LINK>'],https://arxiv.org/abs/1805.09342,"We demonstrate that the striking systematics of two-particle azimuthal Fourier harmonics $v_2$ and $v_3$ in ultrarelativistic collisions of protons, deuterons and helium-3 ions off gold nuclei measured by the PHENIX Collaboration [arXiv:1805.02973] at the Relativistic Heavy Ion Collider (RHIC) is reproduced in the Color Glass Condensate (CGC) effective field theory. This contradicts the claim in [arXiv:1805.02973] that their data rules out initial state based explanations. The underlying systematics of the effect, as discussed previously in [arXiv:1705.00745,arXiv:1706.06260,arXiv:1801.09704], arise from the differing structure of strong color correlations between gluon domains of size $1/Q_S$ at fine ($p_\perp \gtrapprox Q_S$) or coarser ($p_\perp \lessapprox Q_S$) transverse momentum resolution. Further tests of the limits of validity of this framework can be carried out in light-heavy ion collisions at both RHIC and the Large Hadron Collider. Such measurements also offer novel opportunities for further exploration of the role of the surprisingly large short-range nuclear correlations measured at Jefferson Lab. ","Hierarchy of azimuthal anisotropy harmonics in collisions of small
  systems from the Color Glass Condensate"
36,1000032141055492097,22385548,Jane Wang,"['Our new paper on meta-learning with episodic recall, to be presented at ICML! Congrats to Sam Ritter, @sidfix, Zeb, and colleagues, and see you in Stockholm :) \n<LINK> <LINK>']",https://arxiv.org/abs/1805.09692,"Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur - as they do in natural environments - metalearning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems. ","Been There, Done That: Meta-Learning with Episodic Recall"
37,1000017909366906880,805133335794249729,Marcus Liwicki,['New research on novel learning techniques using Bidirectional Learning for improving robustness to white noise and adversarial examples is out at #MindGarage. Check out the latest research paper by Sidney Pontes Filho: <LINK>\n#deeplearning #ai'],https://arxiv.org/abs/1805.08006,"A multilayer perceptron can behave as a generative classifier by applying bidirectional learning (BL). It consists of training an undirected neural network to map input to output and vice-versa; therefore it can produce a classifier in one direction, and a generator in the opposite direction for the same data. The learning process of BL tries to reproduce the neuroplasticity stated in Hebbian theory using only backward propagation of errors. In this paper, two novel learning techniques are introduced which use BL for improving robustness to white noise static and adversarial examples. The first method is bidirectional propagation of errors, which the error propagation occurs in backward and forward directions. Motivated by the fact that its generative model receives as input a constant vector per class, we introduce as a second method the hybrid adversarial networks (HAN). Its generative model receives a random vector as input and its training is based on generative adversarial networks (GAN). To assess the performance of BL, we perform experiments using several architectures with fully and convolutional layers, with and without bias. Experimental results show that both methods improve robustness to white noise static and adversarial examples, and even increase accuracy, but have different behavior depending on the architecture and task, being more beneficial to use the one or the other. Nevertheless, HAN using a convolutional architecture with batch normalization presents outstanding robustness, reaching state-of-the-art accuracy on adversarial examples of hand-written digits. ",Bidirectional Learning for Robust Neural Networks
38,1000013162635292675,704799523,francesca dominici,['New paper led by Rachel Nethery and Fabrizia Mealli on how to handle non overlap in causal inference <LINK> @PennCausal @harvard_data @BayesianNetwork'],https://arxiv.org/abs/1805.09736,"Most causal inference studies rely on the assumption of overlap to estimate population or sample average causal effects. When data exhibit non-overlap, estimation of these estimands requires reliance on model specifications, due to poor data support. All existing methods to address non-overlap, such as trimming or down-weighting data in regions of poor support, change the estimand. In environmental health research, where study results are often intended to influence policy, changes in the estimand can diminish the study's impact, because estimates may not be representative of effects in the population of interest to policymakers. Researchers may be willing to make additional, minimal modeling assumptions in order to preserve the ability to estimate population average causal effects. We seek to make two contributions on this topic. First, we propose a flexible, data-driven definition of propensity score overlap and non-overlap regions. Second, we develop a novel Bayesian framework to estimate population average causal effects with minor model dependence and appropriately large uncertainties in the presence of non-overlap. In this approach, the tasks of estimating causal effects in the overlap and non-overlap regions are delegated to two distinct models, suited to the degree of data support in each region. Tree ensembles are used to non-parametrically estimate individual causal effects in the overlap region, where the data can speak for themselves. In the non-overlap region, where insufficient data support means reliance on model specification is necessary, individual causal effects are estimated by extrapolating trends from the overlap region via a spline model. The promising performance of our method is demonstrated in simulations. Finally, we utilize our method to perform a novel investigation of the causal effect of natural gas compressor station exposure on cancer outcomes. ","Estimating Population Average Causal Effects in the Presence of
  Non-Overlap: The Effect of Natural Gas Compressor Station Exposure on Cancer
  Mortality"
39,999970548255023104,4032064210,Sesh Nadathur,"['New paper out today: A Zeldovich reconstruction method for measuring redshift space distortions using cosmic voids\n\n<LINK> <LINK>', 'Redshift space distortions (RSD) are caused by galaxy velocities affecting the redshifts at which we observe them.\n\nTurning that around means measuring RSD allows us to measure galaxy velocities (statistically speaking).', 'Galaxy velocities are caused by gravity, so measuring the velocities allows us to determine the strength of gravity (via something called the growth rate).\n\nSo RSD in principle allows tests of the theory of gravity.', 'Using voids to do RSD measurements is interesting for two reasons: you test gravity in empty regions with little matter (which happens to be where some alternative theories differ from standard General Relativity); and, the theory is kind of simpler (and fun).', ""In fact, in simulations, the theory is so simple that a completely linear RSD model works on all scales for voids (though it doesn't in the usual case).\n\nI tweeted a bit about this before: https://t.co/TUHqlBprv6"", ""But there's a problem: how do you even find voids? Voids are regions with few galaxies. To find them you need to know where the galaxies are. But galaxy positions are distorted by the RSD you want to measure."", 'In fact we show that if you use galaxies in redshift space to identify voids, it completely messes up your theory, so nothing works any more. (Some other people had said part of this bit before: https://t.co/6C3lFIgmLy)', 'But all is not lost!\n\nThis paper presents a technique for finding the ""real space"" voids given only the ""redshift space"" galaxy information. And when we do this, theory matches the data again!\n\nFor example: https://t.co/ZPEkgVuGBU', ""And since theory now describes the data well again, that means were back on track – we CAN still use RSD with voids to measure the growth rate in low-density environments. \n\nIt's just a bit more fiddly to do it right."", 'I gave a talk about this paper in Oxford recently and someone said it was ""good old-fashioned physical cosmology"". I like that description – and it is true!\n\nBut its a new aspect of old-school cosmology, that can only be done with the best new data from big galaxy surveys.']",https://arxiv.org/abs/1805.09349,"Redshift space distortions (RSD) in the void-galaxy correlation $\xi^s$ provide information on the linear growth rate of structure in low density environments. Accurate modelling of these RSD effects can also allow the use of voids in competitive Alcock-Paczynski measurements. Linear theory models of $\xi^s$ are able to provide extremely good descriptions of simulation data on all scales provided the real space void positions are known. However, by reference to simulation data we demonstrate the failure of the assumptions implicit in current models of $\xi^s$ for voids identified directly in redshift space, as would be simplest using real observational data. To overcome this problem we instead propose using a density-field reconstruction method based on the Zeldovich approximation to recover the real space void positions from redshift space data. We show that this recovers the excellent agreement between theory and data for $\xi^s$. Performing the reconstruction requires an input cosmological model so, to be self-consistent, we have to perform reconstruction for every model to be tested. We apply this method to mock galaxy and void catalogues in the Big MultiDark $N$-body simulation and consistently recover the fiducial growth rate to a precision of $3.4\%$ using the simulation volume of $(2.5\;h^{-1}\mathrm{Gpc})^3$. ","A Zeldovich reconstruction method for measuring redshift space
  distortions using cosmic voids"
40,999964059016548352,197684961,Misha Denil,"['Hyperbolic Attention Networks (<LINK>) New paper with @caglarml and others at @DeepMindAI. We show how incorporating hyperbolic geometry into soft attention improves generalization on several tasks, including Neural Machine Translation and Visual QA.']",https://arxiv.org/abs/1805.09786,"We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact. ",Hyperbolic Attention Networks
41,999962137572397056,928106436,Sid Jayakumar,"['New paper with @janexwang and others @DeepMindAI on Meta-Learning in the presence of recurring tasks! Accepted to #icml2018 :)\n""Been There, Done That: Meta-Learning with Episodic Recall""\n<LINK>']",https://arxiv.org/abs/1805.09692,"Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur - as they do in natural environments - metalearning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems. ","Been There, Done That: Meta-Learning with Episodic Recall"
42,999848752709099526,96779364,Arnab Bhattacharyya,"['New paper! In <LINK>, we look at the problem of testing and learning #causal models by doing interventional experiments. Lots of prior work on such questions, but most in the spirit of classical stats, not algorithmic.', '@lreyzin Any specific pointers? Thanks!']",https://arxiv.org/abs/1805.09697,"We consider testing and learning problems on causal Bayesian networks as defined by Pearl (Pearl, 2009). Given a causal Bayesian network $\mathcal{M}$ on a graph with $n$ discrete variables and bounded in-degree and bounded `confounded components', we show that $O(\log n)$ interventions on an unknown causal Bayesian network $\mathcal{X}$ on the same graph, and $\tilde{O}(n/\epsilon^2)$ samples per intervention, suffice to efficiently distinguish whether $\mathcal{X}=\mathcal{M}$ or whether there exists some intervention under which $\mathcal{X}$ and $\mathcal{M}$ are farther than $\epsilon$ in total variation distance. We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph. Although our algorithms are non-adaptive, we show that adaptivity does not help in general: $\Omega(\log n)$ interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively. Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks. ",Learning and Testing Causal Models with Interventions
43,999649120276140032,3247059142,Graham Ganssle,['Interested in #DeepLearning and #physics? Check out our new paper on @arxiv: <LINK> <LINK>'],https://arxiv.org/abs/1805.08826,Traditional physics-based approaches to infer sub-surface properties such as full-waveform inversion or reflectivity inversion are time-consuming and computationally expensive. We present a deep-learning technique that eliminates the need for these computationally complex methods by posing the problem as one of domain transfer. Our solution is based on a deep convolutional generative adversarial network and dramatically reduces computation time. Training based on two different types of synthetic data produced a neural network that generates realistic velocity models when applied to a real dataset. The system's ability to generalize means it is robust against the inherent occurrence of velocity errors and artifacts in both training and test datasets. ,"Rapid seismic domain transfer: Seismic velocity inversion and modeling
  using deep generative neural networks"
44,999637399385837568,2303004390,Brandon Amos,"[""Here's a new paper on extending depth-limited search to imperfect information games by @polynoamial, T. Sandholm, and me. Using it we created a master HUNL poker AI that can run on a commodity 4-core CPU.\n\n<LINK> <LINK>""]",https://arxiv.org/abs/1805.08195,"A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer. ",Depth-Limited Solving for Imperfect-Information Games
45,999305395905712128,875724629951819776,Blei Lab,['New #arxiv paper by Y. Wang and D. Blei: The Blessings of Multiple Causes. <LINK>'],https://arxiv.org/abs/1805.06826,"Causal inference from observational data often assumes ""ignorability,"" that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects. ",The Blessings of Multiple Causes
46,999270184270606338,2766925212,Andrew Childs,['New paper today with Aaron Ostrander and @yuansu_umd shows how to speed up quantum simulation using randomized product formulas <LINK>'],http://arxiv.org/abs/1805.08385,"Product formulas can be used to simulate Hamiltonian dynamics on a quantum computer by approximating the exponential of a sum of operators by a product of exponentials of the individual summands. This approach is both straightforward and surprisingly efficient. We show that by simply randomizing how the summands are ordered, one can prove stronger bounds on the quality of approximation for product formulas of any given order, and thereby give more efficient simulations. Indeed, we show that these bounds can be asymptotically better than previous bounds that exploit commutation between the summands, despite using much less information about the structure of the Hamiltonian. Numerical evidence suggests that the randomized approach has better empirical performance as well. ",Faster quantum simulation by randomization
47,999257699484389376,932805374,KordingLab 🦖,"['The roles of machine learning in neuroscience: <LINK> new review paper with @joshuaiglaser. @arisbenjamin, @RoozbehFarhoodi Did we miss anything?', '@TheNeuralCoder @joshuaiglaser @arisbenjamin @RoozbehFarhoodi ok. We actually only cover supervised learning. Should have been more precise.']",https://arxiv.org/abs/1805.08239,"Over the last several years, the use of machine learning (ML) in neuroscience has been rapidly increasing. Here, we review ML's contributions, both realized and potential, across several areas of systems neuroscience. We describe four primary roles of ML within neuroscience: 1) creating solutions to engineering problems, 2) identifying predictive variables, 3) setting benchmarks for simple models of the brain, and 4) serving itself as a model for the brain. The breadth and ease of its applicability suggests that machine learning should be in the toolbox of most systems neuroscientists. ",The Roles of Supervised Machine Learning in Systems Neuroscience
48,999082765848055808,99270209,Guillermo Valle,"[""Finally!!! We released the paper with the work I've been doing for the last months! \nWe give a new perspective on why deep neural networks generalize, which I think it's quite interesting, and I think more promising than other approaches. I should write … <LINK>""]",https://arxiv.org/abs/1805.08522,"Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks applied to CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks. ","Deep learning generalizes because the parameter-function map is biased
  towards simple functions"
49,999063795422003202,2621989106,Yang Song,"['Have a look at our new paper on ""generative adversarial examples""! link: <LINK> <LINK>']",https://arxiv.org/abs/1805.07894,"Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks. ",Constructing Unrestricted Adversarial Examples with Generative Models
50,998824991096557568,776743573,Mauricio A. Álvarez,"['New paper with @cdguarnizo on ""Fast Kernel Approximations for Latent Force Models and Convolved Multiple-Output Gaussian processes"" Accepted at UAI 2018 @uai2018 <LINK>']",https://arxiv.org/abs/1805.07460,"A latent force model is a Gaussian process with a covariance function inspired by a differential operator. Such covariance function is obtained by performing convolution integrals between Green's functions associated to the differential operators, and covariance functions associated to latent functions. In the classical formulation of latent force models, the covariance functions are obtained analytically by solving a double integral, leading to expressions that involve numerical solutions of different types of error functions. In consequence, the covariance matrix calculation is considerably expensive, because it requires the evaluation of one or more of these error functions. In this paper, we use random Fourier features to approximate the solution of these double integrals obtaining simpler analytical expressions for such covariance functions. We show experimental results using ordinary differential operators and provide an extension to build general kernel functions for convolved multiple output Gaussian processes. ","Fast Kernel Approximations for Latent Force Models and Convolved
  Multiple-Output Gaussian processes"
51,998816947767795713,3124305238,Anna Scaife,['Our new paper on arXiv today uses recent measurements from @augerobs to place improved limits on intergalactic magnetic field strengths #ApJ #cosmicmagnetism <LINK> <LINK>'],http://arxiv.org/abs/1805.07995,"If ultra-high-energy cosmic rays originate from extragalactic sources, the offsets of their arrival directions from these sources imply an upper limit on the strength of the extragalactic magnetic field. The Pierre Auger Collaboration has recently reported that anisotropy in the arrival directions of cosmic rays is correlated with several types of extragalactic objects. If these cosmic rays originate from these objects, they imply a limit on the extragalactic magnetic field strength of B < 0.7-2.2 x 10^-9 (lambda_B / 1 Mpc)^-1/2 G for coherence lengths lambda_B < 100 Mpc and B < 0.7-2.2 x 10^-10 G at larger scales. This is comparable to existing upper limits at lambda_B = 1 Mpc, and improves on them by a factor 4-12 at larger scales. The principal source of uncertainty in our results is the unknown cosmic-ray composition. ","An upper limit on the strength of the extragalactic magnetic field from
  ultra-high-energy cosmic-ray anisotropy"
52,998673393875533825,19271798,Eilat Glikman,"['New paper out on astro-oh on the demographics of a complete sample of IR-selected, luminous AGN, including the obscured fraction and its evolution. <LINK>']",https://arxiv.org/abs/1805.06961,"We present a spectroscopically complete sample of 147 infrared-color-selected AGN down to a 22 $\mu$m flux limit of 20 mJy over the $\sim$270 deg$^2$ of the SDSS Stripe 82 region. Most of these sources are in the QSO luminosity regime ($L_{\rm bol} \gtrsim 10^{12} L_\odot$) and are found out to $z\simeq3$. We classify the AGN into three types, finding: 57 blue, unobscured Type-1 (broad-lined) sources; 69 obscured, Type-2 (narrow-lined) sources; and 21 moderately-reddened Type-1 sources (broad-lined and $E(B-V) > 0.25$). We study a subset of this sample in X-rays and analyze their obscuration to find that our spectroscopic classifications are in broad agreement with low, moderate, and large amounts of absorption for Type-1, red Type-1 and Type-2 AGN, respectively. We also investigate how their X-ray luminosities correlate with other known bolometric luminosity indicators such as [O III] line luminosity ($L_{\rm [OIII]}$) and infrared luminosity ($L_{6 \mu{\rm m}}$). While the X-ray correlation with $L_{\rm [OIII]}$ is consistent with previous findings, the most infrared-luminous sources appear to deviate from established relations such that they are either under-luminous in X-rays or over-luminous in the infrared. Finally, we examine the luminosity function (LF) evolution of our sample, and by AGN type, in combination with the complementary, infrared-selected, AGN sample of Lacy et al. (2013), spanning over two orders of magnitude in luminosity. We find that the two obscured populations evolve differently, with reddened Type-1 AGN dominating the obscured AGN fraction ($\sim$30%) for $L_{5 \mu{\rm m}} > 10^{45}$ erg s$^{-1}$, while the fraction of Type-2 AGN with $L_{5 \mu{\rm m}} < 10^{45}$ erg s$^{-1}$ rises sharply from 40% to 80% of the overall AGN population. ","Luminous WISE-selected Obscured, Unobscured, and Red Quasars in Stripe
  82"
53,998663776638586880,738769492122214400,Johannes Lischner,"['Finished new paper with @ArashMostofi on bound states of charged defects in transition metal dichalcogenides: <LINK>. We make lots of predictions, so it would be great if experimentalists could follow up on this! #nanotechnology <LINK>']",https://arxiv.org/abs/1805.02167,"Adsorbate engineering is a promising route for controlling the electronic properties of monolayer transition-metal dichalcogenide materials. Here, we study shallow bound states induced by charged adatoms on MoS$_2$ using large-scale tight-binding simulations with screened adatom potentials obtained from ab initio calculations. The interplay of unconventional screening in two-dimensional systems and multivalley effects in the transition-metal dichalcogenide (TMDC) band structure results in a rich diversity of bound impurity states. We present results for impurity state wavefunctions and energies, as well as for the local density of states in the vicinity of the adatom which can be measured using scanning tunnelling spectroscopy. We find that the presence of several distinct valleys in the MoS$_2$ band structure gives rise to crossovers of impurity states at critical charge strengths, altering the orbital character of the most strongly bound state. We compare our results to simpler methods, such as the 2D hydrogen atom and effective mass theory, and we discuss limitations of these approaches. ","Bound States of Charged Adatoms on MoS2: Screening and Multivalley
  Effects"
54,998616102975524864,938463903754862593,George Papamakarios,['New paper on likelihood-free inference with autoregressive flows!\n<LINK>'],https://arxiv.org/abs/1805.07226,"We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit. ","Sequential Neural Likelihood: Fast Likelihood-free Inference with
  Autoregressive Flows"
55,998614924938096645,1132031455,James O' Neill,"['New paper on Siamese Capsule Networks, demonstrated on face verification tasks - <LINK>']",https://arxiv.org/abs/1805.07242,"Capsule Networks have shown encouraging results on \textit{defacto} benchmark computer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are yet to be tested on tasks where (1) the entities detected inherently have more complex internal representations and (2) there are very few instances per class to learn from and (3) where point-wise classification is not suitable. Hence, this paper carries out experiments on face verification in both controlled and uncontrolled settings that together address these points. In doing so we introduce \textit{Siamese Capsule Networks}, a new variant that can be used for pairwise learning tasks. The model is trained using contrastive loss with $\ell_2$-normalized capsule encoded pose features. We find that \textit{Siamese Capsule Networks} perform well against strong baselines on both pairwise learning datasets, yielding best results in the few-shot learning setting where image pairs in the test set contain unseen subjects. ",Siamese Capsule Networks
56,998574545299619841,3040412699,Tim Carleton,"['You want UDGs, you got UDGs! — Our new paper (<LINK>) examines what happens to a population of cluster satellites experiencing tidal stripping and heating. These dwarfs end up looking a lot like the observed population of UDGs! <LINK>', 'In this model, UDGs live in cored ~10^10.5 halos. Tidally-stripped sats in cuspy halos don’t reproduce the UDG population.', 'Additionally, we note that dispersion-based mass estimators can be particularly biased for UDGs, as they have half-light radii comparable to (often greater than) the halo scale radius and stellar masses that contribute significantly to the total mass in the inner regions.', 'Also, we predict that larger UDGs (i.e. those that have experienced the most tidal stripping) should host the oldest stellar populations.']",https://arxiv.org/abs/1805.06896v1,"We propose that the Ultra-Diffuse Galaxy (UDG) population represents a set of satellite galaxies born in $\sim10^{10}-10^{11}~{\rm M_\odot}$ halos, similar to field dwarfs, which suffer a dramatic reduction in surface brightness due to tidal stripping and heating. This scenario is observationally motivated by the radial alignment of UDGs in Coma as well as the significant dependence of UDG abundance on cluster mass. As a test of this formation scenario, we apply a semi-analytic model describing the change in stellar mass and half-light radius of dwarf satellites, occupying either cored or cuspy halos, to cluster subhalos in the Bolshoi simulation. Key to this model are results from simulations which indicate that galaxies in cored dark-matter halos expand significantly in response to tidal stripping and heating, whereas galaxies in cuspy halos experience limited size evolution. Our analysis indicates that a population of tidally-stripped dwarf galaxies, residing in cored halos (like that of low-surface brightness field dwarfs), is able to reproduce the observed sizes, stellar masses, and abundance of UDGs in clusters remarkably well. ","] The Formation of Ultra Diffuse Galaxies in Cored Dark Matter Halos
  Through Tidal Stripping and Heating"
57,998552157459795968,335473253,Jason Kalirai,"['Very happy to finally post this new review paper on “Scientific Discovery with the James Webb Space Telescope”.  An in depth summary of how the telescope works and the incredible science it will soon return (Contemporary Physics, in press).  Enjoy!\n<LINK> #JWST']",https://arxiv.org/abs/1805.06941,"For the past 400 years, astronomers have sought to observe and interpret the Universe by building more powerful telescopes. These incredible instruments extend the capabilities of one of our most important senses, sight, towards new limits such as increased sensitivity and resolution, new dimensions such as exploration of wavelengths across the full electromagnetic spectrum, new information content such as analysis through spectroscopy, and new cadences such as rapid time-series views of the variable sky. The results from these investments, from small to large telescopes on the ground and in space, have completely transformed our understanding of the Universe; including the discovery that Earth is not the center of the Universe, that the Milky Way is one among many galaxies in the Universe, that relic cosmic background radiation fills all space in the early Universe, that that the expansion rate of the Universe is accelerating, that exoplanets are common around stars, that gravitational waves exist, and much more. For modern astronomical research, the next wave of breakthroughs in fields ranging over planetary, stellar, galactic, and extragalactic science motivate a general-purpose observatory that is optimized at near- and mid-infrared wavelengths, and that has much greater sensitivity, resolution, and spectroscopic multiplexing than all previous telescopes. This scientific vision, from measuring the composition of rocky worlds in the nearby Milky Way galaxy to finding the first sources of light in the Universe to other topics at the forefront of modern astrophysics, motivates the state-of-the-art James Webb Space Telescope (Webb). In this review paper, I summarize the design and technical capabilities of Webb and the scientific opportunities that it enables. ",Scientific Discovery with the James Webb Space Telescope
58,998516999805337600,50036150,Jacob Gildenblat,"['A new paper from Roche, the global leader in Pharma and Diagnostics, together with us at SagivTech, on Deep Learning based segmentation for Digital Pathology.\n@SagivTech\n<LINK> <LINK>']",https://arxiv.org/abs/1805.06958,"A key challenge in cancer immunotherapy biomarker research is quantification of pattern changes in microscopic whole slide images of tumor biopsies. Different cell types tend to migrate into various tissue compartments and form variable distribution patterns. Drug development requires correlative analysis of various biomarkers in and between the tissue compartments. To enable that, tissue slides are manually annotated by expert pathologists. Manual annotation of tissue slides is a labor intensive, tedious and error-prone task. Automation of this annotation process can improve accuracy and consistency while reducing workload and cost in a way that will positively influence drug development efforts. In this paper we present a novel one-shot color deconvolution deep learning method to automatically segment and annotate digitized slide images with multiple stainings into compartments of tumor, healthy tissue, and necrosis. We address the task in the context of drug development where multiple stains, tissue and tumor types exist and look into solutions for generalizations over these image populations. ","Generalizing multistain immunohistochemistry tissue segmentation using
  one-shot color deconvolution deep neural networks"
59,998450256382713856,988368254115680258,Lydia Brenner,"['Extremely happy with the publication of my new paper, together with Carsten Burgard. <LINK> <LINK>']",https://arxiv.org/abs/1805.07213,"A code package, BlurRing, is developed as a method to allow for multi-dimensional likelihood visualisation. From the BlurRing visualisation additional information about the likelihood can be extracted. The spread in any direction of the overlaid likelihood curves gives information about the uncertainty on the confidence intervals presented in the two-dimensional likelihood plots. ",BlurRing
60,998173861966372870,33113669,Tim Baldwin,"['New paper on predicting the outcome (votes + political intervention) of online petitions in UK/US based on original petition document, with S. Subramanian and the inimitable @trevorcohn: <LINK> @acl2018 #NLProc #weekendreading']",https://arxiv.org/abs/1805.06566,"Online petitions are a cost-effective way for citizens to collectively engage with policy-makers in a democracy. Predicting the popularity of a petition --- commonly measured by its signature count --- based on its textual content has utility for policy-makers as well as those posting the petition. In this work, we model this task using CNN regression with an auxiliary ordinal regression objective. We demonstrate the effectiveness of our proposed approach using UK and US government petition datasets. ","Content-based Popularity Prediction of Online Petitions Using a Deep
  Regression Model"
61,997353592783716352,18358070,정지훈 Jihoon Jeong 🗣,"['My advising startup <LINK>\'s new paper ""Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data""got accepted to ACL 2018 Multimodal Grand Challenge for Oral presentation <LINK> <LINK>']",https://arxiv.org/abs/1805.06606,"Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction. Previous works involve unimodal analysis of emotion, while recent efforts focus on multi-modal emotion recognition from vision and speech. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset. ","Convolutional Attention Networks for Multimodal Emotion Recognition from
  Speech and Text Data"
62,997184401606893569,3317340592,Eran Segal,['Regularization Network Learning: Our new paper on how to adapt Deep Neural Networks to settings of tabular data where the relative importance of different inputs can greatly vary\nWe show applications for predicting human traits from gut microbiome\n<LINK> <LINK>'],https://arxiv.org/abs/1805.06440,"Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at this https URL ",Regularization Learning Networks: Deep Learning for Tabular Datasets
63,997061956212146176,926386658819366912,Ira Shavitt,"['How to get Neural Networks to perform well on tabular data? Our new paper is out, introducing Regularization Learning Networks.\n<LINK> <LINK>']",https://arxiv.org/abs/1805.06440,"Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at this https URL ",Regularization Learning Networks: Deep Learning for Tabular Datasets
64,996984067169116161,892059194240532480,Mikel Artetxe,"['Check out our @acl2018 paper on ""A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings"" (w/ @glabaka &amp; @eagirre). New SOTA on unsupervised word translation, while more robust than previous adversarial approaches.\n<LINK> <LINK>', '@alex_conneau @acl2018 @glabaka @eagirre In our en-fi experiments on a public dataset, MUSE failed in the 10 runs we tried for 2 different settings. Anders Søgaard, @seb_ruder and @licwu also report negative results for MUSE on en-fi, see https://t.co/fOljApqymx']",https://arxiv.org/abs/1805.06297,"Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at this https URL ","A robust self-learning method for fully unsupervised cross-lingual
  mappings of word embeddings"
65,996964475784781825,33113669,Tim Baldwin,"['New paper on application of memory chains to ROC Story Cloze task (predict most probable story end) with semantic multi-task learning, with Felix Liu and @trevorcohn: <LINK> @acl2018 #NLProc']",https://arxiv.org/abs/1805.06122,"Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art. ",Narrative Modeling with Memory Chains and Semantic Supervision
66,996963792670998529,33113669,Tim Baldwin,"['(Related) new paper on using adversarial learning to debias models, with cool results on cross-domain POS tagging + sentiment analysis, with Yitong Li and @trevorcohn (once again): <LINK> @acl2018 #NLProc']",https://arxiv.org/abs/1805.06093,"Written text often provides sufficient clues to identify the author, their gender, age, and other important attributes. Consequently, the authorship of training and evaluation corpora can have unforeseen impacts, including differing model performance for different user groups, as well as privacy implications. In this paper, we propose an approach to explicitly obscure important author characteristics at training time, such that representations learned are invariant to these attributes. Evaluating on two tasks, we show that this leads to increased privacy in the learned representations, as well as more robust models to varying evaluation conditions, including out-of-domain corpora. ",Towards Robust and Privacy-preserving Text Representations
67,996962823371603968,33113669,Tim Baldwin,"['New paper on application of adversarial learning to multi-domain text classification, achieving state-of-art for language identification (bettering <LINK>) and sentiment analysis, with Yitong Li and @trevorcohn: <LINK> #NAACL2018 #NLProc']",https://arxiv.org/abs/1805.06088,"Most real world language problems require learning from heterogenous corpora, raising the problem of learning robust models which generalise well to both similar (in domain) and dissimilar (out of domain) instances to those seen in training. This requires learning an underlying task, while not learning irrelevant signals and biases specific to individual domains. We propose a novel method to optimise both in- and out-of-domain accuracy based on joint learning of a structured neural model with domain-specific and domain-general components, coupled with adversarial training for domain. Evaluating on multi-domain language identification and multi-domain sentiment analysis, we show substantial improvements over standard domain adaptation techniques, and domain-adversarial training. ","What's in a Domain? Learning Domain-Robust Text Representations using
  Adversarial Training"
68,996905913393995776,22168984,Nicholas FitzGerald,"['""Large-Scale QA-SRL Parsing"" -- new paper with Julian M., @LuhengH and @LukeZettlemoyer introduces a new 75k+ sentence QA-SRL dataset, and a high-quality QA-SRL parser: <LINK> / <LINK> to appear at @acl2018 !']",https://arxiv.org/abs/1805.05377,"We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost. ",Large-Scale QA-SRL Parsing
69,996807702725701632,23724401,Daniel Jiang,['Our new paper on using feedback-based MCTS for reinforcement learning (tested on a video game) is uploaded here: <LINK>. To be presented at #icml2018 @icmlconf! <LINK>'],https://arxiv.org/abs/1805.05935,"Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of artificial intelligence (AI) application domains, we propose a model-based reinforcement learning (RL) technique that iteratively applies MCTS on batches of small, finite-horizon versions of the original infinite-horizon Markov decision process. The terminal condition of the finite-horizon problems, or the leaf-node evaluator of the decision tree generated by MCTS, is specified using a combination of an estimated value function and an estimated policy function. The recommendations generated by the MCTS procedure are then provided as feedback in order to refine, through classification and regression, the leaf-node evaluator for the next iteration. We provide the first sample complexity bounds for a tree search-based RL algorithm. In addition, we show that a deep neural network implementation of the technique can create a competitive AI agent for the popular multi-player online battle arena (MOBA) game King of Glory. ",Feedback-Based Tree Search for Reinforcement Learning
70,996743354661687297,189992555,Luca Tortorelli,"['New paper out! Second paper of the 2018, first with the Cosmology group\n<LINK> #pausurvey #cosmology #galaxysurvey']",https://arxiv.org/abs/1805.05340,"Weak gravitational lensing is a powerful probe of the dark sector, once measurement systematic errors can be controlled. In Refregier & Amara (2014), a calibration method based on forward modeling, called MCCL, was proposed. This relies on fast image simulations (e.g., UFig; Berge et al. 2013) that capture the key features of galaxy populations and measurement effects. The MCCL approach has been used in Herbel et al. (2017) to determine the redshift distribution of cosmological galaxy samples and, in the process, the authors derived a model for the galaxy population mainly based on broad-band photometry. Here, we test this model by forward modeling the 40 narrow-band photometry given by the novel PAU Survey (PAUS). For this purpose, we apply the same forced photometric pipeline on data and simulations using Source Extractor (Bertin & Arnouts 1996). The image simulation scheme performance is assessed at the image and at the catalogues level. We find good agreement for the distribution of pixel values, the magnitudes, in the magnitude-size relation and the interband correlations. A principal component analysis is then performed, in order to derive a global comparison of the narrow-band photometry between the data and the simulations. We use a `mixing' matrix to quantify the agreement between the observed and simulated sets of Principal Components (PCs). We find good agreement, especially for the first three most significant PCs. We also compare the coefficients of the PCs decomposition. While there are slight differences for some coefficients, we find that the distributions are in good agreement. Together, our results show that the galaxy population model derived from broad-band photometry is in good overall agreement with the PAUS data. This offers good prospect for incorporating spectral information to the galaxy model by adjusting it to the PAUS narrow-band data using forward modeling. ",The PAU Survey: A Forward Modeling Approach for Narrow-band Imaging
71,996681125006364672,2560824361,sandro carrara,['#CMOS #Bio #Diagnostics Envisioning new paradigms in diagnostics by drinkable CMOS Bioelectronics! Read the new paper about Body Dust written with my good friend Pantelis (@pgeorgiou_ic ) and now available (in open access) in the ArXiv: <LINK> <LINK>'],https://arxiv.org/abs/1805.05840,"The aim of this paper is to introduce current advances in technology that could enable the development of fully drinkable and autonomous bio-electronic CMOS sensors in the form of dust particles, capable of identifying the source of a disease by targeting a specific region in organs and tissue such as a tumor mass and automatically sending diagnostic information wirelessly outside the body. We call this swarm of sensing dust particles Body Dust. A diagnostic system in the form of Body Dust would need to be small enough to support free circulation in human tissues, which requires a total size of less than 10 um3, in order to mimic the typical sizes of a blood cell (e.g., red cells have the diameter around 7 {\mu}m). Whilst with present state-of-the-art in CMOS technology, this requirement in terms of size is currently un-feasible, recent research has advanced technology such that we can begin to work towards such an approach. Therefore, we present here the current limits of CMOS technology as well as the challenges related to the development of such a system. Towards this goal, this article presents the theoretical feasibility to obtain the first ever-conceived sub-10-um Bio/CMOS integrated circuit with biosensing capability to provide diagnostic telemetry once self-located in human tissue. ","Body Dust: Miniaturized Highly-integrated Low Power Sensing for Remotely
  Powered Drinkable CMOS Bioelectronics"
72,996652416836820992,2432886163,Oscar Barragán,['We just characterized a new planet!\n\nI present you the super-Earth K2-216 b.\n\npaper here -&gt; <LINK> <LINK>'],https://arxiv.org/abs/1805.04774,"The KESPRINT consortium identified K2-216 as a planetary candidate host star in the K2 space mission Campaign 8 field with a transiting super-Earth. The planet has recently been validated as well. Our aim was to confirm the detection and derive the main physical characteristics of K2-216b, including the mass. We performed a series of follow-up observations: high resolution imaging with the FastCam camera at the TCS, the Infrared Camera and Spectrograph at Subaru, and high resolution spectroscopy with HARPS (ESO, La Silla), HARPS-N (TNG), and FIES (NOT). The stellar spectra were analyzed with the SpecMatch-Emp and SME codes to derive the stellar fundamental properties. We analyzed the K2 light curve with the Pyaneti software. The radial-velocity measurements were modelled with both a Gaussian process (GP) regression and the floating chunk offset (FCO) technique to simultaneously model the planetary signal and correlated noise associated with stellar activity. Imaging confirms that K2-216 is a single star. Our analysis discloses that the star is a moderately active K5V star of mass 0.70+/-0.03 Msun and radius 0.72+/-0.03 Rsun. Planet b is found to have a radius of 1.75+0.17-0.10 Rearth and a 2.17-day orbit in agreement with previous results. We find consistent results for the planet mass from both models: 7.4+/-2.2 Mearth from the GP regression, and 8.0+/-1.6 Mearth from the FCO technique, which implies that this planet is a super-Earth. The planet parameters put planet b in the middle of, or just below, the gap of the radius distribution of small planets. The density is consistent with a rocky composition of primarily iron and magnesium silicate. In agreement with theoretical predictions, we find that the planet is a remnant core, stripped of its atmosphere, and is one of the largest planets found that has lost its atmosphere. ",Super-Earth of 8 Mearth in a 2.2-day orbit around the K5V star K2-216
73,996399624536633349,384900803,Shantanu Basu,['Our latest paper is a collaboration between @TUChemnitz @westernuAPMaths and @westernuPhysAst . New model for the stellar initial mass function. @westernuScience #astronomy <LINK>'],http://arxiv.org/abs/1805.05268,"We introduce a new dual power law (DPL) probability distribution function for the mass distribution of stellar and substellar objects at birth, otherwise known as the initial mass function (IMF). The model contains both deterministic and stochastic elements, and provides a unified framework within which to view the formation of brown dwarfs and stars resulting from an accretion process that starts from extremely low mass seeds. It does not depend upon a top down scenario of collapsing (Jeans) masses or an initial lognormal or otherwise IMF-like distribution of seed masses. Like the modified lognormal power law (MLP) distribution, the DPL distribution has a power law at the high mass end, as a result of exponential growth of mass coupled with equally likely stopping of accretion at any time interval. Unlike the MLP, a power law decay also appears at the low mass end of the IMF. This feature is closely connected to the accretion stopping probability rising from an initially low value up to a high value. This might be associated with physical effects of ejections sometimes (i.e., rarely) stopping accretion at early times followed by outflow driven accretion stopping at later times, with the transition happening at a critical time (therefore mass). Comparing the DPL to empirical data, the critical mass is close to the substellar mass limit, suggesting that the onset of nuclear fusion plays an important role in the subsequent accretion history of a young stellar object. ",A Dual Power Law Distribution for the Stellar Initial Mass Function
74,996399260898877442,384900803,Shantanu Basu,['Our latest paper with a new idea for the origin of the peak of the stellar mass distribution - the IMF. From a very satisfying interdisciplinary collaboration with @westernuAPMaths Chris Essex and Karl Heinz Hoffmann and Janett Prehl of @TUChemnitz Germany <LINK>'],http://arxiv.org/abs/1805.05268,"We introduce a new dual power law (DPL) probability distribution function for the mass distribution of stellar and substellar objects at birth, otherwise known as the initial mass function (IMF). The model contains both deterministic and stochastic elements, and provides a unified framework within which to view the formation of brown dwarfs and stars resulting from an accretion process that starts from extremely low mass seeds. It does not depend upon a top down scenario of collapsing (Jeans) masses or an initial lognormal or otherwise IMF-like distribution of seed masses. Like the modified lognormal power law (MLP) distribution, the DPL distribution has a power law at the high mass end, as a result of exponential growth of mass coupled with equally likely stopping of accretion at any time interval. Unlike the MLP, a power law decay also appears at the low mass end of the IMF. This feature is closely connected to the accretion stopping probability rising from an initially low value up to a high value. This might be associated with physical effects of ejections sometimes (i.e., rarely) stopping accretion at early times followed by outflow driven accretion stopping at later times, with the transition happening at a critical time (therefore mass). Comparing the DPL to empirical data, the critical mass is close to the substellar mass limit, suggesting that the onset of nuclear fusion plays an important role in the subsequent accretion history of a young stellar object. ",A Dual Power Law Distribution for the Stellar Initial Mass Function
75,996339013761069056,17055506,Martin Kleppmann,['New paper preprint: “OpSets: Sequential Specifications for Replicated Datatypes” —\xa0our latest work on proving CRDT correctness <LINK>'],https://arxiv.org/abs/1805.04263,"We introduce OpSets, an executable framework for specifying and reasoning about the semantics of replicated datatypes that provide eventual consistency in a distributed system, and for mechanically verifying algorithms that implement these datatypes. Our approach is simple but expressive, allowing us to succinctly specify a variety of abstract datatypes, including maps, sets, lists, text, graphs, trees, and registers. Our datatypes are also composable, enabling the construction of complex data structures. To demonstrate the utility of OpSets for analysing replication algorithms, we highlight an important correctness property for collaborative text editing that has traditionally been overlooked; algorithms that do not satisfy this property can exhibit awkward interleaving of text. We use OpSets to specify this correctness property and prove that although one existing replication algorithm satisfies this property, several other published algorithms do not. We also show how OpSets can be used to develop new replicated datatypes: we provide a simple specification of an atomic move operation for trees, an operation that had previously been thought to be impossible to implement without locking. We use the Isabelle/HOL proof assistant to formalise the OpSets approach and produce mechanised proofs of correctness of the main claims in this paper, thereby eliminating the ambiguity of previous informal approaches, and ruling out reasoning errors that could occur in handwritten proofs. ","OpSets: Sequential Specifications for Replicated Datatypes (Extended
  Version)"
76,996294880212213760,176191683,Olga Zamora,"['Our new @APOGEEsurvey paper is devoted to an amazing stellar nursery: Orion star forming complex\n<LINK> @J_G_FERNANDEZ <LINK>', '@astro_mir ¿ya lo chequeaste? 😁']",https://arxiv.org/abs/1805.04649,"We present an analysis of spectrosopic and astrometric data from APOGEE-2 and Gaia DR2 to identify structures towards the Orion Complex. By applying a hierarchical clustering algorithm to the 6-dimensional stellar data, we identify spatially and/or kinematically distinct groups of young stellar objects with ages ranging from 1 to 12 Myr. We also investigate the star forming history within the Orion Complex, and identify peculiar sub-clusters. With this method we reconstruct the older populations in the regions that are presently largely devoid of molecular gas, such as Orion C (which includes the $\sigma$ Ori cluster), and Orion D (the population that traces Ori OB1a, OB1b, and Orion X). We report on the distances, kinematics, and ages of the groups within the Complex. The Orion D groups is in the process of expanding. On the other hand, Orion B is still in the process of contraction. In $\lambda$ Ori the proper motions are consistent with a radial expansion due to an explosion from a supernova; the traceback age from the expansion exceeds the age of the youngest stars formed near the outer edges of the region, and their formation would have been triggered when they were half-way from the cluster center to their current positions. We also present a comparison between the parallax and proper motion solutions obtained by Gaia DR2, and those obtained towards star-forming regions by Very Long Baseline Array. ","The APOGEE-2 Survey of the Orion Star Forming Complex II:
  Six-dimensional structure"
77,996292393640001536,523241142,Juste Raimbault,"['New paper: An Urban Morphogenesis Model Capturing Interactions between Networks and  Territories\n<LINK>\n[the mathiest part of my thesis, unfortunately still very far from real maths :/ ]']",https://arxiv.org/abs/1805.05195,"Urban systems are composed by complex couplings of several components, and more particularly between the built environment and transportation networks. Their interaction is involved in the emergence of the urban form. We propose in this chapter to introduce an approach to urban morphology grasping both aspects and their interaction. We first define complementary measures, study their empirical values and their spatial correlations on European territorial systems. The behavior of indicators and correlations suggest underlying non-stationary and multi-scalar processes. We then introduce a generative model of urban growth at a mesoscopic scale. Given a fixed exogenous growth rate, population is distributed following a preferential attachment depending on a potential controlled by the local urban form (density, distance to network) and network measures (centralities and generalized accessibilities), and then diffused in space to capture urban sprawl. Network growth is included through a multi-modeling paradigm: implemented heuristics include biological network generation and gravity potential breakdown. The model is calibrated both at the first (measures) and second (correlations) order, the later capturing indirectly relations between networks and territories. ","An Urban Morphogenesis Model Capturing Interactions between Networks and
  Territories"
78,996006570868822016,364410282,Kevin H. Wilson,"['Old project new paper acceptance! ""Using Machine Learning to Assess the Risk of and Prevent Water Main Breaks"" @ KDD. <LINK> 1/', 'This was a project at @datascifellows. We had a _lot_ of co-authors (which I think is awesome). @samedelstein, @adriaf, @CKenneyJr, @rayidghani, Avishek Kumar, Ali Rizvi, Ben Brooks, Ali Vanderveld, Andrew Maxwell, and Joe Zuckerbraun 2/', ""Water main breaks are a huge problem in Rust Belt cities. The infrastructure was built assuming a growing population and growing suburanization. The tax base hasn't kept up with original projections, so maintenance $$$s are declining. 3/ https://t.co/atEYdRrgdf"", ""On the other hand, people aren't moving into the city core, so cities can't simply decomission parts of their infrastructure without cutting water off to residents. The result? https://t.co/3a8Nx8a8Tw 4/"", 'We also know that preventive maintenance is chepar than reactive maintenance. A bunch of restaurants with no water for a day is a _really bad thing_. So how should we order maintenance? 5/', ""A first pass indicates that if you just replaced 50 pipes that have broken in the past, you would stop about 24 water main breaks. This indicates that the way that pipes are temporarily repaired doesn't hold for very long. 6/"", 'On the other hand, a lot of pipes have broken in the past. Can we do even better? Accounting for pipe age, diameter, and material, as well as the surrounding soil type, we could go from preventing 24 breaks to 31 breaks / 50 repaired. 7/ https://t.co/V1Pj6Picq4', ""The methods we used (GBDT, a little clever GIS trickery) aren't revolutionary. The key part of this project was that it took subject matter experts in data science, local government, and water maintenance and asked what they could do. 8/"", 'In particular, our data was very messy. When water mains were laid, meticulous notes were kept about them. But they were laid in the late 1800s, so those notes look like this: 9/ https://t.co/jtyUAlDOFI', 'Without Sam, Adria, Joe, and Andrew, this sort of data would never have been able to go anywhere. 10/', ""Nowadays I'm doing more of this sort of work with @TheLab_DC, finding housing violations, getting people enrolled for benefits, and tackling the rat problem in the District. If you're interested in this kind of work, hit me up. 11/11""]",https://arxiv.org/abs/1805.03597,"Water infrastructure in the United States is beginning to show its age, particularly through water main breaks. Main breaks cause major disruptions in everyday life for residents and businesses. Water main failures in Syracuse, N.Y. (as in most cities) are handled reactively rather than proactively. A barrier to proactive maintenance is the city's inability to predict the risk of failure on parts of its infrastructure. In response, we worked with the city to build a ML system to assess the risk of a water mains breaking. Using historical data on which mains have failed, descriptors of pipes, and other data sources, we evaluated several models' abilities to predict breaks three years into the future. Our results show that our system using gradient boosted decision trees performed the best out of several algorithms and expert heuristics, achieving precision at 1\% (P@1) of 0.62. Our model outperforms a random baseline (P@1 of 0.08) and expert heuristics such as water main age (P@1 of 0.10) and history of past main breaks (P@1 of 0.48). The model is deployed in the City of Syracuse. We are running a pilot by calculating the risk of failure for each city block over the period 2016-2018 using data up to the end of 2015 and, as of the end of 2017, there have been 33 breaks on our riskiest 52 mains. This has been a successful initiative for the city of Syracuse in improving their infrastructure and we believe this approach can be applied to other cities. ","Using Machine Learning to Assess the Risk of and Prevent Water Main
  Breaks"
79,995948699519012865,822867138,Bradley Kavanagh,"['New paper today w/ @tedwards2412 and @C_Weniger: <LINK>. \n\nNew ""benchmark-free"" methods for understanding which parts of #DarkMatter parameter space can be explored and discriminated by future experiments.\n\nWe focus on direct detection, but can apply to anything! <LINK>', ""Technique allows us to do projections for future experiments more quickly and comprehensively over large parameter spaces.\n\nWe map out regions where DM interaction and mass can be discriminated using future Xe and Ar detectors. You can only discriminate both if you're lucky! https://t.co/NoB73WQfNS""]",https://arxiv.org/abs/1805.04117,"Forecasting the signal discrimination power of dark matter (DM) searches is commonly limited to a set of arbitrary benchmark points. We introduce new methods for benchmark-free forecasting that instead allow an exhaustive exploration and visualization of the phenomenological distinctiveness of DM models, based on standard hypothesis testing. Using this method, we reassess the signal discrimination power of future liquid Xenon and Argon direct DM searches. We quantify the parameter regions where various non-relativistic effective operators, millicharged DM, and magnetic dipole DM can be discriminated, and where upper limits on the DM mass can be found. We find that including an Argon target substantially improves the prospects for reconstructing the DM properties. We also show that only in a small region with DM masses in the range 20-100 GeV and DM-nucleon cross sections a factor of a few below current bounds can near-future Xenon and Argon detectors discriminate both the DM-nucleon interaction and the DM mass simultaneously. In all other regions only one or the other can be obtained. ","Dark Matter Model or Mass, but Not Both: Assessing Near-Future Direct
  Searches with Benchmark-free Forecasting"
80,994973617334161410,875724629951819776,Blei Lab,"['.@adjiboussodieng , R. Ranganath, @thejaan and David Blei developed ""Noisin"" a new method  for regularizing Recurrent Neural Networks. The Arxiv version of  the paper is up: <LINK>']",https://arxiv.org/abs/1805.01500,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance. ",Noisin: Unbiased Regularization for Recurrent Neural Networks
81,994874217328979971,952949678533849088,Kareem El-Badry,"[""1/: Why is the ratio between total globular cluster mass and halo mass constant? In a new paper, we show that thanks to the central limit theorem, it's almost impossible for the GC-to-halo mass ratio to *not* be constant if GCs are moderately old. <LINK> <LINK>"", ""2/: If you've worked on models to explain the black hole M-sigma relation, this should sound familiar. We think this picture is actually *more* applicable to GCs, because while black holes can grow through both mergers and gas accretion, GC ages suggest most formed before z=2. https://t.co/jxCj74xsWo"", ""3/: In low-mass halos (Mhalo &lt; 10^11 Msun or so), mergers alone can't enforce a constant GC-to-halo mass ratio. We therefore suggest that the GC populations of low-mass halos retain the most information about the GC formation process."", '4/: We predict other properties of observed GC populations, such as age, metallicity, and color distributions. We argue that these are also in large part set by hierarchical assembly and are only weakly sensitive to the GC formation process. \nalso featuring @bigticketdw @MBKplus https://t.co/5Oe6A1WeK4']",https://arxiv.org/abs/1805.03652,"We use a semi-analytic model for globular cluster (GC) formation built on dark matter merger trees to explore the relative role of formation physics and hierarchical assembly in determining the properties of GC populations. Many previous works have argued that the observed linear relation between total GC mass and halo mass points to a fundamental GC -- dark matter connection or indicates that GCs formed at very high redshift before feedback processes introduced nonlinearity in the baryon-to-dark matter mass relation. We demonstrate that at $M_{\rm vir}(z=0) \gtrsim 10^{11.5} M_{\odot}$, a constant ratio between halo mass and total GC mass is in fact an almost inevitable consequence of hierarchical assembly: by the central limit theorem, it is expected at $z=0$ independent of the GC-to-halo mass relation at the time of GC formation. The GC-to-halo mass relation at $M_{\rm vir}(z=0) < 10^{11.5} M_{\odot}$ is more sensitive to the details of the GC formation process. In our fiducial model, GC formation occurs in galaxies when the gas surface density exceeds a critical value. This model naturally predicts bimodal GC color distributions similar to those observed in nearby galaxies and reproduces the observed relation between GC system metallicity and halo mass. It predicts that the cosmic GC formation rate peaked at $z$ $\sim$ 4, too late for GCs to contribute significantly to the UV luminosity density during reionization. ",The formation and hierarchical assembly of globular cluster populations
82,994621565974335489,289347679,Dr. Axiel Yaël Birenbaum,"['Something funny happens when you put high T_C superconductor monolayer FeSe on SrTiO3: a van der Walls sandwich forms with an emerging extra layer of TiOx in the role of salami!\n\nCheck my new paper out: <LINK> #RealLifeScientist research done @ORNL @VanderbiltU', 'First author is @greatconcavity (sorry for the late addition, I forgot you had a Twitter account)', 'Check it out, and learn about the impact it has on the band structures. Perhaps you can help with elucidating how it impacts the #superconductivity? https://t.co/tWk7awN8MN']",https://arxiv.org/abs/1805.03293,"The sensitive dependence of monolayer materials on their environment often gives rise to unexpected properties. It was recently demonstrated that monolayer FeSe on a SrTiO$_3$ substrate exhibits a much higher superconducting critical temperature T$_C$ than the bulk material. Here, we examine the interfacial structure of FeSe / SrTiO$_3$ and the effect of an interfacial Ti$_{1+x}$O$_2$ layer on the increased T$_C$ using a combination of scanning transmission electron microscopy and density functional theory. We find Ti$_{1+x}$O$_2$ forms its own quasi-two-dimensional layer, bonding to both the substrate and the FeSe film by van der Waals interactions. The excess Ti in this layer electron-dopes the FeSe monolayer in agreement with experimental observations. Moreover, the interfacial layer introduces symmetry-breaking distortions in the FeSe film that favor a T$_C$ increase. These results suggest that this common substrate may be functionalized to modify the electronic structure of a variety of thin films and monolayers. ","Intrinsic interfacial van der Waals monolayers and their effect on the
  high-temperature superconductor FeSe/SrTiO$_3$"
83,994491439722754048,20438952,Martyn Amos,['New paper: solving #Sudoku using Ant Colony Optimisation #ACO <LINK>'],https://arxiv.org/abs/1805.03545,"In this paper we present a new Ant Colony Optimisation-based algorithm for Sudoku, which out-performs existing methods on large instances. Our method includes a novel anti-stagnation operator, which we call Best Value Evaporation. ",Solving Sudoku with Ant Colony Optimisation
84,994381951254413312,930764003277643777,Matias Quiroz,"['Stepping into the Sequential Monte Carlo game with our new paper on arXiv <LINK>, Subsampling SMC for static Bayesian models.', '@SalomoneRob Thanks mate. Looking forward to see what you have done. Haha yeah Feynman-Kac, looking forward to some of that.']",https://arxiv.org/abs/1805.03317,"We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference in large data problems by data subsampling. SMC sequentially updates a cloud of particles through a sequence of distributions, beginning with a distribution that is easy to sample from such as the prior and ending with the posterior distribution. Each update of the particle cloud consists of three steps: reweighting, resampling, and moving. In the move step, each particle is moved using a Markov kernel; this is typically the most computationally expensive part, particularly when the dataset is large. It is crucial to have an efficient move step to ensure particle diversity. Our article makes two important contributions. First, in order to speed up the SMC computation, we use an approximately unbiased and efficient annealed likelihood estimator based on data subsampling. The subsampling approach is more memory efficient than the corresponding full data SMC, which is an advantage for parallel computation. Second, we use a Metropolis within Gibbs kernel with two conditional updates. A Hamiltonian Monte Carlo update makes distant moves for the model parameters, and a block pseudo-marginal proposal is used for the particles corresponding to the auxiliary variables for the data subsampling. We demonstrate both the usefulness and limitations of the methodology for estimating four generalized linear models and a generalized additive model with large datasets. ",Subsampling Sequential Monte Carlo for Static Bayesian Models
85,994353038205702144,33113669,Tim Baldwin,"['New paper on predicting political ""leaning"" of party based on manifesto, based on multi-task structured learning + probabilistic soft logic, with S. Subramanian and @trevorcohn: <LINK> (to appear @NAACLHLT) #NLProc']",https://arxiv.org/abs/1805.02823,"Election manifestos document the intentions, motives, and views of political parties. They are often used for analysing a party's fine-grained position on a particular issue, as well as for coarse-grained positioning of a party on the left--right spectrum. In this paper we propose a two-stage model for automatically performing both levels of analysis over manifestos. In the first step we employ a hierarchical multi-task structured deep model to predict fine- and coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic. We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages. ",Hierarchical Structured Model for Fine-to-coarse Manifesto Text Analysis
86,994219534440480772,22148802,Leo C. Stein 🦁,"['ICYMI: New paper on the arXiv today, ""Black hole scalar charge from a topological horizon integral in Einstein-dilaton-Gauss-Bonnet gravity"", from Prabhu and me. Hope you like it! <LINK> <LINK>']",https://arxiv.org/abs/1805.02668,"In theories of gravity that include a scalar field, a compact object's scalar charge is a crucial quantity since it controls dipole radiation, which can be strongly constrained by pulsar timing and gravitational wave observations. However in most such theories, computing the scalar charge requires simultaneously solving the coupled, nonlinear metric and scalar field equations of motion. In this article we prove that in linearly-coupled Einstein-dilaton-Gauss-Bonnet gravity, a black hole's scalar charge is completely determined by the horizon surface gravity times the Euler characteristic of the bifurcation surface, without solving any equations of motion. Within this theory, black holes announce their horizon topology and surface gravity to the rest of the universe through the dilaton field. In our proof, a 4-dimensional topological density descends to a 2-dimensional topological density on the bifurcation surface of a Killing horizon. We also comment on how our proof can be generalized to other topological densities on general G-bundles, and to theories where the dilaton is non-linearly coupled to the Euler density. ","Black hole scalar charge from a topological horizon integral in
  Einstein-dilaton-Gauss-Bonnet gravity"
87,994206294197374977,722834188966211585,Thomio Watanabe,"['New paper: ""Verisimilar Percept Sequences Tests for Autonomous Driving Intelligent Agent Assessment""\n<LINK>']",https://arxiv.org/abs/1805.02754,The autonomous car technology promises to replace human drivers with safer driving systems. But although autonomous cars can become safer than human drivers this is a long process that is going to be refined over time. Before these vehicles are deployed on urban roads a minimum safety level must be assured. Since the autonomous car technology is still under development there is no standard methodology to evaluate such systems. It is important to completely understand the technology that is being developed to design efficient means to evaluate it. In this paper we assume safety-critical systems reliability as a safety measure. We model an autonomous road vehicle as an intelligent agent and we approach its evaluation from an artificial intelligence perspective. Our focus is the evaluation of perception and decision making systems and also to propose a systematic method to evaluate their integration in the vehicle. We identify critical aspects of the data dependency from the artificial intelligence state of the art models and we also propose procedures to reproduce them. ,"Verisimilar Percept Sequences Tests for Autonomous Driving Intelligent
  Agent Assessment"
88,994126958484164609,2255372478,Maxim Milakov,"['Make your Softmax faster! See our new paper ""Online normalizer calculation for softmax"" <LINK>']",https://arxiv.org/abs/1805.02867,"The Softmax function is ubiquitous in machine learning, multiple previous works suggested faster alternatives for it. In this paper we propose a way to compute classical Softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve Softmax performance on actual hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to 1.3x and Softmax+TopK combined and fused by up to 5x. ",Online normalizer calculation for softmax
89,993904173799411720,14060641,Paul Guinnessy,['New paper about a cubesat detecting an asteroid. There are some interesting possibilities here. <LINK>'],https://arxiv.org/abs/1805.01102,We report results from analyzing the B612 asteroid observation data taken by the sCMOS cameras on board of Planet SkySat-3 using the synthetic tracking technique. The analysis demonstrates the expected sensitivity improvement in the signal-to-noise ratio of the asteroids from properly stacking up the the short exposure images in post-processing. ,"Technical Note: Asteroid Detection Demonstration from SkySat-3 B612 Data
  using Synthetic Tracking"
90,993816861258248194,953616889,Justin Read,"['My first #Gaia paper is out today! Lovely work by Ioana Ciucă hunting for dwarfs associated with Fermi sources using #GaiaDR2. She found known star clusters, but no new dwarfs. This means, sadly, that the Fermi sources are not due to DM annihilation.\n\n<LINK>']",https://arxiv.org/abs/1805.02588,"We make a first attempt to find dwarf galaxies in eight \Fermi-LAT extended, unassociated, source fields using \Gaia\ DR2. We probe previously unexplored heliocentric distances of $d<20$~kpc with an extreme-deconvolution (XD) technique. We find no signature of a dwarf galaxy in any of these fields despite \Gaia's excellent astrometric accuracy. We estimate our detection limits by applying the XD method to mock data, obtaining a conservative limit on the stellar mass of $M_* < 10^4$~M$_{\sun}$ for $d < 20$\, kpc. Such a low stellar mass implies either a low-mass subhalo, or a massive stripped-down subhalo. We use an analytic model for stripped subhalos to argue that, given the sizes and fluxes of the \Fermi-LAT sources, we can reject the hypothesis that they owe to dark matter annihilation. ","A Gaia DR2 search for dwarf galaxies towards Fermi-LAT sources:
  implications for annihilating dark matter"
91,993679048692805632,2569631268,Daniel Huber,"[""New paper up today confirming a systematic offset of #GaiaDR2 parallaxes using the power of asteroseismology (and spectroscopy) in the #Kepler field. A heroic effort by @joelczinn and there's more to come! <LINK>""]",https://arxiv.org/abs/1805.02650,"We present an independent confirmation of the zero-point offset of ${\rm \it Gaia}$ Data Release 2 (DR2) parallaxes using asteroseismic data of evolved stars in the ${\rm \it Kepler}$ field. Using well-characterized red giant branch (RGB) stars from the APOKASC-2 catalogue we identify a ${\rm \it Gaia}$ astrometric pseudo-color ($\nu_{\rm eff}$)- and ${\rm \it Gaia}$ $G$-band magnitude-dependent zero-point offset of $\varpi_{\rm seis} - \varpi_{Gaia} = 52.8 \pm 2.4 {\rm\ (rand.)} \pm 8.6 {\rm\ (syst.)} - (150.7 \pm 22.7)(\nu_{\rm eff} - 1.5) - (4.21 \pm 0.77)(G - 12.2) \mu{\rm as}$, in the sense that ${\rm \it Gaia}$ parallaxes are too small. The offset is found in high and low-extinction samples, as well as among both shell H-burning red giant stars and core He-burning red clump stars. We show that errors in the asteroseismic radius and temperature scales may be distinguished from errors in the ${\rm \it Gaia}$ parallax scale. We estimate systematic effects on the inferred global ${\rm \it Gaia}$ parallax offset, $c$, due to radius and temperature systematics, as well as choices in bolometric correction and the adopted form for ${\rm \it Gaia}$ parallax spatial correlations. Because of possible spatially-correlated parallax errors, as discussed by the ${\rm \it Gaia}$ team, our ${\rm \it Gaia}$ parallax offset model is specific to the ${\rm \it Kepler}$ field, but broadly compatible with the magnitude- and color-dependent offset inferred by the ${\rm \it Gaia}$ team and several subsequent investigations using independent methods. ","Confirmation of the ${\rm \it Gaia}$ DR2 parallax zero-point offset
  using asteroseismology and spectroscopy in the ${\rm \it Kepler}$ field"
92,993649782538227712,724173367645974529,Naoya Takahashi,"['<LINK>\nOur new paper on a novel network architecture ""MMDenseLSTM"" is now available. Got the best results and even outperformed some oracle methods on music source separation at #SiSEC2018']",https://arxiv.org/abs/1805.02410,"Deep neural networks have become an indispensable technique for audio source separation (ASS). It was recently reported that a variant of CNN architecture called MMDenseNet was successfully employed to solve the ASS problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD100 dataset. To further enhance MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip connections to efficiently model long-term structures within an audio context. The experimental results show that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters and processing time of the proposed model are significantly less than those for simple blending. Furthermore, the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task. ","MMDenseLSTM: An efficient combination of convolutional and recurrent
  neural networks for audio source separation"
93,993377963772207109,2441942726,Aleksas Mazeliauskas,['A brief summary of our long paper and some new predictions on QGP equilibration in <LINK>'],https://arxiv.org/abs/1805.01604,"High-energy nuclear collisions produce a nonequilibrium plasma of quarks and gluons which thermalizes and exhibits hydrodynamic flow. There are currently no practical frameworks to connect the early particle production in classical field simulations to the subsequent hydrodynamic evolution. We build such a framework using nonequilibrium Green's functions, calculated in QCD kinetic theory, to propagate the initial energy-momentum tensor to the hydrodynamic phase. We demonstrate that this approach can be easily incorporated into existing hydrodynamic simulations, leading to stronger constraints on the energy density at early times and the transport properties of the QCD medium. Based on (conformal) scaling properties of the Green's functions, we further obtain pragmatic bounds for the applicability of hydrodynamics in nuclear collisions. ","Matching the Nonequilibrium Initial Stage of Heavy Ion Collisions to
  Hydrodynamics with QCD Kinetic Theory"
94,993318393204719616,2416760538,Peter Gao,"['New paper out tonight focusing on cloud microphysics on Hot Jupiters led by the brilliant @DianaPowell8! Xi Zhang, @V_Parmentier, and myself are the coauthors <LINK>']",https://arxiv.org/abs/1805.01468,"We present the first application of a bin-scheme microphysical and vertical transport model to determine the size distribution of titanium and silicate cloud particles in the atmospheres of hot Jupiters. We predict particle size distributions from first principles for a grid of planets at four representative equatorial longitudes, and investigate how observed cloud properties depend on the atmospheric thermal structure and vertical mixing. The predicted size distributions are frequently bimodal and irregular in shape. There is a negative correlation between total cloud mass and equilibrium temperature as well as a positive correlation between total cloud mass and atmospheric mixing. The cloud properties on the east and west limbs show distinct differences that increase with increasing equilibrium temperature. Cloud opacities are roughly constant across a broad wavelength range with the exception of features in the mid-infrared. Forward scattering is found to be important across the same wavelength range. Using the fully resolved size distribution of cloud particles as opposed to a mean particle size has a distinct impact on the resultant cloud opacities. The particle size that contributes the most to the cloud opacity depends strongly on the cloud particle size distribution. We predict that it is unlikely that silicate or titanium clouds are responsible for the optical Rayleigh scattering slope seen in many hot Jupiters. We suggest that cloud opacities in emission may serve as sensitive tracers of the thermal state of a planet's deep interior through the existence or lack of a cold trap in the deep atmosphere. ",Formation of Silicate and Titanium Clouds on Hot Jupiters
95,992326254283120640,3352913423,Dimitri Block,['Our new paper about modular software architecture for correlation-based wideband channel sounding with SDRs <LINK> source code is available at GitHub <LINK> <LINK>'],https://arxiv.org/abs/1805.01236,"Novel industrial wireless applications require wideband, real-time channel characterization due to complex multipath propagation. Rapid machine motion leads to fast time variance of the channel's reflective behavior, which must be captured for radio channel characterization. Additionally, inhomogeneous radio channels demand highly flexible measurements. Existing approaches for radio channel measurements either lack flexibility or wide-band, real-time performance with fast time variance. In this paper, we propose a correlative channel sounding approach utilizing a software-defined architecture. The approach enables real-time, wide-band measurements with fast time variance immune to active interference. The desired performance is validated with a demanding industrial application example. ","A Software-Defined Channel Sounder for Industrial Environments with Fast
  Time Variance"
96,992280441485840384,870723729831145472,Tivadar Danka 🇺🇦,"['Our new paper accompanying modAL, our #Python active learning framework is out @arxiv! Check it out at <LINK>!\n#machinelearning #activelearning']",https://arxiv.org/abs/1805.00979,"modAL is a modular active learning framework for Python, aimed to make active learning research and practice simpler. Its distinguishing features are (i) clear and modular object oriented design (ii) full compatibility with scikit-learn models and workflows. These features make fast prototyping and easy extensibility possible, aiding the development of real-life active learning pipelines and novel algorithms as well. modAL is fully open source, hosted on GitHub at this https URL To assure code quality, extensive unit tests are provided and continuous integration is applied. In addition, a detailed documentation with several tutorials are also available for ease of use. The framework is available in PyPI and distributed under the MIT license. ",modAL: A modular active learning framework for Python
97,992246286479835136,978500233368760320,Biao Zhang,"['Our new paper ""Accelerating Neural Transformer via an Average Attention Network"" (accepted by ACL2018) is available online: <LINK>\nWith cumulative-average operation, we accelerate the neural Transformer by a factor of 4~7 with almost no performance loss.😀']",http://arxiv.org/abs/1805.00631,"With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding. ",Accelerating Neural Transformer via an Average Attention Network
98,991858819910193152,3081907070,Chris Riseley,['New co-author paper out today: <LINK> (led by Annalisa Bonafede of @IRA_INAF) where we use new @LOFAR observations and archival GMRT and @chandraxray data to shed new light on a spectacular galaxy cluster merger event! <LINK>'],https://arxiv.org/abs/1805.00473,"We present results from LOFAR and GMRT observations of the galaxy cluster MACS$\,$J0717.5$+$3745. The cluster is undergoing a violent merger involving at least four sub-clusters, and it is known to host a radio halo. LOFAR observations reveal new sources of radio emission in the Intra-Cluster Medium: (i) a radio bridge that connects the cluster to a head-tail radio galaxy located along a filament of galaxies falling into the main cluster, (ii) a 1.9 Mpc radio arc, that is located North West of the main mass component, (iii) radio emission along the X-ray bar, that traces the gas in the X-rays South West of the cluster centre. We use deep GMRT observations at 608 MHz to constrain the spectral indices of these new radio sources, and of the emission that was already studied in the literature at higher frequency. We find that the spectrum of the radio halo and of the relic at LOFAR frequency follows the same power law as observed at higher frequencies. The radio bridge, the radio arc, and the radio bar all have steep spectra, which can be used to constrain the particle acceleration mechanisms. We argue that the radio bridge could be caused by the re-acceleration of electrons by shock waves that are injected along the filament during the cluster mass assembly. Despite the sensitivity reached by our observations, the emission from the radio halo does not trace the emission of the gas revealed by X-ray observations. We argue that this could be due to the difference in the ratio of kinetic over thermal energy of the intra-cluster gas, suggested by X-ray observations. ","LOFAR discovery of radio emission in MACS$\,$J0717.5$+$3745"
99,991727874657796096,52381876,Edward Frenkel,"['New paper with Davide Gaiotto in which we apply powerful tools of quantum field theory to create a kind of backdoor to the geometric Langlands Program:\n<LINK> <LINK>', ""@chrobertew I wrote an introduction to this general subject for non-mathematicians in Chapters 16 and 17 of LOVE &amp; MATH. That's a good place to start.""]",https://arxiv.org/abs/1805.00203,"We review and extend the vertex algebra framework linking gauge theory constructions and a quantum deformation of the Geometric Langlands Program. The relevant vertex algebras are associated to junctions of two boundary conditions in a 4d gauge theory and can be constructed from the basic ones by following certain standard procedures. Conformal blocks of modules over these vertex algebras give rise to twisted D-modules on the moduli stacks of G-bundles on Riemann surfaces which have applications to the Langlands Program. In particular, we construct a series of vertex algebras for every simple Lie group G which we expect to yield D-module kernels of various quantum Geometric Langlands dualities. We pay particular attention to the full duality group of gauge theory, which enables us to extend the standard qGL duality to a larger duality groupoid. We also discuss various subtleties related to the spin and gerbe structures and present a detailed analysis for the U(1) and SU(2) gauge theories. ","Quantum Langlands dualities of boundary conditions, D-modules, and
  conformal blocks"
100,991723765720481792,70874545,Josh Lothringer,"[""As @V_Parmentier and @lkreidberg have been describing, ultra-hot Jupiters are weird and unique for several reasons, from dissociation of molecules to the presence of H- opacity. Here's just a couple of the things we found in my new paper: <LINK>"", ""Once the planet gets above a Teq of about 2500 K, TiO/VO become dissocated as well, so you might expect the atmosphere to becomes non-inverted. We find that that's not the case; something is doing the work of TiO/VO after those molecules have disappeared. https://t.co/6ojhfnzvry"", 'The short-wavelength radiation being pumped out by early-type host stars is absorbed by atomic metals, metal hydrides, SiO, and bound-free opacities. This, combined with a dearth of IR-active molecules to cool the atmosphere, causes the atmosphere heats up.', 'This plot is showing where the stellar flux is being absorbed: look at all that short wavelength flux being absorbed right where the temperature inversion is beginning, around 10 mbar! https://t.co/mdKZIWDBEh', ""We also show the first self-consistent model of the hottest known jovian planet, KELT-9b (Tdayside = 4600 K). Nearly all molecules are dissocated, even CO, and most atoms are ionized. But the spectrum doesn't look like a blackbody..."", 'H- is the main opacity source whose bound-free opacity varies smoothly with wavelength such that the brightness temperature of KELT-9b varies by about 1000 K across the JWST spectral range! https://t.co/d61L1YPqnv', ""There's much more in the paper, but, in short, ultra-hot/extremely irradiated hot Jupiters are fascinating and unique astrophysical objects, worthy of further characterization. This is good because their hot dayside atmosphere and their inflated radii make these great targets."", 'See also: \nMansfield+ https://t.co/5p3b7Mob1x\n@V_Parmentier+ https://t.co/ReHRrUXo8H\n@lkreidberg+ https://t.co/oc3ZyzrnDC']",https://arxiv.org/abs/1805.00038,"Extremely irradiated hot Jupiters, exoplanets reaching dayside temperatures ${>}$2000 K, stretch our understanding of planetary atmospheres and the models we use to interpret observations. While these objects are planets in every other sense, their atmospheres reach temperatures at low pressures comparable only to stellar atmospheres. In order to understand our \textit{a priori} theoretical expectations for the nature of these objects, we self-consistently model a number of extreme hot Jupiter scenarios with the PHOENIX model atmosphere code. PHOENIX is well-tested on objects from cool brown dwarfs to expanding supernovae shells and its expansive opacity database from the UV to far-IR make PHOENIX well-suited for understanding extremely irradiated hot Jupiters. We find several fundamental differences between hot Jupiters at temperatures ${>}$2500 K and their cooler counterparts. First, absorption by atomic metals like Fe and Mg, molecules including SiO and metal hydrides, and continuous opacity sources like H$^-$ all combined with the short-wavelength output of early-type host stars result in strong thermal inversions, without the need for TiO or VO. Second, many molecular species, including H$_2$O, TiO, and VO are thermally dissociated at pressures probed by eclipse observations, biasing retrieval algorithms that assume uniform vertical abundances. We discuss other interesting properties of these objects, as well as future prospects and predictions for observing and characterizing this unique class of astrophysical object, including the first self-consistent model of the hottest known jovian planet, KELT-9b. ","Extremely Irradiated Hot Jupiters: Non-Oxide Inversions, H- Opacity, and
  Thermal Dissociation of Molecules"
101,991679180340449281,485856511,Channon Visscher,"['Check out our new paper (led by @V_Parmentier) on dissociation, ionization, and condensation in ultra-hot Jupiters (at <LINK>) <LINK>']",https://arxiv.org/abs/1805.00096,"A new class of exoplanets has emerged: the ultra hot Jupiters, the hottest close-in gas giants. Most of them have weaker than expected spectral features in the $1.1-1.7\mu m$ bandpass probed by HST/WFC3 but stronger spectral features at longer wavelengths probed by Spitzer. This led previous authors to puzzling conclusions about the thermal structures and chemical abundances of these planets. Using the SPARC/MITgcm, we investigate how thermal dissociation, ionization, H$^-$ opacity and clouds shape the thermal structures and spectral properties of ultra hot Jupiters with a special focus on WASP-121b. We expand our findings to the whole population of ultra hot Jupiters through analytical quantification of the thermal dissociation and its influence on the strength of spectral features. We predict that most molecules are thermally dissociated and alkalies are ionized in the dayside photospheres of ultra hot Jupiters. This includes H$_{\rm 2}$O, TiO, VO, and H$_{\rm 2}$ but not CO, which has a stronger molecular bond. The vertical molecular gradient created by the dissociation significantly weakens the spectral features from water while the $4.5\mu m$ CO feature remain unchanged. The water band in the HST/WFC3 bandpass is further weakened by H$^-$ continuum opacity. Molecules are expected to recombine before reaching the limb, leading to order of magnitude variations of the chemical composition and cloud coverage between the limb and the dayside. Overall, molecular dissociation provides a qualitative understanding of the lack of strong spectral feature of water in the $1-2\mu m$ bandpass observed in most ultra hot Jupiters. Quantitatively, however, our model does not provide a satisfactory match to the WASP-121b emission spectrum. Together with WASP-33b and Kepler-33Ab, they seem the outliers among the population of ultra hot Jupiters in need of a more thorough understanding. ","From thermal dissociation to condensation in the atmospheres of ultra
  hot Jupiters: WASP-121b in context"
102,991551147906347008,89467037,Mariano Beguerisse,"['New paper from Walid Ahmad, @masonporter and yours truly:\n""Tie-decay temporal networks in continuous time and eigenvector-based  centralities"" <LINK>']",https://arxiv.org/abs/1805.00193,"Network theory is a useful framework for studying interconnected systems of interacting entities. Many networked systems evolve continuously in time, but most existing methods for the analysis of time-dependent networks rely on discrete or discretized time. In this paper, we propose an approach for studying networks that evolve in continuous time by distinguishing between \emph{interactions}, which we model as discrete contacts, and \emph{ties}, which encode the strengths of relationships as functions of time. To illustrate our tie-decay network formalism, we adapt the well-known PageRank centrality score to our tie-decay framework in a mathematically tractable and computationally efficient way. We apply this framework to a synthetic example and then use it to study a network of retweets during the 2012 National Health Service controversy in the United Kingdom. Our work also provides guidance for similar generalizations of other tools from network theory to continuous-time networks with tie decay, including for applications to streaming data. ",Tie-decay networks in continuous time and eigenvector-based centralities
103,991524060587745281,2569631268,Daniel Huber,"['1/n Curious about the Gaia DR2 view of #Kepler stars and exoplanets? Check out our new paper led by @UHIfA grad student @TravisABerger, presenting revised radii for ~190,000 stars and ~4000 planets and planet candidates: <LINK> Main conclusions:', '2/n &gt;65% of all Kepler targets are main-sequence stars. Many revised radii are larger than previous estimates, but the subgiant contamination is not quite as severe as we feared. Should be good news for planet occurrence rates! https://t.co/g0X04MMXZ9', '3/n Gaia confirms the Fulton et al. radius gap, but we find it to be mostly contained to &gt;200 Fearth and closer to 2 Rearth. Systematics at this level are tricky even with Gaia, but a larger gap radius may have implications for the core composition of photoevaporated planets. https://t.co/QZYeWNgBxl', '4/n We find that the Hot Super-Earth Desert (black dashed box) is not completely deserted and the inhabitants are most likely not young, indicating that some of these planets may have evolved from hot Jupiters. https://t.co/VgzONOSz6Z', '5/n We identify a bona-fide (at least in terms of stellar parameters) population of 34 candidate and 8 confirmed planets with &lt;2 Rearth in the ~optimistic habitable zone (green box above). Stellar parameters are no longer a bottleneck for identifying these exciting planets!', '6/n Last but not least a big thank you to the whole @ESAGaia team and #DPAC for this absolutely incredible dataset. The last week has been an amazing thrill, and the fun has only just begun!', '@AdrienCoffinet Artifact of joining two different Teff scales. Will be fixed in future revision!', '@jotajotahermes I suggest we play this on a loop when we bbq on the beach next week.', '@nbatalha @UHIfA @TravisABerger Thanks! The credit for swiftness all goes to @TravisABerger - the Kepler DR25-Gaia HRD landed in my inbox less than 12 hours after DR2 dropped.']",https://arxiv.org/abs/1805.00231,"One bottleneck for the exploitation of data from the $Kepler$ mission for stellar astrophysics and exoplanet research has been the lack of precise radii and evolutionary states for most of the observed stars. We report revised radii of 177,911 $Kepler$ stars derived by combining parallaxes from $Gaia$ Data Release 2 with the DR25 $Kepler$ Stellar Properties Catalog. The median radius precision is $\approx$ 8%, a typical improvement by a factor of 4-5 over previous estimates for typical $Kepler$ stars. We find that $\approx$ 67% ($\approx$ 120,000) of all $Kepler$ targets are main-sequence stars, $\approx$ 21% ($\approx$ 37,000) are subgiants, and $\approx$ 12% ($\approx$ 21,000) are red giants, demonstrating that subgiant contamination is less severe than some previous estimates and that Kepler targets are mostly main-sequence stars. Using the revised stellar radii, we recalculate the radii for 2123 confirmed and 1922 candidate exoplanets. We confirm the presence of a gap in the radius distribution of small, close-in planets, but find that the gap is mostly limited to incident fluxes $>$ 200$F_\oplus$ and its location may be at a slightly larger radius (closer to $\approx$ 2$R_\oplus$) when compared to previous results. Further, we find several confirmed exoplanets occupying a previously-described ""hot super-Earth desert"" at high irradiance, show the relation between gas-giant planet radius and incident flux, and establish a bona-fide sample of eight confirmed planets and 30 planet candidates with $R_{\mathrm{p}}$ $<$ 2$R_\oplus$ in circumstellar ""habitable zones"" (incident fluxes between 0.25--1.50 $F_\oplus$). The results presented here demonstrate the potential for transformative characterization of stellar and exoplanet populations using $Gaia$ data. ",Revised Radii of Kepler Stars and Planets Using Gaia Data Release 2
104,991483998709805056,11778512,Mason Porter,"['New paper: ""Tie-decay temporal networks in continuous time and eigenvector-based centralities"": <LINK>\n\nCoauthors: Walid Ahmad &amp; @marianobegue\n\n[That is an _ex_ temporal network!] <LINK>']",https://arxiv.org/abs/1805.00193,"Network theory is a useful framework for studying interconnected systems of interacting entities. Many networked systems evolve continuously in time, but most existing methods for the analysis of time-dependent networks rely on discrete or discretized time. In this paper, we propose an approach for studying networks that evolve in continuous time by distinguishing between \emph{interactions}, which we model as discrete contacts, and \emph{ties}, which encode the strengths of relationships as functions of time. To illustrate our tie-decay network formalism, we adapt the well-known PageRank centrality score to our tie-decay framework in a mathematically tractable and computationally efficient way. We apply this framework to a synthetic example and then use it to study a network of retweets during the 2012 National Health Service controversy in the United Kingdom. Our work also provides guidance for similar generalizations of other tools from network theory to continuous-time networks with tie decay, including for applications to streaming data. ",Tie-decay networks in continuous time and eigenvector-based centralities
105,1002257256015155202,303553181,"Ben Horne, Ph.D.","['Check out our new paper on Verbatim Content Copying by News Producers, to be presented at @neco_workshop (located @icwsm 2018) <LINK> #news #research']",https://arxiv.org/abs/1805.05939,"In today's news ecosystem, news sources emerge frequently and can vary widely in intent. This intent can range from benign to malicious, with many tactics being used to achieve their goals. One lesser studied tactic is content republishing, which can be used to make specific stories seem more important, create uncertainty around an event, or create a perception of credibility for unreliable news sources. In this paper, we take a first step in understanding this tactic by exploring verbatim content copying across 92 news producers of various characteristics. We find that content copying occurs more frequently between like-audience sources (eg. alternative news, mainstream news, etc.), but there consistently exists sparse connections between these communities. We also find that despite articles being verbatim, the headlines are often changed. Specifically, we find that mainstream sources change more structural features, while alternative sources change many more content features, often changing the emotional tone and bias of the titles. We conclude that content republishing networks can help identify and label the intent of brand-new news sources using the tight-knit community they belong to. In addition, it is possible to use the network to find important content producers in each community, producers that are used to amplify messages of other sources, and producers that distort the messages of other sources. ",An Exploration of Verbatim Content Republishing by News Producers
106,1001720027702333440,846984978,Wilker Aziz,['New #ACL2018 paper: modelling latent variation in translation data with a stochastic decoder <LINK> #NLProc'],https://arxiv.org/abs/1805.10844,"The process of translation is ambiguous, in that there are typically many valid trans- lations for a given sentence. This gives rise to significant variation in parallel cor- pora, however, most current models of machine translation do not account for this variation, instead treating the prob- lem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to ac- count for local lexical and syntactic varia- tion in parallel corpora. We provide an in- depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on sev- eral different language pairs demonstrate that the model consistently improves over strong baselines. ",A Stochastic Decoder for Neural Machine Translation
107,1001284772927344642,3100596960,Walter Scheirer,['New paper from the lab: A Neurobiological Cross-domain Evaluation Metric\nfor Predictive Coding Networks. We introduce and evaluate a human-model similarity metric for determining model correspondence to the human brain.\n\n<LINK>'],https://arxiv.org/abs/1805.10726,"Neuroscience theory posits that the brain's visual system coarsely identifies broad object categories via neural activation patterns, with similar objects producing similar neural responses. Artificial neural networks also have internal activation behavior in response to stimuli. We hypothesize that networks exhibiting brain-like activation behavior will demonstrate brain-like characteristics, e.g., stronger generalization capabilities. In this paper we introduce a human-model similarity (HMS) metric, which quantifies the similarity of human fMRI and network activation behavior. To calculate HMS, representational dissimilarity matrices (RDMs) are created as abstractions of activation behavior, measured by the correlations of activations to stimulus pairs. HMS is then the correlation between the fMRI RDM and the neural network RDM across all stimulus pairs. We test the metric on unsupervised predictive coding networks, which specifically model visual perception, and assess the metric for statistical significance over a large range of hyperparameters. Our experiments show that networks with increased human-model similarity are correlated with better performance on two computer vision tasks: next frame prediction and object matching accuracy. Further, HMS identifies networks with high performance on both tasks. An unexpected secondary finding is that the metric can be employed during training as an early-stopping mechanism. ",A Neurobiological Evaluation Metric for Neural Network Model Search
108,1001150553953636353,776074951262699521,Ivan Titov,"['New #ACL2018 paper: given extended context, neural machine translation (a context-aware Transformer)  learns to resolves coreference.    <LINK>  #NLProc @EdinburghNLP <LINK>']",https://arxiv.org/abs/1805.10163,"Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6). ",Context-Aware Neural Machine Translation Learns Anaphora Resolution
109,999117050881740800,913238472357437445,Fuminobu TAKAHASHI,"['In our new paper we showed that the axion dark matter abundance can be significantly reduced without fine-tuning for low-scale inflation with H_inf &lt; 100 MeV. In other words, the upper bound of the axion window is greatly relaxed!\n\n<LINK> <LINK>']",https://arxiv.org/abs/1805.08763,"We show that the upper bound of the classical QCD axion window can be significantly relaxed for low-scale inflation. If the Gibbons-Hawking temperature during inflation is lower than the QCD scale, the initial QCD axion misalignment angle follows the Bunch-Davies distribution. The distribution is peaked at the strong CP conserving minimum if there is no other light degree of freedom contributing to the strong CP phase. As a result, the axion overproduction problem is significantly relaxed even for an axion decay constant larger than $10^{12}$GeV. We also provide concrete hilltop inflation models where the Hubble parameter during inflation is comparable to or much smaller than the QCD scale, with successful reheating taking place via perturbative decays or dissipation processes. ",The QCD Axion Window and Low Scale Inflation
110,998902692050362375,825088493764407298,Noam Brown (in Pittsburgh),"['Last year I said superhuman poker AIs would be running on smartphones in 5 years. That timeline may have been pessimistic. Our new paper ""Depth-Limited Solving for Imperfect-Information Games"" shows how to develop a master poker AI on a 4-core CPU: <LINK> <LINK>', ""@hbou @soumithchintala We might release hand histories if there's sufficient interest.""]",https://arxiv.org/abs/1805.08195,"A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer. ",Depth-Limited Solving for Imperfect-Information Games
111,998720096221642752,3877821072,Shuai Tang,"['1/ A long paper with adobe researchers was accepted to #RepL4NLP at #ACL2018 \n2/ A new paper with my advisor about ""Multi-view sentence representation learning"" is online! <LINK>', '@sleepinyourhat thanks for discussion!']",https://arxiv.org/abs/1805.07443,"Multi-view learning can provide self-supervision when different views are available of the same data. The distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora. Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we create a unified multi-view sentence representation learning framework, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model, and the training objective is to maximise the agreement specified by the adjacent context information between two views. We show that, after training, the vectors produced from our multi-view training provide improved representations over the single-view training, and the combination of different views gives further representational improvement and demonstrates solid transferability on standard downstream tasks. ",Multi-view Sentence Representation Learning
112,997678140347559936,927909431689596929,Matthew Mizuhara,"['Excited to advertise our new paper on coupled oscillators! We numerically compare the onset of synchronous behaviors in various deterministic and random graph Kuramoto models. \n\n<LINK>', ""As an example of synchronization, here's a movie of a so-called twisted state which reminds me of stadium waves at a football game! https://t.co/NBoWEX0ZXM""]",https://arxiv.org/abs/1805.03786,"In his classical work, Kuramoto analytically described the onset of synchronization in all-to-all coupled networks of phase oscillators with random intrinsic frequencies. Specifically, he identified a critical value of the coupling strength, at which the incoherent state loses stability and a gradual build-up of coherence begins. Recently, Kuramoto's scenario was shown to hold for a large class of coupled systems on convergent families of deterministic and random graphs. Guided by these results, in the present work, we study several model problems illustrating the link between network topology and synchronization in coupled dynamical systems. First, we identify several families of graphs, for which the transition to synchronization in the Kuramoto model starts at the same critical value of the coupling strength and proceeds in practically the same way. These examples include Erd\H{o}s-R\'enyi random graphs, Paley graphs, complete bipartite graphs, and certain stochastic block graphs. These examples illustrate that some rather simple structural properties such as the volume of the graph may determine the onset of synchronization, while finer structural features may affect only higher order statistics of the transition to synchronization. Further, we study the transition to synchronization in the Kuramoto model on power law and small-world random graphs. The former family of graphs endows the Kuramoto model with very good synchronizability: the synchronization threshold can be made arbitrarily low by varying the parameter of the power law degree distribution. For the Kuramoto model on small-world graphs, in addition to the transition to synchronization, we identify a new bifurcation leading to stable random twisted states. The examples analyzed in this work complement the results in [Chiba, Medvedev, The mean field analysis for the Kuramoto model on graphs (parts I and II), arxiv]. ",Bifurcations in the Kuramoto model on graphs
113,997484927393435649,935441422626541568,Rima Alaifari,['New paper on phase retrieval from Gabor measurements and a construction that shows the exponential degradation of stability: <LINK>'],https://arxiv.org/abs/1805.06716,"The problem of reconstructing a function from the magnitudes of its frame coefficients has recently been shown to be never uniformly stable in infinite-dimensional spaces [5]. This result also holds for frames that are possibly continuous [2]. On the other hand, the problem is always stable in finite-dimensional settings. A prominent example of such a phase retrieval problem is the recovery of a signal from the modulus of its Gabor transform. In this paper, we study Gabor phase retrieval and ask how the stability degrades on a natural family of finite-dimensional subspaces of the signal domain $L^2(\mathbb{R})$. We prove that the stability constant scales at least quadratically exponentially in the dimension of the subspaces. Our construction also shows that typical priors such as sparsity or smoothness promoting penalties do not constitute regularization terms for phase retrieval. ",Gabor phase retrieval is severely ill-posed
114,997127023737831424,2269143811,Pramod Kaushik Mudrakarta,['Check out our new #ACL2018 paper on analyzing deep-learning-based question answering models: <LINK> #NLProc <LINK>'],https://arxiv.org/abs/1805.05492,"We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \emph{attribution} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from $61.1\%$ to $19\%$, and that of a tabular question answering model from $33.5\%$ to $3.3\%$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data. ",Did the Model Understand the Question?
115,996370665321172992,185910194,Graham Neubig,"['New #ACL2018 paper ""Neural Factor Graph Models for Cross-lingual Morphological Tagging"" <LINK>\nNot just for morphology, but a powerful &amp; interpretable tool for sequence labeling that integrates graphical models and neural networks! Code: <LINK> <LINK>']",https://arxiv.org/abs/1805.04570,"Morphological analysis involves predicting the syntactic traits of a word (e.g. {POS: Noun, Case: Acc, Gender: Fem}). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches. ",Neural Factor Graph Models for Cross-lingual Morphological Tagging
116,996320945186004993,776074951262699521,Ivan Titov,['New #ACL2018 semantic parsing paper by @chunchuan_lyu:  \nAMR Graph Prediction with Latent Alignment:  +3.4% in AMR (74.4%).  Non-autoregressive model for graph prediction (VAE with Gumbel-Sinkhorn to relax discrete latent alignments)\n<LINK>  #NLProc @EdinburghNLP <LINK>'],https://arxiv.org/abs/1805.05286,"Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational auto-encoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25). ",AMR Parsing as Graph Prediction with Latent Alignment
117,994866998659178498,3353136183,Dimitar Dimitrov,"['Have you ever asked yourself how do search and navigation interplay on Wikipedia? Our new #websci2018 paper with @f_lemmerich, @ffloeck and @mstrohm provides some answers. <LINK>']",https://arxiv.org/abs/1805.04022,"As one of the richest sources of encyclopedic information on the Web, Wikipedia generates an enormous amount of traffic. In this paper, we study large-scale article access data of the English Wikipedia in order to compare articles with respect to the two main paradigms of information seeking, i.e., search by formulating a query, and navigation by following hyperlinks. To this end, we propose and employ two main metrics, namely (i) searchshare -- the relative amount of views an article received by search --, and (ii) resistance -- the ability of an article to relay traffic to other Wikipedia articles -- to characterize articles. We demonstrate how articles in distinct topical categories differ substantially in terms of these properties. For example, architecture-related articles are often accessed through search and are simultaneously a ""dead end"" for traffic, whereas historical articles about military events are mainly navigated. We further link traffic differences to varying network, content, and editing activity features. Lastly, we measure the impact of the article properties by modeling access behavior on articles with a gradient boosting approach. The results of this paper constitute a step towards understanding human information seeking behavior on the Web. ","Query for Architecture, Click through Military: Comparing the Roles of
  Search and Navigation on Wikipedia"
118,994541709395660801,2676457430,MAGIC telescopes 🌴🌺,"['A new paper has been accepted in ApJ, where we report on the search for Earth-skimming tau neutrinos with MAGIC in the energy range between 1PeV - 3 EeV by pointing to the sea. Check out the paper at: <LINK>\n#theMAGICtelescopes #MAGICpaper #theMAGICcollaboration <LINK>']",https://arxiv.org/abs/1805.02750,"A search for tau neutrino induced showers with the MAGIC telescopes is presented. The MAGIC telescopes located at an altitude of 2200 m a.s.l. in the Canary Island of La Palma, can point towards the horizon or a few degrees below across an azimuthal range of about 80 degrees. This provides a possibility to search for air showers induced by tau leptons arising from interactions of tau neutrinos in the Earth crust or the surrounding ocean. In this paper we show how such air showers can be discriminated from the background of very inclined hadronic showers by using Monte Carlo simulations. Taking into account the orography of the site, the point source acceptance and the event rates expected have been calculated for a sample of generic neutrino fluxes from photo-hadronic interactions in AGNs. The analysis of about 30 hours of data taken towards the sealeads to a 90\% C.L. point source limit for tau neutrinos in the energy range from $1.0 \times 10^{15}$ eV to $3.0 \times 10^{18}$ eV of about $E_{\nu_{\tau}}^{2}\times \phi (E_{\nu_{\tau}}) < 2.0 \times 10^{-4}$ GeV cm$^{-2}$ s$^{-1}$ for an assumed power-law neutrino spectrum with spectral index $\gamma$=-2. However, with 300 hours and in case of an optimistic neutrino flare model, limits of the level down to $E_{\nu_{\tau}}^{2}\times \phi (E_{\nu_{\tau}}) < 8.4 \times 10^{-6}$ GeV cm$^{-2}$ s$^{-1}$ can be expected. ","Limits on the flux of tau neutrinos from 1 PeV to 3 EeV with the MAGIC
  telescopes"
119,992430096639250438,3319833287,Nicola Spaldin,['The lead chalcogenides continue to keep us busy. New paper on emphasis in PbSe just posted on the arXiv: <LINK> <LINK>'],https://arxiv.org/abs/1805.01069,"The temperature dependence of the local structure of PbSe has been investigated using pair distribution function (PDF) analysis of x-ray and neutron powder diffraction data and density functional theory (DFT) calculations. Observation of non-Gaussian PDF peaks at high temperature indicates the presence of significant anharmonicity, which can be modeled as Pb off-centering along [100] directions that grows on warming similar to the behavior seen in PbTe and PbS and sometimes called emphanisis. Interestingly, the emphanitic response is smaller in PbSe than in both PbS and PbTe indicating a non-monotonic response with chalcogen atomic number in the PbQ (Q=S, Se, Te) series. The DFT calculations indicate a correlation between band gap and the amplitude of [100] dipolar distortion, suggesting that emphanisis may be behind the anomalous composition and temperature dependencies of the band gaps in this series. ","Emphanitic anharmonicity in PbSe at high temperature and the anomalous
  electronic properties in the PbQ (Q=S, Se, Te) system"
120,991657516781199360,907232486735958018,Jaki Noronha-Hostler,"['Another new paper on arxiv today! We analyze fluctuations of strangeness (from kaons) to find good evidence of a flavor hierarchy in the QCD phase transition! Our results show that strange particles freeze-out at roughly temp 10-15 MeV higher than light. \n\n<LINK>', 'This is not necessarily a new idea, there were hints of a flavor hierarchy from first principle Lattice QCD calculations back in 2014: https://t.co/zj62hdDTCy but that demonstrated a temperature difference (light vs. strange) when the Quark Gluon Plasma turned into particles.', ""While particles could hadronize earlier on that doesn't necessarily mean they will reach chemical equilibrium early as well.  Doing direct comparisons with experimental data from @RHIC_STAR we find that strange particles reach eq. at 10-15 higher temperatures than light."", ""There should also be a flavor hierarchy in the early universe during the QCD phase transition.  I'm not a cosmologist or astrophysicists so I can't really tell you about the implications of that but it's certainly interesting!""]",https://arxiv.org/abs/1805.00088,"We compare the mean-over-variance ratio of the net-kaon distribution calculated within a state-of-the-art hadron resonance gas model to the latest experimental data from the Beam Energy Scan at RHIC by the STAR collaboration. Our analysis indicates that it is not possible to reproduce the experimental results using the freeze-out parameters from the existing combined fit of net-proton and net-electric charge mean-over-variance. The strange mesons need about 10-15 MeV higher temperatures than the light hadrons at the highest collision energies. In view of the future $\Lambda$ fluctuation measurements, we predict the $\Lambda$ variance-over-mean and skewness-times-variance at the light and strange chemical freeze-out parameters. We observe that the $\Lambda$ fluctuations are sensitive to the difference in the freeze-out temperatures established in this analysis. Our results have implications for other phenomenological models in the field of relativistic heavy ion collisions. ",Freeze-out temperature from net-Kaon fluctuations at RHIC
121,1006175063748632576,151193108,Mert R. Sabuncu 🇺🇦,['To obtain a smooth and invertible (diffeomorphic) registration b/w 2 images often involves solving an optimization-which can be very slow. We propose a learning-based strategy to do this very fast. @AdrianDalca will present @MICCAI2018. Paper is here:  <LINK>'],https://arxiv.org/abs/1805.04605,"Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of the algorithm. Our approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees and uncertainty estimates. Our implementation is available online at this http URL . ",Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration
122,1004817892339265537,3059466984,Ananda Theertha Suresh,"['New paper! In <LINK> , we propose a new mechanism for communication-efficient and differentially-private distributed SGD.']",https://arxiv.org/abs/1805.10559,"Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For $d$ variables and $n \approx d$ clients, the proposed method uses $O(\log \log(nd))$ bits of communication per client per coordinate and ensures constant privacy. We also extend and improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest. ","cpSGD: Communication-efficient and differentially-private distributed
  SGD"
123,1002616328883134465,308361234,Murray Brightman,['Find out what we saw when we stared at the two colliding galaxies of M51 with @NASANuSTAR for 60 hours. A weakly accreting supermassive black hole at the center of each. On arXiv today <LINK> <LINK>'],https://arxiv.org/abs/1805.12140,"We present a broadband X-ray spectral analysis of the M51 system, including the dual active galactic nuclei (AGN) and several off-nuclear point sources. Using a deep observation by NuSTAR, new high-resolution coverage of M51b by Chandra, and the latest X-ray torus models, we measure the intrinsic X-ray luminosities of the AGN in these galaxies. The AGN of M51a is found to be Compton thick, and both AGN have very low accretion rates ($\lambda_{\rm Edd} <10^{-4}$). The latter is surprising considering that the galaxies of M51 are in the process of merging, which is generally predicted to enhance nuclear activity. We find that the covering factor of the obscuring material in M51a is $0.26 \pm 0.03$, consistent with the local AGN obscured fraction at $L_{\rm X}\sim 10^{40}$ erg s$^{-1}$. The substantial obscuring column does not support theories that the torus, presumed responsible for the obscuration, disappears at these low accretion luminosities. However, the obscuration may have resulted from the gas infall driven by the merger rather than the accretion process. We report on several extra-nuclear sources with $L_{\rm X}>10^{39}$ erg s$^{-1}$ and find that a spectral turnover is present below 10 keV in most such sources, in line with recent results on ultraluminous X-ray sources. ","A long hard-X-ray look at the dual active galactic nuclei of M51 with
  NuSTAR"
124,1001983568384577538,1152296594,Swabha Swayamdipta,"['Our #ACL2018 paper with Phoebe Mulcaire is now available on arXiv: <LINK>. We find that combining languages for training SRL systems can tricky, because of differences between the languages, and the diverse annotation schemes used in the CoNLL 2009 shared task.']",https://arxiv.org/abs/1805.11598,"Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from a pair of languages in the CoNLL 2009 shared task to build a polyglot semantic role labeler. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in an improvement in SRL performance on multiple languages over a monolingual baseline. Analysis of the polyglot model shows it to be advantageous in lower-resource settings. ",Polyglot Semantic Role Labeling
125,999539153472700416,1601615208,Rui Shu,"['<LINK> \n\nAmortized Inference Regularization: We look at whether it makes sense to regularize the amortized inference model, provide new analysis for denoising VAE, analyze inference-regularized-IWAE, propose importance-weighted SVI, and more!']",http://arxiv.org/abs/1805.08913,"The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs. ",Amortized Inference Regularization
126,999185925765828609,14544467,Daniel Apai,['In a cool new study led by @GijsMulders we show that the Solar System is an outlier: &gt;97% of systems have inner planets closer in. 1st result from our powerful Exoplanet Population Observations Simulator. On GitHub.@EOSNExSS @ilaria_pascucci @fredciesla  <LINK>'],https://arxiv.org/abs/1805.08211,"The Kepler survey provides a statistical census of planetary systems out to the habitable zone. Because most planets are non-transiting, orbital architectures are best estimated using simulated observations of ensemble populations. Here, we introduce EPOS, the Exoplanet Population Observation Simulator, to estimate the prevalence and orbital architectures of multi-planet systems based on the latest Kepler data release, DR25. We estimate that at least 42% of sun-like stars have nearly coplanar planetary systems with 7 or more exoplanets. The fraction of stars with at least one planet within 1 au could be as high as 100% depending on assumptions about the distribution of single transiting planets. We estimate an occurrence rate of planets in the habitable zone around sun-like stars of eta_earth=36+-14%. The innermost planets in multi-planet systems are clustered around an orbital period of 10 days (0.1 au), reminiscent of the protoplanetary disk inner edge or could be explained by a planet trap at that location. Only a small fraction of planetary systems have the innermost planet at long orbital periods, with fewer than ~8% and ~3% having no planet interior to the orbit of Mercury and Venus, respectively. These results reinforce the view that the solar system is not a typical planetary system, but an outlier among the distribution of known exoplanetary systems. We predict that at least half of the habitable zone exoplanets are accompanied by (non-transiting) planets at shorter orbital periods, hence knowledge of a close-in exoplanet could be used as a way to optimize the search for Earth-size planets in the Habitable Zone with future direct imaging missions. ","The Exoplanet Population Observation Simulator. I - The Inner Edges of
  Planetary Systems"
127,999162228422184960,195122448,J W F Valle,"['[1805.08251] P. V. Dong, D. T. Huong, Daniel A. Camargo, Farinaldo S. Queiroz, José W. F. Valle: Asymmetric Dark Matter, Inflation and Leptogenesis from B-L Symmetry Breaking <LINK> \n\nWe propose a unified setup for dark... <LINK>']",http://arxiv.org/abs/1805.08251,"We propose a unified setup for dark matter, inflation and baryon asymmetry generation through the neutrino mass seesaw mechanism. Our scenario emerges naturally from an extended gauge group containing $B-L$ as a non-commutative symmetry, broken by a singlet scalar that also drives inflation. Its decays reheat the universe, producing the lightest right-handed neutrino. Automatic matter parity conservation leads to the stability of an asymmetric dark matter candidate, directly linked to the matter-antimatter asymmetry in the universe. ","Asymmetric Dark Matter, Inflation and Leptogenesis from B-L Symmetry
  Breaking"
128,998870539547611136,869896064802934788,Jan Rybizki,['#GaiaDR2 precision redetermination of the perihelion parameters for the closest known stellar encounter Gliese710. This is part of our new publication <LINK> where we find over 20 new encounters that will be closer than 1pc to the sun and assess the completeness. <LINK>'],http://arxiv.org/abs/1805.07581,"Passing stars may play an important role in the evolution of our solar system. We search for close stellar encounters to the Sun among all 7.2 million stars in Gaia-DR2 that have six-dimensional phase space data. We characterize encounters by integrating their orbits through a Galactic potential and propagating the correlated uncertainties via a Monte Carlo resampling. After filtering to remove spurious data, we find 694 stars that have median (over uncertainties) closest encounter distances within 5 pc, all occurring within 15 Myr from now. 26 of these have at least a 50% chance of coming closer than 1 pc (and 7 within 0.5 pc), all but one of which are newly discovered here. We further confirm some and refute several other previously-identified encounters, confirming suspicions about their data. The closest encounter in the sample is Gl 710, which has a 95% probability of coming closer than 0.08 pc (17 000 AU). Taking mass estimates from Gaia astrometry and multiband photometry for essentially all encounters, we find that Gl 710 also has the largest impulse on the Oort cloud. Using a Galaxy model, we compute the completeness of the Gaia-DR2 encountering sample as a function of perihelion time and distance. Only 15% of encounters within 5 pc occurring within +/- 5 Myr of now have been identified, mostly due to the lack of radial velocities for faint and/or cool stars. Accounting for the incompleteness, we infer the present rate of encounters within 1 pc to be 19.7 +/- 2.2 per Myr, a quantity expected to scale quadratically with the encounter distance out to at least several pc. Spuriously large parallaxes in our sample from imperfect filtering would tend to inflate both the number of encounters found and this inferred rate. The magnitude of this effect is hard to quantify. ",New stellar encounters discovered in the second Gaia data release
129,997096805648216065,408653414,Mattia Mazzoli,"[""I mom, I'm on ArXiv again! We realized this study during our MultiAgent Systems exam, Marco and Jacopo decided to valorize it so we made a paper out of it! <LINK> It is always good to see your works reaching an end""]",https://arxiv.org/abs/1805.05999,"In the last years, the study of rumor spreading on social networks produced a lot of interest among the scientific community, expecially due to the role of social networks in the last political events. The goal of this work is to reproduce real-like diffusions of information and misinformation in a scale-free network using a multi-agent-based model. The data concerning the virtual spreading are easily obtainable, in particular the diffusion of information during the announcement for the discovery of the Higgs Boson on Twitter was recorded and investigated in detail. We made some assumptions on the micro behavior of our agents and registered the effects in a statistical analysis replying the real data diffusion. Then, we studied an hypotetical response to a misinformation diffusion adding debunking agents and trying to model a critic response from the agents using real data from a hoax regarding the Occupy Wall Street movement. After tuning our model to reproduce these results, we measured some network properties and proved the emergence of substantially separated structures like echochambers, independently from the network size scale, i.e. with one hundred, one thousand and ten thousand agents. ",Agent Based Rumor Spreading in a scale-free network
130,996305554619424768,204261944,Matthew Kenworthy,"[""Today we have a paper led by two \n@SterrewachtNL bachelor students, Marit Mol Lous and Erik Weenk, on looking for a transiting planet around Beta Pictoris using the BRITE nanosatellites - spoiler: we didn't find one :( \n <LINK> 1/n"", '@SterrewachtNL The BRITE nanosats are studying pulsations in very bright stars and Beta Pictoris is a Delta Scuti pulsator! @KonstanzeZwintz is leading the analysis of the pulsations, and she agreed that we could use her data to search for a transiting planet (2/n) https://t.co/MFkf0JqsRX', '@SterrewachtNL @KonstanzeZwintz Beta Pictoris famously has a directly imaged exoplanet, #betapicb, in a very nearly edge on orbit. This planet does not transit, but chances are that other exoplanets will be in the same plane and may transit! So Marit and Erik went looking for one.... 3/n https://t.co/mEdyIM8dwt', '@SterrewachtNL @KonstanzeZwintz The photometry from the BRITE satellite had the pulsations removed from the approximately 78 day observing run, and then they folded the light curve and looked for a characteristic dip caused by a planet.... 4/n https://t.co/kKfh0aJNNm', '@SterrewachtNL @KonstanzeZwintz The previous best upper limits were made by Anne-Marie Lagrange and her group using radial velocity measurements. For small orbital periods, we improve those limits 5/n https://t.co/75Tvm20qWk', '@SterrewachtNL @KonstanzeZwintz The students injected fake transits into the BRITE data and then saw how well they could recover the transit, making their detection limits very robust. I think they did a great job! \n\nWe now have more data from BRITE and bRing, so we can continue the search... 6/n', '@SterrewachtNL @KonstanzeZwintz ...and the @AstepBetaPic telescope in Concordia has been getting exquisite photometric data for the past 16 months, and they will combine it with spectroscopy from HARPS to clean their photometry and get even tighter limits - watch this space! 7/7', ""@cosmos4u @SterrewachtNL @KonstanzeZwintz @AstepBetaPic @IamPicSat Well, @IamPicSat would have had greater photometric precision, and so be sensitive to smaller radii exoplanets if they're there to be seen.\n\nWe also have bRing data from the ground and another two seasons of BRITE to look at!""]",https://arxiv.org/abs/1805.05240,"The bright $(V=3.86)$ star $\beta$ Pictoris is a nearby young star with a debris disk and gas giant exoplanet, $\beta$ Pictoris b, in a multi-decade orbit around it. Both the planet's orbit and disk are almost edge-on to our line of sight. We carry out a search for any transiting planets in the $\beta$ Pictoris system with orbits of less than 30 days that are coplanar with the planet $\beta$ Pictoris b. We search for a planetary transit using data from the BRITE-Constellation nanosatellite BRITE-Heweliusz, analyzing the photometry using the Box-Fitting Least Squares Algorithm (BLS). The sensitivity of the method is verified by injection of artificial planetary transit signals using the Bad-Ass Transit Model cAlculatioN (BATMAN) code. No planet was found in the BRITE-Constellation data set. We rule out planets larger than 0.6 $\mathrm{R_J}$ for periods of less than 5 days, larger than 0.75 $\mathrm{R_J}$ for periods of less than 10 days, and larger than 1.05 $\mathrm{R_J}$ for periods of less than 20 days. ",A search for transiting planets in the $\beta$ Pictoris system
131,994929743022653440,3160301736,Andrés A. Plazas Malagón,"['""The Dark Energy Survey Scientists and Celebrities: What do they know about EPO? Do they know things?? Let\'s find out!"" We wrote a paper about EPO in DES (unfortunately, Mr. Peanutbutter is not part of the authors :P :P) <LINK>  #scicomm #STEM #EPO @theDESurvey']",https://arxiv.org/abs/1805.04034,"We present a programmatic study of physicists' and astronomers' attitudes toward education and public outreach (EPO) using 131 survey responses from members of the Dark Energy Survey. We find a disparity between the types of EPO activities researchers deem valuable and those in which they participate. Most respondents are motivated to engage in EPO by a desire to educate the public. Barriers to engagement include career- and skill-related concerns, but lack of time is the main deterrent. We explore the value of centralized EPO efforts and conclude with a list of recommendations for increasing researchers' engagement. ","Astronomers' and Physicists' Attitudes Toward Education & Public
  Outreach: A Programmatic Study with The Dark Energy Survey"
132,994765289001660416,2840725487,Ravi Gupta,['We surveyed members of @theDESurvey about their attitudes and perceptions of #STEM education &amp; public outreach. Then we wrote a paper about it! <LINK> \nGreat to find that most people feel EPO is and should be a personal &amp; professional responsibility.'],https://arxiv.org/abs/1805.04034,"We present a programmatic study of physicists' and astronomers' attitudes toward education and public outreach (EPO) using 131 survey responses from members of the Dark Energy Survey. We find a disparity between the types of EPO activities researchers deem valuable and those in which they participate. Most respondents are motivated to engage in EPO by a desire to educate the public. Barriers to engagement include career- and skill-related concerns, but lack of time is the main deterrent. We explore the value of centralized EPO efforts and conclude with a list of recommendations for increasing researchers' engagement. ","Astronomers' and Physicists' Attitudes Toward Education & Public
  Outreach: A Programmatic Study with The Dark Energy Survey"
133,1001022623273443328,804515472167288834,José Ignacio Orlando ☀️👁️,"['Our @MICCAI2018 paper is now online! We have designed a strategy for summarizing simulated hemodynamic features into a fixed-length vector that can be used in clinical studies to analyze its correlation with #glaucoma! Find the preprint here <LINK> …', 'We are releasing our MATLAB/python implementation in https://t.co/240ht204o4, and we will soon release also a new database of #fundus pictures of #glacoma patients with manual #segmentations of #arteries and #veins! Stay tuned!', 'This is also my last paper as part of @CONICETDialoga, and is part of a collaboration between researchers from @plademaARG, @KU_Leuven and @UZLeuven.']",https://arxiv.org/abs/1805.10273,"Glaucoma is the leading cause of irreversible but preventable blindness in the world. Its major treatable risk factor is the intra-ocular pressure, although other biomarkers are being explored to improve the understanding of the pathophysiology of the disease. It has been recently observed that glaucoma induces changes in the ocular hemodynamics. However, its effects on the functional behavior of the retinal arterioles have not been studied yet. In this paper we propose a first approach for characterizing those changes using computational hemodynamics. The retinal blood flow is simulated using a 0D model for a steady, incompressible non Newtonian fluid in rigid domains. The simulation is performed on patient-specific arterial trees extracted from fundus images. We also propose a novel feature representation technique to comprise the outcomes of the simulation stage into a fixed length feature vector that can be used for classification studies. Our experiments on a new database of fundus images show that our approach is able to capture representative changes in the hemodynamics of glaucomatous patients. Code and data are publicly available in this https URL ","Towards a glaucoma risk index based on simulated hemodynamics from
  fundus images"
134,999343671378984961,504408461,Guillermo Suarez-Tangil,"[""I'm officially on 4chan's /pol/ (*) radar... and only little after going public with our work where we study coordinated hate attacks from this community against users in other Web communities.\n(*) /pol/ is the Politically Incorrect Board of 4chan\nOur Work <LINK> <LINK>"", '4chan-led raids start with a user posting a message with comments like “you know what to do,” resulting into a spike in the number of hateful comments and attacks against the victims elsewhere in the Web. \nThis ""invitation"" to hate his left for us: https://t.co/UC7H0IyztF https://t.co/QmUaL47fAy']",https://arxiv.org/abs/1805.08168,"Video sharing platforms like YouTube are increasingly targeted by aggression and hate attacks. Prior work has shown how these attacks often take place as a result of ""raids,"" i.e., organized efforts by ad-hoc mobs coordinating from third-party communities. Despite the increasing relevance of this phenomenon, however, online services often lack effective countermeasures to mitigate it. Unlike well-studied problems like spam and phishing, coordinated aggressive behavior both targets and is perpetrated by humans, making defense mechanisms that look for automated activity unsuitable. Therefore, the de-facto solution is to reactively rely on user reports and human moderation. In this paper, we propose an automated solution to identify YouTube videos that are likely to be targeted by coordinated harassers from fringe communities like 4chan. First, we characterize and model YouTube videos along several axes (metadata, audio transcripts, thumbnails) based on a ground truth dataset of videos that were targeted by raids. Then, we use an ensemble of classifiers to determine the likelihood that a video will be raided with very good results (AUC up to 94%). Overall, our work provides an important first step towards deploying proactive systems to detect and mitigate coordinated hate attacks on platforms like YouTube. ","""You Know What to Do"": Proactive Detection of YouTube Videos Targeted by
  Coordinated Hate Attacks"
135,998838203158278145,51169895,Gianluca Stringhini,"['In our latest paper we study coordinated hate attacks against YouTube users and try to answer the question: can we predict, at upload time, whether a video will likely attract attacks in the future? <LINK> <LINK>']",https://arxiv.org/abs/1805.08168,"Video sharing platforms like YouTube are increasingly targeted by aggression and hate attacks. Prior work has shown how these attacks often take place as a result of ""raids,"" i.e., organized efforts by ad-hoc mobs coordinating from third-party communities. Despite the increasing relevance of this phenomenon, however, online services often lack effective countermeasures to mitigate it. Unlike well-studied problems like spam and phishing, coordinated aggressive behavior both targets and is perpetrated by humans, making defense mechanisms that look for automated activity unsuitable. Therefore, the de-facto solution is to reactively rely on user reports and human moderation. In this paper, we propose an automated solution to identify YouTube videos that are likely to be targeted by coordinated harassers from fringe communities like 4chan. First, we characterize and model YouTube videos along several axes (metadata, audio transcripts, thumbnails) based on a ground truth dataset of videos that were targeted by raids. Then, we use an ensemble of classifiers to determine the likelihood that a video will be raided with very good results (AUC up to 94%). Overall, our work provides an important first step towards deploying proactive systems to detect and mitigate coordinated hate attacks on platforms like YouTube. ","""You Know What to Do"": Proactive Detection of YouTube Videos Targeted by
  Coordinated Hate Attacks"
136,991657516781199360,907232486735958018,Jaki Noronha-Hostler,"['Another new paper on arxiv today! We analyze fluctuations of strangeness (from kaons) to find good evidence of a flavor hierarchy in the QCD phase transition! Our results show that strange particles freeze-out at roughly temp 10-15 MeV higher than light. \n\n<LINK>', 'This is not necessarily a new idea, there were hints of a flavor hierarchy from first principle Lattice QCD calculations back in 2014: https://t.co/zj62hdDTCy but that demonstrated a temperature difference (light vs. strange) when the Quark Gluon Plasma turned into particles.', ""While particles could hadronize earlier on that doesn't necessarily mean they will reach chemical equilibrium early as well.  Doing direct comparisons with experimental data from @RHIC_STAR we find that strange particles reach eq. at 10-15 higher temperatures than light."", ""There should also be a flavor hierarchy in the early universe during the QCD phase transition.  I'm not a cosmologist or astrophysicists so I can't really tell you about the implications of that but it's certainly interesting!""]",https://arxiv.org/abs/1805.00088,"We compare the mean-over-variance ratio of the net-kaon distribution calculated within a state-of-the-art hadron resonance gas model to the latest experimental data from the Beam Energy Scan at RHIC by the STAR collaboration. Our analysis indicates that it is not possible to reproduce the experimental results using the freeze-out parameters from the existing combined fit of net-proton and net-electric charge mean-over-variance. The strange mesons need about 10-15 MeV higher temperatures than the light hadrons at the highest collision energies. In view of the future $\Lambda$ fluctuation measurements, we predict the $\Lambda$ variance-over-mean and skewness-times-variance at the light and strange chemical freeze-out parameters. We observe that the $\Lambda$ fluctuations are sensitive to the difference in the freeze-out temperatures established in this analysis. Our results have implications for other phenomenological models in the field of relativistic heavy ion collisions. ",Freeze-out temperature from net-Kaon fluctuations at RHIC
