,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,904652473998172160,172346875,Chamkaur Ghag,['And our new paper on ultra-low background mass spectrometry at UCL for rare-event searches: <LINK>'],https://arxiv.org/abs/1708.08860,"Inductively Coupled Plasma Mass Spectrometry (ICP-MS) allows for rapid, high-sensitivity determination of trace impurities, notably the primordial radioisotopes $^{238}$U and $^{232}$Th, in candidate materials for low-background rare-event search experiments. We describe the setup and characterisation of a dedicated low-background screening facility at University College London where we operate an Agilent 7900 ICP-MS. The impact of reagent and carrier gas purity is evaluated and we show that twice-distilled ROMIL-SpA-grade nitric acid and zero-grade Ar gas delivers similar sensitivity to ROMIL-UpA-grade acid and research grade gas. A straightforward procedure for sample digestion and analysis of materials with U/Th concentrations down to 10 ppt g/g is presented. This includes the use of $^{233}$U and $^{230}$Th spikes to correct for signal loss from a range of sources and verification of $^{238}$U and $^{232}$Th recovery through digestion and analysis of a certified reference material with a complex sample matrix. Finally, we demonstrate assays and present results from two sample preparation and assay methods: a high-sensitivity measurement of ultra-pure Ti using open digestion techniques, and a closed vessel microwave digestion of a nickel-chromium-alloy using a multi-acid mixture. ",Ultra-low background mass spectrometry for rare-event searches
1,904652171408527360,172346875,Chamkaur Ghag,['Need low-background? Sorted. Our new paper on Low Background Gamma Spectroscopy at @BoulbyLab\n<LINK>'],https://arxiv.org/abs/1708.06086,"The Boulby Underground Germanium Suite (BUGS) comprises three low background, high-purity germanium detectors operating in the Boulby Underground Laboratory, located 1.1 km underground in the north-east of England, UK. BUGS utilises three types of detector to facilitate a high-sensitivity, high-throughput radioassay programme to support the development of rare-event search experiments. A Broad Energy Germanium (BEGe) detector delivers sensitivity to low-energy gamma-rays such as those emitted by 210Pb and 234Th. A Small Anode Germanium (SAGe) well-type detector is employed for efficient screening of small samples. Finally, a standard p-type coaxial detector provides fast screening of standard samples. This paper presents the steps used to characterise the performance of these detectors for a variety of sample geometries, including the corrections applied to account for cascade summing effects. For low-density materials, BUGS is able to radio-assay to specific activities down to 3.6 mBq/kg for 234Th and 6.6 mBq/kg for 210Pb both of which have uncovered some significant equilibrium breaks in the 238U chain. In denser materials, where gamma-ray self-absorption increases, sensitivity is demonstrated to specific activities of 0.9 mBq/kg for 226Ra, 1.1 mBq/kg for 228 Ra, 0.3 mBq/kg for 224Ra, and 8.6 mBq/kg for 40K with all upper limits at a 90% confidence level. These meet the requirements of most screening campaigns presently under way for rare-event search experiments, such as the LUX-ZEPLIN (LZ) dark matter experiment. We also highlight the ability of the BEGe detector to probe the X-ray fluorescence region which can be important to identify the presence of radioisotopes associated with neutron production; this is of particular relevance in experiments sensitive to nuclear recoils. ",Low Background Gamma Spectroscopy at the Boulby Underground Laboratory
2,903659384181575681,59836926,Fred Hohman,"['Our new paper is up on arXiv!\nA Deep Learning Approach for Population Estimation from Satellite Imagery\n<LINK> <LINK>', 'Code to wrangle GBs of satellite images will be released later this month! https://t.co/7nN56b5vFA']",https://arxiv.org/abs/1708.09086v1,"Knowing where people live is a fundamental component of many decision making processes such as urban development, infectious disease containment, evacuation planning, risk management, conservation planning, and more. While bottom-up, survey driven censuses can provide a comprehensive view into the population landscape of a country, they are expensive to realize, are infrequently performed, and only provide population counts over broad areas. Population disaggregation techniques and population projection methods individually address these shortcomings, but also have shortcomings of their own. To jointly answer the questions of ""where do people live"" and ""how many people live there,"" we propose a deep learning model for creating high-resolution population estimations from satellite imagery. Specifically, we train convolutional neural networks to predict population in the USA at a $0.01^{\circ} \times 0.01^{\circ}$ resolution grid from 1-year composite Landsat imagery. We validate these models in two ways: quantitatively, by comparing our model's grid cell estimates aggregated at a county-level to several US Census county-level population projections, and qualitatively, by directly interpreting the model's predictions in terms of the satellite image inputs. We find that aggregating our model's estimates gives comparable results to the Census county-level population projections and that the predictions made by our model can be directly interpreted, which give it advantages over traditional population disaggregation methods. In general, our model is an example of how machine learning techniques can be an effective tool for extracting information from inherently unstructured, remotely sensed data to provide effective solutions to social problems. ","] A Deep Learning Approach for Population Estimation from Satellite
  Imagery"
3,903313315597778946,3337988021,John Lewis,['Check out our new paper on #arxiv looking at dust and star formation in California with @ESAHerschel <LINK>'],https://arxiv.org/abs/1708.07847,"We present new high resolution and dynamic range dust column density and temperature maps of the California Molecular Cloud derived from a combination of Planck and Herschel dust-emission maps, and 2MASS NIR dust-extinction maps. We used these data to determine the ratio of the 2.2 micron extinction coefficient to the 850 micron opacity and found the value to be close to that found in similar studies of the Orion B and Perseus clouds but higher than that characterizing the Orion A cloud, indicating that variations in the fundamental optical properties of dust may exist between local clouds. We show that over a wide range of extinction, the column density probability distribution function (PDF$_N$) of the cloud can be well described by a simple power law with an index that represents a steeper decline with column density than found in similar studies of the Orion and Perseus clouds. Using only the protostellar population of the cloud and our extinction maps we investigate the Schmidt relation within the cloud. We show that the protostellar surface density, $\Sigma_*$, is directly proportional to the ratio of the protostellar and cloud pdfs. We use the cumulative distribution of protostars to infer the functional forms for both $\Sigma_*$ and PDF$_*$. We find that $\Sigma_*$ is best described by two power-law functions with steeper indicies than found in other local GMCs. We find that the protostellar pdf is a declining function of extinction also best described by two power-laws whose behavior mirrors that of $\Sigma_*$. Our observations suggest that variations both in the slope of the Schmidt relation and in the sizes of the protostellar populations between GMCs are largely driven by variations in the slope of the cloud pdf. This confirms earlier studies suggesting that cloud structure plays a major role in setting the global star formation rates in GMCs. ","HP2 survey: III The California Molecular Cloud--A Sleeping Giant
  Revisited"
4,903063790790610944,472628395,Peter Melchior,"['Our new paper ventures provides a solver for fitting an objective function under an arbitrary number of constraints <LINK>', 'We use it for Non-negative matrix factorization in hyperspectral imaging, and soon for the deblender of @LSST. Stay tuned!', '@LSST And of course the code is python and #opensource https://t.co/IOnbmfhhjJ']",https://arxiv.org/abs/1708.09066,"We introduce a generalization of the linearized Alternating Direction Method of Multipliers to optimize a real-valued function $f$ of multiple arguments with potentially multiple constraints $g_\circ$ on each of them. The function $f$ may be nonconvex as long as it is convex in every argument, while the constraints $g_\circ$ need to be convex but not smooth. If $f$ is smooth, the proposed Block-Simultaneous Direction Method of Multipliers (bSDMM) can be interpreted as a proximal analog to inexact coordinate descent methods under constraints. Unlike alternative approaches for joint solvers of multiple-constraint problems, we do not require linear operators $L$ of a constraint function $g(L\ \cdot)$ to be invertible or linked between each other. bSDMM is well-suited for a range of optimization problems, in particular for data analysis, where $f$ is the likelihood function of a model and $L$ could be a transformation matrix describing e.g. finite differences or basis transforms. We apply bSDMM to the Non-negative Matrix Factorization task of a hyperspectral unmixing problem and demonstrate convergence and effectiveness of multiple constraints on both matrix factors. The algorithms are implemented in python and released as an open-source package. ","Block-Simultaneous Direction Method of Multipliers: A proximal
  primal-dual splitting algorithm for nonconvex problems with multiple
  constraints"
5,902706487155404800,80169173,Kelly O'Connor McNees,['Congratulations to my mister @mcnees and collaborators on their new paper <LINK>!'],https://arxiv.org/abs/1708.08471,"We consider different sets of AdS$_2$ boundary conditions for the Jackiw-Teitelboim model in the linear dilaton sector where the dilaton is allowed to fluctuate to leading order at the boundary of the Poincar\'e disk. The most general set of boundary condtions is easily motivated in the gauge theoretic formulation as a Poisson sigma model and has an $\mathfrak{sl}(2)$ current algebra as asymptotic symmetries. Consistency of the variational principle requires a novel boundary counterterm in the holographically renormalized action, namely a kinetic term for the dilaton. The on-shell action can be naturally reformulated as a Schwarzian boundary action. While there can be at most three canonical boundary charges on an equal-time slice, we consider all Fourier modes of these charges with respect to the Euclidean boundary time and study their associated algebras. Besides the (centerless) $\mathfrak{sl}(2)$ current algebra we find for stricter boundary conditions a Virasoro algebra, a warped conformal algebra and a $\mathfrak{u}(1)$ current algebra. In each of these cases we get one half of a corresponding symmetry algebra in three-dimensional Einstein gravity with negative cosmological constant and analogous boundary conditions. However, on-shell some of these algebras reduce to finite-dimensional ones, reminiscent of the on-shell breaking of conformal invariance in SYK. We conclude with a discussion of thermodynamical aspects, in particular the entropy and some Cardyology. ",Menagerie of AdS$\boldsymbol{_2}$ boundary conditions
6,902618433375887363,875679268126306305,Tom Broekel,['New working paper on measuring technological complexity <LINK> Feedback is welcome #complexity #technology #innovation'],https://arxiv.org/abs/1708.07357,"The paper reviews two prominent approaches for the measurement of technological complexity: the method of reflection and the assessment of technologies' combinatorial difficulty. It discusses their central underlying assumptions and identifies potential problems related to these. A new measure of structural complexity is introduced as an alternative. The paper also puts forward four stylized facts of technological complexity that serve as benchmarks in an empirical evaluation of five complexity measures (increasing development over time, larger R&D efforts, more collaborative R&D, spatial concentration). The evaluation utilizes European patent data for the years 1980 to 2013 and finds the new measure of structural complexity to mirror the four stylized facts as good as or better than traditional measures. ","Measuring technological complexity - Current approaches and a new
  measure of structural complexity"
7,902569577481027584,4313989761,Dr. Andreas Faisst,['Dust in infant galaxies could be weird! Check out my new paper!\n<LINK>\n#science @ScienceNews #astro #BigBang <LINK>'],https://arxiv.org/abs/1708.07842,"Recent studies have found a significant evolution and scatter in the IRX-$\beta$ relation at z > 4, suggesting different dust properties of these galaxies. The total far-infrared (FIR) luminosity is key for this analysis but poorly constrained in normal (main-sequence) star-forming z > 5 galaxies where often only one single FIR point is available. To better inform estimates of the FIR luminosity, we construct a sample of local galaxies and three low-redshift analogs of z > 5 systems. The trends in this sample suggest that normal high-redshift galaxies have a warmer infrared (IR) SED compared to average z < 4 galaxies that are used as prior in these studies. The blue-shifted peak and mid-IR excess emission could be explained by a combination of a larger fraction of the metal-poor inter-stellar medium (ISM) being optically thin to ultra-violet (UV) light and a stronger UV radiation field due to high star formation densities. Assuming a maximally warm IR SED suggests 0.6 dex increased total FIR luminosities, which removes some tension between dust attenuation models and observations of the IRX-$\beta$ relation at z > 5. Despite this, some galaxies still fall below the minimum IRX-$\beta$ relation derived with standard dust cloud models. We propose that radiation pressure in these highly star-forming galaxies causes a spatial offset between dust clouds and young star-forming regions within the lifetime of O/B stars. These offsets change the radiation balance and create viewing-angle effects that can change UV colors at fixed IRX. We provide a modified model that can explain the location of these galaxies on the IRX-$\beta$ diagram. ","Are high redshift Galaxies hot? - Temperature of z &gt; 5 Galaxies and
  Implications on their Dust Properties"
8,902361615819051009,108700310,Mürsel Taşgın,['New paper is ready![1708.08305] Community detection using preference networks <LINK>'],https://arxiv.org/abs/1708.08305,"Community detection is the task of identifying clusters or groups of nodes in a network where nodes within the same group are more connected with each other than with nodes in different groups. It has practical uses in identifying similar functions or roles of nodes in many biological, social and computer networks. With the availability of very large networks in recent years, performance and scalability of community detection algorithms become crucial, i.e. if time complexity of an algorithm is high, it can not run on large networks. In this paper, we propose a new community detection algorithm, which has a local approach and is able to run on large networks. It has a simple and effective method; given a network, algorithm constructs a preference network of nodes where each node has a single outgoing edge showing its preferred node to be in the same community with. In such a preference network, each connected component is a community. Selection of the preferred node is performed using similarity based metrics of nodes. We use two alternatives for this purpose which can be calculated in 1-neighborhood of nodes, i.e. number of common neighbors of selector node and its neighbors and, the spread capability of neighbors around the selector node which is calculated by the gossip algorithm of Lind et.al. Our algorithm is tested on both computer generated LFR networks and real-life networks with ground-truth community structure. It can identify communities accurately in a fast way. It is local, scalable and suitable for distributed execution on large networks. ",Community detection using preference networks
9,902053085190778880,306943314,Tünde Fülöp,['Would like to know how to create 100 MG reconnecting magnetic fields with high intensity lasers? Read our new paper: <LINK>'],https://arxiv.org/abs/1708.07676,"Magnetic reconnection is a fundamental plasma process associated with conversion of the embedded magnetic field energy into kinetic and thermal plasma energy, via bulk acceleration and Ohmic dissipation. In many high-energy astrophysical events, magnetic reconnection is invoked to explain the non-thermal signatures. However, the processes by which field energy is transferred to the plasma to power the observed emission are still not properly understood. Here, via 3D particle-in-cell simulations of a readily available (TW-mJ-class) laser interacting with a micro-scale plasma slab, we show that when the electron beams excited on both sides of the slab approach the end of the plasma structure, ultrafast relativistic magnetic reconnection occurs in a magnetically-dominated (low-$\beta$) plasma. The resulting efficient particle acceleration leads to the emission of relativistic electron jets with cut-off energy $\sim$ 12 MeV. The proposed scenario can significantly improve understanding of fundamental questions such as reconnection rate, field dissipation and particle acceleration in relativistic magnetic reconnection. ","Relativistic magnetic reconnection driven by a laser interacting with a
  micro-scale plasma slab"
10,901195877972418560,40299444,Alexey Petrov,['New paper! Studying implications of a recent LHCb measurement of nonleptonic Bc decays @LHCbPhysics @LHCbExperiment \n<LINK> <LINK>'],https://arxiv.org/abs/1708.07504,"We study implications of a recent observation of non-leptonic $B^+_c\to D^0 K^+$ decay and a bound on $B^+_c\to D^0 \pi^+$ transition on CP-violating asymmetries in $B_c$ decays. In the U-spin symmetry limit, we derive a relation between the CP-asymmetries in the $B^+_c\to D^0 K^+$ and $B^+_c\to D^0 \pi^+$ channels and the corresponding branching ratios. We also derive several relations between non-leptonic $B_c$ decays into the final states with $D$ mesons in the flavor $SU(3)_F$ limit. We point out that a combined study of $SU(3)_F$ amplitudes in these decays can be used to constrain the angle $\gamma$ of the Cabibbo-Kobayashi-Maskawa (CKM) matrix. ",Hadronic decays of B_c mesons with flavor SU(3)_F symmetry
11,900878500852371456,36396172,Diego R. Amancio,['New paper w/ @hfarruda et al: an image analysis approach to network classification: <LINK> #MachineLearning #NetworkScience'],https://arxiv.org/abs/1708.07265,"Text network analysis has received increasing attention as a consequence of its wide range of applications. In this work, we extend a previous work founded on the study of topological features of mesoscopic networks. Here, the geometrical properties of visualized networks are quantified in terms of several image analysis techniques and used as subsidies for authorship attribution. It was found that the visual features account for performance similar to that achieved by using topological measurements. In addition, the combination of these two types of features improved the performance. ",An Image Analysis Approach to the Calligraphy of Books
12,900758384122748928,2180768821,Erik Hoel,"['What is causation? How do you measure it? What causes what? I’m on a new paper that just went up on arXiv about this <LINK>', 'this is truly excellent work by my colleagues Larissa Albantakis, William Marshall, and Giulio Tononi', '@ChristianHoney_ @nattyover In IIT, information is integrated by the system itself. But the observer can measure this using information theory and causal analysis', '@ChristianHoney_ @nattyover There purposefully is no observer in the theory, unlike in traditional information theory. An observer can measure it but does not create it', '@ChristianHoney_ @nattyover Sure thing - DM me or email me at hoelerik at gmail']",https://arxiv.org/abs/1708.06716,"Actual causation is concerned with the question ""what caused what?"" Consider a transition between two states within a system of interacting elements, such as an artificial neural network, or a biological brain circuit. Which combination of synapses caused the neuron to fire? Which image features caused the classifier to misinterpret the picture? Even detailed knowledge of the system's causal network, its elements, their states, connectivity, and dynamics does not automatically provide a straightforward answer to the ""what caused what?"" question. Counterfactual accounts of actual causation based on graphical models, paired with system interventions, have demonstrated initial success in addressing specific problem cases in line with intuitive causal judgments. Here, we start from a set of basic requirements for causation (realization, composition, information, integration, and exclusion) and develop a rigorous, quantitative account of actual causation that is generally applicable to discrete dynamical systems. We present a formal framework to evaluate these causal requirements that is based on system interventions and partitions, and considers all counterfactuals of a state transition. This framework is used to provide a complete causal account of the transition by identifying and quantifying the strength of all actual causes and effects linking the two consecutive system states. Finally, we examine several exemplary cases and paradoxes of causation and show that they can be illuminated by the proposed framework for quantifying actual causation. ","What caused what? A quantitative account of actual causation using
  dynamical causal networks"
13,900304629400993792,329705105,Donal Hill,['Can you look for CP violation in decays where you miss some decay products? We did in my new paper! <LINK> @LHCbExperiment'],https://arxiv.org/abs/1708.06370,"Measurements of $CP$ observables in $B^\pm \rightarrow D^{(*)} K^\pm$ and $B^\pm \rightarrow D^{(*)} \pi^\pm$ decays are presented, where $D^{(*)}$ indicates a neutral $D$ or $D^*$ meson that is an admixture of $D^{(*)0}$ and $\bar{D}^{(*)0}$ states. Decays of the $D^*$ meson to the $D\pi^0$ and $D\gamma$ final states are partially reconstructed without inclusion of the neutral pion or photon, resulting in distinctive shapes in the $B$ candidate invariant mass distribution. Decays of the $D$ meson are fully reconstructed in the $K^\pm \pi^\mp$, $K^+ K^-$ and $\pi^+ \pi^-$ final states. The analysis uses a sample of charged $B$ mesons produced in $pp$ collisions collected by the LHCb experiment, corresponding to an integrated luminosity of 2.0, 1.0 and 2.0 fb$^{-1}$ taken at centre-of-mass energies of $\sqrt{s}$ = 7, 8 and 13 TeV, respectively. The study of $B^{\pm} \to D^{*} K^{\pm}$ and $B^{\pm} \to D^{*} \pi^{\pm}$ decays using a partial reconstruction method is the first of its kind, while the measurement of $B^{\pm} \to D K^{\pm}$ and $B^{\pm} \to D \pi^{\pm}$ decays is an update of previous LHCb measurements. The $B^{\pm} \to D K^{\pm}$ results are the most precise to date. ","Measurement of $CP$ observables in $B^\pm \to D^{(*)} K^\pm$ and $B^\pm
  \to D^{(*)} \pi^\pm$ decays"
14,900205209112260609,523241142,Juste Raimbault,['New paper : Calibration of a Density-based Model of Urban Morphogenesis <LINK>'],https://arxiv.org/abs/1708.06743,"We study a stochastic model of urban growth generating spatial distributions of population densities at an intermediate mesoscopic scale. The model is based on the antagonist interplay between the two opposite abstract processes of aggregation (preferential attachment) and diffusion (urban sprawl). Introducing indicators to quantify precisely urban form, the model is first statistically validated and intensively explored to understand its complex behavior across the parameter space. We then compute real morphological measures on local areas of size 50km covering all European Union, and show that the model can reproduce most of existing urban morphologies in Europe. It implies that the morphological dimension of urban growth processes at this scale are sufficiently captured by the two abstract processes of aggregation and diffusion. ",Calibration of a Density-based Model of Urban Morphogenesis
15,899956565045895168,26716739,Mattias Villani,"['New paper with Josef Wilzén, @wandedob. Physiological Gaussian Process Priors for the Hemodynamics in fMRI Analysis <LINK>']",https://arxiv.org/abs/1708.06152,"Background: Inference from fMRI data faces the challenge that the hemodynamic system that relates neural activity to the observed BOLD fMRI signal is unknown. New Method: We propose a new Bayesian model for task fMRI data with the following features: (i) joint estimation of brain activity and the underlying hemodynamics, (ii) the hemodynamics is modeled nonparametrically with a Gaussian process (GP) prior guided by physiological information and (iii) the predicted BOLD is not necessarily generated by a linear time-invariant (LTI) system. We place a GP prior directly on the predicted BOLD response, rather than on the hemodynamic response function as in previous literature. This allows us to incorporate physiological information via the GP prior mean in a flexible way, and simultaneously gives us the nonparametric flexibility of the GP. Results: Results on simulated data show that the proposed model is able to discriminate between active and non-active voxels also when the GP prior deviates from the true hemodynamics. Our model finds time varying dynamics when applied to real fMRI data. Comparison with Existing Method(s): The proposed model is better at detecting activity in simulated data than standard models, without inflating the false positive rate. When applied to real fMRI data, our GP model in several cases finds brain activity where previously proposed LTI models does not. Conclusions: We have proposed a new non-linear model for the hemodynamics in task fMRI, that is able to detect active voxels, and gives the opportunity to ask new kinds of questions related to hemodynamics. ","Physiological Gaussian Process Priors for the Hemodynamics in fMRI
  Analysis"
16,899814904298975232,267958924,Christian Ott,['New paper w grad student Matt Giesler: <LINK> Black-Hole Low-Mass X-Ray Binaries f. Globular Clusters. Great work by Matt!'],https://arxiv.org/abs/1708.05915,"Recent studies suggest that globular clusters (GCs) may retain a substantial population of stellar-mass black holes (BHs), in contrast to the long-held belief of a few to zero BHs. We model the population of BH low-mass X-ray binaries (BH-LMXBs), an ideal observable proxy for elusive single BHs, produced from a representative group of Milky Way GCs with variable BH populations. We simulate the formation of BH-binaries in GCs through exchange interactions between binary and single stars in the company of tens to hundreds of BHs. Additionally, we consider the impact of the BH population on the rate of compact binaries undergoing gravitational wave driven mergers. The characteristics of the BH-LMXB population and binary properties are sensitive to the GCs structural parameters as well as its unobservable BH population. We find that GCs retaining $\sim 1000$ BHs produce a galactic population of $\sim 150$ ejected BH-LMXBs whereas GCs retaining only $\sim20$ BHs produce zero ejected BH-LMXBs. Moreover, we explore the possibility that some of the presently known BH-LMXBs might have originated in GCs and identify five candidate systems. ",Low-mass X-ray binaries from black-hole retaining globular clusters
17,899628135733501952,618466970,Damiano Brigo (Prof),['New working paper: An indifference approach to the cost of capital constraints: KVA and beyond. <LINK>'],https://arxiv.org/abs/1708.05319,"The strengthening of capital requirements has induced banks and traders to consider charging a so called capital valuation adjustment (KVA) to the clients in OTC transactions. This roughly corresponds to charge the clients ex-ante the profit requirement that is asked to the trading desk. In the following we try to delineate a possible way to assess the impact of capital constraints in the valuation of a deal. We resort to an optimisation stemming from an indifference pricing approach, and we study both the linear problem from the point of view of the whole bank and the non-linear problem given by the viewpoint of shareholders. We also consider the case where one optimises the median rather than the mean statistics of the profit and loss distribution. ","An indifference approach to the cost of capital constraints: KVA and
  beyond"
18,899628035359600640,61623544,Dr./Prof. Renée Hložek,"['Excited to announce my new paper with @doddy_marsh, Dan Grin on axion isocurvature! Lots of hard work on this one <LINK>']",https://arxiv.org/abs/1708.05681,"The cosmic microwave background (CMB) places strong constraints on models of dark matter (DM) that deviate from standard cold DM (CDM), and on initial conditions beyond the scalar adiabatic mode. Here, the full \textit{Planck} data set (including temperature, $E$-mode polarisation, and lensing deflection) is used to test the possibility that some fraction of the DM is composed of ultralight axions (ULAs). This represents the first use of CMB lensing to test the ULA model. We find no evidence for a ULA component in the mass range $10^{-33}\leq m_a\leq 10^{-24}\text{ eV}$. We put percent-level constraints on the ULA contribution to the DM, improving by up to a factor of two compared to the case with temperature anisotropies alone. Axion DM also provides a low-energy window onto the high-energy physics of inflation through the interplay between the vacuum misalignment production of axions and isocurvature perturbations. We perform the first systematic investigation into the parameter space of ULA isocurvature, using an accurate isocurvature transfer function at all $m_{a}$ values. We precisely identify a ""window of co-existence"" for $10^{-25}\text{ eV}\leq m_a\leq10^{-24}\text{ eV}$ where the data allow, simultaneously, a $\sim10\%$ contribution of ULAs to the DM, and $\sim 1\%$ contributions of isocurvature and tensors to the CMB power. ULAs in this window (and \textit{all} lighter ULAs) are shown to be consistent with a large inflationary Hubble parameter, $H_I\sim 10^{14}\text{ GeV}$. The window of co-existence will be fully probed by proposed CMB-S4 observations with increased accuracy in the high-$\ell$ lensing power and low-$\ell$ $E$ and $B$-mode polarisation. If ULAs in the window exist, this could allow for two independent measurements of $H_I$ in the CMB using the axion DM content and isocurvature, and the tensor contribution to $B$-modes. ","Using the Full Power of the Cosmic Microwave Background to Probe Axion
  Dark Matter"
19,898533551917936640,3301643341,Roger Grosse,"['New paper by Yuhuai Wu &amp; Elman Mansimov combining A2C + K-FAC. 3x sample efficiency, continuous control from pixels. <LINK>']",https://arxiv.org/abs/1708.05144,"In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at this https URL ","Scalable trust-region method for deep reinforcement learning using
  Kronecker-factored approximation"
20,898360217557061633,33113669,Tim Baldwin,"['New paper on regression done right for geoloc prediction and dialectology: <LINK> #emnlp2017', ""@jasonbaldridge Interesting idea. We did earlier work on ellipsoid distributions of arbitrary orientation, but didn't get anything out of it empirically"", '@jasonbaldridge @josephreisinger Ah OK, that goes a lot further than what I was thinking. Nice! Will try to get my head around what this would look like in our context.', '@bgmartins @jasonbaldridge @josephreisinger Very cool (generally, as well as specifically for some of the tasks we are playing around with) ... thanks for the pointer!']",https://arxiv.org/abs/1708.04358,"We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset. ","Continuous Representation of Location for Geolocation and Lexical
  Dialectology using Mixture Density Networks"
21,898000268910047233,1439446945,Lav Varshney,"['New paper on coupon-collector model of discovery, inspired by @arbesman <LINK> @CSL_Illinois @ECEILLINOIS @IBMResearch']",https://arxiv.org/abs/1708.03833,"Empirical studies of scientific discovery---so-called Eurekometrics---have indicated that the output of exploration proceeds as a logistic growth curve. Although logistic functions are prevalent in explaining population growth that is resource-limited to a given carrying capacity, their derivation do not apply to discovery processes. This paper develops a generative model for logistic \emph{knowledge discovery} using a novel extension of coupon collection, where an explorer interested in discovering all unknown elements of a set is supported by technology that can respond to queries. This discovery process is parameterized by the novelty and quality of the set of discovered elements at every time step, and randomness is demonstrated to improve performance. Simulation results provide further intuition on the discovery process. ",A Coupon-Collector Model of Machine-Aided Discovery
22,897813871192408064,2799725019,Laura Kreidberg,"['New paper with @AstroCaroline predicts observability of nearby terrestrial worlds with JWST!  <LINK> <LINK>', '@AstroCaroline our planet is lame right now ... hope you enjoy a break thinking about these other ones!']",https://arxiv.org/abs/1708.04239,"Nine transiting Earth-sized planets have recently been discovered around nearby late M dwarfs, including the TRAPPIST-1 planets and two planets discovered by the MEarth survey, GJ 1132b and LHS 1140b. These planets are the smallest known planets that may have atmospheres amenable to detection with JWST. We present model thermal emission and transmission spectra for each planet, varying composition and surface pressure of the atmosphere. We base elemental compositions on those of Earth, Titan, and Venus and calculate the molecular compositions assuming chemical equilibrium, which can strongly depend on temperature. Both thermal emission and transmission spectra are sensitive to the atmospheric composition; thermal emission spectra are sensitive to surface pressure and temperature. We predict the observability of each planet's atmosphere with JWST. GJ 1132b and TRAPPIST-1b are excellent targets for emission spectroscopy with JWST/MIRI, requiring fewer than 10 eclipse observations. Emission photometry for TRAPPIST-1c requires 5-15 eclipses; LHS 1140b and TRAPPIST-1d, TRAPPIST-1e, and TRAPPIST-1f, which could possibly have surface liquid water, may be accessible with photometry. Seven of the nine planets are strong candidates for transmission spectroscopy measurements with JWST, though the number of transits required depends strongly on the planets' actual masses. Using the measured masses, fewer than 20 transits are required for a 5 sigma detection of spectral features for GJ 1132b and six of the TRAPPIST-1 planets. Dedicated campaigns to measure the atmospheres of these nine planets will allow us, for the first time, to probe formation and evolution processes of terrestrial planetary atmospheres beyond our solar system. ","Observing the Atmospheres of Known Temperate Earth-sized Planets with
  JWST"
23,897651025276010496,22399655,Ryota Kanai💡,"['Our new paper on how to learn body affordances by embedding action consequences to low dimensional control space.\n<LINK>', 'The code is available from github.\nhttps://t.co/bLiMh1Ro3e']",https://arxiv.org/abs/1708.04391,"Controlling embodied agents with many actuated degrees of freedom is a challenging task. We propose a method that can discover and interpolate between context dependent high-level actions or body-affordances. These provide an abstract, low-dimensional interface indexing high-dimensional and time- extended action policies. Our method is related to recent ap- proaches in the machine learning literature but is conceptually simpler and easier to implement. More specifically our method requires the choice of a n-dimensional target sensor space that is endowed with a distance metric. The method then learns an also n-dimensional embedding of possibly reactive body-affordances that spread as far as possible throughout the target sensor space. ",Learning body-affordances to simplify action spaces
24,897325233085923328,267958924,Christian Ott,"['New paper: Sperhake+, Long-lived inverse chirp signals from core collapse in massive scalar-tensor gravity -- <LINK>']",https://arxiv.org/abs/1708.03651,This letter considers stellar core collapse in massive scalar--tensor theories of gravity. The presence of a mass term for the scalar field allows for dramatic increases in the radiated gravitational wave signal. There are several potential smoking gun signatures of a departure from general relativity associated with this process. These signatures could show up within existing LIGO--Virgo searches. ,"Long-lived inverse chirp signals from core collapse in massive
  scalar-tensor gravity"
25,895975051299880961,822867138,Bradley Kavanagh,"['""Time-integrated directional detection of dark matter."" My new paper with @cajohare &amp; @ultra_wimp - <LINK> <LINK>']",https://arxiv.org/abs/1708.02959,"The analysis of signals in directional dark matter (DM) detectors typically assumes that the directions of nuclear recoils can be measured in the Galactic rest frame. However, this is not possible with all directional detection technologies. In nuclear emulsions, for example, the recoil events must be detected and measured after the exposure time of the experiment. Unless the entire detector is mounted and rotated with the sidereal day, the recoils cannot be reoriented in the Galactic rest frame. We examine the effect of this `time integration' on the primary goals of directional detection, namely: (1) confirming that the recoils are anisotropic; (2) measuring the median recoil direction to confirm their Galactic origin; and (3) probing below the neutrino floor. We show that after time integration the DM recoil distribution retains a preferred direction and is distinct from that of Solar neutrino-induced recoils. Many of the advantages of directional detection are therefore preserved and it is not crucial to mount and rotate the detector. Rejecting isotropic backgrounds requires a factor of 2 more signal events compared with an experiment with event time information, whereas a factor of 1.5-3 more events are needed to measure a median direction in agreement with the expectation for DM. We also find that there is still effectively no neutrino floor in a time-integrated directional experiment. However to reach a cross section an order of magnitude below the floor, a factor of 8 larger exposure is required than with a conventional directional experiment. We also examine how the sensitivity is affected for detectors with only 2D recoil track readout, and/or no head-tail measurement. As for non-time-integrated experiments, 2D readout is not a major disadvantage, though a lack of head-tail sensitivity is. ",Time-integrated directional detection of dark matter
26,895956327167557632,762420558,Ciaran O'Hare,"['New paper! <LINK>', 'Bit Niche, \nQ: Do I really have to mount a nuclear emulsions dark matter detector on a €255k equatorial telescope?\nA: Not necessarily.']",https://arxiv.org/abs/1708.02959,"The analysis of signals in directional dark matter (DM) detectors typically assumes that the directions of nuclear recoils can be measured in the Galactic rest frame. However, this is not possible with all directional detection technologies. In nuclear emulsions, for example, the recoil events must be detected and measured after the exposure time of the experiment. Unless the entire detector is mounted and rotated with the sidereal day, the recoils cannot be reoriented in the Galactic rest frame. We examine the effect of this `time integration' on the primary goals of directional detection, namely: (1) confirming that the recoils are anisotropic; (2) measuring the median recoil direction to confirm their Galactic origin; and (3) probing below the neutrino floor. We show that after time integration the DM recoil distribution retains a preferred direction and is distinct from that of Solar neutrino-induced recoils. Many of the advantages of directional detection are therefore preserved and it is not crucial to mount and rotate the detector. Rejecting isotropic backgrounds requires a factor of 2 more signal events compared with an experiment with event time information, whereas a factor of 1.5-3 more events are needed to measure a median direction in agreement with the expectation for DM. We also find that there is still effectively no neutrino floor in a time-integrated directional experiment. However to reach a cross section an order of magnitude below the floor, a factor of 8 larger exposure is required than with a conventional directional experiment. We also examine how the sensitivity is affected for detectors with only 2D recoil track readout, and/or no head-tail measurement. As for non-time-integrated experiments, 2D readout is not a major disadvantage, though a lack of head-tail sensitivity is. ",Time-integrated directional detection of dark matter
27,895570194327674880,3869827637,Francesca Vidotto,['New paper out with @Alvise_ on #QuantumGravity affects in the late universe considering #PBH as #DarkMatter! <LINK> #loopy <LINK>'],http://arxiv.org/abs/1708.02588,"It has been recently suggested that small mass black holes (BHs) may become unstable due to quantum-gravitational effects and eventually decay, producing radiation, on a timescale shorter than the Hawking evaporation time. We argue that the existence of a population of low-mass Primordial Black Holes (PBHs) acting as a fraction of the Universe dark matter component can be used to test proposed models of quantum decay of BHs via their effect on galaxy number counts. We study what constraints future galaxy clustering measurements can set on quantum-gravity parameters governing the BH lifetime and PBH abundance. In case of no detection of such effects, this would rule out either the existence of a non-negligible number of small PBHs, or the BH quantum decay scenario (or both). In case of independent observations of PBHs, the observables discussed here could be used to study the quantum effects that modify the final fate of BHs. ","Effects of primordial black holes quantum gravity decay on galaxy
  clustering"
28,895272749438054400,65010804,Hugh Osborn,"['Tau Ceti is one of the closest sunlike stars. A new paper today suggests four super-Earth #exoplanets may orbit it: <LINK> <LINK>', 'Of course, a paper in 2012 announced 5 planets... most of which turned out not to exist. Two (e &amp; f) have survived this new analyses though.', 'The team remove lots of noise to reveal the planetary signals which would, if confirmed, be the lowest-amplitude detections ever with RVs. https://t.co/2oECrOTMm0', 'They also find the planets on orbits far more eccentric than should be possible/expected in a stable solar system.', ""All in all: super interesting work! It's suggestive that Tau Ceti has planets, especially the long-period ones far from stellar rotation."", 'BUT I think more data &amp; analyses is needed before we can call them bona fide, confirmed planets (as the spurious 2012 detections showed).']",https://arxiv.org/abs/1708.02051,"The removal of noise typically correlated in time and wavelength is one of the main challenges for using the radial velocity method to detect Earth analogues. We analyze radial velocity data of tau Ceti and find robust evidence for wavelength dependent noise. We find this noise can be modeled by a combination of moving average models and ""differential radial velocities"". We apply this noise model to various radial velocity data sets for tau Ceti, and find four periodic signals at 20.0, 49.3, 160 and 642 d which we interpret as planets. We identify two new signals with orbital periods of 20.0 and 49.3 d while the other two previously suspected signals around 160 and 600 d are quantified to a higher precision. The 20.0 d candidate is independently detected in KECK data. All planets detected in this work have minimum masses less than 4$M_\oplus$ with the two long period ones located around the inner and outer edges of the habitable zone, respectively. We find that the instrumental noise gives rise to a precision limit of the HARPS around 0.2 m/s. We also find correlation between the HARPS data and the central moments of the spectral line profile at around 0.5 m/s level, although these central moments may contain both noise and signals. The signals detected in this work have semi-amplitudes as low as 0.3 m/s, demonstrating the ability of the radial velocity technique to detect relatively weak signals. ","Color difference makes a difference: four planet candidates around tau
  Ceti"
29,894871361616658432,131879500,John Ilee,"['Our new paper on the chemistry of protoplanetary fragments formed via gravitational instability #astrochemistry\n\n<LINK> <LINK>', 'with @dh4gan @cassidentprone @ExoKenRice et al. (and, of course, the Fab Four)']",http://arxiv.org/abs/1708.01815,"In this paper, we model the chemical evolution of a 0.25 M$_{\odot}$ protoplanetary disc surrounding a 1 M$_{\odot}$ star that undergoes fragmentation due to self-gravity. We use Smoothed Particle Hydrodynamics including a radiative transfer scheme, along with time-dependent chemical evolution code to follow the composition of the disc and resulting fragments over approximately 4000 yrs. Initially, four quasi-stable fragments are formed, of which two are eventually disrupted by tidal torques in the disc. From the results of our chemical modelling, we identify species that are abundant in the fragments (e.g. H$_{\rm 2}$O, H$_{\rm 2}$S, HNO, N$_{\rm 2}$, NH$_{\rm 3}$, OCS, SO), species that are abundant in the spiral shocks within the disc (e.g. CO, CH$_{\rm 4}$, CN, CS, H$_{\rm 2}$CO), and species which are abundant in the circumfragmentary material (e.g. HCO$^{\rm +}$). Our models suggest that in some fragments it is plausible for grains to sediment to the core before releasing their volatiles into the planetary envelope, leading to changes in, e.g., the C/O ratio of the gas and ice components. We would therefore predict that the atmospheric composition of planets generated by gravitational instability should not necessarily follow the bulk chemical composition of the local disc material. ","The chemistry of protoplanetary fragments formed via gravitational
  instabilities"
30,894576746170920961,3885912072,Marshall Johnson,"['New paper: measurements of the spin-orbit misalignments of HAT-P-41 b, WASP-79 b, and Kepler-448 b: <LINK>']",https://arxiv.org/abs/1708.01291,"We present measurements of the spin-orbit misalignments of the hot Jupiters HAT-P-41 b and WASP-79 b, and the aligned warm Jupiter Kepler-448 b. We obtained these measurements with Doppler tomography, where we spectroscopically resolve the line profile perturbation during the transit due to the Rossiter-McLaughlin effect. We analyze time series spectra obtained during portions of five transits of HAT-P-41 b, and find a value of the spin-orbit misalignment of $\lambda = -22.1_{-6.0}^{+0.8 \circ}$. We reanalyze the radial velocity Rossiter-McLaughlin data on WASP-79 b obtained by Addison et al. (2013) using Doppler tomographic methodology. We measure $\lambda=-99.1_{-3.9}^{+4.1\circ}$, consistent with but more precise than the value found by Addison et al. (2013). For Kepler-448 b we perform a joint fit to the Kepler light curve, Doppler tomographic data, and a radial velocity dataset from Lillo-Box et al. (2015). We find an approximately aligned orbit ($\lambda=-7.1^{+4.2 \circ}_{-2.8}$), in modest disagreement with the value found by Bourrier et al. (2015). Through analysis of the Kepler light curve we measure a stellar rotation period of $P_{\mathrm{rot}}=1.27 \pm 0.11$ days, and use this to argue that the full three-dimensional spin-orbit misalignment is small, $\psi\sim0^{\circ}$. ",Spin-Orbit Misalignments of Three Jovian Planets via Doppler Tomography
31,894487709527494656,18247347,Tiago Peixoto,"['New paper on @arxiv: ""Nonparametric weighted stochastic block models"" <LINK> <LINK>']",https://arxiv.org/abs/1708.01432,"We present a Bayesian formulation of weighted stochastic block models that can be used to infer the large-scale modular structure of weighted networks, including their hierarchical organization. Our method is nonparametric, and thus does not require the prior knowledge of the number of groups or other dimensions of the model, which are instead inferred from data. We give a comprehensive treatment of different kinds of edge weights (i.e. continuous or discrete, signed or unsigned, bounded or unbounded), as well as arbitrary weight transformations, and describe an unsupervised model selection approach to choose the best network description. We illustrate the application of our method to a variety of empirical weighted networks, such as global migrations, voting patterns in congress, and neural connections in the human brain. ",Nonparametric weighted stochastic block models
32,894473266307502080,864460226,Ángel J. Gallego,"['New paper with @OrusRoman \n\nGenerative syntax, renormalization, tensor networks, quantum computation, and more... \n\n<LINK>']",https://arxiv.org/abs/1708.01525,"Here we consider some well-known facts in syntax from a physics perspective, allowing us to establish equivalences between both fields with many consequences. Mainly, we observe that the operation MERGE, put forward by N. Chomsky in 1995, can be interpreted as a physical information coarse-graining. Thus, MERGE in linguistics entails information renormalization in physics, according to different time scales. We make this point mathematically formal in terms of language models. In this setting, MERGE amounts to a probability tensor implementing a coarse-graining, akin to a probabilistic context-free grammar. The probability vectors of meaningful sentences are given by stochastic tensor networks (TN) built from diagonal tensors and which are mostly loop-free, such as Tree Tensor Networks and Matrix Product States, thus being computationally very efficient to manipulate. We show that this implies the polynomially-decaying (long-range) correlations experimentally observed in language, and also provides arguments in favour of certain types of neural networks for language processing. Moreover, we show how to obtain such language models from quantum states that can be efficiently prepared on a quantum computer, and use this to find bounds on the perplexity of the probability distribution of words in a sentence. Implications of our results are discussed across several ambits. ",Language Design as Information Renormalization
33,894394979023372288,140770306,Masafumi Oizumi,"['Our new paper ""Fast and exact search for the partition with minimal information loss"" is available on arXiv. <LINK>']",https://arxiv.org/abs/1708.01444,"In analysis of multi-component complex systems, such as neural systems, identifying groups of units that share similar functionality will aid understanding of the underlying structures of the system. To find such a grouping, it is useful to evaluate to what extent the units of the system are separable. Separability or inseparability can be evaluated by quantifying how much information would be lost if the system were partitioned into subsystems, and the interactions between the subsystems were hypothetically removed. A system of two independent subsystems are completely separable without any loss of information while a system of strongly interacted subsystems cannot be separated without a large loss of information. Among all the possible partitions of a system, the partition that minimizes the loss of information, called the Minimum Information Partition (MIP), can be considered as the optimal partition for characterizing the underlying structures of the system. Although the MIP would reveal novel characteristics of the neural system, an exhaustive search for the MIP is numerically intractable due to the combinatorial explosion of possible partitions. Here, we propose a computationally efficient search to precisely identify the MIP among all possible partitions by exploiting the submodularity of the measure of information loss. Mutual information is one such submodular information loss functions, and is a natural choice for measuring the degree of statistical dependence between paired sets of random variables. By using mutual information as a loss function, we show that the search for MIP can be performed in a practical order of computational time for a reasonably large system. We also demonstrate that MIP search allows for the detection of underlying global structures in a network of nonlinear oscillators. ",Fast and exact search for the partition with minimal information loss
34,893524518035234820,251927957,sorelle,"['New workshop paper on Interpretable Active Learning! Undergrad first author @rlanasphillips will present at #WHI2017 <LINK>', '@rlanasphillips Preview of one cool finding: can use new measure for uncertainty bias to see diff in uncertainty by race on @ProPublica recidivism data. https://t.co/g71oSAVWRu', '@krvarshney @rlanasphillips Thanks for organizing the workshop! Wish I could be there.']",https://arxiv.org/abs/1708.00049,"Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model's predictions between one subgroup and another. Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. ",Interpretable Active Learning
35,893481661920419841,932805374,KordingLab 🦖,['Modern machine learning helps a lot for decoding: <LINK> - new paper with Josh Glaser. code: <LINK>'],https://arxiv.org/abs/1708.00909,"Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outperform traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help advance engineering applications such as brain machine interfaces. ",Machine learning for neural decoding
36,893463498222710789,195474093,Jelmer Wolterink,['Our new paper on deep learning-based cardiac MR segmentation and disease classification is now on arXiv <LINK>'],https://arxiv.org/abs/1708.01141v1,"Segmentation of the heart in cardiac cine MR is clinically used to quantify cardiac function. We propose a fully automatic method for segmentation and disease classification using cardiac cine MR images. A convolutional neural network (CNN) was designed to simultaneously segment the left ventricle (LV), right ventricle (RV) and myocardium in end-diastole (ED) and end-systole (ES) images. Features derived from the obtained segmentations were used in a Random Forest classifier to label patients as suffering from dilated cardiomyopathy, hypertrophic cardiomyopathy, heart failure following myocardial infarction, right ventricular abnormality, or no cardiac disease. The method was developed and evaluated using a balanced dataset containing images of 100 patients, which was provided in the MICCAI 2017 automated cardiac diagnosis challenge (ACDC). The segmentation and classification pipeline were evaluated in a four-fold stratified cross-validation. Average Dice scores between reference and automatically obtained segmentations were 0.94, 0.88 and 0.87 for the LV, RV and myocardium. The classifier assigned 91% of patients to the correct disease category. Segmentation and disease classification took 5 s per patient. The results of our study suggest that image-based diagnosis using cine MR cardiac scans can be performed automatically with high accuracy. ","] Automatic Segmentation and Disease Classification Using Cardiac Cine MR
  Images"
37,893129474337845250,124319949,Alessandro Sordoni,"['Our new ICML paper on meta active learning is out ! <LINK> @hugo_larochelle @OriolVinyalsML @elasri_layla @caglarml', '@hugo_larochelle @OriolVinyalsML @elasri_layla @caglarml Thx! in the low-sample OG setting, MinMaxCos is particularly effective. Meta active lrng performs better on the movie recommendation, though']",https://arxiv.org/abs/1708.00088,"We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings. ",Learning Algorithms for Active Learning
38,893032416188694528,2546724918,Francesc Vilardell,"['New paper published in arXiv: ""The massive multiple system HD 64315"": <LINK>']",https://arxiv.org/abs/1708.00849,"The O6 Vn star HD 64315 is believed to belong to the star-forming region known as NGC 2467, but previous distance estimates do not support this association. We explore the multiple nature of this star with the aim of determining its distance, and understanding its connection to NGC 2467. A total of 52 high-resolution spectra have been gathered over a decade. We use their analysis, in combination with the photometric data from All Sky Automated Survey and Hipparcos catalogues, to conclude that HD 64315 is composed of at least two spectroscopic binaries, one of which is an eclipsing binary. HD 64315 contains two binary systems, one of which is an eclipsing binary. The two binaries are separated by 0.09 arcsec (or 500 AU) if the most likely distance to the system, around 5 kpc, is considered. The presence of fainter companions is not excluded by current observations. The non-eclipsing binary (HD 64315 AaAb) has a period of 2.70962901+/-0.00000021 d. Its components are hotter than those of the eclipsing binary, and dominate the appearance of the system. The eclipsing binary (HD 64315 BaBb) has a shorter period of 1.0189569+/-0.0000008 d. We derive masses of 14.6+-2.3 M$_\odot$ for both components of the BaBb system. They are almost identical; both stars are overfilling their respective Roche lobes, and share a common envelope in an overcontact configuration. The non-eclipsing binary is a detached system composed of two stars with spectral types around O6 V with minimum masses of 10.8 M$_\odot$ and 10.2 M$_\odot$, and likely masses aprox. 30 M$_\odot$. HD 64315 provides a cautionary tale about high-mass star isolation and multiplicity. Its total mass is likely above 90 M$_\odot$,but it seems to have formed without an accompanying cluster. It contains one the most massive overcontact binaries known, a likely merger progenitor in a very wide multiple system. ",The massive multiple system HD 64315
39,892801460706082817,916679210,Dr. Yanjun Qi,['Our new paper @ arxiv: <LINK> : Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin'],http://arxiv.org/abs/1708.00339,"The past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together? Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; we call AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long short-term memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map. Code and data are shared at www.deepchrome.org ","Attend and Predict: Understanding Gene Regulation by Selective Attention
  on Chromatin"
40,892743808064135170,2999702157,Anton Ilderton,"['New paper on positron production in strong laser fields, in @MSCActions supported research! <LINK>']",https://arxiv.org/abs/1708.00298,"Showers of $\gamma$-rays and positrons are produced when a high-energy electron beam collides with a super-intense laser pulse. We present scaling laws for the electron beam energy loss, the $\gamma$-ray spectrum, and the positron yield and energy that are valid in the non-linear, radiation-reaction--dominated regime. As an application we demonstrate that by employing the collision of a $>$GeV electron beam with a laser pulse of intensity $>5\times10^{21}\,\text{Wcm}^{-2}$, today's high-intensity laser facilities are capable of producing $O(10^4)$ positrons per shot via light-by-light scattering. ",Scaling laws for positron production in laser--electron-beam collisions
41,892743591206035456,2999702157,Anton Ilderton,['New paper on scaling laws for #positron production in #lasers! By @MattiasMarklund @ChalmersPhysics @PhysicsatYork!  <LINK>'],https://arxiv.org/abs/1708.00298,"Showers of $\gamma$-rays and positrons are produced when a high-energy electron beam collides with a super-intense laser pulse. We present scaling laws for the electron beam energy loss, the $\gamma$-ray spectrum, and the positron yield and energy that are valid in the non-linear, radiation-reaction--dominated regime. As an application we demonstrate that by employing the collision of a $>$GeV electron beam with a laser pulse of intensity $>5\times10^{21}\,\text{Wcm}^{-2}$, today's high-intensity laser facilities are capable of producing $O(10^4)$ positrons per shot via light-by-light scattering. ",Scaling laws for positron production in laser--electron-beam collisions
42,894951186939338752,38637367,Debidatta Dwibedi,"[""Want to detect objects but don't have bounding box annotations,check out our new paper:<LINK> #DeepLearning #computervision <LINK>"", 'Cut out objects, paste them in real scenes with different modes of blending and train an object detector on these synthetic images. https://t.co/mjHB0NjbsN', ""Important takeaways: 1. Synthesized scenes need not look 'realistic' 2. Randomization is essential for effective data augmentation https://t.co/hH9PBJZqca"", '3. Adding synthetic data helps: i) Adds complementary information to existing real images ii) Model needs fewer annotated real images']",https://arxiv.org/abs/1708.01642,"A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data. ","Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection"
43,902468316232601601,778334935086141440,Hamish Silverwood,"['Hot off the presses, we find local #DarkMatter density with SDSS G-dwarfs is ρDM=0.46 GeV/cm3=0.012 Msun/pc3 \n<LINK>']",https://arxiv.org/abs/1708.07836,"We derive the local dark matter density by applying the integrated Jeans equation method from Silverwood et al. (2016) to SDSS-SEGUE G-dwarf data processed and presented by B\""udenbender et al. (2015). We use the MultiNest Bayesian nested sampling software to fit a model for the baryon distribution, dark matter and tracer stars, including a model for the 'tilt term' that couples the vertical and radial motions, to the data. The $\alpha$-young population from B\""udenbender et al. (2015) yields the most reliable result of $\rho_{\rm DM} = 0.46^{+0.07}_{-0.09}\, {{\rm GeV\, cm}^{-3}} = 0.012^{+0.001}_{-0.002}\, {{\rm M}_\odot \, {\rm pc}^{-3}}$. Our analyses yield inconsistent results for the $\alpha$-young and $\alpha$-old data, pointing to problems in the tilt term and its modelling, the data itself, the assumption of a flat rotation curve, or the effects of disequilibria. ",The Local Dark Matter Density from SDSS-SEGUE G-dwarfs
44,893065563806846976,809336237194706944,Rico Sennrich,['UEDIN did well at WMT17 - (tied) best constrained system for most language pairs. Find out what we did at <LINK>'],https://arxiv.org/abs/1708.00726,"This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. ",The University of Edinburgh's Neural MT Systems for WMT17
