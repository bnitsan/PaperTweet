,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,848857130350280705,152342302,Alex Bentley,['Our new paper on the role of neutral evolution in word turnover during centuries of English word popularity: <LINK>'],http://arxiv.org/abs/1703.10698,"Here we test Neutral models against the evolution of English word frequency and vocabulary at the population scale, as recorded in annual word frequencies from three centuries of English language books. Against these data, we test both static and dynamic predictions of two neutral models, including the relation between corpus size and vocabulary size, frequency distributions, and turnover within those frequency distributions. Although a commonly used Neutral model fails to replicate all these emergent properties at once, we find that modified two-stage Neutral model does replicate the static and dynamic properties of the corpus data. This two-stage model is meant to represent a relatively small corpus (population) of English books, analogous to a `canon', sampled by an exponentially increasing corpus of books in the wider population of authors. More broadly, this mode -- a smaller neutral model within a larger neutral model -- could represent more broadly those situations where mass attention is focused on a small subset of the cultural variants. ",Neutral evolution and turnover over centuries of English word popularity
1,847733511008038912,315293016,Duccio Piovani,"[""Many nice and colourful maps in our new paper where we relate a #city's retail distribution with its #roadnetwork <LINK>""]",https://arxiv.org/abs/1703.10419,"The study of the properties and structure of a city's road network has for many years been the focus of much work, as has the mathematical modelling of the location of its retail activity and of the emergence of clustering in retail centres. Despite these two phenomena strongly depending on one another and their fundamental importance in understanding cities, little work has been done in order to compare their evolution and their local and global properties. The contribution of this paper aims to highlight the strong relationship that retail dynamics have with the hierarchical structure of the underlying road network. We achieve this by comparing the results of the entropy maximising retail model with a percolation analysis of the road network in the city of London. We interpret the great agreement in the hierarchical spatial organisation outlined by these two approaches as new evidence of the interdependence of these two crucial dimensions of a city's life. ","Urban retail dynamics: insights from percolation theory and spatial
  interaction modelling"
2,847446755411218437,325551847,Jason Corso,['New ArXiv paper on human-in-the-loop cnn-based inference for pose estimation: Click-Here CNNs. <LINK> #news'],https://arxiv.org/abs/1703.09859,"We motivate and address a human-in-the-loop variant of the monocular viewpoint estimation task in which the location and class of one semantic object keypoint is available at test time. In order to leverage the keypoint information, we devise a Convolutional Neural Network called Click-Here CNN (CH-CNN) that integrates the keypoint information with activations from the layers that process the image. It transforms the keypoint information into a 2D map that can be used to weigh features from certain parts of the image more heavily. The weighted sum of these spatial features is combined with global image features to provide relevant information to the prediction layers. To train our network, we collect a novel dataset of 3D keypoint annotations on thousands of CAD models, and synthetically render millions of images with 2D keypoint information. On test instances from PASCAL 3D+, our model achieves a mean class accuracy of 90.7%, whereas the state-of-the-art baseline only obtains 85.7% mean class accuracy, justifying our argument for human-in-the-loop inference. ","Click Here: Human-Localized Keypoints as Guidance for Viewpoint
  Estimation"
3,847435233410600960,3377401715,Erin Milne,['New paper published on ArXiv! Here I show that my algorithm from last year works for more cases!\n<LINK>'],https://arxiv.org/abs/1703.09737,"We show that our algorithm for inverting the sweep map on (2n, n)-Dyck paths works for any (kn, n)-Dyck path, where k is an arbitrary positive integer. ","Inverting the Sweep Map on (kn,n)-Dyck Paths: A Simple Algorithm"
4,847429416258904064,123664603,Marcelo Gleiser,['New paper applying information theory to predict spontaneous decay rates in atoms w/ N. Jiang <LINK>'],https://arxiv.org/abs/1703.06818,"We show that a newly proposed Shannon-like entropic measure of shape complexity applicable to spatially-localized or periodic mathematical functions known as configurational entropy (CE) can be used as a predictor of spontaneous decay rates for one-electron atoms. The CE is constructed from the Fourier transform of the atomic probability density. For the hydrogen atom with degenerate states labeled with the principal quantum number n, we obtain a scaling law relating the n-averaged decay rates to the respective CE. The scaling law allows us to predict the n-averaged decay rate without relying on the traditional computation of dipole matrix elements. We tested the predictive power of our approach up to n=20, obtaining an accuracy better than 3.7% within our numerical precision, as compared to spontaneous decay tables listed in the literature. ",Predicting Atomic Decay Rates Using an Informational-Entropic Approach
5,847375582090321920,206369044,Sverre Holm,"['New paper ""Spring-damper equivalents of the fractional, poroelastic, and poroviscoelastic models for elastography"" <LINK> <LINK>']",https://arxiv.org/abs/1703.09515,"In MR elastography it is common to use an elastic model for the tissue's response in order to properly interpret the results. More complex models such as viscoelastic, fractional viscoelastic, poroelastic, or poroviscoelastic ones are also used. These models appear at first sight to be very different, but here it is shown that they all may be expressed in terms of elementary viscoelastic models. For a medium expressed with fractional models, many elementary spring-damper combinations are added, each of them weighted according to a long-tailed distribution, hinting at a fractional distribution of time constants or relaxation frequencies. This may open up for a more physical interpretation of the fractional models. The shear wave component of the poroelastic model is shown to be modeled exactly by a three-component Zener model. The extended poroviscoelastic model is found to be equivalent to what is called a non-standard four-parameter model. Accordingly, the large number of parameters in the porous models can be reduced to the same number as in their viscoelastic equivalents. As long as the individual displacements from the solid and fluid parts cannot be measured individually the main use of the poro(visco)elastic models is therefore as a physics based method for determining parameters in a viscoelastic model. ","Spring-damper equivalents of the fractional, poroelastic, and
  poroviscoelastic models for elastography"
6,847345756587479040,80324722,David Long,['New paper studying the coronal magnetic field is now on arXiv: <LINK> Lots of @LeverhulmeTrust collaboration too!'],https://arxiv.org/abs/1703.10020,"""EIT waves"" are freely-propagating global pulses in the low corona which are strongly associated with the initial evolution of coronal mass ejections (CMEs). They are thought to be large-amplitude, fast-mode magnetohydrodynamic waves initially driven by the rapid expansion of a CME in the low corona. An ""EIT wave"" was observed on 6 July 2012 to impact an adjacent trans-equatorial loop system which then exhibited a decaying oscillation as it returned to rest. Observations of the loop oscillations were used to estimate the magnetic field strength of the loop system by studying the decaying oscillation of the loop, measuring the propagation of ubiquitous transverse waves in the loop and extrapolating the magnetic field from observed magnetograms. Observations from the Atmospheric Imaging Assembly onboard the Solar Dynamics Observatory (SDO/AIA) and the Coronal Multi-channel Polarimeter (CoMP) were used to study the event. An Empirical Mode Decomposition analysis was used to characterise the oscillation of the loop system in CoMP Doppler velocity and line width and in AIA intensity. The loop system was shown to oscillate in the 2nd harmonic mode rather than at the fundamental frequency, with the seismological analysis returning an estimated magnetic field strength of ~5.5+/-1.5 G. This compares to the magnetic field strength estimates of ~1-9 G and ~3-9 G found using the measurements of transverse wave propagation and magnetic field extrapolation respectively. ","Measuring the magnetic field of a trans-equatorial loop system using
  coronal seismology"
7,847272356355620864,4438354094,Tom Wong,['New paper! Coined quantum walks on weighted graphs. <LINK> <LINK>'],http://arxiv.org/abs/1703.10134,"We define a discrete-time, coined quantum walk on weighted graphs that is inspired by Szegedy's quantum walk. Using this, we prove that many lackadaisical quantum walks, where each vertex has $l$ integer self-loops, can be generalized to a quantum walk where each vertex has a single self-loop of real-valued weight $l$. We apply this real-valued lackadaisical quantum walk to two problems. First, we analyze it on the line or one-dimensional lattice, showing that it is exactly equivalent to a continuous deformation of the three-state Grover walk with faster ballistic dispersion. Second, we generalize Grover's algorithm, or search on the complete graph, to have a weighted self-loop at each vertex, yielding an improved success probability when $l < 3 + 2\sqrt{2} \approx 5.828$. ",Coined Quantum Walks on Weighted Graphs
8,847252670335209472,21611239,Sean Carroll,['Fields throughout spacetime could conspire to cancel the cosmological constant. New paper with Grant Remmen. <LINK>'],https://arxiv.org/abs/1703.09715,"We construct a model in which the cosmological constant is canceled from the gravitational equations of motion. Our model relies on two key ingredients: a nonlocal constraint on the action, which forces the spacetime average of the Lagrangian density to vanish, and a dynamical way for this condition to be satisfied classically with arbitrary matter content. We implement the former condition with a spatially-constant Lagrange multiplier associated with the volume form and the latter by including a free four-form gauge field strength in the action. These two features are enough to remove the cosmological constant from the Einstein equation. The model is consistent with all cosmological and experimental bounds on modification of gravity and allows for both cosmic inflation and the present epoch of acceleration. ",A Nonlocal Approach to the Cosmological Constant Problem
9,847127557690544130,4541664553,Eugene Belilovsky,['Check out our new paper with scattering networks on imagenet from the Paris mob <LINK> @szagoruyko5'],https://arxiv.org/abs/1703.08961,"We use the scattering network as a generic and fixed ini-tialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 x 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset. ",Scaling the Scattering Transform: Deep Hybrid Networks
10,846910058688602115,3234676147,Andrew Higgins,"['New arXiv paper: Classic CJ detonation solution only valid when media is homogeneous, otherwise super-CJ waves seen. <LINK>']",https://arxiv.org/abs/1703.09321,"Detonation propagation in a compressible medium wherein the energy release has been made spatially inhomogeneous is examined via numerical simulation. The inhomogeneity is introduced via step functions in the reaction progress variable, with the local value of energy release correspondingly increased so as to maintain the same average energy density in the medium, and thus a constant Chapman Jouguet (CJ) detonation velocity. A one-step Arrhenius rate governs the rate of energy release in the reactive zones. The resulting dynamics of a detonation propagating in such systems with one-dimensional layers and two-dimensional squares are simulated using a Godunov-type finite-volume scheme. The resulting wave dynamics are analyzed by computing the average wave velocity and one-dimensional averaged wave structure. In the case of sufficiently inhomogeneous media wherein the spacing between reactive zones is greater than the inherent reaction zone length, average wave speeds significantly greater than the corresponding CJ speed of the homogenized medium are obtained. If the shock transit time between reactive zones is less than the reaction time scale, then the classical CJ detonation velocity is recovered. The spatio-temporal averaged structure of the waves in these systems is analyzed via a Favre averaging technique, with terms associated with the thermal and mechanical fluctuations being explicitly computed. The analysis of the averaged wave structure identifies the super-CJ detonations as weak detonations owing to the existence of mechanical non-equilibrium at the effective sonic point embedded within the wave structure. The correspondence of the super-CJ behavior identified in this study with real detonation phenomena that may be observed in experiments is discussed. ","Propagation of gaseous detonation waves in a spatially inhomogeneous
  reactive medium"
11,846883361251483648,4313989761,Dr. Andreas Faisst,['Wanna know how massive #galaxies die? - Check out my new paper! Hint: they collide!\n<LINK>\n#astro #science #caltech #ApJ <LINK>'],https://arxiv.org/abs/1703.09234,"We use $>$9400 $\log(m/M_{\odot})>10$ quiescent and star-forming galaxies at $z\lesssim2$ in COSMOS/UltraVISTA to study the average size evolution of these systems, with focus on the rare, ultra-massive population at $\log(m/M_{\odot})>11.4$. The large 2-square degree survey area delivers a sample of $\sim400$ such ultra-massive systems. Accurate sizes are derived using a calibration based on high-resolution images from the Hubble Space Telescope. We find that, at these very high masses, the size evolution of star-forming and quiescent galaxies is almost indistinguishable in terms of normalization and power-law slope. We use this result to investigate possible pathways of quenching massive $m>M^*$ galaxies at $z<2$. We consistently model the size evolution of quiescent galaxies from the star-forming population by assuming different simple models for the suppression of star-formation. These models include an instantaneous and delayed quenching without altering the structure of galaxies and a central starburst followed by compaction. We find that instantaneous quenching reproduces well the observed mass-size relation of massive galaxies at $z>1$. Our starburst$+$compaction model followed by individual growth of the galaxies by minor mergers is preferred over other models without structural change for $\log(m/M_{\odot})>11.0$ galaxies at $z>0.5$. None of our models is able to meet the observations at $m>M^*$ and $z<1$ with out significant contribution of post-quenching growth of individual galaxies via mergers. We conclude that quenching is a fast process in galaxies with $ m \ge 10^{11} M_\odot$, and that major mergers likely play a major role in the final steps of their evolution. ","Constraints on Quenching of $z\lesssim2$ Massive Galaxies from the
  Evolution of the average Sizes of Star-Forming and Quenched Populations in
  COSMOS"
12,846880534856810496,21611239,Sean Carroll,"[""New paper: the universe is accelerating because it's approaching its heat death.\n<LINK>"", 'Or, less poetically: the universe becomes smooth and empty (de Sitter spacetime) as its entropy increases. With @aechatwi.', ""@ecopenhaver Yes, but it's not the horizon until we're in de Sitter. Before then, it's something called a Q-screen."", '@nattyover Hoping to, before too long.']",https://arxiv.org/abs/1703.09241,"In a wide class of cosmological models, a positive cosmological constant drives cosmological evolution toward an asymptotically de Sitter phase. Here we connect this behavior to the increase of entropy over time, based on the idea that de Sitter spacetime is a maximum-entropy state. We prove a cosmic no-hair theorem for Robertson-Walker and Bianchi I spacetimes that admit a Q-screen (""quantum"" holographic screen) with certain entropic properties: If generalized entropy, in the sense of the cosmological version of the Generalized Second Law conjectured by Bousso and Engelhardt, increases up to a finite maximum value along the screen, then the spacetime is asymptotically de Sitter in the future. Moreover, the limiting value of generalized entropy coincides with the de Sitter horizon entropy. We do not use the Einstein field equations in our proof, nor do we assume the existence of a positive cosmological constant. As such, asymptotic relaxation to a de Sitter phase can, in a precise sense, be thought of as cosmological equilibration. ","Cosmic Equilibration: A Holographic No-Hair Theorem from the Generalized
  Second Law"
13,846789003969937408,18921367,teuscher.:Lab,"['New paper: Shift-Symmetric Configurations in Two-Dimensional Cellular Automata: Irreversibility, Insolvability,... <LINK>']",https://arxiv.org/abs/1703.09030,"The search for symmetry as an unusual yet profoundly appealing phenomenon, and the origin of regular, repeating configuration patterns have long been a central focus of complexity science and physics. To better grasp and understand symmetry of configurations in decentralized toroidal architectures, we employ group-theoretic methods, which allow us to identify and enumerate these inputs, and argue about irreversible system behaviors with undesired effects on many computational problems. The concept of so-called configuration shift-symmetry is applied to two-dimensional cellular automata as an ideal model of computation. Regardless of the transition function, the results show the universal insolvability of crucial distributed tasks, such as leader election, pattern recognition, hashing, and encryption. By using compact enumeration formulas and bounding the number of shift-symmetric configurations for a given lattice size, we efficiently calculate the probability of a configuration being shift-symmetric for a uniform or density-uniform distribution. Further, we devise an algorithm detecting the presence of shift-symmetry in a configuration. Given the resource constraints, the enumeration and probability formulas can directly help to lower the minimal expected error and provide recommendations for system's size and initialization. Besides cellular automata, the shift-symmetry analysis can be used to study the non-linear behavior in various synchronous rule-based systems that include inference engines, Boolean networks, neural networks, and systolic arrays. ","Shift-Symmetric Configurations in Two-Dimensional Cellular Automata:
  Irreversibility, Insolvability, and Enumeration"
14,846667457888751616,1964403878,Andreas Jechow,['new paper about measuring #lightpollution with DSLR cameras from a boat <LINK> nice trip with @skyglowberlin @andhaenel'],https://arxiv.org/abs/1703.08484,"Near all-sky imaging photometry was performed from a boat on the Gulf of Aqaba to measure the night sky brightness in a coastal environment. The boat was not anchored, and therefore drifted and rocked. The camera was mounted on a tripod without any inertia/motion stabilization. A commercial digital single lens reflex (DSLR) camera and fisheye lens were used with ISO setting of 6400, with the exposure time varied between 0.5 s and 5 s. We find that despite movement of the vessel the measurements produce quantitatively comparable results apart from saturation effects. We discuss the potential and limitations of this method for mapping light pollution in marine and freshwater systems. This work represents the proof of concept that all-sky photometry with a commercial DSLR camera is a viable tool to determine light pollution in an ecological context from a moving boat. ","Measuring Light Pollution with Fisheye Lens Imagery from A Moving Boat,
  A Proof of Concept"
15,846652732396261376,3084456430,Burkhard Morgenstern,"['Our new paper on #genomealignment : <LINK> All comments and criticism welcome, want to submit improved version to journal']",https://arxiv.org/abs/1703.08792,"Alignment of large genomic sequences is a fundamental task in computational genome analysis. Most methods for genomic alignment use high-scoring local alignments as {\em anchor points} to reduce the search space of the alignment procedure. Speed and quality of these methods therefore depend on the underlying anchor points. Herein, we propose to use {\em Filtered Spaced Word Matches} to calculate anchor points for genome alignment. To evaluate this approach, we used these anchor points in the the widely used alignment pipeline {\em Mugsy}. For distantly related sequence sets, we could substantially improve the quality of alignments produced by {\em Mugsy}. ",Anchor points for genome alignment based on Filtered Spaced Word Matches
16,846521012707577856,781309796741947396,EEMS Group @ MIT (PI: Vivienne Sze),"['New paper ""Efficient Processing of Deep Neural Networks: A Tutorial and Survey"" now available at <LINK> #deeplearning']",https://arxiv.org/abs/1703.09039,"Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities. ",Efficient Processing of Deep Neural Networks: A Tutorial and Survey
17,846312146841600000,2253056460,Deniz Eroglu,['.:: New survey paper of us on ‚Äúsync of chaos‚Äù is on arXiv ‚Äî<LINK>'],https://arxiv.org/abs/1703.08296,"Dynamical networks are important models for the behaviour of complex systems, modelling physical, biological and societal systems, including the brain, food webs, epidemic disease in populations, power grids and many other. Such dynamical networks can exhibit behaviour in which deterministic chaos, exhibiting unpredictability and disorder, coexists with synchronization, a classical paradigm of order. We survey the main theory behind complete, generalized and phase synchronization phenomena in simple as well as complex networks and discuss applications to secure communications, parameter estimation and the anticipation of chaos. ",Synchronization of Chaos
18,846174268400582656,3160301736,Andr√©s A. Plazas Malag√≥n,"['Our new paper: ""Nonlinearity and pixel shifting effects in HXRG infrared detectors"" (<LINK>)']",https://arxiv.org/abs/1703.08205,"We study the nonlinearity (NL) in the conversion from charge to voltage in infrared detectors (HXRG) for use in precision astronomy. We present laboratory measurements of the NL function of a H2RG detector and discuss the accuracy to which it would need to be calibrated in future space missions to perform cosmological measurements through the weak gravitational lensing technique. In addition, we present an analysis of archival data from the infrared H1RG detector of the Wide Field Camera 3 in the Hubble Space Telescope that provides evidence consistent with the existence of a sensor effect analogous to the brighter-fatter effect found in Charge-Coupled Devices. We propose a model in which this effect could be understood as shifts in the effective pixel boundaries, and discuss prospects of laboratory measurements to fully characterize this effect. ",Nonlinearity and pixel shifting effects in HXRG infrared detectors
19,846046310562189312,23138688,Tom Potok,"['New paper on deep learning using quantum, HPC, and neuromorphic computing <LINK>, exciting work!!']",https://arxiv.org/abs/1703.05364,"Current Deep Learning approaches have been very successful using convolutional neural networks (CNN) trained on large graphical processing units (GPU)-based computers. Three limitations of this approach are: 1) they are based on a simple layered network topology, i.e., highly connected layers, without intra-layer connections; 2) the networks are manually configured to achieve optimal results, and 3) the implementation of neuron model is expensive in both cost and power. In this paper, we evaluate deep learning models using three different computing architectures to address these problems: quantum computing to train complex topologies, high performance computing (HPC) to automatically determine network topology, and neuromorphic computing for a low-power hardware implementation. We use the MNIST dataset for our experiment, due to input size limitations of current quantum computers. Our results show the feasibility of using the three architectures in tandem to address the above deep learning limitations. We show a quantum computer can find high quality values of intra-layer connections weights, in a tractable time as the complexity of the network increases; a high performance computer can find optimal layer-based topologies; and a neuromorphic computer can represent the complex topology and weights derived from the other architectures in low power memristive hardware. ","A Study of Complex Deep Learning Networks on High Performance,
  Neuromorphic, and Quantum Computers"
20,845610421356781568,2735285540,Mike Czech,['New paper on ArXiv: Predicting Rankings of Software Verification Competitions. Thanks to all who have contributed! <LINK>'],https://arxiv.org/abs/1703.00757,"Software verification competitions, such as the annual SV-COMP, evaluate software verification tools with respect to their effectivity and efficiency. Typically, the outcome of a competition is a (possibly category-specific) ranking of the tools. For many applications, such as building portfolio solvers, it would be desirable to have an idea of the (relative) performance of verification tools on a given verification task beforehand, i.e., prior to actually running all tools on the task. In this paper, we present a machine learning approach to predicting rankings of tools on verification tasks. The method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for verification tasks. Our kernels employ a graph representation for software source code that mixes elements of control flow and program dependence graphs with abstract syntax trees. Using data sets from SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy. In particular, our method outperforms a recently proposed feature-based approach of Demyanova et al. (when applied to rank predictions). ",Predicting Rankings of Software Verification Competitions
21,845294470283366400,1104703454,Francesco Silvestri,"['New paper out: ""Distance-sensitive hashing"" by Aum√ºller, Christiani, @RasmusPagh1 and myself, <LINK>']",https://arxiv.org/abs/1703.07867,"Locality-sensitive hashing (LSH) is an important tool for managing high-dimensional noisy or uncertain data, for example in connection with data cleaning (similarity join) and noise-robust search (similarity search). However, for a number of problems the LSH framework is not known to yield good solutions, and instead ad hoc solutions have been designed for particular similarity and distance measures. For example, this is true for output-sensitive similarity search/join, and for indexes supporting annulus queries that aim to report a point close to a certain given distance from the query point. In this paper we initiate the study of distance-sensitive hashing (DSH), a generalization of LSH that seeks a family of hash functions such that the probability of two points having the same hash value is a given function of the distance between them. More precisely, given a distance space $(X, \text{dist})$ and a ""collision probability function"" (CPF) $f\colon \mathbb{R}\rightarrow [0,1]$ we seek a distribution over pairs of functions $(h,g)$ such that for every pair of points $x, y \in X$ the collision probability is $\Pr[h(x)=g(y)] = f(\text{dist}(x,y))$. Locality-sensitive hashing is the study of how fast a CPF can decrease as the distance grows. For many spaces, $f$ can be made exponentially decreasing even if we restrict attention to the symmetric case where $g=h$. We show that the asymmetry achieved by having a pair of functions makes it possible to achieve CPFs that are, for example, increasing or unimodal, and show how this leads to principled solutions to problems not addressed by the LSH framework. This includes a novel application to privacy-preserving distance estimation. We believe that the DSH framework will find further applications in high-dimensional data management. ",Distance-Sensitive hashing
22,845291127725051907,2886658437,Sean Raymond,"[""Super-Earths break the (resonant) chains! Blog post in the works, for now here's Andre's new paper:  <LINK> <LINK>""]",https://arxiv.org/abs/1703.03634,"""Hot super-Earths"" (or ""Mini-Neptunes"") between 1 and 4 times Earth's size with period shorter than 100 days orbit 30-50\% of Sun-like type stars. Their orbital configuration -- measured as the period ratio distribution of adjacent planets in multi-planet systems -- is a strong constraint for formation models. Here we use N-body simulations with synthetic forces from an underlying evolving gaseous disk to model the formation and long-term dynamical evolution of super-Earth systems. While the gas disk is present, planetary embryos grow and migrate inward to form a resonant chain anchored at the inner edge of the disk. These resonant chains are far more compact than the observed super-Earth systems. Once the gas dissipates resonant chains may become dynamically unstable. They undergo a phase of giant impacts that spreads the systems out. Disk turbulence has no measurable effect on the outcome. Our simulations match observations if a small fraction of resonant chains remain stable, while most super-Earths undergo a late dynamical instability. Our statistical analysis restricts the contribution of stable systems to less than $25\%$. Our results also suggest that the large fraction of observed single planet systems does not necessarily imply any dichotomy in the architecture of planetary systems. Finally, we use the low abundance of resonances in Kepler data to argue that, in reality, the survival of resonant chains happens likely only in $\sim 5\%$ of the cases. This leads to a mystery: in our simulations only 50-60\% of resonant chains became unstable whereas at least 75\% (and probably 90-95\%) must be unstable to match observations. ","Breaking the Chains: Hot Super-Earth systems from migration and
  disruption of compact resonant chains"
23,845280835691577344,24692611,Christian M√∂stl üá∫üá¶,"['Our new paper (under review!) on @arxiv on 8 years of #solarstorm predictions verified near Mercury, Venus and üåè \n\n<LINK>']",https://arxiv.org/abs/1703.00705,"We present an advance towards accurately predicting the arrivals of coronal mass ejections (CMEs) at the terrestrial planets, including Earth. For the first time, we are able to assess a CME prediction model using data over 2/3 of a solar cycle of observations with the Heliophysics System Observatory. We validate modeling results of 1337 CMEs observed with the Solar Terrestrial Relations Observatory (STEREO) heliospheric imagers (HI) (science data) from 8 years of observations by 5 in situ observing spacecraft. We use the self-similar expansion model for CME fronts assuming 60 degree longitudinal width, constant speed and constant propagation direction. With these assumptions we find that 23%-35% of all CMEs that were predicted to hit a certain spacecraft lead to clear in situ signatures, so that for 1 correct prediction, 2 to 3 false alarms would have been issued. In addition, we find that the prediction accuracy does not degrade with the HI longitudinal separation from Earth. Predicted arrival times are on average within 2.6 +/- 16.6 hours difference of the in situ arrival time, similar to analytical and numerical modeling, and a true skill statistic of 0.21. We also discuss various factors that may improve the accuracy of space weather forecasting using wide-angle heliospheric imager observations. These results form a first order approximated baseline of the prediction accuracy that is possible with HI and other methods used for data by an operational space weather mission at the Sun-Earth L5 point. ","Modeling observations of solar coronal mass ejections with heliospheric
  imagers verified with the Heliophysics System Observatory"
24,845173324783087617,3186334207,L Glaser,['New paper on the arXiv= new crackpot emails in the inbox. Still pretty good way to start a day.\n<LINK>'],https://arxiv.org/abs/1703.08160,"In the approach of Causal Dynamical Triangulations (CDT), quantum gravity is obtained as a scaling limit of a non-perturbative path integral over space-times whose causal structure plays a crucial role in the construction. After some general considerations about the relation between quantum gravity and cosmology, we examine which aspects of CDT are potentially interesting from a cosmological point of view, focussing on the emergence of a de Sitter universe in CDT quantum gravity. ",CDT and Cosmology
25,844706885123104768,60724221,Maximiliano Isi,"[""My latest paper: we can use continuous gravitational waves for new tests of Einstein's theory! @LIGO @matt_pitkin <LINK> <LINK>"", 'We can use CWs to directly measure grav polarization content. Einstein only allows 2 out of possible 6 polarizations https://t.co/iRL2aVdXoN', 'But other theories may predict different polarizations, and Nature might surprise us!', ""@MC_Nyberg damn I wish you'd told me this before I wasted a year on this paper""]",http://arxiv.org/abs/1703.07530,"The direct detection of gravitational waves provides the opportunity to measure fundamental aspects of gravity which have never been directly probed before, including the polarization of gravitational waves. In the context of searches for continuous waves from known pulsars, we present novel methods to detect signals of any polarization content, measure the modes present and place upper-limits on the amplitude of non-tensorial components. This will allow us to obtain new model-independent, dynamical constraints on deviations from general relativity. We test this framework on multiple potential sources using simulated data from three advanced-era detectors at design sensitivity. We find that signals of any polarization will become detectable and distinguishable for characteristic strains $h\gtrsim 3\times10^{-27} \sqrt{1~{\rm yr}/T}$, for an observation time $T$. We also find that our ability to detect non-tensorial components depends only on the power present in those modes, irrespective of the strength of the tensorial strain. ","Probing Dynamical Gravity with the Polarization of Continuous
  Gravitational Waves"
26,844567213231259648,1069568448,Steve Crawford,['New paper led by Rick Edelson using Swift and @LCO_Global looking at X-Ray/Optical variability from NGC 4151: <LINK>'],https://arxiv.org/abs/1703.06901,"Swift monitoring of NGC 4151 with ~6 hr sampling over a total of 69 days in early 2016 is used to construct light curves covering five bands in the X-rays (0.3-50 keV) and six in the ultraviolet (UV)/optical (1900-5500 A). The three hardest X-ray bands (>2.5 keV) are all strongly correlated with no measurable interband lag while the two softer bands show lower variability and weaker correlations. The UV/optical bands are significantly correlated with the X-rays, lagging ~3-4 days behind the hard X-rays. The variability within the UV/optical bands is also strongly correlated, with the UV appearing to lead the optical by ~0.5-1 day. This combination of >~3 day lags between the X-rays and UV and <~1 day lags within the UV/optical appears to rule out the ""lamp-post"" reprocessing model in which a hot, X-ray emitting corona directly illuminates the accretion disk, which then reprocesses the energy in the UV/optical. Instead, these results appear consistent with the Gardner & Done picture in which two separate reprocessings occur: first, emission from the corona illuminates an extreme-UV-emitting toroidal component that shields the disk from the corona; this then heats the extreme-UV component which illuminates the disk and drives its variability. ","Swift monitoring of NGC 4151: Evidence for a Second X-ray/UV
  Reprocessing"
27,844521127477231616,3318578529,Martin Hebart,['Our new paper on how to deal with confounds in MVPA (plus explaining significant below-chance accuracies): <LINK>'],https://arxiv.org/abs/1703.06670v2,"Standard neuroimaging data analysis based on traditional principles of experimental design, modelling, and statistical inference is increasingly complemented by novel analysis methods, driven e.g. by machine learning methods. While these novel approaches provide new insights into neuroimaging data, they often have unexpected properties, generating a growing literature on possible pitfalls. We propose to meet this challenge by adopting a habit of systematic testing of experimental design, analysis procedures, and statistical inference. Specifically, we suggest to apply the analysis method used for experimental data also to aspects of the experimental design, simulated confounds, simulated null data, and control data. We stress the importance of keeping the analysis method the same in main and test analyses, because only this way possible confounds and unexpected properties can be reliably detected and avoided. We describe and discuss this Same Analysis Approach in detail, and demonstrate it in two worked examples using multivariate decoding. With these examples, we reveal two previously unknown sources of error: A mismatch between counterbalancing and cross-validation which leads to systematic below-chance accuracies, and linear decoding of a nonlinear effect, a difference in variance. ","] The Same Analysis Approach: Practical protection against the pitfalls of
  novel neuroimaging analysis methods"
28,844356232534142976,2337598033,Geraint F. Lewis,"['A new paper with @lukebarnesastro - ‚ÄúProducing the Deuteron in Stars: Anthropic Limits on Fundamental Constants‚Äù <LINK> <LINK>', '@jossblandhawtho @lukebarnesastro Perhaps you should suggest he should read @AFortunateUni :)']",https://arxiv.org/abs/1703.07161,"Stellar nucleosynthesis proceeds via the deuteron (D), but only a small change in the fundamental constants of nature is required to unbind it. Here, we investigate the effect of altering the binding energy of the deuteron on proton burning in stars. We find that the most definitive boundary in parameter space that divides probably life-permitting universes from probably life-prohibiting ones is between a bound and unbound deuteron. Due to neutrino losses, a ball of gas will undergo rapid cooling or stabilization by electron degeneracy pressure before it can form a stable, nuclear reaction-sustaining star. We also consider a less-bound deuteron, which changes the energetics of the $pp$ and $pep$ reactions. The transition to endothermic $pp$ and $pep$ reactions, and the resulting beta-decay instability of the deuteron, do not seem to present catastrophic problems for life. ","Producing the Deuteron in Stars: Anthropic Limits on Fundamental
  Constants"
29,844274238102159364,2956121356,Russ Salakhutdinov,"['New #iclr2017 paper: Transfer Learning for Sequence Tagging with Hierarchical Recurrent Nets; with Z. Yang, W. Cohen\n<LINK> <LINK>']",https://arxiv.org/abs/1703.06345,"Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks. ","Transfer Learning for Sequence Tagging with Hierarchical Recurrent
  Networks"
30,844117488011034624,80319419,Sophie Murray,['My new paper about @MetOfficeSpace solar flare forecasts is now available online <LINK> via @arxiv #SpaceWeather'],https://arxiv.org/abs/1703.06754,"The Met Office Space Weather Operations Centre produces 24/7/365 space weather guidance, alerts, and forecasts to a wide range of government and commercial end users across the United Kingdom. Solar flare forecasts are one of its products, which are issued multiple times a day in two forms; forecasts for each active region on the solar disk over the next 24 hours, and full-disk forecasts for the next four days. Here the forecasting process is described in detail, as well as first verification of archived forecasts using methods commonly used in operational weather prediction. Real-time verification available for operational flare forecasting use is also described. The influence of human forecasters is highlighted, with human-edited forecasts outperforming original model results, and forecasting skill decreasing over longer forecast lead times. ",Flare forecasting at the Met Office Space Weather Operations Centre
31,844009648713695232,267958924,Christian Ott,['New paper by Jonas Lippuner+! Signatures of hypermassive neutron star lifetimes on r-process nucleosynthesis \n<LINK>'],https://arxiv.org/abs/1703.06216,"We investigate the nucleosynthesis of heavy elements in the winds ejected by accretion disks formed in neutron star mergers. We compute the element formation in disk outflows from hypermassive neutron star (HMNS) remnants of variable lifetime, including the effect of angular momentum transport in the disk evolution. We employ long-term axisymmetric hydrodynamic disk simulations to model the ejecta, and compute r-process nucleosynthesis with tracer particles using a nuclear reaction network containing $\sim 8000$ species. We find that the previously known strong correlation between HMNS lifetime, ejected mass, and average electron fraction in the outflow is directly related to the amount of neutrino irradiation on the disk, which dominates mass ejection at early times in the form of a neutrino-driven wind. Production of lanthanides and actinides saturates at short HMNS lifetimes ($\lesssim 10$ ms), with additional ejecta contributing to a blue optical kilonova component for longer-lived HMNSs. We find good agreement between the abundances from the disk outflow alone and the solar r-process distribution only for short HMNS lifetimes ($\lesssim 10$ ms). For longer lifetimes, the rare-earth and third r-process peaks are significantly under-produced compared to the solar pattern, requiring additional contributions from the dynamical ejecta. The nucleosynthesis signature from a spinning black hole (BH) can only overlap with that from a HMNS of moderate lifetime ($\lesssim 60$ ms). Finally, we show that angular momentum transport not only contributes with a late-time outflow component, but that it also enhances the neutrino-driven component by moving material to shallower regions of the gravitational potential, in addition to providing additional heating. ","Signatures of hypermassive neutron star lifetimes on r-process
  nucleosynthesis in the disk ejecta from neutron star mergers"
32,843988370531348480,11778512,Mason Porter,"['Our new paper, ""The Role of Network Analysis in Industrial and Applied Mathematics"": <LINK>\n\n(And its lovely footnote) <LINK>']",https://arxiv.org/abs/1703.06843,"Many problems in industry --- and in the social, natural, information, and medical sciences --- involve discrete data and benefit from approaches from subjects such as network science, information theory, optimization, probability, and statistics. The study of networks is concerned explicitly with connectivity between different entities, and it has become very prominent in industrial settings, an importance that has intensified amidst the modern data deluge. In this commentary, we discuss the role of network analysis in industrial and applied mathematics, and we give several examples of network science in industry. We focus, in particular, on discussing a physical-applied-mathematics approach to the study of networks. We also discuss several of our own collaborations with industry on projects in network analysis. ",The Role of Network Analysis in Industrial and Applied Mathematics
33,843706011890454529,1163712553,Tristan du Pree,['New #DarkMatter paper - with #Nikhef affiliation :) <LINK> <LINK>'],https://arxiv.org/abs/1703.05703,"Weakly-coupled TeV-scale particles may mediate the interactions between normal matter and dark matter. If so, the LHC would produce dark matter through these mediators, leading to the familiar ""mono-X"" search signatures, but the mediators would also produce signals without missing momentum via the same vertices involved in their production. This document from the LHC Dark Matter Working Group suggests how to compare searches for these two types of signals in case of vector and axial-vector mediators, based on a workshop that took place on September 19/20, 2016 and subsequent discussions. These suggestions include how to extend the spin-1 mediated simplified models already in widespread use to include lepton couplings. This document also provides analytic calculations of the relic density in the simplified models and reports an issue that arose when ATLAS and CMS first began to use preliminary numerical calculations of the dark matter relic density in these models. ","Recommendations of the LHC Dark Matter Working Group: Comparing LHC
  searches for heavy mediators of dark matter production in visible and
  invisible decay channels"
34,842664073162559488,131879500,John Ilee,"['Is Elias 2-27 a self gravitating, or recently self gravitating disc? See our new paper on #astroph today... \n\n<LINK> <LINK>']",https://arxiv.org/abs/1703.05338,"The young star Elias 2-27 has recently been observed to posses a massive circumstellar disc with two prominent large-scale spiral arms. In this Letter we perform three-dimensional Smoothed Particle Hydrodynamics simulations, radiative transfer modelling, synthetic ALMA imaging and an unsharped masking technique to explore three possibilities for the origin of the observed structures -- an undetected companion either internal or external to the spirals, and a self-gravitating disc. We find that a gravitationally unstable disc and a disc with an external companion can produce morphology that is consistent with the observations. In addition, for the latter, we find that the companion could be a relatively massive planetary mass companion (less than approximately 10 - 13 MJup) and located at large radial distances (between approximately 300 - 700 au). We therefore suggest that Elias 2-27 may be one of the first detections of a disc undergoing gravitational instabilities, or a disc that has recently undergone fragmentation to produce a massive companion. ","On the origin of the spiral morphology in the Elias 2-27 circumstellar
  disc"
35,842533779134648322,3100596960,Walter Scheirer,"['New paper with @ruthiecf and @neurobongo: ""Using Human Brain Activity to Guide Machine Learning"" <LINK>']",https://arxiv.org/abs/1703.05463,"Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of ""neurally-weighted"" machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data. ",Using Human Brain Activity to Guide Machine Learning
36,842480932955348992,21859920,Ben Rubinstein,['1/New paper: OASIS system for efficient sampling for evaluating highly class-imbalanced problems like record linkage <LINK>'],https://arxiv.org/abs/1703.00617,"Entity resolution (ER) presents unique challenges for evaluation methodology. While crowdsourcing platforms acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates for rigorous evaluation. This paper addresses this important challenge with the OASIS algorithm: a sampler and F-measure estimator for ER evaluation. OASIS draws samples from a (biased) instrumental distribution, chosen to ensure estimators with optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on unlabelled items providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 83% without loss to estimate accuracy. ","In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential
  Importance Sampling"
37,842444127144886272,88273299,Chris Holdgraf,['Check out our new paper on using the @XSEDEscience cloud + @docker to teach people #DataScience more effectively <LINK>'],https://arxiv.org/abs/1703.04900,"There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the efficiency and ease with which students can learn. This manuscript details recent advances towards using commonly-available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benefits (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility. ","Portable learning environments for hands-on computational instruction:
  Using container- and cloud-based technology to teach data science"
38,842379864057499648,354161158,Francis Tseng,"['first paper w/ @binaricorn and @D_young_69 on Humans of Simulated New York! <LINK> <LINK>', '@jmcmahon443 @binaricorn @D_young_69 thanks!']",https://arxiv.org/abs/1703.05240,"The model presented in this paper experiments with a comprehensive simulant agent in order to provide an exploratory platform in which simulation modelers may try alternative scenarios and participation in policy decision-making. The framework is built in a computationally distributed online format in which users can join in and visually explore the results. Modeled activity involves daily routine errands, such as shopping, visiting the doctor or engaging in the labor market. Further, agents make everyday decisions based on individual behavioral attributes and minimal requirements, according to social and contagion networks. Fully developed firms and governments are also included in the model allowing for taxes collection, production decisions, bankruptcy and change in ownership. The contributions to the literature are multifold. They include (a) a comprehensive model with detailing of the agents and firms' activities and processes and original use of simultaneously (b) reinforcement learning for firm pricing and demand allocation; (c) social contagion for disease spreading and social network for hiring opportunities; and (d) Bayesian networks for demographic-like generation of agents. All of that within a (e) visually rich environment and multiple use of databases. Hence, the model provides a comprehensive framework from where interactions among citizens, firms and governments can be easily explored allowing for learning and visualization of policies and scenarios. ","Humans of Simulated New York (HOSNY): an exploratory comprehensive model
  of city life"
39,842344126838382594,71268692,Dima Karamshuk,"['new #icwsm paper from @ctzhong and the team ""How different are your different social network personae?"" <LINK>']",https://arxiv.org/abs/1703.04791,"This paper investigates when users create profiles in different social networks, whether they are redundant expressions of the same persona, or they are adapted to each platform. Using the personal webpages of 116,998 users on About.me, we identify and extract matched user profiles on several major social networks including Facebook, Twitter, LinkedIn, and Instagram. We find evidence for distinct site-specific norms, such as differences in the language used in the text of the profile self-description, and the kind of picture used as profile image. By learning a model that robustly identifies the platform given a user's profile image (0.657--0.829 AUC) or self-description (0.608--0.847 AUC), we confirm that users do adapt their behaviour to individual platforms in an identifiable and learnable manner. However, different genders and age groups adapt their behaviour differently from each other, and these differences are, in general, consistent across different platforms. We show that differences in social profile construction correspond to differences in how formal or informal the platform is. ","Wearing Many (Social) Hats: How Different are Your Different Social
  Network Personae?"
40,842325592703922177,1245940818,changtao,['How different are your different social network personae? Check our new #icwsm paper. <LINK> @karamshuk @nishanthsastry'],https://arxiv.org/abs/1703.04791,"This paper investigates when users create profiles in different social networks, whether they are redundant expressions of the same persona, or they are adapted to each platform. Using the personal webpages of 116,998 users on About.me, we identify and extract matched user profiles on several major social networks including Facebook, Twitter, LinkedIn, and Instagram. We find evidence for distinct site-specific norms, such as differences in the language used in the text of the profile self-description, and the kind of picture used as profile image. By learning a model that robustly identifies the platform given a user's profile image (0.657--0.829 AUC) or self-description (0.608--0.847 AUC), we confirm that users do adapt their behaviour to individual platforms in an identifiable and learnable manner. However, different genders and age groups adapt their behaviour differently from each other, and these differences are, in general, consistent across different platforms. We show that differences in social profile construction correspond to differences in how formal or informal the platform is. ","Wearing Many (Social) Hats: How Different are Your Different Social
  Network Personae?"
41,842177440688553984,750411947661811712,Dr. Sarah Pearson,"['My new paper ""Gaps in Globular Cluster Streams: Pal 5 and the Galactic Bar"" is out!! <LINK> üíÉ']",https://arxiv.org/abs/1703.04627,"Recent Pan-STARRS data show that the leading arm from the globular cluster Palomar 5 (Pal 5) appears shorter than the trailing arm, while simulations of Pal 5 predict similar angular extents. We demonstrate that including the spinning Galactic bar with appropriate pattern speeds in the dynamical modeling of Pal 5 can reproduce the Pan-STARRS data. As the bar sweeps by, some stream stars experience a difference in net torques near pericenter. This leads to the formation of apparent gaps along Pal 5's tidal streams and these gaps grow due to an energy offset from the rest of the stream members. We conclude that only streams orbiting far from the Galactic center or streams on retrograde orbits (with respect to the bar) can be used to unambiguously constrain dark matter subhalo interactions. Additionally, we expect that the Pal 5 leading arm debris should re-appear south of the Pan-STARRS density truncation. ","Gaps and length asymmetry in the stellar stream Palomar 5 as effects of
  Galactic bar rotation"
42,842177313575919617,3245949691,Rebecca Leane,['New paper! We show gamma rays &amp; neutrinos from long-lived mediators in Sun = powerful probe of DM SD-scattering xsec <LINK> <LINK>'],http://arxiv.org/abs/1703.04629,"Dark matter capture and annihilation in the Sun can produce detectable high-energy neutrinos, providing a probe of the dark matter-proton scattering cross section. We consider the case when annihilation proceeds via long-lived dark mediators, which allows gamma rays to escape the Sun and reduces the attenuation of neutrinos. For gamma rays, there are exciting new opportunities, due to detailed measurements of GeV solar gamma rays with Fermi, and unprecedented sensitivities in the TeV range with HAWC and LHAASO. For neutrinos, the enhanced flux, particularly at higher energies ($\sim$TeV), allows a more sensitive dark matter search with IceCube and KM3NeT. We show that these search channels can be extremely powerful, potentially improving sensitivity to the dark matter spin-dependent scattering cross section by several orders of magnitude relative to present searches for high-energy solar neutrinos, as well as direct detection experiments. ",Powerful Solar Signatures of Long-Lived Dark Mediators
43,842175723146158081,51700215,Phil Bull,['New paper: Priors on the effective Dark Energy equation of state in scalar-tensor theories <LINK>'],https://arxiv.org/abs/1703.05297,"Constraining the Dark Energy (DE) equation of state, w, is one of the primary science goals of ongoing and future cosmological surveys. In practice, with imperfect data and incomplete redshift coverage, this requires making assumptions about the evolution of w with redshift z. These assumptions can be manifested in a choice of a specific parametric form, which can potentially bias the outcome, or else one can reconstruct w(z) non-parametrically, by specifying a prior covariance matrix that correlates values of w at different redshifts. In this work, we derive the theoretical prior covariance for the effective DE equation of state predicted by general scalar-tensor theories with second order equations of motion (Horndeski theories). This is achieved by generating a large ensemble of possible scalar-tensor theories using a Monte Carlo methodology, including the application of physical viability conditions. We also separately consider the special sub-case of the minimally coupled scalar field, or quintessence. The prior shows a preference for tracking behaviors in the most general case. Given the covariance matrix, theoretical priors on parameters of any specific parametrization of w(z) can also be readily derived by projection. ","Priors on the effective Dark Energy equation of state in scalar-tensor
  theories"
44,841710316157308928,2894532745,Priya L. Donti,['Our new paper! (with @brandondamos): Task-based End-to-end Model Learning. \nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1703.04529,"With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications. ",Task-based End-to-end Model Learning in Stochastic Optimization
45,841708692433195010,2303004390,Brandon Amos,['Our new paper (with @priyald17): Task-based End-to-end Model Learning\n\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1703.04529,"With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications. ",Task-based End-to-end Model Learning in Stochastic Optimization
46,841516256276578305,2364749424,Friedemann Zenke,"['New paper w/@poolio &amp; @SuryaGanguli on how ""intelligent synapses"" help to learn continually without forgetting: <LINK>']",https://arxiv.org/abs/1703.04200,"While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency. ",Continual Learning Through Synaptic Intelligence
47,841505769577046016,14103014,Ben Poole,"['New paper w/@hisspikeness, @SuryaGanguli uses intelligent synapses to solve sequences of tasks without forgetting: <LINK>']",https://arxiv.org/abs/1703.04200,"While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency. ",Continual Learning Through Synaptic Intelligence
48,841407411491602432,839913287240278020,Joey Rodriguez,['New paper from KELT identifying Variables for the ongoing K2 campaign 13 observations of Taurus <LINK> #K2Mission'],https://arxiv.org/abs/1703.02522,"One of the most well-studied young stellar associations, Taurus-Auriga, will be observed by the extended Kepler mission, K2, in the spring of 2017. K2 Campaign 13 (C13) will be a unique opportunity to study many stars in this young association at high photometric precision and cadence. Using observations from the Kilodegree Extremely Little Telescope (KELT) survey, we identify ""dippers"", stochastic variables, and periodic variables among K2 C13 target stars. This release of the KELT data (lightcurve data in e-tables) provides the community with long-time baseline observations to assist in the understanding of the more exotic variables in the association. Transient-like phenomena on timescales of months to years are known characteristics in the light curves of young stellar objects, making contextual pre- and post-K2 observations critical to understanding their underlying processes. We are providing a comprehensive set of the KELT light curves for known Taurus-Auriga stars in K2 C13. The combined data sets from K2 and KELT should permit a broad array of investigations related to star formation, stellar variability, and protoplanetary environments. ","Identification of Young Stellar Variables with KELT for K2 I: Campaign
  13 Taurus Dippers and Rotators"
49,840039388490944512,1191642560,Karl Rohe,['New paper: Quickly simulate a big graph using this one weird trick\n\n<LINK> <LINK>'],https://arxiv.org/abs/1703.02998,"Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive ""element-wise"" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5. ",A note on quickly sampling a sparse matrix with low rank expectation
50,839821331709452290,44132347,Stefano Nichele,['Our new paper on #Deep #Reservoir Computing with #Cellular Automata <LINK> #deeplearning Recurrent #NeuralNetworks <LINK>'],https://arxiv.org/abs/1703.02806,"Recurrent Neural Networks (RNNs) have been a prominent concept within artificial intelligence. They are inspired by Biological Neural Networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic Artificial Neural Networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train. In this paper, Cellular Automata are used as reservoir, and are tested on the 5-bit memory task (a well known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata, and a recurrent architecture for handling the sequential aspects of it. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared towards earlier work, in addition to its single-layer version. Results show that the single CA reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs do show a noticeable improvement compared to a single CA reservoir. This indicates potential for further research and provides valuable insight on how to design CA reservoir systems. ",Deep Reservoir Computing Using Cellular Automata
51,839779995144884224,489909633,Prof. Melanie J-H,['Our new paper on optical analysis of A3266. This has a neat interactive fig. of the data embedded! <LINK> @MatthewColless'],https://arxiv.org/abs/1703.02616,"We present spectroscopy of 880 galaxies within a 2-degree field around the massive, merging cluster Abell 3266. This sample, which includes 704 new measurements, was combined with the existing redshifts measurements to generate a sample of over 1300 spectroscopic redshifts; the largest spectroscopic sample in the vicinity of A3266 to date. We define a cluster sub-sample of 790 redshifts which lie within a velocity range of 14,000 to 22,000 kms$^{-1}$ and within 1 degree of the cluster centre. A detailed structural analysis finds A3266 to have a complex dynamical structure containing six groups and filaments to the north of the cluster as well as a cluster core which can be decomposed into two components split along a northeast-southwest axis, consistent with previous X-ray observations. The mean redshift of the cluster core is found to be $0.0594 \pm 0.0005$ and the core velocity dispersion is given as $1462^{+99}_{-99}$ kms$^{-1}$. The overall velocity dispersion and redshift of the entire cluster and related structures are $1337^{+67}_{-67}$ kms$^{-1}$ and $0.0596 \pm 0.0002$, respectively, though the high velocity dispersion does not represent virialised motions but rather is due to relative motions of the cluster components. We posit A3266 is seen following a merger along the northeast southwest axis, however, the rich substructure in the rest of the cluster suggests that the dynamical history is more complex than just a simple merger with a range of continuous dynamical interactions taking place. It is thus likely that turbulence in A3266 is very high, even for a merging cluster. ",Dynamics of Abell 3266 - I. An Optical View of a Complex Merging Cluster
52,839439680382861313,147315191,Valeria Pettorino,['New paper with my student Santiago Casas <LINK> and quick summary <LINK>'],https://arxiv.org/abs/1703.01271,"Modified Gravity theories generally affect the Poisson equation and the gravitational slip (effective anisotropic stress) in an observable way, that can be parameterized by two generic functions ($\eta$ and $\mu$) of time and space. We bin the time dependence of these functions in redshift and present forecasts on each bin for future surveys like Euclid. We consider both Galaxy Clustering and Weak Lensing surveys, showing the impact of the non-linear regime, treated with two different semi-analytical approximations. In addition to these future observables, we use a prior covariance matrix derived from the Planck observations of the Cosmic Microwave Background. Our results show that $\eta$ and $\mu$ in different redshift bins are significantly correlated, but including non-linear scales reduces or even eliminates the correlation, breaking the degeneracy between Modified Gravity parameters and the overall amplitude of the matter power spectrum. We further decorrelate parameters with a Zero-phase Component Analysis and identify which combinations of the Modified Gravity parameter amplitudes, in different redshift bins, are best constrained by future surveys. We also extend the analysis to two particular parameterizations of the time evolution of $\mu$ and $\eta$ and consider, in addition to Euclid, also SKA1, SKA2, DESI: we find in this case that future surveys will be able to constrain the current values of $\eta$ and $\mu$ at the $2-5\%$ level when using only linear scales (wavevector k < 0.15 h/Mpc), depending on the specific time parameterization; sensitivity improves to about $1\%$ when non-linearities are included. ",Linear and non-linear Modified Gravity forecasts with future surveys
53,839348616993075200,2974498451,Dr. Monika Moscibrodzka,['Our new paper about polarization of the supermassive black hole in M87 galaxy is available on astroph: <LINK>'],https://arxiv.org/abs/1703.02390,"Non-VLBI measurements of Faraday rotation at mm wavelengths have been used to constrain mass accretion rates ($\mdot$) onto supermassive black holes in the centre of the Milky Way and in the centre of M87. We constructed general relativistic magnetohydrodynamics models for these sources that qualitatively well describe their spectra and radio/mm images invoking a coupled jet-disk system. Using general relativistic polarized radiative transfer, we now also model the observed mm rotation measure (RM) of M87. The models are tied to the observed radio flux, however, electron temperature and accretion rate are degenerate parameters and are allowed to vary. For the inferred low viewing angles of the M87 jet, the RM is low even as the black hole $\mdot$ increases by a factor of $\simeq100$. In jet-dominated models, the observed linear polarization is produced in the forward-jet, while the dense accretion disk depolarizes the bulk of the near-horizon scale emission which originates in the counter-jet. In the jet-dominated models, with increasing $\mdot$ and increasing Faraday optical depth one is progressively sensitive only to polarized emission in the forward-jet, keeping the measured RM relatively constant. The jet-dominated model reproduces a low net-polarization of $\simeq1$ per cent and RMs in agreement with observed values due to Faraday depolarization, however, with $\mdot$ much larger than the previously inferred limit of $9\times10^{-4}\,\mdotu$. All jet-dominated models produce much higher RMs for inclination angles $i\gtrsim30^\circ$, where the line-of-sight passes through the accretion flow, thereby providing independent constraints on the viewing geometry of the M87 jet. ",Faraday rotation in GRMHD simulations of the jet launching zone of M87
54,839293050828410880,3269585132,limmerlab,['New paper with the Willard group on charge separation at an aqueous electrode: <LINK> !'],https://arxiv.org/abs/1703.02216,"We have used molecular simulation and methods of importance sampling to study the thermodynamics and kinetics of ionic charge separation at a liquid water-metal interface. We have considered this process using canonical examples of two different classes of ions: a simple alkali-halide pair, Na$^+$I$^-$, or classical ions, and the products of water autoionization, H$_3$O$^+$OH$^-$, or water ions. We find that for both ion classes, the microscopic mechanism of charge separation, including water's collective role in the process, is conserved between the bulk liquid and the electrode interface. Despite this, the thermodynamic and kinetic details of the process differ between these two environments in a way that depends on ion type. In the case of the classical ion pairs, a higher free energy barrier to charge separation and a smaller flux over that barrier at the interface, results in a rate of dissociation that is 40x slower relative to the bulk. For water ions, a slightly higher free energy barrier is offset by a higher flux over the barrier from longer lived hydrogen bonding patters at the interface, resulting in a rate of association that is similar both at and away from the interface. We find that these differences in rates and stabilities of charge separation are due to the altered ability of water to solvate and reorganize in the vicinity of the metal interface. ","Microscopic dynamics of charge separation at the aqueous electrochemical
  interface"
55,838992911194796033,10666172,Sabine Hossenfelder,"[""New paper showing that Verlinde's emergent gravity can be made compatible with general relativity <LINK>"", ""@HardyHulley Yeah, that's what I think about each of my papers. Unfortunately the biggest part of the world doesn't share my opinion."", ""@kenneth_mno She's a loner, they say https://t.co/mGpKHSSO4o"", '@kenneth_mno @YouTube Evidently, I go to the library to record music videos.', '@SaschaCaron Yes, can do dark energy as well.']",https://arxiv.org/abs/1703.01415,"A generally covariant version of Erik Verlinde's emergent gravity model is proposed. The Lagrangian constructed here allows an improved interpretation of the underlying mechanism. It suggests that de-Sitter space is filled with a vector-field that couples to baryonic matter and, by dragging on it, creates an effect similar to dark matter. We solve the covariant equation of motion in the background of a Schwarzschild space-time and obtain correction terms to the non-covariant expression. Furthermore, we demonstrate that the vector field can also mimic dark energy. ",A Covariant Version of Verlinde's Emergent Gravity
56,838668075691986945,2656302854,Ronald Drimmel üá∫üá¶,"['New #GaiaDR1 paper!\n#OpenCluster #Astrometry: performance, limitations, and future prospects. \n<LINK>\n#GaiaMission <LINK>']",https://arxiv.org/abs/1703.01131,"Context. The first Gaia Data Release contains the Tycho-Gaia Astrometric Solution (TGAS). This is a subset of about 2 million stars for which, besides the position and photometry, the proper motion and parallax are calculated using Hipparcos and Tycho-2 positions in 1991.25 as prior information. Aims. We investigate the scientific potential and limitations of the TGAS component by means of the astrometric data for open clusters. Methods. Mean cluster parallax and proper motion values are derived taking into account the error correlations within the astrometric solutions for individual stars, an estimate of the internal velocity dispersion in the cluster, and, where relevant, the effects of the depth of the cluster along the line of sight. Internal consistency of the TGAS data is assessed. Results. Values given for standard uncertainties are still inaccurate and may lead to unrealistic unit-weight standard deviations of least squares solutions for cluster parameters. Reconstructed mean cluster parallax and proper motion values are generally in very good agreement with earlier Hipparcos-based determination, although the Gaia mean parallax for the Pleiades is a significant exception. We have no current explanation for that discrepancy. Most clusters are observed to extend to nearly 15 pc from the cluster centre, and it will be up to future Gaia releases to establish whether those potential cluster-member stars are still dynamically bound to the clusters. Conclusions. The Gaia DR1 provides the means to examine open clusters far beyond their more easily visible cores, and can provide membership assessments based on proper motions and parallaxes. A combined HR diagram shows the same features as observed before using the Hipparcos data, with clearly increased luminosities for older A and F dwarfs. ","Gaia Data Release 1. Open cluster astrometry: performance, limitations,
  and future prospects"
57,838050413257584641,961589635,John Urschel,"[""New paper! It's on learning determinantal point processes and the relation to the cycle space of the induced graph. <LINK> <LINK>""]",https://arxiv.org/abs/1703.00539,"Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the \emph{cycle sparsity}; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings. ",Learning Determinantal Point Processes with Moments and Cycles
58,837426906085511168,250454232,Eric Mamajek üá∫üá∏üá∫üá¶,"['New paper by Cameron Bell, @AstroSmurph, and I on nearby 32 Ori Group (24+-4 Myr, ~93 pc, N~46 *s) in northern Orion <LINK> <LINK>', 'Oddly, Bellatrix (B2, ~9Msun) is at center of 32 Ori grp but velcty off by 12 km/s. But: right age, extinction, RV, dist. No evidnce of comp']",https://arxiv.org/abs/1703.00015,"The 32 Orionis group was discovered almost a decade ago and despite the fact that it represents the first northern, young (age ~ 25 Myr) stellar aggregate within 100 pc of the Sun ($d \simeq 93$ pc), a comprehensive survey for members and detailed characterisation of the group has yet to be performed. We present the first large-scale spectroscopic survey for new (predominantly M-type) members of the group after combining kinematic and photometric data to select candidates with Galactic space motion and positions in colour-magnitude space consistent with membership. We identify 30 new members, increasing the number of known 32 Ori group members by a factor of three and bringing the total number of identified members to 46, spanning spectral types B5 to L1. We also identify the lithium depletion boundary (LDB) of the group, i.e. the luminosity at which lithium remains unburnt in a coeval population. We estimate the age of the 32 Ori group independently using both isochronal fitting and LDB analyses and find it is essentially coeval with the {\beta} Pictoris moving group, with an age of $24\pm4$ Myr. Finally, we have also searched for circumstellar disc hosts utilising the AllWISE catalogue. Although we find no evidence for warm, dusty discs, we identify several stars with excess emission in the WISE W4-band at 22 {\mu}m. Based on the limited number of W4 detections we estimate a debris disc fraction of $32^{+12}_{-8}$ per cent for the 32 Ori group. ","A stellar census of the nearby, young 32 Orionis group"
59,837301055742492672,2303004390,Brandon Amos,['Our new paper - OptNet: Differentiable Optimization as a Layer in Neural Nets\n\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1703.00443,"This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures. ",OptNet: Differentiable Optimization as a Layer in Neural Networks
60,837227810724933632,2309687984,Arno Solin,"[""My son helped me out with the experiments for our new paper 'Inertial Odometry on Handheld Smartphones' <LINK> <LINK>""]",https://arxiv.org/abs/1703.00154,"Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. This paper shows that by careful crafting and accounting for the weak information in the sensor samples, smartphones are capable of pure inertial navigation. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. The strength of the model is in learning additive and multiplicative IMU biases online. We are able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion by solving the inference with an extended Kalman filter. The information fusion is completed with zero-velocity updates (if the phone remains stationary), altitude correction from barometric pressure readings (if available), and pseudo-updates constraining the momentary speed. We demonstrate our approach using an iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup. ",Inertial Odometry on Handheld Smartphones
61,837125105100570624,31904918,Sanjeev Satheesh,"[""Our new paper extending CTC to do automatic segmentation is new state-of-art on the hub5'00 benchmark  <LINK>""]",https://arxiv.org/abs/1703.00096,"Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark. ","Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence
  Labelling"
62,837124307893563393,821163173784866818,Reuben Feinman,['Our new paper on the artifacts of adversarial samples: <LINK>. @goodfellow_ian @NicolasPapernot first cleverhans citation?'],https://arxiv.org/abs/1703.00410,"Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples. ",Detecting Adversarial Samples from Artifacts
63,848560271161528320,51570722,Carl Carrie (@üè†),['Paper uses new R queuecomputer package to model queues \n\n#Rstats <LINK>'],https://arxiv.org/abs/1703.02151,"Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call centers and airport passenger terminals. To model such systems accurately, we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straightforward way to infer parameters for such networks if we could simulate data quickly enough. We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package. The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call center and an airport terminal. ","Computationally Efficient Simulation of Queues: The R Package
  queuecomputer"
