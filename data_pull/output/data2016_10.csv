,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,794102727521484800,20117127,Leigh Fletcher,"[""New paper klaxon!  We used CRIRES spectra from @ESO VLT to probe gases deep in Jupiter's weather layer at 5-microns: <LINK> <LINK>""]",https://arxiv.org/abs/1610.09073,"Jupiter's tropospheric composition is studied using high resolution spatially-resolved 5-micron observation from the CRIRES instrument at the Very Large Telescope. The high resolving power (R=96,000) allows us to spectrally resolve the line shapes of individual molecular species in Jupiter's troposphere and, by aligning the slit north-south along Jupiter's central meridian, we are able to search for any latitudinal variability. Despite the high spectral resolution, we find that there are significant degeneracies between the cloud structure and aerosol scattering properties that complicate the retrievals of tropospheric gaseous abundances and limit conclusions on any belt-zone variability. However, we do find evidence for variability between the equatorial regions of the planet and the polar regions. Arsine (AsH$_3$) and phosphine (PH$_3$) both show an enhancement at high latitudes, while the abundance of germane (GeH$_4$) remains approximately constant. These observations contrast with the theoretical predictions from Wang et al. (2016) and we discuss the possible explanations for this difference. ","Latitudinal variability in Jupiter's tropospheric disequilibrium
  species: GeH$_4$, AsH$_3$ and PH$_3$"
1,793457926631333888,2377407248,Daniel Whiteson,['New paper on what we might be missing at the LHC: <LINK>'],https://arxiv.org/abs/1610.09392,"We propose a strategy for searching for theoretically-unanticipated new physics which avoids a large trials factor by focusing on experimental strengths. Searches for resonances decaying into pairs of visible particles are experimentally very powerful due to the localized mass peaks and have a rich history of discovery. Yet, due to a focus on subsets of theoretically-motivated models, the landscape of such resonances is far from thoroughly explored. We survey the existing set of searches, identify untapped experimental opportunities and discuss the theoretical constraints on models which would generate such resonances. ",The unexplored landscape of two-body resonances
2,793269776709586944,3874714693,augustus odena,"['New GAN paper with @ch402 and Jon Shlens <LINK>', '@gstsdn,@ch402 samples: https://t.co/fTwEFHdGuA']",https://arxiv.org/abs/1610.09585,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. ",Conditional Image Synthesis With Auxiliary Classifier GANs
3,792366726579191810,2800204849,Andrew Gordon Wilson,"['New paper uniting Gaussian processes with LSTMs!\n<LINK>\nScalable inference, Keras code &amp; applications to self-driving cars!']",https://arxiv.org/abs/1610.08936,"Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable. ",Learning Scalable Deep Kernels with Recurrent Structure
4,792044658394144768,312448486,Dr. Karan Jani,"[""My new paper on scrutinizing impact of 'Space Diplomacy' for @isro, @Space_Station is out on arxiv. Feedback welcome <LINK> <LINK>""]",https://arxiv.org/abs/1610.08618,"Space-science programs provide a wide range of application to a nation's key sectors of development: science-technology infrastructure, education, economy and national security. However, the cost of sustaining a space-science program has discouraged developing nations from participating in space activities, while developed nations have steadily cut down their space-science budget in past decade. In this study I investigate the role of international cooperation in building ambitious space-science programs, particularly in the context of developing nations. I devise a framework to quantify the impact of international collaborations in achieving the space-science goals as well as in enhancing the key sectors of development of a nation. I apply this framework on two case studies, (i) Indian Space Research Organization - a case of space-science program from a developing nation that has historically engaged in international collaborations, and (ii) International Space Station - a case for a long term collaboration between matured space-science programs. My study concludes that international cooperation in space can significantly enhance scope of science goals, but has relatively little return of investment towards science education and national security. I also highlight limitations of such space diplomacy in the case of China and SAARC nations, and list criteria for future investigation and case studies. ","Impact of International Cooperation for Sustaining Space-Science
  Programs"
5,791961086564048896,2508680244,Sophia Goldberg üë©üèª‚Äçüíª,['My new paper: Cosmology on all scales: a two-parameter expansion. Explanation below! üìÑ‚ú®‚ú®‚ú®\n<LINK> <LINK>'],https://arxiv.org/abs/1610.08882,"We propose and construct a two-parameter perturbative expansion around a Friedmann-Lema\^{i}tre-Robertson-Walker geometry that can be used to model high-order gravitational effects in the presence of non-linear structure. This framework reduces to the weak-field and slow-motion post-Newtonian treatment of gravity in the appropriate limits, but also includes the low-amplitude large-scale fluctuations that are important for cosmological modelling. We derive a set of field equations that can be applied to the late Universe, where non-linear structure exists on supercluster scales, and perform a detailed investigation of the associated gauge problem. This allows us to identify a consistent set of perturbed quantities in both the gravitational and matter sectors, and to construct a set of gauge-invariant quantities that correspond to each of them. The field equations, written in terms of these quantities, take on a relatively simple form, and allow the effects of small-scale structure on the large-scale properties of the Universe to be clearly identified. We find that inhomogeneous structures source the global expansion, that there exist new field equations at new orders, and that there is vector gravitational potential that is a hundred times larger than one might naively expect from cosmological perturbation theory. Finally, we expect our formalism to be of use for calculating relativistic effects in upcoming ultra-large-scale surveys, as the form of the gravitational coupling between small and large scales depends on the non-linearity of Einstein's equations, and occurs at what is normally thought of as first order in cosmological perturbations. ",Cosmology on all scales: a two-parameter perturbation expansion
6,791836553924321280,3100596960,Walter Scheirer,['Check out our new paper on predicting first impressions using deep learning: <LINK>'],https://arxiv.org/abs/1610.08119,"Describable visual facial attributes are now commonplace in human biometrics and affective computing, with existing algorithms even reaching a sufficient point of maturity for placement into commercial products. These algorithms model objective facets of facial appearance, such as hair and eye color, expression, and aspects of the geometry of the face. A natural extension, which has not been studied to any great extent thus far, is the ability to model subjective attributes that are assigned to a face based purely on visual judgements. For instance, with just a glance, our first impression of a face may lead us to believe that a person is smart, worthy of our trust, and perhaps even our admiration - regardless of the underlying truth behind such attributes. Psychologists believe that these judgements are based on a variety of factors such as emotional states, personality traits, and other physiognomic cues. But work in this direction leads to an interesting question: how do we create models for problems where there is no ground truth, only measurable behavior? In this paper, we introduce a new convolutional neural network-based regression framework that allows us to train predictive models of crowd behavior for social attribute assignment. Over images from the AFLW face database, these models demonstrate strong correlations with human crowd ratings. ",Predicting First Impressions with Deep Learning
7,791804814434840577,51700215,Phil Bull,"[""New paper: How about a multi-wavelength/multi-tracer halo model? That's analytic? And fits some real data? <LINK>""]",https://arxiv.org/abs/1610.08948,"The information extracted from large galaxy surveys with the likes of DES, DESI, Euclid, LSST, SKA, and WFIRST will be greatly enhanced if the resultant galaxy catalogues can be cross-correlated with one another. Predicting the nature of the information gain, and developing the tools to realise it, depends on establishing a consistent model of how the galaxies detected by each survey trace the same underlying matter distribution. Existing analytic methods, such as halo occupation distribution (HOD) modelling, are not well-suited for this task, and can suffer from ambiguities and tuning issues when applied to multiple tracers. In this paper, we take the first steps towards constructing an alternative that provides a common model for the connection between galaxies and dark matter halos across a wide range of wavelengths (and thus tracer populations). This is based on a chain of parametrised statistical distributions that model the connection between (a) halo mass and bulk physical properties of galaxies, such as star-formation rate; and (b) those same physical properties and a variety of emission processes. The result is a flexible parametric model that allows analytic halo model calculations of 1-point functions to be carried out for multiple tracers, as well as providing semi-realistic galaxy properties for fast mock catalogue generation. ",A Galaxy-Halo Model for Multiple Cosmological Tracers
8,791560847487340544,4032064210,Sesh Nadathur,"['I have a new paper on the arXiv today, on how galaxy voids trace the gravitational potential. \n<LINK>', ""Not as straightforward as you might imagine, because underdensities in galaxy dist. don't obviously match to maxima of the potential!"", '...particularly given operational complexities in how to identify/demarcate those galaxy underdensities. But we found some nice relations!', 'These relations were crucial inputs to our previous paper measuring the ISW effect of voids. The papers have gone out in reverse order üòâ', ""Today's paper was written with @just_shaun in Auckland and Rob Crittenden (who's not on Twitter) at @UoPCosmology""]",https://arxiv.org/abs/1610.08382,"The properties of large underdensities in the distribution of galaxies in the Universe, known as cosmic voids, are potentially sensitive probes of fundamental physics. We use data from the MultiDark suite of N-body simulations and multiple halo occupation distribution mocks to study the relationship between galaxy voids, identified using a watershed void-finding algorithm, and the gravitational potential $\Phi$. We find that the majority of galaxy voids correspond to local density minima in larger-scale overdensities, and thus lie in potential wells. However, a subset of voids can be identified that closely trace maxima of the gravitational potential and thus stationary points of the velocity field. We identify a new void observable, $\lambda_v$, which depends on a combination of the void size and the average galaxy density contrast within the void, and show that it provides a good proxy indicator of the potential at the void location. A simple linear scaling of $\Phi$ as a function of $\lambda_v$ is found to hold, independent of the redshift and properties of the galaxies used as tracers of voids. We provide an accurate fitting formula to describe the spherically averaged potential profile $\Phi(r)$ about void centre locations. We discuss the importance of these results for the understanding of the evolution history of voids, and for their use in precision measurements of the integrated Sachs-Wolfe effect, gravitational lensing and peculiar velocity distortions in redshift space. ",Tracing the gravitational potential using cosmic voids
9,791109614993444865,252867237,Juan Miguel Arrazola,"['Money, money, money....must be funny, in a quantum world! New paper out! <LINK> <LINK>']",https://arxiv.org/abs/1610.06345,"We present a family of quantum money schemes with classical verification which display a number of benefits over previous proposals. Our schemes are based on hidden matching quantum retrieval games and they tolerate noise up to 23%, which we conjecture reaches 25% asymptotically as the dimension of the underlying hidden matching states is increased. Furthermore, we prove that 25% is the maximum tolerable noise for a wide class of quantum money schemes with classical verification, meaning our schemes are almost optimally noise tolerant. We use methods in semi-definite programming to prove security in a substantially different manner to previous proposals, leading to two main advantages: first, coin verification involves only a constant number of states (with respect to coin size), thereby allowing for smaller coins; second, the re-usability of coins within our scheme grows linearly with the size of the coin, which is known to be optimal. Lastly, we suggest methods by which the coins in our protocol could be implemented using weak coherent states and verified using existing experimental techniques, even in the presence of detector inefficiencies. ",Quantum money with nearly optimal error tolerance
10,791031353986912256,65528671,G.-A. Bilodeau,['New paper on arxiv by FX Derue. About tracking with superpixels and keypoints: <LINK>'],http://arxiv.org/abs/1610.07238,"In visual tracking, part-based trackers are attractive since they are robust against occlusion and deformation. However, a part represented by a rectangular patch does not account for the shape of the target, while a superpixel does thanks to its boundary evidence. Nevertheless, tracking superpixels is difficult due to their lack of discriminative power. Therefore, to enable superpixels to be tracked discriminatively as object parts, we propose to enhance them with keypoints. By combining properties of these two features, we build a novel element designated as a Superpixel-Keypoints structure (SPiKeS). Being discriminative, these new object parts can be located efficiently by a simple nearest neighbor matching process. Then, in a tracking process, each match votes for the target's center to give its location. In addition, the interesting properties of our new feature allows the development of an efficient model update for more robust tracking. According to experimental results, our SPiKeS-based tracker proves to be robust in many challenging scenarios by performing favorably against the state-of-the-art. ",SPiKeS: Superpixel-Keypoints Structure for Robust Visual Tracking
11,790731295148027904,139709337,David Dumas,"['My new research paper with Anna Lenzhen, @KasraRafi, and @taotejing: <LINK>']",https://arxiv.org/abs/1610.07409,"We study the geometry of the Thurston metric on the Teichm\""uller space $\mathcal{T}(S)$ of hyperbolic structures on a surface $S$. Some of our results on the coarse geometry of this metric apply to arbitrary surfaces $S$ of finite type; however, we focus particular attention on the case where the surface is a once-punctured torus, $S_{1,1}$. In that case, our results provide a detailed picture of the infinitesimal, local, and global behavior of the geodesics of the Thurston metric, as well as an analogue of Royden's theorem. ",Coarse and fine geometry of the Thurston metric
12,790712541253099520,101980926,Masahito Yamazaki,"['My new paper ""Quantum Trilogy: Discrete Toda, Y-System and Chaos""\n<LINK>']",https://arxiv.org/abs/1610.06925,"We discuss a discretization of the quantum Toda field theory associated with a semisimple finite-dimensional Lie algebra or a tamely-laced infinite-dimensional Kac-Moody algebra $G$, generalizing the previous construction of discrete quantum Liouville theory for the case $G=A_1$. The model is defined on a discrete two-dimensional lattice, whose spatial direction is of length $L$. In addition we also find a ""discretized extra dimension"" whose width is given by the rank $r$ of $G$, which decompactifies in the large $r$ limit. For the case of $G=A_N$ or $A_{N-1}^{(1)}$, we find a symmetry exchanging $L$ and $N$ under appropriate spatial boundary conditions. The dynamical time evolution rule of the model is a quantizations of the so-called Y-system, and the theory can be well-described by the quantum cluster algebra. We discuss possible implications for recent discussions of quantum chaos, and comment on the relation with the quantum higher Teichmuller theory of type $A_N$. ","Quantum Trilogy: Discrete Toda, Y-System and Chaos"
13,790689863771250688,3199605543,Afonso S. Bandeira,['New paper on Approximate Message Passing for Synchronization: <LINK>'],https://arxiv.org/abs/1610.04583,"Various alignment problems arising in cryo-electron microscopy, community detection, time synchronization, computer vision, and other fields fall into a common framework of synchronization problems over compact groups such as Z/L, U(1), or SO(3). The goal of such problems is to estimate an unknown vector of group elements given noisy relative observations. We present an efficient iterative algorithm to solve a large class of these problems, allowing for any compact group, with measurements on multiple 'frequency channels' (Fourier modes, or more generally, irreducible representations of the group). Our algorithm is a highly efficient iterative method following the blueprint of approximate message passing (AMP), which has recently arisen as a central technique for inference problems such as structured low-rank estimation and compressed sensing. We augment the standard ideas of AMP with ideas from representation theory so that the algorithm can work with distributions over compact groups. Using standard but non-rigorous methods from statistical physics we analyze the behavior of our algorithm on a Gaussian noise model, identifying phases where the problem is easy, (computationally) hard, and (statistically) impossible. In particular, such evidence predicts that our algorithm is information-theoretically optimal in many cases, and that the remaining cases show evidence of statistical-to-computational gaps. ","Message-passing algorithms for synchronization problems over compact
  groups"
14,790542376167235584,290740882,Paolo Barucca,['new paper on #arxiv:\n#Spectral #partitioning in #random #regular #blockmodels\n@INETOxford @uzh_news @IMTLucca  \n<LINK>'],https://arxiv.org/abs/1610.02668,"Graph partitioning problems emerge in a wide variety of complex systems, ranging from biology to finance, but can be rigorously analyzed and solved only for a few graph ensembles. Here, an ensemble of equitable graphs, i.e. random graphs with a block-regular structure, is studied, for which analytical results can be obtained. In particular, the spectral density of this ensemble is computed exactly for a modular and bipartite structure. Kesten-McKay's law for random regular graphs is found analytically to apply also for modular and bipartite structures when blocks are homogeneous. Exact solution to graph partitioning for two equal-sized communities is proposed and verified numerically, and a conjecture on the absence of an efficient recovery detectability transition in equitable graphs is suggested. Final discussion summarizes results and outlines their relevance for the solution of graph partitioning problems in other graph ensembles, in particular for the study of detectability thresholds and resolution limits in stochastic block models. ",Spectral partitioning in equitable graphs
15,790527204044201984,379723680,Linda Blot,"[""My new paper is on the arXiv today! Have a read if you're interested in bispectrum covariance <LINK>""]",http://arxiv.org/abs/1610.06585,"The covariance matrix of the matter and halo power spectrum and bispectrum are studied. Using a large suite of simulations, we find that the non-Gaussianity in the covariance is significant already at mildly nonlinear scales. We compute the leading disconnected non-Gaussian correction to the matter bispectrum covariance using perturbation theory, and find that the corrections result in good agreement in the mildly nonlinear regime. The shot noise contribution to the halo power spectrum and bispectrum covariance is computed using the Poisson model, and the model yields decent agreement with simulation results. However, when the shot noise is estimated from the individual realization, which is usually done in reality, we find that the halo covariance is substantially reduced and gets close to the Gaussian covariance. This is because most of the non-Gaussianity in the covariance arises from the fluctuations in the Poisson shot noise. We use the measured non-Gaussian covariance to access the information content of the power spectrum and bispectrum. The signal-to-noise ratio, S/N, of the matter and halo power spectrum levels off in the mildly nonlinear regime, $k \sim 0.1 - 0.2 \,\mathrm{Mpc} h^{-1}$. In the nonlinear regime the S/N of the matter and halo bispectrum increases but much slower than the Gaussian results suggest. We find that both the S/N for power spectrum and bispectrum are overestimated by the Gaussian covariances, but the problem being much more serious for the bispectrum. Because the bispectrum is affected strongly by nonlinearity and shot noise, inclusion of the bispectrum only adds modest amount of S/N compared to that of the power spectrum. ","Assessment of the Information Content of the Power Spectrum and
  Bispectrum"
16,790353651382353920,13519682,Dr. Michael D. Schneider,['New paper: A principled #Bayesian approach to weak gravitational lensing analysis for #cosmology <LINK>'],https://arxiv.org/abs/1610.06673,"We infer gravitational lensing shear and convergence fields from galaxy ellipticity catalogs under a spatial process prior for the lensing potential. We demonstrate the performance of our algorithm with simulated Gaussian-distributed cosmological lensing shear maps and a reconstruction of the mass distribution of the merging galaxy cluster Abell 781 using galaxy ellipticities measured with the Deep Lens Survey. Given interim posterior samples of lensing shear or convergence fields on the sky, we describe an algorithm to infer cosmological parameters via lens field marginalization. In the most general formulation of our algorithm we make no assumptions about weak shear or Gaussian distributed shape noise or shears. Because we require solutions and matrix determinants of a linear system of dimension that scales with the number of galaxies, we expect our algorithm to require parallel high-performance computing resources for application to ongoing wide field lensing surveys. ",Probabilistic Cosmological Mass Mapping from Weak Lensing Shear
17,789463503396634624,105984261,Tsendsuren,['Just arxived our new paper on Reasoning with Memory Augmented Neural Networks  <LINK> #deeplearning #NLProc'],https://arxiv.org/abs/1610.06454v1,"Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. ","] Reasoning with Memory Augmented Neural Networks for Language
  Comprehension"
18,789211657452998656,4438354094,Tom Wong,"[""My new paper w/ Santos (#25 in pic) is my 3rd on exceptional configs in quantum walk search &amp; 1st w/ Szegedy's walk. <LINK> <LINK>""]",https://arxiv.org/abs/1610.06075,"Quantum walks are standard tools for searching graphs for marked vertices, and they often yield quadratic speedups over a classical random walk's hitting time. In some exceptional cases, however, the system only evolves by sign flips, staying in a uniform probability distribution for all time. We prove that the one-dimensional periodic lattice or cycle with any arrangement of marked vertices is such an exceptional configuration. Using this discovery, we construct a search problem where the quantum walk's random sampling yields an arbitrary speedup in query complexity over the classical random walk's hitting time. In this context, however, the mixing time to prepare the initial uniform state is a more suitable comparison than the hitting time, and then the speedup is roughly quadratic. ",Exceptional Quantum Walk Search on the Cycle
19,789154205101469696,1275120170,Mattia A. Galiazzo,['New paper of mine in collaborations with E. @SilberAstrum &amp; D. Bancelin accepted at Astronomical Notes: <LINK>'],https://arxiv.org/abs/1610.04786,"Asteroids colliding with planets vary in composition and taxonomical type. Among Near-Earth Asteroids (NEAs) are the V-types, basaltic asteroids that are classified via spectroscopic observations. In this work, we study the probability of V-type NEAs colliding with Earth, Mars and Venus, as well as the Moon. We perform a correlational analysis of possible craters produced by V-type NEAs. To achieve this, we performed numerical simulations and statistical analysis of close encounters and impacts between V-type NEAs and the terrestrial planets over the next 10 Myr. We find that V-type NEAs can indeed have impacts with all the planets, the Earth in particular, at an average rate of once per 12 Myr. There are four candidate craters on Earth that were likely caused by V-type NEAs. ","V-type Near-Earth asteroids: dynamics, close encounters and impacts with
  terrestrial planets"
20,789105922303942656,4438354094,Tom Wong,['New paper w/ Santos @lvuniversity showing greater-than-quadratic speedup by quantum walk over classical random walk. <LINK> <LINK>'],https://arxiv.org/abs/1610.06075,"Quantum walks are standard tools for searching graphs for marked vertices, and they often yield quadratic speedups over a classical random walk's hitting time. In some exceptional cases, however, the system only evolves by sign flips, staying in a uniform probability distribution for all time. We prove that the one-dimensional periodic lattice or cycle with any arrangement of marked vertices is such an exceptional configuration. Using this discovery, we construct a search problem where the quantum walk's random sampling yields an arbitrary speedup in query complexity over the classical random walk's hitting time. In this context, however, the mixing time to prepare the initial uniform state is a more suitable comparison than the hitting time, and then the speedup is roughly quadratic. ",Exceptional Quantum Walk Search on the Cycle
21,788964236785356800,3199605543,Afonso S. Bandeira,"[""New paper on a relaxation of the Gromov-Hausdorff distance <LINK> see Soledad's blog post on it <LINK>""]",https://arxiv.org/abs/1610.05214,"The Gromov-Hausdorff distance provides a metric on the set of isometry classes of compact metric spaces. Unfortunately, computing this metric directly is believed to be computationally intractable. Motivated by applications in shape matching and point-cloud comparison, we study a semidefinite programming relaxation of the Gromov-Hausdorff metric. This relaxation can be computed in polynomial time, and somewhat surprisingly is itself a pseudometric. We describe the induced topology on the set of compact metric spaces. Finally, we demonstrate the numerical performance of various algorithms for computing the relaxed distance and apply these algorithms to several relevant data sets. In particular we propose a greedy algorithm for finding the best correspondence between finite metric spaces that can handle hundreds of points. ",A polynomial-time relaxation of the Gromov-Hausdorff distance
22,788566892734717953,169081481,Hanno Rein üí´,['New paper: Using machine learning to predict stability of üåçüåç. Led by Dan Tamayo and my student Ari Silburt. Preprint <LINK>'],https://arxiv.org/abs/1610.05359,"The requirement that planetary systems be dynamically stable is often used to vet new discoveries or set limits on unconstrained masses or orbital elements. This is typically carried out via computationally expensive N-body simulations. We show that characterizing the complicated and multi-dimensional stability boundary of tightly packed systems is amenable to machine learning methods. We find that training an XGBoost machine learning algorithm on physically motivated features yields an accurate classifier of stability in packed systems. On the stability timescale investigated ($10^7$ orbits), it is 3 orders of magnitude faster than direct N-body simulations. Optimized machine learning classifiers for dynamical stability may thus prove useful across the discipline, e.g., to characterize the exoplanet sample discovered by the upcoming Transiting Exoplanet Survey Satellite (TESS). This proof of concept motivates investing computational resources to train algorithms capable of predicting stability over longer timescales and over broader regions of phase space. ","A Machine Learns to Predict the Stability of Tightly Packed Planetary
  Systems"
23,788539793000964096,296161364,Chris Power,"['Fernando et al - how durable are planes of satellites around galaxies like MW, M31? New paper with @Cosmic_Horizons, <LINK>']",https://arxiv.org/abs/1610.05393,"The recently discovered vast thin plane of dwarf satellites orbiting the Andromeda Galaxy (M31) adds to the mystery of the small scale distribution of the Local Group's galaxy population. Such well defined planar structures are apparently rare occurrences in cold dark matter cosmological simulations, and we lack a coherent explanation of their formation and existence. In this paper, we explore the long-term survivability of thin planes of dwarfs in galactic halos, focusing, in particular, on systems mimicking the observed Andromeda distribution. The key results show that, in general, planes of dwarf galaxies are fragile, sensitive to the shape of the dark matter halo and other perturbing effects. In fact, long lived planes of satellites only exist in polar orbits in spherical dark matter halos, presenting a challenge to the observed Andromeda plane which is significantly tilted with respect to the optical disk. Our conclusion is that, in standard cosmological models, planes of satellites are generally short lived, and hence we must be located at a relatively special time in the evolution of the Andromeda Plane, lucky enough to see its coherent pattern. ","On the Stability of Satellite Planes I: Effects of Mass, Velocity, Halo
  Shape and Alignment"
24,788008306531008513,438301250,Ferenc Husz√°r,['New paper by our intern @CasperKaae a top-down view on GAN for SR:\nAmortised MAP Inference for Image Superresolution\n<LINK> <LINK>'],https://arxiv.org/abs/1610.04490,"Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders. ",Amortised MAP Inference for Image Super-resolution
25,787985078685171712,94864729,Andrew Leifer,"[""My lab's new paper is on arXiv!:  Tracking neurons in a deforming brain. <LINK>  Cheers to Nguyen, @shaevitz &amp; co-auths""]",https://arxiv.org/abs/1610.04579,"Advances in optical neuroimaging techniques now allow neural activity to be recorded with cellular resolution in awake and behaving animals. Brain motion in these recordings pose a unique challenge. The location of individual neurons must be tracked in 3D over time to accurately extract single neuron activity traces. Recordings from small invertebrates like C. elegans are especially challenging because they undergo very large brain motion and deformation during animal movement. Here we present an automated computer vision pipeline to reliably track populations of neurons with single neuron resolution in the brain of a freely moving C. elegans undergoing large motion and deformation. 3D volumetric fluorescent images of the animal's brain are straightened, aligned and registered, and the locations of neurons in the images are found via segmentation. Each neuron is then assigned an identity using a new time-independent machine-learning approach we call Neuron Registration Vector Encoding. In this approach, non-rigid point-set registration is used to match each segmented neuron in each volume with a set of reference volumes taken from throughout the recording. The way each neuron matches with the references defines a feature vector which is clustered to assign an identity to each neuron in each volume. Finally, thin-plate spline interpolation is used to correct errors in segmentation and check consistency of assigned identities. The Neuron Registration Vector Encoding approach proposed here is uniquely well suited for tracking neurons in brains undergoing large deformations. When applied to whole-brain calcium imaging recordings in freely moving C. elegans, this analysis pipeline located 150 neurons for the duration of an 8 minute recording and consistently found more neurons more quickly than manual or semi-automated approaches. ",Automatically tracking neurons in a moving and deforming brain
26,786935687249530880,719970962,Louise Howes,"[""New paper! <LINK> - finding ages of stars is hard. In case you didn't know..."", ""@LMHowes also gave me a legitimate chance to cite @catloafing, something I've wanted to do since the start of my PhD :)""]",https://arxiv.org/abs/1610.03852,"Several recent studies of Solar twins in the Solar neighbourhood have shown a tight correlation between various elemental abundances and age, in particular [Y/Mg]. If this relation is real and valid for other types of stars as well as elsewhere in the Galaxy it would provide a very powerful tool to derive ages of stars without the need to resort to determining their masses (evolutionary stage) very precisely. The method would also likely work if the stellar parameters have relatively large errors. The studies presented in the recent literature span a narrow range of [Fe/H]. By studying a larger sample of Solar neighbourhood dwarfs with a much larger range in [Fe/H], we find that the relation between [Y/Mg] and age depends on the [Fe/H] of the stars. Hence, it appears that the [Y/Mg] - age relation is unique to Solar analogues. ","On the metallicity dependance of the [Y/Mg] - age relation for solar
  type stars"
27,786550691963613184,90373079,Carlos Ciller,"['Excited to share our new paper on #AMD #OCT! #medical ""RetiNet: Automatic AMD identification in OCT volumetric data"" <LINK>']",https://arxiv.org/abs/1610.03628,"Optical Coherence Tomography (OCT) provides a unique ability to image the eye retina in 3D at micrometer resolution and gives ophthalmologist the ability to visualize retinal diseases such as Age-Related Macular Degeneration (AMD). While visual inspection of OCT volumes remains the main method for AMD identification, doing so is time consuming as each cross-section within the volume must be inspected individually by the clinician. In much the same way, acquiring ground truth information for each cross-section is expensive and time consuming. This fact heavily limits the ability to acquire large amounts of ground truth, which subsequently impacts the performance of learning-based methods geared at automatic pathology identification. To avoid this burden, we propose a novel strategy for automatic analysis of OCT volumes where only volume labels are needed. That is, we train a classifier in a semi-supervised manner to conduct this task. Our approach uses a novel Convolutional Neural Network (CNN) architecture, that only needs volume-level labels to be trained to automatically asses whether an OCT volume is healthy or contains AMD. Our architecture involves first learning a cross-section pathology classifier using pseudo-labels that could be corrupted and then leverage these towards a more accurate volume-level classification. We then show that our approach provides excellent performances on a publicly available dataset and outperforms a number of existing automatic techniques. ",RetiNet: Automatic AMD identification in OCT volumetric data
28,786350100847087616,3245949691,Rebecca Leane,"['New paper! We show impact of mass gen for spin-1 simplified models, consequent pheno absent in one-mediator approach <LINK>']",http://arxiv.org/abs/1610.03063,"In the simplified dark matter models commonly studied, the mass generation mechanism for the dark fields is not typically specified. We demonstrate that the dark matter interaction types, and hence the annihilation processes relevant for relic density and indirect detection, are strongly dictated by the mass generation mechanism chosen for the dark sector particles, and the requirement of gauge invariance. We focus on the class of models in which fermionic dark matter couples to a spin-1 vector or axial-vector mediator. However, in order to generate dark sector mass terms, it is necessary in most cases to introduce a dark Higgs field and thus a spin-0 scalar mediator will also be present. In the case that all the dark sector fields gain masses via coupling to a single dark sector Higgs field, it is mandatory that the axial-vector coupling of the spin-1 mediator to the dark matter is non-zero; the vector coupling may also be present depending on the charge assignments. For all other mass generation options, only pure vector couplings between the spin-1 mediator and the dark matter are allowed. If these coupling restrictions are not obeyed, unphysical results may be obtained such as a violation of unitarity at high energies. These two-mediator scenarios lead to important phenomenology that does not arise in single mediator models. We survey two-mediator dark matter models which contain both vector and scalar mediators, and explore their relic density and indirect detection phenomenology. ",Impact of Mass Generation for Simplified Dark Matter Models
29,786285169166520320,12131042,Jeremy Blackburn,"[""Does this mean our new paper on 4chan's politically incorrect board, /pol/, is 4chan approved?!\n\n<LINK> <LINK>"", '@yangrunenberger Never claimed the approval meant much ;)']",https://arxiv.org/abs/1610.03452,"The discussion-board site 4chan has been part of the Internet's dark underbelly since its inception, and recent political events have put it increasingly in the spotlight. In particular, /pol/, the ""Politically Incorrect"" board, has been a central figure in the outlandish 2016 US election season, as it has often been linked to the alt-right movement and its rhetoric of hate and racism. However, 4chan remains relatively unstudied by the scientific community: little is known about its user base, the content it generates, and how it affects other parts of the Web. In this paper, we start addressing this gap by analyzing /pol/ along several axes, using a dataset of over 8M posts we collected over two and a half months. First, we perform a general characterization, showing that /pol/ users are well distributed around the world and that 4chan's unique features encourage fresh discussions. We also analyze content, finding, for instance, that YouTube links and hate speech are predominant on /pol/. Overall, our analysis not only provides the first measurement study of /pol/, but also insight into online harassment and hate speech trends in social media. ","Kek, Cucks, and God Emperor Trump: A Measurement Study of 4chan's
  Politically Incorrect Forum and Its Effects on the Web"
30,786208690537762816,261324356,Ray Jayawardhana,"[""Ultimate survivors. Tatooine planets often survive aging stars's shenanigans-our new paper: <LINK> #exoplanets @NASAKepler""]",https://arxiv.org/abs/1610.03436,"Inspired by the recent Kepler discoveries of circumbinary planets orbiting nine close binary stars, we explore the fate of the former as the latter evolve off the main sequence. We combine binary star evolution models with dynamical simulations to study the orbital evolution of these planets as their hosts undergo common-envelope stages, losing in the process a tremendous amount of mass on dynamical timescales. Five of the systems experience at least one Roche-lobe overflow and common-envelope stages (Kepler-1647 experiences three), and the binary stars either shrink to very short orbits or coalesce; two systems trigger a double-degenerate supernova explosion. Kepler's circumbinary planets predominantly remain gravitationally bound at the end of the common-envelope phase, migrate to larger orbits, and may gain significant eccentricity; their orbital expansion can be more than an order of magnitude and can occur over the course of a single planetary orbit. The orbits these planets can reach are qualitatively consistent with those of the currently known post-common-envelope, eclipse-time variations circumbinary candidates. Our results also show that circumbinary planets can experience both modes of orbital expansion (adiabatic and non-adiabatic) if their host binaries undergo more than one common-envelope stage; multiplanet circumbinary systems like Kepler-47 can experience both modes during the same common-envelope stage. Additionally, unlike Mercury orbiting the Sun, a circumbinary planet with the same semi-major axis can survive the common envelope evolution of a close binary star with a total mass of 1 MSun. ","Tatooine's Future: The Eccentric Response of Kepler's Circumbinary
  Planets to Common-Envelope Evolution of their Host Stars"
31,785739936641056768,2594977088,Alban Sauret,"['Our new paper ""Drop morphologies on flexible fibers: influence of elastocapillary effects"" is now on arXiv: <LINK> <LINK>']",https://arxiv.org/abs/1610.02555,"Various materials are made of long thin fibers that are randomly oriented to form a complex network in which drops of wetting liquid tend to accumulate at the nodes. The capillary force exerted by the liquid can bend flexible fibers, which in turn influences the morphology adopted by the liquid. In this paper, we investigate, the role of the fiber flexibility on the shape of a small volume of liquid on a pair of crossed flexible fibers, through a model situation. We characterize the liquid morphologies as we vary the volume of liquid, the angle between the fibers, and the length of the fibers. The drop morphologies previously reported for rigid crossed fibers, i.e., a drop, a column and a mixed morphology, are also observed on flexible crossed fibers with modified domains of existence. In addition, at small tilting angles between the fibers, a new behavior is observed: the fibers bend and collapse. Depending on the volume of liquid, a thin column with or without a drop is reported on the collapsed fibers. Our study suggests that the fiber flexibility adds a rich variety of behaviors that may be important for some applications. ","Drop morphologies on flexible fibers: influence of elastocapillary
  effects"
32,785558042851049472,3433220662,Anthony Bonato,['Our new paper: Geometric random graphs and Rado sets in sequence spaces <LINK>'],https://arxiv.org/abs/1610.02381,"We consider a random geometric graph model, where pairs of vertices are points in a metric space and edges are formed independently with fixed probability $p$ between pairs within threshold distance $\delta $. A countable dense set in a metric space is {\sl Rado} if this random model gives, with probability 1, a graph that is unique up to isomorphism. In earlier work, the first two authors proved that in finite dimensional spaces $\mathbb{R}^n$ equipped with the $\ell_{\infty}$ norm, all countable dense set satisfying a mild non-integrality condition are Rado. In this paper, we extend this result to infinite-dimensional spaces. If the underlying metric space is a separable Banach space, then we show in some cases that we can almost surely recover the Banach space from such a geometric random graph. More precisely, we show that in the sequence spaces $c$ and $c_0$, for measures $\mu$ satisfying certain conditions, $\mu^\N$-almost all countable sets are Rado. Moreover, with probability 1, in $c$ as in $c_0$, all graphs obtained from the random geometric model with a randomly chosen dense countable vertex set are isomorphic to each other. Finally, we show that representatives of the isomorphism classes obtained in this way from $c$ and $c_0$ are non-isomorphic to each other, and also non-isomorphic to their counterparts obtained from finite dimensional spaces. ",Geometric random graphs and Rado sets in sequence spaces
33,785511714334703616,68746721,Fran√ßois Chollet,"['New paper: ""Deep learning with separable convolutions"". <LINK> - exploring what\'s next in convnet design after Inception.', ""Separable convs have been around since 2014, but they're still almost completely absent from convnet design."", 'But in the future, we will build convnets entirely out of separable convs. They will come to replace regular convolutions in most cases.', 'If you\'re curious about the history of separable convs, see the ""prior work"" section --first time their story is told, as far as I can tell.', 'However it seems I have missed two references, so I will quickly release an updated version of the paper.', '@quasimondo Yes, in convnets. But in image processing the term ""separable convolutions"" refers to spatial separability, a different concept.', '@ankesh_anand we will release code and weights files within a few weeks.']",https://arxiv.org/abs/1610.02357,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. ",Xception: Deep Learning with Depthwise Separable Convolutions
34,784316471547527168,1728419371,Dr Joanna Barstow,"['Happy New Paper Day! A consistent analysis of 10 cloudy/hazy hot Jupiters, shortly to appear in ApJ &amp; now on arXiv: <LINK>', '@sarahkendrew mostly making sure the retrievals stuff for the ARIEL yellow book is written before Christmas. Tying up loose ends in general.', '@sarahkendrew getting ready for my new and very different life!']",https://arxiv.org/abs/1610.01841,"We present a consistent optimal estimation retrieval analysis of ten hot Jupiter exoplanets, each with transmission spectral data spanning the visible to near-infrared wavelength range. Using the NEMESIS radiative transfer and retrieval tool, we calculate a range of possible atmospheric states for WASP-6b, WASP-12b, WASP-17b, WASP-19b, WASP-31b, WASP-39b, HD 189733b, HD 209458b, HAT-P-1b and HAT-P-12b. We find that the spectra of all ten planets are consistent with the presence of some atmospheric aerosol; WASP-6b, WASP-12b, WASP-17b, WASP-19b, HD 189733b and HAT-P-12b are all fit best by Rayleigh scattering aerosols, whereas WASP-31b, WASP-39b and HD 209458b are better represented by a grey cloud model. HAT-P-1b has solutions that fall into both categories. WASP-6b, HAT-P-12b, HD 189733b and WASP-12b must have aerosol extending to low atmospheric pressures (below 0.1 mbar). In general, planets with equilibrium temperatures between 1300 and 1700 K are best represented by deeper, grey cloud layers, whereas cooler or hotter planets are better fit using high Rayleigh scattering aerosol. We find little evidence for the presence of molecular absorbers other than H$_2$O. Retrieval methods can provide a consistent picture across a range of hot Jupiter atmospheres with existing data, and will be a powerful tool for the interpretation of James Webb Space Telescope observations. ","A consistent retrieval analysis of 10 Hot Jupiters observed in
  transmission"
35,784217618962014213,16946972,mickbremner,"['New paper with @quantumashley ""Achieving quantum supremacy with sparse and noisy commuting quantum computations"" <LINK>']",https://arxiv.org/abs/1610.01808,"The class of commuting quantum circuits known as IQP (instantaneous quantum polynomial-time) has been shown to be hard to simulate classically, assuming certain complexity-theoretic conjectures. Here we study the power of IQP circuits in the presence of physically motivated constraints. First, we show that there is a family of sparse IQP circuits that can be implemented on a square lattice of n qubits in depth O(sqrt(n) log n), and which is likely hard to simulate classically. Next, we show that, if an arbitrarily small constant amount of noise is applied to each qubit at the end of any IQP circuit whose output probability distribution is sufficiently anticoncentrated, there is a polynomial-time classical algorithm that simulates sampling from the resulting distribution, up to constant accuracy in total variation distance. However, we show that purely classical error-correction techniques can be used to design IQP circuits which remain hard to simulate classically, even in the presence of arbitrary amounts of noise of this form. These results demonstrate the challenges faced by experiments designed to demonstrate quantum supremacy over classical computation, and how these challenges can be overcome. ","Achieving quantum supremacy with sparse and noisy commuting quantum
  computations"
36,784075968021401601,296097040,David S. Amundsen,['New paper on the treatment of overlapping gaseous absorption with the correlated-k method on arXiv today.\n<LINK>'],https://arxiv.org/abs/1610.01389,"The correlated-k method is frequently used to speed up radiation calculations in both one-dimensional and three-dimensional atmosphere models. An inherent difficulty with this method is how to treat overlapping absorption, i.e. absorption by more than one gas in a given spectral region. We have evaluated the applicability of three different methods in hot Jupiter and brown dwarf atmosphere models, all of which have been previously applied within models in the literature: (i) Random overlap, both with and without resorting and rebinning, (ii) equivalent extinction and (iii) pre-mixing of opacities, where (i) and (ii) combine k-coefficients for different gases to obtain k-coefficients for a mixture of gases, while (iii) calculates k-coefficients for a given mixture from the corresponding mixed line-by-line opacities. We find that the random overlap method is the most accurate and flexible of these treatments, and is fast enough to be used in one-dimensional models with resorting and rebinning. In three-dimensional models such as GCMs it is too slow, however, and equivalent extinction can provide a speed-up of at least a factor of three with only a minor loss of accuracy while at the same time retaining the flexibility gained by combining k-coefficients computed for each gas individually. Pre-mixed opacities are significantly less flexible, and we also find that particular care must be taken when using this method in order to to adequately resolve steep variations in composition at important chemical equilibrium boundaries. We use the random overlap method with resorting and rebinning in our one-dimensional atmosphere model and equivalent extinction in our GCM, which allows us to e.g. consistently treat the feedback of non-equilibrium chemistry on the total opacity and therefore the calculated P-T profiles in our models. ","Treatment of overlapping gaseous absorption with the correlated-k method
  in hot Jupiter and brown dwarf atmosphere models"
37,784033790922547200,1937581884,Matthias K√ºmmerer,"['Our new saliency model ""DeepGaze II"" is out! Find the paper at <LINK> and try the webservice at <LINK>']",https://arxiv.org/abs/1610.01563,"Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at this http URL ","DeepGaze II: Reading fixations from deep features trained on object
  recognition"
38,783214449180028928,48944724,Dr Sangeeta Bhatia,['Our new paper is available on arxiv. \n<LINK>'],https://arxiv.org/abs/1610.00077,"Modellers of large scale genome rearrangement events, in which segments of DNA are inverted, moved, swapped, or even inserted or deleted, have found a natural syntax in the language of permutations. Despite this, there has been a wide range of modelling choices, assumptions and interpretations that make navigating the literature a significant challenge. Indeed, even authors of papers that use permutations to model genome rearrangement can struggle to interpret each others' work, because of subtle differences in basic assumptions that are often deeply ingrained (and consequently sometimes not even mentioned). In this paper, we describe the different ways in which permutations have been used to model genomes and genome rearrangement events, presenting some features and limitations of each approach, and show how the various models are related. This paper will help researchers navigate the landscape of genome rearrangement models, and make it easier for authors to present clear and consistent models. ","Position and content paradigms in genome rearrangements: the wild and
  crazy world of permutations in genomics"
39,783209311866589184,2205460310,Gerold Baier üß†üëÇ,['New paper on Transient Rhythms following electrical stimulation in epilepsy: models with 4 variables still tricky <LINK> <LINK>'],https://arxiv.org/abs/1610.00262,"Electro-cortical activity in patients with epilepsy may show abnormal rhythmic transients in response to stimulation. Even when using the same stimulation parameters in the same patient, wide variability in the duration of transient response has been reported. These transients have long been considered important for the mapping of the excitability levels in the epileptic brain but their dynamic mechanism is still not well understood. To understand the occurrence of abnormal transients dynamically, we use a thalamo-cortical neural population model of epileptic spike-wave activity and study the interaction between slow and fast subsystems. In a reduced version of the thalamo-cortical model, slow wave oscillations arise from a fold of cycles (FoC) bifurcation. This marks the onset of a region of bistability between a high amplitude oscillatory rhythm and the background state. In vicinity of the bistability in parameter space, the model has excitable dynamics, showing prolonged rhythmic transients in response to suprathreshold pulse stimulation. We analyse the state space geometry of the bistable and excitable states, and find that the rhythmic transient arises when the impending FoC bifurcation deforms the state space and creates an area of locally reduced attraction to the fixed point. This area essentially allows trajectories to dwell there before escaping to the stable steady state, thus creating rhythmic transients. In the full thalamo-cortical model, we find a similar FoC bifurcation structure. Based on the analysis, we propose an explanation of why stimulation induced epileptiform activity may vary between trials, and predict how the variability could be related to ongoing oscillatory background activity. ","Understanding Epileptiform After-Discharges as Rhythmic Oscillatory
  Transients"
40,783100715069607936,15039770,Dr. Cassandra Granade,"['I am happy to say that we have a new paper out today, describing our statistical inference software, QInfer. <LINK>', 'This paper describes our effort to make data processing in quantum information easier, more robust, and reproducible.', 'Getting to 1.0 has been a long and collaborative effort, and I am proud to be a part in making better tools for getting research done.', ""As a part of our efforts, we've provided a full user's guide (https://t.co/kg3Gcf6nVa) and reproducible examples (https://t.co/Rt9il6AtRB)."", 'It is my hope that these efforts will help make our work a useful tool not just for us, but to the community more generally.']",https://arxiv.org/abs/1610.00336,"Characterizing quantum systems through experimental data is critical to applications as diverse as metrology and quantum computing. Analyzing this experimental data in a robust and reproducible manner is made challenging, however, by the lack of readily-available software for performing principled statistical analysis. We improve the robustness and reproducibility of characterization by introducing an open-source library, QInfer, to address this need. Our library makes it easy to analyze data from tomography, randomized benchmarking, and Hamiltonian learning experiments either in post-processing, or online as data is acquired. QInfer also provides functionality for predicting the performance of proposed experimental protocols from simulated runs. By delivering easy-to-use characterization tools based on principled statistical analysis, QInfer helps address many outstanding challenges facing quantum technology. ",QInfer: Statistical inference software for quantum applications
41,788896425631358976,2337598033,Geraint F. Lewis,['A new paper with @kaflepraj - The Architecture of Andromeda: <LINK> <LINK>'],https://arxiv.org/abs/1610.05920,"We present a quantitative measurement of the amount of clustering present in the inner $\sim30$ kpc of the stellar halo of the Andromeda galaxy (M31). For this we analyse the angular positions and radial velocities of the carefully selected Planetary Nebulae (PNe) in the M31 stellar halo. We study the cumulative distribution of pair-wise distances in angular position and line of sight velocity space, and find that the M31 stellar halo contains substantially more stars in the form of close pairs as compared to that of a featureless smooth halo. In comparison to a smoothed/scrambled distribution we estimate that the clustering excess in the M31 inner halo is roughly $40\%$ at maximum and on average $\sim 20\%$. Importantly, comparing against the 11 stellar halo models of \cite{2005ApJ...635..931B}, which were simulated within the context of the $\Lambda{\rm CDM}$ cosmological paradigm, we find that the amount of substructures in the M31 stellar halo closely resembles that of a typical $\Lambda{\rm CDM}$ halo. ","Architecture of the Andromeda: a quantitative analysis of clustering in
  the inner stellar halo"
42,784332741240160258,340037000,Pedro A. Ortega,"['Our new NIPS paper ""Human Decision-Making under Limited Time"" is out! <LINK> <LINK>']",https://arxiv.org/abs/1610.01698,"Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints - i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations. ",Human Decision-Making under Limited Time
43,786323092360626176,774301639,Dr. Halil Bisgin,['We have published our first study w/ @tozCSS using #TwitterData on #FlintWaterCrisis. Hope enjoy.\n@flintjournal\n<LINK>'],https://arxiv.org/abs/1610.03480,"Attribution of responsibility and blame are important topics in political science especially as individuals tend to think of political issues in terms of questions of responsibility, and as blame carries far more weight in voting behavior than that of credit. However, surprisingly, there is a paucity of studies on the attribution of responsibility and blame in the field of disaster research. The Flint water crisis is a story of government failure at all levels. By studying microblog posts about it, we understand how citizens assign responsibility and blame regarding such a man-made disaster online. We form hypotheses based on social scientific theories in disaster research and then operationalize them on unobtrusive, observational social media data. In particular, we investigate the following phenomena: the source for blame; the partisan predisposition; the concerned geographies; and the contagion of complaining. This paper adds to the sociology of disasters research by exploiting a new, rarely used data source (the social web), and by employing new computational methods (such as sentiment analysis and retrospective cohort study design) on this new form of data. In this regard, this work should be seen as the first step toward drawing more challenging inferences on the sociology of disasters from ""big social data"". ","Attribution of Responsibility and Blame Regarding a Man-made Disaster:
  #FlintWaterCrisis"
