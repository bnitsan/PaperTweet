,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,937420473587007488,2337598033,Geraint F. Lewis,['A new paper on the arXiv - <LINK> <LINK>'],https://arxiv.org/abs/1711.11588,"We present accretion disk size measurements for 15 luminous quasars at $0.7 \leq z \leq 1.9$ derived from $griz$ light curves from the Dark Energy Survey. We measure the disk sizes with continuum reverberation mapping using two methods, both of which are derived from the expectation that accretion disks have a radial temperature gradient and the continuum emission at a given radius is well-described by a single blackbody. In the first method we measure the relative lags between the multiband light curves, which provides the relative time lag between shorter and longer wavelength variations. From this, we are only able to constrain upper limits on disk sizes, as many are consistent with no lag the 2$\sigma$ level. The second method fits the model parameters for the canonical thin disk directly rather than solving for the individual time lags between the light curves. Our measurements demonstrate good agreement with the sizes predicted by this model for accretion rates between 0.3-1 times the Eddington rate. Given our large uncertainties, our measurements are also consistent with disk size measurements from gravitational microlensing studies of strongly lensed quasars, as well as other photometric reverberation mapping results, that find disk sizes that are a factor of a few ($\sim$3) larger than predictions. ","Quasar Accretion Disk Sizes From Continuum Reverberation Mapping From
  the Dark Energy Survey"
1,936685145171611648,190138220,Jonathan Pillow,"['New #arxiv paper with @waq1129 on ""dependent relevance determination"" (DRD) priors for fMRI decoding. Basic idea: non-zero coefficients should be spatially clustered. Works like ARD, but with dependencies in prior variances induced by a Gaussian process.\n<LINK>', '@waq1129 Graphical model showing ""structured-sparse"" weights sampled from DRD prior. https://t.co/LYfM7MJUm0']",https://arxiv.org/abs/1711.10058,"In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as ""region sparsity."" Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging. ","Dependent relevance determination for smooth and structured sparse
  regression"
2,936644857174642688,1524323209,Lenz Lab,"[""Stability from activity - Ananyo's new paper now on the arXiv - <LINK>""]",http://arxiv.org/abs/1711.02407,"Suspensions of actively driven anisotropic objects exhibit distinctively nonequilibrium behaviors, and current theories predict that they are incapable of sustaining orientational order at high activity. By contrast, here we show that nematic suspensions on a substrate can display order at arbitrarily high activity due to a previously unreported, potentially stabilizing active force. The resulting nonequilibrium ordered phase displays robust giant number fluctuations that cannot be suppressed even by an incompressible solvent. Our results apply to virtually all experimental assays used to investigate the active nematic ordering of self-propelled colloids, bacterial suspensions and the cytoskeleton, and have testable implications in interpreting their nonequilibrium behaviors ",Stability from activity
3,936205594012323840,2766925212,Andrew Childs,"['New paper (2+ years in the making) on resource requirements for the simplest useful quantum computations, with Maslov, Nam, Ross, @yuansu_umd: <LINK>']",http://arxiv.org/abs/1711.10980,"With quantum computers of significant size now on the horizon, we should understand how to best exploit their initially limited abilities. To this end, we aim to identify a practical problem that is beyond the reach of current classical computers, but that requires the fewest resources for a quantum computer. We consider quantum simulation of spin systems, which could be applied to understand condensed matter phenomena. We synthesize explicit circuits for three leading quantum simulation algorithms, employing diverse techniques to tighten error bounds and optimize circuit implementations. Quantum signal processing appears to be preferred among algorithms with rigorous performance guarantees, whereas higher-order product formulas prevail if empirical error estimates suffice. Our circuits are orders of magnitude smaller than those for the simplest classically-infeasible instances of factoring and quantum chemistry. ",Toward the first quantum simulation with quantum speedup
4,936139397283696642,633009378,Xavier Gir√≥üéó,"['Our latest paper ""Saliency Weighted Convolutional Features for Instance Search"", sets a new state of the art on the INSTRE dataset. A first publication from @evamohe Phd, advised by @kevinmcguinness &amp; @oconnorn from @insight_centre @DublinCityUni <LINK> <LINK>']",https://arxiv.org/abs/1711.10795,"This work explores attention models to weight the contribution of local convolutional representations for the instance search task. We present a retrieval framework based on bags of local convolutional features (BLCF) that benefits from saliency weighting to build an efficient image representation. The use of human visual attention models (saliency) allows significant improvements in retrieval performance without the need to conduct region analysis or spatial verification, and without requiring any feature fine tuning. We investigate the impact of different saliency models, finding that higher performance on saliency benchmarks does not necessarily equate to improved performance when used in instance search tasks. The proposed approach outperforms the state-of-the-art on the challenging INSTRE benchmark by a large margin, and provides similar performance on the Oxford and Paris benchmarks compared to more complex methods that use off-the-shelf representations. The source code used in this project is available at this https URL ",Saliency Weighted Convolutional Features for Instance Search
5,935857348446445568,704923404978294784,Gudrun Wanner,['Interested in #LISA? A preprint of our new paper is now available (‚Üí <LINK>) describing an experimental demonstration of cross-coupling noise mitigation by imaging systems in a setup representative for a LISA test mass interferometer.'],https://arxiv.org/abs/1711.10320,"Objects sensed by laser interferometers are usually not stable in position or orientation. This angular instability can lead to a coupling of angular tilt to apparent longitudinal displacement -- tilt-to-length coupling (TTL). In LISA this is a potential noise source for both the test mass interferometer and the long-arm interferometer. We have experimentally investigated TTL coupling in a setup representative for the LISA test mass interferometer and used this system to characterise two different imaging systems (a two-lens design and a four-lens design) both designed to minimise TTL coupling. We show that both imaging systems meet the LISA requirement of +-25 um/rad for interfering beams with relative angles of up to +-300 urad. Furthermore, we found a dependency of the TTL coupling on beam properties such as the waist size and location, which we characterised both theoretically and experimentally. ",Reducing tilt-to-length coupling for the LISA test mass interferometer
6,935836995980595202,16674284,Mirco Musolesi,"['New paper available on arXiv: ""Intelligent Notification Systems: A Survey of the State of the Art and\n Research Challenges‚Äù - Comments and suggestions are very welcome\n<LINK>  \\w @mehrotrabhinav']",https://arxiv.org/abs/1711.10171,"Notifications provide a unique mechanism for increasing the effectiveness of real-time information delivery systems. However, notifications that demand users' attention at inopportune moments are more likely to have adverse effects and might become a cause of potential disruption rather than proving beneficial to users. In order to address these challenges a variety of intelligent notification mechanisms based on monitoring and learning users' behavior have been proposed. The goal of such mechanisms is maximizing users' receptivity to the delivered information by automatically inferring the right time and the right context for sending a certain type of information. This article provides an overview of the current state of the art in the area of intelligent notification mechanisms that relies on the awareness of users' context and preferences. More specifically, we first present a survey of studies focusing on understanding and modeling users' interruptibility and receptivity to notifications from desktops and mobile devices. Then, we discuss the existing challenges and opportunities in developing mechanisms for intelligent notification systems in a variety of application scenarios. ","Intelligent Notification Systems: A Survey of the State of the Art and
  Research Challenges"
7,935658579956178944,3094610676,Pranav Rajpurkar,"['Our new paper on ""Malaria Likelihood Prediction By Effectively Surveying Households Using Deep Reinforcement Learning"", with Vinaya Polamreddi (@psvinaya) and Anusha Balakrishnan is up, and accepted at the NIPS ML4Health workshop: <LINK>']",http://arxiv.org/abs/1711.09223,"We build a deep reinforcement learning (RL) agent that can predict the likelihood of an individual testing positive for malaria by asking questions about their household. The RL agent learns to determine which survey question to ask next and when to stop to make a prediction about their likelihood of malaria based on their responses hitherto. The agent incurs a small penalty for each question asked, and a large reward/penalty for making the correct/wrong prediction; it thus has to learn to balance the length of the survey with the accuracy of its final predictions. Our RL agent is a Deep Q-network that learns a policy directly from the responses to the questions, with an action defined for each possible survey question and for each possible prediction class. We focus on Kenya, where malaria is a massive health burden, and train the RL agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey 2015. To investigate the importance of having survey questions be adaptive to responses, we compare our RL agent to a supervised learning (SL) baseline that fixes its set of survey questions a priori. We evaluate on prediction accuracy and on the number of survey questions asked on a holdout set and find that the RL agent is able to predict with 80% accuracy, using only 2.5 questions on average. In addition, the RL agent learns to survey adaptively to responses and is able to match the SL baseline in prediction accuracy while significantly reducing survey length. ","Malaria Likelihood Prediction By Effectively Surveying Households Using
  Deep Reinforcement Learning"
8,935418347352657921,14124467,Clemens Marschner,"['Our new paper on Arxiv: ""Scalable Object Detection for Stylized Objects"" <LINK>']",http://arxiv.org/abs/1711.09822,"Following recent breakthroughs in convolutional neural networks and monolithic model architectures, state-of-the-art object detection models can reliably and accurately scale into the realm of up to thousands of classes. Things quickly break down, however, when scaling into the tens of thousands, or, eventually, to millions or billions of unique objects. Further, bounding box-trained end-to-end models require extensive training data. Even though - with some tricks using hierarchies - one can sometimes scale up to thousands of classes, the labor requirements for clean image annotations quickly get out of control. In this paper, we present a two-layer object detection method for brand logos and other stylized objects for which prototypical images exist. It can scale to large numbers of unique classes. Our first layer is a CNN from the Single Shot Multibox Detector family of models that learns to propose regions where some stylized object is likely to appear. The contents of a proposed bounding box is then run against an image index that is targeted for the retrieval task at hand. The proposed architecture scales to a large number of object classes, allows to continously add new classes without retraining, and exhibits state-of-the-art quality on a stylized object detection task such as logo recognition. ",Scalable Object Detection for Stylized Objects
9,935399835460685825,12866362,Willi Richert,"['Check out our  new paper on computer vision: ""Scalable Object Detection for Stylized Objects"" <LINK>']",https://arxiv.org/abs/1711.09822,"Following recent breakthroughs in convolutional neural networks and monolithic model architectures, state-of-the-art object detection models can reliably and accurately scale into the realm of up to thousands of classes. Things quickly break down, however, when scaling into the tens of thousands, or, eventually, to millions or billions of unique objects. Further, bounding box-trained end-to-end models require extensive training data. Even though - with some tricks using hierarchies - one can sometimes scale up to thousands of classes, the labor requirements for clean image annotations quickly get out of control. In this paper, we present a two-layer object detection method for brand logos and other stylized objects for which prototypical images exist. It can scale to large numbers of unique classes. Our first layer is a CNN from the Single Shot Multibox Detector family of models that learns to propose regions where some stylized object is likely to appear. The contents of a proposed bounding box is then run against an image index that is targeted for the retrieval task at hand. The proposed architecture scales to a large number of object classes, allows to continously add new classes without retraining, and exhibits state-of-the-art quality on a stylized object detection task such as logo recognition. ",Scalable Object Detection for Stylized Objects
10,935372244779520001,624806128,hocine cherifi,"['Community detection: Quality metrics, Clustering metrics, Topology?\nRead our new paper:\n<LINK>']",http://arxiv.org/abs/1711.09472,"Community structure is of paramount importance for the understanding of complex networks. Consequently, there is a tremendous effort in order to develop efficient community detection algorithms. Unfortunately, the issue of a fair assessment of these algorithms is a thriving open question. If the ground-truth community structure is available, various clustering-based metrics are used in order to compare it versus the one discovered by these algorithms. However, these metrics defined at the node level are fairly insensitive to the variation of the overall community structure. To overcome these limitations, we propose to exploit the topological features of the 'community graphs' (where the nodes are the communities and the links represent their interactions) in order to evaluate the algorithms. To illustrate our methodology, we conduct a comprehensive analysis of overlapping community detection algorithms using a set of real-world networks with known a priori community structure. Results provide a better perception of their relative performance as compared to classical metrics. Moreover, they show that more emphasis should be put on the topology of the community structure. We also investigate the relationship between the topological properties of the community structure and the alternative evaluation measures (quality metrics and clustering metrics). It appears clearly that they present different views of the community structure and that they must be combined in order to evaluate the effectiveness of community detection algorithms. ",Community detection algorithm evaluation with ground-truth data
11,935334806870663170,1576235694,Michael Brown,"['New paper - Galaxy And Mass Assembly (GAMA): the G02 field, Herschel-ATLAS target selection and Data Release 3\n<LINK>']",https://arxiv.org/abs/1711.09139,"We describe data release 3 (DR3) of the Galaxy And Mass Assembly (GAMA) survey. The GAMA survey is a spectroscopic redshift and multi-wavelength photometric survey in three equatorial regions each of 60.0 deg^2 (G09, G12, G15), and two southern regions of 55.7 deg^2 (G02) and 50.6 deg^2 (G23). DR3 consists of: the first release of data covering the G02 region and of data on H-ATLAS sources in the equatorial regions; and updates to data on sources released in DR2. DR3 includes 154809 sources with secure redshifts across four regions. A subset of the G02 region is 95.5% redshift complete to r<19.8 over an area of 19.5 deg^2, with 20086 galaxy redshifts, that overlaps substantially with the XXL survey (X-ray) and VIPERS (redshift survey). In the equatorial regions, the main survey has even higher completeness (98.5%), and spectra for about 75% of H-ATLAS filler targets were also obtained. This filler sample extends spectroscopic redshifts, for probable optical counterparts to H-ATLAS sub-mm sources, to 0.8 mag deeper (r<20.6) than the GAMA main survey. There are 25814 galaxy redshifts for H-ATLAS sources from the GAMA main or filler surveys. GAMA DR3 is available at the survey website (www.gama-survey.org/dr3/). ","Galaxy And Mass Assembly (GAMA): the G02 field, Herschel-ATLAS target
  selection and Data Release 3"
12,935184996880527360,12309242,Onur Mutlu,"['Our new paper on #Nanopore #Sequencing with @damlaasenol, Jeremie Kim, @SGwithADD and @calkan_cs posted on @arxiv: <LINK>. <LINK>']",http://arxiv.org/abs/1711.08774,"Nanopore sequencing technology has the potential to render other sequencing technologies obsolete with its ability to generate long reads and provide portability. However, high error rates of the technology pose a challenge while generating accurate genome assemblies. The tools used for nanopore sequence analysis are of critical importance as they should overcome the high error rates of the technology. Our goal in this work is to comprehensively analyze current publicly available tools for nanopore sequence analysis to understand their advantages, disadvantages, and performance bottlenecks. It is important to understand where the current tools do not perform well to develop better tools. To this end, we 1) analyze the multiple steps and the associated tools in the genome assembly pipeline using nanopore sequence data, and 2) provide guidelines for determining the appropriate tools for each step. We analyze various combinations of different tools and expose the tradeoffs between accuracy, performance, memory usage and scalability. We conclude that our observations can guide researchers and practitioners in making conscious and effective choices for each step of the genome assembly pipeline using nanopore sequence data. Also, with the help of bottlenecks we have found, developers can improve the current tools or build new ones that are both accurate and fast, in order to overcome the high error rates of the nanopore sequencing technology. ","Nanopore Sequencing Technology and Tools for Genome Assembly:
  Computational Analysis of the Current State, Bottlenecks and Future
  Directions"
13,934449096097705984,81099811,Bartolomeo Stellato,['We are excited to announce the new release v0.2 of the OSQP solver! \n<LINK>. Arxiv paper: <LINK>  #optimization #control #finance #machinelearning'],https://arxiv.org/abs/1711.08013,"We present a general-purpose solver for convex quadratic programs based on the alternating direction method of multipliers, employing a novel operator splitting technique that requires the solution of a quasi-definite linear system with the same coefficient matrix at almost every iteration. Our algorithm is very robust, placing no requirements on the problem data such as positive definiteness of the objective function or linear independence of the constraint functions. It can be configured to be division-free once an initial matrix factorization is carried out, making it suitable for real-time applications in embedded systems. In addition, our technique is the first operator splitting method for quadratic programs able to reliably detect primal and dual infeasible problems from the algorithm iterates. The method also supports factorization caching and warm starting, making it particularly efficient when solving parametrized problems arising in finance, control, and machine learning. Our open-source C implementation OSQP has a small footprint, is library-free, and has been extensively tested on many problem instances from a wide variety of application areas. It is typically ten times faster than competing interior-point methods, and sometimes much more when factorization caching or warm start is used. OSQP has already shown a large impact with tens of thousands of users both in academia and in large corporations. ",OSQP: An Operator Splitting Solver for Quadratic Programs
14,933634323986698242,20058831,Matthew Pitkin,"[""I've a new paper out today on speeding up solar system barycentring time delay calculations using Reduced Order Modelling <LINK>. This might interest you if you're into pulsar timing or long duration gravitational wave signals.""]",https://arxiv.org/abs/1711.08386,"The frequencies and phases of emission from extra-solar sources measured by Earth-bound observers are modulated by the motions of the observer with respect to the source, and through relativistic effects. These modulations depend critically on the source's sky-location. Precise knowledge of the modulations are required to coherently track the source's phase over long observations, for example, in pulsar timing, or searches for continuous gravitational waves. The modulations can be modelled as sky-location and time-dependent time delays that convert arrival times at the observer to the inertial frame of the source, which can often be the Solar system barycentre. We study the use of reduced order modelling for speeding up the calculation of this time delay for any sky-location. We find that the time delay model can be decomposed into just four basis vectors, and with these the delay for any sky-location can be reconstructed to sub-nanosecond accuracy. When compared to standard routines for time delay calculation in gravitational wave searches, using the reduced basis can lead to speed-ups of 30 times. We have also studied components of time delays for sources in binary systems. Assuming eccentricities $< 0.25$ we can reconstruct the delays to within 100s of nanoseconds, with best case speed-ups of a factor of 10, or factors of two when interpolating the basis for different orbital periods or time stamps. In long-duration phase-coherent searches for sources with sky-position uncertainties, or binary parameter uncertainties, these speed-ups could allow enhancements in their scopes without large additional computational burdens. ","Reduced order modelling in searches for continuous gravitational waves -
  I. Barycentering time delays"
15,933603859339137024,1576276220,Dr. Emily Petroff,['The newest results from @SUPERB_PSR_FRB are out! FOUR NEW FRBs from 2015 and 2016 led by student Shivani Bhandari. Check out the paper here: <LINK>'],https://arxiv.org/abs/1711.08110,"We report the discovery of four Fast Radio Bursts (FRBs) in the ongoing SUrvey for Pulsars and Extragalactic Radio Bursts (SUPERB) at the Parkes Radio Telescope: FRBs 150610, 151206, 151230 and 160102. Our real-time discoveries have enabled us to conduct extensive, rapid multi-messenger follow-up at 12 major facilities sensitive to radio, optical, X-ray, gamma-ray photons and neutrinos on time scales ranging from an hour to a few months post-burst. No counterparts to the FRBs were found and we provide upper limits on afterglow luminosities. None of the FRBs were seen to repeat. Formal fits to all FRBs show hints of scattering while their intrinsic widths are unresolved in time. FRB 151206 is at low Galactic latitude, FRB 151230 shows a sharp spectral cutoff, and FRB 160102 has the highest dispersion measure (DM = $2596.1\pm0.3$ pc cm$^{-3}$) detected to date. Three of the FRBs have high dispersion measures (DM >$1500$ pc cm$^{-3}$), favouring a scenario where the DM is dominated by contributions from the Intergalactic Medium. The slope of the Parkes FRB source counts distribution with fluences $>2$ Jyms is $\alpha=-2.2^{+0.6}_{-1.2}$ and still consistent with a Euclidean distribution ($\alpha=-3/2$). We also find that the all-sky rate is $1.7^{+1.5}_{-0.9}\times10^3$FRBs/($4\pi$ sr)/day above $\sim2$ Jyms and there is currently no strong evidence for a latitude-dependent FRB sky-rate. ","The SUrvey for Pulsars and Extragalactic Radio Bursts II: New FRB
  discoveries and their follow-up"
16,933602887623757826,16174436,Sune Lehmann,"[""New paper with @ulfaslak and @m_rosvall <LINK>\n\nWe combine (then heuristically implemented) thoughts on communities from <LINK> with Martin's beautiful &amp; principled Infomap ideas.""]",https://arxiv.org/abs/1711.07649,"Many real-world networks represent dynamic systems with interactions that change over time, often in uncoordinated ways and at irregular intervals. For example, university students connect in intermittent groups that repeatedly form and dissolve based on multiple factors, including their lectures, interests, and friends. Such dynamic systems can be represented as multilayer networks where each layer represents a snapshot of the temporal network. In this representation, it is crucial that the links between layers accurately capture real dependencies between those layers. Often, however, these dependencies are unknown. Therefore, current methods connect layers based on simplistic assumptions that do not capture node-level layer dependencies. For example, connecting every node to itself in other layers with the same weight can wipe out dependencies between intermittent groups, making it difficult or even impossible to identify them. In this paper, we present a principled approach to estimating node-level layer dependencies based on the network structure within each layer. We implement our node-level coupling method in the community detection framework Infomap and demonstrate its performance compared to current methods on synthetic and real temporal networks. We show that our approach more effectively constrains information inside multilayer communities so that Infomap can better recover planted groups in multilayer benchmark networks that represent multiple modes with different groups and better identify intermittent communities in real temporal contact networks. These results suggest that node-level layer coupling can improve the modeling of information spreading in temporal networks and better capture intermittent community structure. ","Constrained information flows in temporal networks reveal intermittent
  communities"
17,933021799746625536,1843208342,Nick Pawlowski,['New @dltk_ version for easier #DeepLearning on medical images live at <LINK> Now with @TensorFlow Estimators and Model Zoo. Find the corresponding paper at <LINK> or at MI meets #nips2017\nContributions welcome.'],http://arxiv.org/abs/1711.06853,"We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK's reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data ""Multi-Atlas Labeling Beyond the Cranial Vault"". The average test Dice similarity coefficient of $81.5$ exceeds the previously best performing CNN ($75.7$) and the accuracy of the challenge winning method ($79.0$). ","DLTK: State of the Art Reference Implementations for Deep Learning on
  Medical Images"
18,933014113437011970,4179822388,Martin Rajchl,"['We are happy to announce the release of the new version of #DLTK for medical imaging! Now, with @TensorFlow Estimators and all new Model Zoo! MI meets #nips2017 paper: <LINK>; website: <LINK>; src: <LINK>; <LINK>']",https://arxiv.org/abs/1711.06853,"We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK's reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data ""Multi-Atlas Labeling Beyond the Cranial Vault"". The average test Dice similarity coefficient of $81.5$ exceeds the previously best performing CNN ($75.7$) and the accuracy of the challenge winning method ($79.0$). ","DLTK: State of the Art Reference Implementations for Deep Learning on
  Medical Images"
19,933010869662838784,1283308608,Michael West,"['Very interesting new review paper on the history of Einstein‚Äôs cosmological constant: ""One Hundred Years of the Cosmological Constant: from \'Superfluous Stunt\' to Dark Energy"" <LINK> <LINK>']",https://arxiv.org/abs/1711.06890,"We present a centennial review of the history of the term known as the cosmological constant. First introduced to the general theory of relativity by Einstein in 1917 in order to describe a universe that was assumed to be static, the term fell from favour in the wake of the discovery of the expanding universe, only to make a dramatic return in recent times. We consider historical and philosophical aspects of the cosmological constant over four main epochs: (i) the use of the term in static cosmologies (both Newtonian and relativistic); (ii) the marginalization of the term following the discovery of cosmic expansion; (iii) the use of the term to address specific cosmic puzzles such as the timespan of expansion, the formation of galaxies and the redshifts of the quasars; (iv) the re-emergence of the term in today's Lamda-CDM cosmology. We find that the cosmological constant was never truly banished from theoretical models of the universe, but was sidelined by astronomers for reasons of convenience. We also find that the return of the term to the forefront of modern cosmology did not occur as an abrupt paradigm shift due to one particular set of observations, but as the result of a number of empirical advances such as the measurement of present cosmic expansion using the Hubble Space Telescope, the measurement of past expansion using type SN 1a supernovae as standard candles, and the measurement of perturbations in the cosmic microwave background by balloon and satellite. We give a brief overview of contemporary interpretations of the physics underlying the cosmic constant and conclude with a synopsis of the famous cosmological constant problem. ","One Hundred Years of the Cosmological Constant: from 'Superfluous Stunt'
  to Dark Energy"
20,933009972497207296,14283504,Josh Bongard,"['Our new paper (<LINK>) in a nutshell: An unfit #robot (1) evolves into a walker (2). Its descendent ""discovers"" rolling late in life (3). A mutation causes rolling to show up earlier in its descendent (4). The final descendent rolls throughout its life (5). <LINK>']",http://arxiv.org/abs/1711.07387,"Organisms result from adaptive processes interacting across different time scales. One such interaction is that between development and evolution. Models have shown that development sweeps over several traits in a single agent, sometimes exposing promising static traits. Subsequent evolution can then canalize these rare traits. Thus, development can, under the right conditions, increase evolvability. Here, we report on a previously unknown phenomenon when embodied agents are allowed to develop and evolve: Evolution discovers body plans robust to control changes, these body plans become genetically assimilated, yet controllers for these agents are not assimilated. This allows evolution to continue climbing fitness gradients by tinkering with the developmental programs for controllers within these permissive body plans. This exposes a previously unknown detail about the Baldwin effect: instead of all useful traits becoming genetically assimilated, only traits that render the agent robust to changes in other traits become assimilated. We refer to this as differential canalization. This finding also has implications for the evolutionary design of artificial and embodied agents such as robots: robots robust to internal changes in their controllers may also be robust to external changes in their environment, such as transferal from simulation to reality or deployment in novel environments. ",How morphological development can guide evolution
21,932919544561262592,778228628601602048,BIGWaves,['New @LIGO/@ego_virgo paper looking for long (10‚Äì500 s) unmodelled transient #GravitationalWave signals in our first observing run <LINK> #nondetection <LINK>'],https://arxiv.org/abs/1711.06843,"We present the results of a search for long-duration gravitational wave transients in the data of the LIGO Hanford and LIGO Livingston second generation detectors between September 2015 and January 2016, with a total observational time of 49 days. The search targets gravitational wave transients of \unit[10 -- 500]{s} duration in a frequency band of \unit[24 -- 2048]{Hz}, with minimal assumptions about the signal waveform, polarization, source direction, or time of occurrence. No significant events were observed. %All candidate triggers were consistent with the expected background, As a result we set 90\% confidence upper limits on the rate of long-duration gravitational wave transients for different types of gravitational wave signals. We also show that the search is sensitive to sources in the Galaxy emitting at least $\sim$ \unit[$10^{-8}$]{$\mathrm{M_{\odot} c^2}$} in gravitational waves. ","All-sky search for long-duration gravitational wave transients in the
  first Advanced LIGO observing run"
22,932913050000162816,221292469,Richard Everitt,"['New paper: Bootstrapped synthetic likelihood <LINK>', ""@ABC_Research Thanks! I'll cite this in the revised version.""]",https://arxiv.org/abs/1711.05825,"Approximate Bayesian computation (ABC) and synthetic likelihood (SL) techniques have enabled the use of Bayesian inference for models that may be simulated, but for which the likelihood cannot be evaluated pointwise at values of an unknown parameter $\theta$. The main idea in ABC and SL is to, for different values of $\theta$ (usually chosen using a Monte Carlo algorithm), build estimates of the likelihood based on simulations from the model conditional on $\theta$. The quality of these estimates determines the efficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to improve an estimated likelihood at $\theta$ is to simulate more times from the model conditional on $\theta$, which is infeasible in cases where the simulator is computationally expensive. In this paper we describe how to use bootstrapping as a means for improving SL estimates whilst using fewer simulations from the model, and also investigate its use in ABC. Further, we investigate the use of the bag of little bootstraps as a means for applying this approach to large datasets, yielding Monte Carlo algorithms that accurately approximate posterior distributions whilst only simulating subsamples of the full data. Examples of the approach applied to i.i.d., temporal and spatial data are given. ",Bootstrapped synthetic likelihood
23,932801075026976769,2337598033,Geraint F. Lewis,['A new paper on the arxiv from @astroconfusion <LINK> <LINK>'],https://arxiv.org/abs/1711.06682,"The Monoceros Ring (MRi) structure is an apparent stellar overdensity that has been postulated to entirely encircle the Galactic plane and has been variously described as being due to line-of-sight effects of the Galactic warp and flare or of extragalactic origin (via accretion). Despite being intensely scrutinised in the literature for more than a decade, no studies to-date have been able to definitively uncover its origins. Here we use $N$-body simulations and a genetic algorithm to explore the parameter space for the initial position, orbital parameters and, for the first time, the final location of a satellite progenitor. We fit our models to the latest Pan-STARRS data to determine whether an accretion scenario is capable of producing an in- Plane ring-like structure matching the known parameters of the MRi. Our simulations produce streams that closely match the location, proper motion and kinematics of the MRi structure. However, we are not able to reproduce the mass estimates from earlier studies based on Pan-STARRS data. Furthermore, in contrast with earlier studies our best-fit models are those for progenitors on retrograde orbits. If the MRi was produced by satellite accretion, we find that its progenitor has an initial mass upper limit of ~$10^{10}$M$_\odot$ and the remnant is likely located behind the Galactic bulge, making it diffcult to locate observationally. While our models produce realistic MRi-like structures we cannot definitively conclude that the MRi was produced by the accretion of a satellite galaxy. ","On the Origin of the Monoceros Ring - I: Kinematics, proper motions, and
  the nature of the progenitor"
24,932798325748678656,18850305,Zachary Lipton,"[""***Does mitigating ML's disparate impact require disparate treatment?*** New paper with Alex Chouldechova and Julian McAuley suggests so.\n<LINK>""]",https://arxiv.org/abs/1711.07076,"Following related work in law and policy, two notions of disparity have come to shape the study of fairness in algorithmic decision-making. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups, even if the correlation arises unintentionally. Naturally, we can achieve impact parity through purposeful treatment disparity. In one thread of technical work, papers aim to reconcile the two forms of parity proposing disparate learning processes (DLPs). Here, the learning algorithm can see group membership during training but produce a classifier that is group-blind at test time. In this paper, we show theoretically that: (i) When other features correlate to group membership, DLPs will (indirectly) implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) When group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) In general, DLPs provide a suboptimal trade-off between accuracy and impact parity. Based on our technical analysis, we argue that transparent treatment disparity is preferable to occluded methods for achieving impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs vs. per-group thresholds. ",Does mitigating ML's impact disparity require treatment disparity?
25,932630780320321536,392357882,Francesco Corucci,"['A preprint of our new paper entitled: ""Evolving soft locomotion in aquatic and terrestrial environments: effects of material properties and environmental transitions"" is now available at <LINK> <LINK> #evodevosoro #robots #softrobotics']",https://arxiv.org/abs/1711.06605,"Designing soft robots poses considerable challenges: automated design approaches may be particularly appealing in this field, as they promise to optimize complex multi-material machines with very little or no human intervention. Evolutionary soft robotics is concerned with the application of optimization algorithms inspired by natural evolution in order to let soft robots (both morphologies and controllers) spontaneously evolve within physically-realistic simulated environments, figuring out how to satisfy a set of objectives defined by human designers. In this paper a powerful evolutionary system is put in place in order to perform a broad investigation on the free-form evolution of walking and swimming soft robots in different environments. Three sets of experiments are reported, tackling different aspects of the evolution of soft locomotion. The first two sets explore the effects of different material properties on the evolution of terrestrial and aquatic soft locomotion: particularly, we show how different materials lead to the evolution of different morphologies, behaviors, and energy-performance tradeoffs. It is found that within our simplified physics world stiffer robots evolve more sophisticated and effective gaits and morphologies on land, while softer ones tend to perform better in water. The third set of experiments starts investigating the effect and potential benefits of major environmental transitions (land - water) during evolution. Results provide interesting morphological exaptation phenomena, and point out a potential asymmetry between land-water and water-land transitions: while the first type of transition appears to be detrimental, the second one seems to have some beneficial effects. ","Evolving soft locomotion in aquatic and terrestrial environments:
  effects of material properties and environmental transitions"
26,932624947846971392,14283504,Josh Bongard,['Our new paper on the evolutionary transitions between aquatic and terrestrial soft #robots is now up: <LINK> <LINK>'],https://arxiv.org/abs/1711.06605,"Designing soft robots poses considerable challenges: automated design approaches may be particularly appealing in this field, as they promise to optimize complex multi-material machines with very little or no human intervention. Evolutionary soft robotics is concerned with the application of optimization algorithms inspired by natural evolution in order to let soft robots (both morphologies and controllers) spontaneously evolve within physically-realistic simulated environments, figuring out how to satisfy a set of objectives defined by human designers. In this paper a powerful evolutionary system is put in place in order to perform a broad investigation on the free-form evolution of walking and swimming soft robots in different environments. Three sets of experiments are reported, tackling different aspects of the evolution of soft locomotion. The first two sets explore the effects of different material properties on the evolution of terrestrial and aquatic soft locomotion: particularly, we show how different materials lead to the evolution of different morphologies, behaviors, and energy-performance tradeoffs. It is found that within our simplified physics world stiffer robots evolve more sophisticated and effective gaits and morphologies on land, while softer ones tend to perform better in water. The third set of experiments starts investigating the effect and potential benefits of major environmental transitions (land - water) during evolution. Results provide interesting morphological exaptation phenomena, and point out a potential asymmetry between land-water and water-land transitions: while the first type of transition appears to be detrimental, the second one seems to have some beneficial effects. ","Evolving soft locomotion in aquatic and terrestrial environments:
  effects of material properties and environmental transitions"
27,932615009854771201,1408022179,Lia Sartori,"['What happened to @HannysVoorwerp, IC 2497 and its quasar? Have a look at our new @NASANuSTAR paper <LINK> <LINK>']",https://arxiv.org/abs/1711.06270,"We present new Nuclear Spectroscopic Telescope Array (NuSTAR) observations of the core of IC 2497, the galaxy associated with Hanny's Voorwerp. The combined fits of the Chandra (0.5-8 keV) and NuSTAR (3-24 keV) X-ray spectra, together with WISE mid-IR photometry, optical longslit spectroscopy and optical narrow-band imaging, suggest that the galaxy hosts a Compton-thick AGN ($N_{\rm H} \sim 2 \times 10^{24}$ cm$^{-2}$, current intrinsic luminosity $L_{\rm bol} \sim 2-5 \times 10^{44}$ erg s$^{-1}$) whose luminosity dropped by a factor of $\sim$50 within the last $\sim 100$ kyr. This corresponds to a change in Eddington ratio from $\rm \lambda_{Edd} \sim$ 0.35 to $\rm \lambda_{Edd} \sim$ 0.007. We argue that the AGN in IC 2497 should not be classified as a changing-look AGN, but rather we favour the interpretation where the AGN is undergoing a change in accretion state (from radiatively efficient to radiatively inefficient). In this scenario the observed drop in luminosity and Eddington ratio corresponds to the final stage of an AGN accretion phase. Our results are consistent with previous studies in the optical, X-ray and radio although the magnitude of the drop is lower than previously suggested. In addition, we discuss a possible analogy between X-ray binaries and an AGN. ","Joint NuSTAR and Chandra analysis of the obscured quasar in IC 2497 -
  Hanny's Voorwerp system"
28,932557279454412800,794346991627010048,Lauren Oakden-Rayner (Dr.Dr. ü•≥),"['Our new paper: ""Detecting hip fractures with radiologist-level performance using deep neural networks"". \n\n55,000 hip x-rays, big improvement over SOTA, AUC 0.994 (!). Huge team effort, especially from my exceptional co-author William Gale (an undergrad!)\n\n<LINK> <LINK>', ""@AndrewLBeam We discussed this a lot. In the end we decided to pre-print. Many good journals allow pre-prints, just no media. Also, what we do submit will have completely different experiments. Hopefully it doesn't bite us :)"", ""@AndrewLBeam Yeah, it seems to be a medical journal thing. There are other 'big' journals :)"", ""@DrDeclanORegan @DrHughHarvey I'm uncomfortable with cherry picked images in papers which is why there aren't any. The stats speak loudly IMO. But I'm happy to post some up tomorrow, the system does identify really subtle fractures. One that it got right took two CTs and an MRI before dx."", '@alexattia Not yet, we are still working on the project. The dataset is not public right now either, so any attempt to ""re-implement"" is unfortunately doomed. The network itself is just a DenseNet, there are many public implementations.', ""@alexattia We haven't, building a dataset for each new task is a huge effort. I see no reason why it wouldn't be achievable with good data, but that can be hard to achieve with other fractures (patients can be much more easily missed). I do know @enlitic did some work on wrist fractures."", '@alexattia @enlitic In our next experiments we will do some ablation. It depends what we mean by ""reasonable performance"". I think 95 AUC is totally achievable with less data, because the majority of fractures are obvious. Learning to deal with the subtle ones is much harder', '@DrDeclanORegan @DrHughHarvey Here is an example of the minimally displaced ones that it gets right. In the next paper we will include lots of images (it will be more justified with our experiments). https://t.co/MQLo4Cvp4X']",https://arxiv.org/abs/1711.06504v1,"We developed an automated deep learning system to detect hip fractures from frontal pelvic x-rays, an important and common radiological task. Our system was trained on a decade of clinical x-rays (~53,000 studies) and can be applied to clinical data, automatically excluding inappropriate and technically unsatisfactory studies. We demonstrate diagnostic performance equivalent to a human radiologist and an area under the ROC curve of 0.994. Translated to clinical practice, such a system has the potential to increase the efficiency of diagnosis, reduce the need for expensive additional testing, expand access to expert level medical image interpretation, and improve overall patient outcomes. ","] Detecting hip fractures with radiologist-level performance using deep
  neural networks"
29,931468320976592896,1084584086,Giorgio Fagiolo,['Check out my new paper: ‚ÄúIdentifying the community structure of the world food-trade multi-layer network‚Äù <LINK> <LINK>'],https://arxiv.org/abs/1711.05784,"Achieving international food security requires improved understanding of how international trade networks connect countries around the world through the import-export flows of food commodities. The properties of food trade networks are still poorly documented, especially from a multi-network perspective. In particular, nothing is known about the community structure of food networks, which is key to understanding how major disruptions or 'shocks' would impact the global food system. Here we find that the individual layers of this network have densely connected trading groups, a consistent characteristic over the period 2001 to 2011. We also fit econometric models to identify social, economic and geographic factors explaining the probability that any two countries are co-present in the same community. Our estimates indicate that the probability of country pairs belonging to the same food trade community depends more on geopolitical and economic factors -- such as geographical proximity and trade agreements co-membership -- than on country economic size and/or income. This is in sharp contrast with what we know about bilateral-trade determinants and suggests that food country communities behave in ways that can be very different from their non-food counterparts. ","Identifying the community structure of the international food-trade
  multi network"
30,931454797764530178,42413293,James Nichols,['New paper up. Some simple fractional binomial identities yielding interesting results in fractional PDE approximation <LINK> <LINK>'],https://arxiv.org/abs/1711.06197,"A class of discrete time random walks has recently been introduced to provide a stochastic process based numerical scheme for solving fractional order partial differential equations, including the fractional subdiffusion equation. Here we develop a Monte Carlo method for simulating discrete time random walks with Sibuya power law waiting times, providing another approximate solution of the fractional subdiffusion equation. The computation time scales as a power law in the number of time steps with a fractional exponent simply related to the order of the fractional derivative. We also provide an explicit form of a subordinator for discrete time random walks with Sibuya power law waiting times. This subordinator transforms from an operational time, in the expected number of random walk steps, to the physical time, in the number of time steps. ","Subdiffusive discrete time random walks via Monte Carlo and
  subordination"
31,931179465790681088,40299444,Alexey Petrov,"['New paper! We show that it is advantageous to study lepton flavor violation (LFV) with radiative LFV decays of B, D, and even K mesons. Hopefully it will be useful for experimentalists! \n<LINK>\n@WSUResearch @WayneStateCLAS @LHCbPhysics @belle2collab <LINK>']",https://arxiv.org/abs/1711.05314,"We argue that radiative lepton flavor violating (RLFV) decays $P \to \gamma \ell_1 \overline{\ell}_2$ of $P =B^0_q$, $\bar{D}^0$, and $K^0$ meson states are robust probes of new physics models. In particular, they could be used to put constraints on the Wilson coefficients of effective operators describing lepton flavor-changing neutral current interactions at low energy scales. We set up a generic framework for describing these transitions and review new physics constraints from $P \to \ell_1 \bar \ell_2$ decays. There is discussion of how RLFV transitions provide access to the operators that cannot be constrained in two-body decays and we in turn motivate further experimental searches via these channels. ","Radiative lepton flavor violating B, D, and K decays"
32,931069243864567808,2687143885,Johannes Bjerva,"['Excited to have the first paper with my new affiliation @coastalcph, and with @IAugenstein online! Tracking Typological Traits of Uralic Languages in Distributed Language Representations  <LINK> #NLProc']",https://arxiv.org/abs/1711.05468,"Although linguistic typology has a long history, computational approaches have only recently gained popularity. The use of distributed representations in computational linguistics has also become increasingly popular. A recent development is to learn distributed representations of language, such that typologically similar languages are spatially close to one another. Although empirical successes have been shown for such language representations, they have not been subjected to much typological probing. In this paper, we first look at whether this type of language representations are empirically useful for model transfer between Uralic languages in deep neural networks. We then investigate which typological features are encoded in these representations by attempting to predict features in the World Atlas of Language Structures, at various stages of fine-tuning of the representations. We focus on Uralic languages, and find that some typological traits can be automatically inferred with accuracies well above a strong baseline. ","Tracking Typological Traits of Uralic Languages in Distributed Language
  Representations"
33,931007547787456512,2285825876,Johanna,"['#Exoplanet-y friends, check out v2 of our paper w/GJ 9827 planet mass constraints from Magellan/PFS. New &amp; improved! W/help from @sharonxuesong, Angie Wolfgang, &amp; Fei Dai. Read to find out, what do activity and GP analyses show? <LINK>']",https://arxiv.org/abs/1711.01359,"The Kepler mission showed us that planets with sizes between that of Earth and Neptune appear to be the most common type in our Galaxy. These ""super-Earths"" continue to be of great interest for exoplanet formation, evolution, and composition studies. However, the number of super-Earths with well-constrained mass and radius measurements remains small (40 planets with $\sigma_{\rm{mass}}<$ 25\%), due in part to the faintness of their host stars causing ground-based mass measurements to be challenging. Recently, three transiting super-Earth planets were detected by the K2 mission around the nearby star GJ 9827/HIP 115752, at only 30 pc away. The radii of the planets span the ""radius gap""' detected by Fulton et al. (2017), and all orbit within ~6.5 days, easing follow-up observations. Here we report radial velocity (RV) observations of GJ 9827, taken between 2010 and 2016 with the Planet Finder Spectrograph on the Magellan II Telescope. We employ two different RV analysis packages, SYSTEMIC and RadVel, to derive masses and thus densities of the GJ 9827 planets. We also test a Gaussian Process regression analysis, but find the correlated stellar noise is not well constrained by the PFS data, and that the GP tends to over fit the RV semi-amplitudes resulting in a lower K value. Our RV observations are not able to place strong mass constraints on the two outer planets (c & d) but do indicate that planet b, at 1.64 R$_{\oplus}$ and ~8 M$_{\oplus}$, is one of the most massive (and dense) super-Earth planets detected to date. ","Magellan/PFS Radial Velocities of GJ 9827, a late K dwarf at 30 pc with
  Three Transiting Super-Earths"
34,930934702151753729,2800204849,Andrew Gordon Wilson,"['Excited about our new #nips2017 paper, for scalable log determinants &amp; derivatives.\n<LINK>\n+code!\n<LINK>']",https://arxiv.org/abs/1711.03481,"For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an $n \times n$ positive definite matrix, and its derivatives - leading to prohibitive $\mathcal{O}(n^3)$ computations. We propose novel $\mathcal{O}(n)$ approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels. ",Scalable Log Determinants for Gaussian Process Kernel Learning
35,930878942679986177,2235411914,Surya Ganguli,['New paper on resurrecting sigmoids in #deeplearning: <LINK> We use free probability theory to find initializations that make very deep sigmoidal networks learn orders of magnitude faster than ReLUs. Fun collab w/ Jeffrey Pennington &amp; Sam Schoenholz @GoogleBrain'],https://arxiv.org/abs/1711.04735,"It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is $O(1)$ is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near $1$ is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. ","Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice"
36,930853898184773639,2487828968,Zac Kenton,"['Our new paper on SGD, and how the ratio of learning rate to batch size controls geometry of surface explored by SGD <LINK> <LINK>']",http://arxiv.org/abs/1711.04623,"We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process. ",Three Factors Influencing Minima in SGD
37,930778244399366145,156804540,Francisco Rodrigues,['Our new paper on arxiv:  Global teleconnectivity structures of the El Ni√±o-Southern Oscillation and large volcanic eruptions -- An evolving network perspective <LINK>  with @thomas_peron #Physics #climatechange #DataScience @icmc_usp <LINK>'],https://arxiv.org/abs/1711.04670,"Recent work has provided ample evidence that global climate dynamics at time-scales between multiple weeks and several years can be severely affected by the episodic occurrence of both, internal (climatic) and external (non-climatic) perturbations. Here, we aim to improve our understanding on how regional to local disruptions of the ""normal"" state of the global surface air temperature field affect the corresponding global teleconnectivity structure. Specifically, we present an approach to quantify teleconnectivity based on different characteristics of functional climate network analysis. Subsequently, we apply this framework to study the impacts of different phases of the El Ni\~no-Southern Oscillation (ENSO) as well as the three largest volcanic eruptions since the mid 20th century on the dominating spatiotemporal co-variability patterns of daily surface air temperatures. Our results confirm the existence of global effects of ENSO which result in episodic breakdowns of the hierarchical organization of the global temperature field. This is associated with the emergence of strong teleconnections. At more regional scales, similar effects are found after major volcanic eruptions. Taken together, the resulting time-dependent patterns of network connectivity allow a tracing of the spatial extents of the dominating effects of both types of climate disruptions. We discuss possible links between these observations and general aspects of atmospheric circulation. ","Global teleconnectivity structures of the El Ni\~no-Southern Oscillation
  and large volcanic eruptions -- An evolving network perspective"
38,930733631022292994,772809603046334464,Jonathan Mackey,"['New paper out today on @arxiv, well done Dori for getting it finished!\n<LINK>']",https://arxiv.org/abs/1711.04007,"Anomalous surface abundances are observed in a fraction of the low-mass stars of Galactic globular clusters, that may originate from hot-hydrogen-burning products ejected by a previous generation of massive stars. We present and investigate a scenario in which the second generation of polluted low-mass stars can form in shells around cool supergiant stars within a young globular cluster. Simulations of low-Z massive stars (M$_{\rm i}\sim$ 150$-$600 M$_{\odot}$) show that both core-hydrogen-burning cool supergiants and hot ionizing stellar sources are expected to be present simulaneously in young globular clusters. Under these conditions, photoionization-confined shells form around the supergiants. We find that the shell is gravitationally unstable on a timescale that is shorter than the lifetime of the supergiant, and the Bonnor-Ebert mass of the overdense regions is low enough to allow star formation. Since the low-mass stellar generation formed in this shell is made up of the material lost from the supergiant, its composition necessarily reflects the composition of the supergiant wind. We show that the wind contains hot-hydrogen-burning products, and that the shell-stars therefore have very similar abundance anomalies that are observed in the second generation stars of globular clusters. Considering the mass-budget required for the second generation star-formation, we offer two solutions. Either a top-heavy initial mass function is needed with an index of $-$1.71..$-$2.07. Alternatively, we suggest the shell-stars to have a truncated mass distribution, and solve the mass budget problem by justifiably accounting for only a fraction of the first generation. Even without forming a photoionizaton-confined shell, the cool supergiant stars predicted at low-Z could contribute to the pollution of the interstellar medium of the cluster from which the second generation was born. ",Supergiants and their shells in young globular clusters
39,930670141716795395,280403336,Sean Welleck,"['Check out our new paper ""Loss Functions for Multiset Prediction""! <LINK>']",https://arxiv.org/abs/1711.05246,"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others. ",Loss Functions for Multiset Prediction
40,930647809707073536,18850305,Zachary Lipton,"['New paper on deep learning for forecasting stock prices &amp; company fundamentals w @JohnAlberg accepted at NIPS Time Series Workshop!\n<LINK>\n\nStrangely, appears to be first *public* *academic* *paper* applying deep learning to *large-scale* stock market data', ""@intrinsic_motiv @JohnAlberg Yes, thus the carefully placed emphasis on *public* and *academic*! However, likely they're not doing our exact approach :)"", '@filippie509 @hardmaru @JohnAlberg They do run a fund successfully :) but this was a side project! Yes, on backtests it works (read the paper). But the future can always be capricious so maybe?!', '@jackclarkSF @JohnAlberg Three years vs decades for all publicly traded stocks w market cap over 100m. Thus *largescale* in claim', ""@jackclarkSF @JohnAlberg Reading it, it's clear that you didn't! From cursory read it appears that they literally just train on the S&amp;P500 index itself. And just on technical analysis (sorcery). And no real in-sample /out-of-sample split for backtest? Come on, dude!"", ""@jackclarkSF @JohnAlberg I am wary of such claims. That's why I spent several days last summer finding out if anyone had done it. And strangely it does appears to be the first. Note the claim itself expresses my surprise. You ought to be wary about weighing in without (actually) reading the evidence.""]",https://arxiv.org/abs/1711.04837,"On a periodic basis, publicly traded companies are required to report fundamentals: financial data such as revenue, operating income, debt, among others. These data points provide some insight into the financial health of a company. Academic research has identified some factors, i.e. computed features of the reported data, that are known through retrospective analysis to outperform the market average. Two popular factors are the book value normalized by market capitalization (book-to-market) and the operating income normalized by the enterprise value (EBIT/EV). In this paper: we first show through simulation that if we could (clairvoyantly) select stocks using factors calculated on future fundamentals (via oracle), then our portfolios would far outperform a standard factor approach. Motivated by this analysis, we train deep neural networks to forecast future fundamentals based on a trailing 5-years window. Quantitative analysis demonstrates a significant improvement in MSE over a naive strategy. Moreover, in retrospective analysis using an industry-grade stock portfolio simulator (backtester), we show an improvement in compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor model. ","Improving Factor-Based Quantitative Investing by Forecasting Company
  Fundamentals"
41,930631249026322434,907232486735958018,Jaki Noronha-Hostler,['New paper on the @arxiv today studying the connection between the QCD Equation of State and viscosity of the Quark Gluon Plasma <LINK>'],https://arxiv.org/abs/1711.05207,"The QCD equation of state at zero baryon chemical potential is the only element of the standard dynamical framework to describe heavy ion collisions that can be directly determined from first principles. Continuum extrapolated lattice QCD equations of state have been computed using 2+1 quark flavors (up/down and strange) as well as 2+1+1 flavors to investigate the effect of thermalized charm quarks on QCD thermodynamics. Lattice results have also indicated the presence of new strange resonances that not only contribute to the equation of state of QCD matter but also affect hadronic afterburners used to model the later stages of heavy ion collisions. We investigate how these new developments obtained from first principles calculations affect multiparticle correlations in heavy ion collisions. We compare the commonly used equation of state S95n-v1, which was constructed using what are now considered outdated lattice results and hadron states, to the current state-of-the-art lattice QCD equations of state with 2+1 and 2+1+1 flavors coupled to the most up-to-date hadronic resonances and their decays. New hadronic resonances lead to an enhancement in the hadronic spectra at intermediate $p_T$. Using an outdated equation of state can directly affect the extraction of the shear viscosity to entropy density ratio, $\eta/s$, of the quark-gluon plasma and results for different flow observables. The effects of the QCD equation of state on multiparticle correlations of identified particles are determined for both AuAu $\sqrt{s_{NN}}=200$ GeV and PbPb $\sqrt{s_{NN}}=5.02$ TeV collisions. New insights into the $v_2\{2\}$ to $v_3\{2\}$ puzzle in ultracentral collisions are found. Flow observables of heavier particles exhibit more non-linear behavior regardless of the assumptions about the equation of state, which may provide a new way to constrain the temperature dependence of $\eta/s$. ","Effect of the QCD equation of state and strange hadronic resonances on
  multiparticle correlations in heavy ion collisions"
42,930100397972303872,118891923,Nicol√≤ Navarin,"['New paper ""LSTM Networks for Data-Aware Remaining Time Prediction of Business Process Instances""\n<LINK>']",https://arxiv.org/abs/1711.03822,"Predicting the completion time of business process instances would be a very helpful aid when managing processes under service level agreement constraints. The ability to know in advance the trend of running process instances would allow business managers to react in time, in order to prevent delays or undesirable situations. However, making such accurate forecasts is not easy: many factors may influence the required time to complete a process instance. In this paper, we propose an approach based on deep Recurrent Neural Networks (specifically LSTMs) that is able to exploit arbitrary information associated to single events, in order to produce an as-accurate-as-possible prediction of the completion time of running instances. Experiments on real-world datasets confirm the quality of our proposal. ","LSTM Networks for Data-Aware Remaining Time Prediction of Business
  Process Instances"
43,930096915118788608,84043323,Anna Harutyunyan,"['Our new paper is out and accepted to #AAAI2018! \n""Learning with Options that Terminate Off-Policy"" <LINK>']",https://arxiv.org/abs/1711.03817,"A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy exactly, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q(\beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q(\beta) by casting learning with options into a common framework with well-studied multi-step off-policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims. ",Learning with Options that Terminate Off-Policy
44,930069989658329088,1314104323,The Anh Han,['New paper on the distribution of the number of internal equilibria in random  evolutionary games on arxiv: \n<LINK>'],https://arxiv.org/abs/1711.03848,"In this paper, we study the distribution of the number of internal equilibria of a multi-player two-strategy random evolutionary game. Using techniques from the random polynomial theory, we obtain a closed formula for the probability that the game has a certain number of internal equilibria. In addition, by employing Descartes' rule of signs and combinatorial methods, we provide useful estimates for this probability. Finally, we also compare our analytical results with those obtained from samplings. ","On the distribution of the number of internal equilibria in random
  evolutionary games"
45,929993727950680064,1562913787,Nathan Moynihan,['New paper out today! <LINK>'],https://arxiv.org/abs/1711.03956,"Armed with the latest technology in the computation of scattering amplitudes involving massive particles of any spin, we revisit the van Dam-Veltman-Zakharov (vDVZ) discontinuity of massive gravity and show how it may be understood in terms of the Britto-Cachazo-Feng-Witten (BCFW) relations. ","Comments on scattering in massive gravity, vDVZ and BCFW"
46,928828930584330240,2285825876,Johanna,"['Lookie here, bright and shiny new paper all about @sdssurveys-V, led by @thejunaverse! <LINK>', 'TL/DR? Check out the cliff notes here :) https://t.co/zQsg638j0R']",https://arxiv.org/abs/1711.03234,"SDSS-V will be an all-sky, multi-epoch spectroscopic survey of over six million objects. It is designed to decode the history of the Milky Way, trace the emergence of the chemical elements, reveal the inner workings of stars, and investigate the origin of planets. It will also create an integral-field spectroscopic map of the gas in the Galaxy and the Local Group that is 1,000x larger than the current state of the art and at high enough spatial resolution to reveal the self-regulation mechanisms of galactic ecosystems. SDSS-V will pioneer systematic, spectroscopic monitoring across the whole sky, revealing changes on timescales from 20 minutes to 20 years. The survey will thus track the flickers, flares, and radical transformations of the most luminous persistent objects in the universe: massive black holes growing at the centers of galaxies. The scope and flexibility of SDSS-V will be unique among extant and future spectroscopic surveys: it is all-sky, with matched survey infrastructures in both hemispheres; it provides near-IR and optical multi-object fiber spectroscopy that is rapidly reconfigurable to serve high target densities, targets of opportunity, and time-domain monitoring; and it provides optical, ultra-wide-field integral field spectroscopy. SDSS-V, with its programs anticipated to start in 2020, will be well-timed to multiply the scientific output from major space missions (e.g., TESS, Gaia, eROSITA) and ground-based projects. SDSS-V builds on the 25-year heritage of SDSS's advances in data analysis, collaboration infrastructure, and product deliverables. The project is now refining its science scope, optimizing the survey strategies, and developing new hardware that builds on the SDSS-IV infrastructure. We present here an overview of the current state of these developments as we seek to build our worldwide consortium of institutional and individual members. ",SDSS-V: Pioneering Panoptic Spectroscopy
47,928437454364798976,1616352942,Volodymyr Kuleshov üá∫üá¶,['New paper on black-box learning of undirected models using neural variational inference. Also speeds up sampling and helps estimate the partition function. Our #nips2017 paper is online here: <LINK> <LINK>'],https://arxiv.org/abs/1711.02679,"Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets. ",Neural Variational Inference and Learning in Undirected Graphical Models
48,928355113198063617,2816636344,Anirudh Goyal,"['Our new paper! Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. <LINK> . Interested in knowing connections to Neuroscience, may be it could lead to how does brain does BPTT through time?']",https://arxiv.org/abs/1711.02326,"A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic. However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states. ","Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent
  Networks"
49,928345364565958656,1921408860,Kai Sheng Tai,"['Is your stream processing task just classification in disguise? See our new paper: finding heavily-weighted features in linear classifiers with limited memory, w/ applications to online feature selection, streaming explanation, sparse PMI estimation\n<LINK> <LINK>']",https://arxiv.org/abs/1711.02305,"We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the model. This enables memory-limited execution of several statistical analyses over streams, including online feature selection, streaming data explanation, relative deltoid detection, and streaming estimation of pointwise mutual information. Unlike related sketches that capture the most frequently-occurring features (or items) in a data stream, the Weight-Median Sketch captures the features that are most discriminative of one stream (or class) compared to another. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We provide a theoretical analysis that establishes recovery guarantees for batch and online learning, and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods, including count-based sketches and feature hashing. ",Sketching Linear Classifiers over Data Streams
50,928343972531130369,2235411914,Surya Ganguli,"[""Our new paper! Learning asymmetric recurrent neural networks thru variational inference <LINK> w/Yoshua Bengio's group.  #ML #AI""]",https://arxiv.org/abs/1711.02282,"We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems. The proposed training objective, which we derive via principled variational methods, encourages the transition operator to ""walk back"" in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code: this http URL ","Variational Walkback: Learning a Transition Operator as a Stochastic
  Recurrent Net"
51,928265447342067712,76331020,Caius Selhorst,['Our new paper accepted in the ApJ @guiguesp @pjasimoes  <LINK>'],https://arxiv.org/abs/1711.02163,"Radio-bright regions near the solar poles are frequently observed in Nobeyama Radioheliograph (NoRH) maps at 17 GHz, and often in association with coronal holes. However, the origin of these polar brightening has not been established yet. We propose that small magnetic loops are the source of these bright patches, and present modeling results that reproduce the main observational characteristics of the polar brightening within coronal holes at 17 GHz. The simulations were carried out by calculating the radio emission of the small loops, with several temperature and density profiles, within a 2D coronal hole atmospheric model. If located at high latitudes, the size of the simulated bright patches are much smaller than the beam size and they present the instrument beam size when observed. The larger bright patches can be generated by a great number of small magnetic loops unresolved by the NoRH beam. Loop models that reproduce bright patches contain denser and hotter plasma near the upper chromosphere and lower corona. On the other hand, loops with increased plasma density and temperature only in the corona do not contribute to the emission at 17 GHz. This could explain the absence of a one-to-one association between the 17 GHz bright patches and those observed in extreme ultraviolet. Moreover, the emission arising from small magnetic loops located close to the limb may merge with the usual limb brightening profile, increasing its brightness temperature and width. ","Association of radio polar cap brightening with bright patches and
  coronal holes"
52,928201236570148864,2260249807,Rui Ponte Costa üá∫üá¶,"['New paper on cortical microcircuits as gated-recurrent neural networks (LSTMs) @iassael @NandoDF  @TPVogels <LINK> #NIPS17 <LINK>', '@iassael @NandoDF @TPVogels and also w/ @BrendanShilling', ""@ItsNeuronal @iassael @NandoDF @TPVogels Tks! We study 2 options: simple memory decay &amp; network controlled forgetting (as in LSTMs). These make diff. interpretations as for the feedback (local vs non-local). Have to look again at Goldman's models.""]",http://arxiv.org/abs/1711.02448,"Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts. ",Cortical microcircuits as gated-recurrent neural networks
53,928084698873696258,882257115863187457,Sanjeev Arora,"['Our new paper showing that encoder-decoder GAN (Generative Adversarial Net) architectures share some of the shortcomings of simpler GANs, and in fact may be susceptible to learning features that are mere noise. <LINK>']",https://arxiv.org/abs/1711.02651,"Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an inference mechanism to the GANs setup, consisting of a small encoder deep net that maps data-points to their succinct encodings. The intuition is that being forced to train an encoder alongside the usual generator forces the system to learn meaningful mappings from the code to the data-point and vice-versa, which should improve the learning of the target distribution and ameliorate mode-collapse. It should also yield meaningful codes that are useful as features for downstream tasks. The current paper shows rigorously that even on real-life distributions of images, the encode-decoder GAN training objectives (a) cannot prevent mode collapse; i.e. the objective can be near-optimal even when the generated distribution has low and finite support (b) cannot prevent learning meaningless codes for data -- essentially white noise. Thus if encoder-decoder GANs do indeed work then it must be due to reasons as yet not understood, since the training objective can be low even for meaningless solutions. ",Theoretical limitations of Encoder-Decoder GAN architectures
54,928075809922961408,554869994,Yonatan Belinkov,['Breaking neural MT with synthetic and natural noise is fun; fixing it is harder but we have a few ideas. New paper w/ @ybisk  <LINK> #neuralempty #NLProc <LINK>'],https://arxiv.org/abs/1711.02173,"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ",Synthetic and Natural Noise Both Break Neural Machine Translation
55,926250445898682369,841499391508779008,Zico Kolter,"['New paper, with @RICEric22, on provable defenses against adversarial examples.\n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1711.00851,"We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$), and code for all experiments in the paper is available at this https URL ","Provable defenses against adversarial examples via the convex outer
  adversarial polytope"
56,926210736908468224,905159827936075776,Sam Greydanus,['New paper up: Visualizing and Understanding Atari Agents. Lots of fun(?) trying to explain _exactly_ what Atari agents are doing <LINK> <LINK>'],http://arxiv.org/abs/1711.00138,"While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent's decisions and learning behavior. ",Visualizing and Understanding Atari Agents
57,926181279279718400,65876824,Jascha Sohl-Dickstein,['Use trained neural networks without training a neural network! New paper on equivalence of NNs and GPs. <LINK>'],https://arxiv.org/abs/1711.00165,"It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. ",Deep Neural Networks as Gaussian Processes
58,926105639373459456,367297219,Melanie Mitchell,"['New paper from my research group!  ""Semantic Image Retrieval via Active Grounding of Visual Situations""  <LINK>']",https://arxiv.org/abs/1711.00088,"We describe a novel architecture for semantic image retrieval---in particular, retrieval of instances of visual situations. Visual situations are concepts such as ""a boxing match,"" ""walking the dog,"" ""a crowd waiting for a bus,"" or ""a game of ping-pong,"" whose instantiations in images are linked more by their common spatial and semantic structure than by low-level visual similarity. Given a query situation description, our architecture---called Situate---learns models capturing the visual features of expected objects as well the expected spatial configuration of relationships among objects. Given a new image, Situate uses these models in an attempt to ground (i.e., to create a bounding box locating) each expected component of the situation in the image via an active search procedure. Situate uses the resulting grounding to compute a score indicating the degree to which the new image is judged to contain an instance of the situation. Such scores can be used to rank images in a collection as part of a retrieval system. In the preliminary study described here, we demonstrate the promise of this system by comparing Situate's performance with that of two baseline methods, as well as with a related semantic image-retrieval system based on ""scene graphs."" ",Semantic Image Retrieval via Active Grounding of Visual Situations
59,926041764183662594,806058672619212800,Guillaume Lample,['We just released our new paper on Unsupervised Machine Translation. We can now translate languages using monolingual corpora only! <LINK>'],https://arxiv.org/abs/1711.00043,"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time. ",Unsupervised Machine Translation Using Monolingual Corpora Only
60,925952595134259200,14103014,Ben Poole,['New paper on an information-theoretic framework for understanding VAEs! Points to challenges and new directions. <LINK> <LINK>'],https://arxiv.org/abs/1711.00464,"Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code. ",Fixing a Broken ELBO
61,935541922520645632,15496407,"Jason H. Moore, PhD","['Our new paper on ""Benchmarking Relief-Based Feature Selection Methods"" on \n#arXiv <LINK>\n #machinelearning #datascience <LINK>']",https://arxiv.org/abs/1711.08477,"Modern biomedical data mining requires feature selection methods that can (1) be applied to large scale feature spaces (e.g. `omics' data), (2) function in noisy problems, (3) detect complex patterns of association (e.g. gene-gene interactions), (4) be flexibly adapted to various problem domains and data types (e.g. genetic variants, gene expression, and clinical data) and (5) are computationally tractable. To that end, this work examines a set of filter-style feature selection algorithms inspired by the `Relief' algorithm, i.e. Relief-Based algorithms (RBAs). We implement and expand these RBAs in an open source framework called ReBATE (Relief-Based Algorithm Training Environment). We apply a comprehensive genetic simulation study comparing existing RBAs, a proposed RBA called MultiSURF, and other established feature selection methods, over a variety of problems. The results of this study (1) support the assertion that RBAs are particularly flexible, efficient, and powerful feature selection methods that differentiate relevant features having univariate, multivariate, epistatic, or heterogeneous associations, (2) confirm the efficacy of expansions for classification vs. regression, discrete vs. continuous features, missing data, multiple classes, or class imbalance, (3) identify previously unknown limitations of specific RBAs, and (4) suggest that while MultiSURF* performs best for explicitly identifying pure 2-way interactions, MultiSURF yields the most reliable feature selection performance across a wide range of problem types. ","Benchmarking Relief-Based Feature Selection Methods for Bioinformatics
  Data Mining"
62,935541779428007936,710610891058716673,Jan Leike,['Our new paper with RL environments for AI safety problems: <LINK> <LINK>'],https://arxiv.org/abs/1711.09883,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily. ",AI Safety Gridworlds
63,935540850066468870,15496407,"Jason H. Moore, PhD","['Our new paper on ""Relief-Based Feature Selection: Introduction and Review"" on \n#arXiv <LINK>\n #machinelearning #datascience <LINK>']",https://arxiv.org/abs/1711.08421,"Feature selection plays a critical role in biomedical data mining, driven by increasing feature dimensionality in target problems and growing interest in advanced but computationally expensive methodologies able to model complex associations. Specifically, there is a need for feature selection methods that are computationally efficient, yet sensitive to complex patterns of association, e.g. interactions, so that informative features are not mistakenly eliminated prior to downstream modeling. This paper focuses on Relief-based algorithms (RBAs), a unique family of filter-style feature selection algorithms that have gained appeal by striking an effective balance between these objectives while flexibly adapting to various data characteristics, e.g. classification vs. regression. First, this work broadly examines types of feature selection and defines RBAs within that context. Next, we introduce the original Relief algorithm and associated concepts, emphasizing the intuition behind how it works, how feature weights generated by the algorithm can be interpreted, and why it is sensitive to feature interactions without evaluating combinations of features. Lastly, we include an expansive review of RBA methodological research beyond Relief and its popular descendant, ReliefF. In particular, we characterize branches of RBA research, and provide comparative summaries of RBA algorithms including contributions, strategies, functionality, time complexity, adaptation to key data characteristics, and software availability. ",Relief-Based Feature Selection: Introduction and Review
64,935345295105384449,508360150,Peter Henderson,"['Excited to show off our new investigative paper from joint MILA/McGill dialogue group: ""Ethical Challenges in Data-Driven Dialogue Systems"". Would love to hear feedback, so feel free to DM me! Big thanks to all co-authors! <LINK>', 'Supplementary examples, code for experiments, experimental setup, etc. can be found here: https://t.co/cn02FOA3OY']",https://arxiv.org/abs/1711.09050,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems. ",Ethical Challenges in Data-Driven Dialogue Systems
65,932983373848612864,829656407410610176,Jordi Pons,"['New blog entry extending our ML4Audio@NIPS paper: ""Deep end-to-end learning for music audio tagging @PandoraMusic"" - <LINK> \nArXiv reference: <LINK> <LINK>']",https://arxiv.org/abs/1711.02520,"The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models - using waveforms as input with very small convolutional filters; and models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios. ",End-to-end learning for music audio tagging at scale
66,930990907448365057,17916617,Charles Franklin,['This is fun. New GW170608 paper link: <LINK> <LINK>'],https://arxiv.org/abs/1711.05578,"On June 8, 2017 at 02:01:16.49 UTC, a gravitational-wave signal from the merger of two stellar-mass black holes was observed by the two Advanced LIGO detectors with a network signal-to-noise ratio of 13. This system is the lightest black hole binary so far observed, with component masses $12^{+7}_{-2}\,M_\odot$ and $7^{+2}_{-2}\,M_\odot$ (90% credible intervals). These lie in the range of measured black hole masses in low-mass X-ray binaries, thus allowing us to compare black holes detected through gravitational waves with electromagnetic observations. The source's luminosity distance is $340^{+140}_{-140}$ Mpc, corresponding to redshift $0.07^{+0.03}_{-0.03}$. We verify that the signal waveform is consistent with the predictions of general relativity. ",GW170608: Observation of a 19-solar-mass Binary Black Hole Coalescence
67,930855250331803648,503452360,William Wang,"['Are you still using random sampling for negative examples in your embedding models? Our new paper ""KBGAN: Adversarial Learning for Knowledge Graph Embeddings"" introduces a novel method for generating high-quality negative examples. <LINK> #NLProc #DeepLearning <LINK>']",https://arxiv.org/abs/1711.04071,"We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a non-trivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TransE and TransD, each with assistance from one of the two probability-based models, DistMult and ComplEx. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings. ",KBGAN: Adversarial Learning for Knowledge Graph Embeddings
68,927494675279294465,378228706,Hannah Wakeford,['New paper by Nikolay Nikolov up on arXiv today - Measuring the eclipse and thermal emission of HAT-P-32b #exoplanets <LINK> <LINK>'],https://arxiv.org/abs/1711.00859,"We present a thermal emission spectrum of the bloated hot Jupiter HAT-P-32Ab from a single eclipse observation made in spatial scan mode with the Wide Field Camera 3 (WFC3) aboard the Hubble Space Telescope (HST). The spectrum covers the wavelength regime from 1.123 to 1.644 microns which is binned into 14 eclipse depths measured to an averaged precision of 104 parts-per million. The spectrum is unaffected by a dilution from the close M-dwarf companion HAT-P-32B, which was fully resolved. We complemented our spectrum with literature results and performed a comparative forward and retrieval analysis with the 1D radiative-convective ATMO model. Assuming solar abundance of the planet atmosphere, we find that the measured spectrum can best be explained by the spectrum of a blackbody isothermal atmosphere with Tp = 1995 +/- 17K, but can equally-well be described by a spectrum with modest thermal inversion. The retrieved spectrum suggests emission from VO at the WFC3 wavelengths and no evidence of the 1.4 micron water feature. The emission models with temperature profiles decreasing with height are rejected at a high confidence. An isothermal or inverted spectrum can imply a clear atmosphere with an absorber, a dusty cloud deck or a combination of both. We find that the planet can have continuum of values for the albedo and recirculation, ranging from high albedo and poor recirculation to low albedo and efficient recirculation. Optical spectroscopy of the planet's day-side or thermal emission phase curves can potentially resolve the current albedo with recirculation degeneracy. ","Hubble PanCET: An isothermal day-side atmosphere for the bloated
  gas-giant HAT-P-32Ab"
69,926097202333798401,2444302555,Ludovic Denoyer,"['Our student Guillaume (with Marc Aurelio Ranzato and myself) has a new paper about unsupervised machine translation  <LINK> <LINK>', 'The method gets a BLEU of ~15 on En-Fr, and ~13 on De-En on WMT (and  even more than 30 on Multi30k-Task1 En-Fr), which is very impressive.', 'On  WMT, 10M monolingual sentences give equivalent translation  results than  100K parallel sentences using the same underlying model.']",https://arxiv.org/abs/1711.00043,"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time. ",Unsupervised Machine Translation Using Monolingual Corpora Only
70,936661302486777856,1524366572,Dieter Lukas,"['We are excited that we have a preprint of our study on ‚ÄúWomen‚Äôs visibility in academic seminars‚Äù <LINK> on which we welcome comments. A big thank you to everyone who took our survey/reported observations! <LINK>', '@alecia_carter @AlyssaCroftUBC @GillianSocial Some results from our preprint https://t.co/2FQpCsFiPY : Male attendees at academic seminars were over two and half times more likely to ask a question than women attendees (similar to what has been reported for conferences e.g. https://t.co/tMHtU3DBeV)', '@alecia_carter @AlyssaCroftUBC @GillianSocial Some results from our preprint https://t.co/2FQpCsFiPY : women reported more frequently than men that they believed that such a bias exists (see also https://t.co/fi9JAbyptt)', '@alecia_carter @AlyssaCroftUBC @GillianSocial Some results from our preprint https://t.co/2FQpCsFiPY: while not the focus of our study, only 40% of speakers at academic seminars are women (advice for seminar organizers on how to balance speaker lineup on page 2 https://t.co/vAiGk0lFwU by @HannahMRowland &amp; me)', ""@catherinelinnen @alecia_carter @AlyssaCroftUBC @GillianSocial @HannahMRowland Thanks - and sorry to hear about your experience. In our observations, we only counted who asked questions but not who wanted to ask questions - so we can't say how much biases in person calling questions contributes to pattern""]",https://arxiv.org/abs/1711.10985,"The attrition of women in academic careers is a major concern, particularly in Science, Technology, Engineering, and Mathematics subjects. One factor that can contribute to the attrition is the lack of visible role models for women in academia. At early career stages, the behaviour of the local community may play a formative role in identifying ingroup role models, shaping women's impressions of whether or not they can be successful in academia. One common and formative setting to observe role models is the local departmental academic seminar, talk, or presentation. We thus quantified women's visibility through the question-asking behaviour of academics at seminars using observations and an online survey. From the survey responses of over 600 academics in 20 countries, we found that women reported asking fewer questions after seminars compared to men. This impression was supported by observational data from almost 250 seminars in 10 countries: women audience members asked absolutely and proportionally fewer questions than male audience members. When asked why they did not ask questions when they wanted to, women, more than men, endorsed internal factors (e.g., not working up the nerve). However, our observations suggest that structural factors might also play a role; when a man was the first to ask a question, or there were fewer questions, women asked proportionally fewer questions. Attempts to counteract the latter effect by manipulating the time for questions (in an effort to provoke more questions) in two departments were unsuccessful. We propose alternative recommendations for creating an environment that makes everyone feel more comfortable to ask questions, thus promoting equal visibility for women and members of other less visible groups. ","Women's visibility in academic seminars: women ask fewer questions than
  men"
71,936257709749522432,242066463,I√±aki Ugarte,['A first for me. Heard about this X flare first on twitter. Went outside to observe it through a small telescope and even took a phone picture (below). Then we studied it with state-of-the-art space instrumentation and wrote this paper: <LINK> <LINK>'],http://arxiv.org/abs/1711.10826,"We report on the structure and evolution of a current sheet that formed in the wake of an eruptive X8.3 flare observed at the west limb of the Sun on September 10, 2017. Using observations from the EUV Imaging Spectrometer (EIS) on Hinode and the Atmospheric Imaging Assembly (AIA) on the Solar Dynamics Observatory (SDO), we find that plasma in the current sheet reaches temperatures of about 20 MK and that the range of temperatures is relatively narrow. The highest temperatures occur at the base of the current sheet, in the region near the top of the post-flare loop arcade. The broadest high temperature line profiles, in contrast, occur at the largest observed heights. Further, line broadening is strong very early in the flare and diminishes over time. The current sheet can be observed in the AIA 211 and 171 channels, which have a considerable contribution from thermal bremsstrahlung at flare temperatures. Comparisons of the emission measure in these channels with other EIS wavelengths and AIA channels dominated by Fe line emission indicate a coronal composition and suggest that the current sheet is formed by the heating of plasma already in the corona. Taken together, these observations suggest that some flare heating occurs in the current sheet while additional energy is released as newly reconnected field lines relax and become more dipolar. ",Spectroscopic Observations of Current Sheet Formation and Evolution
72,935358406675828736,1439446945,Lav Varshney,"['New manuscript on a kind of constrained code we call ""skip-sliding window codes"" <LINK> and find some counterintuitive performance phenomena @CSL_Illinois @ECEILLINOIS']",https://arxiv.org/abs/1711.09494,"Constrained coding is used widely in digital communication and storage systems. In this paper, we study a generalized sliding window constraint called the skip-sliding window. A skip-sliding window (SSW) code is defined in terms of the length $L$ of a sliding window, skip length $J$, and cost constraint $E$ in each sliding window. Each valid codeword of length $L + kJ$ is determined by $k+1$ windows of length $L$ where window $i$ starts at $(iJ + 1)$th symbol for all non-negative integers $i$ such that $i \leq k$; and the cost constraint $E$ in each window must be satisfied. In this work, two methods are given to enumerate the size of SSW codes and further refinements are made to reduce the enumeration complexity. Using the proposed enumeration methods, the noiseless capacity of binary SSW codes is determined and observations such as greater capacity than other classes of codes are made. Moreover, some noisy capacity bounds are given. SSW coding constraints arise in various applications including simultaneous energy and information transfer. ",Skip-Sliding Window Codes
73,931179465790681088,40299444,Alexey Petrov,"['New paper! We show that it is advantageous to study lepton flavor violation (LFV) with radiative LFV decays of B, D, and even K mesons. Hopefully it will be useful for experimentalists! \n<LINK>\n@WSUResearch @WayneStateCLAS @LHCbPhysics @belle2collab <LINK>']",https://arxiv.org/abs/1711.05314,"We argue that radiative lepton flavor violating (RLFV) decays $P \to \gamma \ell_1 \overline{\ell}_2$ of $P =B^0_q$, $\bar{D}^0$, and $K^0$ meson states are robust probes of new physics models. In particular, they could be used to put constraints on the Wilson coefficients of effective operators describing lepton flavor-changing neutral current interactions at low energy scales. We set up a generic framework for describing these transitions and review new physics constraints from $P \to \ell_1 \bar \ell_2$ decays. There is discussion of how RLFV transitions provide access to the operators that cannot be constrained in two-body decays and we in turn motivate further experimental searches via these channels. ","Radiative lepton flavor violating B, D, and K decays"
74,930878942679986177,2235411914,Surya Ganguli,['New paper on resurrecting sigmoids in #deeplearning: <LINK> We use free probability theory to find initializations that make very deep sigmoidal networks learn orders of magnitude faster than ReLUs. Fun collab w/ Jeffrey Pennington &amp; Sam Schoenholz @GoogleBrain'],https://arxiv.org/abs/1711.04735,"It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is $O(1)$ is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near $1$ is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. ","Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice"
75,928300535756636160,888216099757490176,Maithra Raghu,"['First foray into Deep RL <LINK> We test on a game with continuously tuneable difficulty and *known* optimal policy. We study different RL algorithms, supervised learning, and multiagent play. @jacobandreas <LINK>']",https://arxiv.org/abs/1711.02301,"Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm. Code can be found at: this https URL ",Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?
76,940571109770088448,907232486735958018,Jaki Noronha-Hostler,"['We have predictions for the recent @CERN @uslhc XeXe run comparing both spherical and deformed Xenon.  We find that the deformation plays a role in central collisions elliptical flow.\n<LINK>', '@CERN @uslhc For these calculations we used a relativistic viscous hydrodynamic model with event-by-event TRENTO initial conditions.  We also found that multi-particle cumulants differed more from their eccentricities in XeXe collisions (compared to PbPb collisions).', ""@CERN @uslhc The reason XeXe was interesting  to compare to PbPb is that we're trying to understand how system size affects the Quark Gluon Plasma.  Xe has a smaller nucleus (129 vs 208 from Pb) so it produces a smaller droplet of Quark Gluon Plasma.""]",https://arxiv.org/abs/1711.08499,"We argue that relativistic hydrodynamics is able to make robust predictions for soft particle production in Xe+Xe collisions at the CERN Large Hadron Collider (LHC). The change of system size from Pb+Pb to Xe+Xe provides a unique opportunity to test the scaling laws inherent to fluid dynamics. Using event-by-event hydrodynamic simulations, we make quantitative predictions for several observables: mean transverse momentum, anisotropic flow coefficients, and their fluctuations. Results are shown as function of collision centrality. ",Hydrodynamic predictions for 5.44 TeV Xe+Xe collisions
