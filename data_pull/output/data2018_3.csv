,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,984833148965007360,30420963,Eivind Eriksen,['A new paper is out:\xa0Graded Holonomic D-modules on Monomial Curves <LINK> <LINK>'],https://arxiv.org/abs/1803.04367,"In this paper, we study the holonomic $D$-modules when $D$ is the ring of $k$-linear differential operators on $A = k[\Gamma]$, the coordinate ring of an affine monomial curve over the complex numbers $k = \mathbb C$. In particular, we consider the graded case, and classify the simple graded $D$-modules and compute their extensions. The classification over the first Weyl algebra $D = A_1(k)$ is obtained as a special case. ",Graded Holonomic D-modules on Monomial Curves
1,982975605779173376,117962824,Alex Leow,['Thank you our new Portuguese friends @Publico ! The referenced KDD paper is now on arxiv <LINK> <LINK>'],https://arxiv.org/abs/1803.08986,"The increasing use of electronic forms of communication presents new opportunities in the study of mental health, including the ability to investigate the manifestations of psychiatric diseases unobtrusively and in the setting of patients' daily lives. A pilot study to explore the possible connections between bipolar affective disorder and mobile phone usage was conducted. In this study, participants were provided a mobile phone to use as their primary phone. This phone was loaded with a custom keyboard that collected metadata consisting of keypress entry time and accelerometer movement. Individual character data with the exceptions of the backspace key and space bar were not collected due to privacy concerns. We propose an end-to-end deep architecture based on late fusion, named DeepMood, to model the multi-view metadata for the prediction of mood scores. Experimental results show that 90.31% prediction accuracy on the depression score can be achieved based on session-level mobile phone typing dynamics which is typically less than one minute. It demonstrates the feasibility of using mobile phone metadata to infer mood disturbance and severity. ",DeepMood: Modeling Mobile Phone Typing Dynamics for Mood Detection
2,981634612802801664,97018559,Mariano Chouza,"['New paper in the arXiv: ""Gradient descent in Gaussian random fields as a toy model for high-dimensional optimisation in deep learning"" - <LINK>']",https://arxiv.org/abs/1803.09119,"In this paper we model the loss function of high-dimensional optimization problems by a Gaussian random field, or equivalently a Gaussian process. Our aim is to study gradient descent in such loss functions or energy landscapes and compare it to results obtained from real high-dimensional optimization problems such as encountered in deep learning. In particular, we analyze the distribution of the improved loss function after a step of gradient descent, provide analytic expressions for the moments as well as prove asymptotic normality as the dimension of the parameter space becomes large. Moreover, we compare this with the expectation of the global minimum of the landscape obtained by means of the Euler characteristic of excursion sets. Besides complementing our analytical findings with numerical results from simulated Gaussian random fields, we also compare it to loss functions obtained from optimisation problems on synthetic and real data sets by proposing a ""black box"" random field toy-model for a deep neural network loss function. ","Gradient descent in Gaussian random fields as a toy model for
  high-dimensional optimisation in deep learning"
3,981194149780246529,523241142,Juste Raimbault,['New paper: Co-evolution and morphogenetic systems <LINK>'],https://arxiv.org/abs/1803.11457,"The emerging field of morphogenetic engineering proposes to design complex heterogeneous system focused on the paradigm of emergence. Necessarily at the interface of disciplines, its concepts can be defined through multiple viewpoints. This contribution aims at linking a co-evolutionary perspective on such systems with morphogenesis, and therein at bringing a novel conceptual approach to the bottom-up design of complex systems which allows to fully consider co-evolutive processes. We first situate systems of interest at the interface between biological and social systems, and introduce a multidisciplinary perspective on co-evolution. Building on Holland's signals and boundaries theory of complex adaptive systems, we finally suggest that morphogenetic systems are equivalent to combinations of co-evolutionary niches. This introduces an entry to morphogenetic engineering focused on co-evolution between components of a system. Applications can be found in a broad range of subjects, which we illustrate with the example of planning in territorial systems, suggesting an extended scope for the relevance of morphogenetic engineering concepts. ",Co-evolution and morphogenetic systems
4,981152469085511680,3084456430,Burkhard Morgenstern,"['Our new paper on @arxiv describing our software @MultiSpaM: alignment-free phylogeny reconstruction using multiple spaced-word matches, Maximum Likelihood and quartet trees:  <LINK>']",https://arxiv.org/abs/1803.09222,"Motivation: Word-based or `alignment-free' methods for phylogeny reconstruction are much faster than traditional approaches, but they are generally less accurate. Most of these methods calculate pairwise distances for a set of input sequences, for example from word frequencies, from so-called spaced-word matches or from the average length of common substrings. Results: In this paper, we propose the first word-based approach to tree reconstruction that is based on multiple sequence comparison and Maximum Likelihood. Our algorithm first samples small, gap-free alignments involving four taxa each. For each of these alignments, it then calculates a quartet tree and, finally, the program Quartet MaxCut is used to infer a super tree topology for the full set of input taxa from the calculated quartet trees. Experimental results show that trees calculated with our approach are of high quality. Availability: The source code of the program is available at this https URL Contact: thomas.dencker@stud.uni-goettingen.de ","Multi-SpaM: a Maximum-Likelihood approach to Phylogeny reconstruction
  based on Multiple Spaced-Word Matches"
5,981058409263968256,1710697381,Diego F. Torres,"['Linking here to our new comprehensive paper on an old (and interesting)  pulsar/PWN friend (to appear in ApJ) ""Observing and modeling the gamma-ray emission from pulsar/pulsar wind  nebula complex PSR J0205+6449/3C 58"" <LINK> <LINK>']",https://arxiv.org/abs/1803.10863,"We present the results of the analysis of 8 years of Fermi-LAT data of the pulsar/pulsar wind nebula complex PSR J0205+6449/3C 58. Using a contemporaneous ephemeris, we carried out a detailed analysis of PSR J0205+6449 both during its off-peak and on-peak phase intervals. 3C 58 is significantly detected during the off-peak phase interval. We show that the spectral energy distribution at high energies is the same disregarding the phases considered, and thus that this part of the spectrum is most likely dominated by the nebula radiation. We present results of theoretical models of the nebula and the magnetospheric emission that confirm this interpretation. Possible high-energy flares from 3C 58 were searched for, but none was unambiguously identified. ","Observing and modeling the gamma-ray emission from pulsar/pulsar wind
  nebula complex PSR J0205+6449/3C 58"
6,980837701959274496,958886118879125504,Royon Group,['Congrats Cristian on this fantastic new paper about axion searches at the LHC\n<LINK>\n\n#LHC #axion #science'],https://arxiv.org/abs/1803.10835,"The existence of an axion-like particle (ALP) would induce anomalous scattering of light by light. This process can be probed at the Large Hadron Collider in central exclusive production of photon pairs in proton-proton collisions by tagging the surviving protons using forward proton detectors. Using a detailed simulation, we estimate the expected bounds on the ALP--photon coupling for a wide range of masses. We show that the proposed search is competitive and complementary to other collider bounds for masses above 600 GeV, especially for resonant ALP production between 600 GeV and 2 TeV. Our results are also valid for a CP-even scalar, and the efficiency of the search is independent of the width of the ALP. ",Searching for axion-like particles with proton tagging at the LHC
7,980754967660294145,958312958593064961,Mikayel Samvelyan,"[""I'm excited to share the preprint of our new paper ‚ÄúQMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning‚Äù <LINK>. Very fortunate for the chance to work with such talented people at @whi_rl : @j_foerst @greg_far @shimon8282""]",http://arxiv.org/abs/1803.11485,"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods. ","QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent
  Reinforcement Learning"
8,980728309922385921,721931072,Shimon Whiteson,['Our latest paper: how to learn complex joint value functions for teams of agents whose greedy policies can be computed and executed in a decentralised fashion.  The trick is a new monotonic value function factorisation.  With results on StartCraft 2! <LINK>'],https://arxiv.org/abs/1803.11485,"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods. ","QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent
  Reinforcement Learning"
9,979761588927324160,40737859,Aleksandra Korolova,"['New paper on microtargeting, private info inference, and precise geo-targeting attacks feasible using Facebook‚Äôs advertising system: <LINK>. @faizulla_boy &amp; I discuss Facebook‚Äôs responses to disclosures (2 months!) &amp; call for radical transparency &amp; more research.', ""I first identified and pointed out to Facebook the privacy risks of microtargeting using Facebook's ad platform in 2010: https://t.co/nPK28EPd48. 7+ years later, it‚Äôs still an issue with no scientific innovation on protections from Facebook: https://t.co/yZLN9Xs966""]",https://arxiv.org/abs/1803.10099,"Ad targeting is getting more powerful with introduction of new tools, such as Custom Audiences, behavioral targeting, and Audience Insights. Although this is beneficial for businesses as it enables people to receive more relevant advertising, the power of the tools has downsides. In this paper, we focus on three downsides: privacy violations, microtargeting (i.e., the ability to reach a specific individual or individuals without their explicit knowledge that they are the only ones an ad reaches) and ease of reaching marginalized groups. Using Facebook's ad system as a case study, we demonstrate the feasibility of such downsides. We then discuss Facebook's response to our responsible disclosures of the findings and call for additional policy, science, and engineering work to protect consumers in the rapidly evolving ecosystem of ad targeting. ","Facebook's Advertising Platform: New Attack Vectors and the Need for
  Interventions"
10,979689768253513729,19357166,Brian Alexander Lee,['New paper and #opensource tool on sentence encoding. #NLP  <LINK>'],https://arxiv.org/abs/1803.11175,"We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub. ",Universal Sentence Encoder
11,979687710511304706,14556945,Yves-A. de Montjoye,"['New @imperialcollege CPG preprint: ""Cambridge Analytica is only the beginning and you might have your friends to blame for it"" <LINK> Paper: <LINK> -- In short: Privacy is a network effect and mass surveillance is cheap']",https://arxiv.org/abs/1803.09007,"From the ""right to be left alone"" to the ""right to selective disclosure"", privacy has long been thought as the control individuals have over the information they share and reveal about themselves. However, in a world that is more connected than ever, the choices of the people we interact with increasingly affect our privacy. This forces us to rethink our definition of privacy. We here formalize and study, as local and global node- and edge-observability, Bloustein's concept of group privacy. We prove edge-observability to be independent of the graph structure, while node-observability depends only on the degree distribution of the graph. We show on synthetic datasets that, for attacks spanning several hops such as those implemented by social networks and current US laws, the presence of hubs increases node-observability while a high clustering coefficient decreases it, at fixed density. We then study the edge-observability of a large real-world mobile phone dataset over a month and show that, even under the restricted two-hops rule, compromising as little as 1% of the nodes leads to observing up to 46% of all communications in the network. More worrisome, we also show that on average 36\% of each person's communications would be locally edge-observable under the same rule. Finally, we use real sensing data to show how people living in cities are vulnerable to distributed node-observability attacks. Using a smartphone app to compromise 1\% of the population, an attacker could monitor the location of more than half of London's population. Taken together, our results show that the current individual-centric approach to privacy and data protection does not encompass the realities of modern life. This makes us---as a society---vulnerable to large-scale surveillance attacks which we need to develop protections against. ","Quantifying Surveillance in the Networked Age: Node-based Intrusions and
  Group Privacy"
12,979575312877932544,3290170484,Jarrod McClean,"['Deep NN‚Äôs struggled for some time, in part, due to vanishing gradient problems. Connections to quantum circuits offer a warning to not to repeat history for quantum circuits. Read more in our new paper fresh on the arxiv! <LINK>', '@dabacon @quantumVerd Agree with everything being said! I believe this problem can be avoided, but only if you know to look out for it. As Dave mentioned it‚Äôs shockingly easy to accidentally get stranded on one of these plateaus.']",https://arxiv.org/abs/1803.11173v1,"Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied. ",] Barren plateaus in quantum neural network training landscapes
13,979522070223626241,2971679320,Matt Taylor,"['New paper out on the arXiv! You want dwarf galaxies? We got your dwarf galaxies. How does &gt;600 in the Fornax cluster sound? <LINK> <LINK>', ""@KarinaVoggel Fear not, all is on the way! Nuclear star cluster paper will be on arXiv next week, and working on u'/g' for whole sample."", ""@PatrickDurrell Thanks! We've been looking forward to getting this one out there. :)""]",https://arxiv.org/abs/1803.10784,"We report the discovery of 271 previously undetected dwarf galaxies in the outer Fornax cluster regions at radii $r_{\rm vir}/4<\!r\!<r_{\rm vir}/2$ using data from the Next Generation Fornax Survey (NGFS) with deep coadded $u'$, $g'$ and $i'$ images obtained with Blanco/DECam at Cerro Tololo Interamerican Observatory. From the 271 dwarf candidates we find 39 to be nucleated. Together with our previous study of the central Fornax region, the new dwarfs detected with NGFS data are 392, of which 56 are nucleated. The total Fornax dwarf galaxy population from NGFS and other catalogs rises, therefore, to a total of 643 with 181 being nucleated, yielding an overall nucleation fraction of $28\%$. The absolute $i'$-band magnitudes for the outer NGFS dwarfs are in the range $-18.80\le\,M_{i'}\le\,-8.78$ with effective radii $r_{{\rm eff},i'}\,=\,0.18-2.22$ kpc and an average Sersic index $\langle n\rangle_{i'}\,=\,0.81$. Non-nucleated dwarfs are found to be fainter and smaller by $\Delta\langle M_{i'}\rangle\!=\!2.25$ mag and $\Delta\langle r_{{\rm eff},i'}\rangle\!=\!0.4$ kpc than the nucleated dwarfs. We demonstrate a significant clustering of dwarf galaxies on scales $\lesssim\!100$ kpc, and projected surface number density profile estimates, $\Sigma_N(r)$, show a concentration of dwarfs in the Fornax core region within $r\!\sim\!350$ kpc. $\Sigma_N(r)$ has a flat distribution up to $\sim\!350$ kpc, beyond which it declines for the non-nucleated dwarfs. The nucleated dwarfs have a steeper $\Sigma_N(r)$ distribution, being more concentrated towards NGC 1399 and decreasing rapidly outwards. This is the first time the transition from cluster to field environment has been established for the very faint dwarf galaxy population with robust sample statistics. ","The Next Generation Fornax Survey (NGFS): III. Revealing the Spatial
  Substructure of the Dwarf Galaxy Population inside half of Fornax's Virial
  Radius"
14,979521847397142528,549460404,ÂêâÁî∞ Á¥Ö (Beni Yoshida),['A new paper with Norman Yao. \n<LINK> <LINK>'],https://arxiv.org/abs/1803.10772,"Out-of-time-order correlation (OTOC) functions provide a powerful theoretical tool for diagnosing chaos and the scrambling of information in strongly-interacting, quantum systems. However, their direct and unambiguous experimental measurement remains an essential challenge. At its core, this challenge arises from the fact that the effects of both decoherence and experimental noise can mimic that of information scrambling, leading to decay of OTOCs. Here, we analyze a quantum teleportation protocol that explicitly enables one to differentiate between scrambling and decoherence. Moreover, we demonstrate that within this protocol, one can extract a precise ""noise"" parameter which quantitatively captures the non-scrambling induced decay of OTOCs. Using this parameter, we prove explicit bounds on the true value of the OTOC. Our results open the door to experimentally measuring quantum scrambling with built-in verifiability. ",Disentangling Scrambling and Decoherence via Quantum Teleportation
15,979423691044892674,1158874141,Ben Roberts,['New paper: Search for transient ultralight dark matter signatures with networks of precision measurement devices [1803.10264]\n<LINK>'],https://arxiv.org/abs/1803.10264,"We analyze the prospects of employing a distributed global network of precision measurement devices as a dark matter and exotic physics observatory. In particular, we consider the atomic clocks of the Global Positioning System (GPS), consisting of a constellation of 32 medium-Earth orbit satellites equipped with either Cs or Rb microwave clocks and a number of Earth-based receiver stations, some of which employ highly-stable H-maser atomic clocks. High-accuracy timing data is available for almost two decades. By analyzing the satellite and terrestrial atomic clock data, it is possible to search for transient signatures of exotic physics, such as ""clumpy"" dark matter and dark energy, effectively transforming the GPS constellation into a 50,000km aperture sensor array. Here we characterize the noise of the GPS satellite atomic clocks, describe the search method based on Bayesian statistics, and test the method using simulated clock data. We present the projected discovery reach using our method, and demonstrate that it can surpass the existing constrains by several order of magnitude for certain models. Our method is not limited in scope to GPS or atomic clock networks, and can also be applied to other networks of precision measurement devices. ","Search for transient ultralight dark matter signatures with networks of
  precision measurement devices using a Bayesian statistics method"
16,979010488611868675,3276698761,Albert S. Berahas,['New paper with Profs Richard Byrd and Jorge Nocedal proposing a finite difference quasi-Newton method for minimizing noisy functions in the DFO setting. <LINK>'],https://arxiv.org/abs/1803.10173,"This paper presents a finite difference quasi-Newton method for the minimization of noisy functions. The method takes advantage of the scalability and power of BFGS updating, and employs an adaptive procedure for choosing the differencing interval $h$ based on the noise estimation techniques of Hamming (2012) and Mor\'e and Wild (2011). This noise estimation procedure and the selection of $h$ are inexpensive but not always accurate, and to prevent failures the algorithm incorporates a recovery mechanism that takes appropriate action in the case when the line search procedure is unable to produce an acceptable point. A novel convergence analysis is presented that considers the effect of a noisy line search procedure. Numerical experiments comparing the method to a function interpolating trust region method are presented. ",Derivative-Free Optimization of Noisy Functions via Quasi-Newton Methods
17,978965561689411584,864555701783474179,julesh,"['New preprint: The algebra of predicting agents, with Joe Bolt and Viktor Winschel\n<LINK>\nThis is sort of a negative results paper, and it could benefit from doubling the page limit, but it does contain some pretty diagrams.']",https://arxiv.org/abs/1803.10131,"The category of open games, which provides a strongly compositional foundation of economic game theory, is intermediate between symmetric monoidal and compact closed. More precisely it has counits with no corresponding units, and a partially defined duality. There exist open games with the same types as unit maps, given by agents with the strategic goal of predicting a future value. Such agents appear in earlier work on selection functions. We explore the algebraic properties of these agents via the symmetric monoidal bicategory whose 2-cells are morphisms between open games, and show how the resulting structure approximates a compact closed category with a family of lax commutative bialgebras. ",The algebra of predicting agents
18,978941181882261504,96779364,Arnab Bhattacharyya,"['New paper (<LINK> ) showing that the so-called ""Even Set"" problem (aka, finding the minimum distance of binary linear codes) is W[1]-hard (assuming Gap-ETH). I\'d been obsessed over this problem the last few years, so really satisfying to get a solid result!']",https://arxiv.org/abs/1803.09717,"The $k$-Even Set problem is a parameterized variant of the Minimum Distance Problem of linear codes over $\mathbb F_2$, which can be stated as follows: given a generator matrix $\mathbf A$ and an integer $k$, determine whether the code generated by $\mathbf A$ has distance at most $k$. Here, $k$ is the parameter of the problem. The question of whether $k$-Even Set is fixed parameter tractable (FPT) has been repeatedly raised in literature and has earned its place in Downey and Fellows' book (2013) as one of the ""most infamous"" open problems in the field of Parameterized Complexity. In this work, we show that $k$-Even Set does not admit FPT algorithms under the (randomized) Gap Exponential Time Hypothesis (Gap-ETH) [Dinur'16, Manurangsi-Raghavendra'16]. In fact, our result rules out not only exact FPT algorithms, but also any constant factor FPT approximation algorithms for the problem. Furthermore, our result holds even under the following weaker assumption, which is also known as the Parameterized Inapproximability Hypothesis (PIH) [Lokshtanov et al.'17]: no (randomized) FPT algorithm can distinguish a satisfiable 2CSP instance from one which is only $0.99$-satisfiable (where the parameter is the number of variables). We also consider the parameterized $k$-Shortest Vector Problem (SVP), in which we are given a lattice whose basis vectors are integral and an integer $k$, and the goal is to determine whether the norm of the shortest vector (in the $\ell_p$ norm for some fixed $p$) is at most $k$. Similar to $k$-Even Set, this problem is also a long-standing open problem in the field of Parameterized Complexity. We show that, for any $p > 1$, $k$-SVP is hard to approximate (in FPT time) to some constant factor, assuming PIH. Furthermore, for the case of $p = 2$, the inapproximability factor can be amplified to any constant. ","Parameterized Intractability of Even Set and Shortest Vector Problem
  from Gap-ETH"
19,978899474440474624,16674284,Mirco Musolesi,"['New paper about user identification from metadata accepted at #ICWSM2018: ""You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information‚Äù. The pre-print is now available on arXiv: <LINK> \\w @bmpmila and @gianluca_string <LINK>']",https://arxiv.org/abs/1803.10133,"Metadata are associated to most of the information we produce in our daily interactions and communication in the digital world. Yet, surprisingly, metadata are often still catergorized as non-sensitive. Indeed, in the past, researchers and practitioners have mainly focused on the problem of the identification of a user from the content of a message. In this paper, we use Twitter as a case study to quantify the uniqueness of the association between metadata and user identity and to understand the effectiveness of potential obfuscation strategies. More specifically, we analyze atomic fields in the metadata and systematically combine them in an effort to classify new tweets as belonging to an account using different machine learning algorithms of increasing complexity. We demonstrate that through the application of a supervised learning algorithm, we are able to identify any user in a group of 10,000 with approximately 96.7% accuracy. Moreover, if we broaden the scope of our search and consider the 10 most likely candidates we increase the accuracy of the model to 99.22%. We also found that data obfuscation is hard and ineffective for this type of data: even after perturbing 60% of the training data, it is still possible to classify users with an accuracy higher than 95%. These results have strong implications in terms of the design of metadata obfuscation strategies, for example for data set release, not only for Twitter, but, more generally, for most social media platforms. ","You are your Metadata: Identification and Obfuscation of Social Media
  Users using Metadata Information"
20,978896315940732928,890646669460635648,Alexander Ecker,['Our new paper on one-shot segmentation in cluttered environments is on arXiv: <LINK>. Work with @clmich and @MatthiasBethge. Check it out and let us know your thoughts!'],https://arxiv.org/abs/1803.09597,"We tackle the problem of one-shot segmentation: finding and segmenting a previously unseen object in a cluttered scene based on a single instruction example. We propose a novel dataset, which we call $\textit{cluttered Omniglot}$. Using a baseline architecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task progressively harder. Using oracle models with access to various amounts of ground-truth information, we evaluate different aspects of the problem and show that in this kind of visual search task, detection and segmentation are two intertwined problems, the solution to each of which helps solving the other. We therefore introduce $\textit{MaskNet}$, an improved model that attends to multiple candidate locations, generates segmentation proposals to mask out background clutter and selects among the segmented objects. Our findings suggest that such image recognition models based on an iterative refinement of object detection and foreground segmentation may provide a way to deal with highly cluttered scenes. ",One-Shot Segmentation in Clutter
21,978823938993205248,3439990697,Kosta Derpanis in Toronto,"['Our new #ComputerVision #DeepLearning work on video prediction\n\nAndrew Jaegle (@drew_jaegle), Oleh Rybkin, Konstantinos G. Derpanis, Kostas Daniilidis, Predicting the Future with Transformational States\n\nPaper: <LINK>\nProject page: <LINK> <LINK>']",http://arxiv.org/abs/1803.09760,"An intelligent observer looks at the world and sees not only what is, but what is moving and what can be moved. In other words, the observer sees how the present state of the world can transform in the future. We propose a model that predicts future images by learning to represent the present state and its transformation given only a sequence of images. To do so, we introduce an architecture with a latent state composed of two components designed to capture (i) the present image state and (ii) the transformation between present and future states, respectively. We couple this latent state with a recurrent neural network (RNN) core that predicts future frames by transforming past states into future states by applying the accumulated state transformation with a learned operator. We describe how this model can be integrated into an encoder-decoder convolutional neural network (CNN) architecture that uses weighted residual connections to integrate representations of the past with representations of the future. Qualitatively, our approach generates image sequences that are stable and capture realistic motion over multiple predicted frames, without requiring adversarial training. Quantitatively, our method achieves prediction results comparable to state-of-the-art results on standard image prediction benchmarks (Moving MNIST, KTH, and UCF101). ",Predicting the Future with Transformational States
22,978178882523279360,296558041,Dom Smith,"['Happy days! We released a paper on suppressing the dominant background for new physics searches at the LHC @ CERN <LINK>', '@CatDogLund Someone‚Äôs background is always someone else‚Äôs signal! I‚Äôd be interested to see if this has any strength as a sideband, to enrich with QCD üòä']",http://arxiv.org/abs/1803.07942,"We introduce three alternative angular variables-denoted by $\tilde{\omega}_\text{min}$, $\hat{\omega}_\text{min}$, and $\chi_\text{min}$-for QCD multijet event suppression in supersymmetry searches in events with large missing transverse momentum in proton-proton collisions at the LHC at CERN. In searches in all-hadronic final states in the CMS and ATLAS experiments, the angle $\Delta\varphi_i$, the azimuthal angle between a jet and the missing transverse momentum, is widely used to reduce QCD multijet background events with large missing transverse momentum, which is primarily caused by a jet momentum mismeasurement or neutrinos in hadron decays-the missing transverse momentum is aligned with a jet. A related angular variable-denoted by $\Delta\varphi^*_\text{min}$, the minimum of the azimuthal angles between a jet and the transverse momentum imbalance of the other jets in the event-is used instead in a series of searches in all-hadronic final states in CMS to suppress QCD multijet background events to a negligible level. In this paper, before introducing the alternative variables, we review the variable $\Delta\varphi^*_\text{min}$ in detail and identify room for improvement, in particular, to maintain good acceptances for signal models with high jet multiplicity final states. Furthermore, we demonstrate with simulated event samples that $\hat{\omega}_\text{min}$ and $\chi_\text{min}$ considerably outperform $\Delta\varphi^*_\text{min}$ and $\Delta\varphi_i$ in rejecting QCD multijet background events and that $\hat{\omega}_\text{min}$ and $\tilde{\omega}_\text{min}$ are also useful for reducing the total standard model background events. ","Alternative angular variables for suppression of QCD multijet events in
  new physics searches with missing transverse momentum at the LHC"
23,977189038598885378,823957466,Hanna Wallach,"['New paper on arXiv!!! üéâüéä\n\nLocally Private Bayesian Inference for Count Models\n\nby @AaronSchein, Steven Wu, Mingyuan Zhou &amp; me!\n\nLimited-precision local privacy; a reinterpretation of the geometric mechanism; Poisson, Skellam &amp; Bessel distributions!!! üòÆ\n\n<LINK> <LINK>', ""And we're VERY excited to be working with the incredible @XandaSchofield on the next phase of this project over the summer!!!! üòÉüéâüéä"", 'Turns out Steven Wu is on Twitter!!!! @zstevenwu', '@mdekstrand @zstevenwu This is a great question! I am not sure, but would be very excited to discuss this/think this through with you! Perhaps w/ @_ajbc too...', '@mdekstrand @_ajbc @zstevenwu @DrMehrpouyan Yay!!! Maybe in 2 weeks time?', '@timnitGebru @zstevenwu @goodfellow_ian üòÆ']",https://arxiv.org/abs/1803.08471,"We present a general method for privacy-preserving Bayesian inference in Poisson factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisfies limited precision local privacy, a generalization of local differential privacy, which we introduce to formulate privacy guarantees appropriate for sparse count data. We develop an MCMC algorithm that approximates the locally private posterior over model parameters given data that has been locally privatized by the geometric mechanism (Ghosh et al., 2012). Our solution is based on two insights: 1) a novel reinterpretation of the geometric mechanism in terms of the Skellam distribution (Skellam, 1946) and 2) a general theorem that relates the Skellam to the Bessel distribution (Yuan & Kalbfleisch, 2000). We demonstrate our method in two case studies on real-world email data in which we show that our method consistently outperforms the commonly-used naive approach, obtaining higher quality topics in text and more accurate link prediction in networks. On some tasks, our privacy-preserving method even outperforms non-private inference which conditions on the true data. ",Locally Private Bayesian Inference for Count Models
24,977141095439233024,2872569532,Alejandro S. Borlaff,"['The new paper from @alumbrerasc just came out! On this paper, he studies the properties of blue star-forming low-mass galaxies, cleverly using SHARDS @GTCtelescope data. If you are wondering how galaxies built up check this out! <LINK> <LINK>']",https://arxiv.org/abs/1803.08045,"The physical processes driving the evolution of star formation (SF) in galaxies over cosmic time still present many open questions. Recent galaxy surveys allow now to study these processes in great detail at intermediate redshift. In this work, we build a complete sample of star-forming galaxies and analyze their properties, reaching systems with low stellar masses and low star formation rates (SFRs) at intermediate-to-low redshift. We use data from the SHARDS multiband survey in the GOODS-North field. Its depth (up to magnitude $\langle m_{3\sigma}\rangle\sim26.5$) and its spectro-photometric resolution ($R\sim50$) provides us with an ideal dataset to search for emission line galaxies (ELGs). We develop a new algorithm to identify low-redshift ($z$<0.36) ELGs by detecting the [OIII]5007 and $H\alpha$ emission lines simultaneously. We fit the spectral energy distribution (SED) of the selected sample, using a model with two single stellar populations. We find 160 star-forming galaxies with equivalent widths (EWs) as low as 12 {\AA}, with median values for the sample of $\sim$ 35 {\AA} in [OIII]5007 and $\sim$ 56 {\AA} in $H\alpha$, respectively. Results from the SED fitting show a young stellar population with low median metallicity (36% of the solar value) and extinction ($A_V \sim$ 0.37), with median galaxy stellar mass $\sim$ 10$^{8.5}$ M$_{\odot}$. Gas-phase metallicities measured from available spectra are also low. ELGs in our sample present bluer colors in the UVJ plane than the median color-selected star-forming galaxy in SHARDS. We suggest a new (V-J) color criterion to separate ELGs from non-ELGs in blue galaxy samples. In addition, several galaxies present high densities of O-type stars. Robust fits to the full SEDs can only be obtained including an old stellar population, suggesting the young component is built up by a recent burst of SF in an otherwise old galaxy. ",Star-forming galaxies at low-redshift in the SHARDS survey
25,977110109200449536,45861644,Ali Gregory,['New paper from our #datacentricengineering team @turinginst on the modelling of recovery times for railway bridges integrated with fibre optic sensor networks. arXiv here: <LINK>'],https://arxiv.org/abs/1803.08444,"Statistical techniques play a large role in the structural health monitoring of instrumented infrastructure, such as a railway bridge constructed with an integrated network of fibre optic sensors. One possible way to reason about the structural health of such a railway bridge, is to model the time it takes to recover to a no-load (baseline) state after a train passes over. Inherently, this recovery time is random and should be modelled statistically. This paper uses a non-parametric model, based on empirical quantile approximations, to construct a space-memory efficient baseline distribution for the streaming data from these sensors. A fast statistical test is implemented to detect deviations away from, and recovery back to, this distribution when trains pass over the bridge, yielding a recovery time. Our method assumes that there are no temporal variations in the data. A median-based detrending scheme is used to remove the temporal variations likely due to temperature changes. This allows for the continuous recording of sensor data with a space-memory constraint. ","A Quantile-Based Approach to Modelling Recovery Time in Structural
  Health Monitoring"
26,977087316043943937,152741512,Prof. Andy Way,"['My new paper ""Quality expectations of machine translation"" available now: <LINK>. Chapter in @jossmo\'s new book ""Human and Machine Translation Quality and Evaluation: From Principles to Practice"" published by @SpringerEdu @AdaptCentre @DublinCityUni']",http://arxiv.org/abs/1803.08409,"Machine Translation (MT) is being deployed for a range of use-cases by millions of people on a daily basis. There should, therefore, be no doubt as to the utility of MT. However, not everyone is convinced that MT can be useful, especially as a productivity enhancer for human translators. In this chapter, I address this issue, describing how MT is currently deployed, how its output is evaluated and how this could be enhanced, especially as MT quality itself improves. Central to these issues is the acceptance that there is no longer a single 'gold standard' measure of quality, such that the situation in which MT is deployed needs to be borne in mind, especially with respect to the expected 'shelf-life' of the translation itself. ",Quality expectations of machine translation
27,976996489997180928,2971867205,Ramses Ramirez,['Our new paper shows that really wet ocean worlds with high-pressure ices on the sea floor may be habitable even in the absence of volcanism or plate tectonics! <LINK>'],https://arxiv.org/abs/1803.07717,"Traditional definitions of the habitable zone assume that habitable planets contain a carbonate-silicate cycle that regulates CO2 between the atmosphere, surface, and the interior. Such theories have been used to cast doubt on the habitability of ocean worlds. However, Levi et al (2017) have recently proposed a mechanism by which CO2 is mobilized between the atmosphere and the interior of an ocean world. At high enough CO2 pressures, sea ice can become enriched in CO2 clathrates and sink after a threshold density is achieved. The presence of subpolar sea ice is of great importance for habitability in ocean worlds. It may moderate the climate and is fundamental in current theories of life formation in diluted environments. Here, we model the Levi et al. mechanism and use latitudinally-dependent non-grey energy balance and single-column radiative-convective climate models and find that this mechanism may be sustained on ocean worlds that rotate at least 3 times faster than the Earth. We calculate the circumstellar region in which this cycle may operate for G-M-stars (Teff = 2,600 to 5,800 K), extending from about 1.23 to 1.65, 0.69 to 0.954, 0.38 to 0.528 AU, 0.219 to 0.308 AU, 0.146 to 0.206 AU, and 0.0428 to 0.0617 AU for G2, K2, M0, M3, M5, and M8 stars, respectively. However, unless planets are very young and not tidally locked, our mechanism would be unlikely to apply to stars cooler than a ~M3. We predict C/O ratios for our atmospheres (about 0.5) that can be verified by the JWST mission. ",The Ice Cap Zone: A Unique Habitable Zone for Ocean Worlds
28,976993125175459841,795877354266456064,KoheiKamadaPhys,['<LINK>\nsubmitted a new paper.'],https://arxiv.org/abs/1803.08051,"Primordial magnetic fields in the dark sector can be transferred to magnetic fields in the visible sector due to a gauge kinetic mixing term. We show that the transfer occurs when the evolution of magnetic fields is dominated by dissipation due to finite electric conductivity, and does not occur at later times if the magnetic fields evolve according to magnetohydrodynamics scaling laws. The efficiency of the transfer is suppressed by not only the gauge kinetic mixing coupling but also the ratio between the large electric conductivity and the typical momentum of the magnetic fields. We find that the transfer gives nonzero visible magnetic fields today. However, without possible dynamo amplifications, the field transfer is not efficient enough to obtain the intergalactic magnetic fields suggested by the gamma-ray observations, although there are plenty of possibilities for efficient dark magnetogenesis, which are experimentally unconstrained. ",Magnetic Field Transfer From A Hidden Sector
29,976979547861102592,1558538456,Rodrigo Fern√°ndez,"['What is the effect of strong magnetic fields on sub-photospheric density &amp; velocity fluctuations in intermediate- and high-mass stars?\n\nNew paper led by student Koushik Sen, and in collaboration with @eV_per_baryon:\n \n<LINK>']",https://arxiv.org/abs/1803.08053,"We examine the excitation of unstable magnetosonic waves in the radiative envelopes of intermediate- and high-mass stars with a magnetic field of ~kG strength. Wind clumping close to the star and microturbulence can often be accounted for when including small-scale, sub-photospheric density or velocity perturbations. Compressional waves - with wavelengths comparable to or shorter than the gas pressure scale height - can be destabilized by the radiative flux in optically-thick media when a magnetic field is present, in a process called the Radiation-Driven Magneto-Acoustic Instability (RMI). The instability does not require radiation or magnetic pressure to dominate over gas pressure, and acts independently of sub-surface convection zones. Here we evaluate the conditions for the RMI to operate on a grid of stellar models covering a mass range $3-40M_\odot$ at solar metallicity. For a uniform 1kG magnetic field, fast magnetosonic modes are unstable down to an optical depth of a few tens, while unstable slow modes extend beyond the depth of the iron convection zone. The qualitative behavior is robust to magnetic field strength variations by a factor of a few. When combining our findings with previous results for the saturation amplitude of the RMI, we predict velocity fluctuations in the range ~0.1-10 km/s. These amplitudes are a monotonically increasing function of the ratio of radiation to gas pressure, or alternatively, of the zero-age main sequence mass. ","Sub-photospheric fluctuations in magnetized radiative envelopes:
  contribution from unstable magnetosonic waves"
30,976679556533837824,939498802767044608,Stephan,"['New #ML paper on multi-agent behavioral cloning +Eric Zhan,@yisongyue,Patrick Lucey! Hierarchical team policies that learn + execute #NBA team formations from data. Demo: <LINK> data: <LINK>  code: <LINK>\n\n<LINK> <LINK>']",https://arxiv.org/abs/1803.07612,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts. ",Generating Multi-Agent Trajectories using Programmatic Weak Supervision
31,976623204918808576,976155561522794497,Juliano C√©sar Silva Neves,"['My new paper tries to build a dialogue between Aristotle and Einstein: ""Infinities as natural places"" <LINK>']",https://arxiv.org/abs/1803.07995,"It is shown that a notion of natural place is possible within modern physics. For Aristotle, the elements$-$the primary components of the world$-$follow to their natural places in the absence of forces. On the other hand, in general relativity, the so-called Carter-Penrose diagrams offer a notion of end for objects along the geodesics. Then, the notion of natural place in Aristotelian physics has an analog in the notion of conformal infinities in general relativity. ",Infinities as natural places
32,976503074033491974,2800204849,Andrew Gordon Wilson,"['Code for our new paper, Averaging Weights Leads to Wider Optima and Better Generalization, is now available in @PyTorch:\n<LINK>\nPaper: <LINK>']",https://arxiv.org/abs/1803.05407,"Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead. ",Averaging Weights Leads to Wider Optima and Better Generalization
33,976195487068930048,143264018,Christine Corbett Moran,['I have a new paper out ‚≠êÔ∏è‚ö´Ô∏è <LINK>'],https://arxiv.org/abs/1803.06430,"A promising supermassive black hole seed formation channel is that of direct collapse from primordial gas clouds. We perform a suite of 3D hydrodynamics simulations of an isolated turbulent gas cloud to investigate conditions conducive to forming massive black hole seeds via direct collapse, probing the impact of cloud metallicity, gas temperature floor and cooling physics on cloud fragmentation. We find there is no threshold in metallicity which produces a sharp drop in fragmentation. When molecular cooling is not present, metallicity has little effect on fragmentation. When molecular cooling is present, fragmentation is suppressed by at most $\sim 25\%$, with the greatest suppression seen at metallicities below $2\%$ solar. A gas temperature floor $\sim 10^{4}$K produces the largest drop in fragmentation of any parameter choice, reducing fragmentation by $\sim 60\%$. At metallicities below $2\%$ solar or at temperatures $\sim 10^{3}$K we see a reduction in fragmentation $\sim 20-25 \%$. For a cloud of metallicity $2\%$ solar above and a temperature below $10^3$K, the detailed choices of temperature floor, metallicity, and cooling physics have little impact on fragmentation. ","The effects of metallicity and cooling physics on fragmentation:
  implications on direct-collapse black hole formation"
34,976116735425294341,80319419,Sophie Murray,['New paper accepted for publication in Solar Physics and is now available on @arxiv! <LINK> Check it out to learn about our work comparing results from @EU_HELCATS and @FLARECAST_EU projects related to the source of solar eruptions. <LINK>'],https://arxiv.org/abs/1803.06529,"Coronal mass ejections (CMEs) and other solar eruptive phenomena can be physically linked by combining data from a multitude of ground-based and space-based instruments alongside models, however this can be challenging for automated operational systems. The EU Framework Package 7 HELCATS project provides catalogues of CME observations and properties from the Helio- spheric Imagers onboard the two NASA/STEREO spacecraft in order to track the evolution of CMEs in the inner heliosphere. From the main HICAT catalogue of over 2,000 CME detections, an automated algorithm has been developed to connect the CMEs observed by STEREO to any corresponding solar flares and active region (AR) sources on the solar surface. CME kinematic properties, such as speed and angular width, are compared with AR magnetic field properties, such as magnetic flux, area, and neutral line characteristics. The resulting LOWCAT catalogue is also compared to the extensive AR property database created by the EU Horizon 2020 FLARECAST project, which provides more complex magnetic field parameters derived from vector magnetograms. Initial statistical analysis has been undertaken on the new data to provide insight into the link between flare and CME events, and characteristics of eruptive ARs. Warning thresholds determined from analysis of the evolution of these parameters is shown to be a useful output for operational space weather purposes. Parameters of particular interest for further analysis include total unsigned flux, vertical current, and current helicity. The automated method developed to create the LOWCAT catalogue may also be useful for future efforts to develop operational CME forecasting. ","Connecting Coronal Mass Ejections to their Solar Active Region Sources:
  Combining Results from the HELCATS and FLARECAST Projects"
35,976111421485436929,64444175,Jenn Gustetic üöÄüí™üèª,"["".@MilindTambe_AI, you might want to check out @NASA's new paper on using #AI, #emergingtech and #citzenscience to address grand challenges. This paper focuses on #asteroid science but would have many learnings for your NAE &amp; Social work GC: <LINK> #transformers""]",http://arxiv.org/abs/1803.04564,"Beginning in 2012, NASA utilized a strategic process to identify broad societal questions, or grand challenges, that are well suited to the aerospace sector and align with national priorities. This effort generated NASA's first grand challenge, the Asteroid Grand Challenge, a large scale effort using multidisciplinary collaborations and innovative engagement mechanisms focused on finding and addressing asteroid threats to human populations. In April 2010, President Barack Obama announced a mission to send humans to an asteroid by 2025. This resulted in the agency's Asteroid Redirect Mission to leverage and maximize existing robotic and human efforts to capture and reroute an asteroid, with the goal of eventual human exploration. The AGC, initiated in 2013, complemented ARM by expanding public participation, partnerships, and other approaches to find, understand, and overcome these potentially harmful asteroids. This paper describes a selection of AGC activities implemented from 2013 to 2017 and their results, excluding those conducted by NASA's Near Earth Object Observations Program and other organizations. The strategic development of the initiative is outlined as well as initial successes, strengths, and weaknesses resulting from the first four years of AGC activities and approaches. Finally, we describe lesson learned and areas for continued work and study. The AGC lessons learned and strategies could inform the work of other agencies and organizations seeking to conduct a global scientific investigation with matrixed organizational support, multiple strategic partners, and numerous internal and external open innovation approaches and audiences. ","NASA's Asteroid Grand Challenge: Strategy, Results and Lessons Learned"
36,975897346742550529,2800204849,Andrew Gordon Wilson,['All you need is LOVE!\nOur new paper: <LINK> \nConstant-Time Predictive Distributions for Gaussian Processes\nEaster eggs within! <LINK>'],https://arxiv.org/abs/1803.06058,"One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacrificing accuracy. ",Constant-Time Predictive Distributions for Gaussian Processes
37,975720457302429696,20309837,Michael Veale,"['In a new short paper from me, @RDBinns + @emax, we consider what GDPR-compliant machine learning entails, and how the human‚Äìcomputer interaction community can support it. \n\nSome HCI Priorities for GDPR-Compliant Machine Learning <LINK> #CHI2018 #CHIGDPR #GDPR <LINK>', 'including eager nods to great work on DP+HCI by legends such as @Ew_Luger and @mooseabyte']",https://arxiv.org/abs/1803.06174,"In this short paper, we consider the roles of HCI in enabling the better governance of consequential machine learning systems using the rights and obligations laid out in the recent 2016 EU General Data Protection Regulation (GDPR)---a law which involves heavy interaction with people and systems. Focussing on those areas that relate to algorithmic systems in society, we propose roles for HCI in legal contexts in relation to fairness, bias and discrimination; data protection by design; data protection impact assessments; transparency and explanations; the mitigation and understanding of automation bias; and the communication of envisaged consequences of processing. ",Some HCI Priorities for GDPR-Compliant Machine Learning
38,975704685943418880,797888987675365377,Tom Rainforth,"['Nesting probabilistic programs allows us to model agents reasoning about other agents, but current inference engines typically give invalid estimates.  Check out how to do things correctly in my new paper <LINK>']",https://arxiv.org/abs/1803.06328,"We formalize the notion of nesting probabilistic programming queries and investigate the resulting statistical implications. We demonstrate that while query nesting allows the definition of models which could not otherwise be expressed, such as those involving agents reasoning about other agents, existing systems take approaches which lead to inconsistent estimates. We show how to correct this by delineating possible ways one might want to nest queries and asserting the respective conditions required for convergence. We further introduce a new online nested Monte Carlo estimator that makes it substantially easier to ensure these conditions are met, thereby providing a simple framework for designing statistically correct inference engines. We prove the correctness of this online estimator and show that, when using the recommended setup, its asymptotic variance is always better than that of the equivalent fixed estimator, while its bias is always within a factor of two. ",Nesting Probabilistic Programs
39,974661414924242946,745157314445860864,Tobias Huber,"['We just published ""Are Bitcoin Bubbles Predictable?""‚Äîa new paper that develops a fundamental valuation of #bitcoin and a diagnostic for bitcoin #bubbles: <LINK>']",https://arxiv.org/abs/1803.05663,"We develop a strong diagnostic for bubbles and crashes in bitcoin, by analyzing the coincidence (and its absence) of fundamental and technical indicators. Using a generalized Metcalfe's law based on network properties, a fundamental value is quantified and shown to be heavily exceeded, on at least four occasions, by bubbles that grow and burst. In these bubbles, we detect a universal super-exponential unsustainable growth. We model this universal pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which parsimoniously captures diverse positive feedback phenomena, such as herding and imitation. The LPPLS model is shown to provide an ex-ante warning of market instabilities, quantifying a high crash hazard and probabilistic bracket of the crash time consistent with the actual corrections; although, as always, the precise time and trigger (which straw breaks the camel's back) being exogenous and unpredictable. Looking forward, our analysis identifies a substantial but not unprecedented overvaluation in the price of bitcoin, suggesting many months of volatile sideways bitcoin prices ahead (from the time of writing, March 2018). ","Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law
  and the LPPLS Model"
40,974594359529222146,963342591445094400,Pascal Gehring,['<LINK>\nNew paper on graphene based mechanical break junctions on arxiv'],https://arxiv.org/abs/1803.05642,"The ability to detect and distinguish quantum interference signatures is important for both fundamental research and for the realization of devices including electron resonators, interferometers and interference-based spin filters. Consistent with the principles of subwavelength optics, the wave nature of electrons can give rise to various types of interference effects, such as Fabry-P\'erot resonances, Fano resonances and the Aharonov-Bohm effect. Quantum-interference conductance oscillations have indeed been predicted for multiwall carbon nanotube shuttles and telescopes, and arise from atomic-scale displacements between the inner and outer tubes. Previous theoretical work on graphene bilayers indicates that these systems may display similar interference features as a function of the relative position of the two sheets. Experimental verification is, however, still lacking. Graphene nanoconstrictions represent an ideal model system to study quantum transport phenomena due to the electronic coherence and the transverse confinement of the carriers. Here, we demonstrate the fabrication of bowtie-shaped nanoconstrictions with mechanically controlled break junctions (MCBJs) made from a single layer of graphene. We find that their electrical conductance displays pronounced oscillations at room temperature, with amplitudes that modulate over an order of magnitude as a function of sub-nanometer displacements. Surprisingly, the oscillations exhibit a period larger than the graphene lattice constant. Charge-transport calculations show that the periodicity originates from a combination of quantum-interference and lattice-commensuration effects of two graphene layers that slide across each other. Our results provide direct experimental observation of Fabry-P\'erot-like interference of electron waves that are partially reflected/transmitted at the edges of the graphene bilayer overlap region. ",Mechanically Controlled Quantum Interference in Graphene Break Junctions
41,974372669184401408,307826617,Kev Abazajian ‚§∑‚è≥üåé,['New paper on galaxy formation in WDM or sterile ŒΩ dark matter Universes: ‚ÄúThe discovery of young ultra-faint dwarf galaxies with no ancient star formation -- which do not exist in our CDM simulations -- would therefore provide evidence in support of WDM.‚Äù <LINK> <LINK>'],https://arxiv.org/abs/1803.05424,"We study the impact of a warm dark matter (WDM) cosmology on dwarf galaxy formation through a suite of cosmological hydrodynamical zoom-in simulations of $M_{\rm halo} \approx10^{10}\,M_{\odot}$ dark matter halos as part of the Feedback in Realistic Environments (FIRE) project. A main focus of this paper is to evaluate the combined effects of dark matter physics and stellar feedback on the well-known small-scale issues found in cold dark matter (CDM) models. We find that the $z=0$ stellar mass of a galaxy is strongly correlated with the central density of its host dark matter halo at the time of formation, $z_{\rm f}$, in both CDM and WDM models. WDM halos follow the same $M_{\star}(z=0)-V_{\rm max}(z_{\rm f})$ relation as in CDM, but they form later, are less centrally dense, and therefore contain galaxies that are less massive than their CDM counterparts. As a result, the impact of baryonic effects on the central gravitational potential is typically diminished relative to CDM. However, the combination of delayed formation in WDM and energy input from stellar feedback results in dark matter profiles with lower overall densities. The WDM galaxies studied here have a wider diversity of star formation histories (SFHs) than the same systems simulated in CDM, and the two lowest $M_{\star}$ WDM galaxies form all of their stars at late times. The discovery of young ultra-faint dwarf galaxies with no ancient star formation -- which do not exist in our CDM simulations -- would therefore provide evidence in support of WDM. ","Warm FIRE: Simulating Galaxy Formation with Resonant Sterile Neutrino
  Dark Matter"
42,974321395898265600,162293874,Jeff Clune,"['New Scientist article summarizes our new paper on how evolution routinely outsmarts the scientists that wield it. <LINK> Paper <LINK> Gif from Cully, Clune, Tarapore, Mouret Nature 2015 @CULLYAntoine @jb_mouret @joelbot3000 @DuleFR @newscientist <LINK>']",https://arxiv.org/abs/1803.03453v1,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems. ","] The Surprising Creativity of Digital Evolution: A Collection of
  Anecdotes from the Evolutionary Computation and Artificial Life Research
  Communities"
43,974294668795174912,2562351306,Alexander Engelen,['In this new paper we study the impact of the growth of structure on measurements of gravitational lensing with upcoming 3d surveys of the Universe.  56 pages!  <LINK>'],https://arxiv.org/abs/1803.04975,"We investigate the detection prospects for gravitational lensing of three-dimensional maps from upcoming line intensity surveys, focusing in particular on the impact of gravitational nonlinearities on standard quadratic lensing estimators. Using perturbation theory, we show that these nonlinearities can provide a significant contaminant to lensing reconstruction, even for observations at reionization-era redshifts. However, we show how this contamination can be mitigated with the use of a ""bias-hardened"" estimator. Along the way, we present an estimator for reconstructing long-wavelength density modes, in the spirit of the ""tidal reconstruction"" technique that has been proposed elsewhere, and discuss the dominant biases on this estimator. After applying bias-hardening, we find that a detection of the lensing potential power spectrum will still be challenging for the first phase of SKA-Low, CHIME, and HIRAX, with gravitational nonlinearities decreasing the signal to noise by a factor of a few compared to forecasts that ignore these effects. On the other hand, cross-correlations between lensing and galaxy clustering or cosmic shear from a large photometric survey look promising, provided that systematics can be sufficiently controlled. We reach similar conclusions for a single-dish survey inspired by CII measurements planned for CCAT-prime, suggesting that lensing is an interesting science target not just for 21cm surveys, but also for intensity maps of other lines. ","Lensing reconstruction from line intensity maps: the impact of
  gravitational nonlinearity"
44,974276631736045570,702241209276829697,Cecilia Garraffo üíö,"['In a previous study, we concluded that TRAPPIST-1 planets are probably exposed to extreme space weather (<LINK>). Now we studied the upper atmospheres under this conditions. Check out our new paper: <LINK>']",https://arxiv.org/abs/1706.04617,"Recently, four additional Earth-mass planets were discovered orbiting the nearby ultracool M8 dwarf TRAPPIST-1, making a remarkable total of seven planets with equilibrium temperatures compatible with the presence of liquid water on their surface. Temperate terrestrial planets around an M-dwarf orbit close to their parent star, rendering their atmospheres vulnerable to erosion by the stellar wind and energetic electromagnetic and particle radiation. Here, we use state-of-the-art 3D magnetohydrodynamic models to simulate the wind around TRAPPIST-1 and study the conditions at each planetary orbit. All planets experience a stellar wind pressure between $10^3$ and $10^5$ times the solar wind pressure on Earth. All orbits pass through wind pressure changes of an order of magnitude and most planets spend a large fraction of their orbital period in the sub-Alfv\'enic regime. For plausible planetary magnetic field strengths, all magnetospheres are greatly compressed and undergo much more dynamic change than that of the Earth. The planetary magnetic fields connect with the stellar radial field over much of the planetary surface, allowing direct flow of stellar wind particles onto the planetary atmosphere. These conditions could result in strong atmospheric stripping and evaporation and should be taken into account for any realistic assessment of the evolution and habitability of the TRAPPIST-1 planets. ",The Threatening Environment of the TRAPPIST-1 Planets
45,974255416875147264,185910194,Graham Neubig,"['Posted ""Neural Lattice Language Models"", our new paper (TACL) on LMs that calculate probability of a sentence by marginalizing over a lattice! <LINK>\nIt\'s a nice and flexible framework for LMs that lets you consider ambiguity such as word sense, segmentation, etc <LINK>']",https://arxiv.org/abs/1803.05071,"In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions - including polysemy and existence of multi-word lexical items - into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline. ",Neural Lattice Language Models
46,974087637828071429,2800204849,Andrew Gordon Wilson,['Averaging Weights Leads to Wider Optima and Better Generalization\nOur new paper: <LINK> <LINK>'],http://arxiv.org/abs/1803.05407,"Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead. ",Averaging Weights Leads to Wider Optima and Better Generalization
47,974077485892583424,3242991169,Bharath Ramsundar,"['Check out our new paper on ""Spatial Graph Convolutions for Drug Discovery."" Converts a 3D macro molecular structure into a graph structure that it feeds into a graph convolutional deep network. Matches state-of-art with end-to-end learning. <LINK>', 'The work is written up in a great Medium post by lead author @enfeinberg and corresponding author @vijaypande https://t.co/UwegCC8ni7']",https://arxiv.org/abs/1803.04465,"The arc of drug discovery entails a multiparameter optimization problem spanning vast length scales. They key parameters range from solubility (angstroms) to protein-ligand binding (nanometers) to in vivo toxicity (meters). Through feature learning---instead of feature engineering---deep neural networks promise to outperform both traditional physics-based and knowledge-based machine learning models for predicting molecular properties pertinent to drug discovery. To this end, we present the PotentialNet family of graph convolutions. These models are specifically designed for and achieve state-of-the-art performance for protein-ligand binding affinity. We further validate these deep neural networks by setting new standards of performance in several ligand-based tasks. In parallel, we introduce a new metric, the Regression Enrichment Factor $EF_\chi^{(R)}$, to measure the early enrichment of computational models for chemical data. Finally, we introduce a cross-validation strategy based on structural homology clustering that can more accurately measure model generalizability, which crucially distinguishes the aims of machine learning for drug discovery from standard machine learning tasks. ",PotentialNet for Molecular Property Prediction
48,973872964784582658,2337598033,Geraint F. Lewis,"['A new paper on the arxiv - <LINK> <LINK>', '@MJBiercuk This is what happens when astronomy projects becomes are large as particle physics projects! Big science!']",https://arxiv.org/abs/1803.04869,"We present the results of a search for rapidly evolving transients in the Dark Energy Survey Supernova Programme. These events are characterized by fast light curve evolution (rise to peak in $\lesssim 10$ d and exponential decline in $\lesssim30$ d after peak). We discovered 72 events, including 37 transients with a spectroscopic redshift from host galaxy spectral features. The 37 events increase the total number of rapid optical transients by more than factor of two. They are found at a wide range of redshifts ($0.05<z<1.56$) and peak brightnesses ($-15.75>M_\mathrm{g}>-22.25$). The multiband photometry is well fit by a blackbody up to few weeks after peak. The events appear to be hot ($T\approx10000-30000$ K) and large ($R\approx 10^{14}-2\cdot10^{15}$ cm) at peak, and generally expand and cool in time, though some events show evidence for a receding photosphere with roughly constant temperature. Spectra taken around peak are dominated by a blue featureless continuum consistent with hot, optically thick ejecta. We compare our events with a previously suggested physical scenario involving shock breakout in an optically thick wind surrounding a core-collapse supernova (CCSNe), we conclude that current models for such a scenario might need an additional power source to describe the exponential decline. We find these transients tend to favor star-forming host galaxies, which could be consistent with a core-collapse origin. However, more detailed modeling of the light curves is necessary to determine their physical origin. ",Rapidly evolving transients in the Dark Energy Survey
49,973741504714899456,19606850,David Manheim,"['I co-wrote a new paper formalizing / extending Scott Garrabrant\'s categorization of ""Goodhart-Like"" effects.\nFeedback is welcome!\n<LINK> <LINK>']",http://arxiv.org/abs/1803.04585,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field. ",Categorizing Variants of Goodhart's Law
50,973542264067805185,3301643341,Roger Grosse,"['Flipout makes weight perturbations (evolution strategies, variational BNNs) as mini-batch-friendly as activation perturbations (dropout, batch norm). New paper with Yeming Wen, Paul Vicol, Jimmy Ba, and @dustinvtran \n\n<LINK>', '@roydanroy @dustinvtran They need to be independent and symmetric around 0. But you could extend it to, e.g., matrix variate Gaussian perturbations, by rotating to a coordinate system that satisfies this.']",https://arxiv.org/abs/1803.04386,"Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services. ","Flipout: Efficient Pseudo-Independent Weight Perturbations on
  Mini-Batches"
51,973364773256646657,1243709419,Patrick Shafto,['New paper from my lab: Standing Wave Decomposition Gaussian Process. With Chi-Ken Lu and Scott Yang. Inference on 10^5 data points in half a second on a laptop. <LINK> <LINK>'],https://arxiv.org/abs/1803.03666,"We propose a Standing Wave Decomposition (SWD) approximation to Gaussian Process regression (GP). GP involves a costly matrix inversion operation, which limits applicability to large data analysis. For an input space that can be approximated by a grid and when correlations among data are short-ranged, the kernel matrix inversion can be replaced by analytic diagonalization using the SWD. We show that this approach applies to uni- and multi-dimensional input data, extends to include longer-range correlations, and the grid can be in a latent space and used as inducing points. Through simulations, we show that our approximate method applied to the squared exponential kernel outperforms existing methods in predictive accuracy per unit time in the regime where data are plentiful. Our SWD-GP is recommended for regression analyses where there is a relatively large amount of data and/or there are constraints on computation time. ",Standing Wave Decomposition Gaussian Process
52,973361158756302848,939498802767044608,Stephan,"[""New #MachineLearning paper +@sumanth0811 Richard Murray @yisongyue: detecting adversarial examples with Neural Fingerprinting! We detect state-of-the-art adv ex with ~99% AUC on MNIST, CIFAR and 20-class Imagenet, by verifying a neural net's fingerprints. <LINK> <LINK>""]",http://arxiv.org/abs/1803.03870,"Deep neural networks are vulnerable to adversarial examples, which dramatically alter model output using small input changes. We propose Neural Fingerprinting, a simple, yet effective method to detect adversarial examples by verifying whether model behavior is consistent with a set of secret fingerprints, inspired by the use of biometric and cryptographic signatures. The benefits of our method are that 1) it is fast, 2) it is prohibitively expensive for an attacker to reverse-engineer which fingerprints were used, and 3) it does not assume knowledge of the adversary. In this work, we pose a formal framework to analyze fingerprints under various threat models, and characterize Neural Fingerprinting for linear models. For complex neural networks, we empirically demonstrate that Neural Fingerprinting significantly improves on state-of-the-art detection mechanisms by detecting the strongest known adversarial attacks with 98-100% AUC-ROC scores on the MNIST, CIFAR-10 and MiniImagenet (20 classes) datasets. In particular, the detection accuracy of Neural Fingerprinting generalizes well to unseen test-data under various black- and whitebox threat models, and is robust over a wide range of hyperparameters and choices of fingerprints. ",Detecting Adversarial Examples via Neural Fingerprinting
53,973358319837556737,267173080,Mostafa Gamal Badawy,"['Our new paper, ShuffleSeg: Real-time Semantic Segmentation Network. Under review by ICIP 2018.\n\n<LINK>']",https://arxiv.org/abs/1803.03816,"Real-time semantic segmentation is of significant importance for mobile and robotics related applications. We propose a computationally efficient segmentation network which we term as ShuffleSeg. The proposed architecture is based on grouped convolution and channel shuffling in its encoder for improving the performance. An ablation study of different decoding methods is compared including Skip architecture, UNet, and Dilation Frontend. Interesting insights on the speed and accuracy tradeoff is discussed. It is shown that skip architecture in the decoding method provides the best compromise for the goal of real-time performance, while it provides adequate accuracy by utilizing higher resolution feature maps for a more accurate segmentation. ShuffleSeg is evaluated on CityScapes and compared against the state of the art real-time segmentation networks. It achieves 2x GFLOPs reduction, while it provides on par mean intersection over union of 58.3% on CityScapes test set. ShuffleSeg runs at 15.7 frames per second on NVIDIA Jetson TX2, which makes it of great potential for real-time applications. ",ShuffleSeg: Real-time Semantic Segmentation Network
54,972339873439870976,3293760861,Aaron Chan,"['[1/2] Our #CVPR2018 paper is now on arXiv (with a new title!): <LINK>', ""[2/2] And here's the supplementary video: https://t.co/qyBBxhlQxj""]",https://arxiv.org/abs/1803.01413,"We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence. Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs). ",Egocentric Basketball Motion Planning from a Single First-Person Image
55,972275728027979777,267173080,Mostafa Gamal Badawy,['Our new paper RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY. Under review by ICIP 2018.\n\n<LINK>'],https://arxiv.org/abs/1803.02758,"Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In this paper, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The framework is comprised of different network architectures for feature extraction such as VGG16, Resnet18, MobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures for segmentation that define the decoding methodology. These include SkipNet, UNet, and Dilation Frontend. Experimental results are presented on the Cityscapes dataset for urban scenes. The modular design allows novel architectures to emerge, that lead to 143x GFLOPs reduction in comparison to SegNet. This benchmarking framework is publicly available at ""this https URL"". ",RTSeg: Real-time Semantic Segmentation Comparative Study
56,972047803982770176,772809603046334464,Jonathan Mackey,['New paper on @arxiv about the subsonic structure and optically thick winds from Wolf-Rayet stars like this one: WR124 imaged by @NASAHubble in 1998.  <LINK> <LINK>'],https://arxiv.org/abs/1803.03033,"Wolf-Rayet star's winds can be so dense and so optically thick that the photosphere appears in the highly supersonic part of the outflow, veiling the underlying subsonic part of the star, and leaving the initial acceleration of the wind inaccessible to observations. We investigate the conditions and the structure of the subsonic part of the outflow of Galactic WR stars, in particular of the WNE subclass; our focus is on the conditions at the sonic point. We compute 1D hydrodynamic stellar structure models for massive helium stars adopting outer boundaries at the sonic point. We find that the outflows of our models are accelerated to supersonic velocities by the radiative force from opacity bumps either at temperatures of the order of 200kK by the Fe opacity bump or of the order of 50kK by the HeII opacity bump. For a given mass-loss rate, the conditions in the subsonic part of the outflow are independent from the detailed physical conditions in the supersonic part. The close proximity to the Eddington limit at the sonic point allows us to construct a Sonic HR diagram, relating the sonic point temperature to the L/M ratio and the stellar mass-loss rate, thereby constraining the sonic point conditions, the subsonic structure, and the stellar wind mass-loss rates from observations. The minimum mass-loss rate necessary to have the flow accelerated to supersonic velocities by the Fe opacity bump is derived. A comparison of the observed parameters of Galactic WNE stars to this minimum mass-loss rate indicates that their winds are launched to supersonic velocities by the radiation pressure arising from the Fe-bump. Conversely, models which do not show transonic flows from the Fe opacity bump form inflated envelopes. We derive an analytic criterion for the appearance of envelope inflation in the subphotospheric layers. ",Subsonic structure and optically thick winds from Wolf--Rayet stars
57,971818884335534081,1202760024,Stacy McGaugh,"['New, very brief paper on the the arXiv pointing out that the ‚Äúsurprisingly‚Äù strong 21cm absorption detected at z=17 is entirely expected if the universe is devoid of non-baryonic dark matter. \n<LINK>\nI was talking about this 20 years ago. <LINK>', 'Hopefully there will be more data like these on a timescale shorter than decades. \n\nAnd yes, I understand all the problems this raises. That‚Äôs why it is important. See\n\nhttps://t.co/5LEqFhRup2', '@MBKplus Everything heads towards the Om=1 solution in that limit. If we‚Äôre going to contemplate some modification of gravity that does this, it has to recover GR in the right limit (of course) and not change the expansion history in the early U. Late times a different matter of course.', '@MBKplus Gonna refer everyone to the many articles I‚Äôve written on this. Most are old now but ADS will know then. But in a nutshell I envision the minimal damage possible to cosmology as we know it. It‚Äôs just the ‚Äúdark‚Äù pieces that I suspect of being proxies for something deeper.', '@MBKplus Not sure that one is!', '@MBKplus D‚Äôoh. Now you‚Äôve made me think. I think Bob Sanders did this a loooong time ago, but can‚Äôt remember where. I may be forced to reread our ARA&amp;A review.', '@MBKplus It looks like the mass density enters through the relation for H(z). So when we rewrite the equation in terms of fb, we‚Äôre implicitly setting Om=Ob = small, so we‚Äôre effectively reducing H, so increasing the path length in the early universe, increasing the opacity.', '@MBKplus I‚Äôm not sure we can. This is an eternal complaint of mine: anything for which we need distance metrics, LCDM works great. Don‚Äôt even know how else to talk about it. Vice-Versa for the dynamics of bound systems. perhaps the early universe really is this simple?', '@MBKplus Which is to say, things are definitely messed up!']",https://arxiv.org/abs/1803.02365,"The recently reported detection of redshifted 21cm absorption at $z \approx 17$ is a significant advance in the exploration of the cosmic dark ages. The observed signal ($T_{\mathrm{21}} \approx -0.5$ K with the limit $T_{\mathrm{21}} < -0.3$ K at 99\% c.l.) is anomalously strong for $\Lambda$CDM, which predicts at most $-0.24$ K. Here I show that the strong observed signal is expected in a purely baryonic universe. ","Strong Hydrogen Absorption at Cosmic Dawn: the Signature of a Baryonic
  Universe"
58,971754575349932032,610427323,Desika Narayanan,"[""New paper!  <LINK> .  We postprocess massive zooms with RT to make 'observables' -- Non-parametric merger indicators G-M20 and C-A work super well at low-z, but the complex morphology from highly clustered evirons makes measures fail at high-z 4 massive gals."", ""So for things that might end up as centrals in z=0 groups/clusters, our current non-parametric methods don't perform much better than randomly guessing."", 'Extra fun fact:  paper is my first-ever student led one, and by a super awesome former @haverfordedu  undergraduate!', ""@conselice Thanks for the comments Chris!  The replies are somewhat multifaceted.  First, these simulations are cosmological zooms, which means we follow individual galaxies at high-res, but don't have statistics.  So we can't generate statistics for merger rates."", ""@conselice Second, part of the point of the paper is that G-M20 and C-A may not have success at determining merger rates, at least for very massive systems at high-z.   Of course there are other methods for determining merger rates, though I can't speak to their accuracy in this regime."", ""@conselice The point being -- it's not clear that we can compare the rates from the simulations to the observations when the false positive rate from observations may be significant."", '@conselice Finally, a bit of a philosophical point:  should it matter what the rate normalization is?  Ideally, a merger identifier should identify mergers regardless of the rate, with a minimal false positive rate.', ""@conselice On the Petrosian radii, I'll point you to the 7 pages of methods!  Not to defer, but it's nontrivial to summarize in N characters or less :)   But will be very happy to hear your comments on them.""]",https://arxiv.org/abs/1803.02374,"Non-parametric morphology measures are a powerful tool for identifying galaxy mergers at low redshifts. We employ cosmological zoom simulations using Gizmo with the Mufasa feedback scheme, post-processed using 3D dust radiative transfer into mock observations, to study whether common morphological measures Gini G, M20, concentration C, and asymmetry A are effective at identifying major galaxy mergers at z ~ 2 - 4, i.e. ""Cosmic Noon"". Our zoom suite covers galaxies with 10^8.6 < M_* < 10^11 M_sun at z ~ 2, and broadly reproduces key global galaxy observations. Our primary result is that these morphological measures are unable to robustly pick out galaxies currently undergoing mergers during Cosmic Noon, typically performing no better than a random guess. This improves only marginally if we consider whether galaxies have undergone a merger within the last Gyr. When also considering minor mergers, galaxies display no trend of moving towards the merger regime with increasing merger ratio. From z = 4 -> 2, galaxies move from the non-merger towards the merger regime in all statistics, but this is primarily an effect of mass: Above a given noise level, higher mass galaxies display a more complex outer morphology induced by their clustered environment. We conclude that during Cosmic Noon, these morphological statistics are of limited value in identifying galaxy mergers. ","Identifying Mergers Using Quantitative Morphologies in Zoom Simulations
  of High-Redshift Galaxies"
59,971751310071648257,2381150314,Ali Faqeeh,"['Check out our new paper with Sirag Erkol &amp; Prof. Radicchi @filrad on ""Identifying influential spreaders in noisy networks"": \n<LINK>  @IUSICE #diffusion_dynamics #complex_networks']",http://arxiv.org/abs/1803.02253,"We consider the problem of identifying the most influential nodes for a spreading process on a network when prior knowledge about structure and dynamics of the system is incomplete or erroneous. Specifically, we perform a numerical analysis where the set of top spreaders is determined on the basis of prior information that is artificially altered by a certain level of noise. We then measure the optimality of the chosen set by measuring its spreading impact in the true system. Whereas we find that the identification of top spreaders is optimal when prior knowledge is complete and free of mistakes, we also find that the quality of the top spreaders identified using noisy information doesn't necessarily decrease as the noise level increases. For instance, we show that it is generally possible to compensate for erroneous information about dynamical parameters by adding synthetic errors in the structure of the network. Further, we show that, in some dynamical regimes, even completely losing prior knowledge on network structure may be better than relying on certain but incomplete information. ",Influence maximization in noisy networks
60,971743689285435392,735386827578875904,siegfried Vanaverbek,['A new catalog of false positive exoplanets from the KELT survey.  \nSee our paper released on arxiv:<LINK>'],http://arxiv.org/abs/1803.01869,"The Kilodegree Extremely Little Telescope (KELT) project has been conducting a photometric survey for transiting planets orbiting bright stars for over ten years. The KELT images have a pixel scale of ~23""/pixel---very similar to that of NASA's Transiting Exoplanet Survey Satellite (TESS)---as well as a large point spread function, and the KELT reduction pipeline uses a weighted photometric aperture with radius 3'. At this angular scale, multiple stars are typically blended in the photometric apertures. In order to identify false positives and confirm transiting exoplanets, we have assembled a follow-up network (KELT-FUN) to conduct imaging with higher spatial resolution, cadence, and photometric precision than the KELT telescopes, as well as spectroscopic observations of the candidate host stars. The KELT-FUN team has followed-up over 1,600 planet candidates since 2011, resulting in more than 20 planet discoveries. Excluding ~450 false alarms of non-astrophysical origin (i.e., instrumental noise or systematics), we present an all-sky catalog of the 1,128 bright stars (6<V<10) that show transit-like features in the KELT light curves, but which were subsequently determined to be astrophysical false positives (FPs) after photometric and/or spectroscopic follow-up observations. The KELT-FUN team continues to pursue KELT and other planet candidates and will eventually follow up certain classes of TESS candidates. The KELT FP catalog will help minimize the duplication of follow-up observations by current and future transit surveys such as TESS. ","The KELT Follow-Up Network and Transit False Positive Catalog:
  Pre-vetted False Positives for TESS"
61,971679245293490181,1162081,Carlos Baquero,"['New paper ""Efficient Synchronization of State-based CRDTs‚Äù where we use join-decompositions to optimize delta based synchronisation and derive minimal delta-mutators <LINK>']",https://arxiv.org/abs/1803.02750,"To ensure high availability in large scale distributed systems, Conflict-free Replicated Data Types (CRDTs) relax consistency by allowing immediate query and update operations at the local replica, with no need for remote synchronization. State-based CRDTs synchronize replicas by periodically sending their full state to other replicas, which can become extremely costly as the CRDT state grows. Delta-based CRDTs address this problem by producing small incremental states (deltas) to be used in synchronization instead of the full state. However, current synchronisation algorithms for Delta-based CRDTs induce redundant wasteful delta propagation, performing worse than expected, and surprisingly, no better than State-based. In this paper we: 1) identify two sources of inefficiency in current synchronization algorithms for delta-based CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3) exploit join decompositions to obtain optimal deltas and 4) improve the efficiency of synchronization algorithms; and finally, 5) evaluate the improved algorithms. ",Efficient Synchronization of State-based CRDTs
62,971421923816157185,3301643341,Roger Grosse,"[""Generalization to longer horizons is the Achilles' heel of gradient-based meta-optimization. Short horizon meta-optimizers decay the learning rate really quickly and stop making progress. New paper w/ @Yuhu_ai_,  @mengyer, and Renjie Liao.\n\n<LINK> <LINK>""]",https://arxiv.org/abs/1803.02021,"Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes. ",Understanding Short-Horizon Bias in Stochastic Meta-Optimization
63,971332313572311040,59413748,Reuben Binns,"[""We often criticize tech platforms for using crude behavioral metrics, but what should we replace them with?\n\nNew paper (probably weirdest I've written), asks 'What do users really want?'\n\n(w/ @ulyngs, @emax @Nigel_Shadbolt for #chi2018 #altchi \n\n<LINK>"", '@tavmos @ulyngs @emax @Nigel_Shadbolt it was a busy winter!']",https://arxiv.org/abs/1803.02065,"Equating users' true needs and desires with behavioural measures of 'engagement' is problematic. However, good metrics of 'true preferences' are difficult to define, as cognitive biases make people's preferences change with context and exhibit inconsistencies over time. Yet, HCI research often glosses over the philosophical and theoretical depth of what it means to infer what users really want. In this paper, we present an alternative yet very real discussion of this issue, via a fictive dialogue between senior executives in a tech company aimed at helping people live the life they `really' want to live. How will the designers settle on a metric for their product to optimise? ","""So, Tell Me What Users Want, What They Really, Really Want!"""
64,971310121203781633,721931072,Shimon Whiteson,['Our latest paper is the fruit of a new collaboration with @oxfordrobots: hierarchical learning from demonstration: weakly supervising the demonstrations enables zero-shot learning for robots <LINK> @KyriacosShiarli @IngmarPosner @markus_with_k @whi_rl'],https://arxiv.org/abs/1803.01840,"Many advanced Learning from Demonstration (LfD) methods consider the decomposition of complex, real-world tasks into simpler sub-tasks. By reusing the corresponding sub-policies within and between tasks, they provide training data for each policy from different high-level tasks and compose them to perform novel ones. Existing approaches to modular LfD focus either on learning a single high-level task or depend on domain knowledge and temporal segmentation. In contrast, we propose a weakly supervised, domain-agnostic approach based on task sketches, which include only the sequence of sub-tasks performed in each demonstration. Our approach simultaneously aligns the sketches with the observed demonstrations and learns the required sub-policies. This improves generalisation in comparison to separate optimisation procedures. We evaluate the approach on multiple domains, including a simulated 3D robot arm control task using purely image-based observations. The results show that our approach performs commensurately with fully supervised approaches, while requiring significantly less annotation effort. ",TACO: Learning Task Decomposition via Temporal Alignment for Control
65,970931913602093056,182730982,Andreas R√ºckl√©,"['We just published our new paper on concatenated p-mean embeddings as universal cross-lingual sentence representations! concat. p-mean embeddings outperform more complex methods cross-lingually while being a very hard-to-beat baseline monolingually.\n<LINK>', 'Data, model, and code are also available: https://t.co/DPygWT5Jw6']",https://arxiv.org/abs/1803.01400,"Average word embeddings are a common baseline for more sophisticated sentence embedding techniques. However, they typically fall short of the performances of more complex models such as InferSent. Here, we generalize the concept of average word embeddings to power mean word embeddings. We show that the concatenation of different types of power mean word embeddings considerably closes the gap to state-of-the-art methods monolingually and substantially outperforms these more complex techniques cross-lingually. In addition, our proposed method outperforms different recently proposed baselines such as SIF and Sent2Vec by a solid margin, thus constituting a much harder-to-beat monolingual baseline. Our data and code are publicly available. ","Concatenated Power Mean Word Embeddings as Universal Cross-Lingual
  Sentence Representations"
66,970857249781493760,613453368,Shaojie Bai,"['Our new paper, with @zicokolter and Vladlen Koltun, on extensively evaluating convolutional vs. recurrent approaches to sequence tasks is on arXiv now! <LINK>']",http://arxiv.org/abs/1803.01271,"For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL . ","An Empirical Evaluation of Generic Convolutional and Recurrent Networks
  for Sequence Modeling"
67,970837380587491328,966155324293046272,Constantin Schrade,"['Our new paper is on the arXiv today! It\'s about the ""Majorana Superconducting Qubit"" <LINK>']",https://arxiv.org/abs/1803.01002,"We propose a platform for universal quantum computation that uses conventional $s$-wave superconducting leads to address a topological qubit stored in spatially separated Majorana bound states in a multi-terminal topological superconductor island. Both the manipulation and read-out of this ""Majorana superconducting qubit"" are realized by tunnel couplings between Majorana bound states and the superconducting leads. The ability of turning on and off tunnel couplings on-demand by local gates enables individual qubit addressability while avoiding cross-talk errors. By combining the scalability of superconducting qubit and the robustness of topological qubits, the Majorana superconducting qubit may provide a promising and realistic route towards quantum computation. ",Majorana Superconducting Qubit
68,970617641282031616,2872569532,Alejandro S. Borlaff,"['If you are interested on the structure and evolution of stellar discs, check out our new paper on @arxiv! We found that anti-truncated (Type-III) S0 galaxies were structurally stable for half the age of the  Universe, while fading ~1.5 mag/ac¬≤  since z~0.6 <LINK> <LINK>']",https://arxiv.org/abs/1803.00570,"Type-III S0 galaxies present tight scaling relations between their surface brightness photometric and structural parameters. Several evolutionary models have been proposed for the formation of Type-III S0 galaxies but the observations of are usually limited to the local Universe. We study the evolution of the photometric and structural scaling relations found between the parameters of the surface brightness profiles in the rest-frame R-band of Type-III S0 galaxies with z and the possible differences between the rest-frame (B-R) colours of the inner and outer disc profiles. We make use of a sample of 14 Type-III E/S0--S0 galaxies at 0.2<z<0.6 to study if the correlations found in local Type-III S0 galaxies were present ~6 Gyr ago. We analyse the distribution of the surface brightness characteristic parameters as a function of the stellar mass and if there is a significant change with z. We also derive their rest-frame (B-R) colour profiles and we compare these results with the predictions from a grid of SSP models. We find that the inner and outer scale-lengths of Type-III S0 galaxies at 0.4<z<0.6 follow compatible trends and scaling relations with those observed in local S0 galaxies. We do not detect any significant differences between the location of Rbreak between z~0.6 and z=0 for a fixed stellar mass of the object, whereas the surface brightness at the break radius is ~1.5 mag arcsec-2 dimmer in the local Universe than at z~0.6 for a fixed stellar mass. In contrast to Type-II profiles, the Type-III surface brightness profiles of S0 galaxies present compatible Rbreak values and scaling relations during the last 6 Gyr. This result and the similarity of the colours of the inner and outer discs point to a highly scalable and stable formation process, probably more related to gravitational and dynamical processes than to the evolution of stellar populations (abridged). ","Evolution of the anti-truncated stellar profiles of S0 galaxies since
  $z=0.6$ in the SHARDS survey: II - Structural and photometric evolution"
69,969573162890551298,185910194,Graham Neubig,"[""Uploaded our new paper (<LINK>, to be presented at AMTA2018) on our neural MT toolkit xnmt: <LINK>\nxnmt is designed for quickly testing out new research ideas, so check it out if you're interested in MT or seq2seq for your research!"", ""(and the design principles for creating an easily extensible system may be of interest even if you're doing other things as well)""]",https://arxiv.org/abs/1803.00188,"This paper describes XNMT, the eXtensible Neural Machine Translation toolkit. XNMT distin- guishes itself from other open-source NMT toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results. In this paper we describe the design of XNMT and its experiment configuration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine translation/parsing. XNMT is available open-source at this https URL ",XNMT: The eXtensible Neural Machine Translation Toolkit
70,969535332457861120,15763014,Tom Faulkenberry,['My new paper on approximating Bayes factors from ANOVA summaries was just accepted in Biometrical Letters. You can access the preprint on ArXiV <LINK>'],https://arxiv.org/abs/1803.00360,"Bayesian inference affords scientists with powerful tools for testing hypotheses. One of these tools is the Bayes factor, which indexes the extent to which support for one hypothesis over another is updated after seeing the data. Part of the hesitance to adopt this approach may stem from an unfamiliarity with the computational tools necessary for computing Bayes factors. Previous work has shown that closed form approximations of Bayes factors are relatively easy to obtain for between- groups methods, such as an analysis of variance or t-test. In this paper, I extend this approximation to develop a formula for the Bayes factor that directly uses information that is typically reported for ANOVAs (e.g., the F ratio and degrees of freedom). After giving two examples of its use, I report the results of simulations which show that even with minimal input, this approximate Bayes factor produces similar results to existing software solutions. ","Computing Bayes factors to measure evidence from experiments: An
  extension of the BIC approximation"
71,969510547736670208,771716816817324032,Francesco Tudisco,['Using matrix power means to merge network layers works very well for clustering multilayer graphs ...  check out our new paper @ AISTATS 2018  <LINK>'],https://arxiv.org/abs/1803.00491,"Multilayer graphs encode different kind of interactions between the same set of entities. When one wants to cluster such a multilayer graph, the natural question arises how one should merge the information different layers. We introduce in this paper a one-parameter family of matrix power means for merging the Laplacians from different layers and analyze it in expectation in the stochastic block model. We show that this family allows to recover ground truth clusters under different settings and verify this in real world data. While computing the matrix power mean can be very expensive for large graphs, we introduce a numerical scheme to efficiently compute its eigenvectors for the case of large sparse graphs. ",The Power Mean Laplacian for Multilayer Graph Clustering
72,981104999861248000,3108542843,Fran√ßois-Xavier Briol,"['New paper on ""Stein Points"": <LINK>, or how to use optimization and Stein\'s method to sample from complex posterior distributions!']",https://arxiv.org/abs/1803.10161,"An important task in computational statistics and machine learning is to approximate a posterior distribution $p(x)$ with an empirical measure supported on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when $n$ is small. To this end, we present `Stein Points'. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and $p(x)$. Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method. ",Stein Points
73,977177869905514496,933084565895286786,Dan Hooper,"['In a new paper with Tim Linden, we present the first evidence (~3 sigma) that millisecond pulsars are surrounded by TeV halos. If confirmed, this would provide a new way to test whether the Galactic Center gamma-ray excess is (or is not) generated by MSPs.\n<LINK>', '@di_goldene_pave Good catch. Thanks.']",https://arxiv.org/abs/1803.08046,"Observations by HAWC indicate that many young pulsars (including Geminga and Monogem) are surrounded by spatially extended, multi-TeV emitting regions. It is not currently known, however, whether TeV emission is also produced by recycled, millisecond pulsars (MSPs). In this study, we perform a stacked analysis of 24 MSPs within HAWC's field-of-view, finding between 2.6-3.2 sigma evidence that these sources are, in fact, surrounded by TeV halos. The efficiency with which these MSPs produce TeV halos is similar to that exhibited by young pulsars. This result suggests that several dozen MSPs will ultimately be detectable by HAWC, including many ""invisible"" pulsars without radio beams oriented in our direction. The TeV halos of unresolved MSPs could also dominate the TeV-scale diffuse emission observed at high galactic latitudes. We also discuss the possibility that TeV and radio observations could be used to constrain the population of MSPs that is present in the inner Milky Way, thereby providing us with a new way to test the hypothesis that MSPs are responsible for the Galactic Center GeV excess. ","Millisecond Pulsars, TeV Halos, and Implications For The Galactic Center
  Gamma-Ray Excess"
74,976107527690694656,251623181,Francesca Bosco,['#paper Securing the #InternetofThings: New Perspectives and Research Challenges <LINK>'],https://arxiv.org/abs/1803.05022,"The Internet of Things (IoT) realizes a vision where billions of interconnected devices are deployed just about everywhere, from inside our bodies to the most remote areas of the globe. As the IoT will soon pervade every aspect of our lives and will be accessible from anywhere, addressing critical IoT security threats is now more important than ever. Traditional approaches where security is applied as an afterthought and as a ""patch"" against known attacks are insufficient. Indeed, next-generation IoT challenges will require a new secure-by-design vision, where threats are addressed proactively and IoT devices learn to dynamically adapt to different threats. To this end, machine learning and software-defined networking will be key to provide both reconfigurability and intelligence to the IoT devices. In this paper, we first provide a taxonomy and survey the state of the art in IoT security research, and offer a roadmap of concrete research challenges related to the application of machine learning and software-defined networking to address existing and next-generation IoT security threats. ","Securing the Internet of Things in the Age of Machine Learning and
  Software-defined Networking"
75,972026855082643461,2886658437,Sean Raymond,"[""Our new paper proposes that 'Oumuamua is a fragment of a planetesimal that was rendered extinct before being ejected into interstellar space.  Details here: <LINK> <LINK>""]",https://arxiv.org/abs/1803.02840,"'Oumuamua was discovered passing through our Solar System on a hyperbolic orbit. It presents an apparent contradiction, with colors similar to those of volatile-rich Solar System bodies but with no visible outgassing or activity during its close approach to the Sun. Here we show that this contradiction can be explained by the dynamics of planetesimal ejection by giant planets. We propose that 'Oumuamua is an extinct fragment of a comet-like planetesimal born in a planet-forming disk that also formed Neptune- to Jupiter-mass giant planets. On its pathway to ejection 'Oumuamua's parent body underwent a close encounter with a giant planet and was tidally disrupted into small pieces, similar to comet Shoemaker-Levy 9's disruption after passing close to Jupiter. We use dynamical simulations to show that 0.1-1% of cometary planetesimals undergo disruptive encounters prior to ejection. Rocky asteroidal planetesimals are unlikely to disrupt due to their higher densities. After disruption, the bulk of fragments undergo enough close passages to their host stars to lose their surface volatiles and become extinct. Planetesimal fragments such as 'Oumuamua contain little of the mass in the population of interstellar objects but dominate by number. Our model makes predictions that will be tested in the coming decade by LSST. ","Interstellar object 'Oumuamua as an extinct fragment of an ejected
  cometary planetesimal"
76,971790152644227075,773069606,Federico Ardila,"['Equivariant Volumes of the Permutahedron\nNew paper w my students Andr√©s @Vindas_Melendez + Anna Schindler:\n   <LINK>\nIf a permutation w of [n] has cycle lengths l_1,...,l_m, the subpolytope of the permutahedron Pi_n fixed by w has volume n^(m-2) gcd(l_1,...,l_m). <LINK>', ""This paper is based in part on the Master's theses of Andr√©s and Anna at @SFSU -- very proud of them for their work! It's also what this thread is about (for those who were trying to guess the theorems from the pictures):\nhttps://t.co/16rguvhnOk""]",https://arxiv.org/abs/1803.02377,"We consider the action of the symmetric group $S_n$ on the permutahedron $\Pi_n$. We prove that if $\sigma$ is a permutation of $S_n$ which has $m$ cycles of lengths $l_1, \ldots, l_m$, then the subpolytope of $\Pi_n$ fixed by $\sigma$ has normalized volume $n^{m-2} \gcd(l_1, \ldots, l_m)$. ",The equivariant volumes of the permutahedron
77,969389594448818177,18027886,Thang Luong,"['Excited to share a new work by #GoogleAI resident @thtrieu_ (with @iamandrewdai, me, &amp; Quoc Le) on training very long RNNs (up to 16K long). See paper for extreme cases of zero or little backprop on RNNs ;) <LINK> <LINK>']",https://arxiv.org/abs/1803.00144,"Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation. ",Learning Longer-term Dependencies in RNNs with Auxiliary Losses
78,979003608690843648,321648912,Vishnu Boddeti,['Have you ever wondered why CNN based face representations have hundreds or throusands of dimensions?\n\nWe study and estimate the intrinsic dimensionality of a given face representation. We find that you only need around 10 dimensions.\n\n<LINK>\n\n#DeepLearning'],https://arxiv.org/abs/1803.09672,"This paper addresses the following questions pertaining to the intrinsic dimensionality of any given image representation: (i) estimate its intrinsic dimensionality, (ii) develop a deep neural network based non-linear mapping, dubbed DeepMDS, that transforms the ambient representation to the minimal intrinsic space, and (iii) validate the veracity of the mapping through image matching in the intrinsic space. Experiments on benchmark image datasets (LFW, IJB-C and ImageNet-100) reveal that the intrinsic dimensionality of deep neural network representations is significantly lower than the dimensionality of the ambient features. For instance, SphereFace's 512-dim face representation and ResNet's 512-dim image representation have an intrinsic dimensionality of 16 and 19 respectively. Further, the DeepMDS mapping is able to obtain a representation of significantly lower dimensionality while maintaining discriminative ability to a large extent, 59.75% TAR @ 0.1% FAR in 16-dim vs 71.26% TAR in 512-dim on IJB-C and a Top-1 accuracy of 77.0% at 19-dim vs 83.4% at 512-dim on ImageNet-100. ",On the Intrinsic Dimensionality of Image Representations
79,978963022906580993,935220337880522752,Vinay Kashyap,"['Tiny, fleeting blips in the solar corona appear to cool away by conduction faster than they can radiate away.  Are these common?  Where is the impulse coming from?  We need a high-cadence high-res miriad-filter solar observatory at L2 to find out.\n<LINK>']",https://arxiv.org/abs/1803.09505,"We study the thermal structure and energetics of the point-like EUV brightenings within a system of fan loops observed in the active region \textsl{AR~11520}. These brightenings were simultaneously observed on 2012 July 11 by the HIgh-resolution Coronal (Hi-C) imager and the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO). We identified 27 brightenings by automatically determining intensity enhancements in both Hi-C and AIA ~193~{\AA} light curves. The energetics of these brightenings were studied by using the Differential Emission Measure (DEM) diagnostics. The DEM weighted temperatures of these transients are in the range $\log{T} (K) = 6.2 - 6.6$ with radiative energies ${\approx}10^{24-25}$~ergs and densities ${\approx}$ a few times $10^{9}$~cm$^{-3}$. To the best of our knowledge, these are the smallest brightenings in EUV ever detected. We used these results to determine the mechanism of energy loss in these brightenings. Our analysis reveals that the dominant mechanism of energy loss for all the identified brightenings is conduction rather than radiation. ",Energetics of Hi-C EUV Brightenings
80,978893106593648640,19149703,Karina Voggel ‚ú®üî≠üèÉüèº‚Äç‚ôÄÔ∏è,"[""It's ESO proposal deadline day, but it's also paper day! \nWe tested, whether the super-massive black holes we find in very massive UCDs also exist in lower. Ass UCDS! <LINK>"", 'We find no evidence for BHs in both, but their strong rotation and light profile suggest they might still be stripped nuclei of lower mass dwarf galaxies. https://t.co/YEudgHiLBo', 'Once we know how many of these stripped nuclei exists in a given galaxy or cluster, they could tell us about the past merging &amp; accretion history! So stay tuned, there is more to come :) https://t.co/p4x61EKuvJ']",https://arxiv.org/abs/1803.09750,"The recent discovery of massive black holes (BHs) in the centers of high-mass ultra compact dwarf galaxies (UCDs) suggests that at least some are the stripped nuclear star clusters of dwarf galaxies. We present the first study that investigates whether such massive BHs, and therefore stripped nuclei, also exist in low-mass ($M<10^{7}M_{\odot}$) UCDs. We constrain the BH masses of two UCDs located in Centaurus A (UCD320 and UCD330) using Jeans modeling of the resolved stellar kinematics from adaptive optics VLT/SINFONI data. No massive BHs are found in either UCD. We find a $3\,\sigma$ upper limit on the central BH mass in UCD\,330 of $M_{\bullet}<1.0\times10^{5}M_{\odot}$, which corresponds to 1.7\% of the total mass. This excludes a high mass fraction BH and would only allow a low-mass BHs similar to those claimed to be detected in Local Group GCs. For UCD320, poorer data quality results in a less constraining $3\,\sigma$ upper limit of $M_{\bullet}<1\times10^{6}M_{\odot}$, which is equal to 37.7\% of the total mass. The dynamical $M/L$ of UCD320 and UCD330 are not inflated compared to predictions from stellar population models. The non-detection of BHs in these low-mass UCDs is consistent with the idea that elevated dynamical $M/L$s do indicate the presence of a substantial BH. Despite not detecting massive BHs, these systems could still be stripped nuclei. The strong rotation ($v/\sigma$ of 0.3 to 0.4) in both UCDs and the two-component light profile in UCD330 support the idea that these UCDs may be stripped nuclei of low-mass galaxies where the BH occupation fraction is not yet known. ","Upper limits on the presence of central massive black holes in two
  ultra-compact dwarf galaxies in Centaurus A"
81,976989585648517121,712016577567264769,Hernan Makse,"['See our latest study of how fake news influenced the 2016 US election. We study 170 million tweets and characterize influencers spreading fake, extremely bias, and traditional news from left to right. <LINK> @kcore_analytics #FakeNews #FakeNewsInfluencers']",https://arxiv.org/abs/1803.08491,"The dynamics and influence of fake news on Twitter during the 2016 US presidential election remains to be clarified. Here, we use a dataset of 171 million tweets in the five months preceding the election day to identify 30 million tweets, from 2.2 million users, which contain a link to news outlets. Based on a classification of news outlets curated by www.opensources.co, we find that 25% of these tweets spread either fake or extremely biased news. We characterize the networks of information flow to find the most influential spreaders of fake and traditional news and use causal modeling to uncover how fake news influenced the presidential election. We find that, while top influencers spreading traditional center and left leaning news largely influence the activity of Clinton supporters, this causality is reversed for the fake news: the activity of Trump supporters influences the dynamics of the top fake news spreaders. ","Influence of fake news in Twitter during the 2016 US presidential
  election"
82,975920258518315008,3333301606,"Keith Hawkins, Ph.D.","['Cool result led by Yuan-Sen Ting using a method we developed allowing astronomers to find clump stars burning helium at the cores. We put the method to use to identify 100,000+ of these special stars which can/will be used build a 3-D map our Galaxy!<LINK>']",https://arxiv.org/abs/1803.06650,"Core helium-burning red clump (RC) stars are excellent standard candles in the Milky Way. These stars may have more precise distance estimates from spectrophotometry than from Gaia parallaxes beyond 3 kpc. However, RC stars have $T_{\rm eff}$ and log g very similar to some red giant branch (RGB) stars. Especially for low-resolution spectroscopic studies where $T_{\rm eff}$, log g, and [Fe/H] can only be estimated with limited precision, separating RC stars from RGB through established method can incur ~20% contamination. Recently, Hawkins et al. (2018) demonstrated that the additional information in single-epoch spectra, such as the C/N ratio, can be exploited to cleanly differentiate RC and RGB stars. In this second paper of the series, we establish a data-driven mapping from spectral flux space to independently determined asteroseismic parameters, the frequency and the period spacing. From this, we identify 210,371 RC stars from the publicly available LAMOST DR3 and APOGEE DR14 data, with ~9% of contamination. We provide an RC sample of 92,249 stars with a contamination of only ~3%, by restricting the combined analysis to LAMOST stars with ${\rm S/N}_{\rm pix}$ > 75. This demonstrates that high-S/N, low-resolution spectra covering a broad wavelength range can identify RC samples at least as pristine as their high-resolution counterparts. As coming and ongoing surveys such as TESS, DESI, and LAMOST will continue to improve the overlapping training spectroscopic-asteroseismic sample, the method presented in this study provides an efficient and straightforward way to derive a vast yet pristine RC stars to reveal the 3D structure of the Milky Way. ","A large and pristine sample of standard candles across the Milky Way:
  ~100,000 red clump stars with 3% contamination"
83,974294668795174912,2562351306,Alexander Engelen,['In this new paper we study the impact of the growth of structure on measurements of gravitational lensing with upcoming 3d surveys of the Universe.  56 pages!  <LINK>'],https://arxiv.org/abs/1803.04975,"We investigate the detection prospects for gravitational lensing of three-dimensional maps from upcoming line intensity surveys, focusing in particular on the impact of gravitational nonlinearities on standard quadratic lensing estimators. Using perturbation theory, we show that these nonlinearities can provide a significant contaminant to lensing reconstruction, even for observations at reionization-era redshifts. However, we show how this contamination can be mitigated with the use of a ""bias-hardened"" estimator. Along the way, we present an estimator for reconstructing long-wavelength density modes, in the spirit of the ""tidal reconstruction"" technique that has been proposed elsewhere, and discuss the dominant biases on this estimator. After applying bias-hardening, we find that a detection of the lensing potential power spectrum will still be challenging for the first phase of SKA-Low, CHIME, and HIRAX, with gravitational nonlinearities decreasing the signal to noise by a factor of a few compared to forecasts that ignore these effects. On the other hand, cross-correlations between lensing and galaxy clustering or cosmic shear from a large photometric survey look promising, provided that systematics can be sufficiently controlled. We reach similar conclusions for a single-dish survey inspired by CII measurements planned for CCAT-prime, suggesting that lensing is an interesting science target not just for 21cm surveys, but also for intensity maps of other lines. ","Lensing reconstruction from line intensity maps: the impact of
  gravitational nonlinearity"
84,974276631736045570,702241209276829697,Cecilia Garraffo üíö,"['In a previous study, we concluded that TRAPPIST-1 planets are probably exposed to extreme space weather (<LINK>). Now we studied the upper atmospheres under this conditions. Check out our new paper: <LINK>']",https://arxiv.org/abs/1706.04617,"Recently, four additional Earth-mass planets were discovered orbiting the nearby ultracool M8 dwarf TRAPPIST-1, making a remarkable total of seven planets with equilibrium temperatures compatible with the presence of liquid water on their surface. Temperate terrestrial planets around an M-dwarf orbit close to their parent star, rendering their atmospheres vulnerable to erosion by the stellar wind and energetic electromagnetic and particle radiation. Here, we use state-of-the-art 3D magnetohydrodynamic models to simulate the wind around TRAPPIST-1 and study the conditions at each planetary orbit. All planets experience a stellar wind pressure between $10^3$ and $10^5$ times the solar wind pressure on Earth. All orbits pass through wind pressure changes of an order of magnitude and most planets spend a large fraction of their orbital period in the sub-Alfv\'enic regime. For plausible planetary magnetic field strengths, all magnetospheres are greatly compressed and undergo much more dynamic change than that of the Earth. The planetary magnetic fields connect with the stellar radial field over much of the planetary surface, allowing direct flow of stellar wind particles onto the planetary atmosphere. These conditions could result in strong atmospheric stripping and evaporation and should be taken into account for any realistic assessment of the evolution and habitability of the TRAPPIST-1 planets. ",The Threatening Environment of the TRAPPIST-1 Planets
85,973203630412173312,308587014,Robert Feldt,"['Our mega-study (many, many authors :)) on artificial creativity now also on arxiv (in submission since last spring but we want to be able to also ref it during this (long) process...)\n<LINK>', ""In particular, I'm very happy to be co-author with Karl Sims whose classic work on evolving virtual creatures helped me dare to apply artificial evolution in SW Engineering in my PhD work in late 90's:\nhttps://t.co/C2NMFhhuwq""]",https://arxiv.org/abs/1803.03453,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems. ","The Surprising Creativity of Digital Evolution: A Collection of
  Anecdotes from the Evolutionary Computation and Artificial Life Research
  Communities"
86,971560667596509184,195143286,Song Huang,"['Using deep images from HSC, we find that the size and stellar mass distribution of massive central galaxies do depend on their halo: massive halo hosts larger, more extended central galaxy.  Stay tuned for more insights from weak lensing analysis! <LINK> <LINK>']",https://arxiv.org/abs/1803.02824,"We use ~100 square deg of deep (>28.5 mag arcsec$^{-2}$ in i-band), high-quality (median 0.6 arcsec seeing) imaging data from the Hyper Suprime-Cam (HSC) survey to reveal the halo mass dependence of the surface mass density profiles and outer stellar envelopes of massive galaxies. The i-band images from the HSC survey reach ~4 magnitudes deeper than Sloan Digital Sky Survey and enable us to directly trace stellar mass distributions to 100 kpc without requiring stacking. We conclusively show that, at fixed stellar mass, the stellar profiles of massive galaxies depend on the masses of their dark matter haloes. On average, massive central galaxies with $\log M_{\star, 100\ \mathrm{kpc}}>11.6$ in more massive haloes at 0.3 < z < 0.5 have shallower inner stellar mass density profiles (within ~10-20 kpc) and more prominent outer envelopes. These differences translate into a halo mass dependence of the mass-size relation. Central galaxies in haloes with $\log M_{\rm{Halo}}>14.0$ are ~20% larger in $R_{\mathrm{50}}$ at fixed stellar mass. Such dependence is also reflected in the relationship between the stellar mass within 10 and 100 kpc. Comparing to the mass--size relation, the $\log M_{\star, 100\ \rm{kpc}}$-$\log M_{\star, 10\ \rm{kpc}}$ relation avoids the ambiguity in the definition of size, and can be straightforwardly compared with simulations. Our results demonstrate that, with deep images from HSC, we can quantify the connection between halo mass and the outer stellar halo, which may provide new constraints on the formation and assembly of massive central galaxies. ","A Detection of the Environmental Dependence of the Sizes and Stellar
  Haloes of Massive Central Galaxies"
87,977323877490708480,3219713684,Blakeley H. Payne,['Now that @turing_box is live (!!!) I‚Äôm so excited to share my first paper with @ZivEpstein from #gradschool entitled ‚ÄúClosing the AI Knowledge Gap.‚Äù We believe anyone should be able to study #ai . \n\nPreprint: <LINK>'],https://arxiv.org/abs/1803.07233,"AI researchers employ not only the scientific method, but also methodology from mathematics and engineering. However, the use of the scientific method - specifically hypothesis testing - in AI is typically conducted in service of engineering objectives. Growing interest in topics such as fairness and algorithmic bias show that engineering-focused questions only comprise a subset of the important questions about AI systems. This results in the AI Knowledge Gap: the number of unique AI systems grows faster than the number of studies that characterize these systems' behavior. To close this gap, we argue that the study of AI could benefit from the greater inclusion of researchers who are well positioned to formulate and test hypotheses about the behavior of AI systems. We examine the barriers preventing social and behavioral scientists from conducting such studies. Our diagnosis suggests that accelerating the scientific study of AI systems requires new incentives for academia and industry, mediated by new tools and institutions. To address these needs, we propose a two-sided marketplace called TuringBox. On one side, AI contributors upload existing and novel algorithms to be studied scientifically by others. On the other side, AI examiners develop and post machine intelligence tasks designed to evaluate and characterize algorithmic behavior. We discuss this market's potential to democratize the scientific study of AI behavior, and thus narrow the AI Knowledge Gap. ",Closing the AI Knowledge Gap
88,973506891396583426,51169895,Gianluca Stringhini,['In our latest paper we study how having humans interact with malware apps influences the behavioral models that we can extract for detection purposes <LINK> <LINK>'],https://arxiv.org/abs/1803.03448,"Following the increasing popularity of mobile ecosystems, cybercriminals have increasingly targeted them, designing and distributing malicious apps that steal information or cause harm to the device's owner. Aiming to counter them, detection techniques based on either static or dynamic analysis that model Android malware, have been proposed. While the pros and cons of these analysis techniques are known, they are usually compared in the context of their limitations e.g., static analysis is not able to capture runtime behaviors, full code coverage is usually not achieved during dynamic analysis, etc. Whereas, in this paper, we analyze the performance of static and dynamic analysis methods in the detection of Android malware and attempt to compare them in terms of their detection performance, using the same modeling approach. To this end, we build on MaMaDroid, a state-of-the-art detection system that relies on static analysis to create a behavioral model from the sequences of abstracted API calls. Then, aiming to apply the same technique in a dynamic analysis setting, we modify CHIMP, a platform recently proposed to crowdsource human inputs for app testing, in order to extract API calls' sequences from the traces produced while executing the app on a CHIMP virtual device. We call this system AuntieDroid and instantiate it by using both automated (Monkey) and user-generated inputs. We find that combining both static and dynamic analysis yields the best performance, with F-measure reaching 0.92. We also show that static analysis is at least as effective as dynamic analysis, depending on how apps are stimulated during execution, and, finally, investigate the reasons for inconsistent misclassifications across methods. ","A Family of Droids -- Android Malware Detection via Behavioral Modeling:
  Static vs Dynamic Analysis"
