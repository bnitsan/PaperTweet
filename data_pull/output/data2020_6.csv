,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1282746555963854849,4633095775,Ravi Tej Akella,"['1/ Exciting news! Introducing two new policy gradient methods: (i) Deep Bayesian Quadrature Policy Gradient (DBQPG), and (ii) Uncertainty Aware Policy Gradient (UAPG). Joint work with @kazizzad, Mohammad Ghavamzadeh, @AnimaAnandkumar, and @yisongyue\nPaper: <LINK>', '2/ While there exist numerous policy gradient (PG) algorithms, most of them use the same PG estimator: Monte-Carlo (MC) method. In DBQPG, we replace MC with Bayesian quadrature (BQ) for estimating the PG.\nProject Link: https://t.co/OyJ5eDihge\nCode: https://t.co/M2GBDQ2jYO', '3/ In comparison to MC, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) consistent improvement in the sample complexity and average return, and (iii) the uncertainty in policy gradient estimation.', '4/ Further, we propose a new policy gradient method, UAPG, that uses the estimation uncertainty of the policy gradient to compute reliable policy updates with robust step-sizes.']",https://arxiv.org/abs/2006.15637,"We study the problem of obtaining accurate policy gradient estimates using a finite number of samples. Monte-Carlo methods have been the default choice for policy gradient estimation, despite suffering from high variance in the gradient estimates. On the other hand, more sample efficient alternatives like Bayesian quadrature methods have received little attention due to their high computational complexity. In this work, we propose deep Bayesian quadrature policy gradient (DBQPG), a computationally efficient high-dimensional generalization of Bayesian quadrature, for policy gradient estimation. We show that DBQPG can substitute Monte-Carlo estimation in policy gradient methods, and demonstrate its effectiveness on a set of continuous control benchmarks. In comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) a consistent improvement in the sample complexity and average return for several deep policy gradient algorithms, and, (iii) the uncertainty in gradient estimation that can be incorporated to further improve the performance. ",Deep Bayesian Quadrature Policy Optimization
1,1282032306341863424,23724401,Daniel Jiang,"['New paper ""Lookahead-bounded Q-learning"" with Ibrahim El-Shar (@IElsharr) on using information relaxation to learn approximate upper &amp; lower bounds for constraining Q-learning iterates: <LINK> 😀\n\nTalk: <LINK> #icml2020 <LINK>']",https://arxiv.org/abs/2006.15690,"We introduce the lookahead-bounded Q-learning (LBQL) algorithm, a new, provably convergent variant of Q-learning that seeks to improve the performance of standard Q-learning in stochastic environments through the use of ``lookahead'' upper and lower bounds. To do this, LBQL employs previously collected experience and each iteration's state-action values as dual feasible penalties to construct a sequence of sampled information relaxation problems. The solutions to these problems provide estimated upper and lower bounds on the optimal value, which we track via stochastic approximation. These quantities are then used to constrain the iterates to stay within the bounds at every iteration. Numerical experiments on benchmark problems show that LBQL exhibits faster convergence and more robustness to hyperparameters when compared to standard Q-learning and several related techniques. Our approach is particularly appealing in problems that require expensive simulations or real-world interactions. ",Lookahead-Bounded Q-Learning
2,1280287147157860353,988920586750619649,"Dr. Rocío Joo, PhD","['Hey #movementecology community, check our new paper <LINK> reviewing the last decade in the field, based on text analysis in #RStats of &gt; 8000 papers. Some highlights of our paper in this thread. *OK 1/2 of the paper in this thread (1/n) <LINK>', 'History. The study of #movement is pretty old (check timeline below), but the term #movementecology was not popular before a special feature on movement ecology in which @ran_nathan and colleagues defined the movement ecology framework (MEF). (2/n) https://t.co/rTvlFVmJrZ', 'The MEF consisted of: external factors (environmental conditions that affect #movement), internal state (intrinsic factors affecting motivation and readiness to move), navigation (traits enabling the individual to orient), and motion (traits enabling the individual to move) (3/n) https://t.co/7rhNCOPaCa', 'The outcome of the interactions between these components would be the observed path. We found that, in the last decade, most studies tackled\nmovement in relation to external factors, while a minority of them studied the processes behind #movement, like motion or navigation. (4/n) https://t.co/77bIJu7rmm', 'Technology. While the MEF has not seemed to have radically changed the field, #biologging and #software use have changed. E.g. GPS, accelerometer and video are more popular. #Rstats has become the undisputed preference in the field among #software tools. (5/n) https://t.co/UGAudH3ODH', '#Stats. Our analyses revealed that movement, spatial or time-series statistical tools are not the most popular choices in #movementecology studies, but rather generic tools like GLM (that could eventually have a term accounting for time or space in some way). (6/n) https://t.co/g1pHBXM8o6', '#technology vs. concepts. Overall, the results seem to indicate that technology has played a bigger role in #movementecology than #movement concepts. Is the field more data-driven than ideas-driven? Where do we as #movement researchers stand in this trade-off? (7/n) https://t.co/DZV8akWnTm', 'The methods for #TextAnalysis (including quality control) and #rstats codes are described in detail here: https://t.co/q7alkm2dQ5 (8/n) https://t.co/32b553RAxv', ""Did you participate in our #movementecology survey like a year ago? If you did, thank you so much! Here are the results https://t.co/D3SKl9j575 End of the thread. I'd love to get your thoughts on the paper :) (9/n)""]",https://arxiv.org/abs/2006.00110,"Movement is fundamental to life, shaping population dynamics, biodiversity patterns, and ecosystem structure. Recent advances in tracking technology have enabled fundamental questions about movement to be tackled, leading to the development of the movement ecology framework (MEF), considered a milestone in the field [1]. The MEF introduced an integrative theory of organismal movement, linking internal state, motion capacity and navigation capacity to external factors. Here, a decade later, we investigated the current state of research in the field. Using a text mining approach on >8000 peer-reviewed papers in movement ecology, we explored the main research topics, evaluated the impact of the MEF, and assessed changes in the use of technological devices, software and statistical methods. The number of publications has increased considerably and there have been major technological changes in the past decade (i.e.~increased use of GPS devices, accelerometers and video cameras, and a convergence towards R), yet we found that research focuses on the same questions, specifically, on the effect of environmental factors on movement and behavior. In practice, it appears that movement ecology research does not reflect the MEF. We call on researchers to transform the field from technology-driven to embrace interdisciplinary collaboration, in order to reveal key processes underlying movement (e.g.~navigation), as well as evolutionary, physiological and life-history consequences of particular strategies. ",A decade of movement ecology
3,1280195044876713986,4285487292,Yao Qin,"['Excited to share our new work with Xuezhi Wang, @alexbeutel and @edchi on exploring the relationship between adversarial robustness and uncertainty estimates and further use these correlations to improve uncertainty. Check our paper at <LINK>. <LINK>']",https://arxiv.org/abs/2006.16375,"Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated predictions, i.e., the predicted probability is not a good indicator of how much we should trust our model. In this paper, we study the connection between adversarial robustness and calibration and find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and calibration into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model calibration. ","Improving Calibration through the Relationship with Adversarial
  Robustness"
4,1280172183747256324,1241125617606823938,Pengyu Cheng,"['Seeking a powerful tool to reduce correlation in deep models? Check out our #ICML2020 paper (<LINK>) for a new upper bound of mutual information, which can serve as a critic to diminish the correlation between embeddings. <LINK>']",https://arxiv.org/abs/2006.12013,"Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce a MI minimization training scheme and further accelerate it with a negative sampling strategy. Simulation studies on Gaussian distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, demonstrate the effectiveness of the proposed method. The code is at this https URL ",CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information
5,1279875460374167552,101209471,Vini Netto 🇧🇷🇬🇧🇺🇲🇵🇹,['New paper \n“From form to information: analysing built environments in different spatial cultures”\n \n<LINK>\n\nWe explore Shannon entropy to measure levels of randomness in built form in\xa045 cities of the world.\n\n@NYU_CUSP @SpaceSyntaxNet @cityasnature @SenseableCity <LINK>'],https://arxiv.org/abs/2006.13897,"Cities are different around the world, but does this fact have any relation to culture? The idea that urban form embodies idiosyncrasies related to cultural identities captures the imagination of many in urban studies, but it is an assumption yet to be carefully examined. Approaching spatial configurations in the built environment as a proxy of urban culture, this paper searches for differences potentially consistent with specific regional cultures or cultures of planning in urban development. It does so focusing on the elementary components shaping cities: buildings and how they are aggregated in cellular complexes of built form. Exploring Shannon's work, we introduce an entropy measure to analyse the probability distribution of cellular arrangements in built form systems. We apply it to downtown areas of 45 cities from different regions of the world as a similarity measure to compare and cluster cities potentially consistent with specific spatial cultures. Findings suggest a classification scheme that sheds further light on what we call the ""cultural hypothesis"": the possibility that different cultures and regions find different ways of ordering space. ","From form to information: Analysing built environments in different
  spatial cultures"
6,1279673069901172736,1229357581,Florian Huber,['Our new paper on Bayesian Additive Vector Autoregressive Tree models is now available on arXiv (<LINK>). This is joint work with Luca Rossini (@VUamsterdam ).  Check it out if you are interested in semi-parametric time series models! #econometrics'],https://arxiv.org/abs/2006.16333,"Vector autoregressive (VAR) models assume linearity between the endogenous variables and their lags. This assumption might be overly restrictive and could have a deleterious impact on forecasting accuracy. As a solution, we propose combining VAR with Bayesian additive regression tree (BART) models. The resulting Bayesian additive vector autoregressive tree (BAVART) model is capable of capturing arbitrary non-linear relations between the endogenous variables and the covariates without much input from the researcher. Since controlling for heteroscedasticity is key for producing precise density forecasts, our model allows for stochastic volatility in the errors. We apply our model to two datasets. The first application shows that the BAVART model yields highly competitive forecasts of the US term structure of interest rates. In a second application, we estimate our model using a moderately sized Eurozone dataset to investigate the dynamic effects of uncertainty on the economy. ",Inference in Bayesian Additive Vector Autoregressive Tree Models
7,1279239019541532672,384900803,Shantanu Basu,['New paper on gravitational collapse and star formation with misaligned magnetic and rotation axes. Misalignment leads to larger disks and weaker outflows and complex magnetic field patterns. <LINK> @westernuPhysAst @KyushuUniv_EN #KagoshimaUniversity <LINK>'],https://arxiv.org/abs/2006.13233,"The formation of circumstellar disks is investigated using three-dimensional resistive magnetohydrodynamic simulations, in which the initial prestellar cloud has a misaligned rotation axis with respect to the magnetic field. We examine the effects of (i) the initial angle difference between the global magnetic field and the cloud rotation axis ($\theta_0$) and (ii) the ratio of the thermal to gravitational energy ($\alpha_0$). We study $16$ models in total and calculate the cloud evolution until $\sim \! 5000$ yr after protostar formation. Our simulation results indicate that an initial non-zero $\theta_0$ ($> 0$) promotes the disk formation but tends to suppress the outflow driving, for models that are moderately gravitationally unstable, $\alpha_0 \lesssim 1$. In these models, a large-sized rotationally-supported disk forms and a weak outflow appears, in contrast to a smaller disk and strong outflow in the aligned case ($\theta_0 = 0$). Furthermore, we find that when the initial cloud is highly unstable with small $\alpha_0$, the initial angle difference $\theta_0$ does not significantly affect the disk formation and outflow driving. ","The Effect of Misalignment between Rotation Axis and Magnetic Field on
  Circumstellar Disk"
8,1279135391019229185,77115855,Xuehai Qian,"['Check our new research paper on a new reversible cache coherence to defend transient attacks. It is neither a redo (InvisiSpec, SafeSpec) nor an undo (CleanupSpec) approach. BTW, it is the most sophisticated paper I have written 😃<LINK>']",https://arxiv.org/abs/2006.16535,"We propose the first Reversible Coherence Protocol (RCP), a new protocol designed from ground up that enables invisible speculative load. RCP takes a bold approach by including the speculative loads and merge/purge operation in the interface between processor and cache coherence, and allowing them to participate in the coherence protocol. It means, speculative load, ordinary load/store, and merge/purge can all affect the state of a given cache line. RCP is the first coherence protocol that enables the commit and squash of the speculative load among distributed cache components in a general memory hierarchy. RCP incurs an average slowdown of (3.0%,8.3%,7.4%) on (SPEC2006,SPEC2017,PARSEC), which is lower compared to (26.5%,12%,18.3%) in InvisiSpec and (3.2%,9.4%,24.2%) in CleanupSpec. The coherence traffic overhead is on average 46%, compared to 40% and 27% of InvisiSpec and CleanupSpec, respectively. Even with higher traffic overhead (~46%), the performance overhead of RCP is lower than InvisiSpec and comparable to CleanupSpec. It reveals a key advantage of RCP: the coherence actions triggered by the merge and purge operations are not in the critical path of the execution and can be performed in the cache hierarchy concurrently with processor execution ",A Case for Reversible Coherence Protocol
9,1278731782079352832,738769492122214400,Johannes Lischner,"['Ionic liquids have many interesting and useful properties. In our new paper, we analyze their electronic structure using the GW approach and find excellent agreement with #photoemission spectroscopy. Read here:  <LINK> #compchem <LINK>']",https://arxiv.org/abs/2006.16717,"Room temperature ionic liquids play an important role in many technological applications and a detailed understanding of their frontier molecular orbitals is required to optimize interfacial barriers, reactivity and stability with respect to electron injection and removal. In this work, we calculate quasiparticle energy levels of ionic liquids using first-principles many-body perturbation theory within the GW approximation and compare our results to various mean-field approaches, including semilocal and hybrid density-functional theory and Hartree-Fock. We find that the mean-field results depend qualitatively and quantitatively on the treatment of exchange-correlation effects, while GW calculations produce results that are in excellent agreement with experimental photoelectron spectra of gas phase ion pairs and ionic liquids. These results establish the GW approach as a valuable tool for understanding the electronic structures of ionic liquids. ",Frontier Orbitals and Quasiparticle Energy Levels in Ionic Liquids
10,1278722909268316168,988872626167828480,Zhiting Hu,"['New work connects GAN with Reinforcement Learning under a variational perspective, and stabilizes GAN training w/ off-the-shelf RL techniques\n\nStrongly improves image generation, text generation, text style transfer\n\npaper <LINK>\ncode <LINK>\n\n1/ <LINK>', 'We re-purpose the Proximal Policy Optimization (PPO)’s clipped surrogate objective for GAN generator training. \n\nThe objective discourages excessively large generator updates. \n\nFig: the update size is “trapped” around 1\n\n2/ https://t.co/IcCXy0rf4p', 'For discriminator, the perspective induces importance re-weighting that downplays low-quality fake samples, leading to significantly more stable training for discriminator (and thus generator). \n\nFig: lower variance in G and D losses throughout training\n\n3/ https://t.co/E7MgdA6qna', 'Improvement by applying the new training method on SOTA GAN architectures: \n- image generation (CIFAR10)\n- text generation\n- text style transfer\n\n4/ https://t.co/SNUnaYKogO', 'Joint work with Yue Wu (undergrad junior!), Pan Zhou, Andrew Wilson @andrewgwils, and Eric P Xing']",https://arxiv.org/abs/2006.06900,"Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) often suffer from inferior performance due to unstable training, especially for text generation. To solve this issue, we propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that improves discriminator training by downplaying bad-quality fake samples. Moreover, our variational GAN framework can provably overcome the training issue in many GANs that an optimal discriminator cannot provide any informative gradient to training generator. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation. ","Improving GAN Training with Probability Ratio Clipping and Sample
  Reweighting"
11,1278604126574637056,996840246,Syed Uddin,['Our new paper from the Carnegie Supernova Project.  <LINK>'],https://arxiv.org/abs/2006.15164,"We present optical and near-infrared ($ugriYJH$) photometry of host galaxies of Type Ia supernovae (SN~Ia) observed by the \textit{Carnegie Supernova Project-I}. We determine host galaxy stellar masses and, for the first time, study their correlation with SN~Ia standardized luminosity across optical and near-infrared ($uBgVriYJH$) bands. In the individual bands, we find that SNe~Ia are more luminous in more massive hosts with luminosity offsets ranging between $-0.07 \pm0.03$ mag to $-0.15\pm0.04$ mag after light-curve standardization. The slope of the SN~Ia Hubble residual-host mass relation is negative across all $uBgVriYJH$ bands with values ranging between $-0.036\pm 0.025$ mag/dex to $-0.097\pm 0.027$ mag/dex -- implying that SNe~Ia in more massive galaxies are brighter than expected. The near-constant observed correlations across optical and near-infrared bands indicate that dust may not play a significant role in the observed luminosity offset--host mass correlation. We measure projected separations between SNe~Ia and their host centers, and find that SNe~Ia that explode beyond a projected 10 kpc have a $\rm 30\% \ to \ 50\%$ reduction of the dispersion in Hubble residuals across all bands -- making them a more uniform subset of SNe~Ia. Dust in host galaxies, peculiar velocities of nearby SN~Ia, or a combination of both may drive this result as the color excesses of SNe~Ia beyond 10 kpc are found to be generally lower than those interior, but there is also a diminishing trend of the dispersion as we exclude nearby events. We do not find that SN~Ia average luminosity varies significantly when they are grouped in various host morphological types. Host galaxy data from this work will be useful, in conjunction with future high-redshift samples, in constraining cosmological parameters. ","The Carnegie Supernova Project-I: Correlation Between Type Ia Supernovae
  and Their Host Galaxies from Optical to Near-Infrared Bands"
12,1278497432335028224,466500823,Aman Chadha,['Announcing my new #AI paper on Video Super Resolution that builds on Recurrent Back Projection Networks using GANs with a four-fold loss. \n\nWe’re #1 on the Video Super Resolution leaderboard! 🙂\n \n<LINK> \n\n#DeepLearning  #ArtificialIntelligence #NeuralNetworks <LINK>'],https://arxiv.org/abs/2006.11161,"Recently, learning-based models have enhanced the performance of single-image super-resolution (SISR). However, applying SISR successively to each video frame leads to a lack of temporal coherency. Convolutional neural networks (CNNs) outperform traditional approaches in terms of image quality metrics such as peak signal to noise ratio (PSNR) and structural similarity (SSIM). However, generative adversarial networks (GANs) offer a competitive advantage by being able to mitigate the issue of a lack of finer texture details, usually seen with CNNs when super-resolving at large upscaling factors. We present iSeeBetter, a novel GAN-based spatio-temporal approach to video super-resolution (VSR) that renders temporally consistent super-resolution videos. iSeeBetter extracts spatial and temporal information from the current and neighboring frames using the concept of recurrent back-projection networks as its generator. Furthermore, to improve the ""naturality"" of the super-resolved image while eliminating artifacts seen with traditional algorithms, we utilize the discriminator from super-resolution generative adversarial network (SRGAN). Although mean squared error (MSE) as a primary loss-minimization objective improves PSNR/SSIM, these metrics may not capture fine details in the image resulting in misrepresentation of perceptual quality. To address this, we use a four-fold (MSE, perceptual, adversarial, and total-variation (TV)) loss function. Our results demonstrate that iSeeBetter offers superior VSR fidelity and surpasses state-of-the-art performance. ","iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks"
13,1278286425138569217,707867186971410432,Adrien Hourlier,"[""MiniBooNE's new paper with final dataset is out on arxiv! New background studies help disfavor some of the usual suspects.\n<LINK>\nThanks to everyone involved in this 17-year experiment!\nAlso, come listen to my talk tomorrow at the @nu2020_chicago conference!""]",https://arxiv.org/abs/2006.16883,"The MiniBooNE experiment at Fermilab reports a total excess of $638.0 \pm 132.8$ electron-like events ($4.8 \sigma$) from a data sample corresponding to $18.75 \times 10^{20}$ protons-on-target in neutrino mode, which is a 46\% increase in the data sample with respect to previously published results, and $11.27 \times 10^{20}$ protons-on-target in antineutrino mode. The additional statistics allow several studies to address questions on the source of the excess. First, we provide two-dimensional plots in visible energy and cosine of the angle of the outgoing lepton, which can provide valuable input to models for the event excess. Second, we test whether the excess may arise from photons that enter the detector from external events or photons exiting the detector from $\pi^0$ decays in two model independent ways. Beam timing information shows that almost all of the excess is in time with neutrinos that interact in the detector. The radius distribution shows that the excess is distributed throughout the volume, while tighter cuts on the fiducal volume increase the significance of the excess. We conclude that models of the event excess based on entering and exiting photons are disfavored. ","Updated MiniBooNE Neutrino Oscillation Results with Increased Data and
  New Background Studies"
14,1278266525082886148,2279045005,Samir Bhatt,"[""New paper <LINK>. There is, ofcourse, a keen interest in modelling COVID-19. Many use nonparametric curve fitting, and some will use SEIR type models. A widely used approach is to use the renewal equation. In this tweet i'll (heavily) summarize how it arises"", 'First, all these approaches are kinda the same. There is an equivalence of the renewal and the Erland SEIR (Champredon 2018). Hawkes processes end up as the Renewal in expectation (Rizoiu 2016). And AR(infinite) processes and Hawkes processes are equivalent (Kirchner 2016).', 'One could use all these approaches, but what we wanted to do was see how the renewal equation arises from a stochastic counting process. This way, we can really understand the assumptions behind this model by referencing the stochastic process. This is super important to do.', 'Turns out this problem was solved in 1948 but no less than Richard Bellman, that giant. However, the details in his seminal paper are sparse, and it very very hard to find the full derivation, especially with reference to epidemiology.', ""In this paper we derive it, and generalise two aspects. The derivation is not for the faint of heart, i'm certainly no mathematician and this was challenging (though i am sure easy for others). The derivation exposes how useful generating functions are."", 'Loosely, they allow you to create an object that contains everything about the stochastic process. Then when you want to find the expectation, simply take the derivative. When you do this from the age dependent branching process, you arrive at the renewal equation. Super cool.', 'This exposes some the limitations using the renewal equation. For example - the serial interval distribution is a fixed property and does not change with time. If we want to change it, that does not result in the renewal equation we use (i would love to know what happens though)', 'Now, what to do with all this? Well the renewal equation is particularly easy to use in a Bayesian framework and we include a link to some @mcmc_stan code and provide a simple example for COVID-19 South Korea. We hope this code will serve as a starting point for others.', 'I realise this is not exactly groundbreaking work, but I think having the derivation in all its full glory will interest many. For example, we basically wrote this paper for @ChristoPhraser as the starting point was his great PLoS paper (Fraser 2007).', 'I would like to use these kind of models in phylodynamic frameworks. Let us know if this is interesting or a stupid idea and we should stick with the traditional (and awesome) skyline approaches. @arambaut @erikmvolz', 'Thanks to other authors @creswapi @axel_gandy @flaxter @MellanTom @EttieUnwin  @krisparag1 And thanks as always to @SabineLvE  @MRC_Outbreak @Imperial_JIDEA  @CommunityJameel .']",https://arxiv.org/abs/2006.16487,"Renewal processes are a popular approach used in modelling infectious disease outbreaks. In a renewal process, previous infections give rise to future infections. However, while this formulation seems sensible, its application to infectious disease can be difficult to justify from first principles. It has been shown from the seminal work of Bellman and Harris that the renewal equation arises as the expectation of an age-dependent branching process. In this paper we provide a detailed derivation of the original Bellman Harris process. We introduce generalisations, that allow for time-varying reproduction numbers and the accounting of exogenous events, such as importations. We show how inference on the renewal equation is easy to accomplish within a Bayesian hierarchical framework. Using off the shelf MCMC packages, we fit to South Korea COVID-19 case data to estimate reproduction numbers and importations. Our derivation provides the mathematical fundamentals and assumptions underpinning the use of the renewal equation for modelling outbreaks. ","On the derivation of the renewal equation from an age-dependent
  branching process: an epidemic modelling perspective"
15,1278251956620988416,2209253130,Vito Dichio,"['As true scientists like to say: new paper out!\n(Actually, the first)\nA Gaussian Ansatz for Gene Chains outperforms the KNS Theory.\n<LINK>']",https://arxiv.org/abs/2006.16735,"We consider a population evolving due to mutation, selection and recombination, where selection includes single-locus terms (additive fitness) and two-loci terms (pairwise epistatic fitness). We further consider the problem of inferring fitness in the evolutionary dynamics from one or several snap-shots of the distribution of genotypes in the population. In the recent literature this has been done by applying the Quasi-Linkage Equilibrium (QLE) regime first obtained by Kimura in the limit of high recombination. Here we show that the approach also works in the interesting regime where the effects of mutations are comparable to or larger than recombination. This leads to a modified main epistatic fitness inference formula where the rates of mutation and recombination occur together. We also derive this formula using by a previously developed Gaussian closure that formally remains valid when recombination is absent. The findings are validated through numerical simulations. ","Inferring epistasis from genomic data with comparable mutation and
  outcrossing rate"
16,1278249579817791488,24534760,Richard Booth,"['Happy to announce my new paper with Jake Chandler on Revision by Conditionals, accepted for #KR2020. Version with proofs on arXiv. <LINK>']",https://arxiv.org/abs/2006.15811,The belief revision literature has largely focussed on the issue of how to revise one's beliefs in the light of information regarding matters of fact. Here we turn to an important but comparatively neglected issue: How might one extend a revision operator to handle conditionals as input? Our approach to this question of 'conditional revision' is distinctive insofar as it abstracts from the controversial details of how to revise by factual sentences. We introduce a 'plug and play' method for uniquely extending any iterated belief revision operator to the conditional case. The flexibility of our approach is achieved by having the result of a conditional revision by a Ramsey Test conditional ('arrow') determined by that of a plain revision by its corresponding material conditional ('hook'). It is shown to satisfy a number of new constraints that are of independent interest. ,Revision by Conditionals: From Hook to Arrow
17,1278125654303813638,907232486735958018,Jaki Noronha-Hostler,"['New paper (my first astro paper in collaboration with the group of Nico Yunes) on creating a family neutron star equations of state that can produce the maximum mass of 2.5-2.67Msun from GW190814. All the EOS we generated had quite interesting features.\n\n<LINK>', ""Basically to get such a large maximum mass (and remain causal) the EOS had to have a large rise in the speed of sound at baryon densities between 1.5-4 * nuclear saturation density. This leads to a kink in the EOS that doesn't fit within the extracted EOS from GW170817+GW190814. https://t.co/nzNERrWxQe"", ""Why doesn't it fit? It's because the spectral fit they used cannot capture kinks/bumps etc that arise from a large change in the degrees of freedom. In fact, when you try to fit one of our EOS with it, you get a drastically different Mass-radius relationship https://t.co/ab1wAovt6j"", ""We still remain agnostic if it's a neutron star or black hole, but we argue that from the EOS alone you cannot eliminate it as a neutron star.  We then determine the sensitivity needed to measure lambda to identify an object of M=2.5 Msun. (Lambda=0 is a black hole) https://t.co/R5Jk29i34X"", ""@ronbelmont Thanks! It was so much fun to write but it's been a whirlwind since we had the results for awhile now but knew we had to get the paper out very quickly in light of GW190814""]",https://arxiv.org/abs/2006.16296,"The observation of gravitational waves from an asymmetric binary opens the possibility for heavy neutron stars, but these pose challenges to models of the neutron star equation of state. We construct heavy neutron stars by introducing non-trivial structure in the speed of sound sourced by deconfined QCD matter, which cannot be well recovered by spectral representations. Their moment of inertia, Love number and quadrupole moment are very small, so a tenfold increase in sensitivity may be needed to test this possibility with gravitational waves, which is feasible with third generation detectors. ",Neutron Star Equation of State in light of GW190814
18,1278122949988569089,1186047738607263744,Rogemar A Riffel,"['New paper on arXiv:\xa0<LINK>\nProviding observational constraints in the multi-phase gas kinematics on 10-100 pc scales of galaxies is fundamental to better understand the role feedback from Active Galactic Nuclei (AGN) in the evolution of galaxies.', 'We use Gemini NIFS to map de hot molecular and ionized gas kinematics in the inner 500 pc of NGC1275, the brightest galaxy of the Perseus cluster. https://t.co/48wCobXBRQ', 'From the fitting of the CO absorption bandheads in the K-band, we derive a stellar velocity dispersion of\xa0 ~265\xa0 km/s , which implies a black hole mass of\xa0 ~1.1E9 solar masses. https://t.co/VHASWFxqW3', 'The gas kinematics present\xa0two components, one (narrow) due to gas at velocities close to the systemic\xa0velocity of the galaxy and another (broad) due to outflows produced by the central active nucleus. https://t.co/Mpna6Hn0sY', 'We find hot (T&gt; 1000 K) molecular and ionized outflows with velocities of up to 2000 km/s and mass outflow rates of 2.7E-2 solar masses/year and 1.6 solar masses/year, respectively in each of these gas phases.', 'The kinetic power of the ionized outflows corresponds to only 0.05% of the luminosity of the AGN of NGC 1275, indicating that they are not powerful enough to provide significant AGN feedback, but may be effective in redistributing the gas in the central region of the galaxy.', 'The H2 and [FeII] emission is produced mainly by shocks produced by the AGN winds, as revealed by the emission-line ratio diagnostic diagram and the observed kinematics. https://t.co/AZ8M83ezco', 'My collaborators in this work are @thaisa_sb (UFRGS),  Nadia Zakamska (JHU) and @RiffelRogerio (UFRGS).', '@venkatessh Thanks! Indeed, the ALMA data of NGC1275 were already published by Nagai et al. The ALMA CO maps show the compact molecular disk (&lt;100 pc), but not the outflow. Definitely, we should talk about ALMA data for some nearby Seyferts that we have NIFS data...']",https://arxiv.org/abs/2006.15198,"The role of feedback from Active Galactic Nuclei (AGN) in the evolution of galaxies is still not not fully understood, mostly due to the lack of observational constraints in the multi-phase gas kinematics on the ten to hundred parsec scales. We have used the Gemini Near-infrared Integral Field Spectrograph (NIFS) to map the molecular and ionized gas kinematics in the inner 900$\times$900 pc$^2$ of the Seyfert galaxy NGC1275 at a spatial resolution of $\sim$70 pc. From the fitting of the CO absorption bandheads in the K-band, we derive a stellar velocity dispersion of $265\pm26$ km s$^{-1}$, which implies a black hole mass of $M_{\rm SMBH}=1.1^{+0.9}_{-0.5}\times10^9$ M$_\odot$. We find hot ($T\gtrsim1000$ K) molecular and ionized outflows with velocities of up to 2 000 km s$^{-1}$ and mass outflow rates of $2.7\times10^{-2} {\rm M_\odot}$ yr$^{-1}$ and $1.6 {\rm M_\odot}$ yr$^{-1}$, respectively, in each of these gas phases. The kinetic power of the ionized outflows corresponds to only 0.05 per cent of the luminosity of the AGN of NGC 1275, indicating that they are not powerful enough to provide significant AGN feedback, but may be effective in redistributing the gas in the central region of the galaxy. The AGN driven outflows seem to be responsible for the shocks necessary to produce the observed H$_2$ and [Fe II] line emission. ",Ionized and hot molecular outflows in the inner 500 pc of NGC1275
19,1278025915683962880,804397363402067968,Timothy Raben,"['Cool new paper out <LINK>\n\nI\'m really proud of this one!\n\nWhat\'s the idea: build a lattice version of ""radial quantization"" in order to study a theory like the 3d Ising model.', 'This is the nth in a series of papers my collaborators have been writing about doing lattice field theory on non-euclidean manifolds.', ""What is radial quantization?\n\nBasically it's like the conformal map that takes you from the plane to the sphere (really a cylinder with spherical cross section)"", ""Why is it interesting? Traditional euclidean lattice FT has discrete transnational symmetry, but isn't rotationally invariant and you can't see large scales without using huge lattices (computationally a pain!)"", 'Radial quantization includes exponentially large scales and we show our QFE procedure restores rotational symmetry.', 'One of the fun things we found is that you have to include a Ricci curvature term to get ""good"" results. This term vanishes in the continuum limit, but including it leads to much, much faster convergence.', 'The end result is that we are ready to do real high precision tests of the 3d Ising Model. This is a complimentary approach to things like the ""conformal bootstrap"". We are directly probing the specific theory of 3d Ising.']",https://arxiv.org/abs/2006.15636,"The quantum extension of classical finite elements, referred to as quantum finite elements ({\bf QFE})~\cite{Brower:2018szu,Brower:2016vsl}, is applied to the radial quantization of 3d $\phi^4$ theory on a simplicial lattice for the $\mathbb R \times \mathbb S^2$ manifold. Explicit counter terms to cancel the one- and two-loop ultraviolet defects are implemented to reach the quantum continuum theory. Using the Brower-Tamayo~\cite{Brower:1989mt} cluster Monte Carlo algorithm, numerical results support the QFE ansatz that the critical conformal field theory (CFT) is reached in the continuum with the full isometries of $\mathbb R \times \mathbb S^2$ restored. The Ricci curvature term, while technically irrelevant in the quantum theory, is shown to dramatically improve the convergence opening, the way for high precision Monte Carlo simulation to determine the CFT data: operator dimensions, trilinear OPE couplings and the central charge. ",Radial Lattice Quantization of 3D $\phi^4$ Field Theory
20,1277997415031648256,36762530,Jesse Vig,"['Excited to announce ""BERTology Meets Biology: Interpreting Attention in Protein Language Models"" including new tool for visualizing attention in 3D protein structure (1/5):\n\nPaper: <LINK>\nBlog: <LINK>\nCode: <LINK> <LINK>', 'We show how BERT, trained simply to predict masked amino acids in a sequence, recovers the folding structure of proteins, attending to non-adjacent amino acids that are in contact in the folded protein (2/5): https://t.co/CJALWIJtf6', 'As a predictor of contacts, attention is well-calibrated: the attention weight in one head approximates the probability of two amino acids being in contact. (3/5) https://t.co/lWLxmq8dBa', 'BERT’s attention mechanism also targets binding sites, a key functional component of proteins (4/5): https://t.co/NiWlUF5orU', 'This is joint work with @thisismadani @lrvarshney @caimingxiong @richardsocher @nazneenrajani (5/5)']",https://arxiv.org/abs/2006.15222,"Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at this https URL ","BERTology Meets Biology: Interpreting Attention in Protein Language
  Models"
21,1277984408272855040,988872626167828480,Zhiting Hu,"['New work “Progressive Generation of Long Text”  — a super simple “non-monotonic” use of monotonic language models (eg. GPT2, BART) for generating coherent long text (1000 tokens)\n\npaper: <LINK>\ncode: <LINK> \n\n1/4 <LINK>', 'The approach first produces a sequence of most informative words, then progressively refines by adding finer-grained details in multiple stages, until completing a full passage. Importantly, each stage simply uses any pretrained left-to-right LMs with a little bit fine-tuning. 2/ https://t.co/1hMVwpB8WK', 'The approach is particularly efficient to fine-tune to generate domain-specific text, e.g., using only 10K text passages in the target domain. Strong improvement over GPT2 and BART w.r.t various metrics  3/4 https://t.co/oPtdQH0McE', 'Joint work w/ Bowen Tan, Zichao Yang, Maruan Al-Shedivat @alshedivat , Eric P. Xing   4/4']",https://arxiv.org/abs/2006.15720,"Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent. ",Progressive Generation of Long Text with Pretrained Language Models
22,1277949057466580994,88806960,Dr. Vivienne Baldassare,"['Check out our new paper exploring where dwarf galaxies fall on the M-sigma relation:  <LINK>  @accidentalnova @mgeha', 'We use @keckobservatory spectroscopy to measure stellar velocity dispersions for eight dwarf galaxies with measured black hole masses. Our work doubles the number of dwarf galaxies we can place on M-sigma!', 'We find that the dwarf galaxies are in good agreement with the relation found for higher mass galaxies, which is an important constraint for models of black hole formation and growth. https://t.co/vx1NCMjLS9', '@maximetrebitsch Thanks! They tend to fall below Mbh-Mstar, but they were found to agree pretty well with Mbh-Mbulge (in Schutte et al. 2019).']",https://arxiv.org/abs/2006.15150,"We present high resolution spectroscopy taken with the Keck Echellete Spectrograph and Imager to measure stellar velocity dispersions for eight active dwarf galaxies ($M_{\ast}<3\times10^{9}~M_{\odot}$) with virial black hole masses. We double the number of systems in this stellar mass regime with measurements of both black hole mass ($M_{\rm BH}$) and stellar velocity dispersion ($\sigma_{\ast}$), and place them on the $M_{\rm BH}-\sigma_{\ast}$ relation. The tight relation between $M_{\rm BH}$ and $\sigma_{\ast}$ for higher mass galaxies is a strong piece of evidence for the co-evolution of BHs and their host galaxies, but it has been unclear whether this relation holds in the dwarf galaxy regime. Our sample is in good agreement with the extrapolation of the $M_{\rm BH}-\sigma_{\ast}$ relation to low BH/galaxy masses, suggesting that the processes which produce $M_{\rm BH}-\sigma_{\ast}$ can also operate in dwarf galaxies. These results provide important constraints for massive black hole seed formation models and models exploring the impact of AGN feedback in dwarf galaxies. ",Populating the low-mass end of the $M_{\rm BH}-\sigma_{\ast}$ relation
23,1277912600047730691,3853426059,Stefan Söldner-Rembold,['[2006.16043] Long-baseline neutrino oscillation physics potential of the DUNE experiment - this new paper discusses \u2066@DUNEScience\u2069 sensitivity to CP violation and #neutrino mass ordering.  <LINK>'],https://arxiv.org/abs/2006.16043,"The sensitivity of the Deep Underground Neutrino Experiment (DUNE) to neutrino oscillation is determined, based on a full simulation, reconstruction, and event selection of the far detector and a full simulation and parameterized analysis of the near detector. Detailed uncertainties due to the flux prediction, neutrino interaction model, and detector effects are included. DUNE will resolve the neutrino mass ordering to a precision of 5$\sigma$, for all $\delta_{\mathrm{CP}}$ values, after 2 years of running with the nominal detector design and beam configuration. It has the potential to observe charge-parity violation in the neutrino sector to a precision of 3$\sigma$ (5$\sigma$) after an exposure of 5 (10) years, for 50\% of all $\delta_{\mathrm{CP}}$ values. It will also make precise measurements of other parameters governing long-baseline neutrino oscillation, and after an exposure of 15 years will achieve a similar sensitivity to $\sin^{2} 2\theta_{13}$ to current reactor experiments. ","Long-baseline neutrino oscillation physics potential of the DUNE
  experiment"
24,1277893538144047104,17819190,Vaishak Belle,"['New working paper -- Logic, Probability and Action: A Situation Calculus Perspective <LINK>']",https://arxiv.org/abs/2006.09868,"The unification of logic and probability is a long-standing concern in AI, and more generally, in the philosophy of science. In essence, logic provides an easy way to specify properties that must hold in every possible world, and probability allows us to further quantify the weight and ratio of the worlds that must satisfy a property. To that end, numerous developments have been undertaken, culminating in proposals such as probabilistic relational models. While this progress has been notable, a general-purpose first-order knowledge representation language to reason about probabilities and dynamics, including in continuous settings, is still to emerge. In this paper, we survey recent results pertaining to the integration of logic, probability and actions in the situation calculus, which is arguably one of the oldest and most well-known formalisms. We then explore reduction theorems and programming interfaces for the language. These results are motivated in the context of cognitive robotics (as envisioned by Reiter and his colleagues) for the sake of concreteness. Overall, the advantage of proving results for such a general language is that it becomes possible to adapt them to any special-purpose fragment, including but not limited to popular probabilistic relational models. ","Logic, Probability and Action: A Situation Calculus Perspective"
25,1277892879256600577,1562913787,Nathan Moynihan,"['New paper out today: <LINK>\n\nThanks, Corona!']",https://arxiv.org/abs/2006.15957,"Using the principles of the modern scattering amplitudes programme, we develop a formalism for constructing the amplitudes of three-dimensional topologically massive gauge theories and gravity. Inspired by recent developments in four dimensions, we construct the three-dimensional equivalent of $x$-variables for conserved matter currents coupled to topologically massive gauge bosons or gravitons. Using these, we bootstrap various matter-coupled gauge-theory and gravitational scattering amplitudes, and conjecture that topologically massive gauge theory and topologically massive gravity are related by the double copy. To motivate this idea further, we show explicitly that the Landau gauge propagator on the gauge theory side double copies to the de Donder gauge propagator on the gravity side. ","Scattering Amplitudes and the Double Copy in Topologically Massive
  Theories"
26,1277888145267331072,841031248839618560,Relja Arandjelović,"['Our new paper ""Self-Supervised MultiModal Versatile Networks"" learns from vision, audio and (ASR) language, achieves SOTA self-supervised video and audio representations, and we can deflate nets trained on videos to apply them on images. <LINK>', '@jalayrac, @arecasens, Rosalia Schneider, myself, @jramapuram, @JeffreyDeFauw, Lucas Smaira, @sedielem, Andrew Zisserman', '@rosaliags']",https://arxiv.org/abs/2006.16228,"Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available. ",Self-Supervised MultiModal Versatile Networks
27,1277857230017245184,1381415258,Roberto Garrappa,['Derivatives with non-singular kernel (such as CF and AB) have so many drawbacks that their use is strongly discouraged. The pre-print of a new paper in @FractalCOST activities is available <LINK>'],https://arxiv.org/abs/2006.15237,"In recent years, many papers discuss the theory and applications of new fractional-order derivatives that are constructed by replacing the singular kernel of the Caputo or Riemann-Liouville derivative by a non-singular (i.e., bounded) kernel. It will be shown here, through rigorous mathematical reasoning, that these non-singular kernel derivatives suffer from several drawbacks which should forbid their use. They fail to satisfy the fundamental theorem of fractional calculus since they do not admit the existence of a corresponding convolution integral of which the derivative is the left-inverse; and the value of the derivative at the initial time $t=0$ is always zero, which imposes an unnatural restriction on the differential equations and models where these derivatives can be used. For the particular cases of the so-called Caputo-Fabrizio and Atangana-Baleanu derivatives, it is shown that when this restriction holds the derivative can be simply expressed in terms of integer derivatives and standard Caputo fractional derivatives, thus demonstrating that these derivatives contain nothing new. ",Why fractional derivatives with nonsingular kernels should not be used
28,1277856196444200960,1177063549606203394,Tommi Tenkanen,"['A new #paper out! The title is ""The First Three Seconds: a Review of Possible Expansion Histories of the Early Universe"" and a #preprint can be found here: <LINK> 1/n <LINK>', 'As we say in the abstract, ""While the abundance of light elements indicates that the Universe was radiation dominated during Big Bang Nucleosynthesis (BBN), there is scant evidence that the Universe was radiation dominated prior to BBN."" 2/', '""It is therefore possible that the cosmological history was more complicated, with deviations from the standard radiation domination during the earliest epochs."" 3/n', 'In the paper we reviewed various possible causes and consequences of deviations from this ""radiation domination"" in the early Universe and the known constraints on them. 4/n', ""In particular, we reviewed several interesting proposals regarding the generation of #DarkMatter, matter-#antimatter asymmetry, #GravitationalWaves, primordial #BlackHole's, and #microhalo's. We hope that the review helps in guiding further research on these topics! 5/n"", 'Again, a #preprint is available here: https://t.co/GyQdnDl327. Finally, big thanks to all my co-authors including @carambolos, @ktfreese, @DanHooperAstro, @GordanKrnjaic, @VivPoulin, @160GHz, and @gswatson! 6/6']",https://arxiv.org/abs/2006.16182,"It is commonly assumed that the energy density of the Universe was dominated by radiation between reheating after inflation and the onset of matter domination 54,000 years later. While the abundance of light elements indicates that the Universe was radiation dominated during Big Bang Nucleosynthesis (BBN), there is scant evidence that the Universe was radiation dominated prior to BBN. It is therefore possible that the cosmological history was more complicated, with deviations from the standard radiation domination during the earliest epochs. Indeed, several interesting proposals regarding various topics such as the generation of dark matter, matter-antimatter asymmetry, gravitational waves, primordial black holes, or microhalos during a nonstandard expansion phase have been recently made. In this paper, we review various possible causes and consequences of deviations from radiation domination in the early Universe - taking place either before or after BBN - and the constraints on them, as they have been discussed in the literature during the recent years. ","The First Three Seconds: a Review of Possible Expansion Histories of the
  Early Universe"
29,1277844465261121536,21902101,Jim Geach,"['New paper today on arXiv led by @chrisclovell “Reproducing sub-mm number counts with cosmological hydrodynamic simulations” \n\n<LINK> \n\nwith @desikanarayanan, @RomeelDave and Qi Li']",https://arxiv.org/abs/2006.15156,"Matching the number counts of high-$z$ sub-millimetre-selected galaxies (SMGs) has been a long standing problem for galaxy formation models. In this paper, we use 3D dust radiative transfer to model the sub-mm emission from galaxies in the SIMBA cosmological hydrodynamic simulations, and compare predictions to the latest single-dish observational constraints on the abundance of 850$\mathrm{\mu m}$-selected sources. We find good agreement with the shape of the integrated 850$\mathrm{\mu m}$ luminosity function, and the normalisation is within 0.25 dex at $> 3 \; \mathrm{mJy}$, unprecedented for a fully cosmological hydrodynamic simulation, along with good agreement in the redshift distribution of bright SMGs. The agreement is driven primarily by SIMBA's good match to infrared measures of the star formation rate (SFR) function between $z = 2-4$ at high SFRs. Also important is the self-consistent on-the-fly dust model in SIMBA, which predicts, on average, higher dust masses (by up to a factor of 2.5) compared to using a fixed dust-to-metals ratio of 0.3. We construct a lightcone to investigate the effect of far-field blending, and find that 52% of sources are blends of multiple components, which makes a small contribution to the normalisation of the bright-end of the number counts. We provide new fits to the 850$\mathrm{\mu m}$ luminosity as a function of SFR and dust mass. Our results demonstrate that exotic solutions to the discrepancy between sub-mm counts in simulations and observations, such as a top-heavy IMF, are unnecessary, and that sub-millimetre-bright phases are a natural consequence of massive galaxy evolution. ","Reproducing sub-millimetre galaxy number counts with cosmological
  hydrodynamic simulations"
30,1277636547869921285,958953704568295424,Carlos Esteves,"['In this new paper, we provide theoretical and empirical arguments about the best way to do deep pose regression: <LINK>\n\nWith @kiamada, @jakelevMtl, @KefanChen7, @Jimantha, @akanazawa, and @rostamiz. <LINK>']",http://arxiv.org/abs/2006.14616,"Symmetric orthogonalization via SVD, and closely related procedures, are well-known techniques for projecting matrices onto $O(n)$ or $SO(n)$. These tools have long been used for applications in computer vision, for example optimal 3D alignment problems solved by orthogonal Procrustes, rotation averaging, or Essential matrix decomposition. Despite its utility in different settings, SVD orthogonalization as a procedure for producing rotation matrices is typically overlooked in deep learning models, where the preferences tend toward classic representations like unit quaternions, Euler angles, and axis-angle, or more recently-introduced methods. Despite the importance of 3D rotations in computer vision and robotics, a single universally effective representation is still missing. Here, we explore the viability of SVD orthogonalization for 3D rotations in neural networks. We present a theoretical analysis that shows SVD is the natural choice for projecting onto the rotation group. Our extensive quantitative analysis shows simply replacing existing representations with the SVD orthogonalization procedure obtains state of the art performance in many deep learning applications covering both supervised and unsupervised training. ",An Analysis of SVD for Deep Rotation Estimation
31,1277604213527384069,4639078397,John Wise,"['New paper day! Led by @jaregan (co-authors @bwoshea, Haiman, &amp; Norman). We found that nearly 40% of extremely metal-poor halos can support supermassive star formation during the Epoch of Reionization, perhaps leading to massive black hole seeds <LINK> <LINK>', 'We searched the Renaissance Simulations for atomic cooling halos with high infall rates (&gt;0.1 Msun/yr), building off the ideas in Chon &amp; Omukai (2020) and Tagawa+ (2020) that supermassive stars can form in metal-enriched gas.\n\nhttps://t.co/8yEJxUKrNr\nhttps://t.co/1tilTMOE8Y', ""Whether they can form in each halo remains to be seen, but the fuel is there. It's either going to form a supermassive star, a nuclear star cluster, and/or rapid accretion onto an existing central black hole. We're investigating a couple interesting cases, so stay tuned!""]",https://arxiv.org/abs/2006.14625,"The formation of supermassive stars has generally been studied under the assumption of rapid accretion of pristine metal-free gas. Recently it was found, however, that gas enriched to metallicities up to $Z \sim 10^{-3}$ Z$_{\odot}$ can also facilitate supermassive star formation, as long as the total mass infall rate onto the protostar remains sufficiently high. We extend the analysis further by examining how the abundance of supermassive star candidate haloes would be affected if all haloes with super-critical infall rates, regardless of metallicity were included. We investigate this scenario by identifying all atomic cooling haloes in the Renaissance simulations with central mass infall rates exceeding a fixed threshold. We find that among these haloes with central mass infall rates above 0.1 M$_{\odot}$ yr$^{-1}$ approximately two-thirds of these haloes have metallicities of $Z > 10^{-3}$ Z$_{\odot}$. If metal mixing within these haloes is inefficient early in their assembly and pockets of metal-poor gas can remain then the number of haloes hosting supermassive stars can be increased by at least a factor of four. Additionally the centres of these high infall-rate haloes provide ideal environments in which to grow pre-existing black holes. Further research into the (supermassive) star formation dynamics of rapidly collapsing haloes, with inhomogeneous metal distributions, is required to gain more insight into both supermassive star formation in early galaxies as well as early black hole growth. ",Massive Star Formation in Metal-Enriched Haloes at High Redshift
32,1277595531108397057,297011825,Jacob Deasy,"['Check out my new paper ""Constraining Variational Inference with Geometric Jensen-Shannon Divergence"" <LINK> with @simidjievskin and @pl219_Cambridge! We alter GJS to interpolate between forward and reverse KL and improve VAE reconstruction for intermediate skew! <LINK>', 'Theoretically, we show how skewing our variant of GJS corresponds to quadratically weighting forward and reverse KL, centred around an interpretable (closed-form) distribution, while permitting symmetry at alpha=0.5! Code is available here: https://t.co/gDvUZfAp8C https://t.co/3n1FzjhkQd', 'Empirically, we demonstrate how optimal values of skew (alpha) allow for better reconstruction than common VAE variants across datasets, without simply minimising divergence regularisation! (Table shows test-set reconstruction loss) https://t.co/z78cZanmNs']",https://arxiv.org/abs/2006.10599,"We examine the problem of controlling divergences for latent space regularisation in variational autoencoders. Specifically, when aiming to reconstruct example $x\in\mathbb{R}^{m}$ via latent space $z\in\mathbb{R}^{n}$ ($n\leq m$), while balancing this against the need for generalisable latent representations. We present a regularisation mechanism based on the skew-geometric Jensen-Shannon divergence $\left(\textrm{JS}^{\textrm{G}_{\alpha}}\right)$. We find a variation in $\textrm{JS}^{\textrm{G}_{\alpha}}$, motivated by limiting cases, which leads to an intuitive interpolation between forward and reverse KL in the space of both distributions and divergences. We motivate its potential benefits for VAEs through low-dimensional examples, before presenting quantitative and qualitative results. Our experiments demonstrate that skewing our variant of $\textrm{JS}^{\textrm{G}_{\alpha}}$, in the context of $\textrm{JS}^{\textrm{G}_{\alpha}}$-VAEs, leads to better reconstruction and generation when compared to several baseline VAEs. Our approach is entirely unsupervised and utilises only one hyperparameter which can be easily interpreted in latent space. ","Constraining Variational Inference with Geometric Jensen-Shannon
  Divergence"
33,1277490762893139975,4631815281,Stefano Mandelli,"['My new paper about chemically etched D-band feed-horns array is now on arxiv! The chemical etching techinique is really smart! Now, we have to figure out how to improve it and increase the feed-horns density!\n<LINK>\n#cmb #feedhorns #150GHz #Dband']",https://arxiv.org/abs/2006.14889,"We present the design, manufacturing, and testing of a 37-element array of corrugated feedhorns for Cosmic Microwave Background (CMB) measurements between $140$ and $170$ GHz. The array was designed to be coupled to Kinetic Inductance Detector arrays, either directly (for total power measurements) or through an orthomode transducer (for polarization measurements). We manufactured the array in platelets by chemically etching aluminum plates of $0.3$ mm and $0.4$ mm thickness. The process is fast, low-cost, scalable, and yields high-performance antennas compared to other techniques in the same frequency range. Room temperature electromagnetic measurements show excellent repeatability with an average cross polarization level about $-20$ dB, return loss about $-25$ dB, first sidelobes below $-25$ dB and far sidelobes below $-35$ dB. Our results qualify this process as a valid candidate for state-of-the-art CMB experiments, where large detector arrays with high sensitivity and polarization purity are of paramount importance in the quest for the discovery of CMB polarization $B$-modes. ","A chemically etched corrugated feedhorn array for D-band CMB
  observations"
34,1277480616578166784,2563532985,Prof Anna Watts,"['New paper!  ""Deep model simulation of polar vortices in gas giant atmospheres"", by Garcia, Chambers &amp; Watts. <LINK>', ""We've been working on convection in neutron star oceans (rapid rotation, pattern formation etc).  Turns out some of our findings may be relevant to the formation of coherent polar cyclonic vortices on gas giants, like those seen on Saturn and Jupiter."", ""Quite an excursion from my normal research track!  But it's been really fun.  And a fluid is a fluid is a fluid....."", '@ThomsLrn Thanks! Equatorially asymmetric waves develop from the\nonset in the selected regime (see also our 2019 PRF paper) which may explain the lack of symmetry in our models. Opposite symmetry is also possible, as equations are invariant with respect to reflections in equatorial plane.', 'Plus the figures are gorgeous. https://t.co/PeAYAVgPif']",https://arxiv.org/abs/2006.14817,"The Cassini and Juno probes have revealed large coherent cyclonic vortices in the polar regions of Saturn and Jupiter, a dramatic contrast from the east-west banded jet structure seen at lower latitudes. Debate has centered on whether the jets are shallow, or extend to greater depths in the planetary envelope. Recent experiments and observations have demonstrated the relevance of deep convection models to a successful explanation of jet structure and cyclonic coherent vortices away from the polar regions have been simulated recently including an additional stratified shallow layer. Here we present new convective models able to produce long-lived polar vortices. Using simulation parameters relevant for giant planet atmospheres we find flow regimes that are in agreement with geostrophic turbulence (GT) theory in rotating convection for the formation of large scale coherent structures via an upscale energy transfer fully three-dimensional. Our simulations generate polar characteristics qualitatively similar to those seen by Juno and Cassini: they match the structure of cyclonic vortices seen on Jupiter; or can account for the existence of a strong polar vortex extending downwards to lower latitudes with a marked spiral morphology and the hexagonal pattern seen on Saturn. Our findings indicate that these vortices can be generated deep in the planetary interior. A transition differentiating these two polar flows regimes is described, interpreted in terms of different force balances and compared with previous shallow atmospheric models which characterised polar vortex dynamics in giant planets. In addition, the heat transport properties are investigated confirming recent scaling laws obtained in the context of reduced models of GT. ",Deep model simulation of polar vortices in gas giant atmospheres
35,1277468429323251713,2819715191,Antonella Palmese,"['My new paper on a statistical standard siren measurement of the Hubble constant using gravitational wave events GW190814 GW170814 from @LIGO @ego_virgo and @theDESurvey galaxies, with improved photoz treatment \n<LINK> <LINK>', 'Well localized GW events without counterpart (dark standard sirens) can provide marginal improvement to an H0 measurement from those with a counterpart, provided that a complete galaxy catalog exists in the localization area.', 'Things will get very interesting as we combine hundreds of dark sirens!']",https://arxiv.org/abs/2006.14961v1,"We present a measurement of the Hubble constant $H_0$ using the gravitational wave (GW) event GW190814, which resulted from the coalescence of a 23 $M_\odot$ black hole with a 2.6 $M_\odot$ compact object, as a standard siren. No compelling electromagnetic counterpart with associated host galaxy has been identified for this event, thus our analysis accounts for $\sim$ 2,700 potential host galaxies within a statistical framework. The redshift information is obtained from the photometric redshift (photo-$z$) catalog from the Dark Energy Survey. The luminosity distance is provided by the gravitational wave sky map published by the LIGO/Virgo Collaboration. Since this GW event has the second-smallest sky localization area after GW170817, GW190814 is likely to provide the best constraint on cosmology from a single standard siren without identifying an electromagnetic counterpart. Our analysis uses photo-$z$ probability distribution functions and corrects for photo-$z$ biases. We also reanalyze the binary-black hole GW170814 within this updated framework. We explore how our findings impact the $H_0$ constraints from GW170817, the only GW merger associated with a unique host galaxy, and therefore the most powerful standard siren to date. From a combination of GW190814, GW170814 and GW170817, our analysis yields $H_0 = 69.0^{+ 14}_{- 7.5 }~{\rm km~s^{-1}~Mpc^{-1}}$ (68% Highest Density Interval, HDI) for a prior in $H_0$ uniform between $[20,140]~{\rm km~s^{-1}~Mpc^{-1}}$. The addition of GW190814 and GW170814 to GW170817 improves the 68% HDI from GW170817 alone by $\sim 12\%$, showing how well-localized mergers without counterparts can provide a marginal contribution to standard siren measurements, provided that a complete galaxy catalog is available at the location of the event. ","] A statistical standard siren measurement of the Hubble constant from the
  LIGO/Virgo gravitational wave compact object merger GW190814 and Dark Energy
  Survey galaxies"
36,1276635470215159809,2427184074,Christopher Berry,"['New @NUCIERA paper with @spacedontwait, Mario Spera &amp; Vicky Kalogera asking if we can form #GW190814 via isolated binary evolution?\n<LINK>\n\n*Spoiler* It seems we are missing some physics in binary evolution\n\n🧵1/7 <LINK>', ""To simulate our populations of compact binaries, we used the COSMIC rapid binary population synthesis code https://t.co/uf7B92rFzs\n\nIt's easy to install using pip if you want to try it yourself (https://t.co/I29YEYKON1)\n\n2/7"", 'The mapping between the initial mass of a star and the remnant it leaves behind depends on the supernova mechanism. This plot shows the relation for single stars of different metallicities for the Rapid and Delayed supernova prescriptions based on https://t.co/iiumB9c4Mv\n\n3/7 https://t.co/7IPT6GAKgR', ""The Rapid prescription was designed to accommodate the idea of a mass gap between neutron stars and black holes. GW190814's lighter component was in the mass gap. Can binary physics save the Rapid prescription by moving something into the gap via accretion? Nope!\n\n4/7"", ""To end up with a 23 solar mass black hole (The larger component of GW190814) you need a massive star. These lead short lives, meaning that you can't accrete much mass onto a neutron star, perhaps 0.1 solar masses at most. There's a mass gap when using the Rapid prescription\n\n5/7 https://t.co/5UJrmlgT62"", 'Can we make GW190814-like systems using the Delayed supernova prescription? Yes! \n\nThere are two main channels. A) The black hole forms first B) The lighter component forms first as there is so much mass transfer we get an inversion!\n\n6/7\n\nhttps://t.co/MPzbSaEcpW https://t.co/GAyhishdrU', 'But, even with the Delayed supernova prescription, GW190814-like systems are rare. Too rare to explain the observed rate from @LIGO &amp; @ego_virgo. Could another channel be at play? Are we missing some key physics? The mystery continues…\n\n7/7 https://t.co/kxNRWpMAf9', '@astro_jje @LIGO @ego_virgo I would certainly be interested.\n\nWe were thinking of perhaps seeing if you might be interested in giving a remote talk at some point?', '@astro_jje @LIGO @ego_virgo And similarly, please let us know if you have comments on the paper!']",https://arxiv.org/abs/2006.14573,"On August 14, 2019, the LIGO and Virgo detectors observed GW190814, a gravitational-wave signal originating from the merger of a $\simeq 23 M_\odot$ black hole with a $\simeq 2.6 M_\odot$ compact object. GW190814's compact-binary source is atypical both in its highly asymmetric masses and in its lower-mass component lying between the heaviest known neutron star and lightest known black hole in a compact-object binary. If formed through isolated binary evolution, the mass of the secondary is indicative of its mass at birth. We examine the formation of such systems through isolated binary evolution across a suite of assumptions encapsulating many physical uncertainties in massive-star binary evolution. We update how mass loss is implemented for the neutronization process during the collapse of the proto-compact object to eliminate artificial gaps in the mass spectrum at the transition between neutron stars and black holes. We find it challenging for population modeling to match the empirical rate of GW190814-like systems whilst simultaneously being consistent with the rates of other compact binary populations inferred by gravitational-wave observations. Nonetheless, the formation of GW190814-like systems at any measurable rate requires a supernova engine model that acts on longer timescales such that the proto-compact object can undergo substantial accretion immediately prior to explosion, hinting that if GW190814 is the result of massive-star binary evolution, the mass gap between neutron stars and black holes may be narrower or nonexistent. ","Exploring the Lower Mass Gap and Unequal Mass Regime in Compact Binary
  Evolution"
37,1276554901338894337,75574231,Van Dijk Lab,"['New paper from our lab: Quantum Potential Neural Networks (QPNN) -- We approximate the Hamiltonians of quantum systems by learning a potential function using deep neural networks given one of their wave-functions or data distribution. By postdoc Arijit <LINK> <LINK>', '@vanbettauer Thanks!\nCredit really goes to my postdoc Arijit and collaborators who are experts in QM']",https://arxiv.org/abs/2006.13297,"Attempts to apply Neural Networks (NN) to a wide range of research problems have been ubiquitous and plentiful in recent literature. Particularly, the use of deep NNs for understanding complex physical and chemical phenomena has opened a new niche of science where the analysis tools from Machine Learning (ML) are combined with the computational concepts of the natural sciences. Reports from this unification of ML have presented evidence that NNs can learn classical Hamiltonian mechanics. This application of NNs to classical physics and its results motivate the following question: Can NNs be endowed with inductive biases through observation as means to provide insights into quantum phenomena? In this work, this question is addressed by investigating possible approximations for reconstructing the Hamiltonian of a quantum system in an unsupervised manner by using only limited information obtained from the system's probability distribution. ",Learning Potentials of Quantum Systems using Deep Neural Networks
38,1276534735435632644,1012689495420833792,Simon Powers,"['How can we balance electricity demand across households, and flatten the consumption curve, in a fair way? We need to do this to fully exploit renewable energy sources. Our new paper, to appear in @2020ALIFE, develops a multi-agent systems approach: <LINK>', 'Also shouting out an acknowledgement to @KeeleILAS for supporting this work.']",https://arxiv.org/abs/2006.14526,"Reducing the peak energy consumption of households is essential for the effective use of renewable energy sources, in order to ensure that as much household demand as possible can be met by renewable sources. This entails spreading out the use of high-powered appliances such as dishwashers and washing machines throughout the day. Traditional approaches to this problem have relied on differential pricing set by a centralised utility company. But this mechanism has not been effective in promoting widespread shifting of appliance usage. Here we consider an alternative decentralised mechanism, where agents receive an initial allocation of time-slots to use their appliances and can then exchange these with other agents. If agents are willing to be more flexible in the exchanges they accept, then overall satisfaction, in terms of the percentage of agents time-slot preferences that are satisfied, will increase. This requires a mechanism that can incentivise agents to be more flexible. Building on previous work, we show that a mechanism incorporating social capital - the tracking of favours given and received - can incentivise agents to act flexibly and give favours by accepting exchanges that do not immediately benefit them. We demonstrate that a mechanism that tracks favours increases the overall satisfaction of agents, and crucially allows social agents that give favours to outcompete selfish agents that do not under payoff-biased social learning. Thus, even completely self-interested agents are expected to learn to produce socially beneficial outcomes. ",A mechanism to promote social behaviour in household load balancing
39,1276443156796641280,251623514,Peter McGill,"[""New paper out today (<LINK>) with Andrew Everall, Douglas Boubert (@neuronomer), and Leigh Smith - accepted by MNRAS. \n\nI'll try my best to explain  :-) Thread: [1/24] <LINK>"", 'Gaia DR2 astrometry has allowed the prediction of alignments of stars to such great precision that we can actually predict when a gravitational lensing event will happen\n\nIn some cases, these events provide unique opportunities to extract gravitational masses of the lens [2/24]', 'Gaia DR2 reignited interest in trying to predict these alignments and has led to ~ 5700 predicted events occurring over the next century by many different studies\n\nThe question we investigate is - Can all these predictions be trusted? [3/24]', ""Firstly,  it's worth mentioning that the vast majority of predictions by all studies with maximums far into the future are fine. \n\nWe tried to get to the bottom of some puzzling characteristics of events that Gaia would hopefully observe! [4/24]"", 'The story begins with the predicted event caused by the lens G123-61A. Here is a DSS2 image of the field around the event. \n\nYou can see the lens (square) heading towards the background source (circle), and some unrelated DR2 sources (triangles) [5/24] https://t.co/xYeLHyR7AF', 'Gaia G-band magnitudes are annotated text. But where is the 13th mag background source? [6/24]', 'This missing source also happens to be the background source for another predicted event! Turns out Gaia detected what is likely a binary component of the lens (G123-61B) and they both align with the missing background source causing separate events [7/24]', 'Here is a 2MASS image of the same field. Both the positions of G123-61A (square) and G123-61B (triangle) at the image epoch (2000.0) and the DR2 reference epoch (2015.5) are marked\n\nThe missing source (circle) is sandwiched between G123-61A &amp; B at the Gaia reference epoch [8/24] https://t.co/tghIj2Vstd', 'The source also has a very similar magnitude and colour to the lens G123-61A. The number of times this system should have been observed with Gaia is ~ 65 \n\nThat is, G123-61A,  G123-61B, and the background source should all have (~65) detections [9/24]', ""However, the number of detections are G123-61A (54), G123-61B (42), and the background source (34). \n\nIt's no coincidence that 54 + 42 + 34 = 130 (2x65)  - the total number of detections we would expect for just the binary lens system! [10/24]"", 'We suspect this was a very rare and difficult case for the clustering algorithm used to assign detections to different sources. \n\nIn this case, it looks like some of the detections of the binary lens system were donated to make a likely spurious third source. [11/24]', 'We found some more examples of predicted events where the source appears to be missing so we decided to investigate. \n\nHere are plots of the source-lens magnitude and colour difference as a function Psi (~ measure of how “noisy” the astrometry of the source is) [12/24] https://t.co/kMDmhZGWJw', 'We can see that the vast majority of events are fine, with the background source in these cases generally having a more ""noisy"" astrometric solution if it is fainter [13/24]', 'However, we can see a cluster of events where the source and lens have very similar magnitudes and colour, and in these cases, the sources have very ""noisy"" astrometry for their magnitude [14/24]', 'The vertical line in the plot is one of the thresholds Gaia uses to give a source either 5D or 2D astrometry. \n\nThe cluster of suspicious events all have very bright sources where the astrometry was noisy enough to not give the source 5D astrometry [15/24]', 'We recommend only considering events if the background source has 5D astrometry for events with background sources brighter than G=18 [16/24]', 'While there are only some events in this cluster, these happen to be the best events for detection with Gaia\n\nThis is because the source appears to be quite bright, which is crucial for Gaia to obtain a precise measurement of the lensing signal [17/24]', 'This recommendation eliminates 61% of the predicted events forecast to happen over the Gaia mission with bright sources, including some of the best candidates where we would expect to get precise mass measurements of the lens  [18/24]', 'Requiring that the background source has 5D astrometry also remedies another puzzling aspect of the predicted events \n\nHere is a plot of the distribution of the time of the closest approach for the events [19/24] https://t.co/9bmeOyN5oE', 'We’d expect the rate of events to be constant with a dip around the DR2 reference epoch (2015.5) where the lens and source were too close for Gaia to resolve them, so the event could not be predicted\n\nBut we can see from the plot the rate peaks either side of 2015.5 (Grey)[20/24]', 'If we apply our cut (blue), this gets rid of these sharp peaks. Excitingly this implies that ~ half the predictable microlensing events which Gaia will detect have yet to be identified! \n\nThese events may be found in the later Gaia data releases [21/24]', 'We suspect that in many of the cases of the events we eliminate, the background source is likely to be just a marginal detection of a binary component of the lens and a case like G123-61 is very rare! [22/24]', 'Finally, it goes without saying that all the Gaia data is amazing, and the vast majority of microlensing predictions made with it are fine. We found out you have to be **really** careful when you’re trying to predict microlensing events near to the Gaia reference epoch! [23/24]', 'This work made lots of use of Douglas + Andy’s  + et al Completeness of Gaia-verse project work which you can check out here -  https://t.co/V7jZpQEW7E\n\nAlso huge thanks to the Gaia Help desk, who really helped us understand all of this [24/24]', ""@cosmos4u planning, I can't tweet on the fly it's hard haha""]",https://arxiv.org/abs/2006.13958,"Precision astrometry from the second Gaia data release has allowed astronomers to predict 5,787 microlensing events, with 528 of these having maximums within the extended Gaia mission (J2014.5 - J2026.5). Future analysis of the Gaia time-series astrometry of these events will, in some cases, lead to precise gravitational mass measurements of the lens. We find that 61% of events predicted during the extended Gaia mission with sources brighter than G = 18 are likely to be spurious, with the background source in these cases commonly being either a duplicate detection or a binary companion of the lens. We present quality cuts to identify these spurious events and a revised list of microlensing event candidates. Our findings imply that half of the predictable astrometric microlensing events during the Gaia mission have yet to be identified. ",Predictions of Gaia's prize microlensing events are flawed
40,1276436332047413249,60084334,Frank Verstraete,"['Simulating frustrated spin systems with tensor networks can lead to obstacles similar to the sign problem in Monte Carlo. Our new paper shows how to overcome those: <LINK>', ""@IRFDMRG Thanks for the reference! As far as i understand, Kanamori's method does not give any information about the ground state rules and hence does  not allow to calculate the residual entropy. It might well  be a dual formulation  of the problem.""]",http://arxiv.org/abs/2006.14341,"Motivated by the recent success of tensor networks to calculate the residual entropy of spin ice and kagome Ising models, we develop a general framework to study frustrated Ising models in terms of infinite tensor networks %, i.e. tensor networks that can be contracted using standard algorithms for infinite systems. This is achieved by reformulating the problem as local rules for configurations on overlapping clusters chosen in such a way that they relieve the frustration, i.e. that the energy can be minimized independently on each cluster. We show that optimizing the choice of clusters, including the weight on shared bonds, is crucial for the contractibility of the tensor networks, and we derive some basic rules and a linear program to implement them. We illustrate the power of the method by computing the residual entropy of a frustrated Ising spin system on the kagome lattice with next-next-nearest neighbour interactions, vastly outperforming Monte Carlo methods in speed and accuracy. The extension to finite-temperature is briefly discussed. ",Solving frustrated Ising models using tensor networks
41,1276398565825314818,1014465993463205888,Domenico Barbato,"[""My new paper is finally accepted for publication! This is the last work I've began during my PhD in Turin, and after a whole year spent on analysing the data again and again, writing and rewriting of whole section, here it is!\n\n<LINK>"", '@DidierQueloz Thanks -- this was more meant as a ""personal milestone"" tweet so to speak and less as a paper advertisement. End of an era, in a way. Will surely keep that in mind for the next papers!', '@LouiseDyregaard Thanks! Yes, finally all my old works are done -- ready for new ones at full speed!']",https://arxiv.org/abs/2006.14393,"With the growth of comparative exoplanetology, it is increasingly clear that investigating the relationships between inner and outer planets plays a key role in discriminating between competing formation and evolution models. To do so, it is important to probe the inner region of systems hosting long-period giants in search for undetected lower-mass planetary companions. In this work we present our results on the K-dwarf star BD-11~4672, already known to host a long-period giant planet, as the first output of a subsample of the GAPS programme specifically aimed at assessing the impact of inefficient migration of planets formed beyond the snowline by searching for Neptune-mass and super-Earths planetary companions of known longer-period giants. The high-precision HARPS-N observations of BD-11~4672 are used in conjunction with literature time series in order to search for additional inner planetary signals to be fitted using differential evolution Markov chain Monte Carlo. The long-term stability of the new orbital solutions is tested by N-body dynamical simulations. We report the detection of BD-11~4672~c, a new Neptune-mass planet with an orbital period of $74.20_{-0.08}^{+0.06}\,\rm{d}$, eccentricity $0.40_{-0.15}^{+0.13}$, semimajor axis $0.30\pm0.01\,\rm{au}$ and minimum mass $15.37_{-2.81}^{+2.97}\,M_\oplus$ orbiting slightly outside the inner edge of the optimistic circumstellar habitable zone. In order to assess its impact on the dynamical stability of the habitable zone we compute the angular momentum deficit of the system, showing that planet c has a severe negative impact on the stability of possible additional lower-mass temperate planets. The BD-11~4672 system is notable for its architecture, hosting both a long-period giant planet and an inner lower-mass planet, the latter being also among the most eccentric Neptune-mass planets known at similar periods. ","The GAPS Programme at TNG -- XXIV. An eccentric Neptune-mass planet near
  the inner edge of the BD-11 4672 habitable zone"
42,1276381392981700610,802543221943439360,Andrea Caputo,"['<LINK>\nAmbulance chasing is for boys, ambulance chasing with a 50 pages paper is for men! Take a look at this new huge effort #physics #DarkMatter #XENON <LINK>', 'What we do? \n-- develop an unbinned Monte Carlo analysis and improve the energy reconstruction of the XENON1T events;\n-- highlight, surprisingly for the first time, that all these solar guys may be massive, changing the fluxes and the fits;', '-- propose to study Primakoff also for scalars;\n-- propose a simple toy model to avoid stellar constraints from dense stars like WD and RG;\n-- consider scattering off cosmic rays\n-- consider exothermic dark matter with a keV splitting.']",https://arxiv.org/abs/2006.14521,"Motivated by the recent XENON1T results, we explore various new physics models that can be discovered through searches for electron recoils in O(keV)-threshold direct-detection experiments. First, we consider the absorption of light bosons, either as dark matter relics or being produced directly in the Sun. In the latter case, we find that keV mass bosons produced in the Sun provide an adequate fit to the data but are excluded by stellar cooling constraints. We address this tension by introducing a novel Chameleon-like axion model, which can explain the excess while evading the stellar bounds. We find that absorption of bosonic dark matter provides a viable explanation for the excess only if the dark matter is a dark photon or an axion. In the latter case, photophobic axion couplings are necessary to avoid X-ray constraints. Second, we analyze models of dark matter-electron scattering to determine which models might explain the excess. Standard scattering of dark matter with electrons is generically in conflict with data from lower-threshold experiments. Momentum-dependent interactions with a heavy mediator can fit the data with dark matter mass heavier than a GeV but are generically in tension with collider constraints. Next, we consider dark matter consisting of two (or more) states that have a small mass splitting. The exothermic (down)scattering of the heavier state to the lighter state can fit the data for keV mass splittings. Finally, we consider a subcomponent of dark matter that is accelerated by scattering off cosmic rays, finding that dark matter interacting though an O(100 keV)-mass mediator can fit the data. The cross sections required in this scenario are, however, typically challenged by complementary probes of the light mediator. Throughout our study, we implement an unbinned Monte Carlo analysis and use an improved energy reconstruction of the XENON1T events. ","Exploring New Physics with O(keV) Electron Recoils in Direct Detection
  Experiments"
43,1276316525683634176,3245949691,Rebecca Leane,"[""New paper!\n\nSupernova Muons: New Constraints on Z' Bosons, Axions, and ALPs\n<LINK>\n\nw/ Djuna Croon (@QuantumMessage), @GillyElor + Sam McDermott\n\nWe use supernova muons to find some of the strongest existing limits on light new particles coupled to muons! Thread:"", '1/ Deep in the Large Magellanic Cloud, on the outskirts of the Tarantula Nebula, a blue supergiant named Sanduleak once shone with the brightness of over ten thousand Suns.', ""2/ That was, until one day, hydrostatic burning ended, no more nuclear energy could be released by fusion, and Sanduleak's iron core began to collapse under its own gravity."", '3/ Eventually, its core became so massive that even electron degeneracy pressure could not stabilize it. As Sanduleak contracted, \nphotons began to dissociate the iron atoms of the inner core, which decreased the energy of the star even more and caused it to further contract.', ""4/ Electrons were absorbed onto protons, and converted into neutrons and neutrinos. The escape of the neutrinos further lowered the electron degeneracy pressure, until Sanduleak's core became unstable, and *collapsed*."", ""5/ Once Sanduleak's core reached nuclear densities, the collapse was abruptly *halted*. A shock wave formed between the outer and\ninner core and moved outward from the core through the star."", ""6/ The implosion of the inner core ignited an explosion, and a core-collapse supernova was initiated. A mere 0.3 seconds after the collapse, the shock wave ejected the entire contents of Sanduleak's outer layers, leaving only a compact remnant (likely a neutron star) behind."", '7/ Neutrinos screamed out of the stellar inferno, and were observed in 1987 by the Kamiokande II, IMB, and the Baksan collaborations, in an event known as Supernova 1987A (SN1987A).', '8/ The transformation of Sanduleak into SN1987A has gifted us with a wealth of new insights for particle, nuclear, and astro physics.', '9/ By comparing with simulations, it appears that the number of neutrinos that were observed in SN1987A is consistent with that expected from known physics.', '10/ This provides an exciting avenue to probe other light new particles. That is, if new particles were also created during this explosion, they might leak energy out of the event, leaving less energy available to power the production of the neutrinos.', '11/ This would be in conflict with the neutrinos observed in SN1987A. This means that any other light particles that live long enough to escape the system, can be constrained by the fact that they are not allowed to steal away too much energy.', '12/ When producing these new particles, research in the past has mostly focused on production (and interactions) using the protons or neutrons in the supernova.', '13/ We do something largely different. We consider the impact of interactions instead with the *muons* produced in 1987A.', '14/  While these are less abundant than the protons or neutrons, they can provide a more sensitive probe of particles that would interact directly with muons, or leptons.', ""15/For the first time, we considered limits that arise from Z' bosons (force carriers) interacting with these muons. To get such limits,we exploited the most recent simulations of the muon number density profiles in the explosion, as well as temperature profiles of the explosion."", ""16/ The profiles give us a measure of these variables as a function of the radius from the center of the supernova. This allows us to determine how strongly these new interactions would occur, and how much energy loss they would lead to during Sanduleak's demise."", ""17/ When the Z' only interacts with muons and taus, in a particle model called gauged muon number minus tau number, we get the following limits, where the y-axis is the Z' coupling, and the x-axis is the Z' mass: https://t.co/Xhw0k7B748"", ""18/ The dip on the right hand side comes from neutrinos in the supernova pair coalescing to produce the Z', which would have then have run away stealing some of the supernova energy."", ""19/But it isn't allowed to do that in order to match observation, so it is shown as the excluded hot pink region.The bolder and lighter hot pink excluded regions come from taking a conservative version of the profiles, and a less conservative version of the profiles respectively."", ""20/ The flat line on the left hand side of the plot comes from something called a semi-Compton process, where a Z' could be produced after a photon interacts with a muon, and outputs a Z' and a muon. Again, as this would make the Z' a greedy energy thief, it is excluded."", ""21/ Compared to other processes, we find these limits are largely independent of the Z' mass, and can extend down to arbitrarily low Z' mass, even though they are only shown in a smaller mass window here."", '22/ We also show the previous estimate for this bound (dashed), which had not used the charged muons in the supernova, along with other relevant constraints from Neff, neutrino tridents (""CCFR""), and g-2.', ""23/ As our greedy Z' exclusion covers some of the g-2 region, we exclude the g-2 explananation for this model for Z' masses less than about 10^-5 MeV."", ""24/ We also considered the case that kinetic mixing was present in the Lmu-Ltau model. In this case, the bounds don't change much, but instead, the competing constraints do. You can see this in our plots for different amounts of mixing: https://t.co/KMpDWSsKs7"", ""25/ The resonance in the larger mixing case around 20 MeV comes from producing the Z' from a kinetic mixing loop, off protons rather than muons."", '26/ We also extended these limits to a popular model class, called ""B-L"", which is gauged baryon minus lepton number. This just determines which particles the Z\' will interact with. In that case, we also find new bounds: https://t.co/WRwXS2Rnaz', ""27/ Here, the biggest change from previous estimates arises from the neutrino-pair coalescence process, which gives more sensitive constraints at higher Z' masses."", ""28/ Separate to the Z', there are other types of light particles that could be produced in this event, called axions, or axion-like particles. Probing these particles using supernova muons was studied in a nice recent paper by Bollig, deRocco, Graham, and Jenka (2020)."", '29/ This is also the paper that produced the awesome new simulations.', '30/ That work focused on tree-level couplings of the axion. We were curious about how much these bounds might change when loop processes were also considered. We also extended the calculation to higher masses, to determine the full constraint across *all* axion masses.', ""31/ We found that the loops didn't change the bounds in Bollig et al for their mass range. In the higher, new mass range we were also considering, we found that loops were very important."", '32/ We found the following exclusion on the axion-muon interaction parameter space, where the x-axis is the axion mass, and the y-axis is its muon coupling: https://t.co/5XMG85tXQr', '33/ The flat lines arise from a combination of processes from muon bremsstrahlung, and the semi-Compton process. On the right hand side, the larger couplings lead to the axion getting trapped (and therefore not stealing energy), and so the limits cover less of the couplings.', '34/ We find we can constrain axions up to masses to nearly 800 MeV with one of the profiles. This is really high! The reason we have such good sensitivity here, is that the core temperature is really high, and the trapping is less efficient.', '35/ In our work, we have pointed out and explicitly demonstrated for the first time the broad applicability of supernova muons to provide a sensitive probe of models of new physics. This certainly motivates further studies of how muons behave in supernovae!', 'Shout out to Sanduleak, whose ultimate sacrifice gifted us with so many interesting stories to tell, and so many things we can learn about new physics. :)', 'Also shout out to my awesome collaborators, Djuna, Gilly, and Sam! Stay tuned for another paper we have in the works. ;)']",https://arxiv.org/abs/2006.13942,"New light particles produced in supernovae can lead to additional energy loss and a consequent deficit in neutrino production in conflict with the neutrinos observed from Supernova 1987A (SN1987A). Contrary to the majority of previous SN1987A studies, we examine the impact of $Z'$ bosons, axions, and axion-like particles (ALPs) interacting with the muons produced in SN1987A. For the first time, we find constraints on generic $Z'$ bosons coupled to muons, and apply our results to particle models including gauged $L_\mu-L_\tau$ number, $U(1)_{L_\mu-L_\tau}$, and gauged $B-L$ number, $U(1)_{B-L}$. We constrain $Z'$ bosons with masses up to about 250-500 MeV, and down to about $10^{-9}$ in $Z'$-muon coupling. We also extend previous work on axion-muon couplings by examining the importance of loop-level interactions, as well as performing calculations over a wider range of axion masses. We constrain muon-coupled axions from arbitrarily low masses up to about 200-500 MeV, with bounds extending down to axion-muon couplings of approximately $10^{-8}$ GeV$^{-1}$. We conclude that supernovae broadly provide a sensitive probe of new lightly-coupled particles interacting with muons. ","Supernova Muons: New Constraints on Z' Bosons, Axions, and ALPs"
44,1276315120650588160,82733042,Shubham Kanodia,"[""I'm happy to share our new @HPFspectrograph paper on the confirmation of warm Super Neptune TOI-1728b orbiting an M0 star. This planet is a part of a population of just 2 other super Neptunes around M dwarfs.\xa0\n<LINK>\n\n(1/n) <LINK>"", 'Detected in @TESSatMIT data, we confirmed the transit with ground based photometry from @PSUScience  Davey Lab and Perkins 17"" to measure a radius of ~ 5 Earth radii and a period of 3.5 days. \n\n(2/n) https://t.co/2BsnRO9Dtq', 'We used @HPFspectrograph  to measure its mass to be ~ 26 Earth masses, and also followed up during transit to place an upper limit on He 10830 absorption.\n\n(3/n)', 'Furthermore, its Transmission Spectroscopy Metric of ~ 130 and a relatively bright host star (J~9.6) make it a gootd candidate for transmission\xa0spectroscopy with #JWST, as well Ly Alpha searches with\xa0@NASAHubble !!\n\n(4/n) https://t.co/1K4X4T46US', 'This has been a collaborative effort with significant work put in groups across multiple institutions. Thank you everyone!\n@gummiks, @SuvrathM, @Astro_Wright, @lesliehebb, @rcterrien, @jiayin_dong - sorry if I missed anyone.\n(5/n)']",https://arxiv.org/abs/2006.14546,"We confirm the planetary nature of TOI-1728b using a combination of ground-based photometry, near-infrared Doppler velocimetry and spectroscopy with the Habitable-zone Planet Finder.TOI-1728 is an old, inactive M0 star with \teff{} $= 3980^{+31}_{-32}$ K, which hosts a transiting super Neptune at an orbital period of $\sim$ 3.49 days. Joint fitting of the radial velocities and TESS and ground-based transits yields a planetary radius of $5.05_{-0.17}^{+0.16}$ R$_{\oplus}$, mass $26.78_{-5.13}^{+5.43}$ M$_{\oplus}$ and eccentricity $0.057_{-0.039}^{+0.054}$. We estimate the stellar properties, and perform a search for He 10830 \AA absorption during the transit of this planet and claim a null detection with an upper limit of 1.1$\%$ with 90\% confidence. A deeper level of He 10830 \AA ~ absorption has been detected in the planet atmosphere of GJ 3470b, a comparable gaseous planet. TOI-1728b is the largest super Neptune -- the intermediate subclass of planets between Neptune and the more massive gas-giant planets -- discovered around an M dwarf. With its relatively large mass and radius, TOI-1728 represents a valuable datapoint in the M-dwarf exoplanet mass-radius diagram, bridging the gap between the lighter Neptune-sized planets and the heavier Jovian planets known to orbit M-dwarfs. With a low bulk density of $1.14_{-0.24}^{+0.26}$ g/cm$^3$, and orbiting a bright host star (J $\sim 9.6$, V $\sim 12.4$), TOI-1728b is also a promising candidate for transmission spectroscopy both from the ground and from space, which can be used to constrain planet formation and evolutionary models. ","TOI-1728b: The Habitable-zone Planet Finder confirms a warm super
  Neptune orbiting an M dwarf host"
45,1276262376002555904,924816072036904960,Adam Gleave,"['How do you measure the distance between two reward functions?\n\nOur EPIC distance is invariant to reward shaping, can be approximated efficiently, and is predictive of policy training success and transfer!\n\nNew paper with @MichaelD1729 @janleike et al.\n\n<LINK> <LINK>', 'Thanks to @DeepMind and @CHAI_Berkeley for providing a supportive atmosphere for this work!']",https://arxiv.org/abs/2006.13900,"For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at this https URL ",Quantifying Differences in Reward Functions
46,1276250079356170240,1133088397273313282,francesco croce,"['Check out our new paper on sparse black-box perturbations! New SOTA in query efficiency and success rate. For L0 attacks, changing *only 0.1% pixels* is sufficient to break the models. @maksym_andr \n\nPaper: <LINK>\nCode: <LINK>\n(1/n) <LINK>', 'Sparse perturbations are challenging for gradient-based methods because of the combinatorial constraints. To tackle this problem, we propose a flexible framework based on random search that naturally handles complicated constraints and leads to query-efficient attacks.\n(2/n) https://t.co/1bjL34J7uJ', 'Our framework leads to the first black-box patch and frame attacks that don’t rely on extra knowledge such as a surrogate model. Having access to a surrogate model is a very strong assumption and it can be very expensive to obtain.\n(3/n) https://t.co/xUDwvkXzpM', 'Moreover, although transfer attacks rely on surrogate models, they in general *do not perform well* and patches/frames are no exceptions. Sparse-RS outperforms transfer attacks (Tr-PGD) by a large margin in terms of the success rate.\n(4/n) https://t.co/z4Ijtk6mcM', 'The resulting adversarial patches are very different from the patches found with white-box PGD and some of them are quite interpretable (see the patch for class Peacock).\n(5/n) https://t.co/mWatfd4Ybc', 'We show the versatility of Sparse-RS framework by generating both image-specific and universal patches/frames + L0 perturbations for images and malware.  Sparse-RS outperforms other methods on all these threat models including **white-box** L0-PGD attack, see PGD_0 (wb).\n(6/n) https://t.co/X5bFdrWbT6', 'Takeaway message: don’t spend queries on estimating the gradient, do random search with a properly chosen sampling distribution! \nMore details are in the paper: https://t.co/8MvRlRrdr7\n(7/n)']",https://arxiv.org/abs/2006.12834,"We propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: $l_0$-bounded perturbations, adversarial patches, and adversarial frames. The $l_0$-version of untargeted Sparse-RS outperforms all black-box and even all white-box attacks for different models on MNIST, CIFAR-10, and ImageNet. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of $20\times20$ adversarial patches and $2$-pixel wide adversarial frames for $224\times224$ images. Finally, we show that Sparse-RS can be applied to generate targeted universal adversarial patches where it significantly outperforms the existing approaches. The code of our framework is available at this https URL ","Sparse-RS: a versatile framework for query-efficient sparse black-box
  adversarial attacks"
47,1276193647411765250,101209471,Vini Netto 🇧🇷🇬🇧🇺🇲🇵🇹,"[""Our new paper on whether cities can be understood as 'spatial cultures' as 'distinct ways of ordering space' \n\n<LINK>\n\n@UrbanDemog @Space_Syntax @ElsaArcaute @CASAUCL  @SpaceSyntaxNet @SophiaPsarra @alanpenn @KimonKrenz @urheterotopia @EdgardoBrigatti @arxiv <LINK>""]",https://arxiv.org/abs/2006.13897,"Cities are different around the world, but does this fact have any relation to culture? The idea that urban form embodies idiosyncrasies related to cultural identities captures the imagination of many in urban studies, but it is an assumption yet to be carefully examined. Approaching spatial configurations in the built environment as a proxy of urban culture, this paper searches for differences potentially consistent with specific regional cultures or cultures of planning in urban development. It does so focusing on the elementary components shaping cities: buildings and how they are aggregated in cellular complexes of built form. Exploring Shannon's work, we introduce an entropy measure to analyse the probability distribution of cellular arrangements in built form systems. We apply it to downtown areas of 45 cities from different regions of the world as a similarity measure to compare and cluster cities potentially consistent with specific spatial cultures. Findings suggest a classification scheme that sheds further light on what we call the ""cultural hypothesis"": the possibility that different cultures and regions find different ways of ordering space. ","From form to information: Analysing built environments in different
  spatial cultures"
48,1276184896936054785,3077144869,Preethi Lahoti,"['How can we train ML models to achieve fairness when we do not know protected group memberships? Excited to share a new paper on this (<LINK>) from my internship @GoogleAI. Joint work with @alexbeutel @edchi @chenjilin @Nithum @FlavienProst and others. 1/n', 'Addressing fairness in machine learning systems, without having access to demographic features (at training and inference time) is a crucial challenge for ML practitioners as shown in recent surveys by @mikarv and @jennwvaughan @hannawallach. In this paper we address this... 2/n', 'Our key insight is that when improving worst-case performance for unobserved protected groups, it is valuable to focus the objective on computationally-identifiable regions of errors (related to computationally-indentifiable masses by @mikekimbackward @Omereingold and others) 3/n https://t.co/WEPR5znIX5', 'We leverage this notion of computationally-indentifiable errors, and propose an Adversarially Reweighted Learning approach that optimizes for Rawlsian Max Min Fairness without access to demographic group labels (Paper: https://t.co/Ja4jSPSgi4, Code: https://t.co/J8zihBBxQo). 4/4']",https://arxiv.org/abs/2006.13114,"Much of the previous machine learning (ML) fairness literature assumes that protected features such as race and sex are present in the dataset, and relies upon them to mitigate fairness concerns. However, in practice factors like privacy and regulation often preclude the collection of protected features, or their use for training or inference, severely limiting the applicability of traditional fairness research. Therefore we ask: How can we train an ML model to improve fairness when we do not even know the protected group memberships? In this work we address this problem by proposing Adversarially Reweighted Learning (ARL). In particular, we hypothesize that non-protected features and task labels are valuable for identifying fairness issues, and can be used to co-train an adversarial reweighting approach for improving fairness. Our results show that {ARL} improves Rawlsian Max-Min fairness, with notable AUC improvements for worst-case protected groups in multiple datasets, outperforming state-of-the-art alternatives. ",Fairness without Demographics through Adversarially Reweighted Learning
49,1276073042842660865,134172617,Jonathan Davies,"['Very pleased to release a new paper on the arXiv today in collaboration with @rcrain_astro and @apontzen! We utilise ""genetically modified"" simulations to explore how assembly history directly influences the evolution of a galaxy and its CGM. <LINK> @LJMU_Astro', ""We find that systematically shifting the assembly history of a star-forming disc galaxy's dark matter halo to earlier times results in a quenched, spheroidal galaxy instead. A movie demonstrating this can be found here: https://t.co/Id02q9V6XK""]",https://arxiv.org/abs/2006.13221,"We examine the influence of dark matter halo assembly on the evolution of a simulated $\sim L^\star$ galaxy. Starting from a zoom-in simulation of a star-forming galaxy evolved with the EAGLE galaxy formation model, we use the genetic modification technique to create a pair of complementary assembly histories: one in which the halo assembles later than in the unmodified case, and one in which it assembles earlier. Delayed assembly leads to the galaxy exhibiting a greater present-day star formation rate than its unmodified counterpart, whilst in the accelerated case the galaxy quenches at $z\simeq 1$, and becomes spheroidal. We simulate each assembly history nine times, adopting different seeds for the random number generator used by EAGLE's stochastic subgrid implementations of star formation and feedback. The systematic changes driven by differences in assembly history are significantly stronger than the random scatter induced by this stochasticity. The sensitivity of $\sim L^\star$ galaxy evolution to dark matter halo assembly follows from the close coupling of the growth histories of the central black hole (BH) and the halo, such that earlier assembly fosters the formation of a more massive BH, and more efficient expulsion of circumgalactic gas. In response to this expulsion, the circumgalactic medium reconfigures at a lower density, extending its cooling time and thus inhibiting the replenishment of the interstellar medium. Our results indicate that halo assembly history significantly influences the evolution of $\sim L^\star$ central galaxies, and that the expulsion of circumgalactic gas is a crucial step in quenching them. ","Quenching and morphological evolution due to circumgalactic gas
  expulsion in a simulated galaxy with a controlled assembly history"
50,1276067026478530561,710610891058716673,Jan Leike,"['How do you measure the distance between two reward functions?\n\nOur EPIC distance is invariant to reward shaping, can be approximated efficiently, and is predictive of policy training success and transfer!\n\nNew paper with @ARGleave, @MichaelD1729 et al.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2006.13900,"For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at this https URL ",Quantifying Differences in Reward Functions
51,1275959462218489857,598117717,Farhad Farokhi,"['In the paper below, we have rewritten machine learning with locally-differentially private datasets as a distributionally-robust optimization problem. This results in an entirely new regularizer for training regression models that can be posed as SDP.\n<LINK>']",https://arxiv.org/abs/2006.13488,"We consider machine learning, particularly regression, using locally-differentially private datasets. The Wasserstein distance is used to define an ambiguity set centered at the empirical distribution of the dataset corrupted by local differential privacy noise. The ambiguity set is shown to contain the probability distribution of unperturbed, clean data. The radius of the ambiguity set is a function of the privacy budget, spread of the data, and the size of the problem. Hence, machine learning with locally-differentially private datasets can be rewritten as a distributionally-robust optimization. For general distributions, the distributionally-robust optimization problem can relaxed as a regularized machine learning problem with the Lipschitz constant of the machine learning model as a regularizer. For linear and logistic regression, this regularizer is the dual norm of the model parameters. For Gaussian data, the distributionally-robust optimization problem can be solved exactly to find an optimal regularizer. This approach results in an entirely new regularizer for training linear regression models. Training with this novel regularizer can be posed as a semi-definite program. Finally, the performance of the proposed distributionally-robust machine learning training is demonstrated on practical datasets. ","Distributionally-Robust Machine Learning Using Locally
  Differentially-Private Data"
52,1275865930858483713,2530947115,Max Tegmark,"[""Got data? The code described in our new paper auto-discovers simple and accurate formulas describing it by exploiting modules and generalized symmetry. Let me know if you'd like to collaborate on applications! \n  force search. <LINK> <LINK>""]",https://arxiv.org/abs/2006.10782,"We present an improved method for symbolic regression that seeks to fit data to formulas that are Pareto-optimal, in the sense of having the best accuracy for a given complexity. It improves on the previous state-of-the-art by typically being orders of magnitude more robust toward noise and bad data, and also by discovering many formulas that stumped previous methods. We develop a method for discovering generalized symmetries (arbitrary modularity in the computational graph of a formula) from gradient properties of a neural network fit. We use normalizing flows to generalize our symbolic regression method to probability distributions from which we only have samples, and employ statistical hypothesis testing to accelerate robust brute-force search. ","AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph
  modularity"
53,1275826189421228033,1403815458,Amir Siraj,['New paper on the arXiv today in which we investigate a new habitability factor (and report what it could mean for Proxima b): sterilizing asteroid impacts <LINK>'],https://arxiv.org/abs/2006.12503,We consider the implications that a debris belt located between Proxima b and Proxima c would pose for the rate of large asteroid impacts that could sterilize Proxima b from life. Future observations by ALMA or JWST could constrain the existence of an asteroid belt in the life-threatening regime. We generalize our rate calculation of sterilizing impacts for habitable planets in systems with an asteroid belt and an outer planet. ,Risks for Life on Proxima b from Sterilizing Impacts
54,1275817299241467904,1246825028274008069,Emil J. Bergholtz,"['An occasional reminder that I also still do science -- new paper on ""Phase Transitions and Generalized Biorthogonal Polarization in Non-Hermitian Systems"" out today! \n\n<LINK>']",https://arxiv.org/abs/2006.12898,"Non-Hermitian (NH) Hamiltonians can be used to describe dissipative systems, and are currently intensively studied in the context of topology. A salient difference between Hermitian and NH models is the breakdown of the conventional bulk-boundary correspondence invalidating the use of topological invariants computed from the Bloch bands to characterize boundary modes in generic NH systems. One way to overcome this difficulty is to use the framework of biorthogonal quantum mechanics to define a biorthogonal polarization, which functions as a real-space invariant signaling the presence of boundary states. Here, we generalize the concept of the biorthogonal polarization beyond the previous results to systems with any number of boundary modes, and show that it is invariant under basis transformations as well as local unitary transformations. Additionally, we propose a generalization of a perviously-developed method with which to find all the bulk states of system with open boundaries to NH models. Using the exact solutions in combination with variational states, we elucidate genuinely NH aspects of the interplay between bulk and boundary at the phase transitions. ","Phase Transitions and Generalized Biorthogonal Polarization in
  Non-Hermitian Systems"
55,1275773294168473600,572479189,Manlio De Domenico,"['Happy to briefly discuss the new work led by @valedand about exploiting information theory to compress the phase space of nonlinear dynamical systems and detect state changes. Paper 👉 <LINK>\n\nThread 👇 1 / <LINK>', ""Since the '80s we know how to reconstruct the phase space of a dynamical system from the observation of the series it generates. Methods mostly depend on two unknowns: embedding dimension and time delay. We propose compressibility of the dynamics to solve this issue: it works! 2/ https://t.co/TbGrNT0Fjr"", 'So we started to analyze more complex dynamics, such as coupled chaotic oscillators of different nature and varied their coupling. We observe how the effective embedding dimension change &amp; reveal changes that Lyapunov or Correlation Dim analyses do not reveal separately. 3/ https://t.co/EqneJN1KWo', ""When we do the same analysis for coupled chaotic maps, results are similar: we have to use *both* Lyapunov and CorDim to understand what's going on, whereas our embedding dimension is self-consistent. Why that's relevant?  4/ https://t.co/LMEFs8n7d3"", 'Because calculating Lyapunov exponents and CorDim requires long + possibly non-noisy time series, whereas we demonstrate that our method works faster and with smaller observational size: we exploit that for causal interactions information entropy grows non-extensively. 5/5']",https://arxiv.org/abs/2006.12842,"Equations governing the nonlinear dynamics of complex systems are usually unknown and indirect methods are used to reconstruct their manifolds. In turn, they depend on embedding parameters requiring other methods and long temporal sequences to be accurate. In this paper, we show that an optimal reconstruction can be achieved by lossless compression of system's time course, providing a self-consistent analysis of its dynamics and a measure of its complexity, even for short sequences. Our measure of complexity detects system's state changes such as weak synchronization phenomena, characterizing many systems, in one step, integrating results from Lyapunov and fractal analysis. ","Compressing phase space detects state changes in nonlinear dynamical
  systems"
56,1275770930334175232,1264631618964271104,Ignacio del Moral Castro,"[""If you like active galaxies, you may be interested in our new paper (my second PhD paper!!!) just accepted in @AandA_journal:\n\n 'A Larger  lambda_R in the disc of isolated active spiral galaxies than in their non-active twins’\n\n<LINK>\n\n@IAC_Astrofisica <LINK>"", 'In this work, we present a comparison of lambda_R measured in a region dominated by the galaxy disc between a sample of pairs of nearby seemingly isolated twin galaxies differing in nuclear activity from @CALIFAsurvey. https://t.co/026C71GfWC', 'We find that active galaxies show higher values than their corresponding non-active twins, indicating larger rotational support in the AGN discs.  \n\nI could never have done this work without my supervisors (@astrocra and Begoña) and my co-authors (e.g. @owl_astro and @psanchezb) https://t.co/gfCiIUaIOL']",https://arxiv.org/abs/2006.12654,"We present a comparison of the spin parameter $\lambda_R$, measured in a region dominated by the galaxy disc, between 20 pairs of nearby (0.005$<$z$<$0.03) seemingly isolated twin galaxies differing in nuclear activity. We find that 80--82% of the active galaxies show higher values of $\lambda_R$ than their corresponding non-active twin(s), indicating larger rotational support in the AGN discs. This result is driven by the 11 pairs of unbarred galaxies, for which 100% of the AGN show larger $\lambda_R$ than their twins. These results can be explained by a more efficient angular momentum transfer from the inflowing gas to the disc baryonic matter in the case of the active galaxies. This gas inflow could have been induced by disc or bar instabilities, although we cannot rule out minor mergers if these are prevalent in our active galaxies. This result represents the first evidence of galaxy-scale differences between the dynamics of active and non-active isolated spiral galaxies of intermediate stellar masses (10$^{10}<M_*<10^{11}$ M$_{\odot}$) in the Local Universe. ","Larger $\lambda_R$ in the disc of isolated active spiral galaxies than
  in their non-active twins"
57,1275766116393115649,2766925212,Andrew Childs,"['New paper on symmetries, graph properties, and quantum speedups with Shalev Ben-David, András Gilyén, William Kretschmer, Supartha Podder, &amp; Daochen Wang characterizes symmetries that prevent superpolynomial speedup (basically, only graph symmetries!). <LINK>', 'We also show an exponential quantum speedup for graph property testing, finally resolving a question raised in a paper with @AAmbainis and Yi-Kai Liu almost ten years ago (https://t.co/yjVwYAqJYe).', 'This paper subsumes and extends an earlier preprint with Daochen (https://t.co/Hrn9xx29NF) and another by Shalev and Supartha (https://t.co/bSle537nQa).']",http://arxiv.org/abs/2006.12760,"Aaronson and Ambainis (2009) and Chailloux (2018) showed that fully symmetric (partial) functions do not admit exponential quantum query speedups. This raises a natural question: how symmetric must a function be before it cannot exhibit a large quantum speedup? In this work, we prove that hypergraph symmetries in the adjacency matrix model allow at most a polynomial separation between randomized and quantum query complexities. We also show that, remarkably, permutation groups constructed out of these symmetries are essentially the only permutation groups that prevent super-polynomial quantum speedups. We prove this by fully characterizing the primitive permutation groups that allow super-polynomial quantum speedups. In contrast, in the adjacency list model for bounded-degree graphs (where graph symmetry is manifested differently), we exhibit a property testing problem that shows an exponential quantum speedup. These results resolve open questions posed by Ambainis, Childs, and Liu (2010) and Montanaro and de Wolf (2013). ","Symmetries, graph properties, and quantum speedups"
58,1275635523961466881,320161224,Garima Chauhan,['A new paper exploring HI-Halo mass scaling relation on arXiv now! We investigate the physical processes responsible for the shape and scatter of the relation from z=0-2 and develop a numerical HI-halo mass relation with SHARK. Check it out at - <LINK> <LINK>'],https://arxiv.org/abs/2006.12102v1,"We use SHARK, a semi-analytic galaxy formation model, to investigate the physical processes involved in dictating the shape, scatter and evolution of the HI-halo mass relation at $0\leq z \leq 2$. We compare SHARK with HI clustering and spectral stacking of the HI-halo mass relation derived from observations finding excellent agreement with the former and a deficiency of HI in SHARK at $M_{\rm vir}\approx 10^{12-13} M_{\odot}$ in the latter, but otherwise great agreement below and above that mass threshold. In SHARK, we find that the HI mass increases with the halo mass up to a critical mass of $\approx 10^{11.8} M_{\odot}$; between $\sim 10^{11.8}-10^{13}M_{\odot}$, the scatter in the relation increases by 0.7 dex and the HI mass decreases with the halo mass on average; at $M_{\rm vir} \geq 10^{13} M_{\odot}$, the HI content continues to increase with halo mass. We find that the critical halo mass of $\approx 10^{12} M_{\odot}$ is largely set by feedback from Active Galactic Nuclei (AGN), and the exact shape and scatter of the HI-halo mass relation around that mass is extremely sensitive to how AGN feedback is modelled, with other physical processes playing a less significant role. We determine the main secondary parameters responsible for the scatter of the HI-halo mass relation, namely the halo spin parameter at $M_{\rm vir}\leq 10^{11.8} M_{\odot}$, and the fractional contribution from substructure to the total halo mass for $M_{\rm vir}\geq 10^{13} M_{\odot}$. The scatter at $10^{11.8}<M_{\rm vir}<10^{13} M_{\odot}$ is best described by the black-hole-to-stellar mass ratio of the central galaxy, reflecting the AGN feedback relevance. We present a numerical model to populate dark matter-only simulations with HI at $0\leq z \leq 2$ based solely on halo parameters that are measurable in such simulations. ",] The physical drivers of the atomic hydrogen-halo mass relation
59,1275594277863579649,913238472357437445,Fuminobu TAKAHASHI,"['My new paper with Naoya Kitajima is out; PBH of solar masses or heavier can be produced from the QCD axion with large fa. The QCD axion explains all DM, and the PBH can be responsible for the LIGO events or seed SMBH. Today the LIGO found another event.🧐\n\n<LINK>']",https://arxiv.org/abs/2006.13137,"We propose a scenario in which a strong Peccei-Quinn (PQ) symmetry breaking in the early universe results in large inhomogeneities of the initial QCD axion field value, leading to the formation of very dense axion bubbles. Some of the axion bubbles subsequently collapse into primordial black holes (PBHs). The spatially homogeneous part of the QCD axion explains dark matter of the universe, while the PBHs arising from the axion bubbles can explain the LIGO events or the seed of supermassive black holes. Interestingly, the mass of PBH is determined by the axion decay constant; for $f_a = 10^{17} (10^{16})$ GeV, the PBH mass is heavier than about $10 (10^4) M_\odot$. In addition, axion miniclusters are also formed from the axion bubbles more abundantly than PBHs, and their masses are expected to be heavier than in the usual scenario based on the spontaneous breaking of the PQ symmetry after inflation. ",Primordial Black Holes from QCD Axion Bubbles
60,1275589410529017857,2337598033,Geraint F. Lewis,['Superb new paper on @arxiv - is there dark matter in the Galactic Centre? PhD student Florian List and Nick Rodd use neural nets to take a look! \n\n<LINK> <LINK>'],https://arxiv.org/abs/2006.12504,"A fundamental question regarding the Galactic Center Excess (GCE) is whether the underlying structure is point-like or smooth. This debate, often framed in terms of a millisecond pulsar or annihilating dark matter (DM) origin for the emission, awaits a conclusive resolution. In this work we weigh in on the problem using Bayesian graph convolutional neural networks. In simulated data, our neural network (NN) is able to reconstruct the flux of inner Galaxy emission components to on average $\sim$0.5%, comparable to the non-Poissonian template fit (NPTF). When applied to the actual $\textit{Fermi}$-LAT data, we find that the NN estimates for the flux fractions from the background templates are consistent with the NPTF; however, the GCE is almost entirely attributed to smooth emission. While suggestive, we do not claim a definitive resolution for the GCE, as the NN tends to underestimate the flux of point-sources peaked near the 1$\sigma$ detection threshold. Yet the technique displays robustness to a number of systematics, including reconstructing injected DM, diffuse mismodeling, and unmodeled north-south asymmetries. So while the NN is hinting at a smooth origin for the GCE at present, with further refinements we argue that Bayesian Deep Learning is well placed to resolve this DM mystery. ","The GCE in a New Light: Disentangling the $\gamma$-ray Sky with Bayesian
  Graph Convolutional Neural Networks"
61,1275529437573009408,390847005,Antony Alexos,"['With my research colleagues we wrote a paper on statistical machine learning. More specifically we proposed a new method that combines local competition and a novel uncertainty mechanism, for adversarial robustness.\n<LINK> \n#deep_learning #machine_learning']",https://arxiv.org/abs/2006.10620,"This work attempts to address adversarial robustness of deep networks by means of novel learning arguments. Specifically, inspired from results in neuroscience, we propose a local competition principle as a means of adversarially-robust deep learning. We argue that novel local winner-takes-all (LWTA) nonlinearities, combined with posterior sampling schemes, can greatly improve the adversarial robustness of traditional deep networks against difficult adversarial attack schemes. We combine these LWTA arguments with tools from the field of Bayesian non-parametrics, specifically the stick-breaking construction of the Indian Buffet Process, to flexibly account for the inherent uncertainty in data-driven modeling. As we experimentally show, the new proposed model achieves high robustness to adversarial perturbations on MNIST and CIFAR10 datasets. Our model achieves state-of-the-art results in powerful white-box attacks, while at the same time retaining its benign accuracy to a high degree. Equally importantly, our approach achieves this result while requiring far less trainable model parameters than the existing state-of-the-art. ","Local Competition and Uncertainty for Adversarial Robustness in Deep
  Learning"
62,1275488965353631745,71524168,Jason Hartford,"[""1/10 I'm super excited about our new paper on robust instrumental variable estimation: <LINK> with @victorveitch @dhanya_sridhar and @k_leyton_brown. Want to perform IV estimation, but don't trust your instruments? Keep reading..."", '2/10 Instrumental variable (IV) methods are super powerful - they let you do causal inference with unobserved confounding. But, arguing that instruments satisfy the necessary IV assumptions can be challenging because the assumptions are untestable.', '3/10 What if you have lots of candidate instruments, only some of which are meet all the IV assumptions? Each of the valid instruments should estimate the same causal effect, while invalid instruments can estimate different effects. How do we use this observation?', '4/10 Our ModeIV procedure: With k candidate instruments, fit an ensemble of k different models using your favourite nonlinear IV estimator (e.g. DeepIV, KernelIV, DeepGMM, Adverserial GMM, etc.); then make predictions by outputting the *mode* of the ensemble.', ""5/10 We show this simple procedure gives asymptotically valid estimates if at least half of the instruments are valid (or less than half if the biased instruments are 'nice'). And it works really nicely across a number of simulations."", ""6/10 How does one estimate a continuous mode from a set of discrete atoms? Most papers perform a kernel density estimation and then output the mode of the density estimate, but there's a simpler approach: take the mean of the j closest points (the 'Venter mode')."", ""7/10 Here j is any lower bound on the number of valid instruments (larger j's give lower variance), so it's much easier to think about than tuning bandwidth hyper-parameters of kernel density estimators."", ""8/10 In summary - ModeIV is a simple blackbox method that gives estimates that are robust to violations of the IV assumptions. It's so simple that the pytorch code fits in one tweet 😄\n\nIf `pred` is a tensor of predictions from the ensemble the you can compute the mode with:"", 'sort_pred, _ = torch.sort(pred, axis=1)\nmin_idx = torch.argmin(sort_pred[:,j-1:] - sort_pred[:,:(pred.shape[1] - j + 1)], axis=1)\nmodal_indices = https://t.co/6PMJfADgz8([min_idx[:,None] + i for i in range(j)], dim=1)\nmode = torch.gather(sort_pred, 1, modal_indices).mean(axis=1)', ""10/10 ModeIV is the first robust IV method we're aware of that handles nonlinear conditional average treatment effects. But it doesn't work in the general LATE setting where different valid instruments have different estimands (see the discussion in the paper)."", '@alex_peys @victorveitch @dhanya_sridhar @k_leyton_brown Heterogenous effects with additive unobserved confounding (so Newey and Powell’s y = f(t,x) + eps). So you have to be prepared to assume that this heterogeneity is accounted for by observed features. So this is weaker than the constant treatment effect that’s common in these...', '@alex_peys @victorveitch @dhanya_sridhar @k_leyton_brown robust methods; but it’s certainly true that LATEs break it. I got interested in this from talking to people in the Mendelian randomization community- they were super interested in heterogenous effects from DeepIV, but worry a lot about exclusion violations.']",https://arxiv.org/abs/2006.11386,"Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable ""exclusion"" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is ""black-box"" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem. ",Valid Causal Inference with (Some) Invalid Instruments
63,1275465632692219905,62621489,Joshua Aaron Becker,"[""lab exp's too often constrained by cost etc—enter @empirica_ly! a gen purpose virtual lab enabling massively multi-factor, rapid/adaptive, etc designs AND can be used by typical #compsocsci-entist w/ just javascript!  described in a new paper by... (1/5) <LINK>"", 'by @amaatouq w/ @joshua_a_becker @_jamesphoughton Nicolas Paton @duncanjwatts @MarkWhiting.\n\n@empirica_ly maintains ""build anything you can do in a web browser"" functionality—and yet all you do is modify an auto-generated \'scaffold\' that contains all the things you\'d need.. (2/5)', ""to design/launch/manage an experiment:  randomization to conditions, lobbies for subjects, status monitoring, factor/treatment setup, message handling, database management &amp; multi-player synchronization, and all the really nerdy stuff you don't even care about. (3/5)"", '@empirica_ly is actively being developed for social scientists by social scientists—future plans include direct integration with w/ subject recruitment and payment PLUS many other features via modularized use-what-you-want API. (4/5)', ""i myself have used @empirica_ly in 3 projects and counting and have saved countless hours in development time.  first there was ABM, leading to soooo many wonderful theories... now we have virtual labs—so let's get out there and collect some data! (5/5)""]",https://arxiv.org/abs/2006.11398,"Virtual labs allow researchers to design high-throughput and macro-level experiments that are not feasible in traditional in-person physical lab settings. Despite the increasing popularity of online research, researchers still face many technical and logistical barriers when designing and deploying virtual lab experiments. While several platforms exist to facilitate the development of virtual lab experiments, they typically present researchers with a stark trade-off between usability and functionality. We introduce Empirica: a modular virtual lab that offers a solution to the usability-functionality trade-off by employing a ""flexible defaults"" design strategy. This strategy enables us to maintain complete ""build anything"" flexibility while offering a development platform that is accessible to novice programmers. Empirica's architecture is designed to allow for parameterizable experimental designs, reusable protocols, and rapid development. These features will increase the accessibility of virtual lab experiments, remove barriers to innovation in experiment design, and enable rapid progress in the understanding of distributed human computation. ",Empirica: a virtual lab for high-throughput macro-level experiments
64,1275459867613040650,1072220386237009921,Guillaume Lajoie,['New preprint day! Exploring the advantages of single neuron  adaptation during learning in RNNs. I love this project for several reasons: (1) investigates a cool brain mechanism (2) is the first paper from super talented undergrad Victor Geadah\n<LINK>'],https://arxiv.org/abs/2006.12253,"Dynamic adaptation in single-neuron response plays a fundamental role in neural coding in biological neural networks. Yet, most neural activation functions used in artificial networks are fixed and mostly considered as an inconsequential architecture choice. In this paper, we investigate nonlinear activation function adaptation over the large time scale of learning, and outline its impact on sequential processing in recurrent neural networks. We introduce a novel parametric family of nonlinear activation functions, inspired by input-frequency response curves of biological neurons, which allows interpolation between well-known activation functions such as ReLU and sigmoid. Using simple numerical experiments and tools from dynamical systems and information theory, we study the role of neural activation features in learning dynamics. We find that activation adaptation provides distinct task-specific solutions and in some cases, improves both learning speed and performance. Importantly, we find that optimal activation features emerging from our parametric family are considerably different from typical functions used in the literature, suggesting that exploiting the gap between these usual configurations can help learning. Finally, we outline situations where neural activation adaptation alone may help mitigate changes in input statistics in a given task, suggesting mechanisms for transfer learning optimization. ","Advantages of biologically-inspired adaptive neural activation in RNNs
  during learning"
65,1275384152808992780,799177412013592577,Rianne van den Berg,"['New paper! IDF++: analyzing and improving integer discrete flows for lossless compression, with @agritsenko   @m__dehghani  @CasperKaae @TimSalimans \n\npaper: <LINK> <LINK>', 'Our contributions:\n1. We show that flows for discrete random variables are more flexible than previously thought. https://t.co/4tNKWzXWsS', '2. We analyze the loss landscape &amp; gradient bias of integer discrete flows (@emiel_hoogeboom, @jornpeters, @wellingmax). https://t.co/PgFL2fEbx8', '3. And we introduce modifications to the integer discrete flows architecture that improve its performance on lossless compression --&gt; IDF++. https://t.co/BOXXbyQghK', '@wojczarnecki @agritsenko @m__dehghani @CasperKaae @TimSalimans Thanks! Nicolas Renaud (https://t.co/sgal4B3dgW) helped out and drew the figure in Blender (https://t.co/7QQXR3kZAv).']",https://arxiv.org/abs/2006.12459,"In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Their discrete nature makes them particularly suitable for lossless compression with entropy coding schemes. We start by investigating a recent theoretical claim that states that invertible flows for discrete random variables are less flexible than their continuous counterparts. We demonstrate with a proof that this claim does not hold for integer discrete flows due to the embedding of data with finite support into the countably infinite integer lattice. Furthermore, we zoom in on the effect of gradient bias due to the straight-through estimator in integer discrete flows, and demonstrate that its influence is highly dependent on architecture choices and less prominent than previously thought. Finally, we show how different architecture modifications improve the performance of this model class for lossless compression, and that they also enable more efficient compression: a model with half the number of flow layers performs on par with or better than the original integer discrete flow model. ","IDF++: Analyzing and Improving Integer Discrete Flows for Lossless
  Compression"
66,1275299836699475969,40299444,Alexey Petrov,"['New paper! If you ever thought if you could explain recent XENON1T anomaly with non-relativistic Rayleigh Dark Matter, then the answer is yes! See paper <LINK> for details. <LINK>', ""You'd say that judging by timing it is an ambulance-chasing paper (it is). But there is a back story. We worked on a completely different project, but then Jure had a brilliant idea that with a few strategic changes the idea can be applied to the XENON1T anomaly. So here we are!"", '@rittenhousewest Well, they used gluonic Rayleigh operators. We need photons, as one photon appears in the final state. But Rayleigh DM is a popular model -- more with fermionic DM, not the real scalar as what we used.']",https://arxiv.org/abs/2006.12462,"We point out that a non-relativistic $\sim 2 $ GeV dark matter (DM) which interacts with visible matter through higher dimensional Rayleigh operators could explain the excess of ""electron recoil"" events recently observed by the Xenon1T collaboration. A DM scattering event results in a few keV photon that on average carries most of the deposited energy, while the nuclear recoil energy is only a subleading correction. Since the Xenon1T detector does not discriminate between electrons and photons, such events would be interpreted as excess of the keV electrons. Indirect constraints from dark matter annihilation are avoided for light mediators of ${\mathcal O}(10~{\rm MeV})$ that have sizable couplings to neutrinos. One loop induced spin-independent scattering in dark matter may soon lead to a confirmation signal or already excludes regions of viable parameter space for the Rayleigh DM model, depending on what the exact values of the unknown nonperturbative nuclear matrix elements are. ",Shining dark matter in Xenon1T
67,1275248496170250240,2384206549,Pearl Sandick,['Could the XENON1T excess be a signal of boosted dark matter? Check out our new paper: <LINK> @XENONexperiment <LINK>'],https://arxiv.org/abs/2006.11264,"We propose boosted dark matter (BDM) as a possible explanation for the excess of keV electron recoil events observed by XENON1T. BDM particles have velocities much larger than those typical of virialized dark matter, and, as such, BDM-electron scattering can naturally produce keV electron recoils. We show that the required BDM-electron scattering cross sections can be easily realized in a simple model with a heavy vector mediator. Though these cross sections are too large for BDM to escape from the Sun, the BDM flux can originate from the Galactic Center or from halo dark matter annihilations. Furthermore, a daily modulation of the BDM signal will be present, which could not only be used to differentiate it from various backgrounds, but would also provide important directional information for the BDM flux. ",Boosted Dark Matter Interpretation of the XENON1T Excess
68,1275241300829384704,382008009,Miles Cranmer,"['Very excited to share our new paper ""Discovering Symbolic Models from Deep Learning with Inductive Biases""!\n\nWe describe an approach to convert a deep model into an equivalent symbolic equation.\n\nBlog/code: <LINK>\nPaper: <LINK>\n\nThread👇\n1/n <LINK>', 'Work w/ Alvaro Sanchez-Gonzalez and @PeterWBattaglia (@DeepMind), Rui Xu (@Princeton), @KyleCranmer (@NYUDataScience), and @DavidSpergel and @cosmo_shirley (@FlatironCCA).\n\n2/n', 'Symbolic models are the language of the natural sciences. Unlike deep models, they are compact, explicitly interpretable, and generalize well.\n\nSimple symbolic expressions are a uniquely powerful way of modeling the world. Though the origin of this connection is unknown:\n\n3/n https://t.co/aXbT80TMPE', 'In ML, one can learn symbolic models with genetic algorithms. However, these scale poorly with the number of input features.\n\nOn the other hand, deep neural networks can efficiently learn patterns in high dimensional spaces.\n\nCan we combine the strengths of both?\n4/n', 'We propose one such strategy:\n1. design a deep net with a separable internal structure and well-motivated inductive biases, like a graph neural network\n2. train it while encouraging sparse latent representations\n3. fit symbolic models to each distinct internal function\n\n5/n https://t.co/GbwlhaMDlO', 'Since each internal function only operates with a small number of input features, they can be easily approximated with symbolic regression!\n\nSo, the neural network has essentially broken the problem down into smaller parts that are easier to interpret.\n\n6/n', 'We first validate with small N-body simulations.\n\nBy encouraging sparsity in the messages for a GNN via L1 regularization, it actually learns rotations of the true Newtonian forces.\n\nThese can be easily extracted with symbolic regression:\n\n7/n https://t.co/sg6vcqArBD', 'If the force or other property in the messages is unknown, symbolic regression will give you an interpretable form of it. \n\nWe can see this happen over the course of training in the following video: https://t.co/99VNgiSdrh\n\n8/n', 'Finally, we apply our approach to a real-world problem: dark matter in cosmology. We attempt to discover a new analytic equation to predict the excess density inside a dark matter ""halo"" based on nearby structures.\n\n9/n https://t.co/E8wOXcIeWk', 'After training, the messages only show a single non-sparse feature, meaning the GNN is summing a property over nearby dark matter.\n\nWe approximate the internal functions of our GNN with symbolic regression, then compose them:\n\n(M_j is mass, r_ij distance, C_i constants)\n\n10/n https://t.co/zo2ABUQKNL', 'This is a new analytic expression for overdensity that our GNN discovered. It beats the hand-designed formula!\n\nRemarkably, this expression also generalizes better than the GNN it was extracted from! Analytic expressions seem to be a powerful prior.\n\nSee paper for details.\n\n11/n', 'Lastly, other people working on integration of symbolic + deep learning whose work you should check out includes: @eigensteve @AnimaAnandkumar @SASExperience @_beenkim @tegmark @GJ_Both @GuillaumeLample @GaryMarcus Kevin Ellis @pushmeet (far from a complete list!)\n\n12/n', 'Also, thanks to @samgreydanus for teaching me some of his blog post magic!\n\n13/13', ""@StephenRevesz Thanks! I really appreciate the comment.\n\nWe validated our approach by recovering forces + hamiltonians for the following potentials in 2+3 dimensions.\n\nOther than these and dark matter, we haven't tried others. I'm looking forward to see what people can do with it! https://t.co/StWz4TDmrI""]",https://arxiv.org/abs/2006.11287,"We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn. ",Discovering Symbolic Models from Deep Learning with Inductive Biases
69,1275165552432930816,20877181,Josh Susskind,"['Check out our @Apple research paper on Set Distribution Networks, led by @zhaisf  \n<LINK>\n\nWe introduce a new generative model capable of jointly encoding and sampling image sets. The model reconstructs sets and can generate new sets from a learned prior. <LINK>', 'Co-authors are @waltertalbott, @itsbautistam, @guestrin, and myself.', 'The model jointly learns an encoder that pools images into a discrete set code, a conditional discriminator (energy), and conditional generator and generator. In addition, a learned autoregressive prior controlsl free sampling. https://t.co/vFDtms0yQZ', 'Using an off-the-shelf 3D reconstruction model (Occupancy Networks) we can generate accurate 3D meshes from images generated from an SDN trained on ShapeNet categories for both reconstructions and free samples generated from the prior. https://t.co/ou6wLKYwKT', 'We also train an SDN on faces from VGGFace2 and show that it learns to generate realistic variations in pose, expression, hairstyle, etc., while maintaining traits consistent with identity, both for reconstructions and freely generated samples from the learned prior. https://t.co/ZyPPePOGAF', 'Check out the paper https://t.co/lmDT1xIrsM for more, including quantitative measures of the consistency of our generated sets for ShapeNet and VGGFace2 Experiments.']",https://arxiv.org/abs/2006.10705,"Images with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model $p(\mathbf{x}|\mathbf{y})$. This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images. ",Set Distribution Networks: a Generative Model for Sets of Images
70,1275116197193887744,174576749,Alexey Zaytsev,['Check out our new preprint on Adversarial attacks for discrete categorical sequences.\n\nWe can use any pre-trained language model to generate adversarial sequences.\n\nPaper: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2006.10704,"The video generation task can be formulated as a prediction of future video frames given some past frames. Recent generative models for videos face the problem of high computational requirements. Some models require up to 512 Tensor Processing Units for parallel training. In this work, we address this problem via modeling the dynamics in a latent space. After the transformation of frames into the latent space, our model predicts latent representation for the next frames in an autoregressive manner. We demonstrate the performance of our approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach tends to reduce requirements to 8 Graphical Processing Units for training the models while maintaining comparable generation quality. ",Latent Video Transformer
71,1275042834127687680,4639078397,John Wise,"[""New paper day on Powderday, led by @desikanarayanan. It calculates mock spectra and imaging using Hyperion and FSPS with @yt_astro as the glue. It's also the first paper for my 1st year grad student @Snickersnmocha ! <LINK> <LINK>"", 'A big thanks to all of my co-authors since I only joined the group late. On twitter (that I know of): @powersoffour @astrofrog @AshKelly0 @chrisclovell', 'I missed @gfsnyder !']",https://arxiv.org/abs/2006.10757,"We present Powderday, a flexible, fast, open-source dust radiative transfer package designed to interface with galaxy formation simulations. Powderday builds on FSPS population synthesis models, Hyperion dust radiative transfer, and employs yt to interface between different software packages. We include our stellar population synthesis modeling on the fly, which allows for significant run-time flexibility in the assumed stellar physics. We include a model for nebular line emission that can employ either precomputed Cloudy lookup tables (for efficiency), or direct photoionization calculations for all young stars (for flexibility). The dust content follows either observationally-motivated prescriptions, direct modeling from galaxy formation simulations, or a novel approach that includes the dust content via learning-based algorithms from the SIMBA cosmological galaxy formation simulation. AGN can additionally be included via a range of prescriptions. The output of these models are broadband SEDs, as well as filter-convolved images. Powderday is designed to eliminate last-mile efforts by researchers that employ different hydrodynamic galaxy formation models, and seamlessly interfaces with GIZMO, AREPO, GASOLINE, CHANGA, and ENZO. We demonstrate the capabilities of the code via three applications: a model for the star formation rate (SFR) - infrared luminosity relation in galaxies (including the impact of AGN); the impact of circumstellar dust around AGB stars on the mid-infrared emission from galaxy SEDs; and the impact of galaxy inclination angle on dust attenuation laws. ",Powderday: Dust Radiative Transfer for Galaxy Simulations
72,1275020428973690881,2453374310,Miguel Monteiro,['Check out our new interactive demo (<LINK>) for our paper Stochastic Segmentation Networks (<LINK>).\xa0\nFeaturing real-time sample generation and manipulation!\nPlease be patient while the images download and load from disk :) <LINK>'],https://arxiv.org/abs/2006.06015,"In image segmentation, there is often more than one plausible solution for a given input. In medical imaging, for example, experts will often disagree about the exact location of object boundaries. Estimating this inherent uncertainty and predicting multiple plausible hypotheses is of great interest in many applications, yet this ability is lacking in most current deep learning methods. In this paper, we introduce stochastic segmentation networks (SSNs), an efficient probabilistic method for modelling aleatoric uncertainty with any image segmentation network architecture. In contrast to approaches that produce pixel-wise estimates, SSNs model joint distributions over entire label maps and thus can generate multiple spatially coherent hypotheses for a single image. By using a low-rank multivariate normal distribution over the logit space to model the probability of the label map given the image, we obtain a spatially consistent probability distribution that can be efficiently computed by a neural network without any changes to the underlying architecture. We tested our method on the segmentation of real-world medical data, including lung nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform state-of-the-art for modelling correlated uncertainty in ambiguous images while being much simpler, more flexible, and more efficient. ","Stochastic Segmentation Networks: Modelling Spatially Correlated
  Aleatoric Uncertainty"
73,1274980231158411266,547776192,Chris Lovell,"['Pleasure to be (very peripherally) involved with @desikanarayanan and teams new paper on 🎇Powderday🎇, a dust radiative transfer package for hydro simulations \n\n<LINK>']",https://arxiv.org/abs/2006.10757,"We present Powderday, a flexible, fast, open-source dust radiative transfer package designed to interface with galaxy formation simulations. Powderday builds on FSPS population synthesis models, Hyperion dust radiative transfer, and employs yt to interface between different software packages. We include our stellar population synthesis modeling on the fly, which allows for significant run-time flexibility in the assumed stellar physics. We include a model for nebular line emission that can employ either precomputed Cloudy lookup tables (for efficiency), or direct photoionization calculations for all young stars (for flexibility). The dust content follows either observationally-motivated prescriptions, direct modeling from galaxy formation simulations, or a novel approach that includes the dust content via learning-based algorithms from the SIMBA cosmological galaxy formation simulation. AGN can additionally be included via a range of prescriptions. The output of these models are broadband SEDs, as well as filter-convolved images. Powderday is designed to eliminate last-mile efforts by researchers that employ different hydrodynamic galaxy formation models, and seamlessly interfaces with GIZMO, AREPO, GASOLINE, CHANGA, and ENZO. We demonstrate the capabilities of the code via three applications: a model for the star formation rate (SFR) - infrared luminosity relation in galaxies (including the impact of AGN); the impact of circumstellar dust around AGB stars on the mid-infrared emission from galaxy SEDs; and the impact of galaxy inclination angle on dust attenuation laws. ",Powderday: Dust Radiative Transfer for Galaxy Simulations
74,1274977425420701697,243241062,Dr Lachlan D. Urquhart,"['New paper with @jiahong_chen on the accountability principle, smart homes and domestic cybersecurity management - check it out here!  @UoELawResearch\n@HorizonDER  @EdCDCS @SCRIPTCentre  <LINK>']",https://arxiv.org/abs/2006.11043,"This chapter introduces the Accountability Principle and its role in data protection governance. We focus on what accountability means in the context of cybersecurity management in smart homes, considering the EU General Data Protection Law requirements to secure personal data. This discussion sits against the backdrop of two key new developments in data protection law. Firstly, the law is moving into the home, due to narrowing of the so called household exemption. Concurrently, household occupants may now have legal responsibilities to comply with the GDPR, as they find themselves jointly responsible for compliance, as they are possibly held to determine the means and purposes of data collection with IoT device vendors. As a complex socio-technical space, we consider the interactions between accountability requirements and the competencies of this new class of domestic data controllers (DDCs). Specifically, we consider the value and limitations of edge-based security analytics to manage smart home cybersecurity risks, reviewing a range of prototypes and studies of their use. We also reflect on interpersonal power dynamics in the domestic setting e.g. device control; existing social practices around privacy and security management in smart homes; and usability issues that may hamper DDCs ability to rely on such solutions. We conclude by reflecting on 1) the need for collective security management in homes and 2) the increasingly complex divisions of responsibility in smart homes between device users, account holders, IoT device/software/firmware vendors, and third parties. ","On the Principle of Accountability: Challenges for Smart Homes &
  Cybersecurity"
75,1274965799787278338,210965315,Malcolm Fairbairn,"[""Here is our paper on the recent @XENONexperiment excess.  Could it be due to new interactions between solar neutrinos and electrons?  Maybe, but it seems that stars wouldn't behave the way they do if those new interactions existed.  Anyway, exciting times <LINK>"", '@GordanKrnjaic @SunnyVagnozzi @XENONexperiment Well you can put the neutrino coupling small or the electron coupling small but not both.  So yeah, BBN Neff will be a problem.', '@GordanKrnjaic @SunnyVagnozzi @XENONexperiment For the neutrino coupling I would expect the BBN bounds to be very similar to the 1987a bounds.', '@SunnyVagnozzi @XENONexperiment Yeah we could make that work.  But only in a region where the coupling was bad for both supernovae and stars.', '@chamkaurghag @XENONexperiment Precisely!']",https://arxiv.org/abs/2006.11250,"We examine the recently-reported low-energy electron recoil spectrum observed at the XENON1T underground dark matter direct detection experiment, in the context of new interactions with solar neutrinos. In particular we show that scalar and vector mediators with masses $\lesssim 50$ keV coupled to leptons could already leave a visible signature in the XENON1T experiment, similar to the observed peak below 7 keV. This signals that dark matter detectors are already competing with neutrino scattering experiments such as GEMMA, CHARM-II and Borexino. If these results from XENON1T are interpreted as a new signal of such physics, the parameters which fit the excess face challenges from astrophysics which seem very difficult to overcome. If they are rather viewed as a constraint on new couplings, they herald the start of an era of novel precise probes of physics beyond the standard model with dark matter detectors. ",Light new physics in XENON1T
76,1274749036386496512,810166630412066817,Erik Daxberger,"['Our new paper (with @austinjtripp and @jmhernandez233) on sample-efficient optimization over highly structured input spaces (e.g. molecules) is out now! \n\nA detailed tweet will follow soon, but for those interested, see the image below for a sneak peek!😊\n\n<LINK> <LINK> <LINK>', '@laz_inc @austinjtripp @jmhernandez233 Thank you, Solomon!!']",https://arxiv.org/abs/2006.09191,"Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems. ","Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining"
77,1274684673147514880,1141380787163582464,Niko Hauzenberger,['New working paper online: Flexible mixture priors for time-varying parameter models (<LINK>) #econometrics'],https://arxiv.org/abs/2006.10088,"Time-varying parameter (TVP) models often assume that the TVPs evolve according to a random walk. This assumption, however, might be questionable since it implies that coefficients change smoothly and in an unbounded manner. In this paper, we relax this assumption by proposing a flexible law of motion for the TVPs in large-scale vector autoregressions (VARs). Instead of imposing a restrictive random walk evolution of the latent states, we carefully design hierarchical mixture priors on the coefficients in the state equation. These priors effectively allow for discriminating between periods where coefficients evolve according to a random walk and times where the TVPs are better characterized by a stationary stochastic process. Moreover, this approach is capable of introducing dynamic sparsity by pushing small parameter changes towards zero if necessary. The merits of the model are illustrated by means of two applications. Using synthetic data we show that our approach yields precise parameter estimates. When applied to US data, the model reveals interesting patterns of low-frequency dynamics in coefficients and forecasts well relative to a wide range of competing models. ",Flexible Mixture Priors for Large Time-varying Parameter Models
78,1274066323920822273,841499391508779008,Zico Kolter,"['Another new DEQ-related paper this week! @ezra_winston develops a framework for equilibrium models with unique fixed points convergent solvers, based upon monotone operator theory.\n\nPaper: <LINK>\nCode: <LINK>\nTalk: <LINK>', 'At a high level, the work illustrates a strong connection between the infinite-depth limit of some ""simple"" deep networks, and monotone operator splitting methods.  The result generally connects implicit-depth models to this well-studied paradigm. https://t.co/Tg3ijIOwvA', 'Works better than the ""pure"" version of other implicit-layer models like (augmented) Neural ODEs, while still providing existence, uniqueness, and stability guarantees (unlike traditional DEQ models).  Works well with convolutional networks using FFT-based techniques. https://t.co/uY8k5KwCjr']",https://arxiv.org/abs/2006.08591,"Implicit-depth models such as Deep Equilibrium Networks have recently been shown to match or exceed the performance of traditional deep networks while being much more memory efficient. However, these models suffer from unstable convergence to a solution and lack guarantees that a solution exists. On the other hand, Neural ODEs, another class of implicit-depth models, do guarantee existence of a unique solution but perform poorly compared with traditional networks. In this paper, we develop a new class of implicit-depth model based on the theory of monotone operators, the Monotone Operator Equilibrium Network (monDEQ). We show the close connection between finding the equilibrium point of an implicit network and solving a form of monotone operator splitting problem, which admits efficient solvers with guaranteed, stable convergence. We then develop a parameterization of the network which ensures that all operators remain monotone, which guarantees the existence of a unique equilibrium point. Finally, we show how to instantiate several versions of these models, and implement the resulting iterative solvers, for structured linear operators such as multi-scale convolutions. The resulting models vastly outperform the Neural ODE-based models while also being more computationally efficient. Code is available at this http URL ",Monotone operator equilibrium networks
79,1274050660242206722,153267451,Alexandre YK Bouquin,"['New (co-authored) paper on arXiv today! 🎉\n\nIn short, if we know the galaxy type (early or late), we can estimate its SMBH mass simply by looking at its UV-IR color. 🕶\n\n<LINK>']",https://arxiv.org/abs/2006.10128,"[Abridged] Tight correlations between supermassive black hole (SMBH) mass ($M_{\rm BH}$) and the properties of the host galaxy have useful implications for our understanding of the growth of SMBHs and evolution of galaxies. Here, we present newly observed correlations between $M_{\rm BH}$ and the host galaxy total UV$-$ [3.6] color ($\mathcal{C_{\rm UV,tot}}$, Pearson's r = $0.6-0.7$) for a sample of 67 galaxies (20 early-type galaxies and 47 late-type galaxies) with directly measured $M_{\rm BH}$ in the GALEX/S$^{4}$G survey. The colors are carefully measured in a homogeneous manner using the galaxies' FUV, NUV and 3.6 $\micron$ magnitudes and their multi-component structural decompositions in the literature. We find that more massive SMBHs are hosted by (early- and late-type) galaxies with redder colors, but the $M_{\rm BH}- \mathcal{C_{\rm UV,tot}}$ relations for the two morphological types have slopes that differ at $\sim 2 \sigma$ level. Early-type galaxies define a red sequence in the $M_{\rm BH}- \mathcal{C_{\rm UV,tot}}$ diagrams, while late-type galaxies trace a blue sequence. Within the assumption that the specific star formation rate of a galaxy (sSFR) is well traced by $L_{\rm UV}/L_{\rm 3.6}$, it follows that the SMBH masses for late-type galaxies exhibit a steeper dependence on sSFR than those for early-type galaxies. The $M_{\rm BH}- \mathcal{C_{\rm UV,tot}}$ and $M_{\rm BH}-L_{\rm 3.6,tot}$ relations for the sample galaxies reveal a comparable level of vertical scatter in the log $M_{\rm BH}$ direction, roughly $5\%-27\%$ more than the vertical scatter of the $M_{\rm BH}-\sigma$ relation. Our $M_{\rm BH}- \mathcal{C_{\rm UV,tot}}$ relations suggest different channels of SMBH growth for early- and late-type galaxies, consistent with their distinct formation and evolution scenarios. ","The (black hole mass)-(color) relations for early- and late-type
  galaxies: red and blue sequences"
80,1274028380552343559,53344170,James Allingham,"['Looking for some weekend reading? Check out my new paper ""Depth Uncertainty in Neural Networks"" with @JaviAC7 and @jmhernandez233! We perform inference over depth and train NNs that provide computationally cheap but well-calibrated uncertainty estimates! <LINK> <LINK>', 'Our Depth Uncertainty Networks (DUNs) are composed of subnetworks of increasing depth (colours denote layers with shared parameters). https://t.co/xHSpiRdQBi', 'These subnetworks correspond to increasingly complex functions (colours denote depth at which predictions are made). https://t.co/jsYjt5gRKq', 'Marginalising over depth yields model uncertainty (shown as error bars) through disagreement of these functions. https://t.co/lNwbmbE2vr', 'We provide experimental results for toy, tabular regression, and image classification datasets, and find that DUNs are competitive with more expensive baselines. For example, we test DUNs on the corrupted CIFAR10 dataset (annotations denote # MC samples or ensemble elements). https://t.co/WJoVj2vDJT', 'We also provide some interesting results showing that training with VI rather than exact MLL provides better optima for DUNs. https://t.co/nw5oL3GNkC', 'In summary, DUNs are simple to implement, cheap to deploy, and provide calibrated uncertainty estimates!\n\nThe paper can be found here: https://t.co/rzLEDxwK6E\nAnd code to train DUNs and reproduce all of our experiments is here: https://t.co/bZtJFwQc8R']",https://arxiv.org/abs/2006.08437,"Existing methods for estimating uncertainty in deep learning tend to require multiple forward passes, making them unsuitable for applications where computational resources are limited. To solve this, we perform probabilistic reasoning over the depth of neural networks. Different depths correspond to subnetworks which share weights and whose predictions are combined via marginalisation, yielding model uncertainty. By exploiting the sequential structure of feed-forward networks, we are able to both evaluate our training objective and make predictions with a single forward pass. We validate our approach on real-world regression and image classification tasks. Our approach provides uncertainty calibration, robustness to dataset shift, and accuracies competitive with more computationally expensive baselines. ",Depth Uncertainty in Neural Networks
81,1274026873857740801,234888240,konstantin herbst,"['Can GCRs really be neglected in the HZs of M-stars? To learn more, check our new paper ""On the Diversity of M-Star Astrospheres and the Role of Galactic Cosmic Rays Within"" accepted at ApJ Letters, now available on arXiv: <LINK>\n@Dr_Know_SA']",https://arxiv.org/abs/2006.09941,"With upcoming missions such as the James Webb Space Telescope (JWST), the European Extremely Large Telescope (ELT), and the Atmospheric Remote-sensing Infrared Exoplanet Large-survey (ARIEL), we soon will be on the verge of detecting and characterizing Earth-like exoplanetary atmospheres for the first time. These planets are most likely to be found around smaller and cooler K- and M-type stars. However, recent observations showed that their radiation environment might be much harsher than that of the Sun. Thus, the exoplanets are most likely exposed to an enhanced stellar radiation environment, which could affect their habitability, for example, in the form of a hazardous flux of energetic particles. Knowing the stellar radiation field, and being able to model the radiation exposure on the surface of a planet is crucial to assess its habitability. In this study, we present 3D magnetohydrodynamic (MHD)-based model efforts investigating M-stars, focusing on V374 Peg, Proxima Centauri, and LHS 1140, chosen because of their diverse astrospheric quantities. We show that V374 Peg has a much larger astrosphere (ASP) than our Sun, while Proxima Centauri and LHS 1140 most likely have ASPs comparable or even much smaller than the heliosphere, respectively. Based on a 1D transport model, for the first time, we provide numerical estimates of the modulation of Galactic cosmic rays (GCRs) within the three ASPs. We show that the impact of GCRs on the Earth-like exoplanets Proxima Centauri b and LHS 1140 b cannot be neglected in the context of exoplanetary habitability. ","On the Diversity of M-Star Astrospheres and the Role of Galactic Cosmic
  Rays Within"
82,1274004065899380736,472602187,Chris H,"['New paper from @_aylien science, with Demian Gholipour Ghalandari,  @anotherjohng , and @NghiaPh78451947  <LINK>\nTL;DR we get good results on multi-document summarization with a simple ensembling method that takes advantage of powerful pre-trained LMs like BART', 'code here: https://t.co/jLnHFEvCO0']",https://arxiv.org/abs/2006.08748,"Sequence-to-sequence (s2s) models are the basis for extensive work in natural language processing. However, some applications, such as multi-document summarization, multi-modal machine translation, and the automatic post-editing of machine translation, require mapping a set of multiple distinct inputs into a single output sequence. Recent work has introduced bespoke architectures for these multi-input settings, and developed models which can handle increasingly longer inputs; however, the performance of special model architectures is limited by the available in-domain training data. In this work we propose a simple decoding methodology which ensembles the output of multiple instances of the same model on different inputs. Our proposed approach allows models trained for vanilla s2s tasks to be directly used in multi-input settings. This works particularly well when each of the inputs has significant overlap with the others, as when compressing a cluster of news articles about the same event into a single coherent summary, and we obtain state-of-the-art results on several multi-document summarization datasets. ",DynE: Dynamic Ensemble Decoding for Multi-Document Summarization
83,1273943009701306368,3306943245,Konstantin Klemmer,"['Very happy to present my new #GeoML 🌏🛰️ paper: SXL, a framework for better learning with geographic data using spatially explicit auxiliary tasks\n\nJoint work with Daniel Neill at @NYU_CUSP. \n\nPaper: <LINK>\nCode: <LINK>\nThread: 🔽\n\n1/9 <LINK>', ""We propose to use the local Moran's I metric in auxiliary tasks for training generative and predictive models. As a detector for spatial outliers / clusters, MI can help the learner to better understand and account for spatial dependencies in the data. 2/9"", 'This idea is motivated by (1) the lack of current deep learning methods dealing with complex geospatial patterns and (2) the success of spatial perception auxiliary tasks in computer vision, such as depth prediction or surface normal maps. 3/9', ""As MI is known to be quite scale sensitive, we propose a new, multi-resolution local Moran's I to capture (local) spatial effects at different scales and distances. We first downsample the input, compute the coarsened MI and upsample again to the original input size. 4/9 https://t.co/r8AJ1krMSE"", 'We use the normal MI and the new multi-res variant as auxiliary tasks in generative modeling (with GANs) and spatial interpolation experiments. We show that in both cases, the auxiliary tasks lead to a better performance on the primary task! 5/9', 'For GANs, we can observe that our approach helps to avoid oversmoothing of spatial patterns and reduces (partial) mode collapse, leading to overall better synthetic data. 6/9 https://t.co/pnQNpkooQe', 'For spatial interpolation, we can also show performance increases over ""Vanilla"" neural nets and common geospatial interpolation benchmarks. 7/9 https://t.co/ofE42B4sZz', 'All our code is implemented in @PyTorch and available on Github (see link above). \n\nIf you have any questions about the project or ideas on how to make #GeoML even better, please reach out to me! 8/9', 'TL;DR: Learning spatial autocorrelation patterns as auxiliary tasks helps learners to perform better on the primary task, when working in the geospatial domain\n🌏🤖🛰️ \n#Geospatial #MachineLearning #GeoAI\n9/9']",http://arxiv.org/abs/2006.10461,"Machine learning is gaining popularity in a broad range of areas working with geographic data, such as ecology or atmospheric sciences. Here, data often exhibit spatial effects, which can be difficult to learn for neural networks. In this study, we propose SXL, a method for embedding information on the autoregressive nature of spatial data directly into the learning process using auxiliary tasks. We utilize the local Moran's I, a popular measure of local spatial autocorrelation, to ""nudge"" the model to learn the direction and magnitude of local spatial effects, complementing the learning of the primary task. We further introduce a novel expansion of Moran's I to multiple resolutions, thus capturing spatial interactions over longer and shorter distances simultaneously. The novel multi-resolution Moran's I can be constructed easily and as a multi-dimensional tensor offers seamless integration into existing machine learning frameworks. Throughout a range of experiments using real-world data, we highlight how our method consistently improves the training of neural networks in unsupervised and supervised learning tasks. In generative spatial modeling experiments, we propose a novel loss for auxiliary task GANs utilizing task uncertainty weights. Our proposed method outperforms domain-specific spatial interpolation benchmarks, highlighting its potential for downstream applications. This study bridges expertise from geographic information science and machine learning, showing how this integration of disciplines can help to address domain-specific challenges. The code for our experiments is available on Github: this https URL ","Auxiliary-task learning for geographic data with autoregressive
  embeddings"
84,1273928448134778880,2585907650,Leslie Smith,"['Thanks Jeremy.  Please do check out my new paper ""Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance"" at <LINK> <LINK>']",https://arxiv.org/abs/2006.09363,"Reaching the performance of fully supervised learning with unlabeled data and only labeling one sample per class might be ideal for deep learning applications. We demonstrate for the first time the potential for building one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test accuracies that are comparable to fully supervised learning. Our method combines class prototype refining, class balancing, and self-training. A good prototype choice is essential and we propose a technique for obtaining iconic examples. In addition, we demonstrate that class balancing methods substantially improve accuracy results in semi-supervised learning to levels that allow self-training to reach the level of fully supervised learning performance. Rigorous empirical evaluations provide evidence that labeling large datasets is not necessary for training deep neural networks. We made our code available at this https URL to facilitate replication and for use with future real-world applications. ","Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised
  Performance"
85,1273905827234091009,3287937939,Sarah Bosman,['New paper on arXiv today! Putting quasar spectral reconstructions to the test. \n\nOlder techniques have biases &amp; it explains why previous measurements of reionisation disagreed.\n\nNewer techniques good (pictured).\n\n<LINK>\nw/ @freddavies <LINK>'],https://arxiv.org/abs/2006.10744,"Reconstruction techniques for intrinsic quasar continua are crucial for the precision study of Lyman-$\alpha$ (Ly-$\alpha$) and Lyman-$\beta$ (Ly-$\beta$) transmission at $z>5.0$, where the $\lambda<1215 A$ emission of quasars is nearly completely absorbed. While the number and quality of spectroscopic observations has become theoretically sufficient to quantify Ly-$\alpha$ transmission at $5.0<z<6.0$ to better than $1\%$, the biases and uncertainties arising from predicting the unabsorbed continuum are not known to the same level. In this paper, we systematically evaluate eight reconstruction techniques on a unified testing sample of $2.7<z<3.5$ quasars drawn from eBOSS. The methods include power-law extrapolation, stacking of neighbours, and six variants of Principal Component Analysis (PCA) using direct projection, fitting of components, or neural networks to perform weight mapping. We find that power-law reconstructions and the PCA with fewest components and smallest training sample display the largest biases in the Ly-$\alpha$ forest ($-9.58\%/+8.22\%$ respectively). Power-law extrapolations have larger scatters than previously assumed of $+13.1\%/-13.2\%$ over Ly-$\alpha$ and $+19.9\%/-20.1\%$ over Ly-$\beta$. We present two new PCAs which achieve the best current accuracies of $9\%$ for Ly-$\alpha$ and $17\%$ for Ly-$\beta$. We apply the eight techniques after accounting for wavelength-dependent biases and scatter to a sample $19$ quasars at $z>5.7$ with IR X-Shooter spectroscopy, obtaining well-characterised measurements for the mean flux transmission at $4.7<z<6.3$. Our results demonstrate the importance of testing and, when relevant, training, continuum reconstruction techniques in a systematic way. ","A comparison of quasar emission reconstruction techniques for $z\geq5.0$
  Lyman-$\alpha$ and Lyman-$\beta$ transmission"
86,1273893100574789633,1120626288807428096,Jonathan Scarlett,"['New group testing paper (RANDOM 2020) putting the ""non-"" in ""non-adaptive binary splitting"", with O(k log n) tests and decoding time: <LINK> (1/2) <LINK>', 'See also https://t.co/KS8PtiLrkK for concurrent work by @cheraghchi &amp; V. Nakos introducing the same idea, and applying it in additional contexts including heavy hitters &amp; compressed sensing (2/2)']",https://arxiv.org/abs/2006.10268,"In this paper, we consider the problem of noiseless non-adaptive group testing under the for-each recovery guarantee, also known as probabilistic group testing. In the case of $n$ items and $k$ defectives, we provide an algorithm attaining high-probability recovery with $O(k \log n)$ scaling in both the number of tests and runtime, improving on the best known $O(k^2 \log k \cdot \log n)$ runtime previously available for any algorithm that only uses $O(k \log n)$ tests. Our algorithm bears resemblance to Hwang's adaptive generalized binary splitting algorithm (Hwang, 1972); we recursively work with groups of items of geometrically vanishing sizes, while maintaining a list of ""possibly defective"" groups and circumventing the need for adaptivity. While the most basic form of our algorithm requires $\Omega(n)$ storage, we also provide a low-storage variant based on hashing, with similar recovery guarantees. ",A Fast Binary Splitting Approach to Non-Adaptive Group Testing
87,1273878952520507392,762420558,Ciaran O'Hare,"['New paper on solar axions. Given the recent XENON excitement, thought it timely to push this one out too: <LINK> <LINK>', 'We look at a new source of axions from the Sun (aside from the usual Primakoff and ABC fluxes) which are produced from the conversion of longitudinal plasmons. This has the side effect of the expected axion signal being very sensitive to the magnetic field of the Sun.', 'So we took a look at whether the planned next-gen helioscope #IAXO which will be hosted @desy could serve a dual-purpose as an instrument to measure the solar B-field. This of course requires that solar axions are discovered first (maybe they already have?....)', 'Nevertheless the prospects look quite good we think. The flux effectively probes the Sun from the inside out: the better the energy threshold the farther out in radius you can probe.', 'The code is available for you to have a look at here: https://t.co/7Qb24OxEhn']",https://arxiv.org/abs/2006.10415,"Axion helioscopes search for solar axions and axion-like particles via inverse Primakoff conversion in strong laboratory magnets pointed at the Sun. Anticipating the detection of solar axions, we determine the potential for the planned next-generation helioscope, the International Axion Observatory (IAXO), to measure or constrain the solar magnetic field. To do this we consider a previously neglected component of the solar axion flux at sub-keV energies arising from the conversion of longitudinal plasmons. This flux is sensitively dependent to the magnetic field profile of the Sun, with lower energies corresponding to axions converting into photons at larger solar radii. If the detector technology eventually installed in IAXO has an energy resolution better than 200 eV, then solar axions could become an even more powerful messenger than neutrinos of the magnetic field in the core of the Sun. For energy resolutions better than 10 eV, IAXO could access the inner 70% of the Sun and begin to constrain the field at the tachocline: the boundary between the radiative and convective zones. The longitudinal plasmon flux from a toroidal magnetic field also has an additional 2% geometric modulation effect which could be used to measure the angular dependence of the magnetic field. ",Axion helioscopes as solar magnetometers
88,1273781157524574209,1128608544431861760,Wei Jin,"['📣Our paper ""Self-supervised Learning on Graphs: Deep Insights and New Direction"" is now on arxiv <LINK>.\n\nWe explore when/why/how self-supervised learning can be used in GNNs and introduce SelfTask that obtains SOTA performance. \n\nCode: <LINK>. <LINK>']",https://arxiv.org/abs/2006.10141,"The success of deep learning notoriously requires larger amounts of costly annotated data. This has led to the development of self-supervised learning (SSL) that aims to alleviate this limitation by creating domain specific pretext tasks on unlabeled data. Simultaneously, there are increasing interests in generalizing deep learning to the graph domain in the form of graph neural networks (GNNs). GNNs can naturally utilize unlabeled nodes through the simple neighborhood aggregation that is unable to thoroughly make use of unlabeled nodes. Thus, we seek to harness SSL for GNNs to fully exploit the unlabeled data. Different from data instances in the image and text domains, nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.). Such complexity is a double-edged sword for SSL on graphs. On the one hand, it determines that it is challenging to adopt solutions from the image and text domains to graphs and dedicated efforts are desired. On the other hand, it provides rich information that enables us to build SSL from a variety of perspectives. Thus, in this paper, we first deepen our understandings on when, why, and which strategies of SSL work with GNNs by empirically studying numerous basic SSL pretext tasks on graphs. Inspired by deep insights from the empirical studies, we propose a new direction SelfTask to build advanced pretext tasks that are able to achieve state-of-the-art performance on various real-world datasets. The specific experimental settings to reproduce our results can be found in \url{this https URL}. ",Self-supervised Learning on Graphs: Deep Insights and New Direction
89,1273778294048149511,1376287471,Tharindu Jayasinghe,"['We have a new paper on astro-ph tonight! This work is the culmination of our search for variable stars in the @SuperASASSN V-band data, where we systematically characterized the variability of ~61.5 million stars with V&lt;17 mag.\n\n<LINK> <LINK>', 'Through this endeavor, we discovered ~219,000 new variable stars and identified a total of ~426,000 bright variable stars across the entire sky. Most of our discoveries were in the south. ASAS-SN has substantially improved the consensus of bright variable stars. https://t.co/bfBqrsOiOW', 'Due to our survey characteristics and all-sky coverage, ASAS-SN has excellent synergy with wide-field spectroscopic surveys like APOGEE and LAMOST. We are able to cross-match our variables to these spectroscopic datasets to gain insight on the different types of variable stars. https://t.co/pJ9DdthoZs', 'In this paper, we largely focus on eclipsing binaries (EBs), rotational variables (ROT) and semi-regular variables (SR). We find that the period-temperature distributions of EBs depend on metallicity, with metal-poor binaries having shorter periods at fixed Teff. https://t.co/SwAkViarsJ', 'We find ROT variables on the MS, RGB and the red clump. Curiously, different measures seem to indicate that most (~80%) of the evolved ROT variables are rapidly rotating with vsini&gt;10 km/s. https://t.co/VYMXkT0Pn2', 'We also find that the ROT variables in APOGEE have unusual abundances, distinct from the typical giants and SR variables. We speculate that this could be due to issues in the ASPCAP pipeline when dealing with rapidly rotating giants. https://t.co/VUNuocTizB', 'We work with the wonderful APOGEE DR16 data to study Mira/SR variables (TP-AGB) stars in detail. These are cool, highly evolved, metal-poor stars at the ends of their lives. Most of the Mira/SR variables in our catalog seem to O-rich! https://t.co/voIf2Y8wAn', ""To the O-rich AGB stars, we are able to fit a temperature-'color' relation that returns temperatures to within 0.7% on average. All you need to get a temperature estimate with this fit are Gaia and 2MASS photometry data. https://t.co/si6JNrMkvy"", 'Looking at the abundances of the SR variables, we see that these variables are metal-poor compared to most giants in APOGEE, and we find that the shifts in the high+low-alpha peaks in various abundances are suggestive of alpha enrichment during the TP-AGB (third dredge up?). https://t.co/WqeVSTxQkQ', ""We also find that the Aluminium abundances of these stars are strongly correlated with their pulsation period. We identify 'high-Al' and 'low-Al' sequences for the SR variables. Long-period SR variables appear to be depleted in Al when compared to shorter period sources! https://t.co/KVakp7Dd1a"", 'That was a brief summary of this lengthy paper. Please take a look at the pre-print for more results: https://t.co/CWifZKc5md', 'The catalog of variable stars is publicly accessible here: https://t.co/bBskmhsyE0\nThe V-band light curves of all the ~61.5 million stars are accessible here: \nhttps://t.co/FIiNn2waIn', '@Rohan_Naidu @SuperASASSN Thank you so much Rohan! :)']",https://arxiv.org/abs/2006.10057,"The All-Sky Automated Survey for Supernovae (ASAS-SN) provides long baseline (${\sim}4$ yrs) $V-$band light curves for sources brighter than V$\lesssim17$ mag across the whole sky. We produced V-band light curves for a total of ${\sim}61.5$ million sources and systematically searched these sources for variability. We identified ${\sim} 426,000$ variables, including ${\sim} 219,000$ new discoveries. Most (${\sim}74\%$) of our discoveries are in the Southern hemisphere. Here we use spectroscopic information from LAMOST, GALAH, RAVE, and APOGEE to study the physical and chemical properties of these variables. We find that metal-poor eclipsing binaries have orbital periods that are shorter than metal-rich systems at fixed temperature. We identified rotational variables on the main-sequence, red giant branch and the red clump. A substantial fraction (${\gtrsim}80\%$) of the rotating giants have large $v_{\rm rot}$ or large NUV excesses also indicative of fast rotation. The rotational variables have unusual abundances suggestive of analysis problems. Semi-regular variables tend to be lower metallicity ($\rm [Fe/H]{\sim}-0.5$) than most giant stars. We find that the APOGEE DR16 temperatures of oxygen-rich semi-regular variables are strongly correlated with the $W_{RP}-W_{JK}$ color index for $\rm T_{eff}\lesssim3800$ K. Using abundance measurements from APOGEE DR16, we find evidence for Mg and N enrichment in the semi-regular variables. We find that the Aluminum abundances of the semi-regular variables are strongly correlated with the pulsation period, where the variables with $\rm P\gtrsim 60$ days are significantly depleted in Al. ","The ASAS-SN Catalog of Variable Stars IX: The Spectroscopic Properties
  of Galactic Variable Stars"
90,1273677168279793668,115805003,Matteo Cantiello,['We have a possible explanation for the puzzling dichotomy observed in the distribution of surface magnetic fields in early-type stars. New paper with @FlatironCCA fellow Adam Jermyn <LINK> <LINK>'],https://arxiv.org/abs/2006.08618,"In early-type stars a fossil magnetic field may be generated during the star formation process or be the result of a stellar merger event. Surface magnetic fields are thought to be erased by (sub)surface convection layers, which typically leave behind weak disordered fields. However, if the fossil field is strong enough it can prevent the onset of (sub)surface convection and so be preserved onto the main sequence. We calculate the critical field strength at which this occurs, and find that it corresponds well with the lower limit amplitude of observed fields in strongly magnetised Ap/Bp stars ($\approx$ 300 G). The critical field strength is predicted to increase slightly during the main sequence evolution, which could also explain the observed decline in the fraction of magnetic stars. This supports the conclusion that the bimodal distribution of observed magnetic fields in early-type stars reflects two different field origin stories: strongly magnetic fields are fossils fields inherited from star formation or a merger event, and weak fields are the product of on-going dynamo action. ","The Origin of the Bimodal Distribution of Magnetic Fields in Early-type
  Stars"
91,1273637019911102467,1098681399291330560,Jacob Montgomery,"['Here\'s a new paper joint with @jb_duckmayr and Roman Garnett  applying machine learning ideas to item response measurement models.  We will be presenting at #UAI2020.🧵\n\n""GPIRT: A Gaussian Process Model for Item Response Theory"" \n\n<LINK>', 'TL;DR\n\nWe put Gaussian process priors over the item response functions that map between the latent trait and the observed response.  This allow us to recover any smooth functional form. https://t.co/pD8QpkL9g6', ""The added flexibility let's us estimate interesting characteristics for specific items (like these votes in the 116th Congress), improves fit (in sample and out-of-sample), and can be extended to active learning for test construction/computerized adaptive testing. https://t.co/6CTyaJN8ey"", 'This is my first CS paper.  A pretty different peer review process and very different set of reviewer concerns to worry about.  Grateful to the amazing Roman for showing me the ropes and to  @jb_duckmayr for doing all of the work. \n\nSoftware: https://t.co/8uKWJMekAG']",https://arxiv.org/abs/2006.09900,"The goal of item response theoretic (IRT) models is to provide estimates of latent traits from binary observed indicators and at the same time to learn the item response functions (IRFs) that map from latent trait to observed response. However, in many cases observed behavior can deviate significantly from the parametric assumptions of traditional IRT models. Nonparametric IRT models overcome these challenges by relaxing assumptions about the form of the IRFs, but standard tools are unable to simultaneously estimate flexible IRFs and recover ability estimates for respondents. We propose a Bayesian nonparametric model that solves this problem by placing Gaussian process priors on the latent functions defining the IRFs. This allows us to simultaneously relax assumptions about the shape of the IRFs while preserving the ability to estimate latent traits. This in turn allows us to easily extend the model to further tasks such as active learning. GPIRT therefore provides a simple and intuitive solution to several longstanding problems in the IRT literature. ",GPIRT: A Gaussian Process Model for Item Response Theory
92,1273628976112533505,438692340,"Donald Martin, Jr.","['Understanding societal systems is more important than ever.  I’m super-excited to announce that my new paper  with  @vinodkpg, @jillkuhlberg, and @andrewthesmart on extending the ML design abstraction boundary to incorporate societal context is available:  <LINK>', 'Building off of work by @aselbst, we leverage complex adaptive systems theory to propose a taxonomic model of societal context and introduce the concept of collaborative causal theory formulation (CCTF) as a key discipline for discovering and considering societal context. https://t.co/XNLMbdg0rg', 'This paper was the basis for our  Participatory Problem Formulation paper presented at #ICLR2020.  https://t.co/9SpVA223q5', '... and + @wsisaac out other key co-author. Sorry about that William!']",https://arxiv.org/abs/2006.09663,"Machine learning (ML) fairness research tends to focus primarily on mathematically-based interventions on often opaque algorithms or models and/or their immediate inputs and outputs. Such oversimplified mathematical models abstract away the underlying societal context where ML models are conceived, developed, and ultimately deployed. As fairness itself is a socially constructed concept that originates from that societal context along with the model inputs and the models themselves, a lack of an in-depth understanding of societal context can easily undermine the pursuit of ML fairness. In this paper, we outline three new tools to improve the comprehension, identification and representation of societal context. First, we propose a complex adaptive systems (CAS) based model and definition of societal context that will help researchers and product developers to expand the abstraction boundary of ML fairness work to include societal context. Second, we introduce collaborative causal theory formation (CCTF) as a key capability for establishing a sociotechnical frame that incorporates diverse mental models and associated causal theories in modeling the problem and solution space for ML-based products. Finally, we identify community based system dynamics (CBSD) as a powerful, transparent and rigorous approach for practicing CCTF during all phases of the ML product development process. We conclude with a discussion of how these systems theoretic approaches to understand the societal context within which sociotechnical systems are embedded can improve the development of fair and inclusive ML-based products. ","Extending the Machine Learning Abstraction Boundary: A Complex Systems
  Approach to Incorporate Societal Context"
93,1273627434005471233,931179613178515459,Adilson E. Motter,"['Our Science Advances paper “Spontaneous Oscillations and Negative-Conductance Transitions in Microfluidic Networks” is now in the arXiv: <LINK>\n\nIt is all about networks designed to exhibit nonlinear behaviors that enable new built-in flow control capabilities. <LINK> <LINK>', 'These network are shown to exhibit oscillatory output patterns, bistable flow states, hysteresis, signal amplification, and negative-conductance transitions, all without reliance on dedicated external control hardware, movable parts, flexible components, OR oscillatory inputs!']",https://arxiv.org/abs/2006.09400,"The tendency for flows in microfluidic systems to behave linearly poses a challenge for designing integrated flow control schemes to carry out complex fluid processing tasks. This hindrance has led to the use of numerous external control devices to manipulate flows, thereby thwarting the potential scalability and portability of lab-on-a-chip technology. Here, we devise a microfluidic network exhibiting nonlinear flow dynamics that enable new mechanisms for on-chip flow control. This network is shown to exhibit oscillatory output patterns, bistable flow states, hysteresis, signal amplification, and negative-conductance transitions, all without reliance on dedicated external control hardware, movable parts, flexible components, or oscillatory inputs. These dynamics arise from nonlinear fluid inertia effects in laminar flows that we amplify and harness through the design of the network geometry. We suggest that these results, which are supported by fluid dynamical simulations and theoretical modeling, have the potential to inspire development of new built-in control capabilities, such as on-chip timing and synchronized flow patterns. ","Spontaneous oscillations and negative-conductance transitions in
  microfluidic networks"
94,1273567448042803200,1655032190,Enrico Corsaro,['The new FAMED pipeline for auomated peak bagging of solar-like oscillations is now free for download at \n<LINK>\nOnline docs are available here\n<LINK>\nCheck also the newly accepted paper on A&amp;A\n<LINK>\n#codes #astrophysics <LINK>'],https://arxiv.org/abs/2006.08245,"Stars of low and intermediate mass that exhibit oscillations may show tens of detectable oscillation modes each. Oscillation modes are a powerful to constrain the internal structure and rotational dynamics of the star, hence tool allowing one to obtain an accurate stellar age. The tens of thousands of solar-like oscillators that have been discovered thus far are representative of the large diversity of fundamental stellar properties and evolutionary stages available. Because of the wide range of oscillation features that can be recognized in such stars, it is particularly challenging to properly characterize the oscillation modes in detail, especially in light of large stellar samples. Overcoming this issue requires an automated approach, which has to be fast, reliable, and flexible at the same time. In addition, this approach should not only be capable of extracting the oscillation mode properties of frequency, linewidth, and amplitude from stars in different evolutionary stages, but also able to assign a correct mode identification for each of the modes extracted. Here we present the new freely available pipeline FAMED (Fast and AutoMated pEak bagging with DIAMONDS), which is capable of performing an automated and detailed asteroseismic analysis in stars ranging from the main sequence up to the core-Helium-burning phase of stellar evolution. This, therefore, includes subgiant stars, stars evolving along the red giant branch (RGB), and stars likely evolving toward the early asymptotic giant branch. In this paper, we additionally show how FAMED can detect rotation from dipolar oscillation modes in main sequence, subgiant, low-luminosity RGB, and core-Helium-burning stars. FAMED can be downloaded from its public GitHub repository (this https URL). ",Fast and Automated Peak Bagging with DIAMONDS (FAMED)
95,1273565086771224577,1135614461685960704,Julius von Kügelgen,"['1/ Excited to share our new paper:\n\n""Algorithmic recourse under imperfect causal knowledge: a probabilistic approach""\n(<LINK>)\n\n-- joint work with the excellent Amir-Hossein Karimi (@heikalkhan), Bernhard Schölkopf (@bschoelkopf), and Isabel Valera (@IValeraM). <LINK>', '2/ We tackle the task of ""algorithmic recourse"": providing individuals with actionable recommendations (in the form of interventions) to recover from an unfavourable prediction obtained from an (automated) decision-making system.\n#XAI', '3/ This requires taking the causal structure between features into account (as argued in this prior work: https://t.co/llB7FW2EhF)---an aspect commonly ignored in the related area of so-called ""counterfactual"" explanations, which treat all features as independently manipulable.', '4/ To reason formally about downstream effects of actions on other features and compute counterfactual outcomes, we adopt the structural causal model (SCM) framework of @yudapearl. \n#bookofwhy', '5/ As a motivating negative result, we first show that, without restrictions on the class of SCMs, algorithmic recourse can only be guaranteed if the true structural equations are known.\nSketch: construct 2 empirically indistinguishable SCMs that imply different counterfactuals.', '6/ Unfortunately, the structural equations are generally not known in practice, and assumptions on their form are often untestable since counterfactual data is, by definition, never available.', '7/ Therefore, we propose two probabilistic approaches for algorithmic recourse under imperfect causal knowledge, which do not assume the structural equations to be known but operate under weaker assumptions.', '8/ The first approach assumes additive Gaussian noise and uses probabilistic (GP) regression to average over a family of smooth SCMs to obtain counterfactual distributions under hypothetical actions performed by the individual who seeks recourse.', '9/ The second approach makes no assumptions about the structural equations and only requires the causal graph to be known. It relies on computing the effect of hypothetical recourse actions on a group of individuals who share certain characteristics with the one seeking recourse.', '10/ The latter constitutes a novel subpopulation-based notion of recourse that is inspired by the concept of conditional average treatment effects (CATE). https://t.co/UMM99yvF6R', '11/ Both approaches can be used to recommend actions (interventions) that achieve recourse with high probability, where the desired confidence level can be traded off against the cost of the selected actions with a ""conservativeness-hyperparameter"". https://t.co/OiiMD6e0KD', '12/ For differentiable classifiers, we show how the resulting constrained optimisation problems [min cost s.t. recourse is achieved w. high prob.] can be reformulated as an unconstrained saddle-point problem that can be tackled with gradient descent.', '13/ Many challenges remain such as how to deal with hidden confounding or only partially specified causal graphs, and we are excited to further explore these directions in the future.\n\nAny comments or feedback you may have are warmly appreciated!']",https://arxiv.org/abs/2006.06831,"Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines. ","Algorithmic recourse under imperfect causal knowledge: a probabilistic
  approach"
96,1273557885147058177,94167230,László Molnár,"['Our paper about Betelgeuse, led by @MeridithJoyceGR, is finally ready! Submitted and ready for your comments at <LINK> (and here)\nWe set out to explain the dip - we did a TON of things but that: new light curve, new evo and seismic models, new hydro models. <LINK>', ""We did our trusted evo+seismic approach with MESA+GYRE. Some highlights:\n- B had to go through a merger, initial fast-rotator models don't work. \n- tho @emsque measured Teff by +-25K (yay!), different mixing lengths can shift tracks +- 200K, making evo track fitting uncertain. https://t.co/k1MxF4q3OY"", ""Don't despair tho! B is also a pulsating star so we have a seismic constraint. Turns out the period constrains the mass and radius pretty effectively. We say 750(+62-30) R_Sun and 16.5-19 M_Sun for present-day radius and mass. https://t.co/6qgOucFxgu"", 'Biggest feat? We know the angular diameter, we now have a radius: we can calculate distance! I present you the first seismic parallax/distance of Betelgeuse, 165(+16-8) pc. This agrees with the Hipparcos data, but closer than the latest Hipparcos+radio combined result. https://t.co/DUavfeTC9x', 'Why? We hypothesize that persistent hot spots and other surface features create more/more coherent noise than previous works estimated. Difference btwn linear and non-linear pulsation periods may also play a small role. (Img: Kervella et al. 2018) https://t.co/zje8bODWRg', ""Speaking of variability, we also dug up some old space-based photometry from the SMEI cameras, did our best to correct it and scaled to the public V data. Red is the new data. I think it's pretty rad! https://t.co/xPbNpSjN0C"", 'Finally, we experimented with some implicit hydro calculations. While we run into the same non-linearity issues in super-bright models as others did, we were able to confirm that the star pulsates in the fundamental radial mode. /fin https://t.co/gFu0R6dg0n']",https://arxiv.org/abs/2006.09837,"We conduct a rigorous examination of the nearby red supergiant Betelgeuse by drawing on the synthesis of new observational data and three different modeling techniques. Our observational results include the release of new, processed photometric measurements collected with the space-based SMEI instrument prior to Betelgeuse's recent, unprecedented dimming event. We detect the first radial overtone in the photometric data and report a period of $185\pm13.5$ d. Our theoretical predictions include self-consistent results from multi-timescale evolutionary, oscillatory, and hydrodynamic simulations conducted with the Modules for Experiments in Stellar Astrophysics (MESA) software suite. Significant outcomes of our modeling efforts include a precise prediction for the star's radius: $764^{+116}_{-62} R_{\odot}$. In concert with additional constraints, this allows us to derive a new, independent distance estimate of $168^ {+27}_{-15}$ pc and a parallax of $\pi=5.95^{+0.58}_{-0.85}$ mas, in good agreement with Hipparcos but less so with recent radio measurements. Seismic results from both perturbed hydrostatic and evolving hydrodynamic simulations constrain the period and driving mechanisms of Betelgeuse's dominant periodicities in new ways. Our analyses converge to the conclusion that Betelgeuse's $\approx 400$ day period is the result of pulsation in the fundamental mode, driven by the $\kappa$-mechanism. Grid-based hydrodynamic modeling reveals that the behavior of the oscillating envelope is mass-dependent, and likewise suggests that the non-linear pulsation excitation time could serve as a mass constraint. Our results place $\alpha$ Ori definitively in the core helium-burning phase near the base of the red supergiant branch. We report a present-day mass of $16.5$--$19 ~M_{\odot}$---slightly lower than typical literature values. ","Standing on the shoulders of giants: New mass and distance estimates for
  Betelgeuse through combined evolutionary, asteroseismic, and hydrodynamical
  simulations with MESA"
97,1273538845619957761,1068823110,José Cano,"['The preprint of our ASAP 2020 paper ""Optimizing Grouped Convolutions on Edge Devices"" is already available!\n\nWe propose GSPC, a new and higher performance implementation of Grouped Convolutions. Check it out! <LINK>', ""@MJcomp86 Thanks Mahdi! Not sure about the multiple models in TVM... we haven't explored that yet, but it's interesting!""]",https://arxiv.org/abs/2006.09791,"When deploying a deep neural network on constrained hardware, it is possible to replace the network's standard convolutions with grouped convolutions. This allows for substantial memory savings with minimal loss of accuracy. However, current implementations of grouped convolutions in modern deep learning frameworks are far from performing optimally in terms of speed. In this paper we propose Grouped Spatial Pack Convolutions (GSPC), a new implementation of grouped convolutions that outperforms existing solutions. We implement GSPC in TVM, which provides state-of-the-art performance on edge devices. We analyze a set of networks utilizing different types of grouped convolutions and evaluate their performance in terms of inference time on several edge devices. We observe that our new implementation scales well with the number of groups and provides the best inference times in all settings, improving the existing implementations of grouped convolutions in TVM, PyTorch and TensorFlow Lite by 3.4x, 8x and 4x on average respectively. Code is available at this https URL ",Optimizing Grouped Convolutions on Edge Devices
98,1273519079593312257,481539448,Richard Alexander,"['New paper, led by @PhysicsUoL PhD student (and soon-to-be @QMPlanets postdoc) @GBallabio. Giulia created a new method to understand how emission lines can measure the properties of disc winds, so we can figure out how planet-forming discs lose mass.\n<LINK>', ""This is an old subject, but the observations are complex, and it's hard to interpret them without a theoretical framework. Giulia's new models provide a clear new insight into what can - and can't - be determined from these observations."", 'One key result is that the observed [NeII] 12.81um lines, which trace photoionized gas, are consistent with an isothermal wind with a sound speed of ~10km/s. https://t.co/E1X3zO1FJ9', ""However, the [OI] 6300Å lines, which trace neutral gas, are trickier - we can't match both the blue-shifts and widths with a single model. So either the wind is highly non-isothermal, or the different lines are actually probing different parts of a multi-component disc wind.""]",https://arxiv.org/abs/2006.09811,"Photoevaporation driven by high energy radiation from the central star plays an important role in the evolution of protoplanetary discs. Photoevaporative winds have been unambiguously detected through blue-shifted emission lines, but their detailed properties remain uncertain. Here we present a new empirical approach to make observational predictions of these thermal winds, seeking to fill the gap between theory and observations. We use a self-similar model of an isothermal wind to compute line profiles of several characteristic emission lines (the [Ne${\rm{\scriptsize II}}$] line at 12.81 $\mu$m, and optical forbidden lines such as [O${\rm{\scriptsize I}}$] 6300 $\mathring{A}$ and [S${\rm{\scriptsize II}}$] 4068/4076 $\mathring{A}$), studying how the lines are affected by parameters such as the gas temperature, disc inclinations, and density profile. Our model successfully reproduces blue-shifted lines with $v_{\rm peak} \lesssim 10$ km/s, which decrease with increasing disc inclination. The line widths increase with increasing disc inclinations and range from $\Delta v \sim 15-30$ km/s. The predicted blue-shifts are mostly sensitive to the gas sound speed. The observed [Ne${\rm{\scriptsize II}}$] line profiles are consistent with a thermal wind and point towards a relatively high sound speed, as expected for EUV photoevaporation. However, the observed [O${\rm{\scriptsize I}}$] line profiles require lower temperatures, as expected in X-ray photoevaporation, and show a wider scatter that is difficult to reconcile with a single wind model; it seems likely that these lines trace different components of a multi-phase wind. We also note that the spectral resolution of current observations remains an important limiting factor in these studies, and that higher resolution spectra are required if emission lines are to further our understanding of protoplanetary disc winds. ",Forbidden line diagnostics of photoevaporative disc winds
99,1273502336409886721,17819190,Vaishak Belle,['New working paper -- Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains <LINK>'],https://arxiv.org/abs/2006.08480,"The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence (AI). The deduction camp concerns itself with questions about the expressiveness of formal languages for capturing knowledge about the world, together with proof systems for reasoning from such knowledge bases. The learning camp attempts to generalize from examples about partial descriptions about the world. In AI, historically, these camps have loosely divided the development of the field, but advances in cross-over areas such as statistical relational learning, neuro-symbolic systems, and high-level control have illustrated that the dichotomy is not very constructive, and perhaps even ill-formed. In this article, we survey work that provides further evidence for the connections between logic and learning. Our narrative is structured in terms of three strands: logic versus learning, machine learning for logic, and logic for machine learning, but naturally, there is considerable overlap. We place an emphasis on the following ""sore"" point: there is a common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties. We report on results that challenge this view on the limitations of logic, and expose the role that logic can play for learning in infinite domains. ","Symbolic Logic meets Machine Learning: A Brief Survey in Infinite
  Domains"
100,1273501883395538944,62044012,Michael Bronstein,['<LINK> In a new paper we show structure-aware message passing GNN strictly more powerful that 1-WL and at least not weaker than 2-WL. To my knowledge this is first analysis of expressivity of GNNs outside Weisfeiler-Lehman hierarchy. Raises many deep questions'],https://arxiv.org/abs/2006.09252,"While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often intimately related to downstream tasks. To this end, we propose ""Graph Substructure Networks"" (GSN), a topologically-aware message passing scheme based on substructure encoding. We theoretically analyse the expressive power of our architecture, showing that it is strictly more expressive than the WL test, and provide sufficient conditions for universality. Importantly, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We perform an extensive experimental evaluation on graph classification and regression tasks and obtain state-of-the-art results in diverse real-world settings including molecular graphs and social networks. The code is publicly available at this https URL ","Improving Graph Neural Network Expressivity via Subgraph Isomorphism
  Counting"
101,1273493516056965122,961972787077373952,Aykut Erdem,"['Check out the new version of our paper, which is out now on arXiv: <LINK>\n\nWe have further improved our results and performed a more comprehensive comparison with prior work. Today we are also releasing our code and pretrained models: <LINK> <LINK>']",https://arxiv.org/abs/2006.09845,"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images. ",Burst Photography for Learning to Enhance Extremely Dark Images
102,1273460843548565504,96779364,Arnab Bhattacharyya,"['New paper: ""Efficient Statistics for Sparse Graphical Models from Truncated Samples"" with Rathin Desai, @SaiGaneshNagar1, and Yannis Panageas. <LINK>', 'A common issue in statistical inference is *selection bias*. This occurs when a dataset contains missing samples, and whether a sample is missing or not depends preferentially on its value. E.g., poor people may not want to report their income in a poll.', ""In general, inference is impossible when there's selection bias. E.g., only millionaires may report their earnings skewing our mean income estimate. But what if we make parametric assumptions? Like, suppose the underlying distribution is gaussian."", 'We look at truncated samples: a sample x is seen only when it satisfies some fixed property P and not seen otherwise. The property P can be arbitrary, but we assume that we can efficiently check whether an item satisfies P.', 'The first result in our paper gives improved sample complexity bounds for learning a Gaussian graphical model from truncated samples. The # of samples scales with the sparsity of the precision matrix (unlike the previous work, https://t.co/mwvngTJjeY).', 'The second result in our paper looks at high-dimensional sparse linear regression with truncated samples. Here, we make stronger assumptions to show that you only need O(k^2 log(d)) samples to recover the parameters upto bounded *entrywise* error (k=sparsity, d=dimension).', 'For untruncated samples, the typical approach for both problems is to do l1-regularization, i.e. Lasso. We do the same. The main technical challenge is to show that the Lasso optimum falls in a region that is locally strongly convex.', ""The big open question here is to make our results computationally effective. For gradient descent-type algorithms, it's not clear how to choose the initial point inside the strongly convex region."", 'Another interesting direction is to remove the assumption that we can check the property P. In the low-dimensional setting, there has been exciting work along this line here: https://t.co/3XsEQhSA9m.', 'Of late, my work has been following a pattern :) The general problem is impossible, @yudapearl, @eliasbareinboim and company (https://t.co/puAVNWltqX) give conditions for statistical identifiability, and we show stronger conditions that imply finite sample guarantees.']",https://arxiv.org/abs/2006.09735,"In this paper, we study high-dimensional estimation from truncated samples. We focus on two fundamental and classical problems: (i) inference of sparse Gaussian graphical models and (ii) support recovery of sparse linear models. (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\bf x}$ are generated from a Gaussian $N(\mu,\Sigma)$ and observed only if they belong to a subset $S \subseteq \mathbb{R}^d$. We show that ${\mu}$ and ${\Sigma}$ can be estimated with error $\epsilon$ in the Frobenius norm, using $\tilde{O}\left(\frac{\textrm{nz}({\Sigma}^{-1})}{\epsilon^2}\right)$ samples from a truncated $\mathcal{N}({\mu},{\Sigma})$ and having access to a membership oracle for $S$. The set $S$ is assumed to have non-trivial measure under the unknown distribution but is otherwise arbitrary. (ii) For sparse linear regression, suppose samples $({\bf x},y)$ are generated where $y = {\bf x}^\top{{\Omega}^*} + \mathcal{N}(0,1)$ and $({\bf x}, y)$ is seen only if $y$ belongs to a truncation set $S \subseteq \mathbb{R}$. We consider the case that ${\Omega}^*$ is sparse with a support set of size $k$. Our main result is to establish precise conditions on the problem dimension $d$, the support size $k$, the number of observations $n$, and properties of the samples and the truncation that are sufficient to recover the support of ${\Omega}^*$. Specifically, we show that under some mild assumptions, only $O(k^2 \log d)$ samples are needed to estimate ${\Omega}^*$ in the $\ell_\infty$-norm up to a bounded error. For both problems, our estimator minimizes the sum of the finite population negative log-likelihood function and an $\ell_1$-regularization term. ",Efficient Statistics for Sparse Graphical Models from Truncated Samples
103,1273454271791595520,872188363,Sandro Tacchella,"['Today on @arxiv, new paper by @redshiftless, @NCaplar and me: star formation variability due to gas inflow &amp; GMCs <LINK>']",https://arxiv.org/abs/2006.09382v1,"A key uncertainty in galaxy evolution is the physics regulating star formation, ranging from small-scale processes related to the life-cycle of molecular clouds within galaxies to large-scale processes such as gas accretion onto galaxies. We study the imprint of such processes on the time-variability of star formation with an analytical approach tracking the gas mass of galaxies (""regulator model""). Specifically, we quantify the strength of the fluctuation in the star-formation rate (SFR) on different timescales, i.e. the power spectral density (PSD) of the star-formation history, and connect it to gas inflow and the life-cycle of molecular clouds. We show that in the general case the PSD of the SFR has three breaks, corresponding to the correlation time of the inflow rate, the equilibrium timescale of the gas reservoir of the galaxy, and the average lifetime of individual molecular clouds. On long and intermediate timescales (relative to the dynamical timescale of the galaxy), the PSD is typically set by the variability of the inflow rate and the interplay between outflows and gas depletion. On short timescales, the PSD shows an additional component related to the life-cycle of molecular clouds, which can be described by a damped random walk with a power-law slope of $\beta\approx2$ at high frequencies with a break near the average cloud lifetime. We discuss star-formation ""burstiness"" in a wide range of galaxy regimes, study the evolution of galaxies about the main sequence ridgeline, and explore the applicability of our method for understanding the star-formation process on cloud-scale from galaxy-integrated measurements. ","] Stochastic modelling of star-formation histories II: star-formation
  variability from molecular clouds and gas inflow"
104,1273441031082504194,313190305,Jared Roesch,['Check out our new paper on dynamic check pointing for @PyTorch (<LINK>) credit to @MarisaVeryMoe and others who aren’t on twitter who did all the important bits.'],https://arxiv.org/abs/2006.09616,"Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an $N$-layer linear feedforward network on an $\Omega(\sqrt{N})$ memory budget with only $\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors. ",Dynamic Tensor Rematerialization
105,1273418141243228161,20703003,Peter B Denton,"[""New neutrino paper, this time with Rebekah Pestes a @VTPhysics grad student visiting BNL on the SCGSR program.\n\nWe discuss what deltaCP for neutrino oscillations really means and highlight the impact of how it's not actually a fundamental parameter. <LINK> 1/7"", 'If we do the rotations in the PMNS matrix in a different order, what changes? This figure shows that even if deltaPDG in the usual parameterization is constrained (that is, we ignore the T2K app data), delta in other parameterizations is constrained to within 150-210 degrees! 2/7 https://t.co/l4fHwPPiKl', '(The shaded regions show the effect of varying the osc params within their 3 σ regions.)\n\nWhile the premise might have been expected by some neutrino theorists, how tightly delta in other parameterizations is constrained was surprising to me anyway. 3/7', ""The reason is because |Ue3| is small (that is, theta13 is small). In other parameterizations this looks like sqrt(A+B*cos(delta)) so cos(delta) has to be near -1 for the cancellation to work. It's a bit subtle but we have useful approximations in the paper. 4/7"", ""The above figure also implies that precision in delta doesn't mean much of anything fundamental. We show that here where we plot what a precision on deltaPDG of 15deg looks like in different parameterizations; it could be as small as 1deg! 5/7 https://t.co/AXB0g1CXZX"", ""So what's the right answer? Within the context of oscillations it's always the Jarlskog (pictured here). This quantity is the closest to what is measured and doesn't require as much input from solar parameters to extract unlike using delta. 6/7 https://t.co/YzrZYQ1MFo"", ""While some aspect of these concepts has been discussed in the past, we focus on the situation in light of current measurements. As we zero in on the remaining parameters, keeping straight what is/isn't fundamental is more important than ever! 7/7"", '*this should say ""in the usual parameterization is UNconstrained""']",https://arxiv.org/abs/2006.09384,"CP violation in the lepton mass matrix will be probed with good precision in upcoming experiments. The amount of CP violation present in oscillations can be quantified in numerous ways and is typically parameterized by the complex phase $\delta_{\rm PDG}$ in the standard PDG definition of the lepton mixing matrix. There are additional parameterizations of the lepton mixing matrix as well. Through various examples, we explore how, given the current data, different parameterizations can lead to different conclusions when working with parameterization dependent variables, such as $\delta$. We demonstrate how the smallness of $|U_{e3}|$ governs the scale of these results. We then demonstrate how $\delta$ can be misleading and argue that the Jarlskog is the cleanest means of presenting the amount of CP violation in the lepton sector. We also confirm that, among the different parameterizations considered, the standard PDG parameterization has a number of convenient features. ","The Impact of Different Parameterizations on the Interpretation of CP
  Violation in Neutrino Oscillations"
106,1273398042947805184,20877181,Josh Susskind,"['Check out our Apple research paper introducing a new framework for efficient neural architecture design, led by Etai Littwin called Collegial Ensembles:\n<LINK>.\n\nColleagues are @BenjaminMyara, Sima Sabah, @zhaisf, Oren Golan, and myself. <LINK>', 'The paper makes theoretical contributions backed up by empirical results and has a practical primal/dual optimization form allowing user to make a tradeoff between minimizing capacity (pruning) or maximizing trainability (optimization efficiency).', 'We show empirically that CE can be used to analytically derive optimal group convolution modules without having to train a single model, with strong results on CIFAR and ImageNet classification tasks compared to ResNext baselines.']",https://arxiv.org/abs/2006.07678,"Modern neural network performance typically improves as model size increases. A recent line of research on the Neural Tangent Kernel (NTK) of over-parameterized networks indicates that the improvement with size increase is a product of a better conditioned loss landscape. In this work, we investigate a form of over-parameterization achieved through ensembling, where we define collegial ensembles (CE) as the aggregation of multiple independent models with identical architectures, trained as a single model. We show that the optimization dynamics of CE simplify dramatically when the number of models in the ensemble is large, resembling the dynamics of wide models, yet scale much more favorably. We use recent theoretical results on the finite width corrections of the NTK to perform efficient architecture search in a space of finite width CE that aims to either minimize capacity, or maximize trainability under a set of constraints. The resulting ensembles can be efficiently implemented in practical architectures using group convolutions and block diagonal layers. Finally, we show how our framework can be used to analytically derive optimal group convolution modules originally found using expensive grid searches, without having to train a single model. ",Collegial Ensembles
107,1273346069372567564,270544249,Abdul Saleh,"['Chatbots often say pretty weird things😅 Ever wondered if there was a way to probe them for specific conversational skills? Check out our new paper! Joint work with @boknilev and @pmphlt!\n\nPaper: <LINK> \nCode: <LINK> <LINK>', 'also joint work with @_tovly and @stephenLcasper!']",https://arxiv.org/abs/2006.08331,"The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog. ",Probing Neural Dialog Models for Conversational Understanding
108,1273307187163607040,4639078397,John Wise,"[""New paper day! It's on hyper-accretion onto seed black holes, led by KwangHo Park. A big question is how long hyper-accretion can be sustained in the early universe. We've found that biconical dominated accretion flows (BDAF) provide an efficient way. <LINK> <LINK>"", ""The gas can efficiently accrete from the polar regions because radiation is trapped at smaller radii and doesn't inhibit its infall. Also it isn't halted by the angular momentum barrier. In the end, the BH can accrete most of the gas within the Bondi radius.""]",https://arxiv.org/abs/2006.06781,"Hyperaccretion occurs when the gas inflow rate onto a black hole (BH) is so high that the radiative feedback cannot reverse the accretion flow. This extreme process is a promising mechanism for the rapid growth of seed BHs in the early universe, which can explain high-redshift quasars powered by billion solar mass BHs. In theoretical models, spherical symmetry is commonly adopted for hyperaccretion flows; however, the sustainability of such structures on timescales corresponding to the BH growth has not been addressed yet. Here we show that stochastic interactions between the ionizing radiation from the BH and nonuniform accretion flow can lead to the formation of a rotating gas disk around the BH. Once the disk forms, the supply of gas to the BH preferentially occurs via biconical-dominated accretion flow perpendicular to the disk, avoiding the centrifugal barrier of the disk. Biconical-dominated accretion flows from opposite directions collide in the vicinity of the BH supplying high-density, low angular momentum gas to the BH, whereas most of the gas with nonnegligible angular momentum is deflected to the rotationally supported outflowing decretion disk. The disk becomes reinforced progressively as more mass from the biconical flow transfers to the disk and some of the outflowing gas from the disk is redirected to the biconical accretion funnels through a meridional structure. This axisymmetric hydrodynamic structure of a biconical-dominated accretion flow and decretion disk continues to provide uninterrupted flow of high-density gas to the BH. ","Biconical-dominated Accretion Flow onto Seed Black Holes in a
  Hyper-accretion Regime"
109,1273283099779047424,782634053358919681,Alex Evans,"['Between @UniswapProtocol and @BalancerLabs, @ethereum is now home to nearly $100M worth of LP shares\n\nWe now have LPs with uneven weights, multiple assets, and even weights that change over time.\n\nWhat can they offer liquidity providers? \n\nNew paper: <LINK>', '1/ 1-tweet TLDR:\n\nIf Uniswap/Balancer fixed-weight LPs had no fees, they’d lose value over time. The expected values of these losses depend on weights, vols and correlations of assets in the pool.\n\nWith dynamic weights we can replicate derivatives (eg options) using Balancer LPs', '2/ The first result is due to arb, not necessarily imperm loss.\n\nIn general, low vol, high positive correlation between assets -&gt; fewer expected arb opportunities. \n\nThis generalizes a result first noted by @GuilleAngeris  @tarunchitra @_charlienoyes and others for Uniswap LPs.', '3/ A version of the result also applies to no-fee pools whose weights vary deterministically with time, such as\n\nLBPs: https://t.co/d0BpNqIsQw\n\nRollover Pools: https://t.co/IvtpDTQnwW', '4/ On a more political note, we might be focusing too much on imperm loss. Constant-mix portfolios also have imperm loss (aka negative gamma), but many still find them attractive. Why?', '5/ Because they are known to profit from volatility in flat, oscillating markets.\n\nG3Ms give up these volatility gains to arbitrageurs because they lag the market during rebalancing.\n\nAs a result, they need fees to offset relative losses and attract liquidity providers.', '6/ In exchange for giving up some vol gains, LPs get \n\n1. Automatic, continual rebalancing to target weights (like ETFs) \n\n2. Information from traders on the price (oracle functionality)\n\n""Eliminate imperm loss"" -&gt; lose these properties', '7/ A useful heuristic (h/t @tarunchitra)\n\nThink of the relative losses of no-fee LPs as a convenience fee you pay arbers to rebalance your portfolio for you. \n\nThe price of being passive vs actively trading your position. \n\nFees may offset all or part of the loss, at a price.', '8/ What about pools with dynamic weights?\n\nFor example, this pool from PieDAO adjusts weights as a function of volatility https://t.co/9dsBcV3nD1\n\nHere things get interesting!', '9/  We can use Balancer LPs to replicate derivatives like protective puts and covered calls. \n\nHow? It’s surprisingly simple.\n\nAdjust the weight of the underlying asset to always equal Δ*Price/Value aka the elasticity or ‘omega’ of the derivative.', '10/ While the set of financial contracts one can replicate with LP shares is constrained by leverage (elasticity), we can likely use an external lending market like @AaveAave to replicate levered products like naked options.', 'P.S. Many thanks to @tarunchitra @GuilleAngeris  @ObadiaAlex @hal2001 Akis Kattis for their feedback and edits to the paper', 'the conjecture that this may be possible originally h/t @_charlienoyes']",https://arxiv.org/abs/2006.08806,"Geometric mean market makers (G3Ms), such as Uniswap and Balancer, comprise a popular class of automated market makers (AMMs) defined by the following rule: the reserves of the AMM before and after each trade must have the same (weighted) geometric mean. This paper extends several results known for constant-weight G3Ms to the general case of G3Ms with time-varying and potentially stochastic weights. These results include the returns and no-arbitrage prices of liquidity pool (LP) shares that investors receive for supplying liquidity to G3Ms. Using these expressions, we show how to create G3Ms whose LP shares replicate the payoffs of financial derivatives. The resulting hedges are model-independent and exact for derivative contracts whose payoff functions satisfy an elasticity constraint. These strategies allow LP shares to replicate various trading strategies and financial contracts, including standard options. G3Ms are thus shown to be capable of recreating a variety of active trading strategies through passive positions in LP shares. ",Liquidity Provider Returns in Geometric Mean Markets
110,1273269846944501770,423671718,Joshua Loftus #peace,"['New #algorithmicfairness #fairAI paper with Ke Yang and @stoyanoj \n\nWe propose causal models with multiple sensitive attributes as a formal approach to intersectionality, and apply the framework to fair ranking tasks\n\nPreprint: <LINK> <LINK>', 'These models may allow us to carefully disentangle the ""bundles of sticks,"" to use @maya_sen and @owasow\'s metaphor for sensitive attributes (https://t.co/RB05f0lBT0)\n\nOur ranking idea: treat everyone as though they belong to the same intersectional subgroup (equal treatment?) https://t.co/WMbSxVxvDG', 'We illustrate these ideas on several real and synthetic examples. In the one below, job applicants to a fictional moving company are required to pass a weight-lifting test. There is both direct and indirect discrimination on the basis of both race and gender.\n\n(end of thread) https://t.co/KTgi8rzLol', '(Addendum) In above figure the selection rate is % of subgroup ranked in the top 50, 100, or 200, i.e. would be hired.\n\nBy removing direct discrimination but keeping the weight-lifting requirement, our method results in many more black men being hired in this example', ""@lastpositivist @stoyanoj Thanks! Sorry we missed it earlier, not sure how I forgot because I definitely have it open in a tab somewhere... we'll be sure to cite you in the next update""]",https://arxiv.org/abs/2006.08688,"In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search results to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in fair machine learning explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behaviour under different structural assumptions. ",Causal intersectionality for fair ranking
111,1273242222385868800,757009671366606848,Luigi Acerbi,"['(1/n) New preprint and update!\nVariational Bayesian Monte Carlo (VBMC) to perform Bayesian posterior and model inference also with *noisy* likelihood evaluations, such as those obtained via simulation (aka sampling).\nPaper: <LINK>\nCode: <LINK> <LINK>', '(2/n) As a motivating example, see 👇 a comparison between the ""old"" version of VBMC and the ""new"" VBMC when facing *noisy* evaluations of the log-likelihood. Bayesian inference with noisy likelihoods is hard, but now we can do it! (how? details in the paper!) https://t.co/tn4iiVnWcV', '(3/n) OK, but how do you get log-likelihoods via simulation? Here we mostly use Inverse Binomial Sampling (IBS). IBS yields unbiased and normally distributed estimates of the log-like (with known variance!). \n(See tweeprint: https://t.co/dhUiW2zhsO w/ @basvanopheusden @weijima01)', '(4/n) Mmh, does VBMC work on real noisy problems out-of-the-box? Yes!\nWe tested VBMC with many models and real data from comp/cog-neuro, up to D = 9 parameters. New versions of VBMC (esp. VBMC-VIQR) vastly outperform prev methods (including older versions of VBMC). 👇 https://t.co/UONxWgSGjv', '(5/n) VBMC returns not only the posterior distribution over model parameters, but also an estimate (expected lower bound, or ELBO) on the log marginal likelihood, which can be used for Bayesian model selection (better than AIC/BIC). https://t.co/EXV2LAy1aC', '(6/n) But does it work only for Very Expensive models? \nNot really — VBMC is slowed a bit by noise, but still pretty fast thanks to the combo of variational inference + Bayesian quadrature. If your model takes more than a fraction of a second to evaluate, give VBMC a go. https://t.co/Fti1rnFkar', '(7/n) Also, VBMC-VIQR is quite robust even to fairly large amounts of noise in the log-likelihood estimate.\nPerformance degrades gracefully with extra noise, while other methods are severely affected by even small amounts of noise. https://t.co/bOgE1QyLEO', '(8/n) So... Time to update to (or download) the new version!\nThe updated VBMC toolbox (now v1.0) is available here: https://t.co/WZLmmPx1Qn\nDocs &amp; FAQs: https://t.co/zRNy417cUO\nCheck out the new example in the tutorial 👇 https://t.co/yPZeT1qrDU', '(9/n) Now acks...\nThe main technique to handle noisy observations (variational interquantile range, VIQR) builds upon fantastic recent work by Marko Järvenpää et al. (2020) which you should check out! https://t.co/9N9Smf3BKT\n(of the authors, I think only @avehtari is on Twitter?)', '(10/n) Also, VBMC would not exist without prior work on GPs + Bayesian quadrature by @DavidDuvenaud, @maosbot and @PhilippHennig5 among others. Of the ""older"" papers, you should read about WSABI (NeurIPS 2014): https://t.co/5RVMzJHqOU', '(11/n) And of recent work, check out this brilliant paper on Active Multi-Information Source Bayesian Quadrature by @alpiges, @javiergonzh and @MMahsereci (UAI 2019): https://t.co/hwCwijkZIo', ""(12/n) Finally, there are other approaches to infer posteriors from simulations, such as the amazing work by @jakhmack 's group (w/ @ppjgoncalves &amp; many others) using deep neural density estimators (Sequential Neural Posterior Estimation - SNPE): https://t.co/JET2iIc9Zr"", '(13/n) By the way, if you are already using BADS (https://t.co/foEfPoQw50) for model fitting via optimization, starting to use VBMC should be fairly simple, as described here: https://t.co/rrVAZ2ojMJ', '(14/fin) In conclusion, no more excuses not to be Bayesian!\n\nVBMC toolbox: https://t.co/WZLmmPx1Qn\n(and please get in touch with any comment/question/problem, happy to discuss!) https://t.co/IJg81BHSQR', ""@gabrielmstine Great to hear, good to know that it's being useful!\nThat's exactly why I started working on model-fitting methods, and then it took a life of its own. :)""]",https://arxiv.org/abs/2006.08655,"Variational Bayesian Monte Carlo (VBMC) is a recently introduced framework that uses Gaussian process surrogates to perform approximate Bayesian inference in models with black-box, non-cheap likelihoods. In this work, we extend VBMC to deal with noisy log-likelihood evaluations, such as those arising from simulation-based models. We introduce new `global' acquisition functions, such as expected information gain (EIG) and variational interquantile range (VIQR), which are robust to noise and can be efficiently evaluated within the VBMC setting. In a novel, challenging, noisy-inference benchmark comprising of a variety of models with real datasets from computational and cognitive neuroscience, VBMC+VIQR achieves state-of-the-art performance in recovering the ground-truth posteriors and model evidence. In particular, our method vastly outperforms `local' acquisition functions and other surrogate-based inference methods while keeping a small algorithmic cost. Our benchmark corroborates VBMC as a general-purpose technique for sample-efficient black-box Bayesian inference also with noisy models. ",Variational Bayesian Monte Carlo with Noisy Likelihoods
112,1273229571475812352,1080138653807058945,Bertrand Charpentier,"['Assuming the knowledge of anomalous data at train time is unrealistic. In our new PostNet paper (<LINK>), we use Dirichlet distributions and NF to produce uncertainty-aware predictions without training on OOD data.\n\nWith @DanielZuegner and @guennemann <LINK>']",https://arxiv.org/abs/2006.09239,"Accurate estimation of aleatoric and epistemic uncertainty is crucial to build safe and reliable systems. Traditional approaches, such as dropout and ensemble methods, estimate uncertainty by sampling probability predictions from different submodels, which leads to slow uncertainty estimation at inference time. Recent works address this drawback by directly predicting parameters of prior distributions over the probability predictions with a neural network. While this approach has demonstrated accurate uncertainty estimation, it requires defining arbitrary target parameters for in-distribution data and makes the unrealistic assumption that out-of-distribution (OOD) data is known at training time. In this work we propose the Posterior Network (PostNet), which uses Normalizing Flows to predict an individual closed-form posterior distribution over predicted probabilites for any input sample. The posterior distributions learned by PostNet accurately reflect uncertainty for in- and out-of-distribution data -- without requiring access to OOD data at training time. PostNet achieves state-of-the art results in OOD detection and in uncertainty calibration under dataset shifts. ","Posterior Network: Uncertainty Estimation without OOD Samples via
  Density-Based Pseudo-Counts"
113,1273128407392571394,2566842619,Michal Šustr,"['Search has played a fundamental role in computer game research since the very beginning. But what is sound search in imperfect information games? \n\nOur new theoretical paper <LINK> ! <LINK>', 'Unlike in perfect info games, players may need to randomize their strategies in order to play optimally. Think of Poker - if you always bet with strong cards or fold with weak ones, your opponent will figure this out during repeated play and learn to exploit you. https://t.co/VnpBNhZmg7', 'In two-player zero-sum games we typically look for strong strategies with guarantees against worst-case adversaries: Nash equilibria (or rather their epsilon-approximation).', 'In ""Sound Search in Imperfect Information Games"" we argue that the fixed-strategy definitions of exploitability and epsilon-Nash equilibria are however ill suited to measure the worst-case performance of an **online** (search) algorithm.', 'In fact, the issues with evaluating the worst-case performance are quite subtle and they are very easy to overlook! This has led to a number of discussions with co-authors :)', 'One of the basic problems is that the play strategy may be only locally consistent: it corresponds to some epsilon-equilibrium for a state, but ""stitching"" different strategies together across all states may not yield epsilon-equilibrium anymore!', 'We thus formalize epsilon-soundness, a concept that connects the worst-case performance of an online algorithm to the performance of an (offline) epsilon-Nash equilibrium.', 'We introduce also a consistency framework -- a hierarchy that connects the behavior of an online algorithm to a Nash equilibrium. Our multiple levels of consistency describe in what sense an online algorithm plays ""just like a fixed Nash equilibrium\'\'.', 'Our definition of soundness and the consistency hierarchy finally provide appropriate tools to analyze online algorithms in imp. info. games. We inspect some of the previous online algorithms in a new light, bringing new insights into their worst case performance guarantees.', ""Finally, I'd like to express my gratitude to @Lifrordi, M. Moravcik, N. Burch, @sharky6000 and @MichaelHBowling for the collaboration! Thank you all for your input, comments and discussions, it is very much appreciated.""]",https://arxiv.org/abs/2006.08740,"Search has played a fundamental role in computer game research since the very beginning. And while online search has been commonly used in perfect information games such as Chess and Go, online search methods for imperfect information games have only been introduced relatively recently. This paper addresses the question of what is a sound online algorithm in an imperfect information setting of two-player zero-sum games. We argue that the~fixed-strategy~definitions of exploitability and $\epsilon$-Nash equilibria are ill-suited to measure an online algorithm's worst-case performance. We thus formalize $\epsilon$-soundness, a concept that connects the worst-case performance of an online algorithm to the performance of an $\epsilon$-Nash equilibrium. As $\epsilon$-soundness can be difficult to compute in general, we introduce a consistency framework -- a hierarchy that connects an online algorithm's behavior to a Nash equilibrium. These multiple levels of consistency describe in what sense an online algorithm plays ""just like a fixed Nash equilibrium"". These notions further illustrate the difference between perfect and imperfect information settings, as the same consistency guarantees have different worst-case online performance in perfect and imperfect information games. The definitions of soundness and the consistency hierarchy finally provide appropriate tools to analyze online algorithms in repeated imperfect information games. We thus inspect some of the previous online algorithms in a new light, bringing new insights into their worst-case performance guarantees. ",Sound Algorithms in Imperfect Information Games
114,1273123833780875266,237390501,Paul Hancock,['New paper on the arxiv from the MWA space situational awareness team. Led by Steve Prabu: <LINK>'],https://arxiv.org/abs/2006.04327,"We have extended our previous work to use the Murchison Widefield Array (MWA) as a non-coherent passive radar system in the FM frequency band, using terrestrial FM transmitters to illuminate objects in Low Earth Orbit LEO) and the MWA as the sensitive receiving element for the radar return. We have implemented a blind detection algorithm that searches for these reflected signals in difference images constructed using standard interferometric imaging techniques. From 20 hours of archived MWA observations, we conduct a survey of LEO, detecting 74 unique objects over multiple passes and demonstrating the MWA to be a valuable addition to the global Space Domain Awareness network. We detected objects with ranges up to 977 km and as small as 0.03 m^2 radar cross section. We found that 30 objects were either non-operational satellites or upper-stage rocket body debris. Additionally, we also detected FM reflections from Geminid meteors and aircraft flying over the MWA. Most of the detections of objects in LEO were found to lie within the parameter space predicted by previous feasibility studies, verifying the performance of the MWA for this application. ","A Low Frequency Blind Survey of the Low Earth Orbit Environment using
  Non-Coherent Passive Radar with the Murchison Widefield Array"
115,1273096493185064961,293552287,Hoan Tran,"['New paper out ""Higher-Order Quantum Reservoir Computing""\n<LINK> <LINK>']",https://arxiv.org/abs/2006.08999,"Quantum reservoir computing (QRC) is an emerging paradigm for harnessing the natural dynamics of quantum systems as computational resources that can be used for temporal machine learning tasks. In the current setup, QRC is difficult to deal with high-dimensional data and has a major drawback of scalability in physical implementations. We propose higher-order QRC, a hybrid quantum-classical framework consisting of multiple but small quantum systems that are mutually communicated via classical connections like linear feedback. By utilizing the advantages of both classical and quantum techniques, our framework enables an efficient implementation to boost the scalability and performance of QRC. Furthermore, higher-order settings allow us to implement a FORCE learning or an innate training scheme, which provides flexibility and high operability to harness high-dimensional quantum dynamics and significantly extends the application domain of QRC. We demonstrate the effectiveness of our framework in emulating large-scale nonlinear dynamical systems, including complex spatiotemporal chaos, which outperforms many of the existing machine learning techniques in certain situations. ",Higher-Order Quantum Reservoir Computing
116,1273084289941295104,1102421602606575616,Jaeho Lee,"['Q. What is the *minimum width* of neural network, required for universal approximation?\n\nA: max{dx + 1, dy}.\n... at least for ReLU nets / Lp setup, as we show in our new paper: <LINK>\n\namazing effort by @sejun_park_ w/ Chulhee Yun and @jinwoos0417 [1/n]', 'This ""dual"" scenario (of classical univ. approx. results) have already been studied in many papers, starting from the elegant work by Hanin &amp; Sellke.\n\nWhat we give is (perhaps) the first TIGHT upper and lower bound(s) [2/n]: https://t.co/98EbvKNN5V', 'Quick summary of results:\n1. max{dx + 1, dy} is what we need for ReLU/Lp setup\n2. The same is provably NOT TRUE for ReLU/Cont\n3. ... but if we add Step activation, again the answer is max{dx + 1, dy}\n\n[3/n]', 'Intriguingly, this width-separation of ""Lp can"" vs ""Cont can\'t"" (1&amp;2) is in stark contrast with depth-separation in classical scenarios!\n\nActually, depth-2 is known to be sufficient for Cont, but not for Lp (see, e.g. Qu and Wang 2019).\n\n[4/n]', 'Q. how do we construct the universal approximator with width max{dx+1,dy}?\n\nA. We use a encoder-decoder structure, mainly inspired by memorization/info-theory literatures. This allows us to decouple dx and dy, to improve over the previously known UB of dx+dy+1.\n\n[5/n] https://t.co/hRrUpjjpRM', '... other details can be found in the arXiv version:\nhttps://t.co/EJ3bBMVwX4\n\nor better, you can directly ask @sejun_park_\nwho is actually looking for his next destination!\n\n[n/n]']",https://arxiv.org/abs/2006.08859,"The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width enabling the universal approximation has not been exactly characterized in terms of the input dimension $d_x$ and the output dimension $d_y$. In this work, we provide the first definitive result in this direction for networks using the ReLU activation functions: The minimum width required for the universal approximation of the $L^p$ functions is exactly $\max\{d_x+1,d_y\}$. We also prove that the same conclusion does not hold for the uniform approximation with ReLU, but does hold with an additional threshold activation function. Our proof technique can be also used to derive a tighter upper bound on the minimum width required for the universal approximation using networks with general activation functions. ",Minimum Width for Universal Approximation
117,1273052146288562177,1435944493,Mike Kuhn,"['New paper about stars and gas in the North America/Pelican Nebulae. We used #GaiaDR2 astrometry to refine lists of candidate members. Of the remaining stars, we found them concentrated in several groups, some of which show signs of expansion.\n\n<LINK>\n\n1/7', 'In a several step process, we reject objects with #GaiaDR2 parallaxes and proper motions inconsistent with membership, leaving a sample with a contamination rate of ~3%. This gets rid of a lot of the spatially distributed stars.\n\n2/7 https://t.co/AGFHAnjIAw', 'Among the remaining kinematic members are two O stars, the ionizing source 2MASS J20555125+4352246 identified by Comerón &amp; Pasquali (2005) and HD 199579. However, both appear to be ejected at ~6 km/s.\n\n3/7 https://t.co/sVM2HzKbC4', 'The stars can be subdivided into several spatio-kinematic groups, several of which are expanding. The group that the two O stars belong to has the largest velocity gradient across it. However, the groups have fairly random motions relative to one another. \n\n4/7 https://t.co/hktPgL8kLj', 'We combined the Gaia data with 13CO cloud maps to put together a 6D picture of the star-forming region. From Gaia, we\'ve obtained a new distance estimate of 795 pc to the region, but the ""Gulf of Mexico"" cloud in the south appears to be 35 pc closer and plunging inward. \n\n5/7', 'Stellar members appear to be ~several Myr old across the region. This is similar to the free-fall timescales of the clouds, suggesting that star formation has occurred rapidly. As noted in previous studies, there are already signs that the cloud is being dispersed.\n\n6/7', 'We also created a statistical model (Appendix D) to measure anisotropies in expanding stellar associations.\n\n7/7 https://t.co/wpyu4iejdT']",http://arxiv.org/abs/2006.08622,"We examine the clustering and kinematics of young stellar objects (YSOs) in the North America/Pelican Nebulae, as revealed by Gaia astrometry, in relation to the structure and motions of the molecular gas, as indicated in molecular line maps. The Gaia parallaxes and proper motions allow us to significantly refine previously published lists of YSOs, demonstrating that many of the objects previously thought to form a distributed population turn out to be non-members. The members are subdivided into at least 6 spatio-kinematic groups, each of which is associated with its own molecular cloud component or components. Three of the groups are expanding, with velocity gradients of 0.3-0.5 km/s/pc, up to maximum velocities of ~8 km/s away from the groups' centers. The two known O-type stars associated with the region, 2MASS J20555125+4352246 and HD 199579, are rapidly escaping one of these groups, following the same position-velocity relation as the low-mass stars. We calculate that a combination of gas expulsion and tidal forces from the clumpy distribution of molecular gas could impart the observed velocity gradients within the groups. However, on a global scale, the relative motions of the groups do not appear either divergent or convergent. The velocity dispersion of the whole system is consistent with the kinetic energy gained due to gravitational collapse of the complex. Most of the stellar population has ages similar to the free-fall timescales for the natal clouds. Thus, we suggest the nearly free-fall collapse of a turbulent molecular cloud as the most likely scenario for star formation in this complex. ","The Formation of a Stellar Association in the NGC 7000/IC 5070 Complex:
  Results from Kinematic Analysis of Stars and Gas"
118,1272965018380689408,162099298,Yash Savani,"['I’m fortunate to be in a position to conduct research on debiasing machine learning algorithms. Here is new work with @crwhite_ml and @naveensundarg at @realityengines\n\nBlog: <LINK>\nPaper: <LINK>', 'Code: https://t.co/01mfJv6WU9']",http://arxiv.org/abs/2006.08564,"As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then fine-tune to a specific use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms specifically for this fine-tuning use-case has largely been neglected. In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning. All of our techniques can be used for all popular group fairness measures such as equalized odds or statistical parity difference. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at this https URL ",Intra-Processing Methods for Debiasing Neural Networks
119,1272959132392251392,1138901263439949824,Colin White,['I’m fortunate to be in a position to conduct research on debiasing machine learning algorithms. Here is new work with @yashsavani_ and @naveensundarg at @realityengines\nBlog: <LINK>\nPaper: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2006.08564,"As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then fine-tune to a specific use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms specifically for this fine-tuning use-case has largely been neglected. In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning. All of our techniques can be used for all popular group fairness measures such as equalized odds or statistical parity difference. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at this https URL ",Intra-Processing Methods for Debiasing Neural Networks
120,1272953690429616130,574816439,Charli Sakari,"['New R-Process Alliance paper on astro-ph today, led by Maddie Cain!  <LINK>  \n@annafrebel @Teresehansen @vmplacco\n#rprocess #metalpoorstars', 'And @AstroRana']",https://arxiv.org/abs/2006.08080,"We report the discovery of J1521-3538, a bright (V=12.2), very metal-poor ([Fe/H]=-2.8) strongly r-process enhanced field horizontal branch star, based on a high-resolution, high signal-to-noise Magellan/MIKE spectrum. J1521-3538 shows the largest r-process element over-abundance in any known r-process-enhanced star, with [Eu/Fe]=+2.2, and its chemical abundances of 22 neutron-capture elements closely match the scaled solar r-process pattern. J1521-3538 is also one of few known carbon-enhanced metal-poor stars with r-process enhancement (CEMP-r stars), as found after correcting the measured C abundance for the star's evolutionary status. We propose to extend the existing classification of moderately enhanced (+0.3<=[Eu/Fe]<=+1.0) r-I and strongly r-process enhanced ([Eu/Fe]>+1.0) r-II stars to include an r-III class, for r-process stars such as J1521-3538, with [Eu/Fe]>+2.0 and [Ba/Eu]<-0.5, or >100 times the solar ratio of europium to iron. Using cosmochronometry, we estimate J1521-3538 to be 12.5+-5 Gyr and 8.9+-5 Gyr old, using two different sets of initial production ratios. These ages are based on measurements of the Th line at 4019 A and other r-process element abundances. This is broadly consistent with the old age of a low-mass metal-poor field red horizontal branch star. J1521-3538 likely originated in a low-mass dwarf galaxy that was later accreted by the Milky Way, as evidenced by its highly eccentric orbit. ","The R-Process Alliance: J1521-3538, a very metal-poor, extremely
  r-process-enhanced star with [Eu/Fe]=+2.2, and the class of r-III stars"
121,1272906581324312585,1272253746265948161,Alexander Norcliffe,"['How do Neural ODEs learn 2nd order dynamics? In our new paper “On Second Order Behaviour in Augmented Neural ODEs” we perform an extensive study to answer this question. w/ @CristianBodnar, @itsmebenday, @simidjievskin, @pl219_Cambridge\n\nPaper: <LINK> 1/12 <LINK>', 'We begin by generalising the adjoint sensitivity method to Second Order NODEs (SONODEs). We compare this method with the first order training procedure of the equivalent coupled ODE, which has been used so far in the literature, and find the latter to be more efficient. 2/12 https://t.co/SdNyj6kmPN', 'Furthermore, we study some of the properties of SONODEs and investigate how they manifest on a couple of toy modelling problems. 3/12', 'We consider a generalised parity problem where x is mapped to -x. We show NODEs can do this in even-dimensional spaces using rotations, while SONODEs have a trivial solution where all points pass through the origin with different velocities. 4/12 https://t.co/ZRXysFOi0P', 'We also show that SONODEs, similarly to ANODEs, are not restricted to homeomorphic transformations, and can, therefore, solve the problem of the nested spheres. 5/12 https://t.co/9RNumFBEXs', 'Despite the fact that ANODEs don’t have the inductive biases of SONODEs, we show that they are still flexible enough to learn second-order dynamics in practice. However, they do so by learning to approximate an abstract coupled ODE with an entangled representation. 6/12 https://t.co/WsnBq7FK15', 'We also prove that ANODEs can compactly learn second-order dynamics with minimal augmentation and they do not necessarily need a number of augmented dimensions equal to the dimensionality of the real space. 7/12 https://t.co/2S4DwyqYVi', 'Despite their flexibility, because the augmented dimensions don’t have a clear physical meaning and they can’t be interpreted, ANODEs might be unsuitable for many scientific applications. 8/12', 'In fact, we prove that ANODEs can learn the real-space 2nd order dynamics in an infinite number of (non-trivial) ways and because of this, they always converge to another augmented trajectory. In contrast, SONODEs are constrained to approximate a unique functional form. 9/12 https://t.co/yfuZKQDKyv', 'Ultimately, we find the inductive biases of SONODE to be helpful when modelling second order systems. For instance, SONODE predictions extrapolate extremely well compared to ANODE on a real electronic Duffing oscillator. 10/12 https://t.co/A21Sz8U1wd', 'At the same time, we find SONODEs to be more robust to noise when extrapolating from noisy sine curves with various noise magnitudes. 11/12 https://t.co/FGS8iPCWmW', 'We also consider a higher-order system of airplane vibrations and find that the ability of ANODEs to compactly access higher-order behaviour comes in handy in this scenario. \n\nCode: https://t.co/5X1L6hSyJE 12/12 https://t.co/Eis6hjsjqe']",https://arxiv.org/abs/2006.07220,"Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through infinite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on first order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a first order coupled ODE is equivalent and computationally more efficient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance. ",On Second Order Behaviour in Augmented Neural ODEs
122,1272895370033577986,1022486891772502016,Issam Laradji,['Happy to share our new optimization method! It achieves fast convergence rates and good generalization performance on many standard datasets including large-scale deep learning!\nPaper: <LINK>\nCode: will be released soon! <LINK>'],https://arxiv.org/abs/2006.06835,"Adaptive gradient methods are typically used for training over-parameterized models. To better understand their behaviour, we study a simplistic setting -- smooth, convex losses with models over-parameterized enough to interpolate the data. In this setting, we prove that AMSGrad with constant step-size and momentum converges to the minimizer at a faster $O(1/T)$ rate. When interpolation is only approximately satisfied, constant step-size AMSGrad converges to a neighbourhood of the solution at the same rate, while AdaGrad is robust to the violation of interpolation. However, even for simple convex problems satisfying interpolation, the empirical performance of both methods heavily depends on the step-size and requires tuning, questioning their adaptivity. We alleviate this problem by automatically determining the step-size using stochastic line-search or Polyak step-sizes. With these techniques, we prove that both AdaGrad and AMSGrad retain their convergence guarantees, without needing to know problem-dependent constants. Empirically, we demonstrate that these techniques improve the convergence and generalization of adaptive gradient methods across tasks, from binary classification with kernel mappings to multi-class classification with deep networks. ","Adaptive Gradient Methods Converge Faster with Over-Parameterization
  (but you should do a line-search)"
123,1272877565435248645,33113669,Tim Baldwin,"['New paper with Nitika Mathur and Trevor Cohn to appear #acl2020nlp: Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics <LINK> 1/n', ""Pearson's correlation used to evaluate metrics wrt human performance, but known to be sensitive to outliers ... turns out there are outliers in tasks like WMT metric eval that lead to v high correlations and false conclusions about the reliability of metrics 2/n"", 'remove those and the correlations/findings change substantially, and BLEU and TER, e.g., suddenly look much worse that YiSi-1 and ESIM ... adding ever more evidence to the unreliability of BLEU 3/n', 'Furthermore, when you look at the reliability of system pair evaluation on basis of diff metrics, what show up as sig diffs in BLEU and TER are often spurious (relative to human eval) and lead to false system rankings ... YiSi-1, ESIM and chrF much more reliable n/n', '@mayhewsw @LeonDerczynski Completely random fact for the overfitters: perfect correlation between #acl2020nlp acceptances this year and paper titles with riffs on (old) album titles. Could this be the new post-Sesame Street trend? #no']",https://arxiv.org/abs/2006.06264,"Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation. ","Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine
  Translation Evaluation Metrics"
124,1272874723865579520,1561095932,SanghyukChun,"['Here is our new paper about a new gradient descent-based optimizer <LINK> We show that the momentum could be problematic to scale-invariant operators, e.g., Batch Norm. You can check the code in <LINK> Try `pip install adamp`! <LINK>']",https://arxiv.org/abs/2006.08217,"Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at this https URL ","AdamP: Slowing Down the Slowdown for Momentum Optimizers on
  Scale-invariant Weights"
125,1272869979361357825,1115198081576861696,Kristopher Torp Jensen,"['Do you like onion rings, doughnuts and other topological delights?\nThen check out our new paper with @kao_calvin, @martripodi, and @GJEHennequin where we build Gaussian process latent variable models with non-Euclidean latent spaces!\n<LINK> (1/7) <LINK>', 'As neuroscientists we’re often interested in understanding how low-dimensional quantities are represented in high-dimensional neural activity, and various techniques for dimensionality reduction are used to do this (PCA, tSNE, GPFA …). (2/7)', 'These methods embed observations in a low-dimensional Euclidean space – but we also know that the brain commonly represents non-Euclidean quantities such as head direction or rotational motor plans. (3/7)', 'To address this discrepancy, we developed a method for Gaussian process-based dimensionality reduction with non-Euclidean latent spaces – mGPLVM. (4/7)', 'We build on previous awesome work by @anqiwu_angela  and @jpillowtime to extend GP latent variable models for analyzing neural data to latent manifolds including tori, spheres and SO(3). (5/7) https://t.co/zlmBSoLgy9', 'This allows us to learn latent representations and tuning curves on such manifolds in a completely unsupervised manner, and by comparing mGPLVM log likelihoods we can even distinguish between different latent topologies in both synthetic and biological data. (6/7) https://t.co/qItzzxCNDZ', 'We’re currently working on an mGPLVM pytorch implementation with GPU &amp; CPU compatibility to make it easier for anyone interested in using it with their own data so stay tuned! (7/7)']",https://arxiv.org/abs/2006.07429,"A common problem in neuroscience is to elucidate the collective neural representations of behaviorally important variables such as head direction, spatial location, upcoming movements, or mental spatial transformations. Often, these latent variables are internal constructs not directly accessible to the experimenter. Here, we propose a new probabilistic latent variable model to simultaneously identify the latent state and the way each neuron contributes to its representation in an unsupervised way. In contrast to previous models which assume Euclidean latent spaces, we embrace the fact that latent states often belong to symmetric manifolds such as spheres, tori, or rotation groups of various dimensions. We therefore propose the manifold Gaussian process latent variable model (mGPLVM), where neural responses arise from (i) a shared latent variable living on a specific manifold, and (ii) a set of non-parametric tuning curves determining how each neuron contributes to the representation. Cross-validated comparisons of models with different topologies can be used to distinguish between candidate manifolds, and variational inference enables quantification of uncertainty. We demonstrate the validity of the approach on several synthetic datasets, as well as on calcium recordings from the ellipsoid body of Drosophila melanogaster and extracellular recordings from the mouse anterodorsal thalamic nucleus. These circuits are both known to encode head direction, and mGPLVM correctly recovers the ring topology expected from neural populations representing a single angular variable. ","Manifold GPLVMs for discovering non-Euclidean latent structure in neural
  data"
126,1272817267684904962,1049562817240674304,Marius Lindauer,"['Our new paper on ""Learning Heuristic Selection with Dynamic Algorithm Configuration"" is online at ArXiv (<LINK>). We show how we can use RL to learn offline how to select heuristics for AI planning solvers and that these policies also generalize to new instances.']",https://arxiv.org/abs/2006.08246,"A key challenge in satisficing planning is to use multiple heuristics within one heuristic search. An aggregation of multiple heuristic estimates, for example by taking the maximum, has the disadvantage that bad estimates of a single heuristic can negatively affect the whole search. Since the performance of a heuristic varies from instance to instance, approaches such as algorithm selection can be successfully applied. In addition, alternating between multiple heuristics during the search makes it possible to use all heuristics equally and improve performance. However, all these approaches ignore the internal search dynamics of a planning system, which can help to select the most useful heuristics for the current expansion step. We show that dynamic algorithm configuration can be used for dynamic heuristic selection which takes into account the internal search dynamics of a planning system. Furthermore, we prove that this approach generalizes over existing approaches and that it can exponentially improve the performance of the heuristic search. To learn dynamic heuristic selection, we propose an approach based on reinforcement learning and show empirically that domain-wise learned policies, which take the internal search dynamics of a planning system into account, can exceed existing approaches. ",Learning Heuristic Selection with Dynamic Algorithm Configuration
127,1272804855120302082,711950336,Olof Mogren,"['Our new paper: ""Adversarial representation learning for synthetic replacement of private attributes"". Unlike traditional adv. methods, we sample a synthetic value for the sensitive attribute and improve utility while maintaining a stronger privacy. @EduNiw <LINK> <LINK>', '@dgillblad']",https://arxiv.org/abs/2006.08039,"Data privacy is an increasingly important aspect of many real-world Data sources that contain sensitive information may have immense potential which could be unlocked using the right privacy enhancing transformations, but current methods often fail to produce convincing output. Furthermore, finding the right balance between privacy and utility is often a tricky trade-off. In this work, we propose a novel approach for data privatization, which involves two steps: in the first step, it removes the sensitive information, and in the second step, it replaces this information with an independent random sample. Our method builds on adversarial representation learning which ensures strong privacy by training the model to fool an increasingly strong adversary. While previous methods only aim at obfuscating the sensitive information, we find that adding new random information in its place strengthens the provided privacy and provides better utility at any given level of privacy. The result is an approach that can provide stronger privatization on image data, and yet be preserving both the domain and the utility of the inputs, entirely independent of the downstream task. ","Adversarial representation learning for synthetic replacement of private
  attributes"
128,1272762603631112193,1115880604560691200,NII Yamagishi Lab,"['Our new paper accepted to IJCB 2020, ""Generating Master Faces for Use in Performing Wolf Attacks on Face Recognition Systems,"" is now online: <LINK>', 'An artificial face (""wolf"" sample, top left) generated by StyleGAN in a process called latent variable evolution was falsely accepted by face recognition systems and matched many enrolled IDs. Some of these faces are of the Japanese former prime minister Koizumi Junichiro. https://t.co/ivgrocBGn0', 'This project was done when Huy visited the Idiap Research Institute in Switzerland for 4 months.']",https://arxiv.org/abs/2006.08376,"Due to its convenience, biometric authentication, especial face authentication, has become increasingly mainstream and thus is now a prime target for attackers. Presentation attacks and face morphing are typical types of attack. Previous research has shown that finger-vein- and fingerprint-based authentication methods are susceptible to wolf attacks, in which a wolf sample matches many enrolled user templates. In this work, we demonstrated that wolf (generic) faces, which we call ""master faces,"" can also compromise face recognition systems and that the master face concept can be generalized in some cases. Motivated by recent similar work in the fingerprint domain, we generated high-quality master faces by using the state-of-the-art face generator StyleGAN in a process called latent variable evolution. Experiments demonstrated that even attackers with limited resources using only pre-trained models available on the Internet can initiate master face attacks. The results, in addition to demonstrating performance from the attacker's point of view, can also be used to clarify and improve the performance of face recognition systems and harden face authentication systems. ","Generating Master Faces for Use in Performing Wolf Attacks on Face
  Recognition Systems"
129,1272715794770608128,2279227387,Dr. Jessie Christiansen,['New paper on arxiv tonight from our large K2-Spitzer program! This one is led by @lkreidberg and we tentatively see water in the atmosphere of a warm Neptune-like planet!\n\n<LINK> <LINK>'],https://arxiv.org/abs/2006.07444,"We present a transmission spectrum for the Neptune-size exoplanet HD 106315 c from optical to infrared wavelengths based on transit observations from the Hubble Space Telescope/Wide Field Camera 3, K2, and Spitzer. The spectrum shows tentative evidence for a water absorption feature in the $1.1 - 1.7\mu$m wavelength range with a small amplitude of 30 ppm (corresponding to just $0.8 \pm 0.04$ atmospheric scale heights). Based on an atmospheric retrieval analysis, the presence of water vapor is tentatively favored with a Bayes factor of 1.7 - 2.6 (depending on prior assumptions). The spectrum is most consistent with either enhanced metallicity, high altitude condensates, or both. Cloud-free solar composition atmospheres are ruled out at $>5\sigma$ confidence. We compare the spectrum to grids of cloudy and hazy forward models and find that the spectrum is fit well by models with moderate cloud lofting or haze formation efficiency, over a wide range of metallicities ($1 - 100\times$ solar). We combine the constraints on the envelope composition with an interior structure model and estimate that the core mass fraction is $\gtrsim0.3$. With a bulk composition reminiscent of that of Neptune and an orbital distance of 0.15 AU, HD 106315 c hints that planets may form out of broadly similar material and arrive at vastly different orbits later in their evolution. ","Tentative Evidence for Water Vapor in the Atmosphere of the Neptune-Size
  Exoplanet HD 106315 c"
130,1272697484356141056,1608594464,Jennifer Listgarten,"[""Our new paper on how to autofocus your predictive model for design of proteins, molecules, materials or whatever you're hungry for: <LINK>""]",https://arxiv.org/abs/2006.08052,"Data-driven design is making headway into a number of application areas, including protein, small-molecule, and materials engineering. The design goal is to construct an object with desired properties, such as a protein that binds to a therapeutic target, or a superconducting material with a higher critical temperature than previously observed. To that end, costly experimental measurements are being replaced with calls to high-capacity regression models trained on labeled data, which can be leveraged in an in silico search for design candidates. However, the design goal necessitates moving into regions of the design space beyond where such models were trained. Therefore, one can ask: should the regression model be altered as the design algorithm explores the design space, in the absence of new data? Herein, we answer this question in the affirmative. In particular, we (i) formalize the data-driven design problem as a non-zero-sum game, (ii) develop a principled strategy for retraining the regression model as the design algorithm proceeds---what we refer to as autofocusing, and (iii) demonstrate the promise of autofocusing empirically. ",Autofocused oracles for model-based design
131,1272691131936133120,2800204849,Andrew Gordon Wilson,"['The inductive biases of normalizing flows can be more of a curse than a blessing. Our new paper, ""Why Normalizing Flows Fail to Detect Out-of-Distribution Data"", with @polkirichenko and @Pavel_Izmailov: <LINK> <LINK>']",https://arxiv.org/abs/2006.08545,"Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection. ",Why Normalizing Flows Fail to Detect Out-of-Distribution Data
132,1272585632393555974,989251872107085824,Quoc Le,"['We researchers love pre-training. Our new paper shows that pre-training is unhelpful when we have a lot of labeled data. In contrast, self-training works well even when we have a lot of labeled data. SOTA on PASCAL segmentation &amp; COCO detection.\n\nLink: <LINK> <LINK>', 'This work continues our self-training efforts:\nNoisy Student Training (SOTA on ImageNet): https://t.co/yYIa9xhOVr\nNoisy Student Training for Speech (SOTA on LibriSpeech): https://t.co/IKJDH3L4Fh\nConclusion? Good results on big datasets need self-training (w/ Noisy Student) :)', 'In the paper, we evaluate two “pre-training” methods: supervised and self-supervised. Both do not transfer well to the COCO dataset. https://t.co/lyQZG80Yia', ""Here's how we use self-training (w/ Noisy Student) in our work:\n\n1) Train a model on COCO\n2) Infer labels on ImageNet images\n3) Train a new model on the combined set with noise added\n4) Optional: Iterate the process"", '@karmanya Maybe you should combine self-training with pre-training similar to the PASCAL segmentation experiment?', ""@federico_errica Our paper does show that pre-training is helpful in the small data regime. In the left side of the figure I shared above, initializing from ImageNet pre-trained checkpoint still helps object detection. So it probably doesn't contradict with the previous work.""]",http://arxiv.org/abs/2006.06882,"Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+. ",Rethinking Pre-training and Self-training
133,1272559726421827588,1091042849561473031,Alexander Kolesnikov 🇺🇦,"['Are we still making meaningful progress on ImageNet? What happens if we carefully re-annotate ImageNet val set? How to improve ResNet-50 top-1 accuracy by 2.5% by cleaning training data and using different loss function? See our new paper for the answers: <LINK>. <LINK>', 'New multi-label ImageNet val annotations are available at https://t.co/tq5S3U2IHV. Joint work with @giffmana @olivierhenaff @XiaohuaZhai and @avdnoord.']",https://arxiv.org/abs/2006.07159,"Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition. ",Are we done with ImageNet?
134,1272539522497163270,435463280,Chanyoung Park,"['Check our new #KDD2020 paper, ""Unsupervised Differentiable Multi-aspect Network Embedding."" \n\nPaper: <LINK>\nCode: <LINK>\n\nWe proposed a novel network embedding method for capturing multiple aspects of nodes in a network. (1/2)', 'The main idea is to capture multi-aspects of each node in a network, and at the same time, consider the interactions among the captured aspects. Please check our paper for more detail! (2/2)']",https://arxiv.org/abs/2006.04239,"Network embedding is an influential graph mining technique for representing nodes in a graph as distributed vectors. However, the majority of network embedding methods focus on learning a single vector representation for each node, which has been recently criticized for not being capable of modeling multiple aspects of a node. To capture the multiple aspects of each node, existing studies mainly rely on offline graph clustering performed prior to the actual embedding, which results in the cluster membership of each node (i.e., node aspect distribution) fixed throughout training of the embedding model. We argue that this not only makes each node always have the same aspect distribution regardless of its dynamic context, but also hinders the end-to-end training of the model that eventually leads to the final embedding quality largely dependent on the clustering. In this paper, we propose a novel end-to-end framework for multi-aspect network embedding, called asp2vec, in which the aspects of each node are dynamically assigned based on its local context. More precisely, among multiple aspects, we dynamically assign a single aspect to each node based on its current context, and our aspect selection module is end-to-end differentiable via the Gumbel-Softmax trick. We also introduce the aspect regularization framework to capture the interactions among the multiple aspects in terms of relatedness and diversity. We further demonstrate that our proposed framework can be readily extended to heterogeneous networks. Extensive experiments towards various downstream tasks on various types of homogeneous networks and a heterogeneous network demonstrate the superiority of asp2vec. ",Unsupervised Differentiable Multi-aspect Network Embedding
135,1272510350018121728,355616314,Murray Shanahan,"['New paper with Daniel Pace and Alessandra Russo - ""Learning Diverse Representations for Fast Adaptation to Distribution Shift"": <LINK>']",https://arxiv.org/abs/2006.07119,"The i.i.d. assumption is a useful idealization that underpins many successful approaches to supervised machine learning. However, its violation can lead to models that learn to exploit spurious correlations in the training data, rendering them vulnerable to adversarial interventions, undermining their reliability, and limiting their practical application. To mitigate this problem, we present a method for learning multiple models, incorporating an objective that pressures each to learn a distinct way to solve the task. We propose a notion of diversity based on minimizing the conditional total correlation of final layer representations across models given the label, which we approximate using a variational estimator and minimize using adversarial training. To demonstrate our framework's ability to facilitate rapid adaptation to distribution shift, we train a number of simple classifiers from scratch on the frozen outputs of our models using a small amount of data from the shifted distribution. Under this evaluation protocol, our framework significantly outperforms a baseline trained using the empirical risk minimization principle. ","Learning Diverse Representations for Fast Adaptation to Distribution
  Shift"
136,1272495214561759233,77592002,Ahmad,"['New paper by @justyncw and me, applying his H II region shape analysis to synthetic radio emission from a simulation of mine - including different ages, projections, and noise profiles.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2006.06506,"The statistical shape analysis method developed for probing the link between physical parameters and morphologies of Galactic HII regions is applied here to a set of synthetic observations (SOs) of a numerically modelled HII region. The systematic extraction of HII region shape, presented in the first paper of this series, allows for a quantifiable confirmation of the accuracy of the numerical simulation, with respect to the real observational counterparts of the resulting SOs. A further aim of this investigation is to determine whether such SOs can be used for direct interpretation of the observational data, in a future supervised classification scheme based upon HII region shape. The numerical HII region data was the result of photoionisation and radiation pressure feedback of a 34 Msun star, in a 1000 Msun cloud. The SOs analysed herein comprised four evolutionary snapshots (0.1, 0.2, 0.4 and 0.6 Myr), and multiple viewing projection angles. The shape analysis results provided conclusive evidence of the efficacy of the numerical simulations. When comparing the shapes of the synthetic regions to their observational counterparts, the SOs were grouped in amongst the Galactic HII regions by the hierarchical clustering procedure. There was also an association between the evolutionary distribution of regions and the respective groups. This suggested that the shape analysis method could be further developed for morphological classification of HII regions by using a synthetic data training set, with differing initial conditions of well-defined parameters. ",Shape Analysis of HII Regions -- II. Synthetic Observations
137,1272464287244705793,129802464,Niall Deacon,"['New paper out today. We looked for wide binaries in open clusters. We found some but at a lower rate than seen around stars in general. This suggests clusters like the Pleiades destroy their wide binaries.\n\n<LINK>\n\n1/11', 'Wide binaries are pairs of stars that are bound together by gravity but which are separated by large distances, hundreds or thousands of Earth-Sun distances. We find them by looking for pairs of stars moving across the sky together and are at the same distance from us.\n\n2/11', 'Open clusters are groups of stars that form in the same star forming region and stay together for hundreds of millions of years after their birth. We find them by looking for groups of stars moving across the sky together and are at the same distance from us.\n\n3/11', 'Now if we wanted to look for wide binaries in open clusters you may have spotted the problem. A star that is at the same distance to a member of an open cluster and is moving across the sky with the same speed &amp; direction could be a binary companion or another cluster member\n4/11', 'Luckily we can use a trick to get round this. More widely separated binaries are rarer than closer binaries. While the further you go from a cluster member the bigger your search area so the more chance pairings with other cluster members you will get.\n\n5/11', 'Combining these two factors we were able to separate out the wide binary distribution for three northern open clusters, Alpha Per, the Pleiades and Praesepe. We found wide binaries existed in all three.\n\n6/11 https://t.co/zihjk7XRP8', 'However we found that the rate at which these wide binaries occur in these clusters is about half the rate that they occur in the general population of stars around the Sun.\n\n7/11', 'We then looked at other groups of stars that form together but in much less dense environments than open clusters. These were some groups of stars collectively known as Young Moving Groups &amp; a group of recently discovered stars called the Pisces-Eridanus stream.\n\n8/11', 'We found these looser groups of stars had lots of wide binaries, at or above the level found in stars in general. This suggests lots of close-ish encounters between stars in clusters like the Pleiades might have destroyed their wide binary population.\n\n9/11', 'From our results it looks like most of the wide binaries we see in stars in our neighbourhood of the Galaxy today probably formed in looser associations like the young moving groups. \n\n10/11', 'Small aside, one pairing we found included Asterope/Sterope, one of the naked-eye Pleiades members. We think this pair has a 48% chance of being real. There is also a much wider visual double to Sterope (22 Tau) which was too wide for our analysis.\n\n11/11 https://t.co/CtlNvLvGqn']",https://arxiv.org/abs/2006.06679,"The population statistics of binary stars are an important output of star formation models. However populations of wide binaries evolve over time due to interactions within a system's birth environment and the unfolding of wide, hierarchical triple systems. Hence the wide binary populations observed in star forming regions or OB associations may not accurately reflect the wide binary populations that will eventually reach the field. We use Gaia DR2 data to select members of three open clusters, Alpha~Per, the Pleiades and Praesepe and to flag cluster members that are likely unresolved binaries due to overluminosity or elevated astrometric noise. We then identify the resolved wide binary population in each cluster, separating it from coincident pairings of unrelated cluster members. We find that these clusters have an average wide binary fraction in the 300-3000\,AU projected separation range of 2.1$\pm^{0.4}_{0.2}$\% increasing to 3.0$\pm^{0.8}_{0.7}$\% for primaries with masses in the 0.5-1.5\,$M_{\odot}$ range. This is significantly below the observed field wide binary fraction, but shows some wide binaries survive in these dynamically highly processed environments. We compare our results with another open cluster (the Hyades) and two populations of young stars that likely originated in looser associations (Young Moving Groups and the Pisces-Eridanus stream). We find that the Hyades also has a deficit of wide binaries while the products of looser associations have wide binary fractions at or above field level. ",Wide binaries are rare in open clusters
138,1272464206575603714,2742282828,Andreea Font,"['New paper on arXiv today, in which we propose a new method for identifying the small dark matter subhaloes around galaxies via the imprints they leave in the circumgalactic medium: <LINK>']",https://arxiv.org/abs/2006.06741,"The standard model of cosmology, the LCDM model, robustly predicts the existence of a multitude of dark matter 'subhaloes' around galaxies like the Milky Way. A wide variety of observations have been proposed to look for the gravitational effects such subhaloes would induce in observable matter. Most of these approaches pertain to the stellar or cool gaseous phases of matter. Here we propose a new approach, which is to search for the perturbations that such dark subhaloes would source in the warm/hot circumgalactic medium (CGM) around normal galaxies. With a combination of analytic theory, carefully-controlled high-resolution idealised simulations, and full cosmological hydrodynamical simulations (the ARTEMIS simulations), we calculate the expected signal and how it depends on important physical parameters (subhalo mass, CGM temperature, and relative velocity). We find that dark subhaloes enhance both the local CGM temperature and density and, therefore, also the pressure. For the pressure and density, the fluctuations can vary in magnitude from tens of percent (for subhaloes with M_sub=10^10 Msun) to a few percent (for subhaloes with M_sub=10^8 Msun), although this depends strongly on the CGM temperature. The subhaloes also induce fluctuations in the velocity field ranging in magnitude from a few km/s up to 25 km/s. We propose that X-ray, Sunyaev-Zel'dovich effect, radio dispersion measure, and quasar absorption line observations can be used to measure these fluctuations and place constraints on the abundance and distribution of dark subhaloes, thereby placing constraints on the nature of dark matter. ",The imprint of dark subhaloes on the circumgalactic medium
139,1271709221206151168,1037195648636989442,Hidenori Tanaka,"['Q. Can we find winning lottery tickets, or sparse trainable deep networks at initialization without ever looking at data?\n\nA. Yes, by conserving ""Synaptic Flow"" via our new SynFlow algorithm.\n\nco-led with Daniel Kunin\n&amp; @dyamins, @SuryaGanguli\n\npaper: <LINK>\n1/ <LINK>', 'We can potentially reduce the cost of training if we can prune neural networks at initialization.\n\nThe key challenge is ""layer-collapse,"" the premature pruning of an entire layer making a network untrainable.\n2/ https://t.co/hTB5jEdeuD', 'To better understand the phenomena, we first mathematically formulate and experimentally verify a conservation law.\n\nThis conservation law explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse.\n3/ https://t.co/a8jBLZ5Uoh', 'We then hypothesize that the conservative scoring combined with ""iterative"" re-evaluation can avoid layer collapse. \n\nThis insight also explains how iterative magnitude pruning avoids layer-collapse to identify ""winning-lottery ticket ""subnetworks at initialization.\n4/ https://t.co/kl7sXpTHN2', 'We prove that layer-collapse can be entirely avoided by designing an algorithm with iterative, positive, conservative scoring.\n\nWe design SynFlow satisfying the key requirements and show that it reaches the theoretical limit of max compression without collapsing a network.\n5/ https://t.co/3eLdvdifXB', 'Notably, SynFlow makes no reference to the training data and consistently outperforms existing state-of-the-art\npruning algorithms at initialization on 12 distinct combinations of models and datasets.\n\n6/ https://t.co/q7KjKMK3pd', 'Overall, our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\n\nPlease check out the paper for more details\nhttps://t.co/AAHQchcRKC\n7/', '@tingwuc Yes, we are working to incorporate them into our codebase.\n\nIn the meantime, this paper https://t.co/b5KoA2vPnk did very careful work on how pruning at initialization methods (SNIP, GraSP) compare with ""train-prune"" methods, including IMP and others.', ""@xaqlab Thank you for the question.\nSynFlow naturally avoids layer-bottlenecking that starts well before the eventual collapse.\nThis is why we see a significant gain in performance compared to other methods that don't reach max compression.""]",http://arxiv.org/abs/2006.05467,"Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important. ","Pruning neural networks without any data by iteratively conserving
  synaptic flow"
140,1271564397987667971,790183810973458432,Jakub Tomczak,"['A new exciting work w/ amazing @JohnGatop and M. Stol on generating crisp images using VAEs. The trick is to use image downscaling as a variational posterior and then super-resolution. @CIGroupVU + @BrainCreators\nPaper: <LINK>\nCode: <LINK> <LINK>', '@LucaAmb @JohnGatop @CIGroupVU @BrainCreators What do you mean? 🙂', '@LucaAmb @JohnGatop @CIGroupVU @BrainCreators Haha, right,  we were looking for that kind of cheating 😉']",https://arxiv.org/abs/2006.05218,"The framework of variational autoencoders (VAEs) provides a principled method for jointly learning latent-variable models and corresponding inference models. However, the main drawback of this approach is the blurriness of the generated images. Some studies link this effect to the objective function, namely, the (negative) log-likelihood. Here, we propose to enhance VAEs by adding a random variable that is a downscaled version of the original image and still use the log-likelihood function as the learning objective. Further, by providing the downscaled image as an input to the decoder, it can be used in a manner similar to the super-resolution. We present empirically that the proposed approach performs comparably to VAEs in terms of the negative log-likelihood, but it obtains a better FID score in data synthesis. ",Super-resolution Variational Auto-Encoders
141,1271452307968884737,152207159,Qammer H. Abbasi,"[""Check our new paper, 'Revolutionizing Future Healthcare using Wireless on the Walls (WoW)'  with Prof. Tei Jun Cui from Southeast University   @UofGCSI \n@AAlomainy @AhmedZoha @Prof_M_A_Imran \n<LINK>""]",https://arxiv.org/abs/2006.06479,"Following the standardization and deployment of fifth generation (5G) network, researchers have shifted their focus to beyond 5G communication. Existing technologies have brought forth a plethora of applications that could not have been imagined in the past years. Beyond 5G will enable us to rethink the capability, it will offer in various sectors including agriculture, search and rescue and more specifically in the delivery of health care services. Unobtrusive and non-invasive measurements using radio frequency (RF) sensing, monitoring and control of wearable medical devices are the areas that would potentially benefit from beyond 5G. Applications such as RF sensing, device charging and remote patient monitoring will be a key challenge using millimetre (mmWave) communication. The mmWaves experience multi-path induced fading, where the rate of attenuation is larger as compared to the microwaves. Eventually, mmWave communication systems would require range extenders and guided surfaces. A proposed solution is the use of intelligent reflective surfaces, which will have the ability to manipulate electromagnetic (EM) signals. These intelligent surfaces mounted and/or coated on walls aka - Intelligent Walls are planar and active surfaces, which will be a key element in beyond 5G and 6G communication. These intelligent walls equipped with machine learning algorithm and computation power would have the ability to manipulate EM waves and act as gateways in the heterogeneous network environment. The article presents the application and vision of intelligent walls for next-generation healthcare in the era of beyond 5G. ",Revolutionizing Future Healthcare using Wireless on the Walls (WoW)
142,1271449150937333760,61623544,Dr./Prof. Renée Hložek,"['New paper out today, with awesome work by my student @ikape_margaret as well as fun collaborators @jcolinhill Simone Ferraro and Marcelo Alvarez. It was fun to work with this group on reionization constraints. Our take home plot is here: <LINK> <LINK>', '@ikape_margaret @jcolinhill With future CMB missions, the combined 2pt and 4pt constraints on the patchy reionization signal will mean we can break the degeneracy between the width (dz) of reionization and the optical depth (tau).', '@ikape_margaret @jcolinhill @ikape_margaret already contributed the 2pt constraints for the @SimonsObs - our 2pt constraints will be pretty good already! She’s working on a paper right now looking at constraining these models with current data. Such fun! @DunlapInstitute', '@That_Astro_Chic @ikape_margaret @jcolinhill @SimonsObs @DunlapInstitute 🥰']",https://arxiv.org/abs/2006.06594,"The epoch of reionization is one of the major phase transitions in the history of the universe, and is a focus of ongoing and upcoming cosmic microwave background (CMB) experiments with improved sensitivity to small-scale fluctuations. Reionization also represents a significant contaminant to CMB-derived cosmological parameter constraints, due to the degeneracy between the Thomson-scattering optical depth, $\tau$, and the amplitude of scalar perturbations, $A_s$. This degeneracy subsequently hinders the ability of large-scale structure data to constrain the sum of the neutrino masses, a major target for cosmology in the 2020s. In this work, we explore the kinematic Sunyaev-Zel'dovich (kSZ) effect as a probe of reionization, and show that it can be used to mitigate the optical depth degeneracy with high-sensitivity, high-resolution data from the upcoming CMB-S4 experiment. We discuss the dependence of the kSZ power spectrum on physical reionization model parameters, as well as on empirical reionization parameters, namely $\tau$ and the duration of reionization, $\Delta z$. We show that by combining the kSZ two-point function and the reconstructed kSZ four-point function, degeneracies between $\tau$ and $\Delta z$ can be strongly broken, yielding tight constraints on both parameters. We forecast $\sigma(\tau) = 0.003$ and $\sigma(\Delta z) = 0.25$ for a combination of CMB-S4 and Planck data, including detailed treatment of foregrounds and atmospheric noise. The constraint on $\tau$ is nearly identical to the cosmic-variance limit that can be achieved from large-angle CMB polarization data. The kSZ effect thus promises to yield not only detailed information about the reionization epoch, but also to enable high-precision cosmological constraints on the neutrino mass. ","Mitigating the optical depth degeneracy using the kinematic
  Sunyaev-Zel'dovich effect with CMB-S4"
143,1271407145343467520,961890807023390720,Matteo Rosati,"['To squeeze or not to squeeze, aye there’s the rub! \nCheck out our new paper on Gaussian communication under phase noise: <LINK>.', 'Highlights: \n(i) optimal coherent-state alphabets are generated by energy-coding + random passive interferometers; \n(ii) squeezed-coherent alphabets can surpass all coherent ones; \n(iii) reference-frame alignment is sub-optimal.']",https://arxiv.org/abs/2006.06522,"We study the problem of transmitting classical information using quantum Gaussian states on a family of phase-noise channels with a finite decoherence time, such that the phase-reference is lost after $m$ consecutive uses of the transmission line. This problem is relevant for long-distance communication in free space and optical fiber, where phase noise is typically considered as a limiting factor. The Holevo capacity of these channels is always attained with photon-number encodings, challenging with current technology. Hence for coherent-state encodings the optimal rate depends only on the total-energy distribution and we provide upper and lower bounds for all $m$, the latter attainable at low energies with on/off modulation and photodetection. We generalize this lower bound to squeezed-coherent encodings, exhibiting for the first time to our knowledge an unconditional advantage with respect to any coherent encoding for $m=1$ and a considerable advantage with respect to its direct coherent counterpart for $m>1$. This advantage is robust with respect to moderate attenuation, and persists in a regime where Fock encodings with up to two-photon states are also suboptimal. Finally, we show that the use of part of the energy to establish a reference frame is sub-optimal even at large energies. Our results represent a key departure from the case of phase-covariant Gaussian channels and constitute a proof-of-principle of the advantages of using non-classical, squeezed light in a motivated communication setting. ",Squeezing-enhanced communication without a phase reference
144,1271378636701122560,935631082036359168,Matthias C. Caro,"['New paper out on the @arxiv about a quantum version of binary classification, where we take quantum states as labels: <LINK> \nThis can be seen as a toy problem for the task of learning quantum state preparation procedures.\n#QuantumComputing #LearningTheory\n1/2', ""By measuring the labels, we reduce this to a classical learning problem with noisy labels, for which we establish the optimal sample complexity.\nIn fact, our simple procedure is basically optimal w.r.t. the number of quantum examples.\n\nI'd be happy to hear/read your comments!\n2/2""]",https://arxiv.org/abs/2006.06005,"In classical statistical learning theory, one of the most well studied problems is that of binary classification. The information-theoretic sample complexity of this task is tightly characterized by the Vapnik-Chervonenkis (VC) dimension. A quantum analog of this task, with training data given as a quantum state has also been intensely studied and is now known to have the same sample complexity as its classical counterpart. We propose a novel quantum version of the classical binary classification task by considering maps with classical input and quantum output and corresponding classical-quantum training data. We discuss learning strategies for the agnostic and for the realizable case and study their performance to obtain sample complexity upper bounds. Moreover, we provide sample complexity lower bounds which show that our upper bounds are essentially tight for pure output states. In particular, we see that the sample complexity is the same as in the classical binary classification task w.r.t. its dependence on accuracy, confidence and the VC-dimension. ",Binary Classification with Classical Instances and Quantum Labels
145,1271341158434844673,1189915106831933440,David W. Romero,"[""Equivariance has been successful for vision tasks. But what about other fields? \n\nIn our new paper *Wavelet Networks: Scale Equivariant Learning From Raw Waveforms* <LINK> \n\nWe show it's also powerful for time-series, e.g. audio. \nw/ @erikjbekkers @jmtomczak &amp; MH <LINK>"", ""@michielree @erikjbekkers @jmtomczak Yes! We're looking for ways to integrate the approach into the pipeline. We did some experiments and it seems promising for fault detection as well :D hopefully it'll become a part of SAM4"", '@david_van_dijk @erikjbekkers @jmtomczak Thank you David', ""@LucaAmb @erikjbekkers @jmtomczak Nice to hear! I'd love to hear if you have some applications where you think it could be of relevance :)""]",https://arxiv.org/abs/2006.05259,"Inducing symmetry equivariance in deep neural architectures has resolved into improved data efficiency and generalization. In this work, we utilize the concept of scale and translation equivariance to tackle the problem of learning on time-series from raw waveforms. As a result, we obtain representations that largely resemble those of the wavelet transform at the first layer, but that evolve into much more descriptive ones as a function of depth. Our empirical results support the suitability of our Wavelet Networks which with a simple architecture design perform consistently better than CNNs on raw waveforms and on par with spectrogram-based methods. ",Wavelet Networks: Scale Equivariant Learning From Raw Waveforms
146,1271325739107770369,2563532985,Prof Anna Watts,"['New group paper! Waves in thin oceans on oblate neutron stars, by van Baal, Chambers &amp; Watts, MNRAS in press <LINK>', 'We study how the different families of modes that exist in neutron star oceans are affected by rotationally-induced oblateness (in Newtonian gravity).  This is an important factor for mode models that seek to explain thermonuclear burst oscillations.', 'Along with nuclear burning, relativity, convection, flame spread, etc etc etc!  And is therefore yet one more thing that we have to include in our models. https://t.co/UxLMXPrVJG', ""[One of the mode types we look at are Yanai waves. Interestingly meteorologist Michio Yanai doesn't seen to have a Wikipedia page. But UCLA have an annual Yanai Lecture, and you can read his bio and see some pics there. https://t.co/onE9Gp2msH  ]""]",https://arxiv.org/abs/2006.06382,"Waves in thin fluid layers are important in various stellar and planetary problems. Due to rapid rotation such systems will become oblate, with a latitudinal variation in the gravitational acceleration across the surface of the object. In the case of accreting neutron stars, rapid rotation could lead to a polar radius smaller than the equatorial radius by a factor $\sim 0.8$. We investigate how the oblateness and a changing gravitational acceleration affect different hydrodynamic modes that exist in such fluid layers through analytic approximations and numerical calculations. The wave vectors of $g$-modes and Yanai modes increase for more oblate systems compared to spherical counterparts, although the impact of variations in the changing gravitational acceleration is effectively negligible. We find that for increased oblateness, Kelvin modes show less equatorial confinement and little change in their wave vector. For $r$-modes, we find that for more oblate systems the wave vector decreases. The exact manner of these changes for the $r$-modes depends on the model for the gravitational acceleration across the surface. ",Waves in Thin Oceans on Oblate Neutron Stars
147,1271250743949840384,2695638248,Qing Li,"['(1/3) I am very happy to share our new paper accepted at #icml2020 @VCLA_UCLA \n\n""Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning""\n\npaper with code: <LINK>\n<LINK>', '(2/3) We close the loop of neural-symbolic learning by introducing the grammar model as a symbolic prior to bridge neural perception and symbolic reasoning, and proposing a novel back-search algorithm', '(3/3) which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently.\n\nThanks to my wonderful collaborators Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, Song-Chun Zhu @VCLA_UCLA']",https://arxiv.org/abs/2006.06649,"The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the \textbf{grammar} model as a \textit{symbolic prior} to bridge neural perception and symbolic reasoning, and (2) proposing a novel \textbf{back-search} algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at \url{this https URL}. ","Closed Loop Neural-Symbolic Learning via Integrating Neural Perception,
  Grammar Parsing, and Symbolic Reasoning"
148,1271062581365616640,797858947344568320,Balazs Tarjan,"['Our new paper about the effectiveness of neural text generation based data augmentation for morphologically rich speech recognition has been accepted to TSD 2020 conference!\nLink: <LINK>\nHighlights of the paper in the tweet thread:', '- we demonstrate that the neural augmented language models can help to capture almost 50% of the knowledge of the a RNNLM yet by dropping the second decoding pass and preserving real-time operation https://t.co/Vwubqe6BCd', '- we show that using a neural augmented LM in the first pass followed by a neural second pass decoding, even offline ASR results can be significantly improved', '- we systematically compare word and subword LMs and show that subword-based neural text augmentation can be especially beneficial in under-resourced conditions https://t.co/k6y4e0ci9K']",http://arxiv.org/abs/2006.05129,"Advanced neural network models have penetrated Automatic Speech Recognition (ASR) in recent years, however, in language modeling many systems still rely on traditional Back-off N-gram Language Models (BNLM) partly or entirely. The reason for this are the high cost and complexity of training and using neural language models, mostly possible by adding a second decoding pass (rescoring). In our recent work we have significantly improved the online performance of a conversational speech transcription system by transferring knowledge from a Recurrent Neural Network Language Model (RNNLM) to the single pass BNLM with text generation based data augmentation. In the present paper we analyze the amount of transferable knowledge and demonstrate that the neural augmented LM (RNN-BNLM) can help to capture almost 50% of the knowledge of the RNNLM yet by dropping the second decoding pass and making the system real-time capable. We also systematically compare word and subword LMs and show that subword-based neural text augmentation can be especially beneficial in under-resourced conditions. In addition, we show that using the RNN-BNLM in the first pass followed by a neural second pass, offline ASR results can be even significantly improved. ","On the Effectiveness of Neural Text Generation based Data Augmentation
  for Recognition of Morphologically Rich Speech"
149,1271028444818804738,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper on arXiv, on how to develop use cases for quantum computing <LINK> focuses on the importance of having a standardized approach, rather than looking at each in an ad-hoc way (which usually focuses on positives and ignores negatives) @jqcDurNew @DurhamQlm', 'The purpose of this paper is not to claim that our method is the one definitive answer, but to start a conversation which really needs to be started and to encourage people to think more rigorously about use cases']",https://arxiv.org/abs/2006.05846,"We propose a standardized methodology for developing and evaluating use cases for quantum computers and quantum inspired methods. This methodology consists of a standardized set of questions which should be asked to determine how and indeed if, near term quantum computing can play a role in a given application. Developing such a set of questions is important because it allows different use cases to be evaluated in a fair and objective way, rather than considering each case on an ad hoc basis which could lead to an evaluation which focuses on positives of a use case, while ignoring weaknesses. To demonstrate our methodology we apply it to a concrete use case, ambulance dispatch, and find that there are some ways in which near term quantum computing could be deployed sensibly, but also demonstrate some cases ways in which its use would not be advised. The purpose of this paper is to initiate a dialogue within the community of quantum computing scientists and potential end users on what questions should be asked when developing real world use cases. ","Toward a standardized methodology for constructing quantum computing use
  cases"
150,1270991464214953984,4249537197,Christian Wolf,"['Second paper: we introduce a new semantic loss for VQA adding structure to the VQA answer space estimated from redundancy in annotations, questioning the classification approach to VQA.\nWork by @CorentK, @antigregory,  @moezbac and yours, truly.\n<LINK> <LINK>']",https://arxiv.org/abs/2006.05726,"Since its appearance, Visual Question Answering (VQA, i.e. answering a question posed over an image), has always been treated as a classification problem over a set of predefined answers. Despite its convenience, this classification approach poorly reflects the semantics of the problem limiting the answering to a choice between independent proposals, without taking into account the similarity between them (e.g. equally penalizing for answering cat or German shepherd instead of dog). We address this issue by proposing (1) two measures of proximity between VQA classes, and (2) a corresponding loss which takes into account the estimated proximity. This significantly improves the generalization of VQA models by reducing their language bias. In particular, we show that our approach is completely model-agnostic since it allows consistent improvements with three different VQA models. Finally, by combining our method with a language bias reduction approach, we report SOTA-level performance on the challenging VQAv2-CP dataset. ",Estimating semantic structure for the VQA answer space
151,1270988088806985728,123591961,Sebastian Deffner,"['Exciting new manuscript with Ricardo Puebla and @quantumwizard: <LINK>\n\n#KZM, #QSL, #STA all in one paper!!']",https://arxiv.org/abs/2006.04830,"Geometric quantum speed limits quantify the trade-off between the rate with which quantum states can change and the resources that are expended during the evolution. Counterdiabatic driving is a unique tool from shortcuts to adiabaticity to speed up quantum dynamics while completely suppressing nonequilibrium excitations. We show that the quantum speed limit for counterdiabatically driven systems undergoing quantum phase transitions fully encodes the Kibble-Zurek mechanism by correctly predicting the transition from adiabatic to impulse regimes. Our findings are demonstrated for three scenarios, namely the transverse field Ising, the Landau-Zener, and the Lipkin-Meshkov-Glick models. ","Kibble-Zurek scaling in quantum speed limits for shortcuts to
  adiabaticity"
152,1270984428429479939,22977318,Nuno Castro,"['In this paper with M. Romão and R. Pedro we study how to search for new phenomena at particle colliders without being biased by our assumptions on it. The idea is to use anomaly detection methods to find events that deviate from the expected background.\n<LINK> <LINK>', '@DrAndreDavid @MiguelCRomao @lipwebapps @cienciasuminho True. And also offline, contributing to explore all the data and avoiding missing an eventual discovery :-)']",https://arxiv.org/abs/2006.05432,"In this paper we propose a new strategy, based on anomaly detection methods, to search for new physics phenomena at colliders independently of the details of such new events. For this purpose, machine learning techniques are trained using Standard Model events, with the corresponding outputs being sensitive to physics beyond it. We explore three novel AD methods in HEP: Isolation Forest, Histogram Based Outlier Detection, and Deep Support Vector Data Description; alongside the most customary Autoencoder. In order to evaluate the sensitivity of the proposed approach, predictions from specific new physics models are considered and compared to those achieved when using fully supervised deep neural networks. A comparison between shallow and deep anomaly detection techniques is also presented. Our results demonstrate the potential of semi-supervised anomaly detection techniques to extensively explore the present and future hadron colliders' data. ","Finding New Physics without learning about it: Anomaly Detection as a
  tool for Searches at Colliders"
153,1270977197977763840,2357079560,Andreas Theodorou,"['🚨New paper alert!🚨\n""Contestable Black Boxes"" on the Right to Contest &amp;how that is different from XAI. Accepted for presentation at #RuleML2020. Work done with @vdignum &amp; @bandadmoebius (@UmeaUniversity)  &amp; L. Michael (@oucyprus @RISEupCyprus) \nPreprint: <LINK>']",https://arxiv.org/abs/2006.05133,"The right to contest a decision with consequences on individuals or the society is a well-established democratic right. Despite this right also being explicitly included in GDPR in reference to automated decision-making, its study seems to have received much less attention in the AI literature compared, for example, to the right for explanation. This paper investigates the type of assurances that are needed in the contesting process when algorithmic black-boxes are involved, opening new questions about the interplay of contestability and explainability. We argue that specialised complementary methodologies to evaluate automated decision-making in the case of a particular decision being contested need to be developed. Further, we propose a combination of well-established software engineering and rule-based approaches as a possible socio-technical solution to the issue of contestability, one of the new democratic challenges posed by the automation of decision making. ",Contestable Black Boxes
154,1270922104880263168,1147473161950134273,Akinori F. Ebihara,"['Our new paper,\n""Deep Neural Networks for the Sequential Probability Ratio Test on Non-i.i.d. Data Series""\nis now on arXiv:\n<LINK>', 'The proposed algorithm, ""SPRT-TANDEM""\nand a novel early-classification dataset, ""Nosaic-MNIST""\nare now on GitHub:\nhttps://t.co/9rWtl0qq0O']",https://arxiv.org/abs/2006.05587,"Classifying sequential data as early and as accurately as possible is a challenging yet critical problem, especially when a sampling cost is high. One algorithm that achieves this goal is the sequential probability ratio test (SPRT), which is known as Bayes-optimal: it can keep the expected number of data samples as small as possible, given the desired error upper-bound. However, the original SPRT makes two critical assumptions that limit its application in real-world scenarios: (i) samples are independently and identically distributed, and (ii) the likelihood of the data being derived from each class can be calculated precisely. Here, we propose the SPRT-TANDEM, a deep neural network-based SPRT algorithm that overcomes the above two obstacles. The SPRT-TANDEM sequentially estimates the log-likelihood ratio of two alternative hypotheses by leveraging a novel Loss function for Log-Likelihood Ratio estimation (LLLR) while allowing correlations up to $N (\in \mathbb{N})$ preceding samples. In tests on one original and two public video databases, Nosaic MNIST, UCF101, and SiW, the SPRT-TANDEM achieves statistically significantly better classification accuracy than other baseline classifiers, with a smaller number of data samples. The code and Nosaic MNIST are publicly available at this https URL ","Sequential Density Ratio Estimation for Simultaneous Optimization of
  Speed and Accuracy"
155,1270893041532846080,2337598033,Geraint F. Lewis,"['Cool new paper on the @arxiv from @galahsurvey (including @astro_sven @JossBlandHawtho @_sarahmartell_ @FadAstra and more) -  where’s the lithium???\n\n<LINK> <LINK>', '@TodLauer @arxiv @galahsurvey @astro_sven @JossBlandHawtho @_sarahmartell_ @FadAstra I will pass this to @JossBlandHawtho !']",https://arxiv.org/abs/2006.05173,"Lithium depletion and enrichment in the cosmos is not yet well understood. To help tighten constraints on stellar and Galactic evolution models, we present the largest high-resolution analysis of Li abundances A(Li) to date, with results for over 100 000 GALAH field stars spanning effective temperatures $5900\,\mathrm{K} \lesssim \rm{T_{eff}} \lesssim7000\,\mathrm{K}$ and metallicities $-3 \lesssim \rm[Fe/H] \lesssim +0.5$. We separated these stars into two groups, on the warm and cool side of the so-called Li-dip, a localised region of the Kiel diagram wherein lithium is severely depleted. We discovered that stars in these two groups show similar trends in the A(Li)-[Fe/H] plane, but with a roughly constant offset in A(Li) of 0.4 dex, the warm group having higher Li abundances. At $\rm[Fe/H]\gtrsim-0.5$, a significant increasing in Li abundance with increasing metallicity is evident in both groups, signalling the onset of significant Galactic production. At lower metallicity, stars in the cool group sit on the Spite plateau, showing a reduced lithium of around 0.4 dex relative to the primordial value predicted from Big Bang nucleosynthesis (BBN). However, stars in the warm group between [Fe/H] = -1.0 and -0.5, form an elevated plateau that is largely consistent with the BBN prediction. This may indicate that these stars in fact preserve the primordial Li produced in the early Universe. ","The GALAH Survey: A new constraint on cosmological lithium and Galactic
  lithium evolution from warm dwarf stars"
156,1270441290812002306,831815461956419584,Marek Kwiek,"['Finally, my new paper on ""gender homophily in academic publishing"" is ready! Read it on arXiv: <LINK>.\nPolish scientists (N=25,463): majority of  males collaborate only with males. Majority of females do not collaborate with females at all! (158,743 Scopus art.). <LINK>']",https://arxiv.org/abs/2006.03935,"We examined the male-female collaboration practices of all internationally visible Polish university professors (N = 25,463) based on their Scopus-indexed publications from 2009-2018 (158,743 journal articles). We merged a national registry of 99,935 scientists (with full administrative and biographical data) with the Scopus publication database, using probabilistic and deterministic record linkage. Our unique biographical, administrative, publication, and citation database (The Polish Science Observatory) included all professors with at least a doctoral degree employed in 85 research-involved universities. We determined what we term an individual publication portfolio for every professor, and we examined the respective impacts of biological age, academic position, academic discipline, average journal prestige, and type of institution on the same-sex collaboration ratio. The gender homophily principle (publishing predominantly with scientists of the same sex) was found to apply to male scientists - but not to females. The majority of male scientists collaborate solely with males; most female scientists, in contrast, do not collaborate with females at all. Across all age groups studied, all-female collaboration is marginal, while all-male collaboration is pervasive. Gender homophily in research-intensive institutions proved stronger for males than for females. Finally, we used a multi-dimensional fractional logit regression model to estimate the impact of gender and other individual-level and institutional-level independent variables on gender homophily in research collaboration. ","Gender-Based Homophily in Research: A Large-Scale Study of Man-Woman
  Collaboration"
157,1270435732243578881,19344537,Lukas Ruff,"[""Classification of normal data against few random natural images doesn't sound promising for anomaly detection, right? Well... check out our new preprint 'Rethinking Assumptions in Deep Anomaly Detection'.\n\nPaper: <LINK>\n@PyTorch  Code: <LINK>"", 'The multiscale structure of images seems to fool this intuition. On a recent ImageNet one-class benchmark, classification of the normal training data against as few as 64 random images is able to outperform the current state of the art.', 'This is joint work with @robvdm, @BillyJoeFranks, Klaus-Robert Müller, and Marius Kloft.', '@sabokrou Looking forward to read your work!']",https://arxiv.org/abs/2006.00339,"Though anomaly detection (AD) can be viewed as a classification problem (nominal vs. anomalous) it is usually treated in an unsupervised manner since one typically does not have access to, or it is infeasible to utilize, a dataset that sufficiently characterizes what it means to be ""anomalous."" In this paper we present results demonstrating that this intuition surprisingly seems not to extend to deep AD on images. For a recent AD benchmark on ImageNet, classifiers trained to discern between normal samples and just a few (64) random natural images are able to outperform the current state of the art in deep AD. Experimentally we discover that the multiscale structure of image data makes example anomalies exceptionally informative. ",Rethinking Assumptions in Deep Anomaly Detection
158,1270370067427950592,1107358308,Jack Turner,"['Some new work with Joe, Elliot, and @AmosStorkey: NAS Without Training \n\nArchitecture search directly on architectures (no training steps!). Find a good net for your task in ~1 second. \n\nPaper: <LINK>\n@PyTorch code: <LINK>']",http://arxiv.org/abs/2006.04647,"The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network's trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in untrained networks and motivate how this can give a measure which is usefully indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201, NATS-Bench, and Network Design Spaces. Our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search. Code for reproducing our experiments is available at this https URL ",Neural Architecture Search without Training
159,1270333283755270144,2799887322,Robert Dadashi,"['New paper out: PWIL ! A simple imitation learning method, which reinforces a reward signal based on a distance to expert demonstrations. Makes Humanoid walk with a single demonstration (below). 1/\n\n<LINK> <LINK>', 'Idea: at the start of the episode all expert state-action pairs are available. As the agent takes action a in state s, look for the closest expert state-action pair (s*, a*), pop it, and define a reward r = exp(- d(s, a, s*, a*) ). 2/', 'Conceptually, PWIL defines a suboptimal transport between the agent state-action pairs and the expert state-action pairs. The approach relies on a distance in an MDP; in our case we use expert demonstrations to derive a distance. 3/', 'Contrary to adversarial IL methods, we bypass the minmax optimization problem and reinforce a non-stationary reward function that is not re-parameterized with interactions with the environment, and that relies on 2 hyperparameters. 4/', 'We compare PWIL with DAC, and show results for the original return of the task (not available in real settings) but also in terms of the Wasserstein distance between the agent and the expert. 5/', 'We recover near-optimal expert behaviour on all tasks considered. Joint work with my great collaborators: @leonardhussenot, Matthieu Geist and Olivier Pietquin ! 6/', 'with hopefully a sharper version of our humanoid :) https://t.co/YsrfLl6hiX']",http://arxiv.org/abs/2006.04678,"Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance. ",Primal Wasserstein Imitation Learning
160,1270305937388167169,1258771704995815424,Beren Millidge,"['Excited to announce a new preprint with @a_tschantz @drclbuckley : ""Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs"" \n\npaper: <LINK>\ncode: <LINK>']",https://arxiv.org/abs/2006.04182,"Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures. ","Predictive Coding Approximates Backprop along Arbitrary Computation
  Graphs"
161,1270285007349825537,712960453,Prashant Saxena,"['New paper (<LINK>) where we discuss how wrinkling, snap-through, and symmetry-loss instabilities interact in inflated electroelastic membranes. Lots of interesting mechanics and mathematics with some computations (1/2)', 'Collaboration with colleagues in Glasgow (@GCEC_ ), Kanpur (@basantlalsharma ) and Erlangen (@UniFAU ).']",https://arxiv.org/abs/2006.04427,"We analyse here the problem of large deformation of dielectric elastomeric membranes under coupled electromechanical loading. Extremely large deformations (enclosed volume changes of 100 times and greater) of a toroidal membrane are studied by the use of a variational formulation that accounts for the total energy due to mechanical and electrical fields. A modified shooting method is adopted to solve the resulting system of coupled and highly nonlinear ordinary differential equations. We demonstrate the occurrence of limit point, wrinkling, and symmetry-breaking buckling instabilities in the solution of this problem. Onset of each of these ""reversible"" instabilities depends significantly on the ratio of the mechanical load to the electric load, thereby providing a control mechanism for state switching. ","Coupled electro-elastic deformation and instabilities of a toroidal
  membrane"
162,1270159134869139457,485168442,Christopher Brinton,"['Our new paper on Fog learning, an architecture for large scale machine learning in fog networks: <LINK>']",https://arxiv.org/abs/2006.03594,"Machine learning (ML) tasks are becoming ubiquitous in today's network applications. Federated learning has emerged recently as a technique for training ML models at the network edge by leveraging processing capabilities across the nodes that collect the data. There are several challenges with employing conventional federated learning in contemporary networks, due to the significant heterogeneity in compute and communication capabilities that exist across devices. To address this, we advocate a new learning paradigm called fog learning which will intelligently distribute ML model training across the continuum of nodes from edge devices to cloud servers. Fog learning enhances federated learning along three major dimensions: network, heterogeneity, and proximity. It considers a multi-layer hybrid learning framework consisting of heterogeneous devices with various proximities. It accounts for the topology structures of the local networks among the heterogeneous nodes at each network layer, orchestrating them for collaborative/cooperative learning through device-to-device (D2D) communications. This migrates from star network topologies used for parameter transfers in federated learning to more distributed topologies at scale. We discuss several open research directions to realizing fog learning. ","From Federated to Fog Learning: Distributed Machine Learning over
  Heterogeneous Wireless Networks"
163,1270101457279352832,556151596,Lawrence M. Krauss,"['The new DARWIN scientific collaboration I am a part of is designing a detector that can give interesting new measurements of solar neutrinos.  Here is our newest scientific paper.   <LINK>', '@mustangman6799 There is no ideal size. It depends on what physical questions you are trying to answer.', '@realEdSahakian Not that we know of.', '@wunder_dj Thanks.']",https://arxiv.org/abs/2006.03114,"We detail the sensitivity of the liquid xenon (LXe) DARWIN observatory to solar neutrinos via elastic electron scattering. We find that DARWIN will have the potential to measure the fluxes of five solar neutrino components: $pp$, $^7$Be, $^{13}$N, $^{15}$O and $pep$. The precision of the $^{13}$N, $^{15}$O and $pep$ components is hindered by the double-beta decay of $^{136}$Xe and, thus, would benefit from a depleted target. A high-statistics observation of $pp$ neutrinos would allow us to infer the values of the weak mixing angle, $\sin^2\theta_w$, and the electron-type neutrino survival probability, $P_e$, in the electron recoil energy region from a few keV up to 200 keV for the first time, with relative precision of 5% and 4%, respectively, at an exposure of 300 ty. An observation of $pp$ and $^7$Be neutrinos would constrain the neutrino-inferred solar luminosity down to 0.2%. A combination of all flux measurements would distinguish between the high (GS98) and low metallicity (AGS09) solar models with 2.1-2.5$\sigma$ significance, independent of external measurements from other experiments or a measurement of $^8$B neutrinos through coherent elastic neutrino-nucleus scattering in DARWIN. Finally, we demonstrate that with a depleted target DARWIN may be sensitive to the neutrino capture process of $^{131}$Xe. ",Solar Neutrino Detection Sensitivity in DARWIN via Electron Scattering
164,1270093422691364864,54599008,Chase,"['New paper drop!\n\nAnomaly Detection with TensorNetworks.\n<LINK>', 'The basic idea is really simple. We map our input into a vector space. If the length of our vector is close to 0, we flag it as an anomaly. https://t.co/Ksbk4psTLG', 'We achieve this by taking the inner product of our input/MPO model with itself. Again, very simple. https://t.co/WTHkeKGlTQ', 'One of the awesome advantages of using a tensor network is that our model is entirely linear. This allows us to include a GLOBAL penalty that tries to bring all possible inputs close to 0.\n\nThis penalty is just the partition function of our MPO. https://t.co/2akTwaG8zl', 'These two terms bring us to a really simple and beautiful loss function. The first term tries to bring all normal inputs to the edge of a unit sphere, the second tries to bring everything to 0. https://t.co/CQPYMaGVTO', 'And our results kick ass. I want to point out the wine dataset specifically. This one is hard because its so small and so sparse. The second best method got 60% AUC, our method got 97%. https://t.co/qluZ0Wv2Hr', 'This was a really awesome project, and Jensen knocked it out of the park with how quickly we were able to publish this.\n\nSpecial thanks to @sleichen and @Theteamatx for making this work possible!\n\nCongrats again Jensen on the phenomenal paper!', 'Defund the police.\n\nhttps://t.co/UEVk2XPhzk']",https://arxiv.org/abs/2006.02516,"Originating from condensed matter physics, tensor networks are compact representations of high-dimensional tensors. In this paper, the prowess of tensor networks is demonstrated on the particular task of one-class anomaly detection. We exploit the memory and computational efficiency of tensor networks to learn a linear transformation over a space with dimension exponential in the number of original features. The linearity of our model enables us to ensure a tight fit around training instances by penalizing the model's global tendency to a predict normality via its Frobenius norm---a task that is infeasible for most deep learning models. Our method outperforms deep and classical algorithms on tabular datasets and produces competitive results on image datasets, despite not exploiting the locality of images. ",Anomaly Detection with Tensor Networks
165,1270055525334409224,771267202762670081,Mihail Eric,['Our SIGDial 2020 paper is up! 🚀 <LINK> @AmazonScience \n\nTLDR: We propose extending traditional task-oriented dialogue systems with the ability to incorporate unstructured knowledge access. We release a new dataset and subtasks for benchmarking these abilities.'],https://arxiv.org/abs/2006.03533,"Most prior work on task-oriented dialogue systems are restricted to a limited coverage of domain APIs, while users oftentimes have domain related requests that are not covered by the APIs. In this paper, we propose to expand coverage of task-oriented dialogue systems by incorporating external unstructured knowledge sources. We define three sub-tasks: knowledge-seeking turn detection, knowledge selection, and knowledge-grounded response generation, which can be modeled individually or jointly. We introduce an augmented version of MultiWOZ 2.1, which includes new out-of-API-coverage turns and responses grounded on external knowledge sources. We present baselines for each sub-task using both conventional and neural approaches. Our experimental results demonstrate the need for further research in this direction to enable more informative conversational systems. ","Beyond Domain APIs: Task-oriented Conversational Modeling with
  Unstructured Knowledge Access"
166,1270052297188544513,14141037,Hans Pinckaers,"['New paper! It’s possible to train a ResNet with 268 megapixel images (16384x16384, 4x CIFAR-10 in one image!). \n\nTransfer learning works really well. Reaching SOTA for prostate cancer detection.\n\n🐙 Open sourced all code: <LINK>\n📝 Paper: <LINK> <LINK>']",http://arxiv.org/abs/2006.03394,"Prostate cancer is the most prevalent cancer among men in Western countries, with 1.1 million new diagnoses every year. The gold standard for the diagnosis of prostate cancer is a pathologists' evaluation of prostate tissue. To potentially assist pathologists deep-learning-based cancer detection systems have been developed. Many of the state-of-the-art models are patch-based convolutional neural networks, as the use of entire scanned slides is hampered by memory limitations on accelerator cards. Patch-based systems typically require detailed, pixel-level annotations for effective training. However, such annotations are seldom readily available, in contrast to the clinical reports of pathologists, which contain slide-level labels. As such, developing algorithms which do not require manual pixel-wise annotations, but can learn using only the clinical report would be a significant advancement for the field. In this paper, we propose to use a streaming implementation of convolutional layers, to train a modern CNN (ResNet-34) with 21 million parameters end-to-end on 4712 prostate biopsies. The method enables the use of entire biopsy images at high-resolution directly by reducing the GPU memory requirements by 2.4 TB. We show that modern CNNs, trained using our streaming approach, can extract meaningful features from high-resolution images without additional heuristics, reaching similar performance as state-of-the-art patch-based and multiple-instance learning methods. By circumventing the need for manual annotations, this approach can function as a blueprint for other tasks in histopathological diagnosis. The source code to reproduce the streaming models is available at this https URL . ","Detection of prostate cancer in whole-slide images through end-to-end
  training with image-level labels"
167,1270040298807312384,16024633,Alexis Kirke,"['Always good to have a new paper on arXiv - this one about my stock market opera ""Open Outcry""\n\n<LINK> <LINK>']",https://arxiv.org/abs/2006.03471,"This paper presents and tests a new approach to composing for ensemble singing performance: reality opera. In the performance of such a composition, emotions of the singers are real and emerge as a consequence of their interactions and reaction and to a dynamic narrative. This paper gives background and motivation for the form, based on three key concepts, incorporating the use of technology. Then proposed techniques for creating reality opera are instantiated in an example, which is performed and a behavioral analysis done of performer reactions, leading to support for the feasibility of the reality opera concept. ","Application of Optimization and Simulation to Musical Composition that
  Emerges Dynamically during Ensemble Singing Performance"
168,1269869098462138368,369569444,Takahiro TERADA (寺田 隆広),['Our new paper on No-boundary quantum cosmology &amp; Swampland conjectures: <LINK>\nWe discuss incompatibility between (the volume-weight solution to) the Hartle-Hawking no-boundary proposal &amp; the refined de SItter conjecture as well as the distance conjecture.'],https://arxiv.org/abs/2006.03443,"The Hartle-Hawking no-boundary proposal describes the quantum creation of the universe. To have a non-negligible probability to obtain a classical expanding universe, eternal inflation is required, which is severely constrained by Swampland conjectures such as the refined de Sitter conjecture and the distance conjecture. We discuss this issue in detail and demonstrate the incompatibility. We show that the dimensionless parameters in the refined de Sitter conjecture should be bounded from above by a positive power of the scalar potential to realize the classical expanding universe. In other words, the probability of the classical expanding universe is extremely small under the Swampland conjectures unless the parameters are much smaller than unity. If they are order unity, on the other hand, the saddle-point solution itself ceases to exist implying a genuinely quantum universe. ",Swampland Constraints on No-Boundary Quantum Cosmology
169,1269789783682723840,2337598033,Geraint F. Lewis,['New paper on the @arxiv - “The GALAH survey: Characterization of emission-line stars with spectral modelling using autoencoders” - I love AI/Neural Networks! - <LINK> <LINK>'],https://arxiv.org/abs/2006.03062,"We present a neural network autoencoder structure that is able to extract essential latent spectral features from observed spectra and then reconstruct a spectrum from those features. Because of the training with a set of unpeculiar spectra, the network is able to reproduce a spectrum of high signal-to-noise ratio that does not show any spectral peculiarities even if they are present in an observed spectrum. Spectra generated in this manner were used to identify various emission features among spectra acquired by multiple surveys using the HERMES spectrograph at the Anglo-Australian telescope. Emission features were identified by a direct comparison of the observed and generated spectra. Using the described comparison procedure, we discovered 10,364 candidate spectra with a varying degree of H$\alpha$/H$\beta$ emission component produced by different physical mechanisms. A fraction of those spectra belongs to the repeated observation that shows temporal variability in their emission profile. Among emission spectra, we find objects that feature contributions of a nearby rarefied gas (identified through the emission of [NII] and [SII] lines) that was identified in 4004 spectra, which were not all identified as having H$\alpha$ emission. Positions of identified emission-line objects coincide with multiple known regions that harbour young stars. Similarly, detected nebular emission spectra coincide with visually-prominent nebular clouds observable in the red all-sky photographic composites. ","The GALAH survey: Characterization of emission-line stars with spectral
  modelling using autoencoders"
170,1268906731003957248,237918251,Wei Xu,"['Our new work on automatically extracting COVID-19 events from Twitter. We annotated 7,500 tweets annotated with event QA/slot-filling information. (e.g., Who tested positive? Who is promoting a cure for coronavirus?) #nlproc\n\nPaper: <LINK> <LINK>', '@srchvrs Yes, these are simple baselines. BERT is not trained on Twitter data. So a lot of room for people to explore. Exact span match is also tricky.', '@srchvrs There are a lot of nominals and nested spans in the dataset. I would expect larger training data will improve F1 quite a bit, having better Twitter-based BERT may help some too. https://t.co/MzL7PV2ksL', '@alvations We probably can’t share the user data. We are only going to release the Tweet IDs and event annotations.', '@yanaiela @yoavgo We tried BERTweet. It doesn’t help much, but it was trained on tweets preprocessed by NLTK TweeTokenizer before BPE. We also tried https://t.co/Iw7kBdxUJo, which was trained on smaller amount of tweets.', '@srchvrs We tried this one. It doesn’t help much, but it was trained on tweets preprocessed by NLTK TweeTokenizer before BPE. We also tried https://t.co/Iw7kBdxUJo, which was trained on smaller amount of tweets. I don’t think we can rule out Twitter-specific BERT would not help just yet.', '@srchvrs The F-scores reported in the paper are on extract span matches. So it’s kind of an overly strict metric for this task.', '@yoavgo Missed ""close_contact"" for tested positive:\n\n""Jazz star Donovan Mitchell has tested positive for coronavirus, league sources tell ESPN. Jazz players privately say that &lt;E&gt; Rudy Gobert &lt;/E&gt; had been careless and borderline caviler in the locker room touching other players ...""', '@yoavgo Over-prediction of the ""employer"" for tested positive:\n\n""&lt;E&gt; Lee Health &lt;/E&gt; confirmed Friday night that a patient who tested positive for Covid-19 after arriving at Gulf Coast Medical Center later died.""', '@yoavgo Mismatched span for the ""employer"" for tested positive:\n\n""Brandon from Crown the &lt;E&gt; Empire &lt;/E&gt; tested positive for Coronavirus""\n\n(Crown the Empire is a band.)', ""@yoavgo sounds interesting. haven't tried this.""]",https://arxiv.org/abs/2006.02567,"In this paper, we present a manually annotated corpus of 10,000 tweets containing public reports of five COVID-19 events, including positive and negative tests, deaths, denied access to testing, claimed cures and preventions. We designed slot-filling questions for each event type and annotated a total of 31 fine-grained slots, such as the location of events, recent travel, and close contacts. We show that our corpus can support fine-tuning BERT-based classifiers to automatically extract publicly reported events and help track the spread of a new disease. We also demonstrate that, by aggregating events extracted from millions of tweets, we achieve surprisingly high precision when answering complex queries, such as ""Which organizations have employees that tested positive in Philadelphia?"" We will release our corpus (with user-information removed), automatic extraction models, and the corresponding knowledge base to the research community. ",Extracting a Knowledge Base of COVID-19 Events from Social Media
171,1268844447057444865,226900035,Prof B Buchanan OBE,"['Great paper from @BlockpassIDLab team: @wip_abramson @AJH4LL @pavlosatnapier @nickatnapier on crypto, ID and ML ... building a new world, and soon to be presented at a conference: <LINK> <LINK>']",https://arxiv.org/abs/2006.02456,"When training a machine learning model, it is standard procedure for the researcher to have full knowledge of both the data and model. However, this engenders a lack of trust between data owners and data scientists. Data owners are justifiably reluctant to relinquish control of private information to third parties. Privacy-preserving techniques distribute computation in order to ensure that data remains in the control of the owner while learning takes place. However, architectures distributed amongst multiple agents introduce an entirely new set of security and trust complications. These include data poisoning and model theft. This paper outlines a distributed infrastructure which is used to facilitate peer-to-peer trust between distributed agents; collaboratively performing a privacy-preserving workflow. Our outlined prototype sets industry gatekeepers and governance bodies as credential issuers. Before participating in the distributed learning workflow, malicious actors must first negotiate valid credentials. We detail a proof of concept using Hyperledger Aries, Decentralised Identifiers (DIDs) and Verifiable Credentials (VCs) to establish a distributed trust architecture during a privacy-preserving machine learning experiment. Specifically, we utilise secure and authenticated DID communication channels in order to facilitate a federated learning workflow related to mental health care data. ",A Distributed Trust Framework for Privacy-Preserving Machine Learning
172,1268837557728673794,1171357907574824961,Łukasz Tychoniec,['Super excited about our new paper accepted to A&amp;A!\nWe look into my favorite molecular cloud - Perseus - to measure the masses of youngest disks. \nResult: Early planet formation is a likely solution to the missing mass problem in protoplanetary disks. \n\n<LINK> <LINK>'],https://arxiv.org/abs/2006.02812,"In recent years evidence has been building that planet formation starts early, in the first $\sim$ 0.5 Myr. Studying the dust masses available in young disks enables understanding the origin of planetary systems since mature disks are lacking the solid material necessary to reproduce the observed exoplanetary systems, especially the massive ones. We aim to determine if disks in the embedded stage of star formation contain enough dust to explain the solid content of the most massive exoplanets. We use Atacama Large Millimeter/submillimeter Array (ALMA) Band 6 observations of embedded disks in the Perseus star-forming region together with Very Large Array (VLA) Ka-band (9 mm) data to provide a robust estimate of dust disk masses from the flux densities. Using the DIANA opacity model including large grains, with a dust opacity value of $\kappa_{\rm 9\ mm}$ = 0.28 cm$^{2}$ g$^{-1}$, the median dust masses of the embedded disks in Perseus are 158 M$_\oplus$ for Class 0 and 52 M$_\oplus$ for Class I from the VLA fluxes. The lower limits on the median masses from ALMA fluxes are 47 M$_\oplus$ and 12 M$_\oplus$ for Class 0 and Class I, respectively, obtained using the maximum dust opacity value $\kappa_{\rm 1.3mm}$ = 2.3 cm$^{2}$ g$^{-1}$. The dust masses of young Class 0 and I disks are larger by at least a factor of 10 and 3, respectively, compared with dust masses inferred for Class II disks in Lupus and other regions. The dust masses of Class 0 and I disks in Perseus derived from the VLA data are high enough to produce the observed exoplanet systems with efficiencies acceptable by planet formation models: the solid content in observed giant exoplanets can be explained if planet formation starts in Class 0 phase with an efficiency of $\sim$ 15%. Higher efficiency of $\sim$ 30% is necessary if the planet formation is set to start in Class I disks. ","Dust masses of young disks: constraining the initial solid reservoir for
  planet formation"
173,1268828532203028486,1134375290581524480,Kai Schmitz,"['New paper on the arXiv: ""Leptogenesis and low-energy CP violation in a type-II-dominated left-right seesaw model"" in collaboration with Thomas Rink and Werner Rodejohann. 🎉 <LINK> <LINK>']",https://arxiv.org/abs/2006.03021,"We consider leptogenesis in a left-right-symmetric seesaw scenario in which neutrino mass generation and leptogenesis are dominated by the type-II seesaw term. Motivated by grand unification, we assume that the neutrino Dirac mass matrix is dominated by a single entry of the order of the top-quark mass, which leaves the low-energy phases of the lepton mixing matrix as the only sources of CP violation. Working in a regime where the triplet scalar predominantly decays into leptons, this results in a predictive scenario based on a minimal number of parameters. We perform a detailed analysis of the flavored Boltzmann equations within a revised density matrix framework and demonstrate that the observed baryon asymmetry can be successfully generated in this simple model. We point out that the significance of flavor effects is limited, and we discuss the implications for low-energy observables such as the Dirac CP phase and neutrinoless double beta decay. ","Leptogenesis and low-energy CP violation in a type-II-dominated
  left-right seesaw model"
174,1268827924767158274,358306755,Jani Kastikainen,"['New paper out! It has two parts; in the first part I look at Lovelock scalars which are curvature invariants that appear in Lovelock gravity. \n\n<LINK>\n\n(1/n)', 'There is a formula that tells you how conical singularities give delta function contributions to them and I generalize that to higher dimensional cones. There are two ways to do this and they neatly give the same result.\n\n(2/n)', 'In the second part, I try to apply the formula in a topological theory of gravity in higher dimensions. This theory is a particular Lovelock gravity theory living in AdS space and it can be formulated as a Chern-Simons theory just like Einstein gravity in 3D.\n\n(3/n)', 'Turns out that its Euclidean action behaves in a very simple way in the presence of defects which is crucial for concistency from a holographic point of view.\n\n(4/n)', 'A lot of moments where things magically fell into place happened during this project and those made it all worth it in the end.\n\n(5/n) n=5', '@henriksson_o Thanks Oscar!', '@arjvuori Thanks alot Aleksi!']",https://arxiv.org/abs/2006.02803,"We study codimension-even conical defects that contain a deficit solid angle around each point along the defect. We show that they lead to a delta function contribution to the Lovelock scalar and we compute the contribution by two methods. We then show that these codimension-even defects appear as Euclidean brane solutions in higher dimensional topological AdS gravity which is Lovelock-Chern-Simons gravity without torsion. The theory possesses a holographic Weyl anomaly that is purely of type-A and proportional to the Lovelock scalar. Using the formula for the defect contribution, we prove a holographic duality between codimension-even defect partition functions and codimension-even brane on-shell actions in Euclidean signature. More specifically, we find that the logarithmic divergences match, because the Lovelock-Chern-Simons action localizes on the brane exactly. We demonstrate the duality explicitly for a spherical defect on the boundary which extends as a codimension-even hyperbolic brane into the bulk. For vanishing brane tension, the geometry is a foliation of Euclidean AdS space that provides a one-parameter generalization of AdS-Rindler space. ",Conical defects and holography in topological AdS gravity
175,1268797086205710336,776765039726460929,Carlo Felice Manara,"['New paper by Łukasz Tychoniec: \n<LINK>\nWe show that VLA-based dust masses of Class 0 and I disks in Perseus are high enough to produce the observed\nexoplanet systems with efficiencies acceptable by planet formation models. Early planet formation is the solution? <LINK>', '@TomHaworthAstro I would say no. IMHO Stellar masses of Class 0 stars are still quite uncertain...', '@LukaszT9 @LukaszT9 I did not know you were on Twitter!!!', ""@LukaszT9 Once you start as @ESO Fellow we'll sort that out!"", '@GijsMulders @ilaria_pascucci @megaparsec808 For the class II we report just the median masses, as in the other classes for which we have no stellar mass info @LukaszT9', '@GijsMulders There is no stellar mass information for the class 0 and I. For the class II you know we discussed this in Manara et al 2018', '@LukaszT9 @TomHaworthAstro @r_d_alexander This @GijsMulders', '@GijsMulders @LukaszT9 Graphics. It puts the results in context. If you look the other plots instead there are just cumulative distributions. This gives the idea, and I agree it can be done with the stellar masses for the class IIs...']",https://arxiv.org/abs/2006.02812,"In recent years evidence has been building that planet formation starts early, in the first $\sim$ 0.5 Myr. Studying the dust masses available in young disks enables understanding the origin of planetary systems since mature disks are lacking the solid material necessary to reproduce the observed exoplanetary systems, especially the massive ones. We aim to determine if disks in the embedded stage of star formation contain enough dust to explain the solid content of the most massive exoplanets. We use Atacama Large Millimeter/submillimeter Array (ALMA) Band 6 observations of embedded disks in the Perseus star-forming region together with Very Large Array (VLA) Ka-band (9 mm) data to provide a robust estimate of dust disk masses from the flux densities. Using the DIANA opacity model including large grains, with a dust opacity value of $\kappa_{\rm 9\ mm}$ = 0.28 cm$^{2}$ g$^{-1}$, the median dust masses of the embedded disks in Perseus are 158 M$_\oplus$ for Class 0 and 52 M$_\oplus$ for Class I from the VLA fluxes. The lower limits on the median masses from ALMA fluxes are 47 M$_\oplus$ and 12 M$_\oplus$ for Class 0 and Class I, respectively, obtained using the maximum dust opacity value $\kappa_{\rm 1.3mm}$ = 2.3 cm$^{2}$ g$^{-1}$. The dust masses of young Class 0 and I disks are larger by at least a factor of 10 and 3, respectively, compared with dust masses inferred for Class II disks in Lupus and other regions. The dust masses of Class 0 and I disks in Perseus derived from the VLA data are high enough to produce the observed exoplanet systems with efficiencies acceptable by planet formation models: the solid content in observed giant exoplanets can be explained if planet formation starts in Class 0 phase with an efficiency of $\sim$ 15%. Higher efficiency of $\sim$ 30% is necessary if the planet formation is set to start in Class I disks. ","Dust masses of young disks: constraining the initial solid reservoir for
  planet formation"
176,1268742584202977280,2705638878,Dr. Adelle Goodwin,"['New paper on ArXiv today! A huge team effort involving 7+ telescopes in which we observed an accreting neutron star coming into outburst, and measured a surprisingly long optical to X-ray delay.\n\n@DuncanKGalloway \n<LINK>', 'This research has gained quite a bit of media attention so you can read more here:\n\nhttps://t.co/lkpjBEbIzh\n\nhttps://t.co/Ng62aDTJil\n\nhttps://t.co/XKmm6x9Jpv\n\nhttps://t.co/a9EMdxlqQG\n\nhttps://t.co/VOiFr40LsC\n\nhttps://t.co/1GBukzVqn6', '@lern_b Thanks gorgeous! 😁😁']",https://arxiv.org/abs/2006.02872,"X-ray transients, such as accreting neutron stars, periodically undergo outbursts, thought to be caused by a thermal-viscous instability in the accretion disk. Usually outbursts of accreting neutron stars are identified when the accretion disk has undergone an instability, and the persistent X-ray flux has risen to a threshold detectable by all sky monitors on X-ray space observatories. Here we present the earliest known combined optical, UV, and X-ray monitoring observations of the outburst onset of an accreting neutron star low mass X-ray binary system. We observed a significant, continuing increase in the optical i'-band magnitude starting on July 25, 12 days before the first X-ray detection with Swift/XRT and NICER (August 6), during the onset of the 2019 outburst of SAX J1808.4-3658. We also observed a 4 day optical to X-ray rise delay, and a 2 day UV to X-ray delay, at the onset of the outburst. We present the multiwavelength observations that were obtained, discussing the theory of outbursts in X-ray transients, including the disk instability model, and the implications of the delay. This work is an important confirmation of the delay in optical to X-ray emission during the onset of outbursts in low mass X-ray binaries, which has only previously been measured with less sensitive all sky monitors. We find observational evidence that the outburst is triggered by ionisation of hydrogen in the disk. ","Enhanced optical activity 12 days before X-ray activity, and a 4 day
  X-ray delay during outburst rise, in a low-mass X-ray binary"
177,1268506034823467011,156804540,Francisco Rodrigues,"['Our new paper on @Arxiv is out:\nDisease and information spreading at different speeds in multiplex networks. (arXiv:2006.01965v1 [physics.soc-ph]) <LINK>\nwith @cosnet_bifi @paulocv92 \n#Complexity #Networks <LINK>', '@manlius84 @arxiv @cosnet_bifi @paulocv92 @SciReports Thanks, Manlio, I will take a look at it. Indeed, there are many papers on this topic and it should be good to have a survey about this in the near future.']",http://arxiv.org/abs/2006.01965,"Nowadays, one of the challenges we face when carrying out modeling of epidemic spreading is to develop methods to control disease transmission. In this article we study how the spreading of knowledge of a disease affects the propagation of that disease in a population of interacting individuals. For that, we analyze the interaction between two different processes on multiplex networks: the propagation of an epidemic using the susceptible-infected-susceptible dynamics and the dissemination of information about the disease --and its prevention methods-- using the unaware-aware-unaware dynamics, so that informed individuals are less likely to be infected. Unlike previous related models where disease and information spread at the same time scale, we introduce here a parameter that controls the relative speed between the propagation of the two processes. We study the behavior of this model using a mean-field approach that gives results in good agreement with Monte Carlo simulations on homogeneous complex networks. We find that increasing the rate of information dissemination reduces the disease prevalence, as one may expect. However, increasing the speed of the information process as compared to that of the epidemic process has the counter intuitive effect of increasing the disease prevalence. This result opens an interesting discussion about the effects of information spreading on disease propagation. ","Disease and information spreading at different speeds in multiplex
  networks"
178,1268351828283936768,489909633,Prof. Melanie J-H,"['Examples of artefact reduction in ASKAP data using the new DDF imaging pipeline over ASKAPSoft. Details in new paper by Amanda Wilber, submitted to PASA: <LINK> <LINK>']",http://arxiv.org/abs/2006.01833,"Early science observations from the Australian Square Kilometre Array Pathfinder (ASKAP) have revealed clear signals of diffuse radio emission associated with two clusters detected by the South Pole Telescope via their Sunyaev Zel'dovich signal. SPT CLJ0553-3342 (MACSJ0553.4-3342) and SPT CLJ0638-5358 (Abell S0592) are both high-mass lensing clusters that have undergone major mergers. To improve the data products of these ASKAP early science observations and create science-fidelity images of the galaxy clusters, we performed direction-dependent (DD) calibration and imaging using state-of-the-art software {\sc killMS} and {\sc DDFacet}. We find that artefacts in the ASKAP images are greatly reduced after directional calibration. Here we present our DD calibrated ASKAP radio images of both clusters showing unambiguous giant radio halos with largest linear scales of $\sim1$~Mpc. The halo in MACSJ0553.4-3342 was previously detected with GMRT observations at 323 MHz, but appears more extended in our ASKAP image. Although there is a shock detected in the thermal X-ray emission of this cluster, we find that the particle number density in the shocked region is too low to allow for the generation of a radio shock. The radio halo in Abell S0592 is a new discovery, and the Southwest border of the halo coincides with a shock detected in X-rays. We discuss the origins of these halos considering both the hadronic and turbulent re-acceleration models as well as sources of \textit{seed} electrons. This work gives a positive indication of the potential of ASKAP's Evolutionary Map of the Universe (EMU) survey in detecting intracluster medium radio sources, and showcases the improvement in data products after utilising third-generation calibration techniques. ","ASKAP reveals giant radio halos in two merging SPT galaxy clusters --
  Making the case for a direction-dependent pipeline --"
179,1268281665513717763,1181013626418798592,Leo Duan,['What happens when projecting a continuous Gaussian onto an L1-ball (blue)? We have the Bayesian Lasso 2.0 --- check out our new paper: L1-ball prior\n\n<LINK> <LINK>'],https://arxiv.org/abs/2006.01340,"The l1-regularization is very popular in high dimensional statistics -- it changes a combinatorial problem of choosing which subset of the parameter are zero, into a simple continuous optimization. Using a continuous prior concentrated near zero, the Bayesian counterparts are successful in quantifying the uncertainty in the variable selection problems; nevertheless, the lack of exact zeros makes it difficult for broader problems such as the change-point detection and rank selection. Inspired by the duality of the l1-regularization as a constraint onto an l1-ball, we propose a new prior by projecting a continuous distribution onto the l1-ball. This creates a positive probability on the ball boundary, which contains both continuous elements and exact zeros. Unlike the spike-and-slab prior, this l1-ball projection is continuous and differentiable almost surely, making the posterior estimation amenable to the Hamiltonian Monte Carlo algorithm. We examine the properties, such as the volume change due to the projection, the connection to the combinatorial prior, the minimax concentration rate in the linear problem. We demonstrate the usefulness of exact zeros that simplify the combinatorial problems, such as the change-point detection in time series, the dimension selection of mixture model and the low-rank-plus-sparse change detection in the medical images. ","Bayesian Inference with the l1-ball Prior: Solving Combinatorial
  Problems with Exact Zeros"
180,1268103502892498945,12745042,RJB Goudie,"['New on arXiv: Yang Liu @MRC_BSU’s paper with me on inference for modularised Bayes models when ""cutting"" feedback from a module, due to possible misspecification\n\nProposes a new algorithm for the cut distribution: Stochastic Approximation Cut Algorithm\n\n<LINK> <LINK>']",https://arxiv.org/abs/2006.01584,"Bayesian modelling enables us to accommodate complex forms of data and make a comprehensive inference, but the effect of partial misspecification of the model is a concern. One approach in this setting is to modularize the model, and prevent feedback from suspect modules, using a cut model. After observing data, this leads to the cut distribution which normally does not have a closed-form. Previous studies have proposed algorithms to sample from this distribution, but these algorithms have unclear theoretical convergence properties. To address this, we propose a new algorithm called the Stochastic Approximation Cut algorithm (SACut) as an alternative. The algorithm is divided into two parallel chains. The main chain targets an approximation to the cut distribution; the auxiliary chain is used to form an adaptive proposal distribution for the main chain. We prove convergence of the samples drawn by the proposed algorithm and present the exact limit. Although SACut is biased, since the main chain does not target the exact cut distribution, we prove this bias can be reduced geometrically by increasing a user-chosen tuning parameter. In addition, parallel computing can be easily adopted for SACut, which greatly reduces computation time. ","Stochastic Approximation Cut Algorithm for Inference in Modularized
  Bayesian Models"
181,1267850697095163904,618128128,Shuiwang Ji,['New paper alert: iCapsNets: Towards Interpretable Capsule Networks for Text Classification\n<LINK>'],https://arxiv.org/abs/2006.00075,"Many text classification applications require models with satisfying performance as well as good interpretability. Traditional machine learning methods are easy to interpret but have low accuracies. The development of deep learning models boosts the performance significantly. However, deep learning models are typically hard to interpret. In this work, we propose interpretable capsule networks (iCapsNets) to bridge this gap. iCapsNets use capsules to model semantic meanings and explore novel methods to increase interpretability. The design of iCapsNets is consistent with human intuition and enables it to produce human-understandable interpretation results. Notably, iCapsNets can be interpreted both locally and globally. In terms of local interpretability, iCapsNets offer a simple yet effective method to explain the predictions for each data sample. On the other hand, iCapsNets explore a novel way to explain the model's general behavior, achieving global interpretability. Experimental studies show that our iCapsNets yield meaningful local and global interpretation results, without suffering from significant performance loss compared to non-interpretable methods. ","iCapsNets: Towards Interpretable Capsule Networks for Text
  Classification"
182,1267718301158912002,1101220947607146497,Alon Jacovi,"['We uploaded a new paper! Please check it out: <LINK>\n\nAligning Faithful Interpretations with their Social Attribution\nwith @yoavgo\n\nWhat does it mean for an interpretation to be ""faithful"", or for a faithful interpretation to ""explain decisions"" to people? <LINK>', '@yoavgo We take ""highlight interpretations"" or ""extractive rationales"" as a case study and find that faithfulness is too vague, and incomplete in making people understand model decisions. We clarify what\'s missing for them to be comprehensible explanations, and how to do it', '@yoavgo We find this formalization pretty effective, and with tons of precedence in social and cognitive literature on how humans give and receive explanations of human behavior/decisions']",https://arxiv.org/abs/2006.01067,"We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case-study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We re-formulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations. ",Aligning Faithful Interpretations with their Social Attribution
183,1278943982601465856,2254415004,Xingjun (Daniel) Ma,"['Our #ICML2020 paper ""Normalized loss functions for deep learning with noisy labels"" is trending on <LINK>! Key insight: we show ANY loss function can be made robust by a simple normalization! Motivating new #losses for #deeplearning! <LINK>']",https://arxiv.org/abs/2006.13554,"Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels. ",Normalized Loss Functions for Deep Learning with Noisy Labels
184,1278688135459885057,1072220386237009921,Guillaume Lajoie,"['New paper I had the chance to contribute to (preprint ahead of ICML20). \n\nExploring modularity and signal directionality in neural nets using attentive, context-driven dynamic connections is a promising and exciting direction.\n\n<LINK>\n@MILAMontreal @anirudhg9119']",https://arxiv.org/abs/2006.16981,"Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and short-term memory, such as how the phrase `peanut butter and~...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines. ","Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural
  Networks with Attention over Modules"
185,1277994178081746946,68538286,Dan Hendrycks,"['What methods actually improve robustness? In this paper, we test robustness to changes in geography, time, occlusion, rendition, real image blurs, and so on with 4 new datasets.\nNo published method consistently improves robustness.\n\n<LINK>\n<LINK> <LINK>', '@SebastianCygert @TheNormanMu @sokadv @rahuldesai44 @tyleryzhu @parajuli_samyak @JacobSteinhardt @jmgilmer I imagine that they could combine, but since those very large datasets aren’t public, I’ll continue trying to improve performance with ImageNet’s ~1000 labeled images per class.']",https://arxiv.org/abs/2006.16241,"We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness. ","The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
  Generalization"
186,1277983080259690496,1277462784805154816,Siddharth Garg 🌈,"['1/Excited about CryptoNAS, new paper w/ Zahra Ghodsi, Akshaj Veladanda &amp; Brandon Reagen. CryptoNAS re-thinks deep learning for ""private inference"" enabling mobile users to use a model hosted in the cloud for inference while keeping their data secret. <LINK>', ""2/ It turns out that the cost of private inference depends largely on the number of non-linear ReLU functions in the deep network, while floating point ops like mults and adds are `free.' CryptoNAS designs the most accurate deep nets while using the fewest possible ReLUs""]",https://arxiv.org/abs/2006.08733,"Machine learning as a service has given raise to privacy concerns surrounding clients' data and providers' models and has catalyzed research in private inference (PI): methods to process inferences without disclosing inputs. Recently, researchers have adapted cryptographic techniques to show PI is possible, however all solutions increase inference latency beyond practical limits. This paper makes the observation that existing models are ill-suited for PI and proposes a novel NAS method, named CryptoNAS, for finding and tailoring models to the needs of PI. The key insight is that in PI operator latency cost are non-linear operations (e.g., ReLU) dominate latency, while linear layers become effectively free. We develop the idea of a ReLU budget as a proxy for inference latency and use CryptoNAS to build models that maximize accuracy within a given budget. CryptoNAS improves accuracy by 3.4% and latency by 2.4x over the state-of-the-art. ",CryptoNAS: Private Inference on a ReLU Budget
187,1276521191495340033,1965093145,Charles Tapley Hoyt,['Today is documentation Friday. @keenuniverse is getting lots of improvements based on our new paper (<LINK>)\n\nWhat was the last time you spent a whole day writing documentation like? (papers don’t count :p)'],https://arxiv.org/abs/2006.13365,"The heterogeneity in recently published knowledge graph embedding models' implementations, training, and evaluation has made fair and thorough comparisons difficult. In order to assess the reproducibility of previously published results, we re-implemented and evaluated 21 interaction models in the PyKEEN software package. Here, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a model's performances, and not only determined by the model architecture. We provide evidence that several architectures can obtain results competitive to the state-of-the-art when configured carefully. We have made all code, experimental configurations, results, and analyses that lead to our interpretations available at this https URL and this https URL ","Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge
  Graph Embedding Models Under a Unified Framework"
188,1276199969448493057,970000369072959488,Teresita Suarez,"['I released a new paper on the arxiv! We model absorption line systems as pressure confined clouds, confined by the expected pressure of massive galactic haloes in the early Universe (z &gt;5). Inferred chemical abundances may arise in starburst galaxies.\n<LINK> <LINK>']",https://arxiv.org/abs/2006.13088,"We interpret observations of intergalactic low ionisation metal absorption systems at redshifts z $\gtrsim$5 in terms of pressure-confined clouds. We find clouds confined by the expected pressure of galactic haloes with masses $11<\log M_h/h^{-1}M_\odot<12$ provide a good description of the column density ratios between low ionisation metal absorbers. Some of the ratios, however, require extending conventional radiative transfer models of irradiated slabs to spherical (or cylindrical) clouds to allow for lines of sight passing outside the cores of the clouds. Moderate depletion of silicon onto dust grains is also indicated in some systems. The chemical abundances inferred span the range between solar and massive-star dominated stellar populations as may arise in starburst galaxies. The typical HI column densities matching the data correspond to Damped Lyman-$\alpha$ Absorbers (DLAs) or sub-DLAs, with sizes of 40 pc to 3 kpc, gas masses $3.5<\log M_c/M_\odot<8$ and metallicites $0.001-0.01Z_\odot$. Such systems continue to pose a challenge for galaxy-scale numerical simulations to reproduce. ","Modelling intergalactic low ionisation metal absorption line systems
  near the epoch of reionization"
189,1275959421399597062,800790709599014912,Francis Williams,['Our new paper: Kernels arising from wide nets are a  powerful representation for encoding 3D surfaces. They producing qualitatively and quantitatively superior results to both traditional methods like Poisson Reconstruction and neural-net based methods. <LINK> <LINK>'],https://arxiv.org/abs/2006.13782,"We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants. ",Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks
190,1275793105862696965,1387376042,Roberta Raileanu,"['Excited to share our new paper “Automatic Data Augmentation for Generalization in Deep Reinforcement Learning” w/ @maxagoldstein8, @denisyarats, @ikostrikov, and @rob_fergus!\n\nPaper: <LINK>\nCode: <LINK>\nWebsite: <LINK> <LINK>', 'Do you want to use data augmentation in RL but you’re not sure which augmentation will work best for your environment? \n\nWe propose UCB-DrAC, a method that uses UCB to automatically select an effective augmentation for training an RL agent. https://t.co/AiyKGzxl3r', 'To make the use of data augmentation principled for actor-critic algorithms, we need to introduce two regularization terms for the policy and value function.', 'Without regularizing the policy and value function, the agent sometimes fails to learn or converges to suboptimal policies. https://t.co/4f6jTCmjXq', 'UCB learns to select the most effective augmentation, which significantly improves the agent’s performance on both train and test environments. https://t.co/5blVUSmphE', 'Our method aids generalization to unseen environments - it improves test performance on the Procgen benchmark by ~40% over standard RL and significantly outperforms other baselines. https://t.co/yH9SOH4na3', 'Our agent learns policies and representations that are more robust to changes in the environment that do not affect the agent, such as the background. https://t.co/7tiFTAT550', 'This work has been largely inspired by DrQ (https://t.co/VrnowTJgQJ) from @ikostrikov, @denisyarats, and @rob_fergus.']",https://arxiv.org/abs/2006.12862,"Deep reinforcement learning (RL) agents often fail to generalize to unseen scenarios, even when they are trained on many instances of semantically similar environments. Data augmentation has recently been shown to improve the sample efficiency and generalization of RL agents. However, different tasks tend to benefit from different kinds of data augmentation. In this paper, we compare three approaches for automatically finding an appropriate augmentation. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for certain actor-critic algorithms. We evaluate our methods on the Procgen benchmark which consists of 16 procedurally-generated environments and show that it improves test performance by ~40% relative to standard RL algorithms. Our agent outperforms other baselines specifically designed to improve generalization in RL. In addition, we show that our agent learns policies and representations that are more robust to changes in the environment that do not affect the agent, such as the background. Our implementation is available at this https URL ","Automatic Data Augmentation for Generalization in Deep Reinforcement
  Learning"
191,1275370321353363456,969284009300713477,Jens Hoeijmakers,"['We have written a new paper discussing atoms and molecules the high-resolution transmission spectrum of WASP-121, an ultra-hot Jupiter that shows strong signs of absorption by vaporised metals in its atmosphere (<LINK>) 1/N <LINK>', 'The transmission spectrum (observed with HARPS) is rich! We detect 7 species at various strengths, and also provide stringent upper limits on some molecules and ions. This allows us to draw a wide range of conclusions about the chemistry taking place in the atmosphere 2/N.', 'Notably, we detect vanadium, but not Ti or TiO, suggesting that the temperature is low enough for Ti-bearing molecules to condense out of the gas-phase and be removed from the spectrum; while V remains in the gas phase, along with V-bearing molecules such as VO. 3/N', ""This graph shows the chemistry when assuming chemical equilibrium: Ti-bearing species dominate over V-bearing species, though these have apparently rained out. But our observation of V implies that VO and VO2 have remained in the gas phase. If not, V wouldn't be there either! 4/N https://t.co/iPzxkgU096"", 'This graph has so many interesting features! First of all, it appears that at 2,000 K &amp; 20x solar metallicity, Ti and V have the same abundance, even though Ti is 10x more abundant than V in nature. This makes V a richer optical absorber because it has an extra electron. 5/N', ""Secondly: Many people talk about TiO and VO as being important optical absorbers in hot Jupiters. But what are VO2 and TiO2? And TiS/VS?? I couldn't find exoplanet literature that discusses these; but in some stars they appear to be important absorbers, at least in the NIR 6/N."", 'Big shout-out to Daniel Kitzmann here for curating daunting amounts of chemical data that made these chemistry calculations possible. If you want to simulate these yourself, check out FastChem here! https://t.co/su8yCt2z8m 7/N.', 'We also detect neutral nickel and calcium, which could be firsts in the exoplanet literature (but please let me know if I missed your discovery and failed to cite you -sorry) 8/N. https://t.co/i9Sd9CCXK9', ""Then sodium, pulled out by @JuliaVSeidel: Like many hot Jupiters, WASP-121 b boasts a deep sodium doublet. Apparently, our sodium lines don't have equal depths. We aren't 100% sure that this is real, but if it is; it could mean that Na forms an optically thin envelope ... 9/N"", '... instead of being part of the atmosphere itself. This could be a phenomenon related to atmospheric escape, or complex plasma interactions forming a torus-like envelope of the kind that Jupiter also has. Loads more observations are needed to confirm or refute this, though 10/N. https://t.co/vYcBEA70Cl', 'Side note: Optically thin clouds are really cool. We were able to revisit some classical ISM-equations and adopt them for transiting exoplanets, allowing us to analytically back out the absorbing mass of Na atoms. 11/N https://t.co/IXLHe6sVUq', ""Final important point: (at least here. There's more, so do read the paper!) Atomic metal lines are strong, much stronger than predicted. We don't know exactly why, but without this effect our observations wouldn't have been sensitive enough to reveal them. 12/N"", 'All in all, the wealth of spectral lines that this planet has handed to us gives us new theoretical challenges to worry about, and new opportunities for discovery. Thanks to my wonderful collaborators for making this work a reality!']",https://arxiv.org/abs/2006.11308,"Aims: We survey the transmission spectrum of WASP-121 b for line-absorption by metals and molecules at high spectral resolution, and elaborate on existing interpretations of the optical transmission spectrum observed with HST/STIS and WFC3. Methods: We use the cross-correlation technique and direct differential spectroscopy to search for sodium and other neutral and ionised atoms, TiO, VO and SH in high-resolution transit spectra obtained with the HARPS spectrograph. We inject models assuming chemical and hydrostatic equilibrium with varying temperature and composition to enable model comparison, and employ two bootstrap methods to test the robustness of our detections. Results: We detect neutral Mg, Na, Ca, Cr, Fe, Ni and V, which we predict exists in equilibrium with a significant quantity of VO, supporting earlier observations by HST/WFC3. Non-detections of Ti and TiO support the hypothesis that Ti is depleted via a cold-trap mechanism as has been proposed in the literature. Atomic line depths are under-predicted by hydrostatic models by a factor of 1.5 to 8, confirming recent findings that the atmosphere is extended. We predict the existence of significant concentrations of gas-phase TiO$_2$, VO$_2$ and TiS, which could be important absorbers at optical and NIR wavelengths in hot Jupiter atmospheres, but for which accurate line-list data is currently not available. We find no evidence for absorption by SH, and find that inflated atomic lines can plausibly explain the slope of the transmission spectrum observed in the NUV with HST/STIS. The Na D lines are significantly broadened and show a difference in their respective depths of 15 scale heights, which is not expected from isothermal hydrostatic theory. ","Hot Exoplanet Atmospheres Resolved with Transit Spectroscopy (HEARTS)
  IV. A spectral inventory of atoms and molecules in the high-resolution
  transmission spectrum of WASP-121 b"
192,1273858121467920385,802543221943439360,Andrea Caputo,"['Do axions exist?? If yes, they may shed light on the solar magnetic field in the future! Take a look at my new paper\n<LINK> <LINK>', '#axion #darkmatter']",https://arxiv.org/abs/2006.10415,"Axion helioscopes search for solar axions and axion-like particles via inverse Primakoff conversion in strong laboratory magnets pointed at the Sun. Anticipating the detection of solar axions, we determine the potential for the planned next-generation helioscope, the International Axion Observatory (IAXO), to measure or constrain the solar magnetic field. To do this we consider a previously neglected component of the solar axion flux at sub-keV energies arising from the conversion of longitudinal plasmons. This flux is sensitively dependent to the magnetic field profile of the Sun, with lower energies corresponding to axions converting into photons at larger solar radii. If the detector technology eventually installed in IAXO has an energy resolution better than 200 eV, then solar axions could become an even more powerful messenger than neutrinos of the magnetic field in the core of the Sun. For energy resolutions better than 10 eV, IAXO could access the inner 70% of the Sun and begin to constrain the field at the tachocline: the boundary between the radiative and convective zones. The longitudinal plasmon flux from a toroidal magnetic field also has an additional 2% geometric modulation effect which could be used to measure the angular dependence of the magnetic field. ",Axion helioscopes as solar magnetometers
193,1273760494092984323,993493373171523585,Stefan Lattner,['Somewhat surprising finding in our new paper (accepted for EUSIPCO 2020): Using complex components directly works well for audio generation with GANs (we present a comparison of different representations).\n\nPaper:\n<LINK>\n\nSound examples:\n<LINK>'],https://arxiv.org/abs/2006.09266,"In this paper, we compare different audio signal representations, including the raw audio waveform and a variety of time-frequency representations, for the task of audio synthesis with Generative Adversarial Networks (GANs). We conduct the experiments on a subset of the NSynth dataset. The architecture follows the benchmark Progressive Growing Wasserstein GAN. We perform experiments both in a fully non-conditional manner as well as conditioning the network on the pitch information. We quantitatively evaluate the generated material utilizing standard metrics for assessing generative models, and compare training and sampling times. We show that complex-valued as well as the magnitude and Instantaneous Frequency of the Short-Time Fourier Transform achieve the best results, and yield fast generation and inversion times. The code for feature extraction, training and evaluating the model is available online. ","Comparing Representations for Audio Synthesis Using Generative
  Adversarial Networks"
194,1273268105960185856,1107704216238415873,ShabaniLab,['Our new paper on building voltage-tunable resonators for random access quantum memory: <LINK>'],https://arxiv.org/abs/2006.08683,"In quantum computing architectures, one important factor is the trade-off between the need to couple qubits to each other and to an external drive and the need to isolate them well enough in order to protect the information for an extended period of time. In the case of superconducting circuits, one approach is to utilize fixed frequency qubits coupled to coplanar waveguide resonators such that the system can be kept in a configuration that is relatively insensitive to noise. Here, we propose a scalable voltage-tunable quantum memory (QuMem) design concept compatible with superconducting qubit platforms. Our design builds on the recent progress in fabrication of Josephson field effect transistors (JJ-FETs) which use InAs quantum wells. The JJ-FET is incorporated into a tunable coupler between a transmission line and a high-quality resonator in order to control the overall inductance of the coupler. A full isolation of the high-quality resonator can be achieved by turning off the JJ-FET. This could allow for long coherence times and protection of the quantum information inside the storage cavity. The proposed design would facilitate the implementation of random access memory for storage of quantum information in between computational gate operations. ","Voltage-tunable superconducting resonators: a platform for random access
  quantum memory"
195,1272702484096532481,1071113100739387392,Guanya Shi,"['How valuable are predictions in online control? How many predictions are needed to achieve performance with O(1) dynamic regret? How well does MPC perform? \nWe answer these in our new paper <LINK> Joint with Chenkai Yu, @yisongyue, Soon-Jo Chung and Adam Wierman. <LINK>', 'We focus on online LQR control with stochastic and adversarial disturbances in dynamics. We characterize the cost-optimal and dynamic regret minimizing policies with k predictions of future disturbance, and show the marginal benefit of an extra prediction exponentially decays.', 'We show that the greedy MPC is near-optimal - it only needs O(logT) predictions to reach O(1) dynamic regret (the same order as the required prediction horizon for O(1) regret). The power of predictions reduces the need for algorithmic sophistication due to the structure of LQR.']",https://arxiv.org/abs/2006.07569,"We study the impact of predictions in online Linear Quadratic Regulator control with both stochastic and adversarial disturbances in the dynamics. In both settings, we characterize the optimal policy and derive tight bounds on the minimum cost and dynamic regret. Perhaps surprisingly, our analysis shows that the conventional greedy MPC approach is a near-optimal policy in both stochastic and adversarial settings. Specifically, for length-$T$ problems, MPC requires only $O(\log T)$ predictions to reach $O(1)$ dynamic regret, which matches (up to lower-order terms) our lower bound on the required prediction horizon for constant regret. ",The Power of Predictions in Online Control
196,1272574484336324609,48712353,Sungjin Ahn 🇺🇦,"['Humans can build 3D models of individual objects from partial observations of a complex scene. Check out our new paper about ROOTS for unsupervised representation and rendering of modular, compositional, and 3D objects and scenes: <LINK>  \n\nMore videos follow: <LINK>', 'with Chang Chen (@ShenC) &amp; Fei Deng', 'Model Overview https://t.co/fRTfPaNwUd', 'Object-wise Disentanglement https://t.co/fWpsXac9VX', 'Compositional Rendering https://t.co/3WpkI14OMU']",https://arxiv.org/abs/2006.06130,"A crucial ability of human intelligence is to build up models of individual 3D objects from partial scene observations. Recent works achieve object-centric generation but without the ability to infer the representation, or achieve 3D scene representation learning but without object-centric compositionality. Therefore, learning to represent and render 3D scenes with object-centric compositionality remains elusive. In this paper, we propose a probabilistic generative model for learning to build modular and compositional 3D object models from partial observations of a multi-object scene. The proposed model can (i) infer the 3D object representations by learning to search and group object areas and also (ii) render from an arbitrary viewpoint not only individual objects but also the full scene by compositing the objects. The entire learning process is unsupervised and end-to-end. In experiments, in addition to generation quality, we also demonstrate that the learned representation permits object-wise manipulation and novel scene generation, and generalizes to various settings. Results can be found on our project website: this https URL ",ROOTS: Object-Centric Representation and Rendering of 3D Scenes
197,1271156830803234820,312448486,Dr. Karan Jani,"[""In a new paper led by @GTSciences researcher Deborah Ferguson, we find that the most accurate solutions to Einstein's Equations (on fastest supercomputers!) are just not good enough for the next-gen gravitational-wave detectors.\n\n<LINK>\n\nSubmitted to @PhysRevLett <LINK>""]",https://arxiv.org/abs/2006.04272,"Future detectors such as LISA promise signal-to-noise ratios potentially in the thousands and data containing simultaneous signals. Accurate numerical relativity waveforms will be essential to maximize the science return. A question of interest to the broad gravitational wave community is: Are the numerical relativity codes ready to face this challenge? Towards answering this question, we provide a new criteria to identify the minimum resolution a simulation must have as a function of signal-to-noise ratio in order for the numerical relativity waveform to be indistinguishable from a true signal. This criteria can be applied to any finite-differencing numerical relativity code with multiple simulations of differing resolutions for the desired binary parameters and waveform length. We apply this criteria to binary systems of interest with the fourth-order MAYA code to obtain the first estimate of the minimum resolution a simulation must have to be prepared for next generation detectors. ","Assessing the Readiness of Numerical Relativity for LISA and 3G
  Detectors"
198,1271064318768005121,61434104,Scott H. Hawley,"['Super proud of my **undergrad** student Billy Mitchell, who has a new ML-audio paper out: “Exploring Quality and Generalizability for Parameterized Neural Audio Effects.“ Paper: <LINK>. Audio examples (best viewed on desktop): <LINK>. <LINK>', 'Summary: Most musical audio processed by ML is still noisy and/or low-fi (despite recent papers claiming “hi-fi” but only 16 khz!) Our goal was to try to improve output audio quality by various strategies...none of which ended up helping much. “A null result is still a result.”', 'He released his own dataset too: https://t.co/LOBiGrYaNr.  This young man is going places!  (Code is still https://t.co/BW51g7hTtN)', '(BTW, paper was submitted Tuesday, b/c Wednesday was #BlackOutSTEM day)']",https://arxiv.org/abs/2006.05584,"Deep neural networks have shown promise for music audio signal processing applications, often surpassing prior approaches, particularly as end-to-end models in the waveform domain. Yet results to date have tended to be constrained by low sample rates, noise, narrow domains of signal types, and/or lack of parameterized controls (i.e. ""knobs""), making their suitability for professional audio engineering workflows still lacking. This work expands on prior research published on modeling nonlinear time-dependent signal processing effects associated with music production by means of a deep neural network, one which includes the ability to emulate the parameterized settings you would see on an analog piece of equipment, with the goal of eventually producing commercially viable, high quality audio, i.e. 44.1 kHz sampling rate at 16-bit resolution. The results in this paper highlight progress in modeling these effects through architecture and optimization changes, towards increasing computational efficiency, lowering signal-to-noise ratio, and extending to a larger variety of nonlinear audio effects. Toward these ends, the strategies employed involved a three-pronged approach: model speed, model accuracy, and model generalizability. Most of the presented methods provide marginal or no increase in output accuracy over the original model, with the exception of dataset manipulation. We found that limiting the audio content of the dataset, for example using datasets of just a single instrument, provided a significant improvement in model accuracy over models trained on more general datasets. ","Exploring Quality and Generalizability in Parameterized Neural Audio
  Effects"
199,1270326456137273349,66189873,Umut U. Simsekli,"['It seems the lockdown was not that inefficient after all! In our new paper <LINK> we discovered that SGD can converge to a heavy-tailed distribution with infinite variance! Surprisingly this can even happen in a simple linear regr. problem with Gaussian data! 1/4 <LINK>', ""We rigorously proved this fact for quadratic optimization: large learning-rate(lr) and small batchsize(bs) *do* create heavy tails. Take-home message: changing lr or bs doesn't only change the scale of the noise, it changes the tails of the algorithm. 2/4"", 'We think this might be the main reason why @CalcCon observed heavy-tails in deep nets. We also identified 3 regimes based on tails, which seem quite similar to @yasamanbb\'s recent work: the ""catapult phase"" might in fact correspond to the heavy-tailed regime. 3/4', 'Finally, we discussed the appropriateness of the SDE representations of SGD: state-independent Gaussian noise assumption is not enough, and alpha-stable SDEs https://t.co/zkmXvsCXGv might be indeed a better model! Joint work with @MGurbuzbalaban and Lingjiong Zhu 4/4']",https://arxiv.org/abs/2006.04740,"In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the `flatness' of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the `tail-index', which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD iterates will converge to a \emph{heavy-tailed} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks. ",The Heavy-Tail Phenomenon in SGD
200,1270298349846003714,260971421,Walid Magdy,"['Following on our #StanceDetection (SD) tutorial @icwsm, please find our survey paper w/ @AldayelAbeer that covers the trends in SD research, including methods, datasets, and recent trends.\n\nPaper will be updated frequently with new research.\n\n<LINK>']",https://arxiv.org/abs/2006.03644,"Stance detection on social media is an emerging opinion mining paradigm for various social and political applications in which sentiment analysis may be sub-optimal. There has been a growing research interest for developing effective methods for stance detection methods varying among multiple communities including natural language processing, web science, and social computing. This paper surveys the work on stance detection within those communities and situates its usage within current opinion mining techniques in social media. It presents an exhaustive review of stance detection techniques on social media, including the task definition, different types of targets in stance detection, features set used, and various machine learning approaches applied. The survey reports state-of-the-art results on the existing benchmark datasets on stance detection, and discusses the most effective approaches. In addition, this study explores the emerging trends and different applications of stance detection on social media. The study concludes by discussing the gaps in the current existing research and highlights the possible future directions for stance detection on social media. ",Stance Detection on Social Media: State of the Art and Trends
201,1270270400144896001,844013017830379522,Nayeon Lee,['“Misinformation has High Perplexity” \n\nOur new work that leverages perplexity to debunk COVID-19 related false claims. Also releasing two small COVID-19 related test sets. @yejin_bang @AndreaMadotto @pascalefung\n\nPaper: <LINK>\nDataset: <LINK> <LINK>'],https://arxiv.org/abs/2006.04666,"Debunking misinformation is an important and time-critical task as there could be adverse consequences when misinformation is not quashed promptly. However, the usual supervised approach to debunking via misinformation classification requires human-annotated data and is not suited to the fast time-frame of newly emerging events such as the COVID-19 outbreak. In this paper, we postulate that misinformation itself has higher perplexity compared to truthful statements, and propose to leverage the perplexity to debunk false claims in an unsupervised manner. First, we extract reliable evidence from scientific and news sources according to sentence similarity to the claims. Second, we prime a language model with the extracted evidence and finally evaluate the correctness of given claims based on the perplexity scores at debunking time. We construct two new COVID-19-related test sets, one is scientific, and another is political in content, and empirically verify that our system performs favorably compared to existing systems. We are releasing these datasets publicly to encourage more research in debunking misinformation on COVID-19 and other topics. ",Misinformation Has High Perplexity
202,1269818723373846528,280403336,Sean Welleck,['new paper w/ @kchonyc:\n\n“MLE-guided parameter search for task loss minimization in neural sequence modeling” <LINK>\n\nSequence-level training based on random search around the current parameters and the MLE gradient <LINK>'],https://arxiv.org/abs/2006.03158,"Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empirically performs well as a surrogate objective. Typical approaches to directly optimizing the task loss such as policy gradient and minimum risk training are based around sampling in the sequence space to obtain candidate update directions that are scored based on the loss of a single sequence. In this paper, we develop an alternative method based on random search in the parameter space that leverages access to the maximum likelihood gradient. We propose maximum likelihood guided parameter search (MGS), which samples from a distribution over update directions that is a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its improvement in the task loss. MGS shifts sampling to the parameter space, and scores candidates using losses that are pooled from multiple sequences. Our experiments show that MGS is capable of optimizing sequence-level losses, with substantial reductions in repetition and non-termination in sequence completion, and similar improvements to those of minimum risk training in machine translation. ","MLE-guided parameter search for task loss minimization in neural
  sequence modeling"
203,1282752226415575043,1123446492981755906,Ruosong Wang,"['How to explore an environment without the guidance of any reward function? In our recent work (<LINK>), we use UCB as the bonus to efficiently explore an unknown environment and find good policies for all possible reward functions in the linear MDP setting. 1/4', 'We further prove that under the weaker linear Q* assumption (the optimal Q-function is linear), then any algorithm requires exponential number of samples for reward-free exploration even for deterministic systems. 2/4', 'Our results imply exponential separations on the sample complexity between (1) the linear MDP assumption and the linear Q* assumption, (2) standard RL and reward-free exploration, and (3) reward-free exploration in the generative model and that in the standard RL model. 3/4', 'See our paper for more details! Joint work with @SimonShaoleiDu @lyang36 @rsalakhu. 4/4']",https://arxiv.org/abs/2006.11274,"Reward-free reinforcement learning (RL) is a framework which is suitable for both the batch RL setting and the setting where there are many reward functions of interest. During the exploration phase, an agent collects samples without using a pre-specified reward function. After the exploration phase, a reward function is given, and the agent uses samples collected during the exploration phase to compute a near-optimal policy. Jin et al. [2020] showed that in the tabular setting, the agent only needs to collect polynomial number of samples (in terms of the number states, the number of actions, and the planning horizon) for reward-free RL. However, in practice, the number of states and actions can be large, and thus function approximation schemes are required for generalization. In this work, we give both positive and negative results for reward-free RL with linear function approximation. We give an algorithm for reward-free RL in the linear Markov decision process setting where both the transition and the reward admit linear representations. The sample complexity of our algorithm is polynomial in the feature dimension and the planning horizon, and is completely independent of the number of states and actions. We further give an exponential lower bound for reward-free RL in the setting where only the optimal $Q$-function admits a linear representation. Our results imply several interesting exponential separations on the sample complexity of reward-free RL. ",On Reward-Free Reinforcement Learning with Linear Function Approximation
204,1281273230603489282,3164399995,Susi Lehtola,['Here we propose a novel class of density functionals only relying on the properties of the uniform electron gas.\n\n<LINK>'],https://arxiv.org/abs/2006.16835,"The homogeneous electron gas (HEG) is a key ingredient in the construction of most exchange-correlation functionals of density-functional theory. Often, the energy of the HEG is parameterized as a function of its spin density $n$, leading to the local density approximation (LDA) for inhomogeneous systems. However, the connection between the electron density and kinetic energy density of the HEG can be used to generalize the LDA by evaluating it on a weighted geometric average of the local spin density and the spin density of a HEG that has the local kinetic energy density of the inhomogeneous system, with a mixing ratio $x$. This leads to a new family of functionals that we term meta-local density approximations (meta-LDAs), which are still exact for the HEG, which are derived only from properties of the HEG, and which form a new rung of Jacob's ladder of density functionals. The first functional of this ladder, the local $\tau$ approximation (LTA) of Ernzerhof and Scuseria that corresponds to $x=1$ is unfortunately not stable enough to be used in self-consistent field calculations, because it leads to divergent potentials as we show in this work. However, a geometric averaging of the LDA and LTA densities with smaller values of $x$ not only leads to numerical stability of the resulting functional, but also yields more accurate exchange energies in atomic calculations than the LDA, the LTA, or the tLDA functional ($x=1/4$) of Eich and Hellgren. We choose $x=0.50$ as it gives the best total energy in self-consistent exchange-only calculations for the argon atom. Atomization energy benchmarks confirm that the choice $x=0.50$ also yields improved energetics in combination with correlation functionals in molecules, almost eliminating the well-known overbinding of the LDA and reducing its error by two thirds. ",Meta-local density functionals: a new rung on Jacob's ladder
205,1279168460975738885,99270209,Guillermo Valle,"[""happy to announce our new work, exploring whether SGD approximates Bayesian sampling, for neural networks doing classification.\nWork by Chris Mingard, me, Joar Skalse, Ard Louis.\nAfter MANY experiments, we find that: well, it's remarkably close <LINK> 1/9 <LINK>"", 'almost all experiments are of the sort:\n1 run SGD like 10^6 times to 0 train error, and find the distribution over labellings of a test set (our proxy for ""function"")\n2 compute the probability that Bayesian inference gives these same labellings\nresult: v good correlation! 2/9 https://t.co/kBB5mGHE8D', '* Bayesian inference is done by seizing the Gaussian process approximation to overparametrized neural nets (see @TheGregYang , Roman Novak, @jaschasd , @AdriGarriga et al.). And then we use either MSE (exact inference) or CE/0-1 loss (expectation propagation (EP) \napprox) 3/9', 'in the paper we perform the experiment for many combinations of architectures, datasets, and optimziers to make sure results are robust. \nOne limitation is the EP approx appears to introduce systematic error, which is why we also use MSE, altho less common for classification 4/9 https://t.co/OLmuFbHNyZ', ""Here's an interesting thing. We also computed the function probabilities for NTK, and they appear *less similar* to the SGD (batch 32) ones than the NNGP [all with MSE loss] 5/9 https://t.co/BqvBodD9FL"", 'With the EP approximation we can actually estimate the probability even of very rare functions. And we find that the posterior is super biased, towards low error functions. Also seems to be biased to low complexity functions (complexity of fun correlates well with its error) 6/9 https://t.co/Xz8qns6cWo', 'you can also do fun stuff, like what happens when you change the batch size or learnin rate? Smaller batch size makes the posterior more biased towards the top prob funs, as does training longer. Increasing lr has effect analogous to smaller batch size 7/9 https://t.co/L8KymEwaO1', 'to make doubly sure that this is true, for 0-1 loss (rember: our learning criterion is 0 train error), we compare SGD probs with Bayes posterior probs on the finite neural net, found by rejection sampling (more accurate than EP approx), but on a toy Boolean system  8/9 https://t.co/Ng2Snly3n3', 'if u wanna read our speculations and rambling about why this may be. Read the longe discussion section in the paper. tl;dr: dunno why, maybe because basin sizes vary by so much\nmore details, results, discussion in paperr^^\n9/9']",http://arxiv.org/abs/2006.15191,"Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$. Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance. ","Is SGD a Bayesian sampler? Well, almost"
206,1278943182626213889,2571660243,Cristian Bodnar 🇺🇦,"['Check out this cool new release. It also comes with extended support for higher-order ODE models, which we studied in <LINK> with a focus on 2nd order w/ @alexnorcliffe98 @itsmebenday @simidjievskin @pl219_Cambridge <LINK>']",https://arxiv.org/abs/2006.07220,"Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through infinite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on first order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a first order coupled ODE is equivalent and computationally more efficient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance. ",On Second Order Behaviour in Augmented Neural ODEs
207,1278731782079352832,738769492122214400,Johannes Lischner,"['Ionic liquids have many interesting and useful properties. In our new paper, we analyze their electronic structure using the GW approach and find excellent agreement with #photoemission spectroscopy. Read here:  <LINK> #compchem <LINK>']",https://arxiv.org/abs/2006.16717,"Room temperature ionic liquids play an important role in many technological applications and a detailed understanding of their frontier molecular orbitals is required to optimize interfacial barriers, reactivity and stability with respect to electron injection and removal. In this work, we calculate quasiparticle energy levels of ionic liquids using first-principles many-body perturbation theory within the GW approximation and compare our results to various mean-field approaches, including semilocal and hybrid density-functional theory and Hartree-Fock. We find that the mean-field results depend qualitatively and quantitatively on the treatment of exchange-correlation effects, while GW calculations produce results that are in excellent agreement with experimental photoelectron spectra of gas phase ion pairs and ionic liquids. These results establish the GW approach as a valuable tool for understanding the electronic structures of ionic liquids. ",Frontier Orbitals and Quasiparticle Energy Levels in Ionic Liquids
208,1278500500841721858,314395154,Tengyu Ma,"[""DL models tend to struggle with heteroskedastic and imbalanced datasets, where long-tailed labels have varying levels of uncertainty, partly bc it's hard to distinguish mislabeled, ambiguous, and rare examples. We propose a new regularization technique: <LINK> <LINK>"", 'The main principle is to regularize more strongly for those data that are rare and noisy. Joint work with @caokd8888, Yining Chen, @lu_junwei, Nikos Arechiga, @adnothing.', 'Conceptually, this can be viewed as a follow-up work of our last year NeurIPS paper on learning imbalanced datasets  https://t.co/IHbopWrwCD. In this paper, we need to deal with the interaction of the heteroskedasticity and imbalance more carefully.', '@huyhcmut1997 Thanks for the comments/questions. Yes, the idea can also be used for regression. Actually, the demonstrating examples in the intro are for the regression setting. To use it for regression, one can take equation (6) as the objective and choose $\\tau_i=\\simga_i^a/q_i^b$', '@huyhcmut1997 where a,b &gt; 0 are constants and \\sigma_i is the estimated standard deviation of the noise for that example (e.g., a=4/5, b=2/5 would the best choice predicted by the theory, but in fact, we think the exact choice does not matter that much as long as you tune lambda.)', '@huyhcmut1997 We will add a section on it in the next revision! Thanks!', '@chupvl @huyhcmut1997 Thanks for the comments. Hopefully my answer above to @huyhcmut also answers your question?']",http://arxiv.org/abs/2006.15766,"Real-world large-scale datasets are heteroskedastic and imbalanced -- labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning. ","Heteroskedastic and Imbalanced Deep Learning with Adaptive
  Regularization"
209,1278307077400809473,1104779445007716354,Alexandre Défossez,"['Demucs can now enhance your speech in real time, in the \nwaveform domain 🌊🗣️🎉➰📞👵. We study many tricks and their impact on perceptual quality and intelligibility: spectrogram losses, reverb aug., dry/wet balance. With @syhw and @adiyossLC. <LINK>', 'Audio samples: https://t.co/ajiYnsCyjY', '@srchvrs We are working on that, I’ll update here when we release it :)', '@mnlpariente @syhw @adiyossLC More look ahead definitely help. But you can’t remove it, the model has to wait at some point to have enough samples to compute the next lstm update. This would also be the case for a model working on the spectrogram (most include a LSTM).']",https://arxiv.org/abs/2006.12847,"We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities. We perform evaluations on several standard benchmarks, both using objective metrics and human judgements. The proposed model matches state-of-the-art performance of both causal and non causal methods while working directly on the raw waveform. ",Real Time Speech Enhancement in the Waveform Domain
210,1277998123256709120,1276301845569257472,Yuqing Du,"['Excited to share some recent work! Assisting people can be hard when it’s challenging to infer their goals. We propose another view: learning to increase human empowerment instead.\n\nw/ Stas Tiomkin, @emrek, Daniel Polani, @pabbeel, @ancadianadragan\n\n<LINK> <LINK>', 'In cases where goal inference can fail, Assistance via Empowerment (AvE) uses the intuition that increasing human empowerment can better enable the person to more easily reach whichever goal they want, even when the assistant has no information about the goal.', 'Perhaps the most fun part — we test our approach with learning to assist humans in playing the (deceptively simple) Lunar Lander game from OpenAI Gym! \n\nProject webpage: https://t.co/DE9sjXW6yA https://t.co/ufVzrVXtTw']",http://arxiv.org/abs/2006.14796,"One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s). Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training. ",AvE: Assistance via Empowerment
211,1277962527909920774,1408370898,Stanisław Jastrzębski,"['Representing a reaction as a sequence of graph edits is more natural, but difficult for DNNs 🤔. We successfully applied this idea to retrosynthesis and large datasets. We propose MEGAN and show SOTA top-k accuracy for K&gt;=10 on retro and forward synthesis. <LINK> <LINK>', 'Actual work done by @mikolajsacha and other great collaborators at @MoleculeOne :)', 'Concurrent work by @cwcoley  https://t.co/YJQMa2vPAb with a closely related idea. Please check it out! This line of research has the potential to move us towards more human-like systems for automatic synthesis planning :).']",http://arxiv.org/abs/2006.15426,"The central challenge in automated synthesis planning is to be able to generate and predict outcomes of a diverse set of chemical reactions. In particular, in many cases, the most likely synthesis pathway cannot be applied due to additional constraints, which requires proposing alternative chemical reactions. With this in mind, we present Molecule Edit Graph Attention Network (MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by models that express a chemical reaction as a sequence of graph edits, akin to the arrow pushing formalism. We extend this model to retrosynthesis prediction (predicting substrates given the product of a chemical reaction) and scale it up to large datasets. We argue that representing the reaction as a sequence of edits enables MEGAN to efficiently explore the space of plausible chemical reactions, maintaining the flexibility of modeling the reaction in an end-to-end fashion, and achieving state-of-the-art accuracy in standard benchmarks. Code and trained models are made available online at this https URL ","Molecule Edit Graph Attention Network: Modeling Chemical Reactions as
  Sequences of Graph Edits"
212,1277898690154434560,852542314996277253,Mats Julius Stensrud,['Causal inference conditional on a post-treatment variable is subtle. Here we propose new estimands: the conditional separable effects.\n\n<LINK> \n\n#causalinference <LINK>'],https://arxiv.org/abs/2006.15681,"Researchers are often interested in treatment effects on outcomes that are only defined conditional on a post-treatment event status. For example, in a study of the effect of different cancer treatments on quality of life at end of follow-up, the quality of life of individuals who die during the study is undefined. In these settings, a naive contrast of outcomes conditional on the post-treatment variable is not an average causal effect, even in a randomized experiment. Therefore the effect in the principal stratum of those who would have the same value of the post-treatment variable regardless of treatment, such as the always survivors in a truncation by death setting, is often advocated for causal inference. While this principal stratum effect is a well defined causal contrast, it is often hard to justify that it is relevant to scientists, patients or policy makers, and it cannot be identified without relying on unfalsifiable assumptions. Here we formulate alternative estimands, the conditional separable effects, that have a natural causal interpretation under assumptions that can be falsified in a randomized experiment. We provide identification results and introduce different estimators, including a doubly robust estimator derived from the nonparametric influence function. As an illustration, we estimate a conditional separable effect of chemotherapies on quality of life in patients with prostate cancer, using data from a randomized clinical trial. ",Conditional separable effects
213,1277770305390415872,103370610,Juniper Lovato,"['My first paper on the arXiv! With @all_are @reharp @LHDnets, we propose a “distributed consent” model, consent conditional on that of your network connections, and we look at its impact on privacy and observability in social networks.  <LINK> #dataethics #privacy <LINK>', 'P.S. #firstpaperjitters']",https://arxiv.org/abs/2006.16140,"Personal data are not discrete in socially-networked digital environments. A user who consents to allow access to their profile can expose the personal data of their network connections to non-consented access. Therefore, the traditional consent model (informed and individual) is not appropriate in social networks where informed consent may not be possible for all users affected by data processing and where information is distributed across users. Here, we outline the adequacy of consent for data transactions. Informed by the shortcomings of individual consent, we introduce both a platform-specific model of ""distributed consent"" and a cross-platform model of a ""consent passport."" In both models, individuals and groups can coordinate by giving consent conditional on that of their network connections. We simulate the impact of these distributed consent models on the observability of social networks and find that low adoption would allow macroscopic subsets of networks to preserve their connectivity and privacy. ","Limits of Individual Consent and Models of Distributed Consent in Online
  Social Networks"
214,1277496891371028481,293983295,Vincent Traag,"['In this preprint we study the agreement between metrics and peer review at the institutional level and also quantify the internal agreement of peer review. Our analysis is based on a sample of the Italian VQR exercise organised by @anvur_ita <LINK>', 'The agreement between metrics and peer review is on par with the internal agreement among two reviewers for certain fields of science. (Figure shows median absolute differences on a scale 3-30) https://t.co/xOM5Orfgjh', 'The level of agreement is generally lower at the publication level level than at the institutional level, for both metrics and peer review. https://t.co/BU5WfiXH9J', 'This work follows up on earlier work on the UK REF https://t.co/DR6e4o3nP4', 'All data for replication is available from\nhttps://t.co/8fj8WxPc3s']",https://arxiv.org/abs/2006.14830,"In the past decades, many countries have started to fund academic institutions based on the evaluation of their scientific performance. In this context, peer review is often used to assess scientific performance. Bibliometric indicators have been suggested as an alternative. A recurrent question in this context is whether peer review and metrics tend to yield similar outcomes. In this paper, we study the agreement between bibliometric indicators and peer review at the institutional level. Additionally, we also quantify the internal agreement of peer review at the institutional level. We find that the level of agreement is generally higher at the institutional level than at the publication level. Overall, the agreement between metrics and peer review is on par with the internal agreement among two reviewers for certain fields of science. This suggests that for some fields, bibliometric indicators may possibly be considered as an alternative to peer review for national research assessment exercises. ",Metrics and peer review agreement at the institutional level
215,1277396217245233153,314395154,Tengyu Ma,"['Online learning is a classical approach to address the domain shifts in a data stream but requires iterative labeling. We propose to only query the label of uncertain data and give a regret guarantee that leverages the hidden domain structure.  <LINK>', 'Joint work with Yining Chen, @HaipengLuo, and Chicheng Zhang.']",https://arxiv.org/abs/2006.14481,"Online machine learning systems need to adapt to domain shifts. Meanwhile, acquiring label at every timestep is expensive. We propose a surprisingly simple algorithm that adaptively balances its regret and its number of label queries in settings where the data streams are from a mixture of hidden domains. For online linear regression with oblivious adversaries, we provide a tight tradeoff that depends on the durations and dimensionalities of the hidden domains. Our algorithm can adaptively deal with interleaving spans of inputs from different domains. We also generalize our results to non-linear regression for hypothesis classes with bounded eluder dimension and adaptive adversaries. Experiments on synthetic and realistic datasets demonstrate that our algorithm achieves lower regret than uniform queries and greedy queries with equal labeling budget. ",Active Online Learning with Hidden Shifting Domains
216,1276889796863045632,1011585253423665152,Marija Slavkovik,"['We (@guribye + tweeterless Than and Oda) studied cookie consent notices of news outlets. We looked for dark patterns &amp; found A PLENTY. Regulating consent without regulating interface specs is pointless.. paper accepted @nordichi2020 can read early version <LINK> <LINK>', '@Cristianapt @guribye @nordichi2020 @nataliabielova Pls send us comments and your work that can be cited. This is such a rapid field we need to avoid duplicating effort. We are looking at automatic detection of dark patterns next and would appreciate data.', '@Cristianapt @guribye @nordichi2020 @nataliabielova @CelestinMatte good idea! just drop me an email and we can set it up (it is on the arxiv paper)', '@Cristianapt @guribye @nordichi2020 @nataliabielova I am so sh** with names. Of course we read your paper :) It is very nice work. Looking forward to learning more.', '@soheilhuman @guribye @nordichi2020 Thanks for this!']",https://arxiv.org/abs/2006.13985,"To ensure that users of online services understand what data are collected and how they are used in algorithmic decision-making, the European Union's General Data Protection Regulation (GDPR) specifies informed consent as a minimal requirement. For online news outlets consent is commonly elicited through interface design elements in the form of a pop-up. We have manually analyzed 300 data collection consent notices from news outlets that are built to ensure compliance with GDPR. The analysis uncovered a variety of strategies or dark patterns that circumvent the intent of GDPR by design. We further study the presence and variety of these dark patterns in these ""cookie consents"" and use our observations to specify the concept of dark pattern in the context of consent elicitation. ","Circumvention by design -- dark patterns in cookie consents for online
  news outlets"
217,1276712145288196096,2577596593,Chelsea Finn,"['How can robots learn in changing, open-world environments?\n\nWe study: Deep Reinforcement Learning amidst Lifelong Non-Stationarity\n<LINK>\n\nwith Annie Xie, @jmes_harrison @StanfordAILab (1/5) <LINK>', 'We introduce dynamic-parameter MDPs, to capture environments with persistent, unobserved changes. \n\nThis includes shifting goals, wind, and payloads. (2/5) https://t.co/JdcjODICog', 'Standard RL algorithms (SAC, PPO, and SLAC) struggle in such environments, in comparison to an oracle that directly observes the change. (3/5) https://t.co/3WNVsb2Dle', 'By modeling and anticipating change, LILAC can do much better!\n\nWe expect LILAC to work well in environments where non-stationarity can be modeled to some degree. (4/5) https://t.co/VRDlw5HeqW', 'Very recent work by Yash Chandak et al. also studies a non-stationary MDP setting, with a neat alternative approach that aims to avoid modeling non-stationarity:\nhttps://t.co/WcNqWx42sJ\n(5/5)']",https://arxiv.org/abs/2006.10701,"As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift. ",Deep Reinforcement Learning amidst Lifelong Non-Stationarity
218,1276488455317606402,1049562817240674304,Marius Lindauer,"['To combine knowledge from human experts and the efficiency of Bayesian Optimization, we propose prior-guided Bayesian Optimization, PrBO (<LINK>). One more step to make #AutoML really applicable in practice. Code also available.']",https://arxiv.org/abs/2006.14608,"While Bayesian Optimization (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes BO to waste function evaluations on bad design choices (e.g., machine learning hyperparameters) that the expert already knows to work poorly. To address this issue, we introduce Bayesian Optimization with a Prior for the Optimum (BOPrO). BOPrO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO's standard priors over functions, which are much less intuitive for users. BOPrO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. We show that BOPrO is around 6.67x faster than state-of-the-art methods on a common suite of benchmarks, and achieves a new state-of-the-art performance on a real-world hardware design application. We also show that BOPrO converges faster even if the priors for the optimum are not entirely accurate and that it robustly recovers from misleading priors. ",Bayesian Optimization with a Prior for the Optimum
219,1276316525683634176,3245949691,Rebecca Leane,"[""New paper!\n\nSupernova Muons: New Constraints on Z' Bosons, Axions, and ALPs\n<LINK>\n\nw/ Djuna Croon (@QuantumMessage), @GillyElor + Sam McDermott\n\nWe use supernova muons to find some of the strongest existing limits on light new particles coupled to muons! Thread:"", '1/ Deep in the Large Magellanic Cloud, on the outskirts of the Tarantula Nebula, a blue supergiant named Sanduleak once shone with the brightness of over ten thousand Suns.', ""2/ That was, until one day, hydrostatic burning ended, no more nuclear energy could be released by fusion, and Sanduleak's iron core began to collapse under its own gravity."", '3/ Eventually, its core became so massive that even electron degeneracy pressure could not stabilize it. As Sanduleak contracted, \nphotons began to dissociate the iron atoms of the inner core, which decreased the energy of the star even more and caused it to further contract.', ""4/ Electrons were absorbed onto protons, and converted into neutrons and neutrinos. The escape of the neutrinos further lowered the electron degeneracy pressure, until Sanduleak's core became unstable, and *collapsed*."", ""5/ Once Sanduleak's core reached nuclear densities, the collapse was abruptly *halted*. A shock wave formed between the outer and\ninner core and moved outward from the core through the star."", ""6/ The implosion of the inner core ignited an explosion, and a core-collapse supernova was initiated. A mere 0.3 seconds after the collapse, the shock wave ejected the entire contents of Sanduleak's outer layers, leaving only a compact remnant (likely a neutron star) behind."", '7/ Neutrinos screamed out of the stellar inferno, and were observed in 1987 by the Kamiokande II, IMB, and the Baksan collaborations, in an event known as Supernova 1987A (SN1987A).', '8/ The transformation of Sanduleak into SN1987A has gifted us with a wealth of new insights for particle, nuclear, and astro physics.', '9/ By comparing with simulations, it appears that the number of neutrinos that were observed in SN1987A is consistent with that expected from known physics.', '10/ This provides an exciting avenue to probe other light new particles. That is, if new particles were also created during this explosion, they might leak energy out of the event, leaving less energy available to power the production of the neutrinos.', '11/ This would be in conflict with the neutrinos observed in SN1987A. This means that any other light particles that live long enough to escape the system, can be constrained by the fact that they are not allowed to steal away too much energy.', '12/ When producing these new particles, research in the past has mostly focused on production (and interactions) using the protons or neutrons in the supernova.', '13/ We do something largely different. We consider the impact of interactions instead with the *muons* produced in 1987A.', '14/  While these are less abundant than the protons or neutrons, they can provide a more sensitive probe of particles that would interact directly with muons, or leptons.', ""15/For the first time, we considered limits that arise from Z' bosons (force carriers) interacting with these muons. To get such limits,we exploited the most recent simulations of the muon number density profiles in the explosion, as well as temperature profiles of the explosion."", ""16/ The profiles give us a measure of these variables as a function of the radius from the center of the supernova. This allows us to determine how strongly these new interactions would occur, and how much energy loss they would lead to during Sanduleak's demise."", ""17/ When the Z' only interacts with muons and taus, in a particle model called gauged muon number minus tau number, we get the following limits, where the y-axis is the Z' coupling, and the x-axis is the Z' mass: https://t.co/Xhw0k7B748"", ""18/ The dip on the right hand side comes from neutrinos in the supernova pair coalescing to produce the Z', which would have then have run away stealing some of the supernova energy."", ""19/But it isn't allowed to do that in order to match observation, so it is shown as the excluded hot pink region.The bolder and lighter hot pink excluded regions come from taking a conservative version of the profiles, and a less conservative version of the profiles respectively."", ""20/ The flat line on the left hand side of the plot comes from something called a semi-Compton process, where a Z' could be produced after a photon interacts with a muon, and outputs a Z' and a muon. Again, as this would make the Z' a greedy energy thief, it is excluded."", ""21/ Compared to other processes, we find these limits are largely independent of the Z' mass, and can extend down to arbitrarily low Z' mass, even though they are only shown in a smaller mass window here."", '22/ We also show the previous estimate for this bound (dashed), which had not used the charged muons in the supernova, along with other relevant constraints from Neff, neutrino tridents (""CCFR""), and g-2.', ""23/ As our greedy Z' exclusion covers some of the g-2 region, we exclude the g-2 explananation for this model for Z' masses less than about 10^-5 MeV."", ""24/ We also considered the case that kinetic mixing was present in the Lmu-Ltau model. In this case, the bounds don't change much, but instead, the competing constraints do. You can see this in our plots for different amounts of mixing: https://t.co/KMpDWSsKs7"", ""25/ The resonance in the larger mixing case around 20 MeV comes from producing the Z' from a kinetic mixing loop, off protons rather than muons."", '26/ We also extended these limits to a popular model class, called ""B-L"", which is gauged baryon minus lepton number. This just determines which particles the Z\' will interact with. In that case, we also find new bounds: https://t.co/WRwXS2Rnaz', ""27/ Here, the biggest change from previous estimates arises from the neutrino-pair coalescence process, which gives more sensitive constraints at higher Z' masses."", ""28/ Separate to the Z', there are other types of light particles that could be produced in this event, called axions, or axion-like particles. Probing these particles using supernova muons was studied in a nice recent paper by Bollig, deRocco, Graham, and Jenka (2020)."", '29/ This is also the paper that produced the awesome new simulations.', '30/ That work focused on tree-level couplings of the axion. We were curious about how much these bounds might change when loop processes were also considered. We also extended the calculation to higher masses, to determine the full constraint across *all* axion masses.', ""31/ We found that the loops didn't change the bounds in Bollig et al for their mass range. In the higher, new mass range we were also considering, we found that loops were very important."", '32/ We found the following exclusion on the axion-muon interaction parameter space, where the x-axis is the axion mass, and the y-axis is its muon coupling: https://t.co/5XMG85tXQr', '33/ The flat lines arise from a combination of processes from muon bremsstrahlung, and the semi-Compton process. On the right hand side, the larger couplings lead to the axion getting trapped (and therefore not stealing energy), and so the limits cover less of the couplings.', '34/ We find we can constrain axions up to masses to nearly 800 MeV with one of the profiles. This is really high! The reason we have such good sensitivity here, is that the core temperature is really high, and the trapping is less efficient.', '35/ In our work, we have pointed out and explicitly demonstrated for the first time the broad applicability of supernova muons to provide a sensitive probe of models of new physics. This certainly motivates further studies of how muons behave in supernovae!', 'Shout out to Sanduleak, whose ultimate sacrifice gifted us with so many interesting stories to tell, and so many things we can learn about new physics. :)', 'Also shout out to my awesome collaborators, Djuna, Gilly, and Sam! Stay tuned for another paper we have in the works. ;)']",https://arxiv.org/abs/2006.13942,"New light particles produced in supernovae can lead to additional energy loss and a consequent deficit in neutrino production in conflict with the neutrinos observed from Supernova 1987A (SN1987A). Contrary to the majority of previous SN1987A studies, we examine the impact of $Z'$ bosons, axions, and axion-like particles (ALPs) interacting with the muons produced in SN1987A. For the first time, we find constraints on generic $Z'$ bosons coupled to muons, and apply our results to particle models including gauged $L_\mu-L_\tau$ number, $U(1)_{L_\mu-L_\tau}$, and gauged $B-L$ number, $U(1)_{B-L}$. We constrain $Z'$ bosons with masses up to about 250-500 MeV, and down to about $10^{-9}$ in $Z'$-muon coupling. We also extend previous work on axion-muon couplings by examining the importance of loop-level interactions, as well as performing calculations over a wider range of axion masses. We constrain muon-coupled axions from arbitrarily low masses up to about 200-500 MeV, with bounds extending down to axion-muon couplings of approximately $10^{-8}$ GeV$^{-1}$. We conclude that supernovae broadly provide a sensitive probe of new lightly-coupled particles interacting with muons. ","Supernova Muons: New Constraints on Z' Bosons, Axions, and ALPs"
220,1275965393681526784,3272519155,Bihan Wen,"['Non-local image denoising methods (e.g., BM3D, WNNM, SAIST, PGPD) are effective but pretty slow, due to block matching. We propose a Self-Convolution scheme, to significantly speed it up and benefit large-scale non-local image modeling. Check out:  <LINK> <LINK>']",https://arxiv.org/abs/2006.13714,"Constructing effective image priors is critical to solving ill-posed inverse problems in image processing and imaging. Recent works proposed to exploit image non-local similarity for inverse problems by grouping similar patches and demonstrated state-of-the-art results in many applications. However, compared to classic methods based on filtering or sparsity, most of the non-local algorithms are time-consuming, mainly due to the highly inefficient and redundant block matching step, where the distance between each pair of overlapping patches needs to be computed. In this work, we propose a novel Self-Convolution operator to exploit image non-local similarity in a self-supervised way. The proposed Self-Convolution can generalize the commonly-used block matching step and produce equivalent results with much cheaper computation. Furthermore, by applying Self-Convolution, we propose an effective multi-modality image restoration scheme, which is much more efficient than conventional block matching for non-local modeling. Experimental results demonstrate that (1) Self-Convolution can significantly speed up most of the popular non-local image restoration algorithms, with two-fold to nine-fold faster block matching, and (2) the proposed multi-modality image restoration scheme achieves superior denoising results in both efficiency and effectiveness on RGB-NIR images. The code is publicly available at \href{this https URL}. ","Exploiting Non-Local Priors via Self-Convolution For Highly-Efficient
  Image Restoration"
221,1275671220898476034,1212606587644178432,Romero-Isart Group,['In <LINK> we show that atomic motional states bound to the surface (adsorbed) of a hot optical nanofiber are well quantized and propose how to probe them optically. In collaboration with P. Schneeweiss and A. Rauschenbeutel\n@Daniel_Huemmer @iqoqi @uniinnsbruck <LINK>'],https://arxiv.org/abs/2006.12855,"Quantum control of atoms at ultrashort distances from surfaces would open a new paradigm in quantum optics and offer a novel tool for the investigation of near-surface physics. Here, we investigate the motional states of atoms that are bound weakly to the surface of a hot optical nanofiber. We theoretically demonstrate that with optimized mechanical properties of the nanofiber these states are quantized despite phonon-induced decoherence. We further show that it is possible to influence their properties with additional nanofiber-guided light fields and suggest heterodyne fluorescence spectroscopy to probe the spectrum of the quantized atomic motion. Extending the optical control of atoms to smaller atom-surface separations could create opportunities for quantum communication and instigate the convergence of surface physics, quantum optics, and the physics of cold atoms. ",Probing Surface-Bound Atoms with Quantum Nanophotonics
222,1275435087056613376,526115229,Kevin Heng,"['Detailed spectroscopic study of WASP-121b led by @HoeijmakersJens as part of @davehrenreich\'s HEARTS survey.\n\n<LINK>\n\n""We detect neutral Mg, Na, Ca, Cr, Fe, Ni and V, which we predict exists in equilibrium with a significant quantity of VO.""']",https://arxiv.org/abs/2006.11308,"Aims: We survey the transmission spectrum of WASP-121 b for line-absorption by metals and molecules at high spectral resolution, and elaborate on existing interpretations of the optical transmission spectrum observed with HST/STIS and WFC3. Methods: We use the cross-correlation technique and direct differential spectroscopy to search for sodium and other neutral and ionised atoms, TiO, VO and SH in high-resolution transit spectra obtained with the HARPS spectrograph. We inject models assuming chemical and hydrostatic equilibrium with varying temperature and composition to enable model comparison, and employ two bootstrap methods to test the robustness of our detections. Results: We detect neutral Mg, Na, Ca, Cr, Fe, Ni and V, which we predict exists in equilibrium with a significant quantity of VO, supporting earlier observations by HST/WFC3. Non-detections of Ti and TiO support the hypothesis that Ti is depleted via a cold-trap mechanism as has been proposed in the literature. Atomic line depths are under-predicted by hydrostatic models by a factor of 1.5 to 8, confirming recent findings that the atmosphere is extended. We predict the existence of significant concentrations of gas-phase TiO$_2$, VO$_2$ and TiS, which could be important absorbers at optical and NIR wavelengths in hot Jupiter atmospheres, but for which accurate line-list data is currently not available. We find no evidence for absorption by SH, and find that inflated atomic lines can plausibly explain the slope of the transmission spectrum observed in the NUV with HST/STIS. The Na D lines are significantly broadened and show a difference in their respective depths of 15 scale heights, which is not expected from isothermal hydrostatic theory. ","Hot Exoplanet Atmospheres Resolved with Transit Spectroscopy (HEARTS)
  IV. A spectral inventory of atoms and molecules in the high-resolution
  transmission spectrum of WASP-121 b"
223,1275378762994126854,426509606,Yamir Moreno,"['In our new work, out today in the arXiv (<LINK>), we propose new metrics to describe the performance of dynamical systems as a function of the strength of the couplings within and between clusters. w/ Peng Ji et al. <LINK>']",https://arxiv.org/abs/2006.11357,"The dynamical and structural aspects of cluster synchronization (CS) in complex systems have been intensively investigated in recent years. Here, we study CS of dynamical systems with intra and inter-cluster couplings. We propose new metrics that describe the performance of such systems and evaluate them as a function of the strength of the couplings within and between clusters. We obtain analytical results that indicate that spectral differences between the Laplacian matrices associated with the partition between intra and inter-couplings directly affect the proposed metrics of system performance. Our results show that the dynamics of the system might exhibit an optimal balance that optimizes its performance. Our work provides new insights into the way specific symmetry properties relate to collective behavior, and could lead to new forms to increase the controllability of complex systems and to optimize their stability. ","Impact of intra and inter-cluster coupling balance on the performance of
  nonlinear networked systems"
224,1275318361925115908,495550336,Abhishek Gupta,"['New work led by JD, Suvansh studying what properties of an environment can make sparse reward, non episodic learning easier. We find that highly dynamic environments or environments with natural “environment shaping” can help!\n\n<LINK>\n@svlevine @GlenBerseth']",https://arxiv.org/abs/2006.12478,"Much of the current work on reinforcement learning studies episodic settings, where the agent is reset between trials to an initial state distribution, often with well-shaped reward functions. Non-episodic settings, where the agent must learn through continuous interaction with the world without resets, and where the agent receives only delayed and sparse reward signals, is substantially more difficult, but arguably more realistic considering real-world environments do not present the learner with a convenient ""reset mechanism"" and easy reward shaping. In this paper, instead of studying algorithmic improvements that can address such non-episodic and sparse reward settings, we instead study the kinds of environment properties that can make learning under such conditions easier. Understanding how properties of the environment impact the performance of reinforcement learning agents can help us to structure our tasks in ways that make learning tractable. We first discuss what we term ""environment shaping"" -- modifications to the environment that provide an alternative to reward shaping, and may be easier to implement. We then discuss an even simpler property that we refer to as ""dynamism,"" which describes the degree to which the environment changes independent of the agent's actions and can be measured by environment transition entropy. Surprisingly, we find that even this property can substantially alleviate the challenges associated with non-episodic RL in sparse reward settings. We provide an empirical evaluation on a set of new tasks focused on non-episodic learning with sparse rewards. Through this study, we hope to shift the focus of the community towards analyzing how properties of the environment can affect learning and the ultimate type of behavior that is learned via RL. ",Ecological Reinforcement Learning
225,1275271286130749441,990433714948661250,Sergey Levine,"['We want RL agents that learn from sparse, delayed rewards, without resets. This is hard. In ""Ecological Reinforcement Learning,"" we study which aspects of the environment make this easy.\n\nw/ JD Co-Reyes, S. Sanjeev, @GlenBerseth, @abhishekunique7 \n\n<LINK> <LINK>', 'The idea is pretty simple: reset-free RL is easier if (1) we provide ""environment shaping"" (i.e., a natural curriculum), or if the environment is more dynamic (i.e., more random transitions). While the latter might at first seem *harder*, it makes reset-free learning easier.']",https://arxiv.org/abs/2006.12478,"Much of the current work on reinforcement learning studies episodic settings, where the agent is reset between trials to an initial state distribution, often with well-shaped reward functions. Non-episodic settings, where the agent must learn through continuous interaction with the world without resets, and where the agent receives only delayed and sparse reward signals, is substantially more difficult, but arguably more realistic considering real-world environments do not present the learner with a convenient ""reset mechanism"" and easy reward shaping. In this paper, instead of studying algorithmic improvements that can address such non-episodic and sparse reward settings, we instead study the kinds of environment properties that can make learning under such conditions easier. Understanding how properties of the environment impact the performance of reinforcement learning agents can help us to structure our tasks in ways that make learning tractable. We first discuss what we term ""environment shaping"" -- modifications to the environment that provide an alternative to reward shaping, and may be easier to implement. We then discuss an even simpler property that we refer to as ""dynamism,"" which describes the degree to which the environment changes independent of the agent's actions and can be measured by environment transition entropy. Surprisingly, we find that even this property can substantially alleviate the challenges associated with non-episodic RL in sparse reward settings. We provide an empirical evaluation on a set of new tasks focused on non-episodic learning with sparse rewards. Through this study, we hope to shift the focus of the community towards analyzing how properties of the environment can affect learning and the ultimate type of behavior that is learned via RL. ",Ecological Reinforcement Learning
226,1275127095358902273,22513981,Corentin Dancette,"['Our new work: Overcoming statistical shortcuts for open-ended visual counting <LINK>. We propose a benchmark that penalizes the learning of dataset biases, and a model with architectural priors that help generalization. Done with @RemiCadene @endernewton @quobbe <LINK>', 'First, we propose the Modifying Count Distribution (MCD) benchmark, that penalizes models that over-rely on statistical shortcuts. It contains pairs of training and testing sets that follow different label distribution (for example, even labels in train vs odd labels in test) https://t.co/EkAFFtRvE3', 'Secondly, we propose the Spatial Counting Network, a model that contains several architectural priors that help generalization. It grounds its final count in the image by assigning scores to image regions, and is trained using a regression loss. https://t.co/eNuoObBbUf', 'Our SCN reaches superior accuracy on the MCD benchmark. Questions are taken from the original TallyQA dataset https://t.co/WnRVUx1V60 https://t.co/shcC58EzFh', 'We also show that SCN improves grounding ability (correctly locating counted objects in the image). It is measured with average precision, and our new Grounding Precision metric. https://t.co/BMKbl13rlB', 'This work was done at @mlia_lip6. We thank @Genci_fr for the credits on the Jean Zay GPU cluster that enabled us to perform all our experiments !']",https://arxiv.org/abs/2006.10079,"Machine learning models tend to over-rely on statistical shortcuts. These spurious correlations between parts of the input and the output labels does not hold in real-world settings. We target this issue on the recent open-ended visual counting task which is well suited to study statistical shortcuts. We aim to develop models that learn a proper mechanism of counting regardless of the output label. First, we propose the Modifying Count Distribution (MCD) protocol, which penalizes models that over-rely on statistical shortcuts. It is based on pairs of training and testing sets that do not follow the same count label distribution such as the odd-even sets. Intuitively, models that have learned a proper mechanism of counting on odd numbers should perform well on even numbers. Secondly, we introduce the Spatial Counting Network (SCN), which is dedicated to visual analysis and counting based on natural language questions. Our model selects relevant image regions, scores them with fusion and self-attention mechanisms, and provides a final counting score. We apply our protocol on the recent dataset, TallyQA, and show superior performances compared to state-of-the-art models. We also demonstrate the ability of our model to select the correct instances to count in the image. Code and datasets are available: this https URL ",Overcoming Statistical Shortcuts for Open-ended Visual Counting
227,1273906082910547970,201348151,Jake Bennett,"['My first paper is (scarily) now on arXiv! We boost numerical resolution around shocks on-the-fly and see how it affects gas in the CGM. Find it here: <LINK>', '@NUFCLew Cheers mate!']",https://arxiv.org/abs/2006.10058,"There is an emerging consensus that large amounts of gas do not shock heat in the circumgalactic medium (CGM) of massive galaxies, but instead pierce deep into haloes from the cosmic web via filaments. To better resolve this process numerically, we have developed a novel `shock refinement' scheme within the moving mesh code AREPO that adaptively improves resolution around shocks on-the-fly in galaxy formation simulations. We apply this to a massive $\sim10^{12}$ M$_\odot$ halo at $z=6$ using the successful FABLE model, increasing the mass resolution by a factor of 512. With better refinement there are significantly more dense, metal-poor and fast-moving filaments and clumps flowing into the halo, leading to a more multiphase CGM. We find a $\sim50$ per cent boost in cool-dense gas mass and a 25 per cent increase in inflowing mass flux. Better resolved accretion shocks cause turbulence to increase dramatically, leading to a doubling in the halo's non-thermal pressure support. Despite much higher thermalisation at shocks with higher resolution, increased cooling rates suppress the thermal energy of the halo. In contrast, the faster and denser filaments cause a significant jump in the bulk kinetic energy of cool-dense gas, while in the hot phase turbulent energy increases by up to $\sim150$ per cent. Moreover, HI covering fractions within the CGM increase by up to 60 per cent. Consequently star formation is spread more widely and we predict a population of metal-poor stars forming within primordial filaments that deep JWST observations may be able to probe. ","Resolving shocks and filaments in galaxy formation simulations: effects
  on gas properties and star formation in the circumgalactic medium"
228,1273775635303784449,786071512646377473,Jungo Kasai,"['Autoregressive transformers typically have the same depth in the encoder and decoder.\nCan we find a better layer allocation for fast, accurate translation?\n\nDeep encoder, shallow one-layer decoder!\nNew work with @nik0spapp, Hao Peng, James Cross, @nlpnoah.\n<LINK> <LINK>', 'We examine the speed-quality tradeoff and compare autoregressive (AT) and strong non-autoregressive translation (NAT) models. NAT needs deeper decoders to resolve diverging word order btw the target and source. Source reordering would benefit shallow NAT decoders most. https://t.co/u7hsBxLNj2', ""Deep-shallow AT models continue to speed up inference even with large batch sizes where NAT suffers from increased total computation. Let's rethink our translation speed-quality baselines and evaluation for fast machine translation! https://t.co/sk4yvKgNl7""]",https://arxiv.org/abs/2006.10369,"Much recent effort has been invested in non-autoregressive neural machine translation, which appears to be an efficient alternative to state-of-the-art autoregressive machine translation on modern GPUs. In contrast to the latter, where generation is sequential, the former allows generation to be parallelized across target token positions. Some of the latest non-autoregressive models have achieved impressive translation quality-speed tradeoffs compared to autoregressive baselines. In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a single-layer autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. We show that the speed disadvantage for autoregressive baselines compared to non-autoregressive methods has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. Our results establish a new protocol for future research toward fast, accurate machine translation. Our code is available at this https URL ","Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine
  Translation"
229,1273620153675272192,143860543,Kirill Bykov 🤍💙🤍,"['How Much Can I Trust You? – Quantifying Uncertainties in Explaining Neural Networks\n\nTLDR: We propose a new method for explaining Bayesian Neural Networks that could be used further for quantifying uncertainties in explanations for *any* Neural Network 😎\n<LINK> <LINK>', ""'Bayesianizing' can be performed as a post-processing applied to pretrained non-Bayesian networks: for example through Kroneker-Factored Laplace approximation or MC Dropout. Thus, we can get better explanations even for standard architectures (here is VGG16 example) https://t.co/5EbjGJ75d1""]",https://arxiv.org/abs/2006.09000,"Explainable AI (XAI) aims to provide interpretations for predictions made by learning machines, such as deep neural networks, in order to make the machines more transparent for the user and furthermore trustworthy also for applications in e.g. safety-critical areas. So far, however, no methods for quantifying uncertainties of explanations have been conceived, which is problematic in domains where a high confidence in explanations is a prerequisite. We therefore contribute by proposing a new framework that allows to convert any arbitrary explanation method for neural networks into an explanation method for Bayesian neural networks, with an in-built modeling of uncertainties. Within the Bayesian framework a network's weights follow a distribution that extends standard single explanation scores and heatmaps to distributions thereof, in this manner translating the intrinsic network model uncertainties into a quantification of explanation uncertainties. This allows us for the first time to carve out uncertainties associated with a model explanation and subsequently gauge the appropriate level of explanation confidence for a user (using percentiles). We demonstrate the effectiveness and usefulness of our approach extensively in various experiments, both qualitatively and quantitatively. ","How Much Can I Trust You? -- Quantifying Uncertainties in Explaining
  Neural Networks"
230,1273538845619957761,1068823110,José Cano,"['The preprint of our ASAP 2020 paper ""Optimizing Grouped Convolutions on Edge Devices"" is already available!\n\nWe propose GSPC, a new and higher performance implementation of Grouped Convolutions. Check it out! <LINK>', ""@MJcomp86 Thanks Mahdi! Not sure about the multiple models in TVM... we haven't explored that yet, but it's interesting!""]",https://arxiv.org/abs/2006.09791,"When deploying a deep neural network on constrained hardware, it is possible to replace the network's standard convolutions with grouped convolutions. This allows for substantial memory savings with minimal loss of accuracy. However, current implementations of grouped convolutions in modern deep learning frameworks are far from performing optimally in terms of speed. In this paper we propose Grouped Spatial Pack Convolutions (GSPC), a new implementation of grouped convolutions that outperforms existing solutions. We implement GSPC in TVM, which provides state-of-the-art performance on edge devices. We analyze a set of networks utilizing different types of grouped convolutions and evaluate their performance in terms of inference time on several edge devices. We observe that our new implementation scales well with the number of groups and provides the best inference times in all settings, improving the existing implementations of grouped convolutions in TVM, PyTorch and TensorFlow Lite by 3.4x, 8x and 4x on average respectively. Code is available at this https URL ",Optimizing Grouped Convolutions on Edge Devices
231,1273524325983739904,4236127659,Dr Pratik Dabhade,"['In the quest to unravel the mystery of exceptionally large sizes of Giant Radio Galaxies (#GRGs), we have studied their host galaxies with #IRAM 30-m  telescope.\nSAGAN-II : Molecular gas content of GRGs\n<LINK> @CombesFrancoise  @MousumiMahato10 @_PhilippeSalome <LINK>']",https://arxiv.org/abs/2006.09388,"Radio galaxies with jets of relativistic particles are usually hosted by massive elliptical galaxies with active nuclei powered by accretion of interstellar matter onto a supermassive black hole. In some rare cases (<5%), their jets drive the overall structure to sizes larger than 700 kpc, and they are called giant radio galaxies (GRGs). A very small fraction of the population of such radio galaxies contains molecular and atomic gas in the form of rings or discs that can fuel star formation. The origin of this gas is not well known; it has sometimes been associated with a minor merger with a gas-rich disc galaxy (e.g. Centaurus A) or cooling of material from a hot X-ray atmosphere (e.g. cooling flows). The giant radio jets might be the extreme evolution of these objects, and they can teach us about the radio galaxy evolution. We selected 12 targets from a catalogue of 820 GRGs that are likely to be in a gas-accretion and star formation phase. The targets were selected from the mid-infrared to contain heated dust. We report here the results of IRAM-30m observations, the molecular gas content, and the star formation efficiency, and we discuss the origin of the gas and disc morphology. Three out of our 12 targets are detected, and for the others, we report significant upper limits. We combine our three detections and upper limits with four additional detected GRGs from the literature to discuss the results. Most of the GRG targets belong to the main sequence, and a large fraction are in the passive domain. Their star formation efficiency is comparable to normal galaxies, except for two galaxies that are deficient in molecular gas with a short (~200Myr) depletion time, and a quiescent gas-rich giant spiral galaxy. In general, the depletion time is much longer than the lifetime of the giant radio jet. ",SAGAN-II : Molecular gas content of giant radio galaxies
232,1273269846944501770,423671718,Joshua Loftus #peace,"['New #algorithmicfairness #fairAI paper with Ke Yang and @stoyanoj \n\nWe propose causal models with multiple sensitive attributes as a formal approach to intersectionality, and apply the framework to fair ranking tasks\n\nPreprint: <LINK> <LINK>', 'These models may allow us to carefully disentangle the ""bundles of sticks,"" to use @maya_sen and @owasow\'s metaphor for sensitive attributes (https://t.co/RB05f0lBT0)\n\nOur ranking idea: treat everyone as though they belong to the same intersectional subgroup (equal treatment?) https://t.co/WMbSxVxvDG', 'We illustrate these ideas on several real and synthetic examples. In the one below, job applicants to a fictional moving company are required to pass a weight-lifting test. There is both direct and indirect discrimination on the basis of both race and gender.\n\n(end of thread) https://t.co/KTgi8rzLol', '(Addendum) In above figure the selection rate is % of subgroup ranked in the top 50, 100, or 200, i.e. would be hired.\n\nBy removing direct discrimination but keeping the weight-lifting requirement, our method results in many more black men being hired in this example', ""@lastpositivist @stoyanoj Thanks! Sorry we missed it earlier, not sure how I forgot because I definitely have it open in a tab somewhere... we'll be sure to cite you in the next update""]",https://arxiv.org/abs/2006.08688,"In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search results to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in fair machine learning explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behaviour under different structural assumptions. ",Causal intersectionality for fair ranking
233,1273151578040684546,1050529973751205888,jörn jacobsen,"['Invertible Neural Nets (INNs) / Normalizing Flows are amazing! But are INNs always invertible? \n\nSurprisingly we find that they often violate this constraint!\nBelow a Glow reconstruction on CelebA\n\nwith @JensBehrmann @PaulVicol @kcjacksonwang @RogerGrosse\n\n<LINK> <LINK>', 'Derivatives of inverses can become arbitrarily large =&gt; ""exploding inverse""\n\nThis can lead to analytical invertibility not carrying through to the numerics, INNs become non-invertible!\n\nWe explain this effect by analysing bi-Lipschitz properties of common invertible networks https://t.co/LbIj5woZcG', 'We also find striking differences between INNs. Additive coupling blocks stably train with memory-saving gradients, while affine couplings lead to incorrect gradient computation, highlighting the importance to understand influence of architectural choices on exploding inverses https://t.co/rcMyPymdaf', 'Because memory-saving backprop only requires accurate invertibility on training data, we propose an architecture-agnostic solution ensuring local invertibility: bi-directional finite differences penalties\n\nBut this is not enough for Normalizing Flows (NFs)!', 'For NFs we often want density estimates on samples not from the training data =&gt; We need global invertibility! \n\nIndeed NFs can suffer from exploding inverses on OOD inputs implying meaningless density estimates. Solving this requires stable architectures like Residual Flows! https://t.co/Iqa9OJtczg', 'Take home messages: \n\n1) Analytical invertibility does not necessarily imply numerical invertibility \n\n2) Different tasks have different requirements on invertibility (e.g. local vs. global)\n\n3) Controlling stability is crucial for principled and successful application of INNs', 'All this and much more in our new work: \n\n""Understanding and Mitigating Exploding Inverses in Invertible Neural Networks"" \n\nLink: https://t.co/I4OAOdqM1Z\n\n👩\u200d🔬 We hope our work encourages researchers to consider stability as an important ingredient of INN design 👨\u200d🔬', 'Code here: https://t.co/WjqsNwQ2K2', ""@emiel_hoogeboom That's very interesting! For the sake of brevity we left autoregressive models out, but it should be possible to extend our observations / bounds to this case as well. Note that affine coupling blocks are not globally Lipschitz (as we show)!""]",https://arxiv.org/abs/2006.09347,"Invertible neural networks (INNs) have been used to design generative models, implement memory-saving gradient computation, and solve inverse problems. In this work, we show that commonly-used INN architectures suffer from exploding inverses and are thus prone to becoming numerically non-invertible. Across a wide range of INN use-cases, we reveal failures including the non-applicability of the change-of-variables formula on in- and out-of-distribution (OOD) data, incorrect gradients for memory-saving backprop, and the inability to sample from normalizing flow models. We further derive bi-Lipschitz properties of atomic building blocks of common architectures. These insights into the stability of INNs then provide ways forward to remedy these failures. For tasks where local invertibility is sufficient, like memory-saving backprop, we propose a flexible and efficient regularizer. For problems where global invertibility is necessary, such as applying normalizing flows on OOD data, we show the importance of designing stable INN building blocks. ","Understanding and Mitigating Exploding Inverses in Invertible Neural
  Networks"
234,1273076154229043206,32018884,Julia Stoyanovich,"['Excited to share that ""Causal intersectionality for fair ranking"" by Ke Yang, @joftius and me is on arXiv. We propose a causal modeling approach to intersectional fairness, along with  a flexible method for computing intersectionally fair rankings <LINK>']",https://arxiv.org/abs/2006.08688,"In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search results to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in fair machine learning explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behaviour under different structural assumptions. ",Causal intersectionality for fair ranking
235,1272906581324312585,1272253746265948161,Alexander Norcliffe,"['How do Neural ODEs learn 2nd order dynamics? In our new paper “On Second Order Behaviour in Augmented Neural ODEs” we perform an extensive study to answer this question. w/ @CristianBodnar, @itsmebenday, @simidjievskin, @pl219_Cambridge\n\nPaper: <LINK> 1/12 <LINK>', 'We begin by generalising the adjoint sensitivity method to Second Order NODEs (SONODEs). We compare this method with the first order training procedure of the equivalent coupled ODE, which has been used so far in the literature, and find the latter to be more efficient. 2/12 https://t.co/SdNyj6kmPN', 'Furthermore, we study some of the properties of SONODEs and investigate how they manifest on a couple of toy modelling problems. 3/12', 'We consider a generalised parity problem where x is mapped to -x. We show NODEs can do this in even-dimensional spaces using rotations, while SONODEs have a trivial solution where all points pass through the origin with different velocities. 4/12 https://t.co/ZRXysFOi0P', 'We also show that SONODEs, similarly to ANODEs, are not restricted to homeomorphic transformations, and can, therefore, solve the problem of the nested spheres. 5/12 https://t.co/9RNumFBEXs', 'Despite the fact that ANODEs don’t have the inductive biases of SONODEs, we show that they are still flexible enough to learn second-order dynamics in practice. However, they do so by learning to approximate an abstract coupled ODE with an entangled representation. 6/12 https://t.co/WsnBq7FK15', 'We also prove that ANODEs can compactly learn second-order dynamics with minimal augmentation and they do not necessarily need a number of augmented dimensions equal to the dimensionality of the real space. 7/12 https://t.co/2S4DwyqYVi', 'Despite their flexibility, because the augmented dimensions don’t have a clear physical meaning and they can’t be interpreted, ANODEs might be unsuitable for many scientific applications. 8/12', 'In fact, we prove that ANODEs can learn the real-space 2nd order dynamics in an infinite number of (non-trivial) ways and because of this, they always converge to another augmented trajectory. In contrast, SONODEs are constrained to approximate a unique functional form. 9/12 https://t.co/yfuZKQDKyv', 'Ultimately, we find the inductive biases of SONODE to be helpful when modelling second order systems. For instance, SONODE predictions extrapolate extremely well compared to ANODE on a real electronic Duffing oscillator. 10/12 https://t.co/A21Sz8U1wd', 'At the same time, we find SONODEs to be more robust to noise when extrapolating from noisy sine curves with various noise magnitudes. 11/12 https://t.co/FGS8iPCWmW', 'We also consider a higher-order system of airplane vibrations and find that the ability of ANODEs to compactly access higher-order behaviour comes in handy in this scenario. \n\nCode: https://t.co/5X1L6hSyJE 12/12 https://t.co/Eis6hjsjqe']",https://arxiv.org/abs/2006.07220,"Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through infinite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on first order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a first order coupled ODE is equivalent and computationally more efficient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance. ",On Second Order Behaviour in Augmented Neural ODEs
236,1272872268893487106,735394713981886465,Dr Camilla Elphick,['Building trust between police and citizens is key as #PoliceReform We propose 12 design considerations for digital policing mechanisms @citizenforensic in this preprint <LINK> <LINK>'],https://arxiv.org/abs/2006.07140,"Perceptions of police trustworthiness are linked to citizens' willingness to cooperate with police. Trust can be fostered by introducing accountability mechanisms, or by increasing a shared police/citizen identity, both which can be achieved digitally. Digital mechanisms can also be designed to safeguard, engage, reassure, inform, and empower diverse communities. We systematically scoped 240 existing online citizen-police and relevant third-party communication apps, to examine whether they sought to meet community needs and policing visions. We found that 82% required registration or login details, 55% of those with a reporting mechanism allowed for anonymous reporting, and 10% provided an understandable privacy policy. Police apps were more likely to seek to reassure, safeguard and inform users, while third-party apps were more likely to seek to empower users. As poorly designed apps risk amplifying mistrust and undermining policing efforts, we suggest 12 design considerations to help ensure the development of high quality/fit for purpose Police/Citizen apps. ","Building trust in digital policing: A scoping review of community
  policing apps"
237,1272823641986777088,2830544096,Álvaro Parafita,"['Our new work, ""Causal Inference with Deep Causal Graphs"" is out! @bitenmascarado. We propose Normalizing Causal Flows, a novel deep model capable of counterfactual estimation, with applications to ML #explainability and #fairness. <LINK>']",http://arxiv.org/abs/2006.08380,"Parametric causal modelling techniques rarely provide functionality for counterfactual estimation, often at the expense of modelling complexity. Since causal estimations depend on the family of functions used to model the data, simplistic models could entail imprecise characterizations of the generative mechanism, and, consequently, unreliable results. This limits their applicability to real-life datasets, with non-linear relationships and high interaction between variables. We propose Deep Causal Graphs, an abstract specification of the required functionality for a neural network to model causal distributions, and provide a model that satisfies this contract: Normalizing Causal Flows. We demonstrate its expressive power in modelling complex interactions and showcase applications of the method to machine learning explainability and fairness, using true causal counterfactuals. ",Causal Inference with Deep Causal Graphs
238,1272804113705730048,16837428,John Stott,['QSAGE paper on astroph today starring @richbielby @ntejos @rcrain_astro @SimonLMorris @astro_amos . We find overdensities around the most UV luminous quasars at z=1-2 <LINK>'],https://arxiv.org/abs/2006.07384,"We demonstrate that the UV brightest quasars at z=1-2 live in overdense environments. This is based on an analysis of deep Hubble Space Telescope WFC3 G141 grism spectroscopy of the galaxies along the lines-of-sight to UV luminous quasars in the redshift range z=1-2. This constitutes some of the deepest grism spectroscopy performed by WFC3, with 4 roll angles spread over a year of observations to mitigate the effect of overlapping spectra. Of the 12 quasar fields studied, 8 display evidence for a galaxy overdensity at the redshift of the quasar. One of the overdensities, PG0117+213 at z=1.50, has potentially 36 spectroscopically confirmed members, consisting of 19 with secure redshifts and 17 with single-line redshifts, within a cylinder of radius ~700 kpc. Its halo mass is estimated to be log (M/Msol)=14.7. This demonstrates that spectroscopic and narrow-band observations around distant UV bright quasars may be an excellent route for discovering protoclusters. Our findings agree with previous hints from statistical observations of the quasar population and theoretical works, as feedback regulated black hole growth predicts a correlation between quasar luminosity and halo mass. We also present the high signal-to-noise rest-frame optical spectral and photometric properties of the quasars themselves. ","Quasar Sightline and Galaxy Evolution (QSAGE) survey -- II. Galaxy
  overdensities around UV luminous quasars at z=1-2"
239,1272779057571270662,75249390,Axel Maas,"['We have published a new article, in which we investigate the way how three gluons interact. We could confirm some conjectures, and find that low-energy gluons behave in quite a spectacular way: <LINK>']",https://arxiv.org/abs/2006.08248,"The three-gluon vertex has been found to be a vital ingredient in non-perturbative functional approaches. We present an updated lattice calculation of it in various kinematical configurations for all tensor structures and multiple lattice parameters in three dimensions, and in a subset of those in four dimensions, for SU(2) Yang-Mills theory in minimal Landau gauge. In three dimensions an unambiguous zero crossing for the tree-level form-factor is established, and consistency for all investigated form factors with a power-like divergence towards the infrared is observed. Using very coarse lattices this is even seen towards momenta as low as about 15 MeV. The results in four dimensions are consistent with such a behavior, but do not yet reach deep enough into the infrared to establish it. ","More on the three-gluon vertex in SU(2) Yang-Mills theory in three and
  four dimensions"
240,1272552809913823236,2236047510,Lucas Beyer,"[""Are we done with ImageNet? That's what we set out to answer in <LINK> with @olivierhenaff @__kolesnikov__ @XiaohuaZhai @avdnoord. Answer: it's complicated. On the way, we find a simple technique for +2.5% on ImageNet. <LINK>"", 'New labels for validation set, rater\'s raw ratings, list of ""clean"" training examples all available on GitHub: https://t.co/zGoLMVvnYL']",https://arxiv.org/abs/2006.07159,"Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition. ",Are we done with ImageNet?
241,1272464206575603714,2742282828,Andreea Font,"['New paper on arXiv today, in which we propose a new method for identifying the small dark matter subhaloes around galaxies via the imprints they leave in the circumgalactic medium: <LINK>']",https://arxiv.org/abs/2006.06741,"The standard model of cosmology, the LCDM model, robustly predicts the existence of a multitude of dark matter 'subhaloes' around galaxies like the Milky Way. A wide variety of observations have been proposed to look for the gravitational effects such subhaloes would induce in observable matter. Most of these approaches pertain to the stellar or cool gaseous phases of matter. Here we propose a new approach, which is to search for the perturbations that such dark subhaloes would source in the warm/hot circumgalactic medium (CGM) around normal galaxies. With a combination of analytic theory, carefully-controlled high-resolution idealised simulations, and full cosmological hydrodynamical simulations (the ARTEMIS simulations), we calculate the expected signal and how it depends on important physical parameters (subhalo mass, CGM temperature, and relative velocity). We find that dark subhaloes enhance both the local CGM temperature and density and, therefore, also the pressure. For the pressure and density, the fluctuations can vary in magnitude from tens of percent (for subhaloes with M_sub=10^10 Msun) to a few percent (for subhaloes with M_sub=10^8 Msun), although this depends strongly on the CGM temperature. The subhaloes also induce fluctuations in the velocity field ranging in magnitude from a few km/s up to 25 km/s. We propose that X-ray, Sunyaev-Zel'dovich effect, radio dispersion measure, and quasar absorption line observations can be used to measure these fluctuations and place constraints on the abundance and distribution of dark subhaloes, thereby placing constraints on the nature of dark matter. ",The imprint of dark subhaloes on the circumgalactic medium
242,1272425625710505985,38226810,Abhinav Dhall,"['We propose a Human-Centred AI approach to deepfakes detection based on eye-gaze movements and EEG signatures.\n\nTitle: ""The eyes know it: FakeET -- An Eye-tracking Database to Understand Deepfake Perception""\n\nReport - <LINK> <LINK>']",https://arxiv.org/abs/2006.06961,"We present \textbf{FakeET}-- an eye-tracking database to understand human visual perception of \emph{deepfake} videos. Given that the principal purpose of deepfakes is to deceive human observers, FakeET is designed to understand and evaluate the ease with which viewers can detect synthetic video artifacts. FakeET contains viewing patterns compiled from 40 users via the \emph{Tobii} desktop eye-tracker for 811 videos from the \textit{Google Deepfake} dataset, with a minimum of two viewings per video. Additionally, EEG responses acquired via the \emph{Emotiv} sensor are also available. The compiled data confirms (a) distinct eye movement characteristics for \emph{real} vs \emph{fake} videos; (b) utility of the eye-track saliency maps for spatial forgery localization and detection, and (c) Error Related Negativity (ERN) triggers in the EEG responses, and the ability of the \emph{raw} EEG signal to distinguish between \emph{real} and \emph{fake} videos. ","The eyes know it: FakeET -- An Eye-tracking Database to Understand
  Deepfake Perception"
243,1271709221206151168,1037195648636989442,Hidenori Tanaka,"['Q. Can we find winning lottery tickets, or sparse trainable deep networks at initialization without ever looking at data?\n\nA. Yes, by conserving ""Synaptic Flow"" via our new SynFlow algorithm.\n\nco-led with Daniel Kunin\n&amp; @dyamins, @SuryaGanguli\n\npaper: <LINK>\n1/ <LINK>', 'We can potentially reduce the cost of training if we can prune neural networks at initialization.\n\nThe key challenge is ""layer-collapse,"" the premature pruning of an entire layer making a network untrainable.\n2/ https://t.co/hTB5jEdeuD', 'To better understand the phenomena, we first mathematically formulate and experimentally verify a conservation law.\n\nThis conservation law explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse.\n3/ https://t.co/a8jBLZ5Uoh', 'We then hypothesize that the conservative scoring combined with ""iterative"" re-evaluation can avoid layer collapse. \n\nThis insight also explains how iterative magnitude pruning avoids layer-collapse to identify ""winning-lottery ticket ""subnetworks at initialization.\n4/ https://t.co/kl7sXpTHN2', 'We prove that layer-collapse can be entirely avoided by designing an algorithm with iterative, positive, conservative scoring.\n\nWe design SynFlow satisfying the key requirements and show that it reaches the theoretical limit of max compression without collapsing a network.\n5/ https://t.co/3eLdvdifXB', 'Notably, SynFlow makes no reference to the training data and consistently outperforms existing state-of-the-art\npruning algorithms at initialization on 12 distinct combinations of models and datasets.\n\n6/ https://t.co/q7KjKMK3pd', 'Overall, our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\n\nPlease check out the paper for more details\nhttps://t.co/AAHQchcRKC\n7/', '@tingwuc Yes, we are working to incorporate them into our codebase.\n\nIn the meantime, this paper https://t.co/b5KoA2vPnk did very careful work on how pruning at initialization methods (SNIP, GraSP) compare with ""train-prune"" methods, including IMP and others.', ""@xaqlab Thank you for the question.\nSynFlow naturally avoids layer-bottlenecking that starts well before the eventual collapse.\nThis is why we see a significant gain in performance compared to other methods that don't reach max compression.""]",http://arxiv.org/abs/2006.05467,"Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important. ","Pruning neural networks without any data by iteratively conserving
  synaptic flow"
244,1271447332110901249,806058672619212800,Guillaume Lample,"['Could neural networks find alternatives to classical theories? We show that they can predict abstract mathematical properties of systems involving advanced notions like Fourier transforms, Jacobians, integration. 1/4\n<LINK>\nwith @Amaury_Hayat and @f_charton <LINK>', 'We investigate three classical mathematical problems about differential systems: local stability, controllability, and the existence and behavior of solutions of partial differential equations. Models are trained to predict qualitative or quantitative features of the systems. 2/4', 'Without any built-in math knowledge, and learning only from examples, we found that models can predict abstract properties with up to 95% accuracy, and numerical values with 85% accuracy. 3/4', 'Even shallow and lightweight models (1 layer, dim 64) achieve a high performance, even on very complex calculations, suggesting that the model\nmight be using a different approach than the known theory to make correct predictions. 4/4']",https://arxiv.org/abs/2006.06462,"Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge. ",Learning advanced mathematical computations from examples
245,1271442953039417344,962748619194617856,Alan Morningstar,"['New preprint: We find that the 1D MBL transition---under our model assumptions---is in a totally new universality class! <LINK>', '@RyanPlestid Thanks! Within our model the RG flow shares some features with KT universality, with the critical point being the endpoint of an MBL fixed line. The flow is similarly slow along the critical manifold, but fast perpendicular to it, unlike KT.', '@RyanPlestid Ya so the divergence of the ""correlation length"" is a different form, for example. The driving physics in our model is the proliferation of locally thermal regions (https://t.co/uWfsP0jApx) at longer and longer timescales (lengthscales).', '@RyanPlestid Will do! (Also maybe a better reference than the previous one I sent: https://t.co/nRHKGR2kG1.)']",https://arxiv.org/abs/2006.04825,"We examine the many-body localization (MBL) phase transition in one-dimensional quantum systems with quenched randomness and short-range interactions. Following recent works, we use a strong-randomness renormalization group (RG) approach where the phase transition is due to the so-called avalanche instability of the MBL phase. We show that the critical behavior can be determined analytically within this RG. On a rough $\textit{qualitative}$ level the RG flow near the critical fixed point is similar to the Kosterlitz-Thouless (KT) flow as previously shown, but there are important differences in the critical behavior. Thus we show that this MBL transition is in a new universality class that is different from KT. The divergence of the correlation length corresponds to critical exponent $\nu \rightarrow \infty$, but the divergence is weaker than for the KT transition. ",Many-body localization near the critical point
246,1271372600044732417,940367275,Abhishek Gupta,['You might find this work from @taniadegasperis and I that we did for the @mtlaiethics on using a participatory design approach to building more #responsibleAI systems <LINK> <LINK>'],https://arxiv.org/abs/2006.00432,"With the push for contact- and proximity-tracing solutions as a means to manage the spread of the pandemic, there is a distrust between the citizens and authorities that are deploying these solutions. The efficacy of the solutions relies on meeting a minimum uptake threshold which is hitting a barrier because of a lack of trust and transparency in how these solutions are being developed. We propose participatory design as a mechanism to evoke trust and explore how it might be applied to co-create technological solutions that not only meet the needs of the users better but also expand their reach to underserved and high-risk communities. We also highlight the role of the bazaar model of development and complement that with quantitative and qualitative metrics for evaluating the solutions and convincing policymakers and other stakeholders in the value of this approach with empirical evidence. ",Participatory Design to build better contact- and proximity-tracing apps
247,1271086751700877313,175184725,Brendan O'Donoghue,"['""Stochastic matrix games with bandit feedback"", with Tor Lattimore and @IanOsband, wherein we generalize stochastic multi-armed bandits to two-player zero-sum matrix games, I was surprised to find out that Thompson sampling provably fails in this context: <LINK>']",https://arxiv.org/abs/2006.05145,"We study a version of the classical zero-sum matrix game with unknown payoff matrix and bandit feedback, where the players only observe each others actions and a noisy payoff. This generalizes the usual matrix game, where the payoff matrix is known to the players. Despite numerous applications, this problem has received relatively little attention. Although adversarial bandit algorithms achieve low regret, they do not exploit the matrix structure and perform poorly relative to the new algorithms. The main contributions are regret analyses of variants of UCB and K-learning that hold for any opponent, e.g., even when the opponent adversarially plays the best-response to the learner's mixed strategy. Along the way, we show that Thompson fails catastrophically in this setting and provide empirical comparison to existing algorithms. ",Matrix games with bandit feedback
248,1271027698157182978,1259188770,Olivier Bachem,"[""We spent the last months at @GoogleAI Zurich &amp; Paris trying to understand on-policy RL for locomotion using a large scale study (&gt;250'000 models). Check out <LINK> for insights and practical recommendations on &gt;50 high- and low-level implementation decisions! 1/2 <LINK>"", 'This is joint work w/ Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, @RaphaelMarinier, @leonardhussenot, Matthieu Geist, Olivier Pietquin, @MMMichalski, @sylvain_gelly. 2/2']",https://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents. ","What Matters In On-Policy Reinforcement Learning? A Large-Scale
  Empirical Study"
249,1270984428429479939,22977318,Nuno Castro,"['In this paper with M. Romão and R. Pedro we study how to search for new phenomena at particle colliders without being biased by our assumptions on it. The idea is to use anomaly detection methods to find events that deviate from the expected background.\n<LINK> <LINK>', '@DrAndreDavid @MiguelCRomao @lipwebapps @cienciasuminho True. And also offline, contributing to explore all the data and avoiding missing an eventual discovery :-)']",https://arxiv.org/abs/2006.05432,"In this paper we propose a new strategy, based on anomaly detection methods, to search for new physics phenomena at colliders independently of the details of such new events. For this purpose, machine learning techniques are trained using Standard Model events, with the corresponding outputs being sensitive to physics beyond it. We explore three novel AD methods in HEP: Isolation Forest, Histogram Based Outlier Detection, and Deep Support Vector Data Description; alongside the most customary Autoencoder. In order to evaluate the sensitivity of the proposed approach, predictions from specific new physics models are considered and compared to those achieved when using fully supervised deep neural networks. A comparison between shallow and deep anomaly detection techniques is also presented. Our results demonstrate the potential of semi-supervised anomaly detection techniques to extensively explore the present and future hadron colliders' data. ","Finding New Physics without learning about it: Anomaly Detection as a
  tool for Searches at Colliders"
250,1270793809899765763,249424809,Shariq Iqbal,"['Excited to share our work in collaboration with @whi_rl!\nWe propose a multi-agent RL technique to improve generalization across scenarios where the types and quantities of agents change by ""imagining"" sub-scenarios.\n\n<LINK>', 'Huge thanks to my excellent co-authors @casdewitt, @bei_peng, @WendelinBoehmer, @shimon8282, @feishaAI']",https://arxiv.org/abs/2006.04222,"Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: ``What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?'' By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this ``randomly factorized"" value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings. ","Randomized Entity-wise Factorization for Multi-Agent Reinforcement
  Learning"
251,1270427831068250112,957689165902118912,Alexandre Dauphin,"['New preprint:we study the formation of polarons and domain walls on top of an interaction induced topological insulator.Interestingly, the domain walls can host chiral edge states.We finally discuss a quantum simulation of such systems with cold atoms\n➡ <LINK> <LINK>']",https://arxiv.org/abs/2006.04671,"Many-body interactions in topological quantum systems can give rise to new phases of matter, which simultaneously exhibit both rich spatial features and topological properties. In this work, we consider spinless fermions on a checkerboard lattice with nearest and next-to-nearest neighbor interactions. We calculate the phase diagram at half filling, which presents, in particular, an interaction-induced quantum anomalous Hall phase. We study the system at incommensurate fillings using an unrestricted Hartree-Fock ansatz and report a rich zoo of solutions such as self-trapped polarons and domain walls above an interaction-induced topological insulator. We find that, as a consequence of the interplay between the interaction-induced topology and topological defects, these domain walls separate two phases with opposite topological invariants and host topologically protected chiral edge states. Finally, we discuss experimental prospects to observe these novel phenomena in a quantum simulator based on laser-dressed Rydberg atoms in an optical lattice. ","Self-Trapped Polarons and Topological Defects in a Topological Mott
  Insulator"
252,1270374761055637520,938463903754862593,George Papamakarios,"['New work: The Lipschitz constant of self-attention\n\nWe show that dot-product self-attention is not Lipschitz and propose an alternative that is. Useful when Lipschitz constraints are needed, e.g. for invertible ResNets.\n\n<LINK>\n\nWork led by @hyunjik11, details 👇 <LINK>']",http://arxiv.org/abs/2006.04710,"Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task. ",The Lipschitz Constant of Self-Attention
253,1270354048558997504,18409196,Zafeirios Fountas,"['*New pre-print* with @nsajidt, @PedroMediano and Karl Friston: “Deep active inference agents using Monte-Carlo methods”: <LINK>\n\nWe propose a deep neural architecture that can be used to train scaled-up active inference agents for continuous complex …', '… environments based on amortized inference, MC tree search, MC dropouts and top-down transition precision that encourages disentangled latent representations. \n\n…', '… We test this architecture on two tasks from the Animal-AI Olympics and a new simple object-sorting task based on the dSprites dataset.\nSource code: https://t.co/zV5kSRZ5np']",https://arxiv.org/abs/2006.04176,"Active inference is a Bayesian framework for understanding biological intelligence. The underlying theory brings together perception and action under one single imperative: minimizing free energy. However, despite its theoretical utility in explaining intelligence, computational implementations have been restricted to low-dimensional and idealized situations. In this paper, we present a neural architecture for building deep active inference agents operating in complex, continuous state-spaces using multiple forms of Monte-Carlo (MC) sampling. For this, we introduce a number of techniques, novel to active inference. These include: i) selecting free-energy-optimal policies via MC tree search, ii) approximating this optimal policy distribution via a feed-forward `habitual' network, iii) predicting future parameter belief updates using MC dropouts and, finally, iv) optimizing state transition precision (a high-end form of attention). Our approach enables agents to learn environmental dynamics efficiently, while maintaining task performance, in relation to reward-based counterparts. We illustrate this in a new toy environment, based on the dSprites data-set, and demonstrate that active inference agents automatically create disentangled representations that are apt for modeling state transitions. In a more complex Animal-AI environment, our agents (using the same neural architecture) are able to simulate future state transitions and actions (i.e., plan), to evince reward-directed navigation - despite temporary suspension of visual input. These results show that deep active inference - equipped with MC methods - provides a flexible framework to develop biologically-inspired intelligent agents, with applications in both machine learning and cognitive science. ",Deep active inference agents using Monte-Carlo methods
254,1270055525334409224,771267202762670081,Mihail Eric,['Our SIGDial 2020 paper is up! 🚀 <LINK> @AmazonScience \n\nTLDR: We propose extending traditional task-oriented dialogue systems with the ability to incorporate unstructured knowledge access. We release a new dataset and subtasks for benchmarking these abilities.'],https://arxiv.org/abs/2006.03533,"Most prior work on task-oriented dialogue systems are restricted to a limited coverage of domain APIs, while users oftentimes have domain related requests that are not covered by the APIs. In this paper, we propose to expand coverage of task-oriented dialogue systems by incorporating external unstructured knowledge sources. We define three sub-tasks: knowledge-seeking turn detection, knowledge selection, and knowledge-grounded response generation, which can be modeled individually or jointly. We introduce an augmented version of MultiWOZ 2.1, which includes new out-of-API-coverage turns and responses grounded on external knowledge sources. We present baselines for each sub-task using both conventional and neural approaches. Our experimental results demonstrate the need for further research in this direction to enable more informative conversational systems. ","Beyond Domain APIs: Task-oriented Conversational Modeling with
  Unstructured Knowledge Access"
255,1270033902401286144,782297240765362180,Jannik Ehrich,['2 new preprints\n<LINK>\n<LINK>\nin which we study the thermodynamics of the optimal erasure of a memory in finite time. We generalize the famous Landauer principle which brings us one step closer to the ultimate limits ruling real-world computation'],https://arxiv.org/abs/2006.03242,"We study the thermodynamic cost associated with the erasure of one bit of information over a finite amount of time. We present a general framework for minimizing the average work required when full control of a system's microstates is possible. In addition to exact numerical results, we find simple bounds proportional to the variance of the microscopic distribution associated with the state of the bit. In the short-time limit, we get a closed expression for the minimum average amount of work needed to erase a bit. The average work associated with the optimal protocol can be up to a factor of four smaller relative to protocols constrained to end in local equilibrium. Assessing prior experimental and numerical results based on heuristic protocols, we find that our bounds often dissipate an order of magnitude less energy. ",Finite-time Landauer principle
256,1269921582228611073,1091644491969298432,DARWIN Observatory,"['We studied the sensitivity of DARWIN to solar neutrinos via elastic neutrino-electron scattering. The measurement of the pp-flux will allow us to precisely infer the electron-neutrino survival probability below 200 keV: <LINK>. Work supported by @ERC_Research <LINK>', '@animatedphysics @ERC_Research indeed and we never know what nature may have in stake for us ;-) here we describe a measurement of solar neutrinos with the goal of improving the understanding of our Sun, and also of neutrino oscillation parameter and the weak mixing angle at low energies']",https://arxiv.org/abs/2006.03114,"We detail the sensitivity of the liquid xenon (LXe) DARWIN observatory to solar neutrinos via elastic electron scattering. We find that DARWIN will have the potential to measure the fluxes of five solar neutrino components: $pp$, $^7$Be, $^{13}$N, $^{15}$O and $pep$. The precision of the $^{13}$N, $^{15}$O and $pep$ components is hindered by the double-beta decay of $^{136}$Xe and, thus, would benefit from a depleted target. A high-statistics observation of $pp$ neutrinos would allow us to infer the values of the weak mixing angle, $\sin^2\theta_w$, and the electron-type neutrino survival probability, $P_e$, in the electron recoil energy region from a few keV up to 200 keV for the first time, with relative precision of 5% and 4%, respectively, at an exposure of 300 ty. An observation of $pp$ and $^7$Be neutrinos would constrain the neutrino-inferred solar luminosity down to 0.2%. A combination of all flux measurements would distinguish between the high (GS98) and low metallicity (AGS09) solar models with 2.1-2.5$\sigma$ significance, independent of external measurements from other experiments or a measurement of $^8$B neutrinos through coherent elastic neutrino-nucleus scattering in DARWIN. Finally, we demonstrate that with a depleted target DARWIN may be sensitive to the neutrino capture process of $^{131}$Xe. ",Solar Neutrino Detection Sensitivity in DARWIN via Electron Scattering
257,1269902584481042432,316651357,Luca Pappalardo,"['Our last report about human mobility &amp; #COVID19 in Italy ➡️ <LINK>. We find striking relationships between 1⃣negative variation of mobility and net reproduction number 2⃣lockdown delay and number of confirmed SARS-CoV-2 infections  #COVID19italia <LINK>', '@aroepstorff @DinoPedreschi @SoBigData @WindTreOfficial @StampaCnr @Unipisa @FBKcom @ISSalute_it @SantAnnaPisa @kdd_lab @mesosbrodleto @suneman Here you find the complete report https://t.co/UpefmJpPFN']",https://arxiv.org/abs/2006.03141,"In 2020, countries affected by the COVID-19 pandemic implemented various non-pharmaceutical interventions to contrast the spread of the virus and its impact on their healthcare systems and economies. Using Italian data at different geographic scales, we investigate the relationship between human mobility, which subsumes many facets of the population's response to the changing situation, and the spread of COVID-19. Leveraging mobile phone data from February through September 2020, we find a striking relationship between the decrease in mobility flows and the net reproduction number. We find that the time needed to switch off mobility and bring the net reproduction number below the critical threshold of 1 is about one week. Moreover, we observe a strong relationship between the number of days spent above such threshold before the lockdown-induced drop in mobility flows and the total number of infections per 100k inhabitants. Estimating the statistical effect of mobility flows on the net reproduction number over time, we document a 2-week lag positive association, strong in March and April, and weaker but still significant in June. Our study demonstrates the value of big mobility data to monitor the epidemic and inform control interventions during its unfolding. ","The relationship between human mobility and viral transmissibility
  during the COVID-19 epidemics in Italy"
258,1268464709860294659,2695714700,René Heller,['Proudly announcing the discovery of a fascinating #exoplanet candidate (free PDF: <LINK>). We find a ~85% chance that KOI-456.04 is really a planet of 1.9 Earth radii with virtually Earth-like insolation from its sun-like host star Kepler-160. @hippke @nbatalha <LINK>'],https://arxiv.org/abs/2006.02123,"The Sun-like star Kepler-160 (KOI-456) has been known to host two transiting planets, Kepler-160 b and c, of which planet c shows substantial transit-timing variations (TTVs). We used the archival Kepler photometry of Kepler-160 to search for additional transiting planets using a combination of our Wotan detrending algorithm and our transit least-squares (TLS) detection algorithm. We also used the Mercury N-body gravity code to study the orbital dynamics of the system. First, we recovered the known transit series of planets Kepler-160 b and c. Then we found a new transiting candidate with a radius of 1.91 (+0.17, -0.14) Earth radii (R_ear), an orbital period of 378.417 (+0.028, -0.025) d, and Earth-like insolation. The vespa software predicts that this signal has an astrophysical false-positive probability of FPP_3 = 1.8e-3 when the multiplicity of the system is taken into account. Kepler vetting diagnostics yield a multiple event statistic of MES = 10.7, which corresponds to an ~85 % reliability against false alarms due to instrumental artifacts such as rolling bands. We are also able to explain the observed TTVs of planet c with the presence of a previously unknown planet. The period and mass of this new planet, however, do not match the period and mass of the new transit candidate. Our Markov chain Monte Carlo simulations of the TTVs of Kepler-160 c can be conclusively explained by a new nontransiting planet with a mass between about 1 and 100 Earth masses and an orbital period between about 7 and 50 d. We conclude that Kepler-160 has at least three planets, one of which is the nontransiting planet Kepler-160 d. The expected stellar radial velocity amplitude caused by this new planet ranges between about 1 and 20 m/s. We also find the super-Earth-sized transiting planet candidate KOI-456.04 in the habitable zone of this system, which could be the fourth planet. ","Transit least-squares survey -- III. A $1.9\,R_\oplus$ transit candidate
  in the habitable zone of Kepler-160 and a nontransiting planet characterized
  by transit-timing variations"
259,1268349117849169920,489909633,Prof. Melanie J-H,"['Using an improved ASKAP image processing pipeline we find two giant radio halos in MACSJ0553 and AS0592. Paper led by Amanda Wilber, submitted to PASA: <LINK> <LINK>']",https://arxiv.org/abs/2006.01833,"Early science observations from the Australian Square Kilometre Array Pathfinder (ASKAP) have revealed clear signals of diffuse radio emission associated with two clusters detected by the South Pole Telescope via their Sunyaev Zel'dovich signal. SPT CLJ0553-3342 (MACSJ0553.4-3342) and SPT CLJ0638-5358 (Abell S0592) are both high-mass lensing clusters that have undergone major mergers. To improve the data products of these ASKAP early science observations and create science-fidelity images of the galaxy clusters, we performed direction-dependent (DD) calibration and imaging using state-of-the-art software {\sc killMS} and {\sc DDFacet}. We find that artefacts in the ASKAP images are greatly reduced after directional calibration. Here we present our DD calibrated ASKAP radio images of both clusters showing unambiguous giant radio halos with largest linear scales of $\sim1$~Mpc. The halo in MACSJ0553.4-3342 was previously detected with GMRT observations at 323 MHz, but appears more extended in our ASKAP image. Although there is a shock detected in the thermal X-ray emission of this cluster, we find that the particle number density in the shocked region is too low to allow for the generation of a radio shock. The radio halo in Abell S0592 is a new discovery, and the Southwest border of the halo coincides with a shock detected in X-rays. We discuss the origins of these halos considering both the hadronic and turbulent re-acceleration models as well as sources of \textit{seed} electrons. This work gives a positive indication of the potential of ASKAP's Evolutionary Map of the Universe (EMU) survey in detecting intracluster medium radio sources, and showcases the improvement in data products after utilising third-generation calibration techniques. ","ASKAP reveals giant radio halos in two merging SPT galaxy clusters --
  Making the case for a direction-dependent pipeline --"
260,1268344110491955200,245345590,Haytham Fayek,"['Paper: Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data, in #IJCAI2020 @IJCAIconf.\n\nWe propose an audiovisual model with an attention mechanism to dynamically fuse A/V models for sound recognition -&gt; SOTA on Audioset.\n\n<LINK>\n\nw/ Anurag Kumar <LINK>']",https://arxiv.org/abs/2006.01595,"Recognizing sounds is a key aspect of computational audio scene analysis and machine perception. In this paper, we advocate that sound recognition is inherently a multi-modal audiovisual task in that it is easier to differentiate sounds using both the audio and visual modalities as opposed to one or the other. We present an audiovisual fusion model that learns to recognize sounds from weakly labeled video recordings. The proposed fusion model utilizes an attention mechanism to dynamically combine the outputs of the individual audio and visual models. Experiments on the large scale sound events dataset, AudioSet, demonstrate the efficacy of the proposed model, which outperforms the single-modal models, and state-of-the-art fusion and multi-modal models. We achieve a mean Average Precision (mAP) of 46.16 on Audioset, outperforming prior state of the art by approximately +4.35 mAP (relative: 10.4%). ",Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data
261,1275399447019696129,1007655562908139520,Fabian Fuchs,"['How should we leverage symmetries in point cloud tasks while maintaining detail? We propose combining the powerful concepts of self-attention and equivariance.\n\nDelighted to share the 🔥SE3-Transformer🔥 with @deworrall92, Volker Fischer &amp; @wellingmax \n\n<LINK> <LINK>']",https://arxiv.org/abs/2006.10503,"We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention. ",SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks
262,1274425129771175938,937004359,Marina M.-C. Höhne (née Vidovic),"['In our novel paper ""How much can I trust you? Quantifying Uncertainties in Explaining Neural Networks"" we found that this #cleverHansEffect could be the primary strategy for model prediction #RightForTheWrongReason - Checkout the preprint <LINK> <LINK> <LINK>']",https://arxiv.org/abs/2006.09000,"Explainable AI (XAI) aims to provide interpretations for predictions made by learning machines, such as deep neural networks, in order to make the machines more transparent for the user and furthermore trustworthy also for applications in e.g. safety-critical areas. So far, however, no methods for quantifying uncertainties of explanations have been conceived, which is problematic in domains where a high confidence in explanations is a prerequisite. We therefore contribute by proposing a new framework that allows to convert any arbitrary explanation method for neural networks into an explanation method for Bayesian neural networks, with an in-built modeling of uncertainties. Within the Bayesian framework a network's weights follow a distribution that extends standard single explanation scores and heatmaps to distributions thereof, in this manner translating the intrinsic network model uncertainties into a quantification of explanation uncertainties. This allows us for the first time to carve out uncertainties associated with a model explanation and subsequently gauge the appropriate level of explanation confidence for a user (using percentiles). We demonstrate the effectiveness and usefulness of our approach extensively in various experiments, both qualitatively and quantitatively. ","How Much Can I Trust You? -- Quantifying Uncertainties in Explaining
  Neural Networks"
263,1273788405982232576,2982971256,"Lokender Tiwari, Ph.D","['We Propose a Test Time Replacement of Softmax for Robust Predictions. We call our method--REGroup.\n\nPaper: <LINK>\nDemo code: <LINK>\n#REGroup  #robust #ML #adversarial @icmlconf @CVPRConf @eccvconf  @OfficialIndiaAI @IIITDelhi @iitdelhi <LINK>', '@Montreal_AI @Deep__AI', '@paperswithcode']",https://arxiv.org/abs/2006.10679,"Deep Neural Networks (DNNs) are often criticized for being susceptible to adversarial attacks. Most successful defense strategies adopt adversarial training or random input transformations that typically require retraining or fine-tuning the model to achieve reasonable performance. In this work, our investigations of intermediate representations of a pre-trained DNN lead to an interesting discovery pointing to intrinsic robustness to adversarial attacks. We find that we can learn a generative classifier by statistically characterizing the neural response of an intermediate layer to clean training samples. The predictions of multiple such intermediate-layer based classifiers, when aggregated, show unexpected robustness to adversarial attacks. Specifically, we devise an ensemble of these generative classifiers that rank-aggregates their predictions via a Borda count-based consensus. Our proposed approach uses a subset of the clean training data and a pre-trained model, and yet is agnostic to network architectures or the adversarial attack generation method. We show extensive experiments to establish that our defense strategy achieves state-of-the-art performance on the ImageNet validation set. ","REGroup: Rank-aggregating Ensemble of Generative Classifiers for Robust
  Predictions"
264,1273559540374568962,965547376340619264,Carl Allen,"['As a principled step towards combining statistical learning (#NeuralNetworks) and logical reasoning (#symbolism), we propose a probabilistic model for semi-supervised learning in which the distribution over model outputs can be defined by logical rules <LINK> <LINK>', 'Work with @ibalazevic and @tmh31. \n@SamsungUK @EdiDataScience @InfAtEd']",https://arxiv.org/abs/2006.05896,"Much progress has been made in semi-supervised learning (SSL) by combining methods that exploit different aspects of the data distribution, e.g. consistency regularisation relies on properties of $p(x)$, whereas entropy minimisation pertains to the label distribution $p(y|x)$. Focusing on the latter, we present a probabilistic model for discriminative SSL, that mirrors its classical generative counterpart. Under the assumption $y|x$ is deterministic, the prior over latent variables becomes discrete. We show that several well-known SSL methods can be interpreted as approximating this prior, and can be improved upon. We extend the discriminative model to neuro-symbolic SSL, where label features satisfy logical rules, by showing such rules relate directly to the above prior, thus justifying a family of methods that link statistical learning and logical reasoning, and unifying them with regular SSL. ","A Probabilistic Model for Discriminative and Neuro-Symbolic
  Semi-Supervised Learning"
265,1271513269413359618,742282664,Cory Simon,"['we placed a theoretical upper bound on the deliverable capacity of gas in a porous material.\nwe find that the US DOE deliverable capacity targets for adsorbed CH4 and H2 storage are probably impossible in *rigid* adsorbents.\n\n<LINK>', 'collaboration with David Roundy in Physics.\n\nplease provide feedback on our preprint!', ""@ZeoliteMiFi no, I haven't seen his work. thanks for pointing this out! we shall cite it. phew, looks like this isn't the same as our approach (::wipes sweat from forehead::)."", '@SimonKrause39 @ZeoliteMiFi yes! if the porous material can squeeze out the ""cushion gas"" at the discharge pressure...\n\nhttps://t.co/PopxqQ8MQM\n\nthough I wonder if these materials are stable under repeated opening/closing.\n\nhttps://t.co/oolpkCIY3h']",https://arxiv.org/abs/2006.01886,"Both hydrogen and natural gas are challenging to economically store onboard vehicles as fuels, due to their low volumetric energy density at ambient conditions. One strategy to densify these gases is to pack the fuel tank with a porous adsorbent material. The US Department of Energy (DOE) has set volumetric deliverable capacity targets which, if met, would help enable commercial adoption of hydrogen/natural gas as transportation fuels. Here, we present a theoretical upper bound on the deliverable capacity of a gas in a rigid porous material via an isothermal pressure swing. To provide an extremum, we consider a substrate that provides a spatially uniform potential energy field for the gas. Our bound relies directly on experimentally measured properties of the pure gas. We conclude that the deliverable capacity targets set by the DOE for room-temperature natural gas and hydrogen storage are just barely theoretically possible. The targets are likely to be impossible for any real, rigid porous material because of steric repulsion, which reduces the deliverable capacity below our upper bound. Limitations to the scope of applicability of our upper bound may guide fuel tank design and future material development. Firstly, one could avoid using an isothermal pressure swing by heating the adsorbent to drive off trapped, residual gas. Secondly, our upper bound assumes the material does not change its structure in response to adsorbed gas, suggesting that flexible materials could still satisfy the DOE targets. ","An upper bound to gas storage and delivery via pressure-swing adsorption
  in porous materials"
266,1271156830803234820,312448486,Dr. Karan Jani,"[""In a new paper led by @GTSciences researcher Deborah Ferguson, we find that the most accurate solutions to Einstein's Equations (on fastest supercomputers!) are just not good enough for the next-gen gravitational-wave detectors.\n\n<LINK>\n\nSubmitted to @PhysRevLett <LINK>""]",https://arxiv.org/abs/2006.04272,"Future detectors such as LISA promise signal-to-noise ratios potentially in the thousands and data containing simultaneous signals. Accurate numerical relativity waveforms will be essential to maximize the science return. A question of interest to the broad gravitational wave community is: Are the numerical relativity codes ready to face this challenge? Towards answering this question, we provide a new criteria to identify the minimum resolution a simulation must have as a function of signal-to-noise ratio in order for the numerical relativity waveform to be indistinguishable from a true signal. This criteria can be applied to any finite-differencing numerical relativity code with multiple simulations of differing resolutions for the desired binary parameters and waveform length. We apply this criteria to binary systems of interest with the fourth-order MAYA code to obtain the first estimate of the minimum resolution a simulation must have to be prepared for next generation detectors. ","Assessing the Readiness of Numerical Relativity for LISA and 3G
  Detectors"
267,1270995455560708097,1270993967698423810,Yanqiao ZHU,"['Our recent work on contrastive graph representation learning is available at <LINK>! We propose a novel framework based on maximizing the agreement of node representations in two graph views, generated by corruption at both structure and attributes levels.', 'Code: https://t.co/fFfjrOysdA']",https://arxiv.org/abs/2006.04131,"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications. ",Deep Graph Contrastive Representation Learning
268,1270391472769183745,1107661099904782336,Stefan Leutenegger,"['Excited about this #RSS2020 paper with Dimos Tzoumanikas, Felix Graule, Cheryl Yan, Dhruv Shah and Marija Popovic. We propose a hybrid force and position NMPC for aerial manipulation showcased by a writing drone. <LINK>, <LINK> via @YouTube']",https://arxiv.org/abs/2006.02116,"Aerial manipulation aims at combining the manoeuvrability of aerial vehicles with the manipulation capabilities of robotic arms. This, however, comes at the cost of the additional control complexity due to the coupling of the dynamics of the two systems. In this paper we present a NMPC specifically designed for MAVs equipped with a robotic arm. We formulate a hybrid control model for the combined MAV-arm system which incorporates interaction forces acting on the end effector. We explain the practical implementation of our algorithm and show extensive experimental results of our custom built system performing multiple aerial-writing tasks on a whiteboard, revealing accuracy in the order of millimetres. ","Aerial Manipulation Using Hybrid Force and Position NMPC Applied to
  Aerial Writing"
269,1269994582579859458,1213098396765937664,Gemma De las Cuevas,"['In this new work with David Drexel (<LINK>), we propose to describe classical spin Hamiltonians as automata. The classification of the latter (equivalent to the Chomsky hierarchy) provides a new complexity measure for classical spin models. <LINK>']",https://arxiv.org/abs/2006.03529,"We describe classical spin Hamiltonians as automata and use the classification of the latter to obtain a new complexity measure of Hamiltonians. Specifically, we associate a classical spin Hamiltonian to the formal language consisting of pairs of spin configurations and the corresponding energy, and classify this language in the Chomsky hierarchy. We prove that the language associated to (i) effectively zero-dimensional spin Hamiltonians is regular, (ii) local one-dimensional (1D) spin Hamiltonians is deterministic context-free, (iii) local two-dimensional (2D) or higher-dimensional spin Hamiltonians is context-sensitive, and (iv) totally unbounded spin Hamiltonians is recursively enumerable. It follows that only highly non-physical spin Hamiltonians [(iv)] correspond to Turing machines. It also follows that the Ising model without fields is easy or hard if defined on a 1D or 2D lattice, in contrast to the computational complexity of its ground state energy problem, where the threshold is found between planar and non-planar graphs. Our work puts classical spin Hamiltonians at the same level as automata, and paves the road toward a rigorous comparison of universal spin models and universal Turing machines. ","Describing classical spin Hamiltonians as automata: a new complexity
  measure"
270,1268711552674193409,970362575425888256,Tobias Fischer,"['Now on #arXiv: My paper with @maththrills @QUTSciEng on Place Recognition with event cameras (@_inivation). We find that processing multiple window sizes in parallel and combining them using an #ensemble scheme greatly improves recall performance. Link: <LINK> <LINK>', ""@aalhindawi @maththrills @QUTSciEng @_inivation Thanks, Ahmed! Indeed, it's our Civic :)""]",https://arxiv.org/abs/2006.02826,"Event cameras are bio-inspired sensors capable of providing a continuous stream of events with low latency and high dynamic range. As a single event only carries limited information about the brightness change at a particular pixel, events are commonly accumulated into spatio-temporal windows for further processing. However, the optimal window length varies depending on the scene, camera motion, the task being performed, and other factors. In this research, we develop a novel ensemble-based scheme for combining temporal windows of varying lengths that are processed in parallel. For applications where the increased computational requirements of this approach are not practical, we also introduce a new ""approximate"" ensemble scheme that achieves significant computational efficiencies without unduly compromising the original performance gains provided by the ensemble approach. We demonstrate our ensemble scheme on the visual place recognition (VPR) task, introducing a new Brisbane-Event-VPR dataset with annotated recordings captured using a DAVIS346 color event camera. We show that our proposed ensemble scheme significantly outperforms all the single-window baselines and conventional model-based ensembles, irrespective of the image reconstruction and feature extraction methods used in the VPR pipeline, and evaluate which ensemble combination technique performs best. These results demonstrate the significant benefits of ensemble schemes for event camera processing in the VPR domain and may have relevance to other related processes, including feature tracking, visual-inertial odometry, and steering prediction in driving. ",Event-based visual place recognition with ensembles of temporal windows
271,1267856162902487042,303027989,Akond Rahman,"['Paper accepted at @emsejournal ! We found 5 development anti-patterns for configuration scripts.  Also, development context correlates with developer perceptions for the identified anti-patterns. Pre-print: <LINK> Co-authors: @effat_farhana and @lauriewilliams']",https://arxiv.org/abs/2006.00177,"Context: The 'as code' suffix in infrastructure as code (IaC) refers to applying software engineering activities, such as version control, to maintain IaC scripts. Without the application of these activities, defects that can have serious consequences may be introduced in IaC scripts. A systematic investigation of the development anti-patterns for IaC scripts can guide practitioners in identifying activities to avoid defects in IaC scripts. Development anti-patterns are recurring development activities that relate with defective IaC scripts. Goal: The goal of this paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by identifying development activities that relate with defective IaC scripts. Methodology: We identify development anti-patterns by adopting a mixed-methods approach, where we apply quantitative analysis with 2,138 open source IaC scripts and conduct a survey with 51 practitioners. Findings: We observe five development activities to be related with defective IaC scripts from our quantitative analysis. We identify five development anti-patterns namely, 'boss is not around', 'many cooks spoil', 'minors are spoiler', 'silos', and 'unfocused contribution'. Conclusion: Our identified development anti-patterns suggest the importance of 'as code' activities in IaC because these activities are related to quality of IaC scripts. ","The 'as Code' Activities: Development Anti-patterns for Infrastructure
  as Code"
272,1267725428392853505,899968956253044737,Yanai Elazar,"[""How are different linguistic concepts *used* by models? Probing isn't the tool for answering these questions.\nWe propose a new method for answering this question: Amnesic Probing.\n\nJoint work with @ravfogel @alon_jacovi  and @yoavgo \n\n<LINK> <LINK>"", 'We show that probing performances do not correlate with property usage, and we propose an alternative: by performing an ""amnesic"" operation on the representation, we measure the performance change on the main task.', 'We study how linguistic concepts affect BERT decision on word prediction.\nAdditionally, we show inherent different behaviors by studying the masked representations of BERT, as opposed to the regular ones (used in most probing studies)', 'Finally, we use our method to re-evaluate previous claims about the ""nlp pipeline"". The situation seems to be more complicated than previously thought https://t.co/yD6Be4mWrw']",https://arxiv.org/abs/2006.00995,"A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results. ",Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals
