,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,771311569347026945,143436341,Martin g,"[""The new paper on 'Quantum Supremacy' uses the word 'error' 63 times (ish)\n<LINK>""]",http://arxiv.org/abs/1608.00263,"A critical question for the field of quantum computing in the near future is whether quantum devices without error correction can perform a well-defined computational task beyond the capabilities of state-of-the-art classical computers, achieving so-called quantum supremacy. We study the task of sampling from the output distributions of (pseudo-)random quantum circuits, a natural task for benchmarking quantum computers. Crucially, sampling this distribution classically requires a direct numerical simulation of the circuit, with computational cost exponential in the number of qubits. This requirement is typical of chaotic systems. We extend previous results in computational complexity to argue more formally that this sampling task must take exponential time in a classical computer. We study the convergence to the chaotic regime using extensive supercomputer simulations, modeling circuits with up to 42 qubits - the largest quantum circuits simulated to date for a computational task that approaches quantum supremacy. We argue that while chaotic states are extremely sensitive to errors, quantum supremacy can be achieved in the near-term with approximately fifty superconducting qubits. We introduce cross entropy as a useful benchmark of quantum circuits which approximates the circuit fidelity. We show that the cross entropy can be efficiently measured when circuit simulations are available. Beyond the classically tractable regime, the cross entropy can be extrapolated and compared with theoretical estimates of circuit fidelity to define a practical quantum supremacy test. ",Characterizing Quantum Supremacy in Near-Term Devices
1,771282920304676864,953616889,Justin Read,['Lovely paper from PhD student Kearn Grisdale today. Using the ISM as a new tool to test stellar feedback models:\n\n<LINK>'],http://arxiv.org/abs/1608.08639,"We study the impact of stellar feedback in shaping the density and velocity structure of neutral hydrogen (HI) in disc galaxies. For our analysis, we carry out $\sim 4.6$pc resolution $N$-body+adaptive mesh refinement (AMR) hydrodynamic simulations of isolated galaxies, set up to mimic a Milky Way (MW), and a Large and Small Magellanic Cloud (LMC, SMC). We quantify the density and velocity structure of the interstellar medium using power spectra and compare the simulated galaxies to observed HI in local spiral galaxies from THINGS (The HI Nearby Galaxy Survey). Our models with stellar feedback give an excellent match to the observed THINGS HI density power spectra. We find that kinetic energy power spectra in feedback regulated galaxies, regardless of galaxy mass and size, show scalings in excellent agreement with super-sonic turbulence ($E(k)\propto k^{-2}$) on scales below the thickness of the HI layer. We show that feedback influences the gas density field, and drives gas turbulence, up to large (kpc) scales. This is in stark contrast to density fields generated by large scale gravity-only driven turbulence. We conclude that the neutral gas content of galaxies carries signatures of stellar feedback on all scales. ","The impact of stellar feedback on the density and velocity structure of
  the interstellar medium"
2,771234543105748992,280083723,Yoh Tanimoto,['our new paper~ <LINK>'],http://arxiv.org/abs/1608.08903,"We consider KMS states on a local conformal net on the unit circle with respect to rotations. We prove that, if the conformal net is of type I, namely if it admits only type I DHR representations, then the extremal KMS states are the Gibbs states in an irreducible representation. Completely rational nets, the U(1)-current net, the Virasoro nets and their finite tensor products are shown to be of type I. In the completely rational case, we also give a direct proof that all factorial KMS states are Gibbs states. ",Rotational KMS states and type I conformal nets
3,770544132012072960,575456768,Starck,['New paper on Point Spread Function estimation using dimension reduction methods:\n<LINK>'],https://arxiv.org/abs/1608.08104,"Context: in large-scale spatial surveys, the Point Spread Function (PSF) varies across the instrument field of view (FOV). Local measurements of the PSFs are given by the isolated stars images. Yet, these estimates may not be directly usable for post-processings because of the observational noise and potentially the aliasing. Aims: given a set of aliased and noisy stars images from a telescope, we want to estimate well-resolved and noise-free PSFs at the observed stars positions, in particular, exploiting the spatial correlation of the PSFs across the FOV. Contributions: we introduce RCA (Resolved Components Analysis) which is a noise-robust dimension reduction and super-resolution method based on matrix factorization. We propose an original way of using the PSFs spatial correlation in the restoration process through sparsity. The introduced formalism can be applied to correlated data sets with respect to any euclidean parametric space. Results: we tested our method on simulated monochromatic PSFs of Euclid telescope (launch planned for 2020). The proposed method outperforms existing PSFs restoration and dimension reduction methods. We show that a coupled sparsity constraint on individual PSFs and their spatial distribution yields a significant improvement on both the restored PSFs shapes and the PSFs subspace identification, in presence of aliasing. Perspectives: RCA can be naturally extended to account for the wavelength dependency of the PSFs. ",Constraint matrix factorization for space variant PSFs field restoration
4,770315476476030980,523241142,Juste Raimbault,['New paper out : A Discrepancy-based Framework to Compare Robustness between Multi-Attribute Evaluations | <LINK>'],https://arxiv.org/abs/1608.00840,"Multi-objective evaluation is a necessary aspect when managing complex systems, as the intrinsic complexity of a system is generally closely linked to the potential number of optimization objectives. However, an evaluation makes no sense without its robustness being given (in the sense of its reliability). Statistical robustness computation methods are highly dependent of underlying statistical models. We propose a formulation of a model-independent framework in the case of integrated aggregated indicators (multi-attribute evaluation), that allows to define a relative measure of robustness taking into account data structure and indicator values. We implement and apply it to a synthetic case of urban systems based on Paris districts geography, and to real data for evaluation of income segregation for Greater Paris metropolitan area. First numerical results show the potentialities of this new method. Furthermore, its relative independence to system type and model may position it as an alternative to classical statistical robustness methods. ","A Discrepancy-based Framework to Compare Robustness between
  Multi-Attribute Evaluations"
5,770260053962137600,223923566,David Van Horn,"['New paper with @matthewhammer and Evan Chang to appear at @GPCECONF ""A Vision for Online Verification-Validation"" <LINK>']",http://arxiv.org/abs/1608.06012,"Today's programmers face a false choice between creating software that is extensible and software that is correct. Specifically, dynamic languages permit software that is richly extensible (via dynamic code loading, dynamic object extension, and various forms of reflection), and today's programmers exploit this flexibility to ""bring their own language features"" to enrich extensible languages (e.g., by using common JavaScript libraries). Meanwhile, such library-based language extensions generally lack enforcement of their abstractions, leading to programming errors that are complex to avoid and predict. To offer verification for this extensible world, we propose online verification-validation (OVV), which consists of language and VM design that enables a ""phaseless"" approach to program analysis, in contrast to the standard static-dynamic phase distinction. Phaseless analysis freely interposes abstract interpretation with concrete execution, allowing analyses to use dynamic (concrete) information to prove universal (abstract) properties about future execution. In this paper, we present a conceptual overview of OVV through a motivating example program that uses a hypothetical database library. We present a generic semantics for OVV, and an extension to this semantics that offers a simple gradual type system for the database library primitives. The result of instantiating this gradual type system in an OVV setting is a checker that can progressively type successive continuations of the program until a continuation is fully verified. To evaluate the proposed vision of OVV for this example, we implement the VM semantics (in Rust), and show that this design permits progressive typing in this manner. ",A Vision for Online Verification-Validation
6,769547018960248832,15904040,Renato Cunha,['Got a new paper out on using Machine Learning for supporting job placement decisions in hybrid clouds. <LINK>'],https://arxiv.org/abs/1608.06310,"Several companies and research institutes are moving their CPU-intensive applications to hybrid High Performance Computing (HPC) cloud environments. Such a shift depends on the creation of software systems that help users decide where a job should be placed considering execution time and queue wait time to access on-premise clusters. Relying blindly on turnaround prediction techniques will affect negatively response times inside HPC cloud environments. This paper introduces a tool to make job placement decisions in HPC hybrid cloud environments taking into account the inaccuracy of execution and waiting time predictions. We used job traces from real supercomputing centers to run our experiments, and compared the performance between environments using real speedup curves. We also extended a state-of-the-art machine learning based predictor to work with data from the cluster scheduler. Our main findings are: (i) depending on workload characteristics, there is a turning point where predictions should be disregarded in favor of a more conservative decision to minimize job turnaround times and (ii) scheduler data plays a key role in improving predictions generated with machine learning using job trace data---our experiments showed around 20% prediction accuracy improvements. ","Job Placement Advisor Based on Turnaround Predictions for HPC Hybrid
  Clouds"
7,769212530862325761,18644734,Dr. Alex Parker,"['Does Charon have an atmosphere? We checked New Horizons data and found no evidence for one! Paper led by @AlanStern: <LINK>', ""From UV occultation, surface pressure of a Charon N2 atmosphere must be over a million times lower than Pluto's. https://t.co/NxgXfZBbOi"", ""From crescent images processed by @TodLauer, I showed that any Charon hazes are &gt;7000 times fainter than Pluto's. https://t.co/HF1MqZqPxd"", ""@Katrina13J @AlanStern It's a word now!""]",http://arxiv.org/abs/1608.06955,"We report on a variety of standard techniques used by New Horizons including a solar ultraviolet occultation, ultraviolet airglow observations, and high-phase look-back particulate search imaging to search for an atmosphere around Pluto's large moon Charon during its flyby in July 2015. Analyzing these datasets, no evidence for a present day atmosphere has been found for 14 potential atomic and molecular species, all of which are now constrained to have pressures below 0.3 nanobar, as we describe below, these are much more stringent upper limits than the previously available 15-110 nanobar constraints (e.g., Sicardy et al. 2006); for example, we find a 3$\sigma$ upper limit for an N$_2$ atmosphere on Charon is 4.2 picobars and a 3$\sigma$ upper limit for the brightness of any atmospheric haze on Charon of I/F=2.6x10$^{-5}$. A radio occultation search for an atmosphere around Charon was also conducted by New Horizons but will be published separately by other authors. ",New Horizons Constraints on Charon's Present Day Atmosphere
8,769008068755677184,47192475,John Gizis,['Our new paper on non-habitable planet orbiting nearby star: Spitzer Space Telescope Mid-IR Light Curves of Neptune <LINK>'],http://arxiv.org/abs/1608.07198,"We have used the Spitzer Space Telescope in February 2016 to obtain high cadence, high signal-to-noise, 17-hour duration light curves of Neptune at 3.6 and 4.5 $\mu$m. The light curve duration was chosen to correspond to the rotation period of Neptune. Both light curves are slowly varying with time, with full amplitudes of 1.1 mag at 3.6 $\mu$m and 0.6 mag at 4.5 $\mu$m. We have also extracted sparsely sampled 18-hour light curves of Neptune at W1 (3.4 $\mu$m) and W2 (4.6 $\mu$m) from the WISE/NEOWISE archive at six epochs in 2010-2015. These light curves all show similar shapes and amplitudes compared to the Spitzer light curves but with considerable variation from epoch to epoch. These amplitudes are much larger than those observed with Kepler/K2 in the visible (amplitude $\sim$0.02 mag) or at 845 nm with the Hubble Space Telescope in 2015 and at 763 nm in 2016 (amplitude $\sim$ 0.2 mag). We interpret the Spitzer and WISE light curves as arising entirely from reflected solar photons, from higher levels in Neptune's atmosphere than for K2. Methane gas is the dominant opacity source in Neptune's atmosphere, and methane absorption bands are present in the HST 763, and 845 nm, WISE W1, and Spitzer 3.6 $\mu$m filters. ",Spitzer Space Telescope Mid-IR Light Curves of Neptune
9,768339689996705792,45105022,Riccardo Sapienza,['New paper: How to use a #laser as a miniaturised #sensor <LINK>'],http://arxiv.org/abs/1608.06303,"Here we report a random lasing based sensor which shows pH sensitivity exceeding by 2-orders of magnitude that of a conventional fluorescence sensor. We explain the sensing mechanism as related to gain modifications and lasing threshold nonlinearities. A dispersive diffusive lasing theory matches well the experimental results, and allow us to predict the optimal sensing conditions and a maximal sensitivity as large as 200 times that of an identical fluorescence-based sensor. The simplicity of operation and high sensitivity make it promising for future biosensing applications. ",Explaining the mechanism of random lasing based sensing
10,768117501771255808,17721461,Dan Gezelter,['Our new paper on CO-induced restructuring of stepped Pt surfaces is out! <LINK> <LINK>'],http://arxiv.org/abs/1608.05833,"The effects of plateau width and step edge kinking on carbon monoxide (CO)-induced restructuring of platinum surfaces were explored using molecular dynamics (MD) simulations. Platinum crystals displaying four different vicinal surfaces [(321), (765), (112), and (557)] were constructed and exposed to partial coverages of carbon monoxide. Platinum-CO interactions were fit to recent experimental data and density functional theory (DFT) calculations, providing a classical interaction model that captures the atop binding preference on Pt. The differences in Pt-Pt binding strength between edge atoms on the various facets were found to play a significant role in step edge wandering and reconstruction events. Because the mechanism for step doubling relies on a stochastic meeting of two wandering edges, the widths of the plateaus on the original surfaces was also found to play a role in these reconstructions. On the Pt(321) surfaces, the CO adsorbate was found to assist in reordering the kinked step edges into straight {100} edge segments. ","CO-Induced Restructuring on Stepped Pt Surfaces: A Molecular Dynamics
  Study"
11,767977201170022400,487381183,Leto Peel,['Our new paper on ground truth and community detection w/ @aaronclauset @DanLarremore <LINK> <LINK>'],http://arxiv.org/abs/1608.05878,"Across many scientific domains, there is a common need to automatically extract a simplified view or coarse-graining of how a complex system's components interact. This general task is called community detection in networks and is analogous to searching for clusters in independent vector data. It is common to evaluate the performance of community detection algorithms by their ability to find so-called ""ground truth"" communities. This works well in synthetic networks with planted communities because such networks' links are formed explicitly based on those known communities. However, there are no planted communities in real world networks. Instead, it is standard practice to treat some observed discrete-valued node attributes, or metadata, as ground truth. Here, we show that metadata are not the same as ground truth, and that treating them as such induces severe theoretical and practical problems. We prove that no algorithm can uniquely solve community detection, and we prove a general No Free Lunch theorem for community detection, which implies that there can be no algorithm that is optimal for all possible community detection tasks. However, community detection remains a powerful tool and node metadata still have value so a careful exploration of their relationship with network structure can yield insights of genuine worth. We illustrate this point by introducing two statistical techniques that can quantify the relationship between metadata and community structure for a broad class of models. We demonstrate these techniques using both synthetic and real-world networks, and for multiple types of metadata and community structure. ",The ground truth about metadata and community detection in networks
12,767970811701362688,1430013038,James McGree,['New paper on #arXiv: <LINK> #Bayesian #designofexperiments'],http://arxiv.org/abs/1608.05815,"The generation of decision-theoretic Bayesian optimal designs is complicated by the significant computational challenge of minimising an analytically intractable expected loss function over a, potentially, high-dimensional design space. A new general approach for approximately finding Bayesian optimal designs is proposed which uses computationally efficient normal-based approximations to posterior summaries to aid in approximating the expected loss. This new approach is demonstrated on illustrative, yet challenging, examples including hierarchical models for blocked experiments, and experimental aims of parameter estimation and model discrimination. Where possible, the results of the proposed methodology are compared, both in terms of performance and computing time, to results from using computationally more expensive, but potentially more accurate, Monte Carlo approximations. Moreover the methodology is also applied to problems where the use of Monte Carlo approximations is computationally infeasible. ","An approach for finding fully Bayesian optimal designs using
  normal-based approximations to loss functions"
13,766729278373371905,29405175,sylvain Gigan,"[""new paper from the group : bispectrum imaging through a scattering layer, congrats Tengfei Wu, from Xi'an University <LINK>""]",http://arxiv.org/abs/1608.05200,"Recently introduced speckle-correlations based techniques enable noninvasive imaging of objects hidden behind scattering layers. In these techniques the hidden object Fourier amplitude is retrieved from the scattered light autocorrelation, and the lost Fourier phase is recovered via iterative phase-retrieval algorithms, which suffer from convergence to wrong local-minima solutions and cannot solve ambiguities in object-orientation. Here, inspired by notions used in astronomy, we experimentally demonstrate that in addition to Fourier amplitude, the object phase information is naturally and inherently encoded in scattered light bispectrum (the Fourier transform of triple-correlation), and can also be extracted from a single high-resolution speckle pattern, based on which we present a single-shot imaging scheme to deterministically and unambiguously retrieve diffraction-limited images of objects hidden behind scattering layers. ","Single-shot diffraction-limited imaging through scattering layers via
  bispectrum analysis"
14,766610969502027776,1342630356,Andreas K√ºpper,['Hercules exploded: my new paper with @michelle_lmc and @eteq is out on astro-ph today <LINK>'],http://arxiv.org/abs/1608.05085,"The ultra-faint satellite galaxy Hercules has a strongly elongated and irregular morphology with detections of tidal features up to 1.3 deg (3 kpc) from its center. This suggests that Hercules may be dissolving under the Milky Way's gravitational influence, and hence could be a tidal stream in formation rather than a bound, dark-matter dominated satellite. Using Bayesian inference in combination with N-body simulations, we show that Hercules has to be on a very eccentric orbit (epsilon~0.95) within the Milky Way in this scenario. On such an orbit, Hercules ""explodes"" as a consequence of the last tidal shock at pericenter 0.5 Gyr ago. It is currently decelerating towards apocenter of its orbit with a velocity of V=157 km/s -- of which 99% is directed radially outwards. Due to differential orbital precession caused by the non-spherical nature of the Galactic potential, its debris fans out nearly perpendicular to its orbit. This explains why Hercules has an elongated shape without showing a distance gradient along its main body: it is in fact a stream that is significantly broader than it is long. In other words, it is moving perpendicular to its apparent major axis. In this scenario, there is a spike in the radial velocity profile created by the dominant debris component that formed through the last pericenter passage. This is similar to kinematic substructure that is observed in the real Hercules. Modeling a satellite on such a highly eccentric orbit is strongly dependent on the form of the Galactic potential. We therefore propose that detailed kinematic investigation of Hercules and other exploding satellite candidates can yield strong constraints on the potential of the Milky Way. ","Exploding Satellites -- The Tidal Debris of the Ultra-Faint Dwarf Galaxy
  Hercules"
15,766263248287129600,17721461,Dan Gezelter,['Our new paper on real-space electrostatics (and dielectric properties) is out! <LINK>  <LINK>  #compchem'],http://arxiv.org/abs/1608.04970,"In the first two papers in this series, we developed new shifted potential (SP), gradient shifted force (GSF), and Taylor shifted force (TSF) real-space methods for multipole interactions in condensed phase simulations. Here, we discuss the dielectric properties of fluids that emerge from simulations using these methods. Most electrostatic methods (including the Ewald sum) require correction to the conducting boundary fluctuation formula for the static dielectric constants, and we discuss the derivation of these corrections for the new real space methods. For quadrupolar fluids, the analogous material property is the quadrupolar susceptibility. As in the dipolar case, the fluctuation formula for the quadrupolar susceptibility has corrections that depend on the electrostatic method being utilized. One of the most important effects measured by both the static dielectric and quadrupolar susceptibility is the ability to screen charges embedded in the fluid. We use potentials of mean force between solvated ions to discuss how geometric factors can lead to distance-dependent screening in both quadrupolar and dipolar fluids. ",Real space electrostatics for multipoles. III. Dielectric Properties
16,765533329156345856,279973583,Rudra Poudel,['New paper: Recurrent Fully Convolutional Neural Networks for multi-slice image/video segmentation- <LINK>'],http://arxiv.org/abs/1608.03974,"In cardiac magnetic resonance imaging, fully-automatic segmentation of the heart enables precise structural and functional measurements to be taken, e.g. from short-axis MR images of the left-ventricle. In this work we propose a recurrent fully-convolutional network (RFCN) that learns image representations from the full stack of 2D slices and has the ability to leverage inter-slice spatial dependences through internal memory units. RFCN combines anatomical detection and segmentation into a single architecture that is trained end-to-end thus significantly reducing computational time, simplifying the segmentation pipeline, and potentially enabling real-time applications. We report on an investigation of RFCN using two datasets, including the publicly available MICCAI 2009 Challenge dataset. Comparisons have been carried out between fully convolutional networks and deep restricted Boltzmann machines, including a recurrent version that leverages inter-slice spatial correlation. Our studies suggest that RFCN produces state-of-the-art results and can substantially improve the delineation of contours near the apex of the heart. ","Recurrent Fully Convolutional Neural Networks for Multi-slice MRI
  Cardiac Segmentation"
17,765451802213027840,17055506,Martin Kleppmann,"['New paper by @arberesford and me: ‚ÄúA Conflict-Free Replicated JSON Datatype‚Äù ‚Äî we figured out a JSON CRDT <LINK>', '@bensummers @arberesford It assumes that every edit operation is recorded, so you can‚Äôt quite diff two arbitrary documents.', '@drsm79 If I read CouchDB docs right, you get ‚Äúlatest‚Äù version by default, have to specifically ask for multi-version if you want to merge?', '@samstokes @arberesford Yes! Actually I first tried to make Avro work, but the schema checking made it even more complex. Working on it.', '@tsantero @arberesford Work in progress, will post an update once the code is a bit more respectable.', '@hpgrahsl @arberesford Has some similarity. Riak‚Äôs CRDTs are state-based and don‚Äôt support ordered lists, only maps. Ours is op-based.', '@rymohr @arberesford Fair point, it‚Äôs not ideal, but it satisfies the conventional definition of ‚Äúconflict-free‚Äù as in CRDTs.', '@rymohr @arberesford OT suffers from exactly the same issue ‚Äî with concurrent assignment it must either preserve both values or use LWW', '@rymohr @arberesford With regard to individual field assignment, I believe so. They resolve conflicts on maps, lists, and collaborative str.']",http://arxiv.org/abs/1608.03960,"Many applications model their data in a general-purpose storage format such as JSON. This data structure is modified by the application as a result of user input. Such modifications are well understood if performed sequentially on a single copy of the data, but if the data is replicated and modified concurrently on multiple devices, it is unclear what the semantics should be. In this paper we present an algorithm and formal semantics for a JSON data structure that automatically resolves concurrent modifications such that no updates are lost, and such that all replicas converge towards the same state (a conflict-free replicated datatype or CRDT). It supports arbitrarily nested list and map types, which can be modified by insertion, deletion and assignment. The algorithm performs all merging client-side and does not depend on ordering guarantees from the network, making it suitable for deployment on mobile devices with poor network connectivity, in peer-to-peer networks, and in messaging systems with end-to-end encryption. ",A Conflict-Free Replicated JSON Datatype
18,765444994333278208,112462367,Dan Lucas,['Preprint of our new paper on precession resonance in deep water surface gravity waves is now up on the arXiv <LINK>'],http://arxiv.org/abs/1608.04241,"We describe the theory and present numerical evidence for a new type of nonlinear resonant interaction between gravity waves on the surface of deep water. The resonance constitutes a generalisation of the usual 'exact' resonance as we show that exchanges of energy between the waves can be enhanced when the interaction is three-wave rather than four and the linear frequency mismatch, or detuning, is non-zero i.e. $\omega_1\pm\omega_2\pm\omega_3 \neq0.$ This is possible because the resonance condition is now a match between the so-called 'precession frequency' of a given $\textit{triad interaction}$ and an existent nonlinear frequency in the system. In the limit of weak nonlinearity this precession frequency is simply due to the linear 'drift' of the triad phase; therefore, it tends toward the detuning. This means precession resonance of this type can occur at finite amplitudes, with nonlinear corrections contributing to the resonance. We report energy transfer efficiencies of up to 40%, depending on the model options. To the authors' knowledge this represents the first new type of nonlinear resonance in surface gravity waves since the seminal work of Benjamin & Feir (1967). ",Precession resonance in water waves
19,765362275096129536,14594687,R. Ryan Williams,"['New paper is up: Faster approximate Nearest Nbrs, larger circuit lower bounds, cooler polynomial tricksüìà üìâüìàüìâüòé <LINK>', ""@deliprao we didn't calculate them explicitly ... the reduction of such constants is left as an open problem :)""]",http://arxiv.org/abs/1608.04355,"We design new polynomials for representing threshold functions in three different regimes: probabilistic polynomials of low degree, which need far less randomness than previous constructions, polynomial threshold functions (PTFs) with ""nice"" threshold behavior and degree almost as low as the probabilistic polynomials, and a new notion of probabilistic PTFs where we combine the above techniques to achieve even lower degree with similar ""nice"" threshold behavior. Utilizing these polynomial constructions, we design faster algorithms for a variety of problems: $\bullet$ Offline Hamming Nearest (and Furthest) Neighbors: Given $n$ red and $n$ blue points in $d$-dimensional Hamming space for $d=c\log n$, we can find an (exact) nearest (or furthest) blue neighbor for every red point in randomized time $n^{2-1/O(\sqrt{c}\log^{2/3}c)}$ or deterministic time $n^{2-1/O(c\log^2c)}$. These also lead to faster MAX-SAT algorithms for sparse CNFs. $\bullet$ Offline Approximate Nearest (and Furthest) Neighbors: Given $n$ red and $n$ blue points in $d$-dimensional $\ell_1$ or Euclidean space, we can find a $(1+\epsilon)$-approximate nearest (or furthest) blue neighbor for each red point in randomized time near $dn+n^{2-\Omega(\epsilon^{1/3}/\log(1/\epsilon))}$. $\bullet$ SAT Algorithms and Lower Bounds for Circuits With Linear Threshold Functions: We give a satisfiability algorithm for $AC^0[m]\circ LTF\circ LTF$ circuits with a subquadratic number of linear threshold gates on the bottom layer, and a subexponential number of gates on the other layers, that runs in deterministic $2^{n-n^\epsilon}$ time. This also implies new circuit lower bounds for threshold circuits. We also give a randomized $2^{n-n^\epsilon}$-time SAT algorithm for subexponential-size $MAJ\circ AC^0\circ LTF\circ AC^0\circ LTF$ circuits, where the top $MAJ$ gate and middle $LTF$ gates have $O(n^{6/5-\delta})$ fan-in. ","Polynomial Representations of Threshold Functions and Algorithmic
  Applications"
20,763496402311839744,16176602,Hilding Neilson,['My new paper: Interferometry + Spectra + model stellar atmospheres = awesome <LINK>'],https://arxiv.org/abs/1608.02602,"One of the great challenges in understanding stars is measuring their masses. The best methods for measuring stellar masses include binary interaction, asteroseismology and stellar evolution models, but these methods are not ideal for red giant and supergiant stars. In this work, we propose a novel method for inferring stellar masses of evolved red giant and supergiant stars using interferometric and spectrophotometric observations combined with spherical model stellar atmospheres to measure what we call the stellar mass index, defined as the ratio between the stellar radius and mass. The method is based on the correlation between different measurements of angular diameter, used as a proxy for atmospheric extension, and fundamental stellar parameters. For a given star, spectrophotometry measures the Rosseland angular diameter while interferometric observations generally probe a larger limb-darkened angular diameter. The ratio of these two angular diameters is proportional to the relative extension of the stellar atmosphere, which is strongly correlated to the star's effective temperature, radius and mass. We show that these correlations are strong and can lead to precise measurements of stellar masses. ","Stellar atmospheres, atmospheric extension and fundamental parameters:
  weighing stars using the stellar mass index"
21,763396285244252160,35346624,Jaroslaw Zola,['Preprint of our new paper on exact structure learning of BN is now available! <LINK>'],http://arxiv.org/abs/1608.02682,"Bayesian networks are probabilistic graphical models often used in big data analytics. The problem of exact structure learning is to find a network structure that is optimal under certain scoring criteria. The problem is known to be NP-hard and the existing methods are both computationally and memory intensive. In this paper, we introduce a new approach for exact structure learning. Our strategy is to leverage relationship between a partial network structure and the remaining variables to constraint the number of ways in which the partial network can be optimally extended. Via experimental results, we show that the method provides up to three times improvement in runtime, and orders of magnitude reduction in memory consumption over the current best algorithms. ",Exact Structure Learning of Bayesian Networks by Optimal Path Extension
22,763375572684722176,737140237948715008,Michael J. Biercuk,"['.@DaveWecker New paper: ""Assembly language for efficient physical-layer classical control in quantum processors"" <LINK>']",http://arxiv.org/abs/1608.02607,"The rapid progress seen in the development of quantum coherent devices for information processing has motivated serious consideration of quantum computer architecture and organization. One topic which remains open for investigation and optimization relates to the design of the classical-quantum interface, where control operations on individual qubits are applied according to higher-level algorithms; accommodating competing demands on performance and scalability remains a major outstanding challenge. In this work we present a resource-efficient, scalable framework for the implementation of embedded physical-layer classical controllers for quantum information systems. Design drivers and key functionalities are introduced, leading to the selection of Walsh functions as an effective functional basis for both programming and controller hardware implementation. This approach leverages the simplicity of real-time Walsh-function generation in classical digital hardware, and the fact that a wide variety of physical-layer controls such as dynamic error suppression are known to fall within the Walsh family. We experimentally implement a real-time FPGA-based Walsh controller producing Walsh timing signals and Walsh-synthesized analog waveforms appropriate for critical tasks in error-resistant quantum control and noise characterization. These demonstrations represent the first step towards a unified framework for the realization of physical-layer controls compatible with large-scale quantum information processing. ","A functional basis for efficient physical-layer classical control in
  quantum processors"
23,762813909426315264,1041578714,Benjamin Pope,"['New paper in Radiocarbon now on arXiv: looking for supernovae in C14. Non detection is interesting! <LINK>', ""We had pretty sensitive measurements and didn't detect any - despite previous published claims. Relevant to Miyake events like 773-4 AD"", 'More coming soon on Miyake events, another Dee et al recently accepted to Proc A...', 'Miyake events are sudden spikes in atmospheric radionuclides. Pretty clearly solar activity now. But must be huge CMEs or unusually targeted', 'Original 774 AD Miyake paper: https://t.co/prlEfWWsQs', 'And 994 AD: https://t.co/5orl2I5U3y']",http://arxiv.org/abs/1608.02308,"Single-year spikes in radiocarbon production are caused by intense bursts of radiation from space. Supernovae emit both high-energy particle and electromagnetic radiation, but it is the latter that is most likely to strike the atmosphere all at once and cause a surge in 14C production. In the 1990s, it was claimed that the supernova in 1006 CE produced exactly this effect. With the 14C spikes in the years 775 and 994 CE now attributed to extreme solar events, attention has returned to the question of whether historical supernovae are indeed detectable using annual 14C measurements. Here, we combine new and existing measurements over six documented and putative supernovae, and conclude that no such astrophysical event has yet left a distinct imprint on the past atmospheric 14C record. ","Supernovae and Single-Year Anomalies in the Atmospheric Radiocarbon
  Record"
24,761517750590332928,611769076,Tim Lichtenberg,['New #MNRAS paper about supernova pollution of young planetary systems with @mmcra. <LINK> <LINK>'],http://arxiv.org/abs/1608.01435,"Heating by short-lived radioisotopes (SLRs) such as aluminum-26 and iron-60 fundamentally shaped the thermal history and interior structure of Solar System planetesimals during the early stages of planetary formation. The subsequent thermo-mechanical evolution, such as internal differentiation or rapid volatile degassing, yields important implications for the final structure, composition and evolution of terrestrial planets. SLR-driven heating in the Solar System is sensitive to the absolute abundance and homogeneity of SLRs within the protoplanetary disk present during the condensation of the first solids. In order to explain the diverse compositions found for extrasolar planets, it is important to understand the distribution of SLRs in active planet formation regions (star clusters) during their first few Myr of evolution. By constraining the range of possible effects, we show how the imprint of SLRs can be extrapolated to exoplanetary systems and derive statistical predictions for the distribution of aluminum-26 and iron-60 based on N-body simulations of typical to large clusters (1000-10000 stars) with a range of initial conditions. We quantify the pollution of protoplanetary disks by supernova ejecta and show that the likelihood of enrichment levels similar to or higher than the Solar System can vary considerably, depending on the cluster morphology. Furthermore, many enriched systems show an excess in radiogenic heating compared to Solar System levels, which implies that the formation and evolution of planetesimals could vary significantly depending on the birth environment of their host stars. ","Isotopic enrichment of forming planetary systems from supernova
  pollution"
25,761509513547681793,1283150444,Maurizio Pierini,['New @HEPfit paper on current EW precision &amp; #HiggsBoson coupling analysis and future outlook  #FCC #CEPC #ILC  <LINK>'],http://arxiv.org/abs/1608.01509,"We present results from a state-of-the-art fit of electroweak precision observables and Higgs-boson signal-strength measurements performed using 7 and 8 TeV data from the Large Hadron Collider. Based on the HEPfit package, our study updates the traditional fit of electroweak precision observables and extends it to include Higgs-boson measurements. As a result we obtain constraints on new physics corrections to both electroweak observables and Higgs-boson couplings. We present the projected accuracy of the fit taking into account the expected sensitivities at future colliders. ","Electroweak precision observables and Higgs-boson signal strengths in
  the Standard Model and beyond: present and future"
26,761119779432169472,1707692827,Paul McMillan,"['My new paper is now on the arXiv - ""The mass distribution and gravitational potential of the Milky Way"" : <LINK>', ""I've also put some relevant code (to use the gravitational potentials described) on GitHub: https://t.co/DhKFTJHUG8"", '@davidwhogg @jobovy I think the referee would have insisted it was ""There Is No Distinct Local Standard of Rest"" anyway.', '@davidwhogg @jobovy Certainly a very large difference from value derived locally by SB&amp;D requires odd properties for the masers', ""@jobovy @davidwhogg @rdrimmel Ah, I did forget that. Apologies. Will fix in revision. Can't pretend to really understand how that all fits."", '@rdrimmel To answer q: some perturb from spiral but ‚â≤7kms (can tell from symmetry of terminal vel curve &amp; low vel disp of local young stars)']",http://arxiv.org/abs/1608.00971,"We present mass models of the Milky Way created to fit observational constraints and to be consistent with expectations from theoretical modelling. The method used to create these models is that demonstrated in McMillan (2011), and we improve on those models by adding gas discs to the potential, considering the effects of allowing the inner slope of the halo density profile to vary, and including new observations of maser sources in the Milky Way amongst the new constraints. We provide a best fitting model, as well as estimates of the properties of the Milky Way. Under the assumptions in our main model, we find that the Sun is $R_0 = (8.20\pm0.09)\,\mathrm{kpc}$ from the Galactic Centre, with the circular speed at the Sun being $v_0 = (232.8\pm3.0)\,\mathrm{km}\,\mathrm{s}^{-1}$; that the Galaxy has a total stellar mass of $(54.3\pm5.7)\times10^9\,{\rm M}_\odot$, a total virial mass of $(1.30 \pm 0.30)\times10^{12}\,{\rm M}_\odot$ and a local dark-matter density of $0.38\pm0.04\,\mathrm{GeV\,cm}^{-3}$, where the quoted uncertainties are statistical. These values are sensitive to our choice of priors and constraints. We investigate systematic uncertainties, which in some cases may be larger. For example, if we weaken our prior on $R_0$, we find it to be $(7.97\pm0.15)\,\mathrm{kpc}$ and that $v_0=(226.8\pm4.2)\,\mathrm{km}\,\mathrm{s}^{-1}$. We find that most of these properties, including the local dark-matter density, are remarkably insensitive to the assumed power-law density slope at the centre of the dark-matter halo. We find that it is unlikely that the local standard of rest differs significantly from that found under assumptions of axisymmetry. We have made code to compute the force from our potential, and to integrate orbits within it, publicly available. ",The mass distribution and gravitational potential of the Milky Way
27,761029391681933312,96779364,Arnab Bhattacharyya,"['New paper on testing whether there exists a sparse representation for given set of vectors: <LINK>', ""@lreyzin Hmm, intriguing. Must admit haven't thought in this direction but don't know of a direct connection.""]",http://arxiv.org/abs/1608.01275,"Sparsity is a basic property of real vectors that is exploited in a wide variety of applications. In this work, we describe property testing algorithms for sparsity that observe a low-dimensional projection of the input. We consider two settings. In the first setting, for a given design matrix A in R^{d x m}, we test whether an input vector y in R^d equals Ax for some k-sparse unit vector x. Our algorithm projects the input onto O(k \eps^{-2} log m) dimensions, accepts if the property holds, rejects if ||y - Ax|| > \eps for any O(k/\eps^2)-sparse vector x, and runs in time polynomial in m. Our algorithm is based on the approximate Caratheodory's theorem. Previously known algorithms that solve the problem for arbitrary A with qualitatively similar guarantees run in exponential time. In the second setting, the design matrix A is unknown. Given input vectors y_1, y_2,...,y_p in R^d whose concatenation as columns forms Y in R^{d x p} , the goal is to decide whether Y=AX for matrices A in R^{d x m} and X in R^{m x p} such that each column of X is k-sparse, or whether Y is ""far"" from having such a decomposition. We give such a testing algorithm which projects the input vectors to O(log p/\eps^2) dimensions and assumes that the unknown A satisfies k-restricted isometry. Our analysis gives a new robust characterization of gaussian width in terms of sparsity. ",Testing Sparsity over Known and Unknown Bases
28,760543351896543233,22809805,Dana C. Ernst,['New paper! ‚ÄúImpartial achievement games for generating generalized dihedral groups‚Äù w/ @bretbenesh and N. Sieben. <LINK>'],http://arxiv.org/abs/1608.00259,"We study an impartial game introduced by Anderson and Harary. This game is played by two players who alternately choose previously-unselected elements of a finite group. The first player who builds a generating set from the jointly-selected elements wins. We determine the nim-numbers of this game for generalized dihedral groups, which are of the form $\operatorname{Dih}(A)= \mathbb{Z}_2 \ltimes A$ for a finite abelian group $A$. ",Impartial achievement games for generating generalized dihedral groups
29,760507520553103360,15049113,Katey Alatalo,['My new paper on the mid-IR colors of poststarburst galaxies is on astro-ph in time for #galpath16! <LINK>'],http://arxiv.org/abs/1608.00256,"We investigate the optical and Wide-field Survey Explorer (WISE) colors of ""E+A"" identified post-starburst galaxies, including a deep analysis on 190 post-starbursts detected in the 2{\mu}m All Sky Survey Extended Source Catalog. The post-starburst galaxies appear in both the optical green valley and the WISE Infrared Transition Zone (IRTZ). Furthermore, we find that post-starbursts occupy a distinct region [3.4]-[4.6] vs. [4.6]-[12] WISE colors, enabling the identification of this class of transitioning galaxies through the use of broad-band photometric criteria alone. We have investigated possible causes for the WISE colors of post-starbursts by constructing a composite spectral energy distribution (SED), finding that mid-infrared (4-12{\mu}m) properties of post-starbursts are consistent with either 11.3{\mu}m polycyclic aromatic hydrocarbon emission, or Thermally Pulsating Asymptotic Giant Branch (TP-AGB) and post-AGB stars. The composite SED of extended post- starburst galaxies with 22{\mu}m emission detected with signal to noise >3 requires a hot dust component to produce their observed rising mid-infrared SED between 12 and 22{\mu}m. The composite SED of WISE 22{\mu}m non-detections (S/N<3), created by stacking 22{\mu}m images, is also flat, requiring a hot dust component. The most likely source of this mid-infrared emission of these E+A galaxies is a buried active galactic nucleus. The inferred upper limit to the Eddington ratios of post-starbursts are 1e-2 to 1e-4, with an average of 1e-3. This suggests that AGNs are not radiatively dominant in these systems. This could mean that including selections able to identify active galactic nuclei as part of a search for transitioning and post-starburst galaxies would create a more complete census of the transition pathways taken as a galaxy quenches its star formation. ","Welcome to the Twilight Zone: The Mid-Infrared Properties of
  Poststarburst Galaxies"
30,760417190533337088,589137338,Christopher Wallis,['My new paper is submitted! Image reconstruction on the sphere: <LINK>  with @jasonmcewen and Yves Wiaux'],http://arxiv.org/abs/1608.00553,"We develop techniques to solve ill-posed inverse problems on the sphere by sparse regularisation, exploiting sparsity in both axisymmetric and directional scale-discretised wavelet space. Denoising, inpainting, and deconvolution problems, and combinations thereof, are considered as examples. Inverse problems are solved in both the analysis and synthesis settings, with a number of different sampling schemes. The most effective approach is that with the most restricted solution-space, which depends on the interplay between the adopted sampling scheme, the selection of the analysis/synthesis problem, and any weighting of the l1 norm appearing in the regularisation problem. More efficient sampling schemes on the sphere improve reconstruction fidelity by restricting the solution-space and also by improving sparsity in wavelet space. We apply the technique to denoise Planck 353 GHz observations, improving the ability to extract the structure of Galactic dust emission, which is important for studying Galactic magnetism. ",Sparse image reconstruction on the sphere: analysis and synthesis
31,760377156396613632,339322327,Sebastian Neubert,['New paper: LHCb does not confirm and puts strong limits on the production of the claimed X(5568) state <LINK>'],http://arxiv.org/abs/1608.00435,"The $B_s^0\pi^\pm$ invariant mass distribution is investigated in order to search for possible exotic meson states. The analysis is based on a data sample recorded with the LHCb detector corresponding to $3$ fb$^{-1}$ of $pp$ collision data at $\sqrt{s} = 7$ and $8$ TeV. No significant excess is found, and upper limits are set on the production rate of the claimed $X(5568)$ state within the LHCb acceptance. Upper limits are also set as a function of the mass and width of a possible exotic meson decaying to the $B_s^0\pi^\pm$ final state. The same limits also apply to a possible exotic meson decaying through the chain $B_s^{*0}\pi^\pm$, $B_s^{*0} \to B_s^0 \gamma$ where the photon is excluded from the reconstructed decays. ",Search for structure in the $B_s^0\pi^\pm$ invariant mass spectrum
32,760298908853690368,4552714514,Kerrie Mengersen,"['Birth notice: our new paper on transferability of point referenced spatial data, just posted on arxiv. \n<LINK>']",http://arxiv.org/abs/1608.00086,"When making inferences concerning the environment, ground truthed data will frequently be available as point referenced (geostatistical) observations that are clustered into multiple sites rather than uniformly spaced across the area of interest. In such situations, the similarity of the dominant processes influencing the observed data across sites and the accuracy with which models fitted to data from one site can predict data from another site provide valuable information for scientists seeking to make inferences from these data. Such information may motivate a more informed second round of modelling of the data and also provides insight into the generality of the models developed and an indication of how these models may perform at predicting observations from other sites. We have investigated the geographic transferability of site specific models and compared the results of using different implementations of site specific effects in models for data combined from two sites. Since we have access to data on a broad collection of environmental characteristics that each held potential to aid the interpolation of our geostatistical response observations we have investigated these issues within the framework of a computationally efficient method for variable selection when the number of explanatory variables exceeds the number of observations. We have applied Least Absolute Shrinkage Selection Operator (LASSO) regularized Multiple Linear Regression (MLR) as fitted by the computationally efficient Least Angle Regression algorithm. The response variable in our case study, soil carbon, is of interest as a potential location for the sequestration of atmospheric carbon dioxide and for its positive contribution to soil health and fertility. ","Assessing Site Effects and Geographic Transferability when Interpolating
  Point Referenced Spatial Data: A Digital Soil Mapping Case Study"
33,760289241670701056,107309976,Will Pii,"[""<LINK> Here's a new paper. \\m/ #Mathematics""]",http://arxiv.org/abs/1608.00058,"This paper discusses some issues arising from the category $\mathfrak{H}$ of hypergraphs, the category $\mathfrak{M}$ of (undirected) multigraphs, and the topos $\mathfrak{Q}$ of quivers. First, the natural inclusion of $\mathfrak{M}$ into $\mathfrak{H}$ admits a right adjoint functor by deleting all nontraditional edges. Dually, the operations of taking the underlying multigraph of a quiver and taking the associated digraph of a multigraph form an adjoint pair between $\mathfrak{M}$ and $\mathfrak{Q}$. On the other hand, neither $\mathfrak{H}$ nor $\mathfrak{M}$ is cartesian closed, meaning that neither is a topos like $\mathfrak{Q}$. Moreover, despite $\mathfrak{M}$ being a subcategory of $\mathfrak{H}$, $\mathfrak{H}$ does not have enough projective objects while $\mathfrak{M}$ admits a projective cover for every object. ",A Functorial Link between Quivers and Hypergraphs
34,766580600153206784,523241142,Juste Raimbault,['New paper out : Investigating the Empirical Existence of Static User Equilibrium | <LINK>'],https://arxiv.org/abs/1608.05266,"The Static User Equilibrium is a powerful framework for the theoretical study of traffic. Despite the restricting assumption of stationary flows that intuitively limit its application to real traffic systems, many operational models implementing it are still used without an empirical validation of the existence of the equilibrium. We investigate its existence on a traffic dataset of three months for the region of Paris, FR. The implementation of an application for interactive spatio-temporal data exploration allows to hypothesize a high spatial and temporal heterogeneity, and to guide further quantitative work. The assumption of locally stationary flows is invalidated in a first approximation by empirical results, as shown by a strong spatial and temporal variability in shortest paths and in network topological measures such as betweenness centrality. Furthermore, the behavior of spatial autocorrelation index of congestion patterns at different spatial ranges suggest a chaotic evolution at the local scale, especially during peak hours. We finally discuss the implications of these empirical findings and describe further possible developments based on the estimation of Lyapunov dynamical stability of traffic flows. ",Investigating the Empirical Existence of Static User Equilibrium
35,763692834499080192,727147185310044161,Mundher Al-Shabi,['<LINK>\nMy new paper. It show how to combine #SIFT and #CNN features to improve the accuracy on small data #deeplearning'],https://arxiv.org/abs/1608.02833,"Deriving an effective facial expression recognition component is important for a successful human-computer interaction system. Nonetheless, recognizing facial expression remains a challenging task. This paper describes a novel approach towards facial expression recognition task. The proposed method is motivated by the success of Convolutional Neural Networks (CNN) on the face recognition problem. Unlike other works, we focus on achieving good accuracy while requiring only a small sample data for training. Scale Invariant Feature Transform (SIFT) features are used to increase the performance on small data as SIFT does not require extensive training data to generate useful features. In this paper, both Dense SIFT and regular SIFT are studied and compared when merged with CNN features. Moreover, an aggregator of the models is developed. The proposed approach is tested on the FER-2013 and CK+ datasets. Results demonstrate the superiority of CNN with Dense SIFT over conventional CNN and CNN with SIFT. The accuracy even increased when all the models are aggregated which generates state-of-art results on FER-2013 and CK+ datasets, where it achieved 73.4% on FER-2013 and 99.1% on CK+. ",Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator
36,770963109422956544,296097040,David S. Amundsen,['We have used the @metoffice climate model to study the climate on an #exoplanet quite different from the Earth. <LINK>'],http://arxiv.org/abs/1608.08593,"To study the complexity of hot Jupiter atmospheres revealed by observations of increasing quality, we have adapted the UK Met Office global circulation model (GCM), the Unified Model (UM), to these exoplanets. The UM solves the full 3D Navier-Stokes equations with a height-varying gravity, avoiding the simplifications used in most GCMs currently applied to exoplanets. In this work we present the coupling of the UM dynamical core to an accurate radiation scheme based on the two-stream approximation and correlated-k method with state-of-the-art opacities from ExoMol. Our first application of this model is devoted to the extensively studied hot Jupiter HD 209458b. We have derived synthetic emission spectra and phase curves, and compare them to both previous models also based on state-of-the-art radiative transfer, and to observations. We find a reasonable a agreement between observations and both our day side emission and hot spot offset, however, our night side emission is too large. Overall our results are qualitatively similar to those found by Showman et al. (2009) with the SPARC/MITgcm, however, we note several quantitative differences: Our simulations show significant variation in the position of the hottest part of the atmosphere with pressure, as expected from simple timescale arguments, and in contrast to the ""vertical coherency"" found by Showman et al. (2009). We also see significant quantitative differences in calculated synthetic observations. Our comparisons strengthen the need for detailed intercomparisons of dynamical cores, radiation schemes and post-processing tools to understand these differences. This effort is necessary in order to make robust conclusions about these atmospheres based on GCM results. ","The UK Met Office GCM with a sophisticated radiation scheme applied to
  the hot Jupiter HD 209458b"
37,770255237613576192,304669884,J Fern√°ndez Rossier,['Our new preprint on spin relaxation in nanoengineered spin chains\n<LINK>\n\nWe find RKKY-like oscillation in T1 and T2'],http://arxiv.org/abs/1608.07462,"Exchange interactions with itinerant electrons are known to act as a relaxation mechanism for individual local spins. The same exchange interactions are also known to induce the so called RKKY indirect exchange interaction between two otherwise decoupled local spins. Here we show that both the spin relaxation and the RKKY coupling can be seen as the dissipative and reactive response to the coupling of the local spins with the itinerant electrons. We thereby predict that the spin relaxation rates of magnetic nanostructures of exchanged coupled local spins, such as as nanoengineered spin chains, have an oscillatory dependence on $k_F d$ , where $k_F$ is the Fermi wave length and $d$ is the inter-spin distance, very much like the celebrated oscillations in the RKKY interaction. We demonstrate that both $T_1$ and $T_2$ can be enhanced or suppressed, compared to the single spin limit, depending on the interplay between the Fermi surface and the nanostructure geometrical arrangement. Our results open a route to engineer spin relaxation and decoherence in atomically designed spin structures. ","RKKY oscillations in the spin relaxation rates of atomic scale
  nanomagnets"
