,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1202671950079741952,826924323113807872,Mohammad Rastegari,"[""Thanks to @labs_henry for making a video describing our new paper on What's hidden in a randomly weighted neural network? <LINK>\n<LINK>\n@RamanujanVivek @Mitchnw""]",https://arxiv.org/abs/1911.13299,"Training a neural network is synonymous with learning the values of the weights. By contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 trained on ImageNet. Not only do these ""untrained subnetworks"" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ""untrained subnetwork"" approaches a network with learned weights in accuracy. Our code and pretrained models are available at this https URL ",What's Hidden in a Randomly Weighted Neural Network?
1,1202028465899225091,3242469649,Mingie Jian,"['Our new paper online: <LINK>.\nWe identified 23 new heavy element spectral lines of of Zn I, Sr II, Y II, Zr I, Ba II, Sm II, Eu II, and Dy II in 0.9-1.3 micron. Still the line list of heavy elements are far from complete. <LINK>']",https://arxiv.org/abs/1911.11277,"Stellar absorption lines of heavy elements can give us various insights into the chemical evolution of the Galaxy and nearby galaxies. Recently developed spectrographs for the near-infrared wavelengths are becoming more and more powerful for producing a large number of high-quality spectra, but identification and characterization of the absorption lines in the infrared range remain to be fulfilled. We searched for lines of the elements heavier than the iron group, i.e., those heavier than Ni, in the Y (9760--11100 AA) and J (11600--13200 AA) bands. We considered the lines in three catalogs, i.e., Vienna Atomic Line Database (VALD), the compilation by R. Kurucz, and the list published in 1999 by Melendez and Barbuy. Candidate lines were selected based on synthetic spectra and the confirmation was done by using WINERED spectra of 13 supergiants and giants within FGK spectral types (spanning 4000--7200 K in the effective temperature). We have detected lines of Zn I, Sr II, Y II, Zr I, Ba II, Sm II, Eu II, and Dy II, in the order of atomic number. Although the number of the lines is small, 23 in total, they are potentially useful diagnostic lines of the Galactic chemical evolution, especially in the regions for which interstellar extinction hampers detailed chemical analyses with spectra in shorter wavelengths. We also report the detection of lines whose presence was not predicted by the synthetic spectra created with the above three line lists. ","Identification of Absorption Lines of Heavy Metals in the Wavelength
  Range 0.97--1.32 Micron"
2,1201634347377287168,49625016,Eli Shlizerman,"['Our new paper ""Predict &amp; Cluster - Unsupervised Skeleton Based Action Recognition"" is on the #arxiv <LINK> &amp; video <LINK>. We achieve #AI state-of-the-art  &amp; beyond  performance by introducing training that weakens the decoder in #seq2seq nets <LINK>']",https://arxiv.org/abs/1911.12409,"We propose a novel system for unsupervised skeleton-based action recognition. Given inputs of body keypoints sequences obtained during various movements, our system associates the sequences with actions. Our system is based on an encoder-decoder recurrent neural network, where the encoder learns a separable feature representation within its hidden states formed by training the model to perform prediction task. We show that according to such unsupervised training the decoder and the encoder self-organize their hidden states into a feature space which clusters similar movements into the same cluster and distinct movements into distant clusters. Current state-of-the-art methods for action recognition are strongly supervised, i.e., rely on providing labels for training. Unsupervised methods have been proposed, however, they require camera and depth inputs (RGB+D) at each time step. In contrast, our system is fully unsupervised, does not require labels of actions at any stage, and can operate with body keypoints input only. Furthermore, the method can perform on various dimensions of body keypoints (2D or 3D) and include additional cues describing movements. We evaluate our system on three extensive action recognition benchmarks with different number of actions and examples. Our results outperform prior unsupervised skeleton-based methods, unsupervised RGB+D based methods on cross-view tests and while being unsupervised have similar performance to supervised skeleton-based action recognition. ",PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition
3,1201551373407416320,245345590,Haytham Fayek,['I’m excited to share our new paper where we use audio question answering (AQA) to study temporal reasoning in machine learning. We put forward a new dataset and model for AQA.\n\nPaper: Temporal Reasoning via Audio Question Answering \nw/ @jcjohnss \n<LINK>'],http://arxiv.org/abs/1911.09655,"Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this paper, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA. ",Temporal Reasoning via Audio Question Answering
4,1201467908548784129,2815170184,Oliver Müller,"['New paper on arxiv today (<LINK>) with the title ""Hunting ghosts: the iconic stellar stream(s) around NG5907 under scrutiny"" by me, Ana Vudragovic and Michal Bilek as a follow-up of the by the Dragonfly team earlier this year on the stream(s) around NGC5907. <LINK>', 'Dragonfly re-imaged the famous double loop around NGC5907 and to their surprise, they find rather a single stream structure. No sign of the double loop. Also, the positions of the apparent features were different. So many question marks raised.', 'Which is why we also re-observed – for the sake of reproducibility of LSB features – the double loop part with the newly commissioned 1.4 meter Milankovic telescope in Serbia. We employed the luminance L band filter and a total of 7.2 hours of integration time.', 'What we get is this. This is completely consistent with @DokkumPieter works with Dragonfly. Blinking between the two images reveals the same features (note we have a much smaller field of view). No double loop in sight. https://t.co/mmrDOnljzx', 'This, for me, is super puzzling. Now two different &amp; independent teams have re-observed the galaxy and are consistent with each other, but get completely different results from the many images already taken by amateur astronomers.', 'To remind you, this is the breathtaking image by @astro_delgado and his amateur astronomer team. See the difference? https://t.co/qjt8ALb9R1', 'We go to a surface brightness limit of almost 30 magnitudes in the g band (1 sigma measured as the std in 10x10 arcsec boxes which were apparently star free). This, of course is the level where artefacts, psf wings, reflections, and all the ugly stuff really kicks in.', 'To have a good handle on this issue, we dithered the images, masked all objects, extracted the background from this masked images, and created from 86 images a background model to get the best possible background subtraction. The images are super flat.', 'For comparisons, here we indicate with red and blue where the double loop should be. The black box indicates where we had a 100% coverage of our pointings. So we really probe the disputed region. https://t.co/REzYgfdeIh', 'We provide all the raw data via CDS for you to play around with the data and show where we went wrong (if so)... okay, the upload is broken atm, but I am in contact with the CDS to fix it. For the moment, I happily share it via Drobpox. Message me, the link is ready (800 MB).', ""Also, the code for the calibration of the data is on git (https://t.co/S9ve7sB4Qm). Sadly, the last step, i.e. the stacking of the data, is still done in IRAF, because I couldn't figure out the imcombine command with @astropy, because astropy crops the data, which I do not want."", 'So to conclude, here a super uncontroversial statement: ""It is important to reproduce low-surface brightness features with different telescopes and different means.""\nYou can read more in the accepted A&amp;A Letter (https://t.co/z15zDccU8k).']",http://arxiv.org/abs/1911.12577,"Stellar streams are regarded as crucial objects to test galaxy formation models, with their morphology tracing the underlying potentials and their occurrence tracking the assembly history of the galaxies. The existence of one of the most iconic stellar streams, the double loop around NGC5907, has recently been questioned by new observations with the Dragonfly telescope. This new work only finds parts of the stream, even though they reach a 1 sigma surface brightness limit of 30.3 mag per sq. arcsec in the g-band. Using 7.2 hours of Luminance L-band imaging with the Milankovi\'c 1.4 meter telescope, we have re-observed the putative double loop part to confirm or reject this assessment. We do not find signs of the double loop, but see only a single, knee-shaped stellar stream. Comparing our to the data by the Dragonfly team, we find the same features. Our observations reach a 1 sigma surface brightness limit of 29.7 mag per sq. arcsec in the g-band. These findings emphasize the need for independent confirmation of detections of very low-surface brightness features. ","Hunting ghosts: the iconic stellar stream(s) around NG5907 under
  scrutiny"
5,1199875107470090240,1047227002388959232,Giannis Daras,"['Excited to announce our paper: Your Local GAN.\n\nPaper: <LINK>\nCode: <LINK>\n\nWe obtain 14.53% FID ImageNet improvement on SAGAN by only changing the attention layer.\n\nWe introduce a new sparse attention layer with 2-D locality. Thread: 1/n <LINK>', 'Our sparse attention layer has an interesting property: It is certifiably as expressive as the dense attention layer and it only attends to O(nsqrt(n)) positions! We call this property Full Information and we design it with Information Flow Graphs. Apparently, less is more! 2/n', 'We achieve Full Information by extending attention to multiple steps. At each step, we attend to nearby positions to introduce a local bias in our model. Our bias respects the grid arrangement and thus it is especially effective for grid-structured data, such as images. 3/n https://t.co/5YUK0TIB8j', 'We wanted to visualize how attention works in real images so we came across the inversion problem: Given an image x, how can we find a z so that G(z) approximates x?. Inverting models with attention is hard and in our experiments, previously reported techniques mostly failed. 4/n', 'To solve the problem of inverting GANs with attention, we propose to use attention! For a given real image, we extract a saliency map from the Discriminator, which we then use to weight a new loss function. The results are really promising! 5/n https://t.co/sEkl6vRbBa', ""Inverting real images allows us to visualize our 2-D local attention layer in those images as well. Let's take the image of a redshank. There are heads that attend mostly to the bird and others that attend everywhere, indicating that our model can capture long dependencies. 6/n https://t.co/I7pDChRV5S"", 'We show that the model sometimes attends to arbitrarly long positions but other times use our grid locality bias to model homogeneous areas, such as backgrounds. 7/n https://t.co/5pGSeDkULj', 'The suprising result of improving FID by attending to less positions with a 2-D locality bias, encourages furthers research on this area. We open-source a trained model and our code. You can also quicky play with it in this Google collab: https://t.co/80oH7mpMvk\n8/n', 'Hopefully, you will be able to explore our new technique for inverting GANs with attention and also generate some cool images for the ImageNet category of your choice. For the end, a gif exploring the latent space of maltese dogs. 9/9 https://t.co/y6Ix0ZGXOL', 'This project is the result of a great collaboration with @AlexGDimakis from @UTAustin  and @gstsdn @Han_Zhang_ . from @GoogleAI.\nI am grateful because for my first extended research work, I had such an amazing group of people to work with.']",https://arxiv.org/abs/1911.12287,"We introduce a new local sparse attention layer that preserves two-dimensional geometry and locality. We show that by just replacing the dense attention layer of SAGAN with our construction, we obtain very significant FID, Inception score and pure visual improvements. FID score is improved from $18.65$ to $15.94$ on ImageNet, keeping all other parameters the same. The sparse attention patterns that we propose for our new layer are designed using a novel information theoretic criterion that uses information flow graphs. We also present a novel way to invert Generative Adversarial Networks with attention. Our method extracts from the attention layer of the discriminator a saliency map, which we use to construct a new loss function for the inversion. This allows us to visualize the newly introduced attention heads and show that they indeed capture interesting aspects of two-dimensional geometry of real images. ","Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for
  Generative Models"
6,1199864305522331648,767659609,Yoshihiko Hasegawa,"['My new paper entitled ""Quantum Thermodynamic Uncertainty Relation for Continuous Measurement"" appeared in arXiv. <LINK>']",https://arxiv.org/abs/1911.11982,"We use quantum estimation theory to derive a thermodynamic uncertainty relation in Markovian open quantum systems, which bounds the fluctuation of continuous measurements. The derived quantum thermodynamic uncertainty relation holds for arbitrary continuous measurements satisfying a scaling condition. We derive two relations; the first relation bounds the fluctuation by the dynamical activity and the second one does so by the entropy production. We apply our bounds to a two-level atom driven by a laser field and a three-level quantum thermal machine with jump and diffusion measurements. Our result shows that there exists a universal bound upon the fluctuations, regardless of continuous measurements. ",Quantum thermodynamic uncertainty relation for continuous measurement
7,1199760615520325632,1068545181576773632,Kenneth Brown,"['New paper: Fault-tolerant protocols for 2D compass codes <LINK>. Shilin Huang and I find that elongated codes can outperform the surface code for biased memory errors and unbiased gate errors.  We also describe a weighted Union-Find decoder.', ""The weighted Union-Find decoder is Shilin's new decoder that sits between the Union-Find decoder and Minimum Weight Perfect Matching in terms of both performance and runtime.""]",https://arxiv.org/abs/1911.11317,"We study a class of gauge fixings of the Bacon-Shor code at the circuit level, which includes a subfamily of generalized surface codes. We show that for these codes, fault tolerance can be achieved by direct measurements of the stabilizers. By simulating our fault-tolerant scheme under biased noise, we show the possibility of optimizing the performance of the surface code by stretching the bulk stabilizer geometry. To decode the syndrome efficiently and accurately, we generalize the union-find decoder to biased noise models. Our decoder obtains a $0.83\%$ threshold value for the surface code in quadratic time complexity. ",Fault-tolerant Compass Codes
8,1199709069038247936,1048984881131401217,Cora Dvorkin,"['New paper out today with my current postdoc Julian Muñoz and former Harvard postdoc (and now New Mexico faculty!) Francis-Yan Cyr-Racine: <LINK>\nProbing the Small-Scale Matter Power Spectrum with Large-Scale 21-cm Data.', '@Harvard @harvardphysics @HarvardITC']",https://arxiv.org/abs/1911.11144,"The distribution of matter fluctuations in our universe is key for understanding the nature of dark matter and the physics of the early cosmos. Different observables have been able to map this distribution at large scales, corresponding to wavenumbers $k\lesssim 10$ Mpc$^{-1}$, but smaller scales remain much less constrained. The 21-cm line is a promising tracer of early stellar formation, which took place in small haloes (with masses $M\sim 10^6-10^8M_\odot$), formed out of matter overdensities with wavenumbers as large as $k\approx100$ Mpc$^{-1}$. Here we forecast how well both the 21-cm global signal, and its fluctuations, could probe the matter power spectrum during cosmic dawn ($z=12-25$). We find that the long-wavelength modes (with $k\lesssim40$ Mpc$^{-1}$) are highly degenerate with astrophysical parameters, whereas the modes with $k= (40-80)$ Mpc$^{-1}$ are more readily observable. This is further illustrated in terms of the principal components of the matter power spectrum, which peak at $k\sim 50$ Mpc$^{-1}$ both for a typical experiment measuring the 21-cm global signal and its fluctuations. We find that, imposing broad priors on astrophysical parameters, a global-signal experiment can measure the amplitude of the matter power spectrum integrated over $k= (40-80)$ Mpc$^{-1}$ with a precision of tens of percent. A fluctuation experiment, on the other hand, can constrain the power spectrum to a similar accuracy over both the $k=(40-60)$ Mpc$^{-1}$ and $(60-80)$ Mpc$^{-1}$ ranges even without astrophysical priors. The constraints outlined in this work would be able to test the behavior of dark matter at the smallest scales yet measured, for instance probing warm-dark matter masses up to $m_{\rm WDM}=8$ keV for the global signal and $14$ keV for the 21-cm fluctuations. This could shed light on the nature of dark matter beyond the reach of other cosmic probes. ","Probing the Small-Scale Matter Power Spectrum with Large-Scale 21-cm
  Data"
9,1199671993345462272,1052254632,Remzi Çelebi,"['Our new arxiv paper is now online : <LINK>', 'Title : Towards FAIR protocols and workflows: The OpenPREDICT case study #FAIR #FAIRWorkflows #FAIRsoftware']",https://arxiv.org/abs/1911.09531,"It is essential for the advancement of science that scientists and researchers share, reuse and reproduce workflows and protocols used by others. The FAIR principles are a set of guidelines that aim to maximize the value and usefulness of research data, and emphasize a number of important points regarding the means by which digital objects are found and reused by others. The question of how to apply these principles not just to the static input and output data but also to the dynamic workflows and protocols that consume and produce them is still under debate and poses a number of challenges. In this paper we describe our inclusive and overarching approach to apply the FAIR principles to workflows and protocols and demonstrate its benefits. We apply and evaluate our approach on a case study that consists of making the PREDICT workflow, a highly cited drug repurposing workflow, open and FAIR. This includes FAIRification of the involved datasets, as well as applying semantic technologies to represent and store data about the detailed versions of the general protocol, of the concrete workflow instructions, and of their execution traces. A semantic model was proposed to better address these specific requirements and were evaluated by answering competency questions. This semantic model consists of classes and relations from a number of existing ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then to formulate and answer new kinds of competency questions. Our evaluation shows the high degree to which our FAIRified OpenPREDICT workflow now adheres to the FAIR principles and the practicality and usefulness of being able to answer our new competency questions. ",Towards FAIR protocols and workflows: The OpenPREDICT case study
10,1199619195530620929,77815209,Brian Skinner,"['New paper on a quantum system that can only partially decohere, and therefore at long time has dynamics that are neither classical nor quantum: <LINK>\n\nUnderstanding this system comes down to the statistical mechanics of a process that looks like this: <LINK>', '@sofista137 Oh yeah, this work has Adam written all over it.  There\'s a moments where it gets to ""this dynamics can be solved exactly by mapping to a well-studied model of fully packed loops in d dimensions"" and all you can do is roll your eyes.']",https://arxiv.org/abs/1911.11169,"Coupling a many-body system to a thermal environment typically destroys the quantum coherence of its state, leading to an effective classical dynamics at the longest time scales. We show that systems with anyon-like defects can exhibit universal late-time dynamics that is stochastic, but fundamentally non-classical, because some of the quantum information about the state is topologically protected from the environment. Our coarse-grained model describes one-dimensional systems with domain-wall defects carrying Majorana modes. These defects undergo Brownian motion due to coupling with a bath. Since the fermion parity of a given pair of defects is nonlocal, it cannot be measured by the bath until the two defects happen to come into contact. We examine how such a system anneals to zero temperature via the diffusion and pairwise annihilation of Majorana defects, and we characterize the nontrivial entanglement structure that arises in such stochastic processes. Separately, we also investigate simplified ""quantum measurement circuits"" in one or more dimensions, involving repeated pairwise measurement of fermion parities for a lattice of Majoranas. The dynamics of these circuits can be solved by exact mappings to classical loop models. They yield analytically tractable examples of measurement-induced phase transitions, with critical entanglement structures that are governed by nonunitary conformal fixed points. In the system of diffusing and annihilating Majorana defects, the relaxation to the ground state is analogous to coarsening in a classical 1D Ising model via domain wall annihilation. Here, however, configurations are labeled not only by the defect positions but by a nonlocal entanglement structure. The resulting dynamical process is a new universality class for the coarsening of topological domain walls, whose universal properties can be obtained from an exact mapping. ","Entanglement and dynamics of diffusion-annihilation processes with
  Majorana defects"
11,1199518745653370880,536302437,Joy Christian,"['Does the moon exist when no one is looking?\n\nFor what it’s worth, I give my answer in this new paper:\n\n<LINK>']",https://arxiv.org/abs/1911.11578,"In this pedagogical paper, John S. Bell's amusing example of Dr. Bertlmann's socks is reconsidered, first within a toy model of a two-dimensional one-sided world of a non-orientable M\""obius strip, and then within a real world of three-dimensional quaternionic sphere, S^3, which results from an addition of a single point to R^3 at infinity. In the quaternionic world, which happens to be the spatial part of a solution of Einstein's field equations of general relativity, the singlet correlations between a pair of entangled fermions can be understood as classically as those between Dr. Bertlmann's colorful socks. ",Dr. Bertlmann's Socks in the Quaternionic World of Ambidextral Reality
12,1199505172734431232,171674815,Mark Marley,"['New @PlanetImager paper by Nielsen+ on the mass of beta Pic b is out. Direct imaging + astrometry! <LINK>', ""@PlanetImager Tomorrow I'll say a bit about the use of our new evolution models in this paper.""]",Https://arxiv.org/abs/1911.11273,"We present new observations of the planet beta Pictoris b from 2018 with GPI, the first GPI observations following conjunction. Based on these new measurements, we perform a joint orbit fit to the available relative astrometry from ground-based imaging, the Hipparcos Intermediate Astrometric Data (IAD), and the Gaia DR2 position, and demonstrate how to incorporate the IAD into direct imaging orbit fits. We find a mass consistent with predictions of hot-start evolutionary models and previous works following similar methods, though with larger uncertainties: 12.8 [+5.3, -3.2] M_Jup. Our eccentricity determination of 0.12 [+0.04, -0.03] disfavors circular orbits. We consider orbit fits to several different imaging datasets, and find generally similar posteriors on the mass for each combination of imaging data. Our analysis underscores the importance of performing joint fits to the absolute and relative astrometry simultaneously, given the strong covariance between orbital elements. Time of conjunction is well constrained within 2.8 days of 2017 September 13, with the star behind the planet's Hill sphere between 2017 April 11 and 2018 February 16 (+/- 18 days). Following the recent radial velocity detection of a second planet in the system, beta Pic c, we perform additional two-planet fits combining relative astrometry, absolute astrometry, and stellar radial velocities. These joint fits find a significantly smaller mass for the imaged planet beta Pic b, of 8.0 +/- 2.6 M_Jup, in a somewhat more circular orbit. We expect future ground-based observations to further constrain the visual orbit and mass of the planet in advance of the release of Gaia DR4. ","The Gemini Planet Imager Exoplanet Survey: Dynamical Mass of the
  Exoplanet beta Pictoris b from Combined Direct Imaging and Astrometry"
13,1199394225676341249,1127518127015772160,J. Enrique Vázquez-Lozano,"['New paper on arXiv: Towards Chiral Sensing and Spectroscopy Enabled by All-Dielectric Integrated Photonic Waveguides.\n\nHere we show the feasibility to perform chiroptical applications in all-dielectric integrated photonic waveguides.\n\nFound out more here: <LINK> <LINK>', '@LWLDN @AMartinezUPV @upvntc Thanks Lei !! 😀']",https://arxiv.org/abs/1911.11106,"Chiral spectroscopy is a powerful technique that enables to identify the chirality of matter through optical means. So far, experiments to check the chirality of matter or nanostructures have been carried out using free-space propagating light beams. However, for the sake of miniaturization, it would be desirable to perform chiral spectroscopy in photonic integrated platforms, with the additional benefit of massive parallel detection, low-cost production, repeatability, and portability of such a chiroptical device. Here we show that all-dielectric integrated photonic waveguides can support chiral modes under proper combination of the fundamental eigenmodes. In particular, we investigate two mainstream configurations: a dielectric wire with square cross-section and a slotted waveguide. We analyze numerically three different scenarios in which such waveguides could be used for chiral detection: all-dielectric waveguides as near-field probes, evanescent-induced chiral fields, and chiroptical interaction in void slots. In all the cases we consider a metallic nanohelix as a chiral probe, though all the approaches can be extended to other kinds of chiral nanostructures as well as matter. Our results establish that chiral applications such as sensing and spectroscopy could be realized in standard integrated optics, in particular, with silicon-based technology. ","Towards Chiral Sensing and Spectroscopy Enabled by All-Dielectric
  Integrated Photonic Waveguides"
14,1199378917179691011,282700930,utku,"['End-to-end training of sparse deep neural networks with little-to-no performance loss. Check out our new paper: “Rigging the Lottery: Making All Tickets Winners” (RigL👇) !\n📃 <LINK>\n📁 <LINK>\n\nwith @Tgale96 @jacobmenick  @pcastr and @erich_elsen <LINK>', '2) RigL starts with a random sparse network and trains it without sacrificing accuracy relative to existing dense-to-sparse training methods. It  updates  the  topology  of  the  network  during  training  by  using  parameter  magnitudes  and infrequent  gradient  calculations. https://t.co/PxIEJ3I406', '3) We match or exceed the pruning performance on Resnet-50/ImageNet. RigL performs better than SET (Mocanu 2018) and requires many fewer resources compared with SNFS (Dettmers, 2019) and Magnitude based iterative Pruning (Zhu 2018) (see first plot).', '4) Sparsity of individual layers are fixed; this makes the cost of training predictable and allows for targeting a specific inference cost. Furthermore, RigL, combined with a non-uniform sparsity distribution (ERK👇) performs *better* than pruning at the same sparsity. https://t.co/1N8qcpJpCP', '5) MobileNets are efficient networks and difficult to sparsify. With RigL we can train 75% sparse MobileNets with almost no drop in accuracy. We can also train wider sparse networks with the *same* cost as the dense baseline (Big-Sparse👇) and get a 4.3% increase in accuracy! https://t.co/0OEJqRomaF', '6) A common concern is: “But sparse networks are hard to accelerate!”. Check out Fast Sparse ConvNets (📃 https://t.co/BMKa5tM9bC) which achieves large speedups on mobile CPUs for inference with sparse MobileNets and stay tuned for more! https://t.co/OIyHhD9D7b', '7) Finally, we investigate the energy landscape of sparse networks. Our results suggest that training with static connectivity converges to bad local minima, while RigL allows us to escape such bad critical points. For more results and discussions: check out the paper! https://t.co/s4U1CIovlH']",http://arxiv.org/abs/1911.11134,"Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in github.com/google-research/rigl. ",Rigging the Lottery: Making All Tickets Winners
15,1199160199887507456,114562472,Prof. Emily Levesque 🤓✨🔭📚,"['New paper from the @UW Massive Stars group accepted to ApJ! Led by @KathrynNeugent with @MassiveStarGuy, this one derives the luminosity function of red supergiants in M31 &amp; uses it to test the mass loss rates used in current models. Check it out on arXiv! <LINK>']",https://arxiv.org/abs/1911.10638,"The mass-loss rates of red supergiant stars (RSGs) are poorly constrained by direct measurements, and yet the subsequent evolution of these stars depends critically on how much mass is lost during the RSG phase. In 2012 the Geneva evolutionary group updated their mass-loss prescription for RSGs with the result that a 20 solar mass star now loses 10x more mass during the RSG phase than in the older models. Thus, higher mass RSGs evolve back through a second yellow supergiant phase rather than exploding as Type II-P supernovae, in accord with recent observations (the so-called ""RSG Problem""). Still, even much larger mass-loss rates during the RSG phase cannot be ruled out by direct measurements of their current dust-production rates, as such mass-loss is episodic. Here we test the models by deriving a luminosity function for RSGs in the nearby spiral galaxy M31 which is sensitive to the total mass loss during the RSG phase. We carefully separate RSGs from asymptotic giant branch stars in the color-magnitude diagram following the recent method exploited by Yang and collaborators in their Small Magellanic Cloud studies. Comparing our resulting luminosity function to that predicted by the evolutionary models shows that the new prescription for RSG mass-loss does an excellent job of matching the observations, and we can readily rule out significantly larger values. ",The Luminosity Function of Red Supergiants in M31
16,1199152801374334978,296161364,Chris Power,"['New paper on @arxiv - our @ICRAR @ARC_ASTRO3D PhD student, Rhys Poulton, introduces new software tools to study halo orbits in cosmological simulations, and shows how published prescriptions to estimate merger timescales overestimate them substantially - <LINK> <LINK>']",https://arxiv.org/abs/1911.10281,"Hierarchical models of structure formation predict that dark matter halo assembly histories are characterised by episodic mergers and interactions with other haloes. An accurate description of this process will provide insights into the dynamical evolution of haloes and the galaxies that reside in them. Using large cosmological N-body simulations, we characterise halo orbits to study the interactions between substructure haloes and their hosts, and how different evolutionary histories map to different classes of orbits. We use two new software tools - WhereWolf, which uses halo group catalogues and merger trees to ensure that haloes are tracked accurately in dense environments, and OrbWeaver, which quantifies each halo's orbital parameters. We demonstrate how WhereWolf improves the accuracy of halo merger trees, and we use OrbWeaver to quantify orbits of haloes. We assess how well analytical prescriptions for the merger timescale from the literature compare to measured merger timescales from our simulations and find that existing prescriptions perform well, provided the ratio of substructure-to-host mass is not too small. In the limit of small substructure-to-host mass ratio, we find that the prescriptions can overestimate the merger timescales substantially, such that haloes are predicted to survive well beyond the end of the simulation. This work highlights the need for a revised analytical prescription for the merger timescale that more accurately accounts for processes such as catastrophic tidal disruption. ","Extracting galaxy merger timescales I: Tracking haloes with WhereWolf
  and spinning orbits with OrbWeaver"
17,1198997678501310471,738769492122214400,Johannes Lischner,"['In our new paper, we demonstrate how electronic phases (correlated insulator states, superconductivity) can be switched on and off by changing the thickness of the dielectric spacer layer between twisted bilayer #graphene and metallic gates. Read here: <LINK> <LINK>']",https://arxiv.org/abs/1911.08464,"The effective interaction between electrons in two-dimensional materials can be modified by their environment, enabling control of electronic correlations and phases. Here, we study the dependence of electronic correlations in twisted bilayer graphene (tBLG) on the separation to the metallic gate(s) in two device configurations. Using an atomistic tight-binding model, we determine the Hubbard parameters of the flat bands as a function of gate separation, taking into account the screening from the metallic gate(s), the dielectric spacer layers and the tBLG itself. We determine the critical gate separation at which the Hubbard parameters become smaller than the critical value required for a transition from a correlated insulator state to a (semi-)metallic phase. We show how this critical gate separation depends on twist angle, doping and the device configuration. These calculations may help rationalise the reported differences between recent measurements of tBLG's phase diagram and suggests that correlated insulator states can be screened out in devices with thin dielectric layers. ","Critical role of device geometry for the phase diagram of twisted
  bilayer graphene"
18,1198966580627017730,2825948269,Sharpe Actuary,['New paper on calibrating the generalised Pareto distribution using the reference intrinsic calibration method <LINK> <LINK>'],https://arxiv.org/abs/1911.10117,"We study two Bayesian (Reference Intrinsic and Jeffreys prior) and two frequentist (MLE and PWM) approaches to calibrating the Pareto and related distributions. Three of these approaches are compared in a simulation study and all four to investigate how much equity risk capital banks subject to Basel II banking regulations must hold. The Reference Intrinsic approach, which is invariant under one-to-one transformations of the data and parameter, performs better when fitting a generalised Pareto distribution to data simulated from a Pareto distribution and is competitive in the case study on equity capital requirements ","Calibration of the Pareto and related distributions -a
  reference-intrinsic approach"
19,1198784316639195136,256513537,Dr Chiara Mingarelli,"[""My new paper in collaboration with @AstroStephen, B. S. Sathyaprakash, and  @farrwill. We show how Omega_gw is related between different gravitational wave experiments, and how to relate these Omega_gw measurements. SKA curve from @JeffreyHazboun's Hasasia <LINK> <LINK>""]",http://arxiv.org/abs/1911.09745,"In this paper we provide a comprehensive derivation of the energy density in the stochastic gravitational-wave background $\Omega_\mathrm{gw}(f)$, and show how this quantity is measured in ground-based detectors such as Laser Interferometer Gravitational-Wave Observatory (LIGO), space-based Laser Interferometer Space Antenna (LISA), and Pulsar Timing Arrays. By definition $\Omega_\mathrm{gw}(f) \propto S_h(f)$ -- the power spectral density (PSD) of the Fourier modes of the gravitational-wave background. However, this is often confused with the PSD of the strain signal, which we call $S_\mathrm{gw}(f)$, and is a detector-dependent quantity. This has led to confusing definitions of $\Omega_\mathrm{gw}(f)$ in the literature which differ by factors of up to 5 when written in a detector-dependent way. In addition to clarifying this confusion, formulas presented in this paper facilitate easy comparison of results from different detector groups, and how to convert from one measure of the strength of the background (or an upper limit) to another. Our codes are public and on GitHub. ",Understanding $\Omega_\mathrm{gw}(f)$ in Gravitational Wave Experiments
20,1198777891200352256,1376287471,Tharindu Jayasinghe,"[""New paper day, and the last one for this year! <LINK>\nHere, we have studied a large, all-sky sample of ~71,200 contact binaries ('peanut stars') in @SuperASASSN V-band data (sky distribution shown in the Figure below). <LINK>"", 'Contact binary stars are close binary systems whose components fill their Roche lobes. These systems are commonly known as W Ursae Majoris (W UMa/EW) variables and they are fairly abundant (~0.2% of all FGK stars are EW variables). Here are some example light curves: https://t.co/X148rfz2cC', 'Since both the stars overflow their Roche lobes, their orbital period is closely related to the mean stellar densities. This means that these contact binaries follow a period-luminosity relationship (PLR)! Looking at this PLR, we note that there is a break at log(P)~-0.3🧐 https://t.co/uP7bQnHAxh', 'Looking at the distribution of the orbital periods, we see strong evidence for two different populations, and it seems that they follow distinct PLRs! Historically, they have been separated based on period. Early-type systems have log(P)&gt;-0.25, late-type systems have log(P)&lt;-0.25 https://t.co/Nc5zs63zEA', 'We exploit the synergy between @SuperASASSN and wide-field spectroscopic surveys such as LAMOST, GALAH and APOGEE, and cross-match our catalog to these surveys. Through this process, we obtained spectroscopic information for ~7200 EW binaries (~10% of the full catalog)..', ""We look at their surface temperatures and orbital periods, and find something remarkable! There are two distinct 'clusters' of EW binaries in Teff-log(P) space that are clearly separated. It is also clear that a simple cut in log(P) cannot separate these systems well.... https://t.co/EWtf46VNnS"", 'Also interesting is that these two EW sub-types follow different trends in temperature with orbital period. Early-type systems get cooler as the orbital period increases, whereas late-type systems get hotter!', 'Looking at the Gaia DR2 CMD, early-type systems are also more luminous and appear to be younger than the late-type systems. https://t.co/JxIJdV21yq', 'It is well known that the Kraft break implies substantial changes in the envelope structure,winds and angular momentum loss for stars on the main-sequence. This occurs at ~1.3 M_sun. Stars above the Kraft break are hotter and rotate more rapidly than those below the Kraft break.', 'The transition from slow to fast rotation occurs over the temperature range 6200-6700 K and it cannot be a coincidence that the split between early and late type contact binaries occurs at a similar temperature! 🧐🧐', 'Formation models for these contact binaries generally invoke changes in the efficiency of angular momentum loss on the MS which is exactly the physics leading to the Kraft break!There is a clear gap between early and late type systems, which seems not to be predicted by theory!😮', 'In summary, there is a clean break in the EW \nperiod-luminosity relation at log(P)~-0.3, separating the early-type (A sub-type) EW binaries from the late-type (W sub-type) systems. However, the two populations are even more cleanly separated in the space of log(P) and Teff!', 'Early-type and late-type\nEW binaries follow opposite trends in Teff with orbital period. The dichotomy of contact \nbinaries is almost certainly related to the Kraft break and the related changes in envelope structure, winds and angular momentum loss.', ""@SuperASASSN Didn't think of that, but yes! 😊"", '@astro_jje Hi Dr. Eldridge! Thank you for your insightful comments! :) I have not modeled this behavior with homology, but I agree that convective/radiative envelopes are important to fully describe these observations. I will take a look at the slopes in log (Teff) - log(P/d) space!']",http://arxiv.org/abs/1911.09685,"We characterize ${\sim} 71,200$ W UMa type (EW) contact binaries, including ${\sim} 12,600$ new discoveries, using ASAS-SN $V$-band all-sky light curves along with archival data from Gaia, 2MASS, AllWISE, LAMOST, GALAH, RAVE, and APOGEE. There is a clean break in the EW period-luminosity relation at $\rm \log (\rm P/d){\simeq}-0.30$, separating the longer period early-type EW binaries from the shorter period, late-type systems. The two populations are even more cleanly separated in the space of period and effective temperature, by $\rm T_{eff}=6710\,K-1760\,K\,\log(P/0.5\,d)$. Early-type and late-type EW binaries follow opposite trends in $\rm T_{eff}$ with orbital period. For longer periods, early-type EW binaries are cooler, while late-type systems are hotter. We derive period-luminosity relationships (PLRs) in the $W_{JK}$, $V$, Gaia DR2 $G$, $J$, $H$, $K_s$ and $W_1$ bands for the late-type and early-type EW binaries separated both by period and effective temperature, and by period alone. The dichotomy of contact binaries is almost certainly related to the Kraft break and the related changes in envelope structure, winds and angular momentum loss. ","The ASAS-SN Catalog of Variable Stars VII: Contact Binaries are
  Different Above and Below the Kraft Break"
21,1198772050984394752,775393656693850112,Alec Jacobson,"['We prove that the discretization everyone has been using for the Bi-Laplacian on triangle mesh surfaces is ... \n\n... just fine :-)\n\nNew math paper with Oded Stein, Max Wardetzky and Eitan Grinspun\n\n<LINK>']",https://arxiv.org/abs/1911.08029,"The biharmonic equation with Dirichlet and Neumann boundary conditions discretized using the mixed finite element method and piecewise linear (with the possible exception of boundary triangles) finite elements on triangular elements has been well-studied for domains in R2. Here we study the analogous problem on polyhedral surfaces. In particular, we provide a convergence proof of discrete solutions to the corresponding smooth solution of the biharmonic equation. We obtain convergence rates that are identical to the ones known for the planar setting. Our proof focuses on three different problems: solving the biharmonic equation on the surface, solving the biharmonic equation in a discrete space in the metric of the surface, and solving the biharmonic equation in a discrete space in the metric of the polyhedral approximation of the surface. We employ inverse discrete Laplacians to bound the error between the solutions of the two discrete problems, and generalize a flat strategy to bound the remaining error between the discrete solutions and the exact solution on the curved surface. ","A mixed finite element method with piecewise linear elements for the
  biharmonic equation on surfaces"
22,1198023733086228480,3247908407,Rob Leech,"[""Erik Fagerholm's new paper, estimating scale-symmetries and applying it to the brain, with Rosalyn Moran, Matthew Foulkes, @FarlKriston and others.\n<LINK>""]",https://arxiv.org/abs/1911.00775,"In contrast to the symmetries of translation in space, rotation in space, and translation in time, the known laws of physics are not universally invariant under transformation of scale. However, the action can be invariant under change of scale in the special case of a scale free dynamical system that can be described in terms of a Lagrangian, that itself scales inversely with time. Crucially, this means symmetries under change of scale can exist in dynamical systems under certain constraints. Our contribution lies in the derivation of a generalised scale invariant Lagrangian - in the form of a power series expansion - that satisfies these constraints. This generalised Lagrangian furnishes a normal form for dynamic causal models (i.e., state space models based upon differential equations) that can be used to distinguish scale invariance (scale symmetry) from scale freeness in empirical data. We establish face validity with an analysis of simulated data and then show how scale invariance can be identified - and how the associated conserved quantities can be estimated - in neuronal timeseries. ","Estimating quantities conserved by virtue of scale invariance in
  timeseries"
23,1197840100031422465,151303574,Alex Gomez-Villa 🏳️‍🌈🏳️‍⚧️,['Can GANs generate new visual illusions? Find out in our new paper! Now publicly available at:\n<LINK> <LINK>'],https://arxiv.org/abs/1911.09599,"Visual illusions are a very useful tool for vision scientists, because they allow them to better probe the limits, thresholds and errors of the visual system. In this work we introduce the first ever framework to generate novel visual illusions with an artificial neural network (ANN). It takes the form of a generative adversarial network, with a generator of visual illusion candidates and two discriminator modules, one for the inducer background and another that decides whether or not the candidate is indeed an illusion. The generality of the model is exemplified by synthesizing illusions of different types, and validated with psychophysical experiments that corroborate that the outputs of our ANN are indeed visual illusions to human observers. Apart from synthesizing new visual illusions, which may help vision researchers, the proposed model has the potential to open new ways to study the similarities and differences between ANN and human visual perception. ",Synthesizing Visual Illusions Using Generative Adversarial Networks
24,1197779082508943363,40639812,Colin Cotter,"['New paper on the ArXiV with Jacob Deasy and Tristan Pryer. \n<LINK>\nThis paper started as Jacob’s undergraduate project! He is now a Computer Science PhD student in Cambridge.', 'It was inspired by Tristan’s work on the infinity-Laplacian, obtained as a limit of the variational formulation of the r-Laplacian. I thought it would be fun to look at the same idea for evolutionary nonlinear PDEs.', 'The Hunter-Saxton equation has a conserved energy which is the L^2 norm of u_x. The new equation conserves the L^r norm of u_x. The equation is no longer integrable for r&gt;2, but still has a lot of interesting symmetries. It also still has piecewise linear weak solutions.', 'Just like the r=2 case, the solutions blow up in finite time. But the blow up time gets later and later as r increases. This is tantalising because if the limit conserves the L^infinity norm of u_x, this is enough regularity for the Lagrangian solutions to exist for all time.']",https://arxiv.org/abs/1911.09619,"In this work we introduce the r-Hunter-Saxton equation, a generalisation of the Hunter-Saxton equation arising as extremals of an action principle posed in L_r. We characterise solutions to the Cauchy problem, quantifying the blow-up time and studying various symmetry reductions. We construct piecewise linear functions and show that they are weak solutions to the r-Hunter-Saxton equation. ","The r-Hunter-Saxton equation, smooth and singular solutions and their
  approximation"
25,1197166057372471296,251927957,sorelle,"['Code takes energy to run and long-running jobs have large CO2 footprints. How can you easily measure and communicate the energy usage and CO2 emissions of your code?\n\nNew paper @climatechangeai workshop, Python package, and associated curriculum!\n\n<LINK> <LINK>', ""I've been working with Env. Studies prof @Medullosa and @haverfordedu students Kadan Lottick and Silvia Susai to make it easier to determine energy usage of functions.\n\nWant to check your own code?\n\npip install energyusage\nenergyusage.evaluate(func, args)\n\nhttps://t.co/LMw6CCmMPU"", ""Interested in incorporating the environmental impact of code into your data structures or algorithms curriculum? We've developed some course components related to complexity.\n\nhttps://t.co/0FkN8yBmJv\n\n(with thanks to @mozilla responsible CS challenge!)"", 'If you find this useful or build on it in any way, do let me know! This is all work in progress, and we hope this will be an ongoing community conversation.', ""@krismicinski @ClimateChangeAI Thanks! From a pedagogical point of view it's been interesting to think about how harmful ignoring constants in big-Oh notation is when caring about energy usage. We still have so much to learn as a field."", '@djp3 @ClimateChangeAI Wow - very cool!  Hope it goes well!']",https://arxiv.org/abs/1911.08354,"The carbon footprint of algorithms must be measured and transparently reported so computer scientists can take an honest and active role in environmental sustainability. In this paper, we take analyses usually applied at the industrial level and make them accessible for individual computer science researchers with an easy-to-use Python package. Localizing to the energy mixture of the electrical power grid, we make the conversion from energy usage to CO2 emissions, in addition to contextualizing these results with more human-understandable benchmarks such as automobile miles driven. We also include comparisons with energy mixtures employed in electrical grids around the world. We propose including these automatically-generated Energy Usage Reports as part of standard algorithmic accountability practices, and demonstrate the use of these reports as part of model-choice in a machine learning context. ","Energy Usage Reports: Environmental awareness as part of algorithmic
  accountability"
26,1197134583474589700,2766925212,Andrew Childs,"['New paper with Gorjan Alagic and @hungshihhan shows how to classically verify a quantum computation with only two messages, using a parallel repetition theorem for the Mahadev protocol. <LINK>']",http://arxiv.org/abs/1911.08101,"In a recent breakthrough, Mahadev constructed an interactive protocol that enables a purely classical party to delegate any quantum computation to an untrusted quantum prover. In this work, we show that this same task can in fact be performed non-interactively and in zero-knowledge. Our protocols result from a sequence of significant improvements to the original four-message protocol of Mahadev. We begin by making the first message instance-independent and moving it to an offline setup phase. We then establish a parallel repetition theorem for the resulting three-message protocol, with an asymptotically optimal rate. This, in turn, enables an application of the Fiat-Shamir heuristic, eliminating the second message and giving a non-interactive protocol. Finally, we employ classical non-interactive zero-knowledge (NIZK) arguments and classical fully homomorphic encryption (FHE) to give a zero-knowledge variant of this construction. This yields the first purely classical NIZK argument system for QMA, a quantum analogue of NP. We establish the security of our protocols under standard assumptions in quantum-secure cryptography. Specifically, our protocols are secure in the Quantum Random Oracle Model, under the assumption that Learning with Errors is quantumly hard. The NIZK construction also requires circuit-private FHE. ",Non-interactive classical verification of quantum computation
27,1196877796288737280,1127910304292134912,Çetin Kaya Koç,['New paper: RAPDARTS: Resource-Aware Progressive Differentiable Architecture Search <LINK>'],https://arxiv.org/abs/1911.05704,"Early neural network architectures were designed by so-called ""grad student descent"". Since then, the field of Neural Architecture Search (NAS) has developed with the goal of algorithmically designing architectures tailored for a dataset of interest. Recently, gradient-based NAS approaches have been created to rapidly perform the search. Gradient-based approaches impose more structure on the search, compared to alternative NAS methods, enabling faster search phase optimization. In the real-world, neural architecture performance is measured by more than just high accuracy. There is increasing need for efficient neural architectures, where resources such as model size or latency must also be considered. Gradient-based NAS is also suitable for such multi-objective optimization. In this work we extend a popular gradient-based NAS method to support one or more resource costs. We then perform in-depth analysis on the discovery of architectures satisfying single-resource constraints for classification of CIFAR-10. ",RAPDARTS: Resource-Aware Progressive Differentiable Architecture Search
28,1196804810173042688,97707247,Gautam Kamath,"['New paper on arXiv: Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning. With \nClément L. Canonne (@ccanonne_),\xa0Xi Chen,\xa0Amit Levi, and\xa0Erik Waingarten. <LINK> Check out this thread for a summary 1/n <LINK>', ""Say you have a distribution over a discrete support, and you want to test if it's uniform. Birthday paradox says you need sqrt(domain size) samples, and this is achievable. This is sublinear (great), but in high dimensions, the domain is exponentially large (not great) 2/n"", 'Enter the conditional sampling model. You can ask for samples, conditioned on being from some subset of the domain. Canonne, Ron, and Servedio showed that uniformity can be tested with a number of samples independent of the domain size! But the subsets queried may be complex 3/n', 'We study the problem in a weaker model, where you can only condition on ""subcubes"" of the domain. We show the a nearly optimal upper bound of ~sqrt(dimension). That is, sqrt(log(domain size)), and matching the complexity of testing product distributions with vanilla samples 4/n', 'The main technical tool is a structural statement, lower bounding the mean distance after a random restriction by the total variation distance after a random projection. Proved via a ""robust"" (a la Khot-Minzer-Safra) Pisier inequality 5/n', 'A useful subroutine of independent interest is for testing whether a distribution on the hypercube is uniform, or has mean far from zero. Can be done with same ~sqrt(dimension) complexity, generalizing previous results with the same complexity for product distributions. 6/6']",https://arxiv.org/abs/1911.07357,"We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm. ","Random Restrictions of High-Dimensional Distributions and Uniformity
  Testing with Subcube Conditioning"
29,1196756473268854784,712960453,Prashant Saxena,"[""Glad to collaborate with @basantlalsharma specially since we're aware of each other's existence since 2008. :)\n\n<LINK>\nNew paper on electroelasticity <LINK>""]",https://arxiv.org/abs/1911.07090,"We derive the equations of nonlinear electroelastostatics using three different variational formulations involving the deformation function and an independent field variable representing the electric character - considering either one of the electric field $\mathbb{E}$, electric displacement $\mathbb{D}$, or electric polarization $\mathbb{P}$. The first variation of the energy functional results in the set of Euler-Lagrange partial differential equations which are the equilibrium equations, boundary conditions, { and certain constitutive equations} for the electroelastic system. The partial differential equations for obtaining the bifurcation point have been also found using the second variation based bilinear functional. We show that the well-known Maxwell stress in vacuum is a natural outcome of the derivation of equations from the variational principles and does not depend on the formulation used. As a result of careful analysis it is found that there are certain terms in the bifurcation equation which appear difficult to obtain by an ordinary perturbation based analysis of the Euler-Lagrange equation. From a practical viewpoint, the formulations based on $\mathbb{E}$ and $\mathbb{D}$ result in simpler equations and are anticipated to be more suitable for analysing problems of stability as well as post-buckling behaviour. ","On equilibrium equations and their perturbations using three different
  variational formulations of nonlinear electroelastostatics"
30,1196747652131635201,1019760963569049601,Almog Yalinewich,"['New paper on the arxiv, with my SURP mentee Andrey Remorov\nHighlights:\n1. New simulation results for oblique planetary collisions\n2. Analytic model for shock propagation, agrees with cratering experiment results to two decimal places\n<LINK> <LINK>', 'From the floor of the editing room https://t.co/mNCw4URxMR']",https://arxiv.org/abs/1911.06828,"We present a mathematical model for the propagation of the shock waves that occur during planetary collisions. Such collisions are thought to occur during the formation of terrestrial planets, and they have the potential to erode the planet's atmosphere. We show that under certain assumptions, this evolution of the shock wave can be determined using the method of self similar solutions. This self similar solution is of type II, which means that it only applies to a finite region behind the shock front. This region is bounded by the shock front and the sonic point. Energy and matter continuously flow through the sonic point, so that energy in the self similar region is not conserved, as is the case for type I solutions. Instead, the evolution of the shock wave is determined by boundary conditions at the shock front and at the sonic point. We show how the evolution can be determined for different equations of state, allowing these results to be readily used to calculate the atmospheric mass loss from planetary cores made of different materials. ",Self similar Shocks in Atmospheric Mass Loss due to Planetary Collisions
31,1196712613339639808,409901833,Peter Boorman,"['🚨 New @NASANuSTAR paper time! Work led by Steph LaMassa with new NuSTAR and archival @chandraxray  data of the spiral galaxy NGC 4968 revealed the accreting supermassive black hole at its center to be obscured by very thick thunderclouds of gas ⛈ <LINK>', 'Pivotal to this work was the high-energy data from @NASANuSTAR which gave us powerful ""X-ray vision"" to separately study the material that reflects stray X-rays towards us from the material absorbing X-rays travelling along our line of sight.', 'This tells us not only how obscured the black hole is along our line of sight, but also how much material surrounds the black hole on average. One would expect these amounts to be different for a clumpy configuration, for instance.', ""But why should you care? Well... NGC 4968 is very close to us (just 7x as far as the Whirlpool), yet the supermassive black hole would have gone unnoticed in high-energy X-rays due to that pesky obscuration if it weren't for the sensitivity of @NASANuSTAR."", 'This is known to be a big problem for detecting heavily obscured black holes with X-ray vision, since large amounts of material can severley hinder our chances of even the highest energy X-rays from reaching our telescopes.', 'But finding them is very important. Evidence suggests that supermassive black holes grow most rapidly during heavily obscured phases, so studying them could be the key to understanding how these monsters have grown so huge since their birth near the beginning of the Universe!']",https://arxiv.org/abs/1911.05813,"We present the analysis of Chandra and NuSTAR spectra of NGC 4968, a local (D$\sim$44 Mpc) 12$\mu$m-selected Seyfert 2 galaxy, enshrouded within Compton-thick layers of obscuring gas. We find no evidence of variability between the Chandra and NuSTAR observations (separated by 2 years), and between the two NuSTAR observations (separated by 10 months). Using self-consistent X-ray models, we rule out the scenario where the obscuring medium is nearly spherical and uniform, contradicting the results implied by the $<$10 keV Chandra spectrum. The line-of-sight column density, from intervening matter between the source and observer that intercepts the intrinsic AGN X-ray emission, is well within the Compton-thick regime, with a minimum column density of $2\times10^{24}$ cm$^{-2}$. The average global column density is high ($> 3\times10^{23}$ cm$^{-2}$), with both Compton-thick and Compton-thin solutions permitted depending on the X-ray spectral model. The spectral models provide a range of intrinsic AGN continuum parameters and implied 2-10 keV luminosities ($L_{\rm 2-10keV,intrinsic}$), where the higher end of $L_{\rm 2-10keV,intrinsic}$ is consistent with expectations from the 12$\mu$m luminosity ($L_{\rm 2-10keV,intrinisc} \sim 7\times10^{42}$ erg s$^{-1}$). Compared with Compton-thick AGN previously observed by {\it NuSTAR}, NGC 4968 is among the most intrinsically X-ray luminous. However, despite its close proximity and relatively high intrinsic X-ray luminosity, it is undetected by the 105 month Swift-BAT survey, underscoring the importance of multi-wavelength selection for obtaining the most complete census of the most hidden black holes. ",NuSTAR Uncovers an Extremely Local Compton-thick AGN in NGC 4968
32,1196632634467569664,836417100864344064,Takuma Udagawa,"['Our #AAAI2020 paper ""An Annotated Corpus of Reference Resolution for Interpreting Common Grounding"" is up on arXiv: <LINK>! We propose a new resource of reference resolution to study the intermediate process of common grounding.']",https://arxiv.org/abs/1911.07588,"Common grounding is the process of creating, repairing and updating mutual understandings, which is a fundamental aspect of natural language conversation. However, interpreting the process of common grounding is a challenging task, especially under continuous and partially-observable context where complex ambiguity, uncertainty, partial understandings and misunderstandings are introduced. Interpretation becomes even more challenging when we deal with dialogue systems which still have limited capability of natural language understanding and generation. To address this problem, we consider reference resolution as the central subtask of common grounding and propose a new resource to study its intermediate process. Based on a simple and general annotation schema, we collected a total of 40,172 referring expressions in 5,191 dialogues curated from an existing corpus, along with multiple judgements of referent interpretations. We show that our annotation is highly reliable, captures the complexity of common grounding through a natural degree of reasonable disagreements, and allows for more detailed and quantitative analyses of common grounding strategies. Finally, we demonstrate the advantages of our annotation for interpreting, analyzing and improving common grounding in baseline dialogue systems. ","An Annotated Corpus of Reference Resolution for Interpreting Common
  Grounding"
33,1196602972353916928,1158034481141260288,行田康晃/Yasuaki GYODA,['Our new paper was exposed to arXiv.\n<LINK>\nWe generalize the compatibility degree of generalized associahedra to that of cluster complexes. This is joint work with Prof. Changjian Fu at Sichuan University.'],https://arxiv.org/abs/1911.07193,"We introduce a new function on the set of pairs of cluster variables via $f$-vectors, which we call it the compatibility degree (of cluster complexes). The compatibility degree is a natural generalization of the classical compatibility degree introduced by Fomin and Zelevinsky. In particular, we prove that the compatibility degree has the duality property, the symmetry property, the embedding property and the compatibility property, which the classical one has. We also conjecture that the compatibility degree has the exchangeability property. As pieces of evidence of this conjecture, we establish the exchangeability property for cluster algebras of rank 2, acyclic skew-symmetric cluster algebras, cluster algebras arising from weighted projective lines, and cluster algebras arising from marked surfaces. ",Compatibility degree of cluster complexes
34,1196419411093139457,1185613800839503873,Sotirios (Sotos) Tsaftaris,['New paper out of my team. Great work from Agis. Multimodal learning check. Zero shot check. Fix misalignment check. Sync/pair inputs check. Evaluate disentanglement check. Cardiac &amp; abdominal MRI.  #AI #healthcare #MachineLearning #medicalimaging <LINK> <LINK>'],https://arxiv.org/abs/1911.04417,"Magnetic resonance (MR) protocols rely on several sequences to assess pathology and organ status properly. Despite advances in image analysis, we tend to treat each sequence, here termed modality, in isolation. Taking advantage of the common information shared between modalities (an organ's anatomy) is beneficial for multi-modality processing and learning. However, we must overcome inherent anatomical misregistrations and disparities in signal intensity across the modalities to obtain this benefit. We present a method that offers improved segmentation accuracy of the modality of interest (over a single input model), by learning to leverage information present in other modalities, even if few (semi-supervised) or no (unsupervised) annotations are available for this specific modality. Core to our method is learning a disentangled decomposition into anatomical and imaging factors. Shared anatomical factors from the different inputs are jointly processed and fused to extract more accurate segmentation masks. Image misregistrations are corrected with a Spatial Transformer Network, which non-linearly aligns the anatomical factors. The imaging factor captures signal intensity characteristics across different modality data and is used for image reconstruction, enabling semi-supervised learning. Temporal and slice pairing between inputs are learned dynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and Blood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2 abdominal segmentation. Code is available at this https URL ","Disentangle, align and fuse for multimodal and semi-supervised image
  segmentation"
35,1196267948056424448,3245949691,Rebecca Leane,"['New paper!\n\nCooking Pasta with Dark Matter: Kinetic &amp; Annihilation Heating of Neutron Star Crusts\nw/@JavierFAcevedo1 @josephbramante @PhysicsNirmal\n<LINK>\n\nWe explore dark matter interactions in neutron stars incl. gnocchi, spaghetti &amp; lasagna scattering! Thread: <LINK>', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 1/ Neutron stars are extremely dense objects. They are thought to form after a massive star explodes as a supernova and gravitationally collapses, if the star is not heavy enough to form a black hole.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 2/ Neutron stars might interact with dark matter. One of the ways you can look for evidence for dark matter interactions in neutron stars is as follows:', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 3/ Dark matter is already around in our galaxy. Neutron stars are really heavy and dense, and so have strong gravitational pull. They can pull in dark matter as they pass through it, and accelerate it to about 70 percent of the speed of light as it falls into the neutron star.', ""@JavierFAcevedo1 @josephbramante @PhysicsNirmal 4/ The sped-up dark matter can then interact with the contents of the neutron star as it falls in. By scattering with the stellar matter, it can lose energy and become captured in the star's deep gravitational well."", '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 5/ The energy the speedy dark matter loses when it becomes captured in the neutron star is called kinetic energy.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 6/ This deposited kinetic energy can heat the star to temperatures above what is expected if dark matter did not interact with the star. If the captured dark matter then also annihilates, it can heat the star even further.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 7/ We can then use next generation telescopes, such as the James Webb, Thirty Meter, or European Extremely Large Telescopes, to look for the extra heating in infrared of nearby neutron stars.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 8/ This could tell you about how dark matter does or does not interact. This idea was discussed in Baryakhtar, Bramante, Li, Linden, and Raj, Phys. Rev. Lett. 119 (2017).', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 9/ In the past, the general approach to dark matter interactions in neutron stars was usually just to approximate it as a ball of neutrons in its core. This is a good approximation, as the core makes up most of the mass of the star.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 10/ However, the material in the core of neutron stars is not really known. The core could be exotic, containing things like meson condensates, deconfined quark matter, or strange quark matter, which could exist in a color flavor locked phase.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 11/ Dark matter interactions with the core could be forbidden, or suppressed, either from these phases or for other reasons.', ""@JavierFAcevedo1 @josephbramante @PhysicsNirmal 12/ Overall, the truth is that we don't know what's in the core for sure. So, we can't really be confident that the calculated dark matter interactions with the core definitely give the right temperatures for the telescopes to expect."", ""@JavierFAcevedo1 @josephbramante @PhysicsNirmal 13/ We started to think about how to make any statements about the dark matter heating of neutron stars more robust. We want telescopes to look for this signal! We also want to confirm we could make some conclusion about what we do or don't see for the neutron star temperature."", '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 14/ Turns out, there is an absolute playground of matter states inside neutron stars that we can use. These other states are much more robustly understood than the core.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 15/ Check out the layers in our graphic: neutron stars have an atmosphere, ocean, crust, and then an inner and outer core. https://t.co/rpadxUc8ZB', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 16/ The crust hosts nuclei, but also something that may be strongest material in the universe: nuclear pasta. The phases of nuclear pasta include gnocchi, spaghetti, lasagna, bucatini (anti-spaghetti), and swiss cheese (anti-gnocchi). Yeah, you read right.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 17/ To get an idea of how nuclear pasta looks, check out this awesome simulation from Caplan, Schneider, and Horowitz, Phys. Rev. Lett. 121, 2018: https://t.co/E3qa7QOzSc', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 18/ Nuclear pasta has important implications for properties of neutron stars. In the past, scientists have studied it because the neutrino-pasta interactions affect neutrino opacity in supernovae, and electron-pasta interactions impact e.g. thermal and electrical conductivity.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 19/ For the first time, we investigated how dark matter could interact with the pasta phases. Pasta can capture dark matter in the star! This includes dark matter-gnocchi scattering, dark matter-spaghetti scattering, and dark matter-lasagna scattering!', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 20/ We also considered dark matter scattering with the rest of the crust, which includes scattering on the crust nucleons. What do our results from each layer _boil down_ to? Here is what we find: https://t.co/DIp1EPlG9M', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 21/ For lighter dark matter masses, we also expect some heating sensitivity by exciting collective phonon modes in the neutron superfluid in the inner crust: https://t.co/xA7ZoYaV3M', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 22/ The crust heating can lead to neutron stars at temperatures of 1630 K, while without dark matter interactions, old neutron stars should be around 100 K hot.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 23/ Comparing with the full core, the crust is within 3 or less orders of magnitude of the core sensitivity. This is great news:', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 24/ It tells us that even if the core turns out to be exotic, or if for any reason dark matter interactions in the core are suppressed, we are guaranteed to get a detectable signal or constraint from neutron star heating! Amazing!', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 25/ It is also important to note that while these results are more robust than the core, the most robust calculations occur moving further out from the center star.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 26/ The nuclear pasta interaction estimates we show are certainly less robust than the inner crust nucleon scattering estimates, and the outer core scattering estimates are the most robust.', ""@JavierFAcevedo1 @josephbramante @PhysicsNirmal 27/ This is just because we understand less how matter should behave under more extreme pressures. As such, we've shown you each layer independently in our plot."", '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 28/ You might also be interested in how this compares to direct detection experiments. These are experiments designed to detect dark matter scattering on Earth, and usually have the best bounds for sufficiently heavy dark matter.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 29/ For spin-independent interactions, we have (our results are shown as ""Full Crust"", while the direct detection experiments are shown with their names): https://t.co/biqHUTYBbw', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 30/ The spin-dependent results are even more promising, with significant power over direct detection experiments: https://t.co/MQhzRUgjRG', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 31/ Note the limits shown are for ""saturation"" amounts of dark matter. If sub-saturation amounts are captured, the star will not be as hot, but provided its hotter than the old neutron stars, there can still potentially be sensitivity.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 32/ This means the bounds could extend roughly an additional 4+ orders of magnitude below what they are shown to be. So crust heating could potentially, eventually, even outperform the spin-independent direct detection sensitivity above 1 TeV.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 33/ Lastly, we found something pretty neat with the crusts: they help out for thermalization when the core is not enough. The crust alone is enough to thermalize electroweak-scale dark matter, regardless of whether the dark matter ever scatters with the neutron star core.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal 34/ This implies efficient annihilation in neutron stars for any electroweak-scale dark matter with inelastic mass splittings up to 200 MeV. This includes the elusive Higgsino, significantly increasing the prospects of its discovery.', '@JavierFAcevedo1 @josephbramante @PhysicsNirmal Hope you enjoyed reading about the dark matter bolognese we served up for you today! \nThis work/recipe was brought to you by a joint venture from the kitchens of @McDonaldInst, @queensu, @Perimeter, @MIT, and @TRIUMFLab.']",https://arxiv.org/abs/1911.06334,"Neutron stars serve as excellent next-generation thermal detectors of dark matter, heated by the scattering and annihilation of dark matter accelerated to relativistic speeds in their deep gravitational wells. However, the dynamics of neutron star cores are uncertain, making it difficult at present to unequivocally compute dark matter scattering in this region. On the other hand, the physics of an outer layer of the neutron star, the crust, is more robustly understood. We show that dark matter scattering solely with the low-density crust still kinetically heats neutron stars to infrared temperatures detectable by forthcoming telescopes. We find that for both spin-independent and spin-dependent scattering on nucleons, the crust-only cross section sensitivity is $10^{-43} - 10^{-41}$~cm$^2$ for dark matter masses of 100 MeV $-$ 1 PeV, with the best sensitivity arising from dark matter scattering with a crust constituent called nuclear pasta (including gnocchi, spaghetti, and lasagna phases). For dark matter masses from 10 eV to 1 MeV, the sensitivity is $10^{-39} - 10^{-34}$~cm$^2$, arising from exciting collective phonon modes in a neutron superfluid in the inner crust. Furthermore, for any $s$-wave or $p$-wave annihilating dark matter, we show that dark matter will efficiently annihilate by thermalizing just with the neutron star crust, regardless of whether the dark matter ever scatters with the neutron star core. This implies efficient annihilation in neutron stars for any electroweakly interacting dark matter with inelastic mass splittings of up to 200 MeV, including Higgsinos. We conclude that neutron star crusts play a key role in dark matter scattering and annihilation in neutron stars. ","Warming Nuclear Pasta with Dark Matter: Kinetic and Annihilation Heating
  of Neutron Star Crusts"
36,1195884450153816064,1059369365525557248,Shion Honda,"['Our new paper, ""SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery"" is now on arXiv!\nWe pre-trained Transformer with many unlabeled SMILES to extract molecular fingerprints for small-data QSPR. Evaluated on MoleculeNet.\n<LINK> <LINK>']",https://arxiv.org/abs/1911.04738,"In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency. ","SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug
  Discovery"
37,1195273925544087552,481539448,Richard Alexander,"[""New paper, led by @PhysicsUoL's @bec_nealon, looking at the effects of stellar flybys on planet-hosting discs. \n\n<LINK> <LINK>""]",https://arxiv.org/abs/1911.05760,"We now have several observational examples of misaligned broken protoplanetary discs, where the disc inner regions are strongly misaligned with respect to the outer disc. Current models suggest that this disc structure can be generated with an internal misaligned companion (stellar or planetary), but the occurrence rate of these currently unobserved companions remains unknown. Here we explore whether a strong misalignment between the inner and outer disc can be formed without such a companion. We consider a disc that has an existing gap --- essentially separating the disc into two regions --- and use a flyby to disturb the discs, leading to a misalignment. Despite considering the most optimistic parameters for this scenario, we find maximum misalignments between the inner and outer disc of $\sim$45$^{\circ}$ and that these misalignments are short-lived. We thus conclude that the currently observed misaligned discs must harbour internal, misaligned companions. ",Flyby-induced misalignments in planet-hosting discs
38,1195185082065141760,306687043,Lorenzo Ruffoni,"['New paper on holonomy preserving deformations of branched complex projective structures, and their effect on the underlying complex structure <LINK> @FSUMath @FSUResearch @FSUPostdocs']",https://arxiv.org/abs/1911.05290,"We study a class of continuous deformations of branched complex projective structures on closed surfaces of genus $g\geq 2$, which preserve the holonomy representation of the structure and the order of the branch points. In the case of non-elementary holonomy we show that when the underlying complex structure is infinitesimally preserved the branch points are necessarily arranged on a canonical divisor, and we establish a partial converse for hyperelliptic structures. ","Local deformations of branched projective structures: Schiffer
  variations and the Teichm\""uller map"
39,1194978432062513153,1004365363574902784,Kevin J. Kelly,"['New paper out today! I worked with @PedroANMachado and Roni Harnik on the idea of using neutrinos that come from a ""decay-at-rest"" process to measure neutrino oscillations.\n\n<LINK>', ""@PedroANMachado When we try to understand neutrino oscillations, knowing how far they've travelled and how much energy they have is crucial for interpreting a result.\n\nDecay-at-rest neutrinos are special in that certain types can only have one energy. https://t.co/uoUy67PGyL"", '@PedroANMachado If we can measure them after travelling hundreds of kilometers, then we can start to pinpoint how neutrinos oscillate over many different lengths and energies, and start to stress-test our theories of neutrino oscillations.', '@PedroANMachado In the next decade, the J-PARC Spallation Neutron Source and the Hyper-Kamiokande experiment are a great opportunity for this type of measurement -- we can start to add stars to that panel and really hone in on neutrino oscillations.']",https://arxiv.org/abs/1911.05088,"In addition to the next generation of beam-based neutrino experiments and their associated detectors, a number of intense, low-energy neutrino production sources from decays at rest will be in operation. In this work, we explore the physics opportunities with decay-at-rest neutrinos for complementary measurements of oscillation parameters at long baselines. The J-PARC Spallation Neutron Source, for example, will generate neutrinos from a variety of decay-at-rest (DAR) processes, specifically those of pions, muons, and kaons. Other proposed sources will produce large numbers of stopped pions and muons. We demonstrate the ability of the upcoming Hyper-Kamiokande experiment to detect the monochromatic kaon decay-at-rest neutrinos from J-PARC after they have travelled several hundred kilometers and undergone oscillations. This measurement will serve as a valuable cross-check in constraining our understanding of neutrino oscillations in a new regime of neutrino energy and baseline length. We also study the expected event rates from pion and muon DAR neutrinos in liquid Argon and water detectors and their sensitivities to to the CP violating phase $\delta_\mathrm{CP}$. ","Prospects of Measuring Oscillated Decay-at-Rest Neutrinos at Long
  Baselines"
40,1194792737134010369,106843613,Jacob Haqq Misra,"['Planets orbiting binary stars are probably just as good targets as single-star planets to search for life, but they might have some wacky seasons. \n\nCheck out our new paper about climate variations on circumbinary terrestrial planets \n<LINK>']",https://arxiv.org/abs/1911.05577,"Planets that revolve around a binary pair of stars are known as circumbinary planets. The orbital motion of the stars around their center of mass causes a periodic variation in the total instellation incident upon a circumbinary planet. This study uses both an analytic and numerical energy balance model to calculate the extent to which this effect can drive changes in surface temperature on circumbinary terrestrial planets. We show that the amplitude of the temperature variation is largely constrained by the effective heat capacity, which corresponds to the ocean-to-land ratio on the planet. Planets with large ocean fractions should experience only modest warming and cooling of only a few degrees, which suggests that habitability cannot be precluded for such circumbinary planets. Planets with large land fractions that experience extreme periodic forcing could be prone to changes in temperature of tens of degrees or more, which could drive more extreme climate changes that inhibit continuously habitable conditions. ","Constraining the magnitude of climate extremes from time-varying
  instellation on a circumbinary terrestrial planet"
41,1194712172183474176,24859650,Jan-Willem van de Meent,"['1/ New work by Alican (@alicanb_) and Babak (@BabakEsmaeili10): ""Evaluating Combinatorial Generalization in Variational Autoencoders"" (<LINK>)\n\nIn this paper we ask the question:  ""To what extent do VAEs generalize to unseen combinations of features?""(thread) <LINK>', '2/ In any dataset that is characterized by even a moderate number of factors of variation, we cannot hope to have representatives of all combinations of latent features in our training data, since the size of a ""complete"" dataset grows exponentially with the number of features.', '3/ In this paper, we test what happens when test-time examples differ substantially from training examples with respect to a least one feature. We are particularly interested in the effect of model capacity on generalization.', '4/ Alican and Babak trained over 3000 VAE instances to systematically evaluate the effect of network width &amp; depth, KL regularization, the amount of training data, and the density of the training set in feature space. We were surprised by the results! https://t.co/qGEOmKEc9V', '5/ For easy generalization problems (i.e. when test set examples are similar in pixel space to training examples), increasing model capacity always improves generalization. In this regime, memorizing training data does not hurt generalization performance. https://t.co/FipiYRorev', '6/ For more difficult problems we observe a range of behaviors. In this regime, increasing the layer depth can lead to data memorization, which adversely affects generalization when the test data are not similar to the training data.  However this is not always the case.', '7/ The level of KL regularization also plays a crucial role, as evidenced by rate-distortion analysis. 1-layer networks show a U-shaped curve as a function of the level of regularization. In 3-layer networks, decreasing the regularization paradoxically improves generalization. https://t.co/Q92RNMVx9U', '8/ Our results suggest that the generalization characteristics of VAEs are by no means straightforward. Network depth, KL regularization, and the difficulty of the problem can give rise of a range of possible behaviors, and need to be considered when evaluating generalization.', '9/ In this paper, we restrict ourselves to fully-connected encoders and decoders in this work. As part of this paper, we will be releasing our Tetraminoes dataset, along with source code for all experiments. We would be excited to hear about results for different architectures! https://t.co/RaflagGjfB']",https://arxiv.org/abs/1911.04594,"Variational autoencoders optimize an objective that combines a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is often interpreted as a regularizer that controls the degree of compression. We here examine whether inclusion of the rate also acts as an inductive bias that improves generalization. We perform rate-distortion analyses that control the strength of the rate term, the network capacity, and the difficulty of the generalization problem. Decreasing the strength of the rate paradoxically improves generalization in most settings, and reducing the mutual information typically leads to underfitting. Moreover, we show that generalization continues to improve even after the mutual information saturates, indicating that the gap on the bound (i.e. the KL divergence relative to the inference marginal) affects generalization. This suggests that the standard Gaussian prior is not an inductive bias that typically aids generalization, prompting work to understand what choices of priors improve generalization in VAEs. ",Rate-Regularization and Generalization in VAEs
42,1194606557272645633,139122509,Stephen Burgess,"['New paper on an approach for performing robust instrumental variable analyses by Andrew Grant and myself: ""An efficient and robust approach to Mendelian randomization with measured pleiotropic effects in a high-dimensional setting"" - <LINK>. Tweetorial follows!:', 'Suppose genetic variants influence the outcome not via the risk factor (hence are not valid instruments), but pleiotropic pathways operate via a measured variable. Then we can perform multivariable Mendelian randomization including the variable to recover an unbiased estimate.', 'However, there may be multiple different measured potentially pleiotropic variables. If we ignore them, then we get bias, but if we adjust for all of them, we will typically have an inefficient estimate as not all the pathways may influence the outcome.', ""Additionally, if there are more potentially pleiotropic variables than genetic variants, then we can't include all variables in the analysis model."", 'We have developed a lasso approach that only includes relevant pleiotropic variables in the analysis - hence the estimate is unbiased, but more efficient than that including all variables. The approach is implemented using summarized genetic association data.', 'One problem is inference post-variable selection - we discuss this at length in the paper. Thanks to Stijn Vansteelandt for helpful conversations on this topic.', 'We present as an example the effect of serum urate on coronary heart disease risk, showing that blood pressure is the key pleiotropic variable, and that the estimated direct effect of urate on CHD risk not via blood pressure is substantially attenuated.', 'We recommend the method when it is believed that pleiotropic effects of genetic predictors of a risk factor could influence the outcome via measured pleiotropic pathways - could use as a main analysis or sensitivity. Comments welcome!', '@dpsg108 True, could be either direct (horizontal) or indirect (vertical/mediated) pleiotropy.']",https://arxiv.org/abs/1911.00347,"Valid estimation of a causal effect using instrumental variables requires that all of the instruments are independent of the outcome conditional on the risk factor of interest and any confounders. In Mendelian randomization studies with large numbers of genetic variants used as instruments, it is unlikely that this condition will be met. Any given genetic variant could be associated with a large number of traits, all of which represent potential pathways to the outcome which bypass the risk factor of interest. Such pleiotropy can be accounted for using standard multivariable Mendelian randomization with all possible pleiotropic traits included as covariates. However, the estimator obtained in this way will be inefficient if some of the covariates do not truly sit on pleiotropic pathways to the outcome. We present a method which uses regularization to identify which out of a set of potential covariates need to be accounted for in a Mendelian randomization analysis in order to produce an efficient and robust estimator of a causal effect. The method can be used in the case where individual-level data are not available and the analysis must rely on summary-level data only. It can also be used in the case where there are more covariates under consideration than instruments, which is not possible using standard multivariable Mendelian randomization. We show the results of simulation studies which demonstrate the performance of the proposed regularization method in realistic settings. We also illustrate the method in an applied example which looks at the causal effect of urate plasma concentration on coronary heart disease. ","An efficient and robust approach to Mendelian randomization with
  measured pleiotropic effects in a high-dimensional setting"
43,1194549508199370752,335941225,Saeed Jahromi,['Check out our new paper on fine-grained tensor network methods:\n<LINK> <LINK>'],https://arxiv.org/abs/1911.04882,"We develop a strategy for tensor network algorithms that allows to deal very efficiently with lattices of high connectivity. The basic idea is to fine-grain the physical degrees of freedom, i.e., decompose them into more fundamental units which, after a suitable coarse-graining, provide the original ones. Thanks to this procedure, the original lattice with high connectivity is transformed by an isometry into a simpler structure, which is easier to simulate via usual tensor network methods. In particular this enables the use of standard schemes to contract infinite 2d tensor networks - such as Corner Transfer Matrix Renormalization schemes - which are more involved on complex lattice structures. We prove the validity of our approach by numerically computing the ground-state properties of the ferromagnetic spin-1 transverse-field Ising model on the 2d triangular and 3d stacked triangular lattice, as well as of the hard-core and soft-core Bose-Hubbard models on the triangular lattice. Our results are benchmarked against those obtained with other techniques, such as perturbative continuous unitary transformations and graph projected entangled pair states, showing excellent agreement and also improved performance in several regimes. ",Fine-Grained Tensor Network Methods
44,1194522805124993024,864023169212055552,Mojtaba Raouf,"['Our new paper using the GAMA DR3, corresponding to the first probe for stellar population of central galaxies in different #group_relaxedness_state accepted in the past week for publication in ApJ.  The version of arxiv can be found here: <LINK>']",https://arxiv.org/abs/1911.02976,"We study the stellar populations of the brightest group galaxies (BGGs) in groups with different dynamical states, using GAMA survey data. We use two independent, luminosity dependent indicators to probe the relaxedness of their groups; the magnitude gap between the two most luminous galaxies ($\Delta M_{12}$), and offset between BGG and the luminosity center ($D_{offset}$) of the group. Combined, these two indicators were previously found useful for identifying relaxed and unrelaxed groups. We find that the BGGs of unrelaxed groups have significantly bluer NUV-r colours than in relaxed groups. This is also true at the fixed sersic index. We find the bluer colours cannot be explained away by differing dust fraction, suggesting there are real differences in their stellar populations. SFRs derived from SED-fitting tend to be higher in unrelaxed systems. This is in part because of a greater fraction of BGGs with non-elliptical morphology, but also because unrelaxed systems have larger numbers of mergers, some of which may bring fuel for star formation. The SED-fitted stellar metallicities of BGGs in unrelaxed systems also tend to be higher by around 0.05 dex, perhaps because their building blocks were more massive. We find that the $\Delta M_{12}$ parameter is the most important parameter behind the observed differences in the relaxed/unrelaxed groups, in contrast with the previous study of Trevisan et al. (2017). We also find that groups selected to be unrelaxed using our criteria tend to have higher velocity offsets between the BGG and their group. ","The impact of the dynamical state of galaxy groups on the stellar
  populations of central galaxies"
45,1194500459131785217,2425754287,Roman Orus,"['New paper out! Fine-graining tensor networks <LINK>', ""@TsuyoshiOkubo I just noticed and informed Kenji, we'll include it in the revised version""]",https://arxiv.org/abs/1911.04882,"We develop a strategy for tensor network algorithms that allows to deal very efficiently with lattices of high connectivity. The basic idea is to fine-grain the physical degrees of freedom, i.e., decompose them into more fundamental units which, after a suitable coarse-graining, provide the original ones. Thanks to this procedure, the original lattice with high connectivity is transformed by an isometry into a simpler structure, which is easier to simulate via usual tensor network methods. In particular this enables the use of standard schemes to contract infinite 2d tensor networks - such as Corner Transfer Matrix Renormalization schemes - which are more involved on complex lattice structures. We prove the validity of our approach by numerically computing the ground-state properties of the ferromagnetic spin-1 transverse-field Ising model on the 2d triangular and 3d stacked triangular lattice, as well as of the hard-core and soft-core Bose-Hubbard models on the triangular lattice. Our results are benchmarked against those obtained with other techniques, such as perturbative continuous unitary transformations and graph projected entangled pair states, showing excellent agreement and also improved performance in several regimes. ",Fine-Grained Tensor Network Methods
46,1194440135913410560,2255802320,Kartik Prabhu,['New paper on angular momentum of Einstein-Maxwell at null infinity with Beatrice Bonga and Alexander M. Grant. Go get it! <LINK>'],https://arxiv.org/abs/1911.04514,"On Minkowski spacetime, the angular momentum flux through null infinity of Maxwell fields, computed using the stress-energy tensor, depends not only on the radiative degrees of freedom, but also on the Coulombic parts. However, the angular momentum also can be computed using other conserved currents associated with a Killing field, such as the Noether current and the canonical current. The flux computed using these latter two currents is purely radiative. A priori, it is not clear which of these is to be considered the true flux of angular momentum for Maxwell fields. This situation carries over to Maxwell fields on non-dynamical, asymptotically flat spacetimes for fluxes associated with the Lorentz symmetries in the asymptotic Bondi-Metzner-Sachs (BMS) algebra. We investigate this question of angular momentum flux in the full Einstein-Maxwell theory. Using the prescription of Wald and Zoupas, we compute the charges associated with any BMS symmetry on cross-sections of null infinity. The change of these charges along null infinity then provides a flux. For Lorentz symmetries, the Maxwell fields contribute an additional term, compared to the Wald-Zoupas charge in vacuum general relativity, to the charge on a cross-section. With this additional term, the flux associated with Lorentz symmetries, e.g., the angular momentum flux, is purely determined by the radiative degrees of freedom of the gravitational and Maxwell fields. In fact, the contribution to this flux by the Maxwell fields is given by the radiative Noether current flux and not by the stress-energy flux. ",Angular momentum at null infinity in Einstein-Maxwell theory
47,1194428484254339073,281541028,mehmet,"[""New paper!\n\n<LINK>\n\nI haven't been this excited about a result in a long time. In a nutshell, we show that there is a really strong correlation between the (easily measured) brightness of the central galaxy of a group and the total luminosity of it's satellites."", ""What's unprecedented is that this correlation is tighter than the correlation between the central galaxy's stellar mass with the total satellite luminosity!"", 'But what is perhaps most exciting is that this total satellite luminosity correlates *really well* with the dark matter halo mass of the group. In the paper we go over several other central galaxy properties and how they link to this indirect probe of halo mass.', 'All this is tremendously useful for constraining how galaxies evolve as a function of environment, and even more importantly it allows for a new way to estimate halo masses from galaxy survey data.', 'Also hi #rstats thank you for the incredible packages https://t.co/xZZzkfScO3', ""If you'd like to find out more about how we compute total satellite luminosity and see some more detail on how it traces halo properties, check this paper out, which also came out today!\n\nhttps://t.co/jRMwSz50eq""]",https://arxiv.org/abs/1911.04509,"The total luminosity of satellite galaxies around a central galaxy, L$_{sat}$, is a powerful metric for probing dark matter halos. In this paper we use data from the Sloan Digital Sky Survey and DESI Legacy Imaging Surveys to explore the relationship between L$_{sat}$ and various observable galaxy properties for a sample of 117,966 central galaxies out to $z = 0.15$. At fixed stellar mass, every galaxy property we explore shows a correlation with L$_{sat}$. This implies that dark matter halos play a possibly significant role in determining these secondary galaxy properties. We quantify these correlations by computing the mutual information between L$_{sat}$ and secondary properties and explore how this mutual information varies as a function of stellar mass and when separating the sample into star-forming and quiescent central galaxies. We find that absolute r-band magnitude correlates more strongly with L$_{sat}$ than stellar mass across all galaxy populations; and that effective radius, velocity dispersion, and S\'ersic index do so as well for star-forming and quiescent galaxies. L$_{sat}$ is sensitive to both the mass of the host halo as well as the halo formation history, with younger halos having higher L$_{sat}$. L$_{sat}$ by itself cannot distinguish between these two effects, but measurements of galaxy large-scale environment can break this degeneracy. For star-forming central galaxies, we find that r$_{\rm eff}$, $\sigma_v$, and S\'ersic index all correlate with large-scale density, implying that these halo age plays a role in determining these properties. For quiescent galaxies, we find that all secondary properties are independent of environment, implying that correlations with L$_{sat}$ are driven only by halo mass. These results are a significant step forward in quantifying the full extent of the galaxy-halo connection, and present a new test of galaxy formation models. ","Connecting SDSS central galaxies to their host halos using total
  satellite luminosity"
48,1194358811420712960,1177963595826651137,MetaIgna,['Our new paper on Meta-Active Learning is already available! Take a look at it!\n\n <LINK>'],https://arxiv.org/abs/1911.00314,"Active Learning techniques are used to tackle learning problems where obtaining training labels is costly. In this work we use Meta-Active Learning to learn to select a subset of samples from a pool of unsupervised input for further annotation. This scenario is called Static Pool-based Meta- Active Learning. We propose to extend existing approaches by performing the selection in a manner that, unlike previous works, can handle the selection of each sample based on the whole selected subset. ","Picking groups instead of samples: A close look at Static Pool-based
  Meta-Active Learning"
49,1194338002434084864,548718054,Marc Khoury,"['In a new paper, we study the robustness of classifiers found by adaptive and standard descent methods to adversarial examples. Further we describe how L2-adversarial training affects the geometry of the loss-landscape in least-squares linear regression. \n<LINK>']",https://arxiv.org/abs/1911.03784,Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. In this paper we study how the choice of optimization algorithm influences the robustness of the resulting classifier to adversarial examples. Specifically we show an example of a learning problem for which the solution found by adaptive optimization algorithms exhibits qualitatively worse robustness properties against both $L_{2}$- and $L_{\infty}$-adversaries than the solution found by non-adaptive algorithms. Then we fully characterize the geometry of the loss landscape of $L_{2}$-adversarial training in least-squares linear regression. The geometry of the loss landscape is subtle and has important consequences for optimization algorithms. Finally we provide experimental evidence which suggests that non-adaptive methods consistently produce more robust models than adaptive methods. ,"Adaptive versus Standard Descent Methods and Robustness Against
  Adversarial Examples"
50,1194335282734227456,23000769,Christopher Conselice,"['On the @arxiv on Monday, Amy Whitney (Notts PhD student) et al. released a new paper on the evolution of differential galaxy sizes, accepted to ApJ.  A number of things are worth noting in this paper. A thread. (1/n)\n\n<LINK>', 'We develop a new method of distinguishing high-z galaxies from foreground galaxies. Because of the Lyman-break, we are able to remove contamination.  We call this ""2D Lyman-break imaging"" \n\nExample here, 2nd from left is original image, far right, cleaned of foreground.  (2/n) https://t.co/mLS3LRMbzJ', ""Using the Petrosian radius, which is independent of distances and surface brightness dimming we can measure how 'inner' and 'outer' parts of galaxies grow with time.   The 'eta' values are the ratio of the surface brightness at a radius to the surface brightness within a radius. https://t.co/8GSqk37aEt"", 'Taking the difference in the outer and inner radii it is clear that the outer radii are growing at a fast rate than the inner radii at z &lt; 7.   We confirm that this is an actual effect and not a bias by carrying out extensive simulations. (4/n) https://t.co/z4SgezyTok', 'There is more detail in the paper (have a look) but in general this demonstrates that inside-out growth of galaxies is the dominate process for forming galaxies at z=7 down to z=1.   How this happens is another question which we discuss, but minor mergers is one good way. (5/n)']",https://arxiv.org/abs/1911.02589,"We present a size analysis of a sample of $\sim$ 49,000 galaxies from the CANDELS GOODS North and South fields using redshift-independent relative surface brightness metrics to determine an unbiased measure of the differential size evolution of galaxies at $1 \leq z \leq 7$. We introduce a novel method of removing foreground objects from distant galaxy ($z > 3$) images that makes use of the Lyman-break at 912{\AA}, in what we call `2-D Lyman-Break Imaging'. The images used are in the rest-frame optical at $z < 3$ and progressively bluer bands at $z > 3$. They are therefore subject to K-correction and cosmological dimming effects which are tested and corrected for. We separately consider a mass-selected sample (with masses in the range 10$^9$M$_{\odot}$$\leq$M$_*$$\leq$10$^{10.5}$M$_{\odot}$) and a number density selected sample (using a constant number density of $n = 1\times10^{-4}$Mpc$^{-3}$). Instead of utilising the commonly used, but potentially biased, effective radii for size measurements, we measure the redshift-independent Petrosian radius, defined by the parameter $\eta$, for each galaxy for three values of $\eta$ and use this as a proxy for size. The evolution of the measured radii can be described by a power-law of the form $R_{Petr} = \alpha(1+z)^\beta$kpc where $\beta < 0$. We find that the outer radius increases more rapidly, suggesting that as a galaxy grows mass is added to its outer regions via an inside-out growth. This growth is stronger for the number density selected sample, with a growth rate of nearly three in the outer radii compared to the inner. We test and confirm these results using a series of image simulations. ","Unbiased Differential Size Evolution and the Inside-Out Growth of
  Galaxies in the Deep CANDELS GOODS Fields at $1 \leq z \leq 7$"
51,1194227346838036486,23000769,Christopher Conselice,['New paper out today led by Nottingham PhD student @AstroSunnyC on using an autoencoder to find gravitational lenses with unsupervised machine learning.  This method will have to be used with @EC_Euclid and LSST etc. to find unique strong lens systems.\n\n<LINK>'],https://arxiv.org/abs/1911.04320,"In this paper we develop a new unsupervised machine learning technique comprised of a feature extractor, a convolutional autoencoder (CAE), and a clustering algorithm consisting of a Bayesian Gaussian mixture model (BGM). We apply this technique to visual band space-based simulated imaging data from the Euclid Space Telescope using data from the Strong Gravitational Lenses Finding Challenge. Our technique promisingly captures a variety of lensing features such as Einstein rings with different radii, distorted arc structures, etc, without using predefined labels. After the clustering process, we obtain several classification clusters separated by different visual features which are seen in the images. Our method successfully picks up $\sim$63\ percent of lensing images from all lenses in the training set. With the assumed probability proposed in this study, this technique reaches an accuracy of $77.25\pm 0.48$\% in binary classification using the training set. Additionally, our unsupervised clustering process can be used as the preliminary classification for future surveys of lenses to efficiently select targets and to speed up the labelling process. As the starting point of the astronomical application using this technique, we not only explore the application to gravitationally lensed systems, but also discuss the limitations and potential future uses of this technique. ","Identifying Strong Lenses with Unsupervised Machine Learning using
  Convolutional Autoencoder"
52,1194225459086671873,245373685,Javier Nogales,['Our new paper finished: massive clustering of time-series <LINK> #datascience #smartmeters #energyanalytics <LINK>'],https://arxiv.org/abs/1911.03336,"In order to improve the efficiency and sustainability of electricity systems, most countries worldwide are deploying advanced metering infrastructures, and in particular household smart meters, in the residential sector. This technology is able to record electricity load time series at a very high frequency rates, information that can be exploited to develop new clustering models to group individual households by similar consumptions patterns. To this end, in this work we propose three hierarchical clustering methodologies that allow capturing different characteristics of the time series. These are based on a set of ""dissimilarity"" measures computed over different features: quantile auto-covariances, and simple and partial autocorrelations. The main advantage is that they allow summarizing each time series in a few representative features so that they are computationally efficient, robust against outliers, easy to automatize, and scalable to hundreds of thousands of smart meters series. We evaluate the performance of each clustering model in a real-world smart meter dataset with thousands of half-hourly time series. The results show how the obtained clusters identify relevant consumption behaviors of households and capture part of their geo-demographic segmentation. Moreover, we apply a supervised classification procedure to explore which features are more relevant to define each cluster. ","Hierarchical Clustering for Smart Meter Electricity Loads based on
  Quantile Autocovariances"
53,1194216713962872832,938560263334387712,Alexandria Volkening,"['Uploaded a new paper on stripe patterns on growing fish fins: <LINK> (in collaboration with @BjornSandstede and undergraduate students MR Abbott, D Catey, N Chandra, B Dubois, &amp; F Lim). And I got to include a pun at the tail end 😀. #zebrafish #tailfins #soPunny <LINK>']",https://arxiv.org/abs/1911.03758,"As zebrafish develop, black and gold stripes form across their skin due to the interactions of brightly colored pigment cells. These characteristic patterns emerge on the growing fish body, as well as on the anal and caudal fins. While wild-type stripes form parallel to a horizontal marker on the body, patterns on the tailfin gradually extend distally outward. Interestingly, several mutations lead to altered body patterns without affecting fin stripes. Through an exploratory modeling approach, our goal is to help better understand these differences between body and fin patterns. By adapting a prior agent-based model of cell interactions on the fish body, we present an in silico study of stripe development on tailfins. Our main result is a demonstration that two cell types can produce stripes on the caudal fin. We highlight several ways that bone rays, growth, and the body-fin interface may be involved in patterning, and we raise questions for future work related to pattern robustness. ",Modeling stripe formation on growing zebrafish tailfins
54,1194084067886526464,823277120944242689,Will Kinney,['New paper with my student Wei-Chen Lin.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1911.03736,"We propose a more general version of the Trans-Planckian Censorship Conjecture (TCC) which can apply to models of inflation with varying speed of sound. We find that inflation models with $c_S < 1$ are in general more strongly constrained by censorship of trans-Planckian modes than canonical inflation models, with the upper bound on the tensor/scalar ratio reduced by as much as three orders of magnitude for sound speeds consistent with bounds from data. In particular, models which satisfy the TCC, and therefore the de Sitter Swampland Conjecture, can still violate the more general condition for non-classicality of trans-Planckian modes. As a concrete example, we apply the constraint to Dirac-Born-Infeld inflation models motivated by string theory. ",Trans-Planckian Censorship and $k$-inflation
55,1194069930955530242,1146409804610584577,Alexander Jahn,['Our new paper on central charges of holographic tensor network models is out. A lot of thanks to Zoltán Zimborás and @jenseisert! <LINK>'],https://arxiv.org/abs/1911.03485,"Central to the AdS/CFT correspondence is a precise relationship between the curvature of an anti-de Sitter (AdS) spacetime and the central charge of the dual conformal field theory (CFT) on its boundary. Our work shows that such a relationship can also be established for tensor network models of AdS/CFT based on regular bulk geometries, leading to an analytical form of the maximal central charges exhibited by the boundary states. We identify a class of tensors based on Majorana dimer states that saturate these bounds in the large curvature limit, while also realizing perfect and block-perfect holographic quantum error correcting codes. Furthermore, the renormalization group description of the resulting model is shown to be analogous to the strong disorder renormalization group, thus giving the first example of an exact quantum error correcting code that gives rise to a well-understood critical system. These systems exhibit a large range of fractional central charges, tunable by the choice of bulk tiling. Our approach thus provides a precise physical interpretation of tensor network models on regular hyperbolic geometries and establishes quantitative connections to a wide range of existing models. ",Central charges of aperiodic holographic tensor network models
56,1194062996961517568,11778512,Mason Porter,"['My new paper: ""Nonlinearity + Networks: A 2020 Vision"": <LINK>\n\n""I highlight a few methods and ideas, including several of personal interest, that I anticipate to be especially important during the next several years.""\n\nCan you find all of the easter eggs in it?']",https://arxiv.org/abs/1911.03805,"I briefly survey several fascinating topics in networks and nonlinearity. I highlight a few methods and ideas, including several of personal interest, that I anticipate to be especially important during the next several years. These topics include temporal networks (in which the entities and/or their interactions change in time), stochastic and deterministic dynamical processes on networks, adaptive networks (in which a dynamical process on a network is coupled to dynamics of network structure), and network structure and dynamics that include ""higher-order"" interactions (which involve three or more entities in a network). I draw examples from a variety of scenarios, including contagion dynamics, opinion models, waves, and coupled oscillators. ",Nonlinearity + Networks: A 2020 Vision
57,1193967646477127680,548718054,Marc Khoury,"['How good is a triangulation as an approximation to a smooth surface? In a new paper, we prove sharp bounds on the interpolation and normal error for points clouds sampled from smooth surfaces and manifolds. With Jonathan Shewchuk. \n\n<LINK>', '@paul_pearce Depends on the triangulation and the density of the samples. The bounds are statements of the type if a triangle is ""small"", wrt a certain measure, and the sample is dense, then the triangle normal closely approximates the true normal at the vertices.', ""@paul_pearce Given a triangulation these statements allow you to measure its quality. A lot of the effort is in establish the sharpest possible bound wrt constants. Many of these results are central to provable manifold reconstruction and haven't been improved in years."", '@paul_pearce Even better: arbitrarily good.']",https://arxiv.org/abs/1911.03424,"How good is a triangulation as an approximation of a smooth curved surface or manifold? We provide bounds on the {\em interpolation error}, the error in the position of the surface, and the {\em normal error}, the error in the normal vectors of the surface, as approximated by a piecewise linearly triangulated surface whose vertices lie on the original, smooth surface. The interpolation error is the distance from an arbitrary point on the triangulation to the nearest point on the original, smooth manifold, or vice versa. The normal error is the angle separating the vector (or space) normal to a triangle from the vector (or space) normal to the smooth manifold (measured at a suitable point near the triangle). We also study the {\em normal variation}, the angle separating the normal vectors (or normal spaces) at two different points on a smooth manifold. Our bounds apply to manifolds of any dimension embedded in Euclidean spaces of any dimension, and our interpolation error bounds apply to simplices of any dimension, although our normal error bounds apply only to triangles. These bounds are expressed in terms of the sizes of suitable medial balls (the {\em empty ball size} or {\em local feature size} measured at certain points on the manifold), and have applications in Delaunay triangulation-based algorithms for provably good surface reconstruction and provably good mesh generation. Our bounds have better constants than the prior bounds we know of---and for several results in higher dimensions, our bounds are the first to give explicit constants. ","Approximation Bounds for Interpolation and Normals on Triangulated
  Surfaces and Manifolds"
58,1193946899142905857,3257473185,Marcus D. R. Klarqvist ⚪,"['New paper by me, @pshufb, @lemire describing the novel positional population count operation using SIMD instructions. <LINK>', 'a) The classic popcount operation computes the number of set bits in a machine word\nb) The positional popcount operation computes the bitwise histogram of an input stream of words https://t.co/SPdSg93qcS', 'Associated code with paper\nhttps://t.co/oJMHOsB6QO', 'Our best approach uses up to 400 times fewer instructions and is up to 50 times faster than baseline code using only regular (non-SIMD) instructions.']",https://arxiv.org/abs/1911.02696,"In several fields such as statistics, machine learning, and bioinformatics, categorical variables are frequently represented as one-hot encoded vectors. For example, given 8 distinct values, we map each value to a byte where only a single bit has been set. We are motivated to quickly compute statistics over such encodings. Given a stream of k-bit words, we seek to compute k distinct sums corresponding to bit values at indexes 0, 1, 2, ..., k-1. If the k-bit words are one-hot encoded then the sums correspond to a frequency histogram. This multiple-sum problem is a generalization of the population-count problem where we seek the sum of all bit values. Accordingly, we refer to the multiple-sum problem as a positional population-count. Using SIMD (Single Instruction, Multiple Data) instructions from recent Intel processors, we describe algorithms for computing the 16-bit position population count using less than half of a CPU cycle per 16-bit word. Our best approach uses up to 400 times fewer instructions and is up to 50 times faster than baseline code using only regular (non-SIMD) instructions, for sufficiently large inputs. ","Efficient Computation of Positional Population Counts Using SIMD
  Instructions"
59,1193903144691830784,1053370661798924294,James Johnson,"['New paper on the @arxiv today! We explore chemical evolution models with simple starbursts, and discuss their potential observational signatures. <LINK>', '@arxiv Fun fact: while alpha enhanced stars are commonly invoked as the observational signature of a starburst, our models suggest that under some starburst models, alpha deficient stars form as well https://t.co/H1gvvQL6jN']",https://arxiv.org/abs/1911.02598,"We investigate the impact of bursts in star formation on the predictions of one-zone chemical evolution models, adopting oxygen (O), iron (Fe), and strontium (Sr), as representative $\alpha$, iron-peak, and s-process elements, respectively. To this end, we develop the Versatile Integrator for Chemical Evolution (VICE), a python package. Starbursts driven by a temporary boost of gas accretion rate create loops in [O/Fe]-[Fe/H] evolutionary tracks and a peak in the stellar [O/Fe] distribution at intermediate values. Bursts driven by a temporary boost of star formation efficiency have a similar effect, and they also produce a population of $\alpha$-deficient stars during the depressed star formation phase that follows the burst. This $\alpha$-deficient population is more prominent if the outflow rate is tied to a time-averaged star formation rate (SFR) instead of the instantaneous SFR. Theoretical models of Sr production predict a strong metallicity dependence of supernova and asymptotic giant branch (AGB) star yields, though comparison to data suggests an additional source that is nearly metallicity-independent. Evolution of [Sr/Fe] and [Sr/O] during a starburst is complex because of the yield metallicity dependence and the multiple timescales in play. Moderate amplitude (10-20\%) sinusoidal oscillations in SFR produce loops in [O/Fe]-[Fe/H] tracks and multiple peaks in [O/Fe] distributions, which could be one source of intrinsic scatter in observed sequences. We investigate models that have a factor of ~2 enhancement of SFR at t = 12 Gyr, as suggest by some recent Milky Way observations. A late episode of enhanced star formation could help explain the existence of young stars with moderate $\alpha$-enhancements and the surprisingly young median age found for solar metallicity stars in the solar neighborhood, while also raising the possibility that this starburst has not fully decayed. ",The Impact of Starbursts on Element Abundance Ratios
60,1193877668506169345,795877354266456064,KoheiKamadaPhys,['New paper has appeared! <LINK> Practically first paper as faculty.'],https://arxiv.org/abs/1911.02657,"We revisit the gravitational leptogenesis scenario in which the inflaton is coupled to gravity by the Chern-Simons term and the lepton asymmetry is generated through the gravitational anomaly in the lepton number current during inflation. We constrain the possible model parameter space by requiring the absence of ghost modes below the Planck scale, which would suggest the breakdown of the effective theory, and evaluate the net baryon asymmetry for various reheating processes. We find that the mechanism with these requirements is insufficient to explain the observed baryon asymmetry of the Universe for standard reheating scenarios. We show that, however, with the kination scenario realized in e.g. the k- and G-inflation models, a sufficient baryon asymmetry can be generated within a feasible range of the model parameters. ",Gravitational leptogenesis with kination and gravitational reheating
61,1193829042379726848,523241142,Juste Raimbault,"['New paper by @pumain1 and myself: ""Perspectives on urban theories"", conclusion of the forthcoming ""Theories and models of urbanization"" Geodivercity book, combining theoretical insights into urban theories with quantitative epistemology <LINK>']",https://arxiv.org/abs/1911.02854,"At the end of the five years of work in our GeoDiverCity program, we brought together a diversity of authors from different disciplines. Each person was invited to present an important question about the theories and models of urbanization. They are representative of a variety of currents in urban research. Rather than repeat here the contents of all chapters, we propose two ways to synthesize the scientific contributions of this book. In a first part we replace them in relation to a few principles that were experimented in our program, and in a second part we situate them with respect to a broader view of international literature on these topics. ",Perspectives on urban theories
62,1193711549015187457,17797390,Sharad Goel,"[""In a new paper, @iamwillcai, Hans Gaebler, @NikhGarg, and I propose a simple algorithm to increase fairness of allocation decisions (like gov't loans) by additionally screening people who would otherwise be rejected but may in fact be strong candidates. <LINK> <LINK>"", '@iamwillcai @NikhGarg and Hans - aka @jgaeb1 - is now on twitter to celebrate!']",https://arxiv.org/abs/1911.02715,"Public and private institutions must often allocate scare resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers -- before allocating resources -- can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information -- like those without credit scores -- our approach can substantially improve the allocation of limited assets. ",Fair Allocation through Selective Information Acquisition
63,1193031432664440832,1123977306979041288,Sylvia Biscoveanu,"['If you like gamma-ray bursts, relativistic jets, neutron stars, gravitational waves, or hierarchical Bayesian inference check out my new paper with Eric Thrane and @sasomao! <LINK> 1/n', 'We develop a new method to combine information from gravitational wave and gamma-ray burst observations to constrain the properties of the GRB jet like the opening angle, Lorentz factor, energy, and angular structure. 2/n', 'This method is completely independent of afterglow observations (no jet break needed) because we parametrize the observed GRB fluence in terms of the jet properties and the inclination angle and distance to the source from GW observations. 3/n', 'We find that we can constrain the hyper-parameters of the distributions governing the jet parameters with a population of 100 simulated GW events with either a GRB detection or fluence upper limit for both top-hat and power-law structured jets. 4/n', 'Measurements like these can be used to shed light on the enigmatic GRB central engine and on the circum-burst environment! 5/n', 'As a side note this paper took two years longer than originally planned and I nearly lost my faith in Bayesian inference but I learned a lot about GRBs and jets and hierarchical modeling but I’m very glad it’s ready for public consumption now #inbayeswetrust']",https://arxiv.org/abs/1911.01379,"Gamma-ray burst (GRB) prompt emission is highly beamed, and understanding the jet geometry and beaming configuration can provide information on the poorly understood central engine and circum-burst environment. Prior to the advent of gravitational-wave astronomy, astronomers relied on observations of jet breaks in the multi-wavelength afterglow to determine the GRB opening angle, since the observer's viewing angle relative to the system cannot be determined from the electromagnetic data alone. Gravitational-wave observations, however, provide an independent measurement of the viewing angle. We describe a Bayesian method for determining the geometry of short GRBs using coincident electromagnetic and gravitational-wave observations. We demonstrate how an ensemble of multi-messenger detections can be used to measure the distributions of the jet energy, opening angle, Lorentz factor, and angular profile of short GRBs; we find that for a population of 100 such observations, we can constrain the mean of the opening angle distribution to within 10 degrees regardless of the angular emission profile. Conversely, the constraint on the energy distribution depends on the shape of the profile, which can be distinguished. ","Constraining short gamma-ray burst jet properties with gravitational
  waves and gamma rays"
64,1192851687758868480,69202541,Jonathan Le Roux,"['New paper by Fatemeh Pishdadian, Gordon Wichern, and myself on how to train separation algorithms without access to isolated sounds: ""Finding Strength in Weakness: Learning to Separate Sounds with Weak Supervision"" <LINK>']",https://arxiv.org/abs/1911.02182,"While there has been much recent progress using deep learning techniques to separate speech and music audio signals, these systems typically require large collections of isolated sources during the training process. When extending audio source separation algorithms to more general domains such as environmental monitoring, it may not be possible to obtain isolated signals for training. Here, we propose objective functions and network architectures that enable training a source separation system with weak labels. In this scenario, weak labels are defined in contrast with strong time-frequency (TF) labels such as those obtained from isolated sources, and refer either to frame-level weak labels where one only has access to the time periods when different sources are active in an audio mixture, or to clip-level weak labels that only indicate the presence or absence of sounds in an entire audio clip. We train a separator that estimates a TF mask for each type of sound event, using a sound event classifier as an assessor of the separator's performance to bridge the gap between the TF-level separation and the ground truth weak labels only available at the frame or clip level. Our objective function requires the classifier applied to a separated source to assign high probability to the class corresponding to that source and low probability to all other classes. The objective function also enforces that the separated sources sum up to the mixture. We benchmark the performance of our algorithm using synthetic mixtures of overlapping events created from a database of sounds recorded in urban environments. Compared to training a network using isolated sources, our model achieves somewhat lower but still significant SI-SDR improvement, even in scenarios with significant sound event overlap. ","Finding Strength in Weakness: Learning to Separate Sounds with Weak
  Supervision"
65,1192778772724035589,789125136481910784,Raphael Shirley,"['Have we seen all the galaxies that comprise the cosmic infrared background at 250\\,μm ≤λ≤ 500\\,μm? <LINK>\nNew paper from Steven Duivenvoorden and others.']",https://arxiv.org/abs/1911.01437,"The cosmic infrared background (CIB) provides a fundamental observational constraint on the star-formation history of galaxies over cosmic history. We estimate the contribution to the CIB from catalogued galaxies in the COSMOS field by using a novel map fitting technique on the \textit{Herschel} SPIRE maps. Prior galaxy positions are obtained using detections over a large range in wavelengths in the $K_{\rm s}$--3\,GHz range. Our method simultaneously fits the galaxies, the system foreground, and the leakage of flux from galaxies located in masked areas and corrects for an ""over-fitting"" effect not previously accounted for in stacking methods. We explore the contribution to the CIB as a function of galaxy survey wavelength and depth. We find high contributions to the CIB with the deep $r$ ($m_{\rm AB} \le 26.5$), $K_{\rm s}$ ($m_{\rm AB} \le 24.0$) and 3.6\,$\mu$m ($m_{\rm AB} \le 25.5$) catalogues. We combine these three deep catalogues and find a total CIB contributions of 10.5 $\pm$ 1.6, 6.7 $\pm$ 1.5 and 3.1 $\pm$ 0.7\,nWm$^{-2}$sr$^{-1}$ at 250, 350 and 500\,$\mu$m, respectively. Our CIB estimates are consistent with recent phenomenological models, prior based SPIRE number counts and with (though more precise than) the diffuse total measured by FIRAS. Our results raise the interesting prospect that the CIB contribution at $\lambda \le 500\,\mu$m from known galaxies has converged. Future large-area surveys like those with the Large Synoptic Survey Telescope are therefore likely to resolve a substantial fraction of the population responsible for the CIB at 250\,$\mu$m $\leq \lambda \leq$ 500\,$\mu$m. ","Have we seen all the galaxies that comprise the cosmic infrared
  background at 250\,$\mu$m $\le \lambda \le$ 500\,$\mu$m?"
66,1192425504298668032,34870460,Elisa Quintana,"['Check out this new paper by Jonathan Brande @YoniAstro: The Feasibility of Directly Imaging Nearby Cold Jovian Planets with MIRI/JWST  <LINK>', 'Jonathan (Yoni) is an awesome postbac at @NASAGoddard working on exoplanet detection and dynamics using #TESS and #JWST (and applying to grad programs!)']",https://arxiv.org/abs/1911.02022,"The upcoming launch of the James Webb Space Telescope (JWST) will dramatically increase our understanding of exoplanets, particularly through direct imaging. Microlensing and radial velocity surveys indicate that some M-dwarfs host long period giant planets. Some of these planets will likely be just a few parsecs away and a few AU from their host stars, a parameter space that cannot be probed by existing high-contrast imagers. We studied whether the coronagraphs on the Mid-Infrared Instrument on JWST can detect Jovian-type planets around nearby M-dwarfs. For a sample of 27 very nearby M-dwarfs, we simulated a sample of Saturn--Jupiter-mass planets with three atmospheric configurations, three orbital separations, observed in three different filters. We found that the f1550c $15.5\mu$m filter is best suited for detecting Jupiter-like planets. Jupiter-like planets with patchy cloud cover, 2 AU from their star, are detectable at $15.5\mu$m around 14 stars in our sample, while Jupiters with clearer atmospheres are detectable around all stars in the sample. Saturns were most detectable at 10.65 and $11.4\mu$m (f1065c and f1140c filters), but only with cloud-free atmospheres and within 3 pc (6 stars). Surveying all 27 stars would take $<170$ hours of JWST integration time, or just a few hours for a shorter survey of the most favorable targets. There is one potentially detectable known planet in our sample -- GJ~832~b. Observations aimed at detecting this planet should occur in 2024--2026, when the planet is maximally separated from the star. ","The Feasibility of Directly Imaging Nearby Cold Jovian Planets with
  MIRI/JWST"
67,1192364892809875456,20174338,Daniel Cotton,"['We (@JeremyBailey5, @quasibody, Ain De Horta, Darren Maybour)  have a new paper out describing HIPPI-2 – the 2nd generation of our high precision FLC-based polarimeter: <LINK>', ""@JeremyBailey5 @quasibody HIPPI-2 can be used on everything from 0.5-m to 8-m telescopes. And it's achieving better precision than any other optical polarimeter. It's about 3 parts-per-million (ppm) at green wavelengths and approaches 1 ppm in the reddest bands.""]",https://arxiv.org/abs/1911.02123,"We describe the High-Precision Polarimetric Instrument-2 (HIPPI-2) a highly versatile stellar polarimeter developed at the University of New South Wales (UNSW). Two copies of HIPPI-2 have been built and used on the 60-cm telescope at Western Sydney University's (WSU) Penrith Observatory, the 8.1-m Gemini North Telescope at Mauna Kea and extensively on the 3.9-m Anglo-Australian Telescope (AAT). The precision of polarimetry, measured from repeat observations of bright stars in the SDSS g' band, is better than 3.5 ppm (parts per million) on the 3.9-m AAT and better than 11 ppm on the 60-cm WSU telescope. The precision is better at redder wavelengths and poorer in the blue. On the Gemini North 8-m telescope the performance is limited by a very large and strongly wavelength dependent telescope polarization that reached 1000's of ppm at blue wavelengths and is much larger than we have seen on any other telescope. ",HIPPI-2: A Versatile High Precision Polarimeter
68,1192351040512892929,948995274961309697,Andrea Botteon,"['Paper day: ultra-steep synchrotron sources in galaxy clusters, by Mandal+ (@astro_jit)\nThree new cluster revived fossil plasma sources (radio phoenixes) are studied. Sources were selected from NVSS and TGSS and followed-up with @LOFAR and GMRT 1/2\n<LINK> <LINK>', 'These sources have peculiar/filamentary morphologies. Multi-frequency data show complex spectral index distributions in the sources and hints of spectral curvature. Their emission is likely related to AGN fossil plasma revived by shock/ICM motions 2/2']",https://arxiv.org/abs/1911.02034,"It is well established that particle acceleration by shocks and turbulence in the intra-cluster medium can produce cluster-scale synchrotron emitting sources. However, the detailed physics of these particle acceleration processes is still not well understood. One of the main open questions is the role of fossil relativistic electrons that have been deposited in the intra-cluster medium by radio galaxies. These synchrotron-emitting electrons are very difficult to study, as their radiative life time is only tens of Myrs at GHz frequencies, and are therefore a relatively unexplored population. Despite the typical steep radio spectrum due to synchrotron losses, these fossil electrons are barely visible even at radio frequencies well below a GHz. However, when a pocket of fossil radio plasma is compressed, it boosts the visibility at sub-GHz frequencies, creating so-called radio phoenices. This compression can be the result of bulk motion and shocks in the ICM due to merger activity. In this paper, we demonstrate the discovery potential of low frequency radio sky surveys to find and study revived fossil plasma sources in galaxy clusters. We used the 150~MHz TGSS and 1.4 GHz NVSS sky surveys to identify candidate radio phoenices. A subset of three candidates were studied in detail using deep multi-band radio observations (LOFAR and GMRT), X-ray (\textit{Chandra} or \textit{XMM-Newton}) and archival optical observations. Two of the three sources are new discoveries. Using these observations, we identified common observational properties (radio morphology, ultra-steep spectrum, X-ray luminosity, dynamical state) that will enable us to identify this class of sources more easily, and help to understand the physical origin of these sources. ",Revived Fossil Plasma Sources in Galaxy Clusters
69,1192294318360412166,1138762581164855298,Christoph Ternes,"['New paper on @arxiv,  <LINK> with @MariamTortola. We show that models with steriles Neutrinos and altered dispersion relations (which were claimed to explain global oscillation data, including SBL anomalies) are actually ruled out by current oscillation data.']",https://arxiv.org/abs/1911.02329,"In this paper we investigate neutrino oscillations with altered dispersion relations in the presence of sterile neutrinos. Modified dispersion relations represent an agnostic way to parameterize new physics. Models of this type have been suggested to explain global neutrino oscillation data, including deviations from the standard three-neutrino paradigm as observed by a few experiments. We show that, unfortunately, in this type of models new tensions arise turning them incompatible with global data. ",Sterile neutrinos with altered dispersion relations revisited
70,1192261673383604225,22216766,Noah Stephens-Davidowitz,"['New paper with Divesh Aggarwal, Huck Bennett, and Sasha Golovnev. Fine-grained hardness of CVP(P)--- Everything that we can prove (and nothing else).\n<LINK> .']",https://arxiv.org/abs/1911.02440,"We show a number of fine-grained hardness results for the Closest Vector Problem in the $\ell_p$ norm ($\mathrm{CVP}_p$), and its approximate and non-uniform variants. First, we show that $\mathrm{CVP}_p$ cannot be solved in $2^{(1-\varepsilon)n}$ time for all $p \notin 2\mathbb{Z}$ and $\varepsilon > 0$, assuming the Strong Exponential Time Hypothesis (SETH). Second, we extend this by showing that there is no $2^{(1-\varepsilon)n}$-time algorithm for approximating $\mathrm{CVP}_p$ to within a constant factor $\gamma$ for such $p$ assuming a ""gap"" version of SETH, with an explicit relationship between $\gamma$, $p$, and the arity $k = k(\varepsilon)$ of the underlying hard CSP. Third, we show the same hardness result for (exact) $\mathrm{CVP}_p$ with preprocessing (assuming non-uniform SETH). For exact ""plain"" $\mathrm{CVP}_p$, the same hardness result was shown in [Bennett, Golovnev, and Stephens-Davidowitz FOCS 2017] for all but finitely many $p \notin 2\mathbb{Z}$, where the set of exceptions depended on $\varepsilon$ and was not explicit. For the approximate and preprocessing problems, only very weak bounds were known prior to this work. We also show that the restriction to $p \notin 2\mathbb{Z}$ is in some sense inherent. In particular, we show that no ""natural"" reduction can rule out even a $2^{3n/4}$-time algorithm for $\mathrm{CVP}_2$ under SETH. For this, we prove that the possible sets of closest lattice vectors to a target in the $\ell_2$ norm have quite rigid structure, which essentially prevents them from being as expressive as $3$-CNFs. We prove these results using techniques from many different fields, including complex analysis, functional analysis, additive combinatorics, and discrete Fourier analysis. E.g., along the way, we give a new (and tighter) proof of Szemer\'{e}di's cube lemma for the boolean cube. ","Fine-grained hardness of CVP(P) -- Everything that we can prove (and
  nothing else)"
71,1192121587467784192,68746721,François Chollet,"['I\'ve just released a fairly lengthy paper on defining &amp; measuring intelligence, as well as a new AI evaluation dataset, the ""Abstraction and Reasoning Corpus"". I\'ve been working on this for the past 2 years, on &amp; off.\n\nPaper: <LINK>\n\nARC: <LINK> <LINK>', ""The paper contains:\n\n1) A discussion of the history of how we've defined and evaluated AI so far\n\n2) A new formal definition of intelligence + concrete guidelines for what a general AI benchmark should look like\n\n3) A presentation of the goals of ARC and the logic behind it."", ""This definition is my attempt to formalize things I've been talking about for the past 10 years (in talks, conversations, on Twitter), into something useful and actionable. ARC itself has led me towards very interesting research directions, and I hope you will find it useful too."", 'I want to emphasize that the definition provided here isn\'t meant to be the ""one true"" definition. That\'s not the point. Intelligence is complex, and may be different things in different contexts. Many possible definitions of intelligence or AI can be valid.', ""Lately I've made good progress on an algorithm that is capable of solving at least part of ARC, based on a theory of cognition (autonomous abstraction) that I've been working on for a long time. I hope to be able to share these ideas &amp; code soon"", 'The theory has been in the making since 2009. It draws significant elements from project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), which was a general AI architecture I worked on from 2009 to 2012 (then again briefly in 2014), then mostly abandoned', 'ONEIROS started from a classic RL mindset, and centered on 1) learning modular-hierarchical ""maps"" of spatiotemporal features (using PMI matrix factorization as opposed to gradient descent), 2) context-switching via a form of attention, and 3) intrinsic motivation (curiosity) https://t.co/fMNMkZAEIL', 'The tagline was ""cognition is a dynamic, hierarchical-modular map of the space of sensorimotor information"". I also called it ""map theory"" -- there are perhaps two people in the world who will remember me preaching about it around 2010-2012', ""Dr Filliat @drFilliat may remember it because I did my best to explain it to him in 2011. I won't @ the other person"", '@drFilliat I think ONEIROS got a number of things right (especially given the context at the time -- this was mostly designed in 2010, with an attempt to implement a simplification of it in 2012), but ultimately it did not touch the most essential point: the nature of abstraction', ""@drFilliat Which is what I'm working on now. This is fundamentally what ARC is about"", '@drFilliat Fun fact: the name ""Keras"" derives from ONEIROS (cf. the gates from book 19 of the Odyssey)', '@MiguelCRomao Of course it was typeset in LaTeX. I mean, the .tex source is literally available for download from ArXiv. Weird reply tbh', ""@GzdeGlSahin1 We'll run a public competition on the benchmark, which should provide concrete answers to this question.""]",https://arxiv.org/abs/1911.01547,"To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ""buy"" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans. ",On the Measure of Intelligence
72,1192112726069436416,1085750147114508289,Karthik Ramanathan,['What kind of cool science can you do with an intense source of neutrons like at the future @essneutron? Turns out you can use the huge byproduct flux of neutrinos to study their nature and constrain a lot of new physics! Check out our paper at <LINK>'],https://arxiv.org/abs/1911.00762,"The European Spallation Source (ESS), presently well on its way to completion, will soon provide the most intense neutron beams for multi-disciplinary science. Fortuitously, it will also generate the largest pulsed neutrino flux suitable for the detection of Coherent Elastic Neutrino-Nucleus Scattering (CE$\nu$NS), a process recently measured for the first time at ORNL's Spallation Neutron Source. We describe innovative detector technologies maximally able to profit from the order-of-magnitude increase in neutrino flux provided by the ESS, along with their sensitivity to a rich particle physics phenomenology accessible through high-statistics, precision CE$\nu$NS measurements. ","Coherent Elastic Neutrino-Nucleus Scattering at the European Spallation
  Source"
73,1192012376171671552,1115880604560691200,NII Yamagishi Lab,['We are happy to announce that our new paper on the ASVspoof 2019 database (submitted to CSL) is now available on ArXiv!  <LINK>'],https://arxiv.org/abs/1911.01601,"Automatic speaker verification (ASV) is one of the most natural and convenient means of biometric person recognition. Unfortunately, just like all other biometric systems, ASV is vulnerable to spoofing, also referred to as ""presentation attacks."" These vulnerabilities are generally unacceptable and call for spoofing countermeasures or ""presentation attack detection"" systems. In addition to impersonation, ASV systems are vulnerable to replay, speech synthesis, and voice conversion attacks. The ASVspoof 2019 edition is the first to consider all three spoofing attack types within a single challenge. While they originate from the same source database and same underlying protocol, they are explored in two specific use case scenarios. Spoofing attacks within a logical access (LA) scenario are generated with the latest speech synthesis and voice conversion technologies, including state-of-the-art neural acoustic and waveform model techniques. Replay spoofing attacks within a physical access (PA) scenario are generated through carefully controlled simulations that support much more revealing analysis than possible previously. Also new to the 2019 edition is the use of the tandem detection cost function metric, which reflects the impact of spoofing and countermeasures on the reliability of a fixed ASV system. This paper describes the database design, protocol, spoofing attack implementations, and baseline ASV and countermeasure results. It also describes a human assessment on spoofed data in logical access. It was demonstrated that the spoofing data in the ASVspoof 2019 database have varied degrees of perceived quality and similarity to the target speakers, including spoofed data that cannot be differentiated from bona-fide utterances even by human subjects. ","ASVspoof 2019: A large-scale public database of synthesized, converted
  and replayed speech"
74,1191975791850070018,1702174146,Janis Keuper,"['Is this an image of real person? #deepfakes are hard to identify... in our new paper ""Unmasking DeepFakes with simple Features"" we are able to solve #deefake detection in face images with up to 100% accuracy on several benchmarks: <LINK> <LINK>', 'paper: https://t.co/MFML2IyIMt #deepfakes #deepfake https://t.co/jXimdYiuV4', 'Source code for our #deepfake detection is available: https://t.co/CsEa5Prjic #deepfakes https://t.co/RnfVhWIuCO']",https://arxiv.org/abs/1911.00686,"Deep generative models have recently achieved impressive results for many real-world applications, successfully generating high-resolution and diverse samples from complex datasets. Due to this improvement, fake digital contents have proliferated growing concern and spreading distrust in image content, leading to an urgent need for automated ways to detect these AI-generated fake images. Despite the fact that many face editing algorithms seem to produce realistic human faces, upon closer examination, they do exhibit artifacts in certain domains which are often hidden to the naked eye. In this work, we present a simple way to detect such fake face images - so-called DeepFakes. Our method is based on a classical frequency domain analysis followed by basic classifier. Compared to previous systems, which need to be fed with large amounts of labeled data, our approach showed very good results using only a few annotated training samples and even achieved good accuracies in fully unsupervised scenarios. For the evaluation on high resolution face images, we combined several public datasets of real and fake faces into a new benchmark: Faces-HQ. Given such high-resolution images, our approach reaches a perfect classification accuracy of 100% when it is trained on as little as 20 annotated samples. In a second experiment, in the evaluation of the medium-resolution images of the CelebA dataset, our method achieves 100% accuracy supervised and 96% in an unsupervised setting. Finally, evaluating a low-resolution video sequences of the FaceForensics++ dataset, our method achieves 91% accuracy detecting manipulated videos. Source Code: this https URL ",Unmasking DeepFakes with simple Features
75,1191898083539460096,3222301354,Matthew Joseph,"['New paper: <LINK>. We revisit pan-privacy as a middle ground between central and local differential privacy, e.g. for uniformity testing over [k], sample complexity k dependence is Θ(k^1/2) for central, Θ(k) for local, and we show it’s Θ(k^2/3) for pan-privacy.', 'Some other stuff in there: pan-privacy against multiple intrusions is equivalent to sequentially interactive local privacy. And that previous local uniformity testing lower bound is for noninteractive protocols: we extend it to sequential interaction.', 'All of this is work with Kareem Amin and Jieming Mao from a fun summer at Google Research New York.', '@ccanonne_ If I understand the Goldreich result, uniformity ≽ identity reduction should go through, since the pan-private algorithm operator can still apply the same sequence of filters to each stream element (which they see in the clear) and then uniformity test on the filtered samples.', '@ccanonne_ I’m even less familiar with the question of instance optimal identity testing, I’ll have to look at that more.']",http://arxiv.org/abs/1911.01452,"A centrally differentially private algorithm maps raw data to differentially private outputs. In contrast, a locally differentially private algorithm may only access data through public interaction with data holders, and this interaction must be a differentially private function of the data. We study the intermediate model of pan-privacy. Unlike a locally private algorithm, a pan-private algorithm receives data in the clear. Unlike a centrally private algorithm, the algorithm receives data one element at a time and must maintain a differentially private internal state while processing this stream. First, we show that pure pan-privacy against multiple intrusions on the internal state is equivalent to sequentially interactive local privacy. Next, we contextualize pan-privacy against a single intrusion by analyzing the sample complexity of uniformity testing over domain $[k]$. Focusing on the dependence on $k$, centrally private uniformity testing has sample complexity $\Theta(\sqrt{k})$, while noninteractive locally private uniformity testing has sample complexity $\Theta(k)$. We show that the sample complexity of pure pan-private uniformity testing is $\Theta(k^{2/3})$. By a new $\Omega(k)$ lower bound for the sequentially interactive setting, we also separate pan-private from sequentially interactive locally private and multi-intrusion pan-private uniformity testing. ",Pan-Private Uniformity Testing
76,1191894165669343232,1075649842955866114,Luca Cortese,['New paper out today by joint KIAA/@ICRAR\n @UWAresearch postdoc Kexin Guo using @SAMI_survey @ARC_ASTRO3D  to show that most of the stars in the local Universe are found in rotating systems. <LINK> <LINK>'],https://arxiv.org/abs/1911.01433,"We use the complete Sydney-AAO Multi-object Integral field spectrograph (SAMI) Galaxy Survey to determine the contribution of slow rotators, as well as different types of fast rotators, to the stellar mass function of galaxies in the local Universe. We use stellar kinematics not only to discriminate between fast and slow rotators, but also to distinguish between dynamically cold systems (i.e., consistent with intrinsic axis ratios$<0.3$) and systems including a prominent dispersion-supported bulge. We show that fast rotators account for more than $80\%$ of the stellar mass budget of nearby galaxies, confirming that their number density overwhelms that of slow rotators at almost all masses from $10^{9}$ to $10^{11.5}{\rm M_\odot}$. Most importantly, dynamically cold disks contribute to at least $25\%$ of the stellar mass budget of the local Universe, significantly higher than what is estimated from visual morphology alone. For stellar masses up to $10^{10.5}{\rm M_\odot}$, this class makes up $>=30\%$ of the galaxy population in each stellar mass bin. The fact that many galaxies that are visually classified as having two-components have stellar spin consistent with dynamically cold disks suggests that the inner component is either rotationally-dominated (e.g., bar, pseudo-bulge) or has little effect on the global stellar kinematics of galaxies. ","The SAMI Galaxy Survey: The contribution of different kinematic classes
  to the stellar mass function of nearby galaxies"
77,1191764110897082369,3433220662,Anthony Bonato,"['New paper on the burning number of certain Cartesian grid graphs, or fences as we like to call them. <LINK> <LINK>']",https://arxiv.org/abs/1911.01342,"Graph burning studies how fast a contagion, modeled as a set of fires, spreads in a graph. The burning process takes place in synchronous, discrete rounds. In each round, a fire breaks out at a vertex, and the fire spreads to all vertices that are adjacent to a burning vertex. The burning number of a graph $G$ is the minimum number of rounds necessary for each vertex of $G$ to burn. We consider the burning number of the $m \times n$ Cartesian grid graphs, written $G_{m,n}$.\ For $m = \omega(\sqrt{n})$, the asymptotic value of the burning number of $G_{m,n}$ was determined, but only the growth rate of the burning number was investigated in the case $m = O(\sqrt{n})$, which we refer to as fence graphs. We provide new explicit bounds on the burning number of fence graphs $G_{c\sqrt{n},n}$, where $c > 0$. ",Improved Bounds for Burning Fence Graphs
78,1191695915700117504,216729597,Marcel S. Pawlowski,"['Our paper lead by Pengfei Li, @lellifede &amp; @DudeDarkmatter on a new measurement of the dark matter halo mass function from HI kinematics has been accepted! <LINK>\n\n@lellifede has you covered with the twitter summary of this work’s context and results. <LINK>']",https://arxiv.org/abs/1911.00517v1,"We present an empirical method to measure the halo mass function (HMF) of galaxies. We determine the relation between the \hi\ line-width from single-dish observations and the dark matter halo mass ($M_{200}$) inferred from rotation curve fits in the SPARC database, then we apply this relation to galaxies from the \hi\ Parkes All Sky Survey (HIPASS) to derive the HMF. This empirical HMF is well fit by a Schecther function, and matches that expected in $\Lambda$CDM over the range $10^{10.5} < M_{200} < 10^{12}\;\mathrm{M}_{\odot}$. More massive halos must be poor in neutral gas to maintain consistency with the power law predicted by $\Lambda$CDM. We detect no discrepancy at low masses. The lowest halo mass probed by HIPASS, however, is just greater than the mass scale where the Local Group missing satellite problem sets in. The integrated mass density associated with the dark matter halos of \hi-detected galaxies sums to $\Omega_{\rm m,gal} \approx 0.03$ over the probed mass range. ",] The halo mass function of late-type galaxies from HI kinematics
79,1191690795679670273,797888987675365377,Tom Rainforth,"['Our new paper ""A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments"" introduces a new approach for designing Bayesian-Optimal experiments in far higher dimensions than previously possible.\n\nCheck it out here: <LINK>', 'Joint work with Adam Foster, Matthew O’Meara, Martin Jankowiak, and @yeewhye']",https://arxiv.org/abs/1911.00294,"We introduce a fully stochastic gradient based approach to Bayesian optimal experimental design (BOED). Our approach utilizes variational lower bounds on the expected information gain (EIG) of an experiment that can be simultaneously optimized with respect to both the variational and design parameters. This allows the design process to be carried out through a single unified stochastic gradient ascent procedure, in contrast to existing approaches that typically construct a pointwise EIG estimator, before passing this estimator to a separate optimizer. We provide a number of different variational objectives including the novel adaptive contrastive estimation (ACE) bound. Finally, we show that our gradient-based approaches are able to provide effective design optimization in substantially higher dimensional settings than existing approaches. ","A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal
  Experiments"
80,1191659013911564288,1358895091,Karen Aplin,"[""New paper out on how charge in clouds isn't all about thunderstorms. arXiv version link for those without a journal subscription <LINK> <LINK>""]",https://arxiv.org/abs/1911.00410,"Charging of upper and lower horizontal boundaries of extensive layer clouds results from current flow in the global electric circuit. Layer-cloud charge accumulation has previously been considered a solely electrostatic phenomenon, but it does not occur in isolation from meteorological processes, which can transport charge. Thin layer clouds provide special circumstances for investigating this dynamical charge transport, as disruption at the cloud-top may reach the cloud base, observable from the surface. Here, a thin (~300 m) persistent layer-cloud with base at 300 m and strong wind shear at cloud-top was observed to generate strongly correlated fluctuations in cloud base height, optical thickness and surface electric Potential Gradient (PG) beneath. PG changes are identified to precede the cloud base fluctuations by 2 minutes, consistent with shear-induced cloud-top electrical changes followed by cloud base changes. These observations demonstrate, for the first time, dynamically driven modification of charge within a layer-cloud. Even in weakly charged layer-clouds, redistribution of charge will modify local electric fields within the cloud and the collisional behaviour of interacting charged cloud droplets. Local field intensification may also explain previously observed electrostatic discharges in warm clouds. ",Shear-induced electrical changes in the base of thin layer-cloud
81,1191639878683045888,907240748600512513,Alessio Brutti,['New paper on on-line supervised speaker diarization using sample mean loss by @DonkeyShot21:\n<LINK>\nCheck out the repo --&gt; <LINK>\n#speakerdiarization'],https://arxiv.org/abs/1911.01266,"Recently, a fully supervised speaker diarization approach was proposed (UIS-RNN) which models speakers using multiple instances of a parameter-sharing recurrent neural network. In this paper we propose qualitative modifications to the model that significantly improve the learning efficiency and the overall diarization performance. In particular, we introduce a novel loss function, we called Sample Mean Loss and we present a better modelling of the speaker turn behaviour, by devising an analytical expression to compute the probability of a new speaker joining the conversation. In addition, we demonstrate that our model can be trained on fixed-length speech segments, removing the need for speaker change information in inference. Using x-vectors as input features, we evaluate our proposed approach on the multi-domain dataset employed in the DIHARD II challenge: our online method improves with respect to the original UIS-RNN and achieves similar performance to an offline agglomerative clustering baseline using PLDA scoring. ","Supervised online diarization with sample mean loss for multi-domain
  data"
82,1191606085603483649,109603566,Stephen James,['Having robot learning blues after the end of #CoRL2019? Fear not! Our new paper takes sim2real further and explores how we can learn to one-shot imitate humans (using TecNets) without any real-world data during training!\n\n<LINK>\n\n<LINK> <LINK>'],https://arxiv.org/abs/1911.01103,"Humans can naturally learn to execute a new task by seeing it performed by other individuals once, and then reproduce it in a variety of configurations. Endowing robots with this ability of imitating humans from third person is a very immediate and natural way of teaching new tasks. Only recently, through meta-learning, there have been successful attempts to one-shot imitation learning from humans; however, these approaches require a lot of human resources to collect the data in the real world to train the robot. But is there a way to remove the need for real world human demonstrations during training? We show that with Task-Embedded Control Networks, we can infer control polices by embedding human demonstrations that can condition a control policy and achieve one-shot imitation learning. Importantly, we do not use a real human arm to supply demonstrations during training, but instead leverage domain randomisation in an application that has not been seen before: sim-to-real transfer on humans. Upon evaluating our approach on pushing and placing tasks in both simulation and in the real world, we show that in comparison to a system that was trained on real-world data we are able to achieve similar results by utilising only simulation data. ",Learning One-Shot Imitation from Humans without Humans
83,1191488364395917312,774170436057731073,Alexis Conneau,"['⚙️Release: CCNet is our new tool for extracting high-quality and large-scale monolingual corpora from CommonCraw in more than a hundred languages.\n\nPaper: <LINK>\nTool: <LINK>\n\nBy G. Wenzek, M-A Lachaux, @EXGRV, @armandjoulin <LINK>']",https://arxiv.org/abs/1911.00359,"Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia. ",CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data
84,1191358773555687430,199531005,Curtis G. Northcutt,"['Confident Learning for finding label errors in datasets and learning with noisy labels is finally here!  \nRead about it: <LINK>\nReddit: <LINK>\nPaper: <LINK>\nNEW Python Package: <LINK>', '@david_sontag @rahulgk @amuellerml @TlkngMchns', '@nicoteiz Specifically which link? Where is it? thanks!']",https://arxiv.org/abs/1911.00068,"Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 ""missile"" images are mislabeled as their parent class ""projectile""), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release. ",Confident Learning: Estimating Uncertainty in Dataset Labels
85,1191341127779201024,14118089,Henry Segerman,"['New paper on the arXiv today with @SaulSchleimer, ""From veering triangulations to link spaces and back again"" <LINK> <LINK>', '@SaulSchleimer We made lots of pretty figures! https://t.co/6e1Krf08t9']",https://arxiv.org/abs/1911.00006,"This paper is the third in a sequence establishing a dictionary between the combinatorics of veering triangulations equipped with appropriate filling slopes, and the dynamics of pseudo-Anosov flows (without perfect fits) on closed three-manifolds. Our motivation comes from the work of Agol and Gu\'eritaud. Agol introduced veering triangulations of mapping tori as a tool for understanding the surgery parents of pseudo-Anosov mapping tori. Gu\'eritaud gave a new construction of veering triangulations of mapping tori using the orbit spaces of their suspension flows. Generalizing this, Agol and Gu\'eritaud announced a method that, given a closed manifold with a pseudo-Anosov flow (without perfect fits), produces a veering triangulation equipped with filling slopes. In this paper we build, from a veering triangulation, a canonical circular order on the cusps of the universal cover. Using this we build the veering circle and the link space. These are the first entries in the promised dictionary. The latter corresponds to the orbit space of a flow; the former corresponds to Fenley's boundary at infinity of the orbit space. In the other direction, and using our previous work, we prove that the veering triangulation is recovered (up to canonical isomorphism) from the dynamics of the fundamental group acting on the link space. This is the first step in proving that our dictionary gives a bijection between the two theories. ",From veering triangulations to link spaces and back again
86,1191295689168490498,481539448,Richard Alexander,"['New paper, led by @PhysicsUoL postdoc Simon Joyce, along with @jonny_nichols and others. We used @NASASwift to observe the X-ray and UV emission from the young, planet-hosting star PDS 70.\n\n<LINK>']",https://arxiv.org/abs/1911.00297,"PDS 70 is a $\sim$5 Myr old star with a gas and dust disc in which several proto-planets have been discovered. We present the first UV detection of the system along with X-ray observations taken with the \textit{Neil Gehrels Swift Observatory} satellite. PDS 70 has an X-ray flux of 3.4$\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ in the 0.3-10.0 keV range, and UV flux (U band) of 3.5$\times 10^{-13}$ erg cm$^{-2}$ s$^{-1}$ . At the distance of 113.4 pc determined from Gaia DR2 this gives luminosities of 5.2$\times 10^{29}$ erg s$^{-1}$ and 5.4$\times 10^{29}$ erg s$^{-1}$ respectively. The X-ray luminosity is consistent with coronal emission from a rapidly rotating star close to the log $\frac{L_{\mathrm{X}}}{L_{\mathrm{bol}}} \sim -3$ saturation limit. We find the UV luminosity is much lower than would be expected if the star were still accreting disc material and suggest that the observed UV emission is coronal in origin. ","A Swift view of X-ray and UV radiation in the planet-forming T-Tauri
  system PDS 70"
87,1202068883001053184,3292006950,zhou Yu,"['Our new AAAI paper <LINK> \nand the code is here <LINK>\nOne dialog context can have multiple dialog responses that can be appropriate for finishing the goal. Existing datasets do not count in all the possible responses.', 'We develop a method to augment the existing datasets to count in all the possible alternative responses. So the model trained will not biased towards one particular policy. We obtained state-of-the-art dialog generation results on MultiWOZ.', '@tianchezhao Thanks! Feel free to use our code', 'Forget to mention, we also publish a better and cleaner version of MultiWoz. We corrected delexicalization errors in multi-domain tasks.Unified slots such as phone number from different domains. https://t.co/vaLA3JmwSi']",https://arxiv.org/abs/1911.10484,"Conversations have an intrinsic one-to-many property, which means that multiple responses can be appropriate for the same dialog context. In task-oriented dialogs, this property leads to different valid dialog policies towards task completion. However, none of the existing task-oriented dialog generation approaches takes this property into account. We propose a Multi-Action Data Augmentation (MADA) framework to utilize the one-to-many property to generate diverse appropriate dialog responses. Specifically, we first use dialog states to summarize the dialog history, and then discover all possible mappings from every dialog state to its different valid system actions. During dialog system training, we enable the current dialog state to map to all valid system actions discovered in the previous process to create additional state-action pairs. By incorporating these additional pairs, the dialog policy learns a balanced action distribution, which further guides the dialog model to generate diverse responses. Experimental results show that the proposed framework consistently improves dialog policy diversity, and results in improved response diversity and appropriateness. Our model obtains state-of-the-art results on MultiWOZ. ","Task-Oriented Dialog Systems that Consider Multiple Appropriate
  Responses under the Same Context"
88,1201793226149126144,2963878756,Ana Triana,"['fMRI preprocessing is overwhelming. There are many choices to clean the data, but how do the choices affect results? \nWe explore this in our new paper <LINK>, where we focus on how spatial smoothing affects on-group level differences in functional brain networks', 'Here, @OnervaKorhonen, @eglerean, @JariSaramaki and I investigate the effects of spatial smoothing on the difference between patients and controls for two clinical conditions: autism spectrum and bipolar disorder considering rs-fMRI data smoothed with Gaussian kernels. https://t.co/BC0MQNOLM4', '@OnervaKorhonen @eglerean @JariSaramaki What did we find?\n1. the choice of spatial smoothing kernel affects group-level differences in resting-state functional brain networks\n2. significantly different links between the two groups are usually found at specific kernels, just a few are consistent across smoothing levels https://t.co/D0QUyNps4h', '@OnervaKorhonen @eglerean @JariSaramaki 3. the effects are not explained by the spatial properties of the parcellation used. This is independent of the ROI size, but the effects of link length vary.\n4. These effects are in general non-systematic and difficult to predict.', '@OnervaKorhonen @eglerean @JariSaramaki The take-home message: the choice of the smoothing kernel affects the findings in network differences.']",https://arxiv.org/abs/1911.12124,"Brain connectivity with functional Magnetic Resonance Imaging (fMRI) is a popular approach for detecting differences between healthy and clinical populations. Before creating a functional brain network, the fMRI time series must undergo several preprocessing steps to control for artifacts and to improve data quality. However, preprocessing may affect the results in an undesirable way. Spatial smoothing, for example, is known to alter functional network structure. Yet, its effects on group-level network differences remain unknown. Here, we investigate the effects of spatial smoothing on the difference between patients and controls for two clinical conditions: autism spectrum disorder and bipolar disorder, considering fMRI data smoothed with Gaussian kernels (0-32 mm). We find that smoothing affects network differences between groups. For weighted networks, incrementing the smoothing kernel makes networks more different. For thresholded networks, larger smoothing kernels lead to more similar networks, although this depends on the network density. Smoothing also alters the effect sizes of the individual link differences. This is independent of the ROI size, but vary with link length. The effects of spatial smoothing are diverse, non-trivial, and difficult to predict. This has important consequences: the choice of smoothing kernel affects the observed network differences. ","Effects of spatial smoothing on group-level differences in functional
  brain networks"
89,1201338456250703874,1162541483431301120,James Beattie,['New paper on anisotropic structures in projections of the density fields in MHD turbulence. The gist: magnetic fields turn anisotropy on and off and the strength of the turbulence controls the type of anisotropy.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1911.13090,"Stars form in highly-magnetised, supersonic turbulent molecular clouds. Many of the tools and models that we use to carry out star formation studies rely upon the assumption of cloud isotropy. However, structures like high-density filaments in the presence of magnetic fields, and magnetosonic striations introduce anisotropies into the cloud. In this study we use the two-dimensional (2D) power spectrum to perform a systematic analysis of the anisotropies in the column density for a range of Alfv\'en Mach numbers ($\mathcal{M}_A=0.1$--$10$) and turbulent Mach numbers ($\mathcal{M}=2$--$20$), with 20 high-resolution, three-dimensional (3D) turbulent magnetohydrodynamic simulations. We find that for cases with a strong magnetic guide field, corresponding to $\mathcal{M}_A<1$, and $\mathcal{M}\lesssim 4$, the anisotropy in the column density is dominated by thin striations aligned with the magnetic field, while for $\mathcal{M}\gtrsim 4$ the anisotropy is significantly changed by high-density filaments that form perpendicular to the magnetic guide field. Indeed, the strength of the magnetic field controls the degree of anisotropy and whether or not any anisotropy is present, but it is the turbulent motions controlled by $\mathcal{M}$ that determine which kind of anisotropy dominates the morphology of a cloud. ","Filaments and striations: anisotropies in observed, supersonic,
  highly-magnetised turbulent clouds"
90,1199992195819937792,57571700,Taha Yasseri,"['New Preprint:\n""Fooling with facts:\nQuantifying anchoring bias through a large-scale online experiment"" \nMy first paper based on an actual experiment! \n\n<LINK>\n\n@oiioxford #SocialDataScience <LINK>']",https://arxiv.org/abs/1911.12275,"Living in the 'Information Age' means that not only access to information has become easier but also that the distribution of information is more dynamic than ever. Through a large-scale online field experiment, we provide new empirical evidence for the presence of the anchoring bias in people's judgment due to irrational reliance on a piece of information that they are initially given. The comparison of the anchoring stimuli and respective responses across different tasks reveals a positive, yet complex relationship between the anchors and the bias in participants' predictions of the outcomes of events in the future. Participants in the treatment group were equally susceptible to the anchors regardless of their level of engagement, previous performance, or gender. Given the strong and ubiquitous influence of anchors quantified here, we should take great care to closely monitor and regulate the distribution of information online to facilitate less biased decision making. ","Fooling with facts: Quantifying anchoring bias through a large-scale
  online experiment"
91,1199868634673434624,29178343,Alex Dimakis,['New paper: Your Local GAN: a new layer of two-dimensional sparse attention and a new generative model. Also progress on inverting GANs which may be useful for inverse problems. \n<LINK>\nwith  @giannis_daras from NTUA and @gstsdn  @Han_Zhang_ from @googleai'],https://arxiv.org/abs/1911.12287,"We introduce a new local sparse attention layer that preserves two-dimensional geometry and locality. We show that by just replacing the dense attention layer of SAGAN with our construction, we obtain very significant FID, Inception score and pure visual improvements. FID score is improved from $18.65$ to $15.94$ on ImageNet, keeping all other parameters the same. The sparse attention patterns that we propose for our new layer are designed using a novel information theoretic criterion that uses information flow graphs. We also present a novel way to invert Generative Adversarial Networks with attention. Our method extracts from the attention layer of the discriminator a saliency map, which we use to construct a new loss function for the inversion. This allows us to visualize the newly introduced attention heads and show that they indeed capture interesting aspects of two-dimensional geometry of real images. ","Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for
  Generative Models"
92,1199280497953775618,19989030,Luis Lamb,"['Our new machine learning/GNN paper is on arxiv: <LINK> with Marco Gori @Melleo54Sis, P. Avelar and A. Tavares.', '@Melleo54Sis And here is the title: Discrete and Continuous Deep Residual Learning Over Graphs: https://t.co/3JU9jZ7N0e']",https://arxiv.org/abs/1911.09554,"In this paper we propose the use of continuous residual modules for graph kernels in Graph Neural Networks. We show how both discrete and continuous residual layers allow for more robust training, being that continuous residual layers are those which are applied by integrating through an Ordinary Differential Equation (ODE) solver to produce their output. We experimentally show that these residuals achieve better results than the ones with non-residual modules when multiple layers are used, mitigating the low-pass filtering effect of GCN-based models. Finally, we apply and analyse the behaviour of these techniques and give pointers to how this technique can be useful in other domains by allowing more predictable behaviour under dynamic times of computation. ",Discrete and Continuous Deep Residual Learning Over Graphs
93,1197630181306966021,1070655917920829440,Tom Zahavy,"['In a new paper, <LINK>, we study the projection method of Abbeel and Ng (2004) for Apprenticeship Learning. We show that it is, in fact, an instantiation of the Frank-Wolfe method -- a projection free method for convex optimization. (1/5)', 'This insight allows us to:\n(a) Analyze Apprenticeship Learning with tools from the convex optimization literature and derive tighter convergence bounds.\n(b) Simplify the post-processing step (no need for a QP solver). (2/5)', ""(c) Show that a variation of Frank-Wolfe that is taking 'away steps' achieves a linear rate of convergence (for Apprenticeship Learning) — theoretically SOTA!\n(d) Derive a stochastic Apprenticeship Learning algorithm that avoids precise estimation of feature expectations. (3/5)"", 'Joint work with my colleagues @GoogleAI : Alon Cohen, Haim Kaplan, and Yishay Mansour (a.k.a the ML foundations team). Come and chat with us @AAAI2020 and @NeurIPSConf -- Optimization Foundations for RL workshop. (4/5)', 'On a personal note, in the second year of my BSc I decided to become a ML researcher after taking @ng ML course @coursera. @pabbeel’s research influenced every DRL paper I wrote. It is my greatest respect to work on such a fundamental result from these inspiring professors! (5/5)']",https://arxiv.org/abs/1911.01679,"We consider the applications of the Frank-Wolfe (FW) algorithm for Apprenticeship Learning (AL). In this setting, we are given a Markov Decision Process (MDP) without an explicit reward function. Instead, we observe an expert that acts according to some policy, and the goal is to find a policy whose feature expectations are closest to those of the expert policy. We formulate this problem as finding the projection of the feature expectations of the expert on the feature expectations polytope -- the convex hull of the feature expectations of all the deterministic policies in the MDP. We show that this formulation is equivalent to the AL objective and that solving this problem using the FW algorithm is equivalent well-known Projection method of Abbeel and Ng (2004). This insight allows us to analyze AL with tools from convex optimization literature and derive tighter convergence bounds on AL. Specifically, we show that a variation of the FW method that is based on taking ""away steps"" achieves a linear rate of convergence when applied to AL and that a stochastic version of the FW algorithm can be used to avoid precise estimation of feature expectations. We also experimentally show that this version outperforms the FW baseline. To the best of our knowledge, this is the first work that shows linear convergence rates for AL. ",Apprenticeship Learning via Frank-Wolfe
94,1195868508925513730,101810581,Animesh Garg,"['New paper on Goal-Based Imitation from Third Person Videos.\nMotion reasoning that combines task\n&amp; motion planning to resolve semantic ambiguity demonstrator intent and outputs symbolic goal representations from video\n\n<LINK>\nPaper: <LINK>', 'joint work at @NvidiaAI  with @deanh_tw, Yu-Wei Chao, Chris Paxton, @jcniebles, @drfeifei, Dieter Fox', 'This continues our efforts in neuro-symbolic planning for one-shot imitation in multi-step reasoning domains. \n\n1. Neural Task Programs:  https://t.co/BxuuDsOs87\n2. Neural Task Graphs: https://t.co/szcZkDOKSJ\n3. Continuous Relaxation of Symbolic Planner: https://t.co/Hi39jDUt1I']",https://arxiv.org/abs/1911.05864,"We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach by collecting a dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment. ",Motion Reasoning for Goal-Based Imitation Learning
95,1192490719031656448,774170436057731073,Alexis Conneau,"['Our new paper: Unsupervised Cross-lingual Representation Learning at Scale <LINK>\n\nWe release XLM-R, a Transformer MLM trained in 100 langs on 2.5 TB of text data. \n\nDouble digit gains on XLU benchmarks + strong per-language performance (~XLNet on GLUE). [1/6] <LINK>', 'We present a comprehensive analysis of the capacity and limits of unsupervised multilingual masked language modeling at scale.\n\nIn particular, we study the high-res/low-res and transfer/interference trade-offs, and expose what we like to call ""the curse of multilinguality"". [2/6] https://t.co/NJvVV6HSyo', 'We show that multilingual models can outperform monolingual BERTs by leveraging training sets in multiple languages at fine-tuning time.\n\nOur work generally demonstrates the possibility of having one model for all languages while not giving up on per-language performance. [3/6] https://t.co/b4QF0hBuG2', 'XLM-R especially outperforms mBERT and XLM-100 on low-resource languages, for which CommonCrawl data (see #CCNet) enables representation learning: +13.7% and +9.3% for Urdu, +21.6% and +13.8% accuracy for Swahili on XNLI. [4/6] https://t.co/xeS76EjP3B', 'Play with our XLM-R and XLM-R_Base with @Pytorch hub: https://t.co/ypmoBqCwnN. Available on FairSeq, Pytext and XLM. Soon on @HuggingFace Transformer repo and @TensorFlow hub. [5/6] https://t.co/mIb13oEUgS', 'Joint work with the @facebookai team: @kakemeister @NamanGoyal21 @VishravC @gwenzek @guzmanhe @EXGRV @myleott @LukeZettlemoyer @vesko_st [6/6]']",https://arxiv.org/abs/1911.02116,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. ",Unsupervised Cross-lingual Representation Learning at Scale
96,1192150464177020928,19152000,Michael Spagat,['New working paper just up.  Comments welcome.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1911.01994,"We are able to unify various disparate claims and results in the literature, that stand in the way of a unified description and understanding of human conflict. First, we provide a reconciliation of the numerically different exponent values for fatality distributions across entire wars and within single wars. Second, we explain how ignoring the details of how conflict datasets are compiled, can generate falsely negative evaluations from power-law distribution fitting. Third, we explain how a generative theory of human conflict is able to provide a quantitative explanation of how most observed casualty distributions follow approximate power-laws and how and why they deviate from it. In particular, it provides a unified mechanistic interpretation of the origin of these power-law deviations in terms of dynamical processes within the conflict. Combined, our findings strengthen the notion that a unified framework can be used to understand and quantitatively describe human conflict. ","Toward a Unified Understanding of Casualty Distributions in Human
  Conflict"
97,1204434620521107457,8514822,asim kadav,"['Grounding task involves matching entities to image bboxes (e.g. useful for interpretable VQA). We propose a simple baseline using Flickr30K dataset, that achieves SOTA performance w/ any xtra data. Code: <LINK> Paper: <LINK> (at VIGIL #Neurips19) <LINK>']",https://arxiv.org/abs/1911.02133,"In this paper, we introduce a contextual grounding approach that captures the context in corresponding text entities and image regions to improve the grounding accuracy. Specifically, the proposed architecture accepts pre-trained text token embeddings and image object features from an off-the-shelf object detector as input. Additional encoding to capture the positional and spatial information can be added to enhance the feature quality. There are separate text and image branches facilitating respective architectural refinements for different modalities. The text branch is pre-trained on a large-scale masked language modeling task while the image branch is trained from scratch. Next, the model learns the contextual representations of the text tokens and image objects through layers of high-order interaction respectively. The final grounding head ranks the correspondence between the textual and visual representations through cross-modal interaction. In the evaluation, we show that our model achieves the state-of-the-art grounding accuracy of 71.36% over the Flickr30K Entities dataset. No additional pre-training is necessary to deliver competitive results compared with related work that often requires task-agnostic and task-specific pre-training on cross-modal dadasets. The implementation is publicly available at this https URL ",Contextual Grounding of Natural Language Entities in Images
98,1203893359481372673,1202505894367551488,Xu Sun,['Our recent work to be presented at NeurIPS 2019:\nUnderstanding and improving layer normalization--\n\nWe find that the normalization effects on derivatives are much more important than forward normalization.\n\nPaper: <LINK>'],https://arxiv.org/abs/1911.07013,"Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets. ",Understanding and Improving Layer Normalization
99,1201551373407416320,245345590,Haytham Fayek,['I’m excited to share our new paper where we use audio question answering (AQA) to study temporal reasoning in machine learning. We put forward a new dataset and model for AQA.\n\nPaper: Temporal Reasoning via Audio Question Answering \nw/ @jcjohnss \n<LINK>'],http://arxiv.org/abs/1911.09655,"Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this paper, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA. ",Temporal Reasoning via Audio Question Answering
100,1201454333180698624,1200003750162829312,Aarynn Carter,"['Hey Twitter! Check out arXiv (<LINK>)  for my first paper which shows off the latest @NASA_TESS, @NasaHubble and @ESO VLT data for the hot Jupiter exoplanet WASP-6b. We find Na, K, and H2O in its atmosphere and account for some pesky stellar activity effects. <LINK>', '@AstroJake Thanks Jake!']",https://arxiv.org/abs/1911.12628,"We present new observations of the transmission spectrum of the hot Jupiter WASP-6b both from the ground with the Very Large Telescope (VLT) FOcal Reducer and Spectrograph (FORS2) from 0.45-0.83 $\mu$m, and space with the Transiting Exoplanet Survey Satellite (TESS) from 0.6-1.0 $\mu$m and the Hubble Space Telescope (HST) Wide Field Camera 3 from 1.12-1.65 $\mu$m. Archival data from the HST Space Telescope Imaging Spectrograph (STIS) and Spitzer is also reanalysed on a common Gaussian process framework, of which the STIS data show a good overall agreement with the overlapping FORS2 data. We also explore the effects of stellar heterogeneity on our observations and its resulting implications towards determining the atmospheric characteristics of WASP-6b. Independent of our assumptions for the level of stellar heterogeneity we detect Na I, K I and H$_2$O absorption features and constrain the elemental oxygen abundance to a value of [O/H] $\simeq -0.9\pm0.3$ relative to solar. In contrast, we find that the stellar heterogeneity correction can have significant effects on the retrieved distributions of the [Na/H] and [K/H] abundances, primarily through its degeneracy with the sloping optical opacity of scattering haze species within the atmosphere. Our results also show that despite this presence of haze, WASP-6b remains a favourable object for future atmospheric characterisation with upcoming missions such as the James Webb Space Telescope. ","Detection of Na, K and H$_2$O in the hazy atmosphere of WASP-6b"
101,1201427634305015808,20438952,Martyn Amos,"['Our new preprint presents a Turing Test for crowds; we show, using a study with students (n=384) that people are able to reliably *distinguish* real crowds from simulated crowds, but (perhaps surprisingly) they are unable to tell which is which... <LINK>', '@DirkHelbing Have emailed you.']",https://arxiv.org/abs/1911.06783,"The realism and believability of crowd simulations underpins computational studies of human collective behaviour, with implications for urban design, policing, security and many other areas. Realism concerns the closeness of the fit between a simulation and observed data, and believability concerns the human perception of plausibility. In this paper, we ask two questions, via a so-called ""Turing Test"" for crowds: (1) Can human observers distinguish between real and simulated crowds, and (2) Can human observers identify real crowds versus simulated crowds? In a study with student volunteers (n=384), we find convincing evidence that non-specialist individuals are able to reliably distinguish between real and simulated crowds. A rather more surprising result is that such individuals are overwhelmingly unable to identify real crowds. That is, they can tell real from simulated crowds, but are unable to say which is which. Our main conclusion is that (to the lay-person, at least) realistic crowds are not believable (and vice versa). ",A Turing Test for Crowds
102,1199760615520325632,1068545181576773632,Kenneth Brown,"['New paper: Fault-tolerant protocols for 2D compass codes <LINK>. Shilin Huang and I find that elongated codes can outperform the surface code for biased memory errors and unbiased gate errors.  We also describe a weighted Union-Find decoder.', ""The weighted Union-Find decoder is Shilin's new decoder that sits between the Union-Find decoder and Minimum Weight Perfect Matching in terms of both performance and runtime.""]",https://arxiv.org/abs/1911.11317,"We study a class of gauge fixings of the Bacon-Shor code at the circuit level, which includes a subfamily of generalized surface codes. We show that for these codes, fault tolerance can be achieved by direct measurements of the stabilizers. By simulating our fault-tolerant scheme under biased noise, we show the possibility of optimizing the performance of the surface code by stretching the bulk stabilizer geometry. To decode the syndrome efficiently and accurately, we generalize the union-find decoder to biased noise models. Our decoder obtains a $0.83\%$ threshold value for the surface code in quadratic time complexity. ",Fault-tolerant Compass Codes
103,1199715545068650496,838292815,Ofir Nachum,"['<LINK> Offline RL -what do you need to know about this notoriously difficult regime? Although recent papers propose a variety of algorithmic novelties, we find many of these unnecessary in practice. Extensive studies will hopefully guide future research &amp;practice <LINK>']",https://arxiv.org/abs/1911.11361,"In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting. ",Behavior Regularized Offline Reinforcement Learning
104,1199635812616491009,454838126,Jos de Bruijne,"['""A closer look at the spur, blob, wiggle &amp; gaps in GD-1"" <LINK> ""We constrain the stream track and density [...] and find 3 large under densities and significant residuals in the stream track lining up with these gaps"" #GaiaMission #GaiaDR2 <LINK>']",https://arxiv.org/abs/1911.05745,"The GD-1 stream is one of the longest and coldest stellar streams discovered to date, and one of the best objects for constraining the dark matter properties of the Milky Way. Using data from {\it Gaia} DR2 we study the proper motions, distance, morphology and density of the stream to uncover small scale perturbations. The proper motion cleaned data shows a clear distance gradient across the stream, ranging from 7 to 12 kpc. However, unlike earlier studies that found a continuous gradient, we uncover a distance minimum at $\varphi_{1}\approx$-50 deg, after which the distance increases again. We can reliably trace the stream between -85$<\varphi_{1}<$15 deg, showing an even further extent to GD-1 beyond the earlier extension of \citet{Price-Whelan18a}. We constrain the stream track and density using a Boolean matched filter approach and find three large under densities and find significant residuals in the stream track lining up with these gaps. In particular, a gap is visible at $\varphi_{1}$=-3 deg, surrounded by a clear sinusoidal wiggle. We argue that this wiggle is due to a perturbation since it has the wrong orientation to come from a progenitor. We compute a total initial stellar mass of the stream segment of 1.58$\pm$0.07$\times$10$^{4}$ M$_{\odot}$. With the extended view of the spur in this work, we argue that the spur may be unrelated to the adjacent gap in the stream. Finally, we show that an interaction with the Sagittarius dwarf can create features similar to the spur. ","A closer look at the spur, blob, wiggle, and gaps in GD-1"
105,1199166081664835584,909921405235531777,Sarah Blunt,"[""It's #arxiv day for Brendan Bowler, myself, and Eric Nielsen!! Check out our paper here: <LINK>. In a few words, we find evidence for distinct formation channels for directly imaged brown dwarfs &amp; giant planets by modeling their eccentricity distributions."", '@bpbowler forgot to tag you sorry.', ""@bpbowler We used #orbitize! to model the individual eccentricity posteriors of 26 systems, then Brendan did some hierarchical Bayesian modeling to get at the underlying population-level eccentricity distributions. Long story short: they're distinct!"", ""@bpbowler This project really put orbitize! through its paces. A lot of early development and testing I did for orbitize! was for this project. That's one of the things I love about orbitize! (and open-source astronomy code in general) -- I get to play developer and user."", '@bpbowler **27', '@bpbowler Figure 21 says it all. When you split up the population of substellar imaged companions by mass or mass ratio, the eccentricity distributions are distinct. This points to different formation mechanisms. https://t.co/hIPTz7ab28', '@aussiastronomer @AgolEric @bpbowler I\'ll use my catchphrase wrt this project: ""Brendan made a figure for that!"" See Fig 11. Points with black outlines in one of the two dashed boxes make up our sample. https://t.co/UR73oFc7Le', '@aussiastronomer @AgolEric @bpbowler https://t.co/Z4fjRlplph', '@TheChedgehog @bpbowler In the bottom panel, the populations were divided by mass ratio, not companion mass.']",https://arxiv.org/abs/1911.10569,"The orbital eccentricities of directly imaged exoplanets and brown dwarf companions provide clues about their formation and dynamical histories. We combine new high-contrast imaging observations of substellar companions obtained primarily with Keck/NIRC2 together with astrometry from the literature to test for differences in the population-level eccentricity distributions of 27 long-period giant planets and brown dwarf companions between 5-100 AU using hierarchical Bayesian modeling. Orbit fits are performed in a uniform manner for companions with short orbital arcs; this typically results in broad constraints for individual eccentricity distributions, but together as an ensemble these systems provide valuable insight into their collective underlying orbital patterns. The shape of the eccentricity distribution function for our full sample of substellar companions is approximately flat from e=0-1. When subdivided by companion mass and mass ratio, the underlying distributions for giant planets and brown dwarfs show significant differences. Low mass ratio companions preferentially have low eccentricities, similar to the orbital properties of warm Jupiters found with radial velocities and transits. We interpret this as evidence for in situ formation on largely undisturbed orbits within massive, extended disks. Brown dwarf companions exhibit a broad peak at e $\approx$ 0.6-0.9 with evidence for a dependence on orbital period. This closely resembles the orbital properties and period-eccentricity trends of wide (1-200 AU) stellar binaries, suggesting that brown dwarfs in this separation range predominantly form in a similar fashion. We also report evidence that the ""eccentricity dichotomy"" observed at small separations extends to planets on wide orbits: the mean eccentricity for the multi-planet system HR 8799 is lower than for systems with single planets. (Abridged) ","Population-Level Eccentricity Distributions of Imaged Exoplanets and
  Brown Dwarf Companions: Dynamical Evidence for Distinct Formation Channels"
106,1198777891200352256,1376287471,Tharindu Jayasinghe,"[""New paper day, and the last one for this year! <LINK>\nHere, we have studied a large, all-sky sample of ~71,200 contact binaries ('peanut stars') in @SuperASASSN V-band data (sky distribution shown in the Figure below). <LINK>"", 'Contact binary stars are close binary systems whose components fill their Roche lobes. These systems are commonly known as W Ursae Majoris (W UMa/EW) variables and they are fairly abundant (~0.2% of all FGK stars are EW variables). Here are some example light curves: https://t.co/X148rfz2cC', 'Since both the stars overflow their Roche lobes, their orbital period is closely related to the mean stellar densities. This means that these contact binaries follow a period-luminosity relationship (PLR)! Looking at this PLR, we note that there is a break at log(P)~-0.3🧐 https://t.co/uP7bQnHAxh', 'Looking at the distribution of the orbital periods, we see strong evidence for two different populations, and it seems that they follow distinct PLRs! Historically, they have been separated based on period. Early-type systems have log(P)&gt;-0.25, late-type systems have log(P)&lt;-0.25 https://t.co/Nc5zs63zEA', 'We exploit the synergy between @SuperASASSN and wide-field spectroscopic surveys such as LAMOST, GALAH and APOGEE, and cross-match our catalog to these surveys. Through this process, we obtained spectroscopic information for ~7200 EW binaries (~10% of the full catalog)..', ""We look at their surface temperatures and orbital periods, and find something remarkable! There are two distinct 'clusters' of EW binaries in Teff-log(P) space that are clearly separated. It is also clear that a simple cut in log(P) cannot separate these systems well.... https://t.co/EWtf46VNnS"", 'Also interesting is that these two EW sub-types follow different trends in temperature with orbital period. Early-type systems get cooler as the orbital period increases, whereas late-type systems get hotter!', 'Looking at the Gaia DR2 CMD, early-type systems are also more luminous and appear to be younger than the late-type systems. https://t.co/JxIJdV21yq', 'It is well known that the Kraft break implies substantial changes in the envelope structure,winds and angular momentum loss for stars on the main-sequence. This occurs at ~1.3 M_sun. Stars above the Kraft break are hotter and rotate more rapidly than those below the Kraft break.', 'The transition from slow to fast rotation occurs over the temperature range 6200-6700 K and it cannot be a coincidence that the split between early and late type contact binaries occurs at a similar temperature! 🧐🧐', 'Formation models for these contact binaries generally invoke changes in the efficiency of angular momentum loss on the MS which is exactly the physics leading to the Kraft break!There is a clear gap between early and late type systems, which seems not to be predicted by theory!😮', 'In summary, there is a clean break in the EW \nperiod-luminosity relation at log(P)~-0.3, separating the early-type (A sub-type) EW binaries from the late-type (W sub-type) systems. However, the two populations are even more cleanly separated in the space of log(P) and Teff!', 'Early-type and late-type\nEW binaries follow opposite trends in Teff with orbital period. The dichotomy of contact \nbinaries is almost certainly related to the Kraft break and the related changes in envelope structure, winds and angular momentum loss.', ""@SuperASASSN Didn't think of that, but yes! 😊"", '@astro_jje Hi Dr. Eldridge! Thank you for your insightful comments! :) I have not modeled this behavior with homology, but I agree that convective/radiative envelopes are important to fully describe these observations. I will take a look at the slopes in log (Teff) - log(P/d) space!']",http://arxiv.org/abs/1911.09685,"We characterize ${\sim} 71,200$ W UMa type (EW) contact binaries, including ${\sim} 12,600$ new discoveries, using ASAS-SN $V$-band all-sky light curves along with archival data from Gaia, 2MASS, AllWISE, LAMOST, GALAH, RAVE, and APOGEE. There is a clean break in the EW period-luminosity relation at $\rm \log (\rm P/d){\simeq}-0.30$, separating the longer period early-type EW binaries from the shorter period, late-type systems. The two populations are even more cleanly separated in the space of period and effective temperature, by $\rm T_{eff}=6710\,K-1760\,K\,\log(P/0.5\,d)$. Early-type and late-type EW binaries follow opposite trends in $\rm T_{eff}$ with orbital period. For longer periods, early-type EW binaries are cooler, while late-type systems are hotter. We derive period-luminosity relationships (PLRs) in the $W_{JK}$, $V$, Gaia DR2 $G$, $J$, $H$, $K_s$ and $W_1$ bands for the late-type and early-type EW binaries separated both by period and effective temperature, and by period alone. The dichotomy of contact binaries is almost certainly related to the Kraft break and the related changes in envelope structure, winds and angular momentum loss. ","The ASAS-SN Catalog of Variable Stars VII: Contact Binaries are
  Different Above and Below the Kraft Break"
107,1197976468506071040,709593994095935488,Diego Aldarondo,"['Happy to share this work! We built a model of a rodent, trained it to solve four tasks, and used methods common in neuroscience to study how the rodent controls its body. <LINK> @Jessedmarshall, Josh Merel, Yuval Tassa, Greg Wayne and @BOlveczky', '@jessedmarshall @BOlveczky We think the virtual rodent will be a useful tool for modeling embodied motor control. Hopefully, refining the mechanics and neural architecture toward increasing biological realism will improve our understanding of both artificial and biological control.', ""@jessedmarshall @BOlveczky Here's a short clip of the rodent solving a modified version of the two-tap task! https://t.co/JAtk7mZg6v"", ""@jessedmarshall @BOlveczky And another clip demonstrating how dynamics within the network's activity reflect the production of behaviors at several timescales. https://t.co/hwR8q3bzKd""]",https://arxiv.org/abs/1911.09451,"Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience. ",Deep neuroethology of a virtual rodent
108,1197571944591192064,2341435873,Josh Simon,"['Hey dwarf galaxy enthusiasts!  Check out our latest on spectroscopy of the Milky Way satellites discovered by @theDESurvey: <LINK>.\n\nThis time we studied three ultra-faint satellites, Grus II, Tucana IV, and Tucana V.\n\n#CarnegieScience <LINK>', '@theDESurvey @sazabi_li @deniserkal The highlight of this paper is that our measurements of the 3D velocity of Tucana IV show that it had a direct collision with the Large Magellanic Cloud less than 150 million years ago!  The second plot shows the separation between the two going backward in time. #GalaxyCrashes https://t.co/eIG8LOoFYG', ""@theDESurvey @sazabi_li @deniserkal We also find that Grus II is very metal-poor, and we think it's a dwarf galaxy, but it has a very small velocity dispersion so we could only obtain an upper limit on its mass and #darkmatter content.\n\nWe confirmed just 3 stars in Tucana V, so we can't say much about that one yet.""]",https://arxiv.org/abs/1911.08493,"We present Magellan/IMACS spectroscopy of three recently discovered ultra-faint Milky Way satellites, Grus II, Tucana IV, and Tucana V. We measure systemic velocities of V_hel = -110.0 +/- 0.5 km/s, V_hel = 15.9 +/- 1.8 km/s, and V_hel = -36.2 +/-2.5 km/s for the three objects, respectively. Their large relative velocity differences demonstrate that the satellites are unrelated despite their close physical proximity. We determine a velocity dispersion for Tuc IV of sigma = 4.3^+1.7_-1.0 km/s, but we cannot resolve the velocity dispersions of the other two systems. For Gru II we place an upper limit (90% confidence) on the dispersion of sigma < 1.9 km/s, and for Tuc V we do not obtain any useful limits. All three satellites have metallicities below [Fe/H] = -2.1, but none has a detectable metallicity spread. We determine proper motions for each satellite based on Gaia astrometry and compute their orbits around the Milky Way. Gru II is on a tightly bound orbit with a pericenter of 25 kpc and orbital eccentricity of 0.45. Tuc V likely has an apocenter beyond 100 kpc, and could be approaching the Milky Way for the first time. The current orbit of Tuc IV is similar to that of Gru II, with a pericenter of 25 kpc and an eccentricity of 0.36. However, a backward integration of the position of Tuc IV demonstrates that it collided with the Large Magellanic Cloud at an impact parameter of 4 kpc ~120 Myr ago, deflecting its trajectory and possibly altering its internal kinematics. Based on their sizes, masses, and metallicities, we classify Gru II and Tuc IV as likely dwarf galaxies, but the nature of Tuc V remains uncertain. ","Birds of a Feather? Magellan/IMACS Spectroscopy of the Ultra-Faint
  Satellites Grus II, Tucana IV, and Tucana V"
109,1197121963757576197,454838126,Jos de Bruijne,"['""Novel constraints on the particle nature of dark matter from stellar streams"" <LINK> ""[We find] a 95% lower limit on the mass of warm dark matter thermal relics mWDM&gt;4.6 keV; adding dwarf satellite counts strengthens this to mWDM&gt;6.3 keV"" #GaiaMission #GaiaDR2 <LINK>']",https://arxiv.org/abs/1911.02662,"New data from the $\textit{Gaia}$ satellite, when combined with accurate photometry from the Pan-STARRS survey, allow us to accurately estimate the properties of the GD-1 stream. Here, we analyze the stellar density perturbations in the GD-1 stream and show that they cannot be due to known baryonic structures like giant molecular clouds, globular clusters, or the Milky Way's bar or spiral arms. A joint analysis of the GD-1 and Pal 5 streams instead requires a population of dark substructures with masses $\approx 10^{7}$ to $10^9 \ M_{\rm{\odot}}$. We infer a total abundance of dark subhalos normalised to standard cold dark matter $n_{\rm sub}/n_{\rm sub, CDM} = 0.4 ^{+0.3}_{-0.2}$ ($68 \%$), which corresponds to a mass fraction contained in the subhalos $f_{\rm{sub}} = 0.14 ^{+0.11}_{-0.07} \%$, compatible with the predictions of hydrodynamical simulation of cold dark matter with baryons. ","Evidence of a population of dark subhalos from Gaia and Pan-STARRS
  observations of the GD-1 stream"
110,1196632634467569664,836417100864344064,Takuma Udagawa,"['Our #AAAI2020 paper ""An Annotated Corpus of Reference Resolution for Interpreting Common Grounding"" is up on arXiv: <LINK>! We propose a new resource of reference resolution to study the intermediate process of common grounding.']",https://arxiv.org/abs/1911.07588,"Common grounding is the process of creating, repairing and updating mutual understandings, which is a fundamental aspect of natural language conversation. However, interpreting the process of common grounding is a challenging task, especially under continuous and partially-observable context where complex ambiguity, uncertainty, partial understandings and misunderstandings are introduced. Interpretation becomes even more challenging when we deal with dialogue systems which still have limited capability of natural language understanding and generation. To address this problem, we consider reference resolution as the central subtask of common grounding and propose a new resource to study its intermediate process. Based on a simple and general annotation schema, we collected a total of 40,172 referring expressions in 5,191 dialogues curated from an existing corpus, along with multiple judgements of referent interpretations. We show that our annotation is highly reliable, captures the complexity of common grounding through a natural degree of reasonable disagreements, and allows for more detailed and quantitative analyses of common grounding strategies. Finally, we demonstrate the advantages of our annotation for interpreting, analyzing and improving common grounding in baseline dialogue systems. ","An Annotated Corpus of Reference Resolution for Interpreting Common
  Grounding"
111,1196490342369046529,118321407,Julien Barrier,"['We studied rhomboedral #graphite, and found:\n· gapped bulk states\n· electron transport dominated by surface states\n· it allows observation of the #Quantum Hall Effect\n· phase transition between gappless and gapped phases\n· giant Berry curvature\nPreprint: <LINK>']",https://arxiv.org/abs/1911.04565,"Of the two stable forms of graphite, hexagonal (HG) and rhombohedral (RG), the former is more common and has been studied extensively. RG is less stable, which so far precluded its detailed investigation, despite many theoretical predictions about the abundance of exotic interaction-induced physics. Advances in van der Waals heterostructure technology have now allowed us to make high-quality RG films up to 50 graphene layers thick and study their transport properties. We find that the bulk electronic states in such RG are gapped and, at low temperatures, electron transport is dominated by surface states. Because of topological protection, the surface states are robust and of high quality, allowing the observation of the quantum Hall effect, where RG exhibits phase transitions between gapless semimetallic phase and gapped quantum spin Hall phase with giant Berry curvature. An energy gap can also be opened in the surface states by breaking their inversion symmetry via applying a perpendicular electric field. Moreover, in RG films thinner than 4 nm, a gap is present even without an external electric field. This spontaneous gap opening shows pronounced hysteresis and other signatures characteristic of electronic phase separation, which we attribute to emergence of strongly-correlated electronic surface states. ","Electronic phase separation in topological surface states of
  rhombohedral graphite"
112,1195325155855228928,718084071,Denis Erkal,"['Our data paper on GD-1 led by @astro_tdboer using @ESAGaia and @PanSTARRS1 is out today <LINK>. We confirm the spur and blob feature found last year by @adrianprw and @anabonaca. We find a highly perturbed stream with 3 gaps and wiggles in the stream track. 1/5 <LINK>', 'We find that a close passage with the Sagittarius dwarf ~ 3 Gyr ago can create features like the spur and perhaps the blob. The panels below show 5 different realizations of the interaction with Sgr. That said, more work is needed to check if this is what happened to GD-1 2/5 https://t.co/KEyPEmf9uc', 'We also argue that the spur is likely unrelated to the gap which it is next to since the spur should connect onto the near side of the gap instead of passing over the gap. Instead, that gap may be where the progenitor of GD-1 used to be as argued in https://t.co/skBCQBD8lt 3/5 https://t.co/PVcZ4disY9', 'Despite the perturbations to GD-1, we find that the 3d velocity of the stream stars is closely aligned with the stream itself, showing that GD-1 is still a well-behaved stream. This shows that the perturbations were relatively small unlike those from LMC on the Orphan stream 4/5 https://t.co/aOEQi9Agk8', ""Finally, this data was the basis of the excellent constraint on DM led by @NilanjanBanik https://t.co/1BNP54C8bU, https://t.co/r2g1bs963C which showed that baryonic structures cannot account for GD-1's perturbations and that we need mWDM &gt; 6.3 keV if these are due to DM subhaloes https://t.co/vQcujuwyRF""]",https://arxiv.org/abs/1911.05745v1,"The GD-1 stream is one of the longest and coldest stellar streams discovered to date, and one of the best objects for constraining the dark matter properties of the Milky Way. Using data from {\it Gaia} DR2 we study the proper motions, distance, morphology and density of the stream to uncover small scale perturbations. The proper motion cleaned data shows a clear distance gradient across the stream, ranging from 7 to 12 kpc. However, unlike earlier studies that found a continuous gradient, we uncover a distance minimum at $\varphi_{1}\approx$-50 deg, after which the distance increases again. We can reliably trace the stream between -85$<\varphi_{1}<$15 deg, showing an even further extent to GD-1 beyond the earlier extension of \citet{Price-Whelan18a}. We constrain the stream track and density using a Boolean matched filter approach and find three large under densities and find significant residuals in the stream track lining up with these gaps. In particular, a gap is visible at $\varphi_{1}$=-3 deg, surrounded by a clear sinusoidal wiggle. We argue that this wiggle is due to a perturbation since it has the wrong orientation to come from a progenitor. We compute a total initial stellar mass of the stream segment of 1.58$\pm$0.07$\times$10$^{4}$ M$_{\odot}$. With the extended view of the spur in this work, we argue that the spur may be unrelated to the adjacent gap in the stream. Finally, we show that an interaction with the Sagittarius dwarf can create features similar to the spur. ","] A closer look at the spur, blob, wiggle, and gaps in GD-1"
113,1195259981467136000,157973000,Michael Pfarrhofer,"['New #econometrics WP by @NHauzenberger and me, analyzing time-varying network effects of US #monetary policy on industry returns. We propose a Bayesian state-space spatial panel model to capture heterogeneities across industries and over time.\n<LINK>']",https://arxiv.org/abs/1911.06206,"Understanding disaggregate channels in the transmission of monetary policy is of crucial importance for effectively implementing policy measures. We extend the empirical econometric literature on the role of production networks in the propagation of shocks along two dimensions. First, we allow for industry-specific responses that vary over time, reflecting non-linearities and cross-sectional heterogeneities in direct transmission channels. Second, we allow for time-varying network structures and dependence. This feature captures both variation in the structure of the production network, but also differences in cross-industry demand elasticities. We find that impacts vary substantially over time and the cross-section. Higher-order effects appear to be particularly important in periods of economic and financial uncertainty, often coinciding with tight credit market conditions and financial stress. Differentials in industry-specific responses can be explained by how close the respective industries are to end-consumers. ","Bayesian state-space modeling for analyzing heterogeneous network
  effects of US monetary policy"
114,1194531073553846272,1030804177855959042,Michael Scherer,"['Intertwined orders and emergent symmetries play a critical role in condensed matter physics. In a combined QMC and FRG study   of interacting Dirac fermions, we establish stability of multi-critical points with enhanced symmetries and phase-coexistence <LINK>. <LINK>']",https://arxiv.org/abs/1911.01244,"The quantum phase diagram and critical behavior of two-dimensional Dirac fermions coupled to two compatible order-parameter fields with $O(N_1)\oplus O(N_2)$ symmetry is investigated. Recent numerical studies of such systems have reported evidence for non-Landau-Ginzburg-Wilson transitions and emergent $O(N_1+N_2)$ symmetry between the two ordered states, which has been interpreted within a scenario of deconfined quantum criticality in (2+1)-dimensional Dirac materials. Here, we provide two theoretical approaches to refine the phase diagrams of such systems. In the immediate vicinity of the multicritical point between the ordered phases and the semimetallic phase, we employ a non-perturbative field-theoretical analysis based on the functional renormalization group. For the particular case of $N_1=3$, $N_2=1$, we perform a large-scale quantum Monte Carlo analysis of the strong-coupling region, where both orders meet. Our findings support the robust emergence of enhanced symmetry at the multicritical point and suggest the transition between the two ordered phases to take place via a sequence of continuous transitions. In particular, we find that intermediate regimes of coexistence are present in the phase diagram for all values of $N_1$ and $N_2$. ",Emergent symmetries and coexisting orders in Dirac fermion systems
115,1194338002434084864,548718054,Marc Khoury,"['In a new paper, we study the robustness of classifiers found by adaptive and standard descent methods to adversarial examples. Further we describe how L2-adversarial training affects the geometry of the loss-landscape in least-squares linear regression. \n<LINK>']",https://arxiv.org/abs/1911.03784,Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. In this paper we study how the choice of optimization algorithm influences the robustness of the resulting classifier to adversarial examples. Specifically we show an example of a learning problem for which the solution found by adaptive optimization algorithms exhibits qualitatively worse robustness properties against both $L_{2}$- and $L_{\infty}$-adversaries than the solution found by non-adaptive algorithms. Then we fully characterize the geometry of the loss landscape of $L_{2}$-adversarial training in least-squares linear regression. The geometry of the loss landscape is subtle and has important consequences for optimization algorithms. Finally we provide experimental evidence which suggests that non-adaptive methods consistently produce more robust models than adaptive methods. ,"Adaptive versus Standard Descent Methods and Robustness Against
  Adversarial Examples"
116,1193934303480008704,524971431,William Cai,"['Paper! We propose and analyze a fair way to acquire information on potential recipients and allocate a scarce resource (like government loans or subsidies) when both are constrained by a common budget. <LINK> \n\nwith @jgaeb1, @NikhGarg, and @5harad.']",https://arxiv.org/abs/1911.02715,"Public and private institutions must often allocate scare resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers -- before allocating resources -- can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information -- like those without credit scores -- our approach can substantially improve the allocation of limited assets. ",Fair Allocation through Selective Information Acquisition
117,1192440088610902016,1073390737868365824,Yoni Brande,"[""My first first-author paper is up! If you're into JWST direct imaging and nearby cool stars, check out how we did some simulations to figure out the best way to find Jovian planets with MIRI and JWST!\n\n<LINK>"", ""And a huge thanks to .@mrtommyb, .@elsisrad, .@AstroEricL, and Josh Schlieder (no Twitter) for being the best advisor/collaborators I could've asked for on this project!"", ""@mrtommyb @elsisrad @AstroEricL @JoshuaSchlieder welp, I forgot his @. that's a good start to the day""]",https://arxiv.org/abs/1911.02022,"The upcoming launch of the James Webb Space Telescope (JWST) will dramatically increase our understanding of exoplanets, particularly through direct imaging. Microlensing and radial velocity surveys indicate that some M-dwarfs host long period giant planets. Some of these planets will likely be just a few parsecs away and a few AU from their host stars, a parameter space that cannot be probed by existing high-contrast imagers. We studied whether the coronagraphs on the Mid-Infrared Instrument on JWST can detect Jovian-type planets around nearby M-dwarfs. For a sample of 27 very nearby M-dwarfs, we simulated a sample of Saturn--Jupiter-mass planets with three atmospheric configurations, three orbital separations, observed in three different filters. We found that the f1550c $15.5\mu$m filter is best suited for detecting Jupiter-like planets. Jupiter-like planets with patchy cloud cover, 2 AU from their star, are detectable at $15.5\mu$m around 14 stars in our sample, while Jupiters with clearer atmospheres are detectable around all stars in the sample. Saturns were most detectable at 10.65 and $11.4\mu$m (f1065c and f1140c filters), but only with cloud-free atmospheres and within 3 pc (6 stars). Surveying all 27 stars would take $<170$ hours of JWST integration time, or just a few hours for a shorter survey of the most favorable targets. There is one potentially detectable known planet in our sample -- GJ~832~b. Observations aimed at detecting this planet should occur in 2024--2026, when the planet is maximally separated from the star. ","The Feasibility of Directly Imaging Nearby Cold Jovian Planets with
  MIRI/JWST"
118,1192389408298848256,955602606,Daniele Avitabile,"['In this paper with Helmut Schmidt we find oscillons in networks of spiking neurons, and in their mean field. They arise because of a stimulus (analog to Faraday waves) but also spontaneously, if excitation and inhibition compete. <LINK>']",https://arxiv.org/abs/1911.02437,"We study localized patterns in an exact mean-field description of a spatially-extended network of quadratic integrate-and-fire (QIF) neurons. We investigate conditions for the existence and stability of localized solutions, so-called bumps, and give an analytic estimate for the parameter range where these solutions exist in parameter space, when one or more microscopic network parameters are varied. We develop Galerkin methods for the model equations, which enable numerical bifurcation analysis of stationary and time-periodic spatially-extended solutions. We study the emergence of patterns composed of multiple bumps, which are arranged in a snake-and-ladder bifurcation structure if a homogeneous or heterogeneous synaptic kernel is suitably chosen. Furthermore, we examine time-periodic, spatially-localized solutions (oscillons) in the presence of external forcing, and in autonomous, recurrently coupled excitatory and inhibitory networks. In both cases we observe period doubling cascades leading to chaotic oscillations. ",Bumps and Oscillons in Networks of Spiking Neurons
119,1191268319590043648,2926097863,Yonatan Aljadeff,"['Happy to share this work, with Claudia Clopath, Rob Froemke &amp; Co.\n<LINK>\nWe study how cortical synapses can rely on limited error information to decide whether to potentiate/depress. \nTo approach theoretical capacity, plasticity must have certain properties.\n1/2', 'We find signatures of these properties in previously published in vivo, in vitro data describing acetylcholine dependent and inhibitory plasticity. \n2/2']",http://arxiv.org/abs/1911.00307,"The cortex learns to make associations between stimuli and spiking activity which supports behaviour. It does this by adjusting synaptic weights. The complexity of these transformations implies that synapses have to change without access to the full error information, a problem typically referred to as ""credit-assignment"". However, it remains unknown how the cortex solves this problem. We propose that a combination of plasticity rules, 1) Hebbian, 2) acetylcholine-dependent and 3) noradrenaline-dependent excitatory plasticity, together with 4) inhibitory plasticity restoring E/I balance, effectively solves the credit assignment problem. We derive conditions under-which a neuron model can learn a number of associations approaching its theoretical capacity. We confirm our predictions regarding acetylcholine-dependent and inhibitory plasticity by reanalysing experimental data. Our work suggests that detailed cortical E/I balance reduces the dimensionality of the problem of associating inputs with outputs, thereby allowing imperfect ""supervision"" by neuromodulatory systems to guide learning effectively. ","Cortical credit assignment by Hebbian, neuromodulatory and inhibitory
  plasticity"
120,1202207793396232192,2445322540,Pascal Fua,"['Existing performance measures for road delineation algorithms tend to behave inconsistently, which makes comparisons difficult. We propose a solution in <LINK>']",https://arxiv.org/abs/1911.12467,"Existing performance measures rank delineation algorithms inconsistently, which makes it difficult to decide which one is best in any given situation. We show that these inconsistencies stem from design flaws that make the metrics insensitive to whole classes of errors. To provide more reliable evaluation, we design three new metrics that are far more consistent even though they use very different approaches to comparing ground-truth and reconstructed road networks. We use both synthetic and real data to demonstrate this and advocate the use of these corrected metrics as a tool to gauge future progress. ",Towards Reliable Evaluation of Road Network Reconstructions
121,1202204279148367872,127414067,Roel Dobbe,"['At @NeurIPSConf\'s AI for Social Good workshop, we\'ll ask what makes AI Systems ""safe"". We critique strategies in the ""AI Safety"" domain &amp; propose sociotechnical commitments to address systems, values AND politics throughout design, training and deployment.\n<LINK> <LINK>', '@NeurIPSConf This is a working paper. We very much welcome feedback and ideas for extending this manuscript as we are working towards a longer draft and follow up work.']",https://arxiv.org/abs/1911.09005,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development. ","Hard Choices in Artificial Intelligence: Addressing Normative
  Uncertainty through Sociotechnical Commitments"
122,1197630181306966021,1070655917920829440,Tom Zahavy,"['In a new paper, <LINK>, we study the projection method of Abbeel and Ng (2004) for Apprenticeship Learning. We show that it is, in fact, an instantiation of the Frank-Wolfe method -- a projection free method for convex optimization. (1/5)', 'This insight allows us to:\n(a) Analyze Apprenticeship Learning with tools from the convex optimization literature and derive tighter convergence bounds.\n(b) Simplify the post-processing step (no need for a QP solver). (2/5)', ""(c) Show that a variation of Frank-Wolfe that is taking 'away steps' achieves a linear rate of convergence (for Apprenticeship Learning) — theoretically SOTA!\n(d) Derive a stochastic Apprenticeship Learning algorithm that avoids precise estimation of feature expectations. (3/5)"", 'Joint work with my colleagues @GoogleAI : Alon Cohen, Haim Kaplan, and Yishay Mansour (a.k.a the ML foundations team). Come and chat with us @AAAI2020 and @NeurIPSConf -- Optimization Foundations for RL workshop. (4/5)', 'On a personal note, in the second year of my BSc I decided to become a ML researcher after taking @ng ML course @coursera. @pabbeel’s research influenced every DRL paper I wrote. It is my greatest respect to work on such a fundamental result from these inspiring professors! (5/5)']",https://arxiv.org/abs/1911.01679,"We consider the applications of the Frank-Wolfe (FW) algorithm for Apprenticeship Learning (AL). In this setting, we are given a Markov Decision Process (MDP) without an explicit reward function. Instead, we observe an expert that acts according to some policy, and the goal is to find a policy whose feature expectations are closest to those of the expert policy. We formulate this problem as finding the projection of the feature expectations of the expert on the feature expectations polytope -- the convex hull of the feature expectations of all the deterministic policies in the MDP. We show that this formulation is equivalent to the AL objective and that solving this problem using the FW algorithm is equivalent well-known Projection method of Abbeel and Ng (2004). This insight allows us to analyze AL with tools from convex optimization literature and derive tighter convergence bounds on AL. Specifically, we show that a variation of the FW method that is based on taking ""away steps"" achieves a linear rate of convergence when applied to AL and that a stochastic version of the FW algorithm can be used to avoid precise estimation of feature expectations. We also experimentally show that this version outperforms the FW baseline. To the best of our knowledge, this is the first work that shows linear convergence rates for AL. ",Apprenticeship Learning via Frank-Wolfe
