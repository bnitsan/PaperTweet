,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1029306224431521792,1029293142665846784,Katarina Tothova,['Check out our new paper on estimating uncertainty in shape modelling <LINK> to appear in shapemi@miccai2018. Collaboration with researchers from @bmic_eth @BioMedIAICL @AimBrainHQ @HeartFlow @cvg_ethz @KingsImaging'],https://arxiv.org/abs/1807.11272,"Surface reconstruction is a vital tool in a wide range of areas of medical image analysis and clinical research. Despite the fact that many methods have proposed solutions to the reconstruction problem, most, due to their deterministic nature, do not directly address the issue of quantifying uncertainty associated with their predictions. We remedy this by proposing a novel probabilistic deep learning approach capable of simultaneous surface reconstruction and associated uncertainty prediction. The method incorporates prior shape information in the form of a principal component analysis (PCA) model. Experiments using the UK Biobank data show that our probabilistic approach outperforms an analogous deterministic PCA-based method in the task of 2D organ delineation and quantifies uncertainty by formulating distributions over predicted surface vertex positions. ","Uncertainty Quantification in CNN-Based Surface Prediction Using Shape
  Priors"
1,1027352291718836224,23980621,"Brett Morris, PhD","['Building on my most recent paper (<LINK>), my new ApJ Letter applies the ""self-contamination"" technique to TRAPPIST-1 w/ Spitzer to find:\n\nNon-detection of Contamination by Stellar Activity in the Spitzer Transit Light Curves of TRAPPIST-1\n<LINK> <LINK>']",https://arxiv.org/abs/1807.04886,"We typically measure the radii of transiting exoplanets from the transit depth, which is given by the ratio of cross-sectional areas of the planet and star. However, if a star has dark starspots (or bright regions) distributed throughout the transit chord, the transit depth will be biased towards smaller (larger) values, and thus the inferred planet radius will be smaller (larger) if unaccounted for. We reparameterize the transit light curve to account for ""self-contamination"" by photospheric inhomogeneities by splitting the parameter $R_p/R_\star$ into two parameters: one for the radius ratio -- which controls the duration of ingress and egress -- and another which measures the possibly contaminated transit depth. We show that this is equivalent to the formulation for contamination by a second star (with positive or negative flux), and that it is sensitive to time-steady inhomogeneity of the stellar photosphere. We use synthetic light curves of spotted stars at high signal-to-noise to show that the radius recovered from measurement of the ingress/egress duration can recover the true radii of planets transiting spotted stars with axisymmetric spot distributions if the limb-darkening parameters are precisely known. We fit time-averaged high signal-to-noise transit light curves from Kepler and Spitzer of ten planets to measure the planet radii and search for evidence of spot distributions. We find that this sample has a range of measured depths and ingress durations which are self-consistent, providing no strong evidence for contamination by spots. However, there is suggestive evidence for occultation of starspots on Kepler-17, and that relatively bright regions are occulted by the planets of Kepler-412 and HD 80606. Future observations with the James Webb Space Telescope may enable this technique to yield accurate planetary radii in the presence of stellar inhomogeneities. ","Robust Transiting Exoplanet Radii in the Presence of Starspots from
  Ingress and Egress Durations"
2,1024924817194139648,701342856452112384,Biagio Brattoli,['Check out our new #ECCV2018 paper on arxiv <LINK>. Our model exploits context and time information to learn general representation for objects and actions. An RL policy guides the supervision during training to boost the efficient of the surrogate task. <LINK>'],https://arxiv.org/abs/1807.11293,"Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval. ",Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning
3,1024620477870997504,913238472357437445,Fuminobu TAKAHASHI,['Our new paper is out. It is about eternal inflation and swampland conjectures. <LINK>'],https://arxiv.org/abs/1807.11938,"We study if eternal inflation is realized while satisfying the recently proposed string Swampland criteria concerning the range of scalar field excursion, $|\Delta \phi| < \mathcal{D} \cdot M_{\rm P}$, and the potential gradient, $|\nabla V| > c \cdot V/M_{\rm P}$, where $\mathcal{D}$ and $c$ are constants of order unity, and $M_{\rm P}$ is the reduced Planck mass. We find that only the eternal inflation of chaotic type is possible for $c \sim {\cal O}(0.01)$ and $1/\mathcal{D} \sim {\cal O}(0.01)$, and that the Hubble parameter during the eternal inflation is parametrically close to the Planck scale, and is in the range of $2 \pi c \lesssim H_{\rm inf}/M_{\rm P} < 1/\sqrt{3}$. ",Eternal Inflation and Swampland Conjectures
4,1024560606291091456,2338285848,Johan Ö,"['My new paper ""Fast and Robust Symmetric Image Registration Based on Intensity and  Spatial Information""., developing a novel way to align images efficiently and reliably.\n\n<LINK>']",https://arxiv.org/abs/1807.11599,"Intensity-based image registration approaches rely on similarity measures to guide the search for geometric correspondences with high affinity between images. The properties of the used measure are vital for the robustness and accuracy of the registration. In this study a symmetric, intensity interpolation-free, affine registration framework based on a combination of intensity and spatial information is proposed. The excellent performance of the framework is demonstrated on a combination of synthetic tests, recovering known transformations in the presence of noise, and real applications in biomedical and medical image registration, for both 2D and 3D images. The method exhibits greater robustness and higher accuracy than similarity measures in common use, when inserted into a standard gradient-based registration framework available as part of the open source Insight Segmentation and Registration Toolkit (ITK). The method is also empirically shown to have a low computational cost, making it practical for real applications. Source code is available. ","Fast and Robust Symmetric Image Registration Based on Distances
  Combining Intensity and Spatial Information"
5,1024499681240539136,2596589880,Nikolaus Kriegeskorte,"['New preprint: Cognitive Computational Neuroscience -- a review/perspective paper with @pamelitadouglas <LINK> <LINK>', 'Functional imaging gives us unprecedentedly rich measurements of brain activity, but theory-driven analyses with brain-computational models are needed to reveal computational mechanisms. https://t.co/qQwvTEfx3g https://t.co/OS6DlrIjMe', 'Cognitive computational neuroscience combines the criteria of success of cognitive science, computational neuroscience, and AI: to explain detailed patterns of behavior and brain activity using neurobiologically plausible computational models that perform complex tasks. https://t.co/1Kq20mts9R', 'Explaining how cognition is implemented in the brain requires biologically plausible process models that perform cognitive functions. We face a tradeoff between biological and cognitive fidelity, but the tradeoff can turn into a synergy. https://t.co/qQwvTEfx3g https://t.co/1vYuIM0hEl', 'Building large-scale biologically plausible task-performing models that explain brain and behavioral data will require new forms of cross-disciplinary collaboration. Tasks, data, models, and tests are the key shareable components to coordinate efforts across labs and disciplines. https://t.co/ytzqa6p1od']",https://arxiv.org/abs/1807.11819,"To learn how cognition is implemented in the brain, we must build computational models that can perform cognitive tasks, and test such models with brain and behavioral experiments. Cognitive science has developed computational models of human cognition, decomposing task performance into computational components. However, its algorithms still fall short of human intelligence and are not grounded in neurobiology. Computational neuroscience has investigated how interacting neurons can implement component functions of brain computation. However, it has yet to explain how those components interact to explain human cognition and behavior. Modern technologies enable us to measure and manipulate brain activity in unprecedentedly rich ways in animals and humans. However, experiments will yield theoretical insight only when employed to test brain-computational models. It is time to assemble the pieces of the puzzle of brain computation. Here we review recent work in the intersection of cognitive science, computational neuroscience, and artificial intelligence. Computational models that mimic brain information processing during perceptual, cognitive, and control tasks are beginning to be developed and tested with brain and behavioral data. ",Cognitive computational neuroscience
6,1024360282745958400,252867237,Juan Miguel Arrazola,"['New paper out! ""Machine learning method for state preparation and gate synthesis on photonic quantum computers""\n<LINK>\nThread below: Wigner functions of target states and states prepared by the quantum circuits optimized with our method.', 'Single photon (wavefunction shown below the Wigner function) https://t.co/F55O2vzpYl', 'Hex GKP state https://t.co/9qxO2sHsHy', 'Random state https://t.co/f66Qi06M7s', 'Code publicly available at https://t.co/P4YHTfCnEP']",https://arxiv.org/abs/1807.10781,"We show how techniques from machine learning and optimization can be used to find circuits of photonic quantum computers that perform a desired transformation between input and output states. In the simplest case of a single input state, our method discovers circuits for preparing a desired quantum state. In the more general case of several input and output relations, our method obtains circuits that reproduce the action of a target unitary transformation. We use a continuous-variable quantum neural network as the circuit architecture. The network is composed of several layers of optical gates with variable parameters that are optimized by applying automatic differentiation using the TensorFlow backend of the Strawberry Fields photonic quantum computer simulator. We demonstrate the power and versatility of our methods by learning how to use short-depth circuits to synthesize single photons, Gottesman-Kitaev-Preskill states, NOON states, cubic phase gates, random unitaries, cross-Kerr interactions, as well as several other states and gates. We routinely obtain high fidelities above 99\% using short-depth circuits, typically consisting of a few hundred gates. The circuits are obtained automatically by simply specifying the target state or gate and running the optimization algorithm. ","Machine learning method for state preparation and gate synthesis on
  photonic quantum computers"
7,1024096745771347968,2272134908,Claudia Lagos,"['Very important paper day for me! A new semi-analytic model for galaxy formation: open, very flexible, designed to explore the physical processes of galaxies in an easy/fast way. A milestone for Genesis @ARC_ASTRO3D. @ICRAR @uwanews <LINK> <LINK> <LINK>']",https://arxiv.org/abs/1807.11180,"We present a new, open source, free semi-analytic model (SAM) of galaxy formation, Shark, designed to be highly flexible and modular, allowing easy exploration of different physical processes and ways of modelling them. We introduce the philosophy behind Shark and provide an overview of the physical processes included in the model. Shark is written in C++11 and has been parallelized with OpenMP. In the released version (v1.1), we implement several different models for gas cooling, active galactic nuclei, stellar and photo-ionisation feedback, and star formation (SF). We demonstrate the basic performance of Shark using the Planck15 cosmology SURFS simulations, by comparing against a large set of observations, including: the stellar mass function (SMF) and stellar-halo mass relation at z=0-4; the cosmic evolution of the star formation rate density (SFRD), stellar mass, atomic and molecular hydrogen; local gas scaling relations; and structural galaxy properties, finding excellent agreement. Significant improvements over previous SAMs are seen in the mass-size relation for disks/bulges, the gas-stellar mass and stellar mass-metallicity relations. To illustrate the power of Shark in exploring the systematic effects of the galaxy formation modelling, we quantify how the scatter of the SF main sequence and the gas scaling relations changes with the adopted SF law, and the effect of the starbursts H$_2$ depletion timescale on the SFRD and $\Omega_{\rm H_2}$. We compare Shark with other SAMs and the hydrodynamical simulation EAGLE, and find that SAMs have a much higher halo baryon fractions due to large amounts of intra-halo gas, which in the case of EAGLE is in the intergalactic medium. ","Shark: introducing an open source, free and flexible semi-analytic model
  of galaxy formation"
8,1023948245754671104,21114887,Raphael Gottardo 📊🧪🖥️,['New #clustering paper with postdoc Evan Greene and @GregFinak “Selective Clustering Annotated using Modes of Projections”\n<LINK>\n#rstats package available at <LINK> Feedbacks and comments welcome!'],https://arxiv.org/abs/1807.10328,"Selective clustering annotated using modes of projections (SCAMP) is a new clustering algorithm for data in $\mathbb{R}^p$. SCAMP is motivated from the point of view of non-parametric mixture modeling. Rather than maximizing a classification likelihood to determine cluster assignments, SCAMP casts clustering as a search and selection problem. One consequence of this problem formulation is that the number of clusters is $\textbf{not}$ a SCAMP tuning parameter. The search phase of SCAMP consists of finding sub-collections of the data matrix, called candidate clusters, that obey shape constraints along each coordinate projection. An extension of the dip test of Hartigan and Hartigan (1985) is developed to assist the search. Selection occurs by scoring each candidate cluster with a preference function that quantifies prior belief about the mixture composition. Clustering proceeds by selecting candidates to maximize their total preference score. SCAMP concludes by annotating each selected cluster with labels that describe how cluster-level statistics compare to certain dataset-level quantities. SCAMP can be run multiple times on a single data matrix. Comparison of annotations obtained across iterations provides a measure of clustering uncertainty. Simulation studies and applications to real data are considered. A C++ implementation with R interface is $\href{this https URL}{available\ online}$. ",Selective Clustering Annotated using Modes of Projections
9,1023934181469024256,1339581511,Ben Rackham,"['New paper on the atmosphere of the hot Jupiter WASP-19b, led by @nespinozap, on arXiv today: <LINK>\n\nRetrievals considering both planetary and stellar spectral features offer a promising path forward for disentangling their contributions to transmission spectra. <LINK>']",https://arxiv.org/abs/1807.10652,"The short period ($0.94$-day) transiting exoplanet WASP-19b is an exceptional target for transmission spectroscopy studies, due to its relatively large atmospheric scale-height ($\sim 500$ km) and equilibrium temperature ($\sim 2100$ K). Here we report on six precise spectroscopic Magellan/IMACS observations, five of which target the full optical window from $0.45-0.9\mu$m and one targeting the $0.4-0.55\mu$m blue-optical range. Five of these datasets are consistent with a transmission spectrum without any significant spectral features, while one shows a significant slope as a function of wavelength, which we interpret as arising from photospheric heterogeneities in the star. Coupled with HST/WFC3 infrared observations, our optical/near-infrared measurements point to the presence of high altitude clouds in WASP-19b's atmosphere in agreement with previous studies. Using a semi-analytical retrieval approach, considering both planetary and stellar spectral features, we find a water abundance consistent with solar for WASP-19b and strong evidence for sub-solar abundances for optical absorbers such as TiO and Na; no strong optical slope is detected, which suggests that if hazes are present, they are much weaker than previously suggested. In addition, two spot-crossing events are observed in our datasets and analyzed, including one of the first unambiguously detected bright spot-crossing events on an exoplanet host star. ","ACCESS: A featureless optical transmission spectrum for WASP-19b from
  Magellan/IMACS"
10,1023863156773412864,96629016,Néstor Espinoza,"['New paper on the atmosphere of WASP-19b today in the arXiv: <LINK>. \n\nAfter observing 6 transits in the 0.45-0.9um range from Magellan/IMACS at @LCOAstro, we see none of the features previously reported in a @NatureAstronomy paper in 5 observations. Thread:', 'First of all, this has been four long years of work: my entire PhD was focused in making sure Magellan/IMACS worked for transmission spectroscopy. We gathered two transits in 2014, one in 2015 and two more on 2017 for this system. Our best transit has 270 ppm precision! https://t.co/2NEAlDOFdX', 'What we did was to obtain lightcurves at different colors while the planet was transiting. The idea is that different gases absorb at different wavelengths, and so the planet looks bigger at colors where gasses absorb: https://t.co/6MZ7ELFhZt', 'It\'s hard to look at the lightcurves directly, right? So that\'s why we plot the transit depths, the square-root of which is the ratio between the planet\'s radius and the stellar radius. This is what we call a ""transmission spectrum"". Looks quite flat, doesn\'t it? https://t.co/a2x9I1udXj', ""But there's a catch. In the previous tweet I showed five datasets. But we took six. Well, one of them *does* show some features. To interpret them, with @benrackham we wrote a retrieval code, which models features not only from the planet, but also from the star. https://t.co/suks0CtKxL"", ""Our conclusion was that the star *is* imprinting features in the observed transmission spectrum in this case. Weird, but expected given how variable/active the star is! Here's the flux variation in time of the *star*, most likely produced by starspots coming in and out of view: https://t.co/W7FDBLhKVu"", 'So we combined the spectra that were not contaminated with stellar features, and joined that data with previous data taken by the @NASAHubble and @NASAspitzer. We ran our retrieval code on this dataset, which helped us to understand what is going on in the atmosphere of WASP-19b: https://t.co/4LWHmXZ2VX', 'We conclude that the water vapor content in the atmosphere is consistent with ""solar"", and that the pressure of the optically thick part of the atmosphere (which we called P0 just to confuse theorists out there) is consistent with being produced by high altitude clouds. https://t.co/ejVbGLdfM2', 'Oh: and we also detected spot-crossing events including a BRIGHT spot crossing event! A picture is better than 150 characters: https://t.co/MMlKnRFES0', 'Because we have color information, we were able to estimate the temperature of the spots: the bright spot seems to be ~140K hotter than the star, while the cold spot appears to be ~200 K cooler. https://t.co/nYu6uvF3v8', '@NatureAstronomy @LCOAstro Oops! Thanks for the heads-up!', '@brettmor @LCOAstro @NatureAstronomy Thanks Brett!']",https://arxiv.org/abs/1807.10652,"The short period ($0.94$-day) transiting exoplanet WASP-19b is an exceptional target for transmission spectroscopy studies, due to its relatively large atmospheric scale-height ($\sim 500$ km) and equilibrium temperature ($\sim 2100$ K). Here we report on six precise spectroscopic Magellan/IMACS observations, five of which target the full optical window from $0.45-0.9\mu$m and one targeting the $0.4-0.55\mu$m blue-optical range. Five of these datasets are consistent with a transmission spectrum without any significant spectral features, while one shows a significant slope as a function of wavelength, which we interpret as arising from photospheric heterogeneities in the star. Coupled with HST/WFC3 infrared observations, our optical/near-infrared measurements point to the presence of high altitude clouds in WASP-19b's atmosphere in agreement with previous studies. Using a semi-analytical retrieval approach, considering both planetary and stellar spectral features, we find a water abundance consistent with solar for WASP-19b and strong evidence for sub-solar abundances for optical absorbers such as TiO and Na; no strong optical slope is detected, which suggests that if hazes are present, they are much weaker than previously suggested. In addition, two spot-crossing events are observed in our datasets and analyzed, including one of the first unambiguously detected bright spot-crossing events on an exoplanet host star. ","ACCESS: A featureless optical transmission spectrum for WASP-19b from
  Magellan/IMACS"
11,1023170058430947329,1910301474,MartinWeides,['New paper on microwave spectroscopy on eight qubit Tavis-Cummings-simulator (theory) on arXiv\n<LINK>'],https://arxiv.org/abs/1807.09567,"We demonstrate how heating of an environment can invert the line shape of a driven cavity. We consider a superconducting coplanar cavity coupled to multiple artificial atoms. The measured cavity transmission is characterized by Fano-type resonances with a shape that is continuously tunable by bias current through nearby (magnetic flux) control lines. In particular, the same dispersive shift of the microwave cavity can be observed as a peak or a dip. We find that this Fano-peak inversion is possible due to a tunable interference between a microwave transmission through a background, with reactive and dissipative properties, and through the cavity, affected by bias-current induced heating. The background transmission occurs due to crosstalk between the control and transmission lines. We show how such background can be accounted for by Jaynes-Cummings type models via modified boundary conditions between the cavity and transmission lines. We find generally that whereas resonance positions determine system energy levels, resonance shapes give information on system fluctuations and dissipation. ","Resonance inversion in a superconducting cavity coupled to artificial
  atoms and a microwave background"
12,1022872964910465024,14852684,Bertram Ludäscher,"['Kudos to Shawn and Tim for combining theory &amp; practice (logic inference &amp; #YesWorkflow) in powerful new ways! #IPAW2018 Best Paper is available here: <LINK> <LINK>', '..and here is Shawn, giving the talk at the recent #ProvenanceWeek ! https://t.co/dr8BbtvkEn']",https://arxiv.org/abs/1807.09899,"An advantage of scientific workflow systems is their ability to collect runtime provenance information as an execution trace. Traces include the computation steps invoked as part of the workflow run along with the corresponding data consumed and produced by each workflow step. The information captured by a trace is used to infer ""lineage"" relationships among data items, which can help answer provenance queries to find workflow inputs that were involved in producing specific workflow outputs. Determining lineage relationships, however, requires an understanding of the dependency patterns that exist between each workflow step's inputs and outputs, and this information is often under-specified or generally assumed by workflow systems. For instance, most approaches assume all outputs depend on all inputs, which can lead to lineage ""false positives"". In prior work, we defined annotations for specifying detailed dependency relationships between inputs and outputs of computation steps. These annotations are used to define corresponding rules for inferring fine-grained data dependencies from a trace. In this paper, we extend our previous work by considering the impact of dependency annotations on workflow specifications. In particular, we provide a reasoning framework to ensure the set of dependency annotations on a workflow specification is consistent. The framework can also infer a complete set of annotations given a partially annotated workflow. Finally, we describe an implementation of the reasoning framework using answer-set programming. ","Validation and Inference of Schema-Level Workflow Data-Dependency
  Annotations"
13,1022827784673132550,923231130383536128,Eduardo Fonseca,"['Our new paper is out, describing the Task 2 of DCASE2018 Challenge, result of the collaboration between @mtg_upf and Google Machine Perception Team: <LINK>  “General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline”']",https://arxiv.org/abs/1807.09902,"This paper describes Task 2 of the DCASE 2018 Challenge, titled ""General-purpose audio tagging of Freesound content with AudioSet labels"". This task was hosted on the Kaggle platform as ""Freesound General-Purpose Audio Tagging Challenge"". The goal of the task is to build an audio tagging system that can recognize the category of an audio clip from a subset of 41 diverse categories drawn from the AudioSet Ontology. We present the task, the dataset prepared for the competition, and a baseline system. ","General-purpose Tagging of Freesound Audio with AudioSet Labels: Task
  Description, Dataset, and Baseline"
14,1022566042881679362,783061931334766596,Kay L. Kirkpatrick \🥄\🥄,"['Our new paper on the arxiv: \n\nTransport of a quantum particle in a time-dependent white-noise potential, with Hislop, Olla, and Schenker: <LINK>']",https://arxiv.org/abs/1807.08317,"We show that a quantum particle in $\mathbb{R}^d$, for $d \geq 1$, subject to a white-noise potential, moves super-ballistically in the sense that the mean square displacement $\int \|x\|^2 \langle \rho(x,x,t) \rangle ~dx$ grows like $t^{3}$ in any dimension. The white noise potential is Gaussian distributed with an arbitrary spatial correlation function and a delta correlation function in time. This is a known result in one dimension (see refs. Fischer, Leschke, M\""uller and Javannar, Kumar}. The energy of the system is also shown to increase linearly in time. We also prove that for the same white-noise potential model on the lattice $\mathbb{Z}^d$, for $d \geq 1$, the mean square displacement is diffusive growing like $t^{1}$. This behavior on the lattice is consistent with the diffusive behavior observed for similar models in the lattice $\mathbb{Z}^d$ with a time-dependent Markovian potential (see ref. Kang, Schenker). ","Transport of a quantum particle in a time-dependent white-noise
  potential"
15,1022436422614179841,839104540985151490,IPPP Durham,"['New IPPP paper: ""VBS studies in CMS"" by Raquel Gomez-Ambrosio <LINK>']",http://arxiv.org/abs/1807.09634,"In these proceedings we discuss the family of Vector Boson Scattering (VBS) processes, in particular we look at a very recent result from the CMS collaboration where a search was performed for VBS in the four-lepton and two-jet final state using proton-proton collisions at 13 TeV. The electroweak production of two Z bosons in association with two jets was measured with an observed (expected) significance of 2.7 (1.6) standard deviations, using a multivariate classifier. Additionally an expected significance of 1.2 standard deviations was found using matrix elements techniques. In this work we discuss the latter approach in detail. ",VBS studies in CMS
16,1022318565741944834,296161364,Chris Power,"['""Does slow and steady win the race? Investigating feedback processes in giant molecular clouds"" - impressive new paper from her PhD thesis by our @ARC_ASTRO3D @ICRAR postdoc Lilian Garrett-Smithson on the arXiv today - <LINK> <LINK>']",https://arxiv.org/abs/1807.09489,"We investigate the effects of gradual heating on the evolution of turbulent molecular clouds of mass $2\times 10^6$ M$_\odot$ and virial parameters ranging between $0.7-1.2$. This gradual heating represents the energy output from processes such as winds from massive stars or feedback from High Mass X-ray binaries (HMXBs), contrasting the impulsive energy injection from supernovae (SNe). For stars with a mass high enough that their lifetime is shorter than the life of the cloud, we include a SN feedback prescription. Including both effects, we investigate the interplay between slow and fast forms of feedback and their effectiveness at triggering/suppressing star formation. We find that SN feedback can carve low density chimneys in the gas, offering a path of least resistance for the energy to escape. Once this occurs the more stable, but less energetic, gradual feedback is able to keep the chimneys open. By funneling the hot destructive gas away from the centre of the cloud, chimneys can have a positive effect on both the efficiency and duration of star formation. Moreover, the critical factor is the number of high mass stars and SNe (and any subsequent HMXBs) active within the free-fall time of each cloud. This can vary from cloud to cloud due to the stochasticity of SN delay times and in HMXB formation. However, the defining factor in our simulations is the efficiency of the cooling, which can alter the Jeans mass required for sink particle formation, along with the number of massive stars in the cloud. ","Does slow and steady win the race? Investigating feedback processes in
  giant molecular clouds"
17,1022303822851579904,255860782,Kristopher Micinski,['New Arxiv draft! Racets: Faceted Execution in Racket. First paper w/ my student Zhanpeng at @haverfordedu <LINK>'],https://arxiv.org/abs/1807.09377,"Faceted Execution is a linguistic paradigm for dynamic information-flow control. Under faceted execution, secure program data is represented by faceted values: decision trees that encode how the data should appear to its owner (represented by a label) versus everyone else. When labels are allowed to be first-class (i.e., predicates that decide at runtime which data to reveal), faceted execution enables policy-agnostic programming: a programming style that allows privacy policies for data to be enforced independently of code that computes on that data. To date, implementations of faceted execution are relatively heavyweight: requiring either changing the language runtime or the application code (e.g., by using monads). Following Racket's languages-as-libraries approach, we present Racets: an implementation of faceted execution as a library of macros. Given Racket's highly-expressive macro system, our implementation follows relatively directly from the semantics of faceted execution. To demonstrate how Racets can be used for policy-agnostic programming, we use it to build a web-based game of Battleship. Our implementation sheds light on several interesting issues in interacting with code written without faceted execution. Our Racets implementation is open source, under development, and available online. ",Racets: Faceted Execution in Racket
18,1022085429036834816,762420558,Ciaran O'Hare,"['New paper out today. A stellar stream has been found in SDSS-#Gaia data that intersects the Solar system. We look at whether or not a DM detector will be able to see it. <LINK>', 'https://t.co/T3ESsTqlta']",https://arxiv.org/abs/1807.09004,"The recently discovered S1 stream passes through the Solar neighbourhood on a low inclination, counter-rotating orbit. The progenitor of S1 is a dwarf galaxy with a total mass comparable to the present-day Fornax dwarf spheroidal, so the stream is expected to have a significant DM component. We compute the effects of the S1 stream on WIMP and axion detectors as a function of the density of its unmeasured dark component. In WIMP detectors the S1 stream supplies more high energy nuclear recoils so will marginally improve DM detection prospects. We find that even if S1 comprises less than 10% of the local density, multi-ton xenon WIMP detectors can distinguish the S1 stream from the bulk halo in the relatively narrow mass range between 5 and 25 GeV. In directional WIMP detectors such as CYGNUS, S1 increases DM detection prospects more substantially since it enhances the anisotropy of the WIMP signal. Finally, we show that axion haloscopes possess by far the greatest potential sensitivity to the S1 stream. Once the axion mass has been discovered, the distinctive velocity distribution of S1 can easily be extracted from the axion power spectrum. ","A Dark Matter Hurricane: Measuring the S1 Stream with Dark Matter
  Detectors"
19,1022046415223250945,1731561564,Guillaume Rousselet,"['Skipped correlations handle  multivariate outliers (Spearman’s doesn’t). New paper with Wilcox &amp; @CyrilRPernet on how to deal with multiple comparisons:\n\n<LINK>\n\nPreprint:\n\n<LINK>\n\nReproducibility package:\n\n<LINK>', '@dan_marinazzo @CyrilRPernet @OmnesResNetwork yes we did, but before getting the proofs.']",https://arxiv.org/abs/1807.05048,"A skipped correlation has the advantage of dealing with outliers in a manner that takes into account the overall structure of the data cloud. For p-variate data, $p \ge 2$, there is an extant method for testing the hypothesis of a zero correlation for each pair of variables that is designed to control the probability of one or more Type I errors. And there are methods for the related situation where the focus is on the association between a dependent variable and $p$ explanatory variables. However, there are limitations and several concerns with extant techniques. The paper describes alternative approaches that deal with these issues. ","Improved Methods for Making Inferences About Multiple Skipped
  Correlations"
20,1021731270651457537,223923566,David Van Horn,"['New paper: Constructive Galois Connections, w/ @daviddarais to appear in @CUP_JFP  <LINK>', ""@rob_rix @daviddarais It's full, archival version.""]",https://arxiv.org/abs/1807.08711,"Galois connections are a foundational tool for structuring abstraction in semantics and their use lies at the heart of the theory of abstract interpretation. Yet, mechanization of Galois connections using proof assistants remains limited to restricted modes of use, preventing their general application in mechanized metatheory and certified programming. This paper presents constructive Galois connections, a variant of Galois connections that is effective both on paper and in proof assistants; is complete with respect to a large subset of classical Galois connections; and enables more general reasoning principles, including the ""calculational"" style advocated by Cousot. To design constructive Galois connections we identify a restricted mode of use of classical ones which is both general and amenable to mechanization in dependently-typed functional programming languages. Crucial to our metatheory is the addition of monadic structure to Galois connections to control a ""specification effect."" Effectful calculations may reason classically, while pure calculations have extractable computational content. Explicitly moving between the worlds of specification and implementation is enabled by our metatheory. To validate our approach, we provide two case studies in mechanizing existing proofs from the literature: the first uses calculational abstract interpretation to design a static analyzer; the second forms a semantic basis for gradual typing. Both mechanized proofs closely follow their original paper-and-pencil counterparts, employ reasoning principles not captured by previous mechanization approaches, support the extraction of verified algorithms, and are novel. ",Constructive Galois Connections
21,1021661603354365956,292832009,Tomasz Kacprzak,['New paper out today! We are exploring the advantages of using Deep Learning for cosmology with weak lensing.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1807.08732,"Deep learning is a powerful analysis technique that has recently been proposed as a method to constrain cosmological parameters from weak lensing mass maps. Due to its ability to learn relevant features from the data, it is able to extract more information from the mass maps than the commonly used power spectrum, and thus achieve better precision for cosmological parameter measurement. We explore the advantage of Convolutional Neural Networks (CNN) over the power spectrum for varying levels of shape noise and different smoothing scales applied to the maps. We compare the cosmological constraints from the two methods in the $\Omega_M-\sigma_8$ plane for sets of 400 deg$^2$ convergence maps. We find that, for a shape noise level corresponding to 8.53 galaxies/arcmin$^2$ and the smoothing scale of $\sigma_s = 2.34$ arcmin, the network is able to generate 45% tighter constraints. For smaller smoothing scale of $\sigma_s = 1.17$ the improvement can reach $\sim 50 \%$, while for larger smoothing scale of $\sigma_s = 5.85$, the improvement decreases to 19%. The advantage generally decreases when the noise level and smoothing scales increase. We present a new training strategy to train the neural network with noisy data, as well as considerations for practical applications of the deep learning approach. ","Cosmological constraints from noisy convergence maps through deep
  learning"
22,1021641004863119360,2326897792,matthieu bulté,['Very proud to annouce our new tutorial paper:\n“A practical example for the non-linear Bayesian filtering of model parameters”.\nAn accessible and reproducible intro Bayesian filtering and Sequential Monte Carlo.\nPreprint: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/1807.08713,"In this tutorial we consider the non-linear Bayesian filtering of static parameters in a time-dependent model. We outline the theoretical background and discuss appropriate solvers. We focus on particle-based filters and present Sequential Importance Sampling (SIS) and Sequential Monte Carlo (SMC). Throughout the paper we illustrate the concepts and techniques with a practical example using real-world data. The task is to estimate the gravitational acceleration of the Earth $g$ by using observations collected from a simple pendulum. Importantly, the particle filters enable the adaptive updating of the estimate for $g$ as new observations become available. For tutorial purposes we provide the data set and a Python implementation of the particle filters. ","A practical example for the non-linear Bayesian filtering of model
  parameters"
23,1021612243627126784,238054276,Xianyu Tan,['Our new paper about atmospheric circulation of brown dwarfs and Jupiter-like giant planets is on arXiv -- it discusses mechanisms generating robust zonal jets and long-term oscillations.    <LINK>'],https://arxiv.org/abs/1807.08433,"Brown dwarfs and directly imaged giant planets exhibit significant evidence for active atmospheric circulation, which induces a large-scale patchiness in the cloud structure that evolves significantly over time, as evidenced by infrared light curves and Doppler maps. These observations raise critical questions about the fundamental nature of the circulation, its time variability, and the overall relationship to the circulation on Jupiter and Saturn. Jupiter and Saturn themselves exhibit numerous robust zonal (east-west) jet streams at the cloud level; moreover, both planets exhibit long-term stratospheric oscillations involving perturbations of zonal wind and temperature that propagate downward over time on timescales of ~4 years (Jupiter) and ~15 years (Saturn). These oscillations, dubbed the Quasi Quadrennial Oscillation (QQO) for Jupiter and the Semi-Annual Oscillation (SAO) on Saturn, are thought to be analogous to the Quasi-Biennial Oscillation (QBO) on Earth, which is driven by upward propagation of equatorial waves from the troposphere. To investigate these issues, we here present global, three-dimensional, high-resolution numerical simulations of the flow in the stratified atmosphere--overlying the convective interior--of brown dwarfs and Jupiter-like planets. The effect of interior convection is parameterized by inducing small-scale, randomly varying perturbations in the radiative-convective boundary at the base of the model. In the simulations, the convective perturbations generate atmospheric waves and turbulence that interact with the rotation to produce numerous zonal jets. Moreover, the equatorial stratosphere exhibits stacked eastward and westward jets that migrate downward over time, exactly as occurs in the terrestrial QBO, Jovian QQO, and Saturnian SAO. This is the first demonstration of a QBO-like phenomenon in 3D numerical simulations of a giant planet. ","Atmospheric Circulation of Brown Dwarfs and Jupiter and Saturn-like
  Planets: Zonal Jets, Long-term Variability, and QBO-type Oscillations"
24,1021494627344498688,3087725630,Zack Kilpatrick (he/him/his),"['New on arXiv: ""Optimizing a jump-diffusion model of a starving forager"" Fun paper with talented undergrad N Krishnan showing the lifetime of the Bhat-Redner starving forager is increased by considering a combination of short &amp; long range movement:\n<LINK>', 'Panel D shows the behavior of a mixed jump-diffusion forager clearing out ""food deserts."" This type of forager lives longer than a pure diffuser or jumper when foragers can only last a finite time without food. https://t.co/ZweKZeyaII']",https://arxiv.org/abs/1807.06740,"We analyze the movement of a starving forager on a one-dimensional periodic lattice, where each location contains one unit of food. As the forager lands on sites with food, it consumes the food, leaving the sites empty. If the forager lands consecutively on $s$ empty sites, then it will starve. The forager has two modes of movement: it can either diffuse, by moving with equal probability to adjacent sites on the lattice, or it can jump to a uniformly randomly chosen site on the lattice. We show that the lifetime $T$ of the forager in either paradigm can be approximated by the sum of the cover time $\tau_{\rm cover}$ and the starvation time $s$, when $s$ far exceeds the number $n$ of lattice sites. Our main findings focus on the hybrid model, where the forager has a probability of either jumping or diffusing. The lifetime of the forager varies non-monotonically according to $p_j$, the probability of jumping. By examining a small system, analyzing a heuristic model, and using direct numerical simulation, we explore the tradeoff between jumps and diffusion, and show that the strategy that maximizes the forager lifetime is a mixture of both modes of movement. ",Optimizing a jump-diffusion model of a starving forager
25,1021311817589682176,739505640326987777,Guido Roberts-Borsani,"['New paper out today! <LINK>\nVery pleased with this one: looking at cold gas inflows and outflows in local, normal galaxies using stacking techniques of SDSS spectra - aiming to constrain the strength/properties of the flows and whether they can quench the galaxy! <LINK>', 'In the local Universe, two populations of galaxies are observed: blue+star-forming and red+passive galaxies that have stopped forming stars. The general consensus is that galaxies transit from this blue population to the red one - ie., something is quenching the star formation. https://t.co/joi7BKF32y', 'One of the major unanswered questions in astronomy is how does this happen? Here we look at the role of galaxy outflows and aim to constrain whether they can expel enough Hydrogen gas out of the galaxy - thereby removing the ""fuel"" for star formation - to ""quench"" the host. https://t.co/fLvv55ZLfR', 'Many authors have done great work on this, but generally looked at extreme objects with somewhat incomplete data sets. Here we expand on this by looking at all galaxy populations with one of the largest astronomical surveys to date (@sdssurveys).']",https://arxiv.org/abs/1807.07575,"We perform a stacking analysis of the neutral \nad\,$\lambda\lambda$5889,5895\,\AA\ ISM doublet using the SDSS DR7 spectroscopic data set to probe the prevalence and characteristics of cold (T\,$\lesssim$\,10$^{4}$\,K) galactic-scale gas flows in local (0.025$\leqslant z\leqslant$0.1) inactive and AGN-host galaxies across the SFR-M$_{*}$ plane. We find low-velocity outflows to be prevalent in regions of high SFRs and stellar masses (10 $\lesssim$log M$_{*}$/M$_{\odot}$ $\lesssim$ 11.5), however we do not find any detections in the low mass (log M$_{*}$/M$_{\odot}$ $\lesssim$ 10) regime. We also find tentative detections of inflowing gas in high mass galaxies across the star-forming population. We derive mass outflow rates in the range of 0.14-1.74\,M$_{\odot}$yr$^{-1}$ and upper limits on inflow rates <1\,M$_{\odot}$yr$^{-1}$, allowing us to place constraints on the mass loading factor ($\eta$=$\dot{M}_{\text{out}}$/SFR) for use in simulations of the local Universe. We discuss the fate of the outflows by comparing the force provided by the starburst to the critical force needed to push the outflow outward, and find the vast majority of the outflows unlikely to escape the host system. Finally, as outflow detection rates and central velocities do not vary strongly with the presence of a (weak) active supermassive black hole, we determine that star formation appears to be the primary driver of outflows at $z\sim$0. ","The prevalence and properties of cold gas inflows and outflows around
  galaxies in the local Universe"
26,1021105257152036865,968555423467945984,Matthew Schlegel,['Put up a new paper on Arxiv (my first 😆)! General Value Function Networks: <LINK>.'],https://arxiv.org/abs/1807.06763,"State construction is important for learning in partially observable environments. A general purpose strategy for state construction is to learn the state update using a Recurrent Neural Network (RNN), which updates the internal state using the current internal state and the most recent observation. This internal state provides a summary of the observed sequence, to facilitate accurate predictions and decision-making. At the same time, specifying and training RNNs is notoriously tricky, particularly as the common strategy to approximate gradients back in time, called truncated Back-prop Through Time (BPTT), can be sensitive to the truncation window. Further, domain-expertise--which can usually help constrain the function class and so improve trainability--can be difficult to incorporate into complex recurrent units used within RNNs. In this work, we explore how to use multi-step predictions to constrain the RNN and incorporate prior knowledge. In particular, we revisit the idea of using predictions to construct state and ask: does constraining (parts of) the state to consist of predictions about the future improve RNN trainability? We formulate a novel RNN architecture, called a General Value Function Network (GVFN), where each internal state component corresponds to a prediction about the future represented as a value function. We first provide an objective for optimizing GVFNs, and derive several algorithms to optimize this objective. We then show that GVFNs are more robust to the truncation level, in many cases only requiring one-step gradient updates. ",General Value Function Networks
27,1020371111878438913,14344469,Pete Skomoroch,['Pangloss: Fast Entity Linking in Noisy Text Environments. Our new #KDD2018 paper <LINK> on understanding workplace conversations <LINK>'],https://arxiv.org/abs/1807.06036,"Entity linking is the task of mapping potentially ambiguous terms in text to their constituent entities in a knowledge base like Wikipedia. This is useful for organizing content, extracting structured data from textual documents, and in machine learning relevance applications like semantic search, knowledge graph construction, and question answering. Traditionally, this work has focused on text that has been well-formed, like news articles, but in common real world datasets such as messaging, resumes, or short-form social media, non-grammatical, loosely-structured text adds a new dimension to this problem. This paper presents Pangloss, a production system for entity disambiguation on noisy text. Pangloss combines a probabilistic linear-time key phrase identification algorithm with a semantic similarity engine based on context-dependent document embeddings to achieve better than state-of-the-art results (>5% in F1) compared to other research or commercially available systems. In addition, Pangloss leverages a local embedded database with a tiered architecture to house its statistics and metadata, which allows rapid disambiguation in streaming contexts and on-device disambiguation in low-memory environments such as mobile phones. ",Pangloss: Fast Entity Linking in Noisy Text Environments
28,1020367061430648832,14344469,Pete Skomoroch,"['Pangloss: Fast Entity Linking in Noisy Text Environments. Our new #KDD2018 paper on entity disambiguation for workplace dialogue using context-dependent document embeddings <LINK> \n\nWatch this video to see it in action <LINK>', ""I'll be in London August 19-23 with my team from @Workday presenting our paper on entity linking at the #KDD2018 conference, reach out if you want to connect""]",https://arxiv.org/abs/1807.06036,"Entity linking is the task of mapping potentially ambiguous terms in text to their constituent entities in a knowledge base like Wikipedia. This is useful for organizing content, extracting structured data from textual documents, and in machine learning relevance applications like semantic search, knowledge graph construction, and question answering. Traditionally, this work has focused on text that has been well-formed, like news articles, but in common real world datasets such as messaging, resumes, or short-form social media, non-grammatical, loosely-structured text adds a new dimension to this problem. This paper presents Pangloss, a production system for entity disambiguation on noisy text. Pangloss combines a probabilistic linear-time key phrase identification algorithm with a semantic similarity engine based on context-dependent document embeddings to achieve better than state-of-the-art results (>5% in F1) compared to other research or commercially available systems. In addition, Pangloss leverages a local embedded database with a tiered architecture to house its statistics and metadata, which allows rapid disambiguation in streaming contexts and on-device disambiguation in low-memory environments such as mobile phones. ",Pangloss: Fast Entity Linking in Noisy Text Environments
29,1020362261276381184,1009799569390096384,Brenden Lake,"['Recurrent neural nets have trouble combining familiar words in new ways, such as inferring the meaning of ""around right"" from the meaning of ""around"" and ""right."" @JoaoLoula\'s new paper is on arXiv, with Marco Baroni and me\n\n<LINK>']",https://arxiv.org/abs/1807.07545,"Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it's seen as key to humans' capacity for generalization in language. Recent work has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool, requiring models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as ""around"" and ""right"") in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of ""X around right"" to ""jump around right""), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of ""around right"" from those of ""right"" and ""around""). ","Rearranging the Familiar: Testing Compositional Generalization in
  Recurrent Networks"
30,1020190568692019200,176191683,Olga Zamora,['Our new @APOGEEsurvey\npaper is available on arXiv. Galactic halo: what are you made of ? #starwars\n<LINK> <LINK>'],https://arxiv.org/abs/1807.07269,"We report an analysis of the metal-rich tail ([Fe/H] $> -0.75$) of halo stars located at distances from the Galactic plane $z$ up to $|z| \sim 10$ kpc, observed by the Apache Point Observatory Galactic Evolution Experiment (APOGEE). We examine the chemistry, kinematics, and dynamics of this metal-rich halo sample using chemical abundances and radial velocities provided by the fourteenth APOGEE data release (DR14) and proper motions from the second Gaia data release (DR2). The analysis reveals three chemically different stellar populations in the [Mg/Fe] vs. [Fe/H] space -- the two distinct halo populations already reported in the literature, and a third group with intermediate [Mg/Fe] $\sim$+0.1. We derive the $U$, $V$ and $W$ velocity components with respect to the Local Standard of Rest, as well as orbits for the three stellar groups, and find that they differ also in their kinematical and dynamical properties. The high-[Mg/Fe] population exhibits a mean prograde rotation, as well as orbits that are more bound and closer to the plane, whereas the low-[Mg/Fe] population has $<V>$ closer to 0, and stars that move in less-bound orbits reaching larger distances from the centre and the Galactic plane. The intermediate-Mg stars exhibit different orbital characteristics, moving with a strong prograde rotation and low excentricity, but in less-bound orbits. This stellar population resembles the two stellar overdensities lying about $|z| \sim 5$ kpc recently reported in the literature, for which a disc origin has been claimed. ","The metal-rich halo component extended in z: a characterization with
  Gaia DR2 and APOGEE"
31,1020106065059328000,837133583558987776,Colin Raffel,"['New paper w/ @D_Berthelot_ML Aurko Roy and @goodfellow_ian where we propose an adversarial regularizer for improving interpolation in autoencoders and measure whether it also improves representation learning performance. Paper <LINK>, code <LINK> <LINK>']",http://arxiv.org/abs/1807.07543,"Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can ""interpolate"": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations. ","Understanding and Improving Interpolation in Autoencoders via an
  Adversarial Regularizer"
32,1019883069518557184,64954849,Ethan Buchman,['Published new paper on @tendermint_team #consensus w/ complete description of protocol &amp; proofs of safety+liveness. Main contrib to #BFT literature is new liveness mechanism via #p2p gossip. Please find errors!\n\nPreprint: <LINK>\nSource: <LINK> <LINK>'],https://arxiv.org/abs/1807.04938,"The paper presents Tendermint, a new protocol for ordering events in a distributed network under adversarial conditions. More commonly known as Byzantine Fault Tolerant (BFT) consensus or atomic broadcast, the problem has attracted significant attention in recent years due to the widespread success of blockchain-based digital currencies, such as Bitcoin and Ethereum, which successfully solved the problem in a public setting without a central authority. Tendermint modernizes classic academic work on the subject and simplifies the design of the BFT algorithm by relying on a peer-to-peer gossip protocol among nodes. ",The latest gossip on BFT consensus
33,1019394064838324225,1576235694,Michael Brown,"['One of my PhD students, @vparkash2014, has her first lead author paper on arxiv today! \n\nNew WISE photometry of nearby galaxies! Relationships between HI, stellar mass and star formation! #astronomy @coffee_samurai \n<LINK> <LINK>']",https://arxiv.org/abs/1807.06246,"We have measured the relationships between HI mass, stellar mass and star formation rate using the HI Parkes All Sky-Survey Catalogue (HICAT) and the Wide-field Infrared Survey Explorer (WISE). Of the 3,513 HICAT sources, we find 3.4 micron counterparts for 2,896 sources (80%) and provide new WISE matched aperture photometry for these galaxies. For our principal sample of spiral galaxies with W1 $\le$ 10 mag and z $\le$ 0.01, we identify HI detections for 93% of the sample. We measure lower HI-stellar mass relationships that HI selected samples that do not include spiral galaxies with little HI gas. Our observations of the spiral sample show that HI mass increases with stellar mass with a power-law index 0.35; however, this value is dependent on T-type, which affects both the median and the dispersion of HI mass. We also observe an upper limit on the HI gas fraction, which is consistent with a halo spin parameter model. We measure the star formation efficiency of spiral galaxies to be constant 10$^{-9.57}$ yr$^{-1}$ $\pm$ 0.4 dex for 2.5 orders of magnitude in stellar mass, despite the higher stellar mass spiral showing evidence of quenched star formation. ","Relationships between HI Gas Mass, Stellar Mass and Star Formation Rate
  of HICAT+WISE (HI-WISE) Galaxies"
34,1019342561469755398,373457376,Manuel J Marin-Jimenez,"['Given a depth map containing a person, we propose a new method to estimate his/her 3D body pose. Paper: <LINK>\nVideo with results: <LINK> \n#DeepLearning #IngenieriaInteligente #ComputerVision #HumanPose #GitHub']",https://arxiv.org/abs/1807.05389,"Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predefined 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is defined as a ConvNet that computes the specific weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which respectively depict realistic and synthetic samples, defining a new state-of-the-art on them. ","3D human pose estimation from depth maps using a deep combination of
  poses"
35,1019332223294533632,3011447606,Chris DeGroot 🚲,"['New paper published ""Automatic differentiation of a finite-volume-based transient heat conduction code for sensitivity analysis"". Available from journal site <LINK> and also on arXiv <LINK>']",http://arxiv.org/abs/1807.04410,"A general method for computing derivatives of solution fields and other simulation outputs, with respect to arbitrary input quantities, is proposed. The method of automatic differentiation is used to carry out differentiation and propagate derivatives through the simulation code by chain rule, in forward order. An object-oriented approach using the operator overloading and templating features of the C++ programming language is presented. Verification results are given for a plane wall with surface convection, where the derivative of the dimensionless temperature field with respect to the Biot number is computed and compared to an analytical solution. Further results are given for conduction in a composite material with regions of different thermal conductivity. The derivative of the temperature field is computed with respect to the conductivity of one of the phases using the proposed method. ","Automatic Differentiation of a Finite-Volume-Based Transient Heat
  Conduction Code for Sensitivity Analysis"
36,1019265453372276737,1513989272,Dr. Breanna Binder,['My new paper on my favorite X-ray binary is on arXiv today! SN 2010da has gone and turned itself into an ultraluminous X-ray source: <LINK>'],https://arxiv.org/abs/1807.05309,"We have obtained near-simultaneous Swift/XRT imaging and Gemini GMOS spectroscopy for the ultraluminous X-ray source (ULX) NGC~300 ULX-1 (formerly designated SN~2010da). The observed X-ray emission is consistent with an inhomogeneous wind that partially obscures a central, bright inner accretion disk. We simultaneously fit eleven 0.3-10 keV spectra obtained over a $\sim$1 year time period (2016 April to 2017 July) using the same partial covering model, and find that although the covering fraction varies significantly (from 78% to consistent with 0%), the unabsorbed luminosity remains essentially constant across all observations ($2-6\times10^{39}$ erg s$^{-1}$). A relatively high 0.3-10 keV fractional variability amplitude ($F_{\rm var}$) of $\sim$30% is observed in all eleven observations. Optical spectra from Gemini exhibit numerous emission lines (e.g., H$\alpha$, H$\beta$, He II $\lambda$4686) which suggest that the neutron star primary is photoionizing material in the immediate vicinity of the binary. We compare the He II $\lambda$4686 line luminosity ($\sim7-9\times10^{35}$ erg s$^{-1}$) to the contemporaneous soft X-ray emission and find the X-ray emission is broadly consistent with the observed He II line luminosity. The combination of our X-ray observations and optical spectroscopy suggest that geometric beaming effects in the ULX-1 system are minimal, making ULX-1 one of only a few bona fide ULXs to be powered by accretion onto a neutron star. ","No Strong Geometric Beaming in the Ultraluminous Neutron Star Binary NGC
  300 ULX-1 (SN 2010da) from Swift and Gemini"
37,1019140183000846336,2432886163,Oscar Barragán,['Today in arXiv: our new paper on the mass determination of two transiting hot Jupiters detected by K2.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1807.05865,"We report the discovery from K2 of two transiting hot Jupiter systems. K2-295 (observed in Campaign 8) is a K5 dwarf which hosts a planet slightly smaller than Jupiter, orbiting with a period of 4.0 d. We have made an independent discovery of K2-237 b (Campaign 11), which orbits an F6 dwarf every 2.2 d and has an inflated radius 50 - 60 per cent larger than that of Jupiter. We use high-precision radial velocity measurements, obtained using the HARPS and FIES spectrographs, to measure the planetary masses. We find that K2-295 b has a similar mass to Saturn, while K2-237 b is a little more massive than Jupiter. ",K2-295 b and K2-237 b: two transiting hot Jupiters
38,1019055460035506176,3100596960,Walter Scheirer,"['New paper from my lab coming out of the @DARPA Medifor program: ""Beyond Pixels: Image Provenance Analysis Leveraging Metadata"". A continuation of our work analyzing fake content on the Internet. <LINK>']",https://arxiv.org/abs/1807.03376,"Creative works, whether paintings or memes, follow unique journeys that result in their final form. Understanding these journeys, a process known as ""provenance analysis"", provides rich insights into the use, motivation, and authenticity underlying any given work. The application of this type of study to the expanse of unregulated content on the Internet is what we consider in this paper. Provenance analysis provides a snapshot of the chronology and validity of content as it is uploaded, re-uploaded, and modified over time. Although still in its infancy, automated provenance analysis for online multimedia is already being applied to different types of content. Most current works seek to build provenance graphs based on the shared content between images or videos. This can be a computationally expensive task, especially when considering the vast influx of content that the Internet sees every day. Utilizing non-content-based information, such as timestamps, geotags, and camera IDs can help provide important insights into the path a particular image or video has traveled during its time on the Internet without large computational overhead. This paper tests the scope and applicability of metadata-based inferences for provenance graph construction in two different scenarios: digital image forensics and cultural analytics. ",Beyond Pixels: Image Provenance Analysis Leveraging Metadata
39,1019024654634135552,2720270551,Matheus Gadelha,['Our new ECCV paper on point cloud processing is also on arxiv: <LINK>'],https://arxiv.org/abs/1807.03520,"We present multiresolution tree-structured networks to process point clouds for 3D shape understanding and generation tasks. Our network represents a 3D shape as a set of locality-preserving 1D ordered list of points at multiple resolutions. This allows efficient feed-forward processing through 1D convolutions, coarse-to-fine analysis through a multi-grid architecture, and it leads to faster convergence and small memory footprint during training. The proposed tree-structured encoders can be used to classify shapes and outperform existing point-based architectures on shape classification benchmarks, while tree-structured decoders can be used for generating point clouds directly and they outperform existing approaches for image-to-shape inference tasks learned using the ShapeNet dataset. Our model also allows unsupervised learning of point-cloud based shapes by using a variational autoencoder, leading to higher-quality generated shapes. ",Multiresolution Tree Networks for 3D Point Cloud Processing
40,1018887181702746113,2650692774,Christian Baumgartner,['Check out our new work on scribble-supervised medical image segmentation @dlmia_miccai  <LINK>. Learning from scribbles alone we get less than a 5% performance drop compared to full supervision. Also my first paper as last author! #MICCAI2018 #MachineLearning <LINK>'],https://arxiv.org/abs/1807.04668,Semantic segmentation of medical images is a crucial step for the quantification of healthy anatomy and diseases alike. The majority of the current state-of-the-art segmentation algorithms are based on deep neural networks and rely on large datasets with full pixel-wise annotations. Producing such annotations can often only be done by medical professionals and requires large amounts of valuable time. Training a medical image segmentation network with weak annotations remains a relatively unexplored topic. In this work we investigate training strategies to learn the parameters of a pixel-wise segmentation network from scribble annotations alone. We evaluate the techniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation datasets. We find that the networks trained on scribbles suffer from a remarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate) with respect to a network trained on full annotations. ,Learning to Segment Medical Images with Scribble-Supervision Alone
41,1018857354190680066,704363213245120512,Rahil Garnavi,['Check out our new paper on Glaucoma Detection:\n<LINK>\n\n#AI #DeepLearning #MedicalImaging #Retina #Glaucoma #IBMResearch'],https://arxiv.org/abs/1807.04855,"Optical coherence tomography (OCT) based measurements of retinal layer thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell with inner plexiform layer (GCIPL) are commonly used for the diagnosis and monitoring of glaucoma. Previously, machine learning techniques have utilized segmentation-based imaging features such as the peripapillary RNFL thickness and the cup-to-disc ratio. Here, we propose a deep learning technique that classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network (CNN). We compared the accuracy of this technique with various feature-based machine learning algorithms and demonstrated the superiority of the proposed deep learning based method. Logistic regression was found to be the best performing classical machine learning technique with an AUC of 0.89. In direct comparison, the deep learning approach achieved a substantially higher AUC of 0.94 with the additional advantage of providing insight into which regions of an OCT volume are important for glaucoma detection. Computing Class Activation Maps (CAM), we found that the CNN identified neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and its surrounding areas as the regions significantly associated with the glaucoma classification. These regions anatomically correspond to the well established and commonly used clinical markers for glaucoma diagnosis such as increased cup volume, cup diameter, and neuroretinal rim thinning at the superior and inferior segments. ",A feature agnostic approach for glaucoma detection in OCT volumes
42,1018785860337090560,939772046615236609,Ed Tarleton,"['Preprint of our new paper: Discrete dislocation plasticity HELPs understand hydrogen effects in bcc materials, is now on arXiv, <LINK>']",http://arxiv.org/abs/1807.05101,"In an attempt to bridge the gap between atomistic and continuum plasticity simulations of hydrogen in iron, we present three dimensional discrete dislocation plasticity simulations incorporating the hydrogen elastic stress and a hydrogen dependent dislocation mobility law. The hydrogen induced stress is incorporated following the formulation derived by Gu and El-Awady (2018) which here we extend to a finite boundary value problem, a microcantilever beam, via the superposition principle. The hydrogen dependent mobility law is based on first principle calculations by Katzarov et al. (2017) and was found to promote dislocation generation and enhance slip planarity at a bulk hydrogen concentration of 0.1 appm; which is typical for bcc materials. The hydrogen elastic stress produced the same behaviour, but only when the bulk concentration was extremely high. In a microcantilever, hydrogen was found to promote dislocation activity which lowered the flow stress and generated more pronounced slip steps on the free surfaces. These observations are consistent with the hydrogen enhanced localized plasticity (HELP) mechanism, and it is concluded that both the hydrogen elastic stress and hydrogen increased dislocation mobility are viable explanations for HELP. However it is the latter that dominates at the low concentrations typically found in bcc metals. ","Discrete dislocation plasticity HELPs understand hydrogen effects in bcc
  materials"
43,1017822961099792385,264403483,Jared Sylvester,"['#ICML2018 attendees: check out @EdwardRaffML presenting some of our new work on Fair ML on Sunday\n<LINK>\n<LINK>\n\nIf reading the paper is too much here’s a haiku:\n\nDown the hill we descend,\nClimbing up from proscribed traits,\nUphill, to fair nets. <LINK>']",https://arxiv.org/abs/1807.00392,"No methods currently exist for making arbitrary neural networks fair. In this work we introduce GRAD, a new and simplified method to producing fair neural networks that can be used for auto-encoding fair representations or directly with predictive networks. It is easy to implement and add to existing architectures, has only one (insensitive) hyper-parameter, and provides improved individual and group fairness. We use the flexibility of GRAD to demonstrate multi-attribute protection. ",Gradient Reversal Against Discrimination
44,1017818428290158593,1704344136,David Stansby,['New preprint: <LINK> - we finally put together a short paper about the new Helios proton core data set I produced a while back.'],https://arxiv.org/abs/1807.04376,"In the near future, Parker Solar Probe and Solar Orbiter will provide the first comprehensive in-situ measurements of the solar wind in the inner heliosphere since the Helios mission in the 1970s. We describe a reprocessing of the original Helios ion distribution functions to provide reliable and reproducible data to characterise the proton core population of the solar wind in the inner heliosphere. A systematic fitting of bi-Maxwellian distribution functions was performed to the raw Helios ion distribution function data to extract the proton core number density, velocity, and temperatures parallel and perpendicular to the magnetic field. We present radial trends of these derived proton parameters, forming a benchmark from which new measurements in the inner heliosphere will be compared to. The new dataset has been made openly available for other researchers to use, along with the source code used to generate it. ","A new inner heliosphere proton parameter data set from the Helios
  mission"
45,1017782008771170305,520763766,Dana Greetham,['Our new paper on recurrence quantification analysis for energy data is now available on <LINK> @energySysCat @ANDTechResearch'],https://arxiv.org/abs/1807.02896,"Recurrence Quantification Analysis (RQA) can help to detect significant events and phase transitions of a dynamical system, but choosing a suitable set of parameters is crucial for the success. From recurrence plots different RQA variables can be obtained and analysed. Currently, most of the methods for RQA radius optimisation are focusing on a single RQA variable. In this work we are proposing two new methods for radius optimisation that look for an optimum in the higher dimensional space of the RQA variables, therefore synchronously optimising across several variables. We illustrate our approach using two case studies: a well known Lorenz dynamical system, and a time-series obtained from monitoring energy consumption of a small enterprise. Our case studies show that both methods result in plausible values and can be used to analyse energy data. ","Optimising Parameters in Recurrence Quantification Analysis of Smart
  Energy Systems"
46,1017669783712600064,280083723,Yoh Tanimoto,"['in our new paper <LINK> we prove ""scale =&gt; conformal"" covariance in 2d without using stress-energy tensor~']",http://arxiv.org/abs/1807.04707,"Given a two-dimensional Haag-Kastler net which is Poincar\'e-dilation covariant with additional properties, we prove that it can be extended to a M\""obius covariant net. Additional properties are either a certain condition on modular covariance, or a variant of strong additivity. The proof relies neither on the existence of stress-energy tensor nor any assumption on scaling dimensions. We exhibit some examples of Poincar\'e-dilation covariant net which cannot be extended to a M\""obius covariant net, and discuss the obstructions. ","Scale and M\""obius covariance in two-dimensional Haag-Kastler net"
47,1017624300415799298,2337598033,Geraint F. Lewis,"['New paper on the arXiv, led by PhD student Zhen Wan, with @pythonomer, @dougalmackey, Sanjib Sharma and Rodrigo Ibata <LINK> <LINK>']",https://arxiv.org/abs/1807.03664,"The stars within our Galactic halo presents a snapshot of its ongoing growth and evolution, probing galaxy formation directly. Here, we present our first analysis of the stellar halo from detailed maps of Blue Horizontal Branch (BHB) stars drawn from the SkyMapper Southern Sky Survey. To isolate candidate BHB stars from the overall population, we develop a machine-learning approach through the application of an Artificial Neural Network (ANN), resulting in a relatively pure sample of target stars. From this, we derive the absolute $u$ magnitude for the BHB sample to be $\sim2\ mag$, varying slightly with $(v-g)_0$ and $(u-v)_0$ colours. We examine the BHB number density distribution from 5272 candidate stars, deriving a double power-law with a break radius of $r_s = 11.8\pm0.3\ kpc$, and inner and outer slopes of $\alpha_{in} = -2.5\pm0.1$ and $\alpha_{out} = -4.5\pm0.3$ respectively. Through isochrone fitting of simulated BHB stars, we find a colour-age/metallicity correlation, with older/more metal-poor stars being bluer, and establish a parameter to indicate this age (or metallicity) variation. Using this, we construct the three-dimensional population distribution of BHB stars in the halo and identify significant substructure. Finally, in agreement with previous studies, we also identify a systemic age/metallicity shift spanning $\sim3\ kpc$ to $ \sim20\ kpc$ in galactocentric distance. ","Galactic cartography with SkyMapper: I. Population sub-structure and the
  stellar number density of the inner halo"
48,1017394142736011265,750411947661811712,Dr. Sarah Pearson,"['What might the Magellanic Clouds have evolved into, were they far from the Milky Way? We constrain the initial encounter parameters of an isolated analog of the Clouds, NGC 4490/85, and the timescales involved in gas cycling. Read our new paper here: <LINK> <LINK>']",https://arxiv.org/abs/1807.03791,"Discoveries of low mass galaxy pairs and groups are increasing. Studies indicate that dwarf galaxy pairs are gas rich in the field and exhibit elevated star formation rates, suggestive of interactions. Lacking are dynamical models of observed dwarf galaxy pairs to disentangle the physical processes regulating their baryon cycles. We present new optical data and the first detailed theoretical model of an observed tidal encounter between two isolated low mass galaxies, NGC 4490 & NGC 4485. This system is an isolated analog of the Magellanic Clouds and is surrounded by a ~50 kpc extended HI envelope. We use hybrid $N$-body and test-particle simulations along with a visualization interface $Identikit$ to simultaneously reproduce the observed present-day morphology and kinematics. Our results demonstrate how repeated encounters between two dwarf galaxies can ""park"" baryons at very large distances, without the aid of environmental effects. Our best match to the data is an 8:1 mass ratio encounter where a one-armed spiral is induced in the NGC 4490-analog, which we postulate explains the nature of diffuse starlight presented in the new optical data. We predict that the pair will fully merge in ~370 Myr, but that the extended tidal features will continue to evolve and return to the merged remnant over ~5 Gyr. This pre-processing of baryons will affect the efficiency of gas stripping if such dwarf pairs are accreted by a massive host. In contrast, in isolated environments this study demonstrates how dwarf-dwarf interactions can create a long-lived supply of gas to the merger remnant. ","Modeling the Baryon Cycle in Low Mass Galaxy Encounters: the Case of NGC
  4490 & NGC 4485"
49,1017386172773957632,30845153,Gavin Morley,['We have a new paper on 3D laser-writing of nitrogen vacancy qubits in diamond with long spin coherence time <LINK> @cjsfused @physicsangelo @YashnaLekhai @phildiggle @JasonandthePNG @DiamondCDT'],https://arxiv.org/abs/1807.03643,"Three-dimensional arrays of silicon transistors increase the density of bits. Solid-state qubits are much larger so could benefit even more from using the third dimension given that useful fault-tolerant quantum computing will require at least 100,000 physical qubits and perhaps one billion. Here we use laser writing to create 3D arrays of nitrogen-vacancy centre (NVC) qubits in diamond. This would allow 5 million qubits inside a commercially available 4.5x4.5x0.5 mm diamond based on five nuclear qubits per NVC and allowing $(10 \mu m)^3$ per NVC to leave room for our laser-written electrical control. The spin coherence times we measure are an order of magnitude longer than previous laser-written qubits and at least as long as non-laser-written NVC. As well as NVC quantum computing, quantum communication and nanoscale sensing could benefit from the same platform. Our approach could also be extended to other qubits in diamond and silicon carbide. ","Three-dimensional solid-state qubit arrays with long-lived spin
  coherence"
50,1017072782956351488,40285266,Stanislav Fort at EAGx Prague ¬(🔥📎🔥📎),"[""I will be presenting a part of my new paper <LINK> on using low-dim hypersurfaces to understand neural net loss landscapes at the #ICML2018 workshop on Modern Trends in Nonconvex Optimization for Machine Learning #nonconvex Saturday. Come by if you're around.""]",http://arxiv.org/abs/1807.02581,"We explore the loss landscape of fully-connected and convolutional neural networks using random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian, $H$, of the loss function on these hypersurfaces, we observe 1) an unusual excess of the number of positive eigenvalues of $H$, and 2) a large value of $\mathrm{Tr}(H) / ||H||$ at a well defined range of configuration space radii, corresponding to a thick, hollow, spherical shell we refer to as the \textit{Goldilocks zone}. We observe this effect for fully-connected neural networks over a range of network widths and depths on MNIST and CIFAR-10 datasets with the $\mathrm{ReLU}$ and $\tanh$ non-linearities, and a similar effect for convolutional networks. Using our observations, we demonstrate a close connection between the Goldilocks zone, measures of local convexity/prevalence of positive curvature, and the suitability of a network initialization. We show that the high and stable accuracy reached when optimizing on random, low-dimensional hypersurfaces is directly related to the overlap between the hypersurface and the Goldilocks zone, and as a corollary demonstrate that the notion of intrinsic dimension is initialization-dependent. We note that common initialization techniques initialize neural networks in this particular region of unusually high convexity/prevalence of positive curvature, and offer a geometric intuition for their success. Furthermore, we demonstrate that initializing a neural network at a number of points and selecting for high measures of local convexity such as $\mathrm{Tr}(H) / ||H||$, number of positive eigenvalues of $H$, or low initial loss, leads to statistically significantly faster training on MNIST. Based on our observations, we hypothesize that the Goldilocks zone contains an unusually high density of suitable initialization configurations. ","The Goldilocks zone: Towards better understanding of neural network loss
  landscapes"
51,1016953239424598016,2999702157,Anton Ilderton,['New paper by @ChalmersPhysics @PlymUni @PhysicsatYork researchers on reaching the nonperturbative regime of #strongfieldQED using intense #lasers\n <LINK>'],https://arxiv.org/abs/1807.03730,"It is conjectured that all perturbative approaches to quantum electrodynamics (QED) break down in the collision of a high-energy electron beam with an intense laser, when the laser fields are boosted to `supercritical' strengths far greater than the critical field of QED. As field strengths increase toward this regime, cascades of photon emission and electron-positron pair creation are expected, as well as the onset of substantial radiative corrections. Here we identify the important role played by the collision angle in mitigating energy losses to photon emission that would otherwise prevent the electrons reaching the supercritical regime. We show that a collision between an electron beam with energy in the tens of GeV and a laser pulse of intensity $10^{24}~\text{W}\text{cm}^{-2}$ at oblique, or even normal, incidence is a viable platform for studying the breakdown of perturbative strong-field QED. Our results have implications for the design of near-term experiments as they predict that certain quantum effects are enhanced at oblique incidence. ",Reaching supercritical field strengths with intense lasers
52,1016925260313186304,21902101,Jim Geach,"['New paper out today: <LINK> A magnified view of circumnuclear star formation and feedback around an AGN at z=2.6', 'Remember the source 9io9 discovered in @SpaceWarps? We looked at it with @almaobs and have seen a dense ring of molecular gas with a diameter of 16000 ly, spinning at 350 km/s and forming stars at a rate of over 2000 solar masses per year @chrislintott @ProfBrianCox @daraobriain', ""We also detect emission from the CN molecule, which we think traces dense gas close to the SMBH that has been fragged in the intense UV/X-ray field of the AGN. It's velocity width is much larger than the rotation speed, indicating outflow"", 'But the ring of gas will consume itself in star formation within a few 10s Myr, so it looks like AGN feedback will do little to curtail this current phase of stellar mass growth', ""Here's what the @almaobs data look like https://t.co/5OXeaesH0I""]",https://arxiv.org/abs/1807.03313,"We present Atacama Large Millimeter/submillimeter Array observations of a radio-loud and millimeter-bright galaxy at z=2.6. Gravitational lensing by a foreground galaxy at z~0.2 provides access to physical scales of approximately 360 pc, and we resolve a 2.5 kpc-radius ring of star-forming molecular gas, traced by atomic carbon CI(1-0) and carbon monoxide CO(4-3). We also detect emission from the cyanide radical, CN(4-3). With a velocity width of 680 km/s, this traces dense molecular gas travelling at velocities nearly a factor of two larger than the rotation speed of the molecular ring. While this could indicate the presence of a dynamical and photochemical interaction between the active galactic nucleus and molecular interstellar medium on scales of a few 100 pc, on-going feedback is unlikely to have a significant impact on the assembly of stellar mass in the molecular ring, given the ~10s Myr depletion timescale due to star formation. ","A magnified view of circumnuclear star formation and feedback around an
  AGN at z=2.6"
53,1016921267419713536,17121984,Max Bannach,"['We have a new paper: ""Computing Kernels in Parallel: Lower and Upper Bounds"" <LINK> It will be presented at IPEC 2018.']",https://arxiv.org/abs/1807.03604,"Parallel fixed-parameter tractability studies how parameterized problems can be solved in parallel. A surprisingly large number of parameterized problems admit a high level of parallelization, but this does not mean that we can also efficiently compute small problem kernels in parallel: known kernelization algorithms are typically highly sequential. In the present paper, we establish a number of upper and lower bounds concerning the sizes of kernels that can be computed in parallel. An intriguing finding is that there are complex trade-offs between kernel size and the depth of the circuits needed to compute them: For the vertex cover problem, an exponential kernel can be computed by AC$^0$-circuits, a quadratic kernel by TC$^0$-circuits, and a linear kernel by randomized NC-circuits with derandomization being possible only if it is also possible for the matching problem. Other natural problems for which similar (but quantitatively different) effects can be observed include tree decomposition problems parameterized by the vertex cover number, the undirected feedback vertex set problem, the matching problem, or the point line cover problem. We also present natural problems for which computing kernels is inherently sequential. ",Computing Kernels in Parallel: Lower and Upper Bounds
54,1016862363981475840,383142451,Shinnosuke Takamichi (高道 慎之介),"['Our new paper is out! von-Mises-distribution DNNs are efficiently applied to model phases and group delay of a speech signal. \n\n<LINK> <LINK>', '@fakufakurevenge @hsaruwatari727 今はありませんが、後日用意します！']",https://arxiv.org/abs/1807.03474,"This paper presents a deep neural network (DNN)-based phase reconstruction from amplitude spectrograms. In audio signal and speech processing, the amplitude spectrogram is often used for processing, and the corresponding phase spectrogram is reconstructed from the amplitude spectrogram on the basis of the Griffin-Lim method. However, the Griffin-Lim method causes unnatural artifacts in synthetic speech. Addressing this problem, we introduce the von-Mises-distribution DNN for phase reconstruction. The DNN is a generative model having the von Mises distribution that can model distributions of a periodic variable such as a phase, and the model parameters of the DNN are estimated on the basis of the maximum likelihood criterion. Furthermore, we propose a group-delay loss for DNN training to make the predicted group delay close to a natural group delay. The experimental results demonstrate that 1) the trained DNN can predict group delay accurately more than phases themselves, and 2) our phase reconstruction methods achieve better speech quality than the conventional Griffin-Lim method. ","Phase reconstruction from amplitude spectrograms based on
  von-Mises-distribution deep neural network"
55,1016732820763529217,3716338821,Mikko Tuomi,"['We believe there are three planets orbiting #LHS1140 based on a careful bias-minimising analysis of HARPS radial velocity data.\n\nOur new submitted paper: ""Minimizing the bias in exoplanet detection - application to radial  velocities of LHS 1140"" <LINK> <LINK>']",https://arxiv.org/abs/1807.02483,"A rocky planet orbiting LHS 1140 with a period of 24.7d has been found based on the discovery of transits in its light and high precision radial velocity data (Dittmann et al. 2017). This discovery by two independent methods is an observational tour-de-force, however, we find that a conservative analysis of the data gives a different solution. A three planet system is apparent in the radial velocity data based on our diagnosis of stellar activity. We encourage further targeted photometric and radial velocity observations in order to constrain the mini-Neptune and super-Earth mass objects apparently causing the 3.8 and 90 day radial velocity signals. We use our package Agatha (this https URL) to provide a comprehensive strategy to disentangle planetary signals from stellar activity in radial velocity data. ","Minimizing the bias in exoplanet detection - application to radial
  velocities of LHS 1140"
56,1016676725172899841,2444384845,Dr Viviane Pons,['A new paper of mine on the arXiv! Feels nice :) :) <LINK>'],https://arxiv.org/abs/1807.03277,"We construct a Hopf algebra on integer binary relations that contains under the same roof several well-known Hopf algebras related to the permutahedra and the associahedra: the Malvenuto-Reutenauer algebra on permutations, the Loday-Ronco algebra on planar binary trees, and the Chapoton algebras on ordered partitions and on Schr\""oder trees. We also derive from our construction new Hopf structures on intervals of the weak order on permutations and of the Tamari order on binary trees. ",The Hopf algebra of integer binary relations
57,1016672188412366850,4662190697,André Nicolet,['How to perform a modal expansion in electrodynamics with dispersive materials when the permittivity depends on the very resonant frequencies you try to compute and your operators are not even self-adjoint?\n\nSome answers in our new paper: <LINK> <LINK>'],http://arxiv.org/abs/1807.02355,"We present exact modal expansions for photonic systems including highly dispersive media. The formulas, based on a simple version of the Keldysh theorem, are very general since both permeability and permittivity can be dispersive, anisotropic, and even possibly non reciprocal. A simple dispersive test case where both plasmonic and geometrical resonances strongly interact exemplifies the numerical efficiency of our approach. ",Photonics in highly dispersive media: The exact modal expansion
58,1016650321110630400,19238958,Daniel S. Katz,"['two new co-authored pre-prints: \nHEP Software Foundation Community White Paper Working Group - Training, Staffing and Careers (w lots of people) - <LINK>\n\nMapping the research software sustainability space (w @stdruskat) - <LINK>']",https://arxiv.org/abs/1807.02875,"The rapid evolution of technology and the parallel increasing complexity of algorithmic analysis in HEP requires developers to acquire a much larger portfolio of programming skills. Young researchers graduating from universities worldwide currently do not receive adequate preparation in the very diverse fields of modern computing to respond to growing needs of the most advanced experimental challenges. There is a growing consensus in the HEP community on the need for training programmes to bring researchers up to date with new software technologies, in particular in the domains of concurrent programming and artificial intelligence. We review some of the initiatives under way for introducing new training programmes and highlight some of the issues that need to be taken into account for these to be successful. ","HEP Software Foundation Community White Paper Working Group - Training,
  Staffing and Careers"
59,1016587153038508032,780751772717502464,Dr Claire Burke,['New astro-eco paper is online for reads! We discuss some of the challenges with using thermal for tracking poachers and animals. Paper 2 coming soon with solutions.\n<LINK>'],https://arxiv.org/abs/1807.03157,"Using thermal infrared detectors mounted on drones, and applying techniques from astrophysics, we hope to support the field of conservation ecology by creating an automated pipeline for the detection and identification of certain endangered species and poachers from thermal infrared data. We test part of our system by attempting to detect simulated poachers in the field. Whilst we find that we can detect humans hiding in the field in some types of terrain, we also find several environmental factors that prevent accurate detection, such as ambient heat from the ground, absorption of infrared emission by the atmosphere, obscuring vegetation and spurious sources from the terrain. We discuss the effect of these issues, and potential solutions which will be required for our future vision for a fully automated drone-based global conservation monitoring system. ","Addressing environmental and atmospheric challenges for capturing
  high-precision thermal infrared data in the field of astro-ecology"
60,1016584943147855872,2337598033,Geraint F. Lewis,"['New paper on the arXiv with PhD student Eromanga Adermann, Pascal Elahi and @doctorcbpower <LINK> <LINK>']",http://arxiv.org/abs/1807.02938,"We compare the evolution of voids formed under the standard cosmological model and two alternative cosmological models. The two models are a quintessence model ($\phi$CDM) and a Coupled Dark Matter-Dark Energy (CDE) model, both of which have evolving and interacting dark sectors. From $N$-body adiabatic hydrodynamical simulations of these models, we measure the statistics and quantify the properties of voids over the redshift range $z=1.5-12$: these include their population size, volumes, shapes and average densities. We find that the latter property has potential as a probe of cosmology, particularly dark energy, as significant differences in average void densities exist between the alternative models and the standard model. We postulate that this signature arises from an increased evacuation rate of particles out of voids, or an earlier start to void evacuation, in the alternative models as a direct consequence of the dynamical scalar field, which also leads to greater void merger rates. Additionally, differences between the two alternative models are likely due to the drag force arising from dark sector coupling, acting on dark matter particles in our coupled model. ","Cosmic Voids in Evolving Dark Sector Cosmologies: the High Redshift
  Universe"
61,1016567562975444992,39688015,Chris Usher,['I have a new paper out with the E-MOSAICS team (inc @rcrain_astro) on the origin of the blue tilt <LINK>'],https://arxiv.org/abs/1807.03084,"The metal-poor sub-population of globular cluster (GC) systems exhibits a correlation between the GC average colour and luminosity, especially in those systems associated with massive elliptical galaxies. More luminous (more massive) GCs are typically redder and hence more metal-rich. This 'blue tilt' is often interpreted as a mass-metallicity relation stemming from GC self-enrichment, whereby more massive GCs retain a greater fraction of the enriched gas ejected by their evolving stars, fostering the formation of more metal-rich secondary generations. We examine the E-MOSAICS simulations of the formation and evolution of galaxies and their GC populations, and find that their GCs exhibit a colour-luminosity relation similar to that observed in local galaxies, without the need to invoke mass-dependent self-enrichment. We find that the blue tilt is most appropriately interpreted as a dearth of massive, metal-poor GCs: the formation of massive GCs requires high interstellar gas surface densities, conditions that are most commonly fostered by the most massive, and hence most metal rich, galaxies, at the peak epoch of GC formation. The blue tilt is therefore a consequence of the intimate coupling between the small-scale physics of GC formation and the evolving properties of interstellar gas hosted by hierarchically-assembling galaxies. ","The origin of the 'blue tilt' of globular cluster populations in the
  E-MOSAICS simulations"
62,1016360787244388354,322636963,Jonathan Berant,"['Check out our new paper authored by @crazydonkey200 with collaborators on improving policy optimization with a per-example memory buffer, leading to a new state-of-the-art on WikiTableQuestions and great results on WikiSQL with weak supervision. <LINK> #taunlp']",https://arxiv.org/abs/1807.02322,"We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at this https URL ","Memory Augmented Policy Optimization for Program Synthesis and Semantic
  Parsing"
63,1016347656069603328,888216099757490176,Maithra Raghu,"['1/4 Research at the intersection of #MachineLearning, #DeepLearning and #healthcare has been a longstanding interest, so very excited about our new paper:\n\nPaper: <LINK>\nBlogpost: <LINK>\n\nWith @oziadias @m_sendhil Jon Kleinberg #GoogleBrain', '2/4 Doctors can often disagree significantly on a patient diagnosis, which is a serious challenge in applying ML to health. We study the effectiveness of using machine learning to 𝘱𝘳𝘦𝘥𝘪𝘤𝘵 cases that will be the source of highest disagreement. https://t.co/VkGYEjgaV6', 'This is done in the context of diagnosing Diabetic Retinopathy (DR) from retinal (fundus) images. We find 𝘥𝘪𝘳𝘦𝘤𝘵 𝘶𝘯𝘤𝘦𝘳𝘵𝘢𝘪𝘯𝘵𝘺 𝘱𝘳𝘦𝘥𝘪𝘤𝘵𝘪𝘰𝘯: predicting an agree/disagree target for each image, works substantially better than uncertainty via classification. https://t.co/UJFW51OBUl', '4/4 We can use this trained classifier to not only accurately identify images with high disagreement in a held out set, but also highlight cases where an 𝘢𝘥𝘫𝘶𝘥𝘪𝘤𝘢𝘵𝘪𝘰𝘯 labeling process is most helpful and suggest label allocation schemes. Check out the paper for more!']",https://arxiv.org/abs/1807.01771,"The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be trained to give uncertainty scores to data instances that might result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two-step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application. ",Direct Uncertainty Prediction for Medical Second Opinions
64,1016321406550921217,223923566,David Van Horn,['New paper: Gradual Liquid Type Inference with @nikivazou and Éric Tanter.  To appear at OOPSLA 2018 (after final changes). <LINK> <LINK>'],https://arxiv.org/abs/1807.02132,"Liquid typing provides a decidable refinement inference mechanism that is convenient but subject to two major issues: (1) inference is global and requires top-level annotations, making it unsuitable for inference of modular code components and prohibiting its applicability to library code, and (2) inference failure results in obscure error messages. These difficulties seriously hamper the migration of existing code to use refinements. This paper shows that gradual liquid type inference---a novel combination of liquid inference and gradual refinement types---addresses both issues. Gradual refinement types, which support imprecise predicates that are optimistically interpreted, can be used in argument positions to constrain liquid inference so that the global inference process e effectively infers modular specifications usable for library components. Dually, when gradual refinements appear as the result of inference, they signal an inconsistency in the use of static refinements. Because liquid refinements are drawn from a nite set of predicates, in gradual liquid type inference we can enumerate the safe concretizations of each imprecise refinement, i.e. the static refinements that justify why a program is gradually well-typed. This enumeration is useful for static liquid type error explanation, since the safe concretizations exhibit all the potential inconsistencies that lead to static type errors. We develop the theory of gradual liquid type inference and explore its pragmatics in the setting of Liquid Haskell. ",Gradual Liquid Type Inference
65,1016228598875918336,20444488,Seth Moortgat,"['Proud to announce our new paper on ArXiv:\n<LINK>\nA study on how to pinpoint the origin of an #EFT signal at the LHC, in the broad #SMEFT landscape, using multi-class #NeuralNetworks. With the ttbb signature in a lead role as a very interesting case-study! <LINK>']",https://arxiv.org/abs/1807.02130,"In the context of the Standard Model effective field theory (SMEFT), we study the LHC sensitivity to four fermion operators involving heavy quarks by employing cross section measurements in the $t\bar{t}b\bar{b}$ final state. Starting from the measurement of total rates, we progressively exploit kinematical information and machine learning techniques to optimize the projected sensitivity at the end of Run III. Indeed, in final states with high multiplicity containing inter-correlated kinematical information, multi-variate methods provide a robust way of isolating the regions of phase space where the SMEFT contribution is enhanced. We also show that training for multiple output classes allows for the discrimination between operators mediating the production of tops in different helicity states. Our projected sensitivities not only constrain a host of new directions in the SMEFT parameter space but also improve on existing limits demonstrating that, on one hand, $t\bar{t}b\bar{b}$ production is an indispensable component in a future global fit for top quark interactions in the SMEFT, and on the other, multi-class machine learning algorithms can be a valuable tool for interpreting LHC data in this framework. ","Learning to pinpoint effective operators at the LHC: a study of the
  $t\bar{t}b\bar{b}$ signature"
66,1015209743554629632,2403146034,Sergio Hernández Cerezo,"['Wanted to know about ""Fractal AI"" but not reading a 40+ pages doc? New short paper by @spirosbax @Miau_DB and myself going just to the point (fixed link):\n\n ""Solving Atari Games Using Fractals And Entropy""\n<LINK>']",https://arxiv.org/abs/1807.01081,"In this paper, we introduce a novel MCTS based approach that is derived from the laws of the thermodynamics. The algorithm coined Fractal Monte Carlo (FMC), allows us to create an agent that takes intelligent actions in both continuous and discrete environments while providing control over every aspect of the agent behavior. Results show that FMC is several orders of magnitude more efficient than similar techniques, such as MCTS, in the Atari games tested. ",Solving Atari Games Using Fractals And Entropy
67,1015155511304622081,2676457430,MAGIC telescopes 🌴🌺,['a new MAGIC paper has been accepted in MNRAS!the authors investigate the nature of the flare which lead to the  discovery of very-high-energy gamma-rays from the source S2 0109+22. Check it out! <LINK>\n#GammaRays #iact #AGN #VHE <LINK>'],https://arxiv.org/abs/1807.02095v1,"The MAGIC telescopes observed S2 0109+22 in 2015 July during its flaring activity in high energy gamma rays observed by Fermi-LAT. We analyse the MAGIC data to characterise the very high energy (VHE) gamma-ray emission of S2 0109+22, which belongs to the subclass of intermediate synchrotron peak (ISP) BL Lac objects. We study the multi-frequency emission in order to investigate the source classification. Finally, we compare the source long-term behaviour to other VHE gamma-ray emitting (TeV) blazars. We performed a temporal and spectral analysis of the data centred around the MAGIC interval of observation (MJD 57225-57231). Long-term radio and optical data have also been investigated using the discrete correlation function. The redshift of the source is estimated through optical host-galaxy imaging and also using the amount of VHE gamma-ray absorption. The quasi-simultaneous multi-frequency spectral energy distribution (SED) is modelled with the conventional one-zone synchrotron self-Compton (SSC) model. MAGIC observations resulted in the detection of the source at a significance level of $5.3\,\sigma$. The VHE gamma-ray emission of S2 0109+22 is variable on a daily time scale. VHE gamma-ray luminosity of the source is lower than the average of TeV BL Lacs. The optical polarization, and long-term optical/radio behaviour of the source are different from the general population of TeV blazars. All these findings agree with the classification of the source as an ISP BL Lac object. We estimate the source redshift as $z = 0.36 \pm 0.07$. The SSC parameters describing the SED are rather typical for blazars. ","] The broad-band properties of the intermediate synchrotron peaked BL Lac
  S2 0109+22 from radio to VHE gamma rays"
68,1015122084077473792,70614241,Dr Fiona H. Panther,['My new paper is now on the arXiv: positrons interacting with alkali metals in the ISM\n<LINK>'],https://arxiv.org/abs/1807.01880,"In the Milky Way galaxy, positrons, which are responsible for the diffuse $511\,\mathrm{keV}$ gamma ray emission observed by space-based gamma ray observatories, are thought to annihilate predominantly through charge exchange interactions with neutral hydrogen. These charge exchange interactions can only take place if positrons have energies greater than $6.8\,\mathrm{eV}$, the minimum energy required to liberate the electron bound to the hydrogen atom and then form positronium, a short-lived bound state composed of a positron-electron pair. Here we demonstrate the importance of positron interactions with neutral alkali metals in the warm interstellar medium (ISM). Positrons may undergo charge exchange with these atoms at any energy. In particular, we show that including positron interactions with sodium at solar abundance in the warm ISM can significantly reduce the annihilation timescale of positrons with energies below $6.8\,\mathrm{eV}$ by at least an order of magnitude. We show that including these interactions in our understanding of positron annihilation in the Milky Way rules out the idea that the number of positrons in the Galactic ISM could be maintained in steady state by injection events occurring at a typical periodicity $>\mathrm{Myr}$. ",The effect of positron-alkali metal atom interactions in the diffuse ISM
69,1014891929794797568,39525395,Aditya Grover,"['Amidst all the interest in compressed sensing using deep generative models, our new paper shows that sparsity assumptions can co-exist &amp; aid generative models. Long oral at @icmlconf next week! w/ Manik Dhar &amp; @ermonste Paper: <LINK> Code: <LINK> <LINK>']",https://arxiv.org/abs/1807.01442,"In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain. ","Modeling Sparse Deviations for Compressed Sensing using Generative
  Models"
70,1014875131842359298,252867237,Juan Miguel Arrazola,"[""New paper by @XanaduAI : Gaussian Boson Sampling using threshold detectors <LINK> \nIt's a special paper because (i) it's my first collaboration with my close friend @corsairenard and (ii) it's the first appearance of the Torontonian <LINK>"", ""@quantum_neville @XanaduAI @corsairenard Yes! I do try to have fun with my research, otherwise what's the point of doing it for a living ;-)"", '@quantumVerd @XanaduAI @corsairenard ON states where a coincidence, but yes, the Torontonian is an homage to this great city that we call home.']",https://arxiv.org/abs/1807.01639,"We study what is arguably the most experimentally appealing Boson Sampling architecture: Gaussian states sampled with threshold detectors. We show that in this setting, the probability of observing a given outcome is related to a matrix function that we name the Torontonian, which plays an analogous role to the permanent or the Hafnian in other models. We also prove that, provided that the probability of observing two or more photons in a single output mode is sufficiently small, our model remains intractable to simulate classically under standard complexity-theoretic conjectures. Finally, we leverage the mathematical simplicity of the model to introduce a physically motivated, exact sampling algorithm for all Boson Sampling models that employ Gaussian states and threshold detectors. ",Gaussian Boson Sampling using threshold detectors
71,1014858072836190210,52381876,Edward Frenkel,['New paper on the quantum Langlands duality\n#loveandmath\n<LINK> <LINK>'],https://arxiv.org/abs/1807.01536,We prove duality isomorphisms of certain representations of W-algebras which play an essential role in the quantum geometric Langlands Program and some related results. ,Quantum Langlands duality of representations of W-algebras
72,1014821110939029504,179893276,Sinan Kalkan,"['Our paper on a new performance metric for object detection (<LINK> | <LINK>) has been accepted to #ECCV2018. Congratulations Kemal, Baris, Emre @metu_imagelab.']",https://arxiv.org/abs/1807.01696,"Average precision (AP), the area under the recall-precision (RP) curve, is the standard performance measure for object detection. Despite its wide acceptance, it has a number of shortcomings, the most important of which are (i) the inability to distinguish very different RP curves, and (ii) the lack of directly measuring bounding box localization accuracy. In this paper, we propose 'Localization Recall Precision (LRP) Error', a new metric which we specifically designed for object detection. LRP Error is composed of three components related to localization, false negative (FN) rate and false positive (FP) rate. Based on LRP, we introduce the 'Optimal LRP', the minimum achievable LRP error representing the best achievable configuration of the detector in terms of recall-precision and the tightness of the boxes. In contrast to AP, which considers precisions over the entire recall domain, Optimal LRP determines the 'best' confidence score threshold for a class, which balances the trade-off between localization and recall-precision. In our experiments, we show that, for state-of-the-art object (SOTA) detectors, Optimal LRP provides richer and more discriminative information than AP. We also demonstrate that the best confidence score thresholds vary significantly among classes and detectors. Moreover, we present LRP results of a simple online video object detector which uses a SOTA still image object detector and show that the class-specific optimized thresholds increase the accuracy against the common approach of using a general threshold for all classes. At this https URL we provide the source code that can compute LRP for the PASCAL VOC and MSCOCO datasets. Our source code can easily be adapted to other datasets as well. ","Localization Recall Precision (LRP): A New Performance Metric for Object
  Detection"
73,1014782674374987776,718195928335822853,Alexander Buchholz,"['Our new paper on Quasi-Monte Carlo and Variational Inference (#icml 18, forthcoming) is now on arxiv: <LINK>']",https://arxiv.org/abs/1807.01604,"Many machine learning problems involve Monte Carlo gradient estimators. As a prominent example, we focus on Monte Carlo variational inference (MCVI) in this paper. The performance of MCVI crucially depends on the variance of its stochastic gradients. We propose variance reduction by means of Quasi-Monte Carlo (QMC) sampling. QMC replaces N i.i.d. samples from a uniform probability distribution by a deterministic sequence of samples of length N. This sequence covers the underlying random variable space more evenly than i.i.d. draws, reducing the variance of the gradient estimator. With our novel approach, both the score function and the reparameterization gradient estimators lead to much faster convergence. We also propose a new algorithm for Monte Carlo objectives, where we operate with a constant learning rate and increase the number of QMC samples per iteration. We prove that this way, our algorithm can converge asymptotically at a faster rate than SGD. We furthermore provide theoretical guarantees on QMC for Monte Carlo objectives that go beyond MCVI, and support our findings by several experiments on large-scale data sets from various domains. ",Quasi-Monte Carlo Variational Inference
74,1014691455422455808,726837554000084993,Jeremy Bailin,"['New paper: A Model for Clumpy Self-Enrichment in Globular Clusters.\n<LINK>\nUltrasummary: If GCs formed with subclumps like in observed + simulated star formation regions, supernova enrichment between clumps results in observed metallicity spreads within clusters.', 'Code is at https://t.co/gzKugNzMe4']",https://arxiv.org/abs/1807.01447,"Detailed observations of globular clusters (GCs) have revealed evidence of self-enrichment: some of the heavy elements that we see in stars today were produced by cluster stars themselves. Moreover, GCs have internal subpopulations with different elemental abundances, including, in some cases, in elements such as iron that are produced by supernovae. This paper presents a theoretical model for GC formation motivated by observations of Milky Way star forming regions and simulations of star formation, where giant molecular clouds fragment into multiple clumps which undergo star formation at slightly different times. Core collapse supernovae from earlier-forming clumps can enrich later-forming clumps to the degree that the ejecta can be retained within the gravitational potential well, resulting in subpopulations with different total metallicities once the clumps merge to form the final cluster. The model matches the mass-metallicity relation seen in GC populations around massive elliptical galaxies, and predicts metallicity spreads within clusters in excellent agreement with those seen in Milky Way GCs, even for those whose internal abundance spreads are so large that their entire identity as a GC is in question. The internal metallicity spread serves as an excellent measurement of how much self-enrichment has occurred in a cluster, a result that is very robust to variation in the model parameters. ",A Model for Clumpy Self-Enrichment in Globular Clusters
75,1014681462082174978,811646204635287552,Jacob Buckman,"['New paper: STEVE, a hybrid model-based/model-free RL algorithm that outperforms pure model-free, even in complex environments! <LINK> w/ @danijarh @georgejtucker @yablak @honglaklee']",http://arxiv.org/abs/1807.01675,"Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments. ","Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value
  Expansion"
76,1014593184695975937,811644390963748864,Mark Mace,"['Our (@vskokov + others) new paper on correlations in proton-lead collisions at the @CERN LHC is out! From a simple CGC power-counting argument, we make theory to @ATLASexperiment-data comparisons, w/ numerics to check this! See: <LINK> @BrookhavenLab @stonybrooku']",https://arxiv.org/abs/1807.00825,"Simple power counting arguments in the dilute-dense framework of the Color Glass Condensate (CGC) Effective Field Theory predict that even and odd azimuthal anisotropy harmonics of two-particle correlations in proton-nucleus collisions at the LHC will respectively satisfy v^2_{2n} ~ N_{ch}^0 and v^2_{2n+1} ~ N_{ch}, where N_{ch} denotes the number of charged particles. We show that these expectations are borne out qualitatively, and even quantitatively, within systematic uncertainties, for v_2 and v_4 in comparisons with data from the ATLAS collaboration. We also observe that ATLAS data for the v_3 azimuthal harmonic are in excellent agreement with our qualitative expectation; quantitative comparisons are numerically challenging at present. The lessons from this study fully complement those gained by the recent comparison of the CGC dilute-dense framework [arXiv:1805.09342] to data from the PHENIX collaboration on small system collisions at RHIC. ","Systematics of azimuthal anisotropy harmonics in proton-nucleus
  collisions at the LHC from the Color Glass Condensate"
77,1014436369735745537,2676457430,MAGIC telescopes 🌴🌺,['A new MAGIC paper has been accepted on A&amp;A! This work describes an impressive outburst of the galaxy S5 0716+714  which was registered by many instruments simultaneously. check it out! <LINK> <LINK>'],http://arxiv.org/abs/1807.00413,"The BL Lac object S5~0716+714, a highly variable blazar, underwent an impressive outburst in January 2015 (Phase A), followed by minor activity in February (Phase B). The MAGIC observations were triggered by the optical flux observed in Phase A, corresponding to the brightest ever reported state of the source in the R-band. The comprehensive dataset collected is investigated in order to shed light on the mechanism of the broadband emission. Multi-wavelength light curves have been studied together with the broadband Spectral Energy Distributions (SEDs). The data set collected spans from radio, optical photometry and polarimetry, X-ray, high-energy (HE, 0.1 GeV < E < 100 GeV) with \textit{Fermi}-LAT to the very-high-energy (VHE, E>100 GeV) with MAGIC. The flaring state of Phase A was detected in all the energy bands, providing for the first time a multi-wavelength sample of simultaneous data from the radio band to the VHE. In the constructed SED the \textit{Swift}-XRT+\textit{NuSTAR} data constrain the transition between the synchrotron and inverse Compton components very accurately, while the second peak is constrained from 0.1~GeV to 600~GeV by \textit{Fermi}+MAGIC data. The broadband SED cannot be described with a one-zone synchrotron self-Compton model as it severely underestimates the optical flux in order to reproduce the X-ray to $\gamma$-ray data. Instead we use a two-zone model. The EVPA shows an unprecedented fast rotation. An estimation of the redshift of the source by combined HE and VHE data provides a value of $z = 0.31 \pm 0.02_{stats} \pm 0.05_{sys}$, confirming the literature value. The data show the VHE emission originating in the entrance and exit of a superluminal knot in and out a recollimation shock in the inner jet. A shock-shock interaction in the jet seems responsible for the observed flares and EVPA swing. This scenario is also consistent with the SED modelling. ","Multi-wavelength characterization of the blazar S5~0716+714 during an
  unprecedented outburst phase"
78,1014430198098485248,1523174335,Kolja Kleineberg,"[""As a bonus for today's new paper (<LINK>) I have created an infographic about geometric correlations in multiplex networks. @net_science @FuturICT @cxdig <LINK>""]",https://arxiv.org/abs/1807.01190,"Recent progress towards unraveling the hidden geometric organization of real multiplexes revealed significant correlations across the hyperbolic node coordinates in different network layers, which facilitated applications like trans-layer link prediction and mutual navigation. But are geometric correlations alone sufficient to explain the topological relation between the layers of real systems? Here we provide the negative answer to this question. We show that connections in real systems tend to persist from one layer to another irrespectively of their hyperbolic distances. This suggests that in addition to purely geometric aspects the explicit link formation process in one layer impacts the topology of other layers. Based on this finding, we present a simple modification to the recently developed Geometric Multiplex Model to account for this effect, and show that the extended model can reproduce the behavior observed in real systems. We also find that link persistence is significant in all considered multiplexes and can explain their layers' high edge overlap, which cannot be explained by coordinate correlations alone. Furthermore, by taking both link persistence and hyperbolic distance correlations into account we can improve trans-layer link prediction. These findings guide the development of multiplex embedding methods, suggesting that such methods should be accounting for both coordinate correlations and link persistence across layers. ",Link persistence and conditional distances in multiplex networks
79,1014425622985232384,1523174335,Kolja Kleineberg,"['New paper with Fragkiskos Papadopoulos explains overlap in multiplex networks by persistent links and network geometry, with applications to link prediction. See ""Link persistence and conditional distances in multiplex networks"" at <LINK> @net_science @FuturICT <LINK>']",https://arxiv.org/abs/1807.01190,"Recent progress towards unraveling the hidden geometric organization of real multiplexes revealed significant correlations across the hyperbolic node coordinates in different network layers, which facilitated applications like trans-layer link prediction and mutual navigation. But are geometric correlations alone sufficient to explain the topological relation between the layers of real systems? Here we provide the negative answer to this question. We show that connections in real systems tend to persist from one layer to another irrespectively of their hyperbolic distances. This suggests that in addition to purely geometric aspects the explicit link formation process in one layer impacts the topology of other layers. Based on this finding, we present a simple modification to the recently developed Geometric Multiplex Model to account for this effect, and show that the extended model can reproduce the behavior observed in real systems. We also find that link persistence is significant in all considered multiplexes and can explain their layers' high edge overlap, which cannot be explained by coordinate correlations alone. Furthermore, by taking both link persistence and hyperbolic distance correlations into account we can improve trans-layer link prediction. These findings guide the development of multiplex embedding methods, suggesting that such methods should be accounting for both coordinate correlations and link persistence across layers. ",Link persistence and conditional distances in multiplex networks
80,1014348384713375744,1616352942,Volodymyr Kuleshov 🇺🇦,"['New paper with @ermonste on accurate uncertainties for Bayesian deep learning. Addresses model overconfidence which arises from misspecification and computational approximations. Will be presented at @icmlconf next week! <LINK> <LINK>', ""@ogrisel @ermonste @icmlconf Yes, that's in the works!""]",https://arxiv.org/abs/1807.00263,"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks. ",Accurate Uncertainties for Deep Learning Using Calibrated Regression
81,1014303489806209024,321794593,José G. Fernández-Trincado,['Check out our new paper:  \nA peculiar interacting Be star binary in the Small Magellanic Cloud —<LINK>'],https://arxiv.org/abs/1807.00907,"We find that the emission line object OGLEJ005039.05-725751.4, a member of the cluster OGLE-CL SMC 64, exhibits a peculiar light curve pattern repeating with a recurrence time of 141.45 days. The light curve resembles periodic outbursts with a duty cycle of 20%. A second long-cycle of 2500 days is also detected in the photometric dataset. Two X-SHOOTER spectra obtained at minimum and maximum reveal a Be star dominating at minimum light resembling the Classical Be star 48 Lib. The larger H$\alpha$ emission, the stronger NaD absorption and the appearance of emission in the infrared Ca II triplet at maximum, might indicate periodic mass transfer in a complex binary system. ",A peculiar interacting Be star binary in the Small Magellanic Cloud
82,1014101054156431360,839820726777544705,Alexia Jolicoeur-Martineau,"['My new paper is out! "" The relativistic discriminator: a key element missing from standard GAN"" explains how most GANs are missing a key ingredient which makes them so much better and much more stable! #Deeplearning #AI\n\n<LINK>\n<LINK>', 'You might be interested in this @goodfellow_ian, you could use this to most likely get more stability and lower FID than HingeGAN on ImageNet!', '@goodfellow_ian  https://t.co/eZx5uSo0ME', ""@goodfellow_ian Yes, actually the typo is on equation 17. It's written correctly on p15, it's actually -log(D(x))-log(1-D(x)). The discriminator is properly defined in equation 18."", ""@goodfellow_ian Well rethinking about it, I don't think it change at all. The discriminator still maximize the loss as usual, leading to the optimal discriminator as you describe in your GAN paper. The difference is now that the generator actually minimize it more properly (needs proof)."", ""@goodfellow_ian It's the minimization part, that should be looked at. It's all intuition but I bet its more like a proper divergence minimization since we minimize both parts of the loss as I mention on  p6. If we assume an optimal D, we should check what it looks like."", '@goodfellow_ian With saturating loss, D optimal = JSD and G optimal = min JSD so we expect D(x)=1/2 for all x which we cannot do because we actually strive for D(x_f)=1, D(x_r)=0 (or D(x_r=1) w/ SGAN). But with non-saturating loss, G optimal would expect D(x_f)=1, D(x_r)=0 which is what we get.', ""@andrey_kurenkov @gradientpub Sure that's a great idea! You can reach me at alexia.jolicoeur-martineau@mail.mcgill.ca. It could be made more extensive than the blog while as simple as possible (compared to the paper).""]",https://arxiv.org/abs/1807.00734,"In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization. ",The relativistic discriminator: a key element missing from standard GAN
83,1014039642755272704,192826908,Jorge Lillo-Box,"['A new TROY paper is out today! ""Multi-technique constraints on exotrojans in nine planetary systems"". A brief summary follows in this 5-tweets thread. But, check the paper for a full overview and some nice figures ;).  <LINK> <LINK>', '1/5: We have looked for co-orbitals in 9 planetary systems that showed signs of mass imbalance between their two Lagrangian points as found by our first TROY paper (https://t.co/O81lnw3IWQ) https://t.co/M9Jg5Oumgl', '2/5: We have observed the Lagrangian point passage in front of these stars with precision photometry looking for possible trojan transits and combining this information with new RVs, TTVs, and a dynamical analysis to constrain the parameter space for trojans in these systems. https://t.co/r48AfwUHd1', '3/5: We combined new data from CAFOS and @CARMENES_exopl  (at @CalarAltoObs), HARPS (La Silla, @ESO), WiFSIP (STELLA), and archival data from many other facilities to constrain the trojan mass vs. libration amplitude parameter space https://t.co/IOqi6yGhr5', '4/5:No co-orbitals were found in the new light curves. BUT! We can constrain the presence of trojans in these close-in planets down to the super-Earth regime. This has implications in the theories of inward migration of co-orbital planets. No big trojans in close-in planets. Why?', '5/5: Inward migration might be the key as it implies an increase in the libration amplitude of large trojans, finally becoming unstable,  and either being ejected from the system or colliding with the star or the planet. More on the TROY search coming soon ;)']",https://arxiv.org/abs/1807.00773,"Co-orbital bodies are the byproduct of planet formation and evolution, as we know from the Solar System. Although planet-size co-orbitals do not exists in our planetary system, dynamical studies show that they can remain stable for long periods of time in the gravitational well of massive planets. Should they exist, their detection is feasible with the current instrumentation. In this paper, we present new ground-based observations searching for these bodies co-orbiting with nine close-in (P<5 days) planets, using different observing techniques. The combination of all of them allows us to restrict the parameter space of any possible trojan in the system. We use multi-technique observations (radial velocity, precision photometry and transit timing variations), both newly acquired in the context of the TROY project and publicly available, to constrain the presence of planet-size trojans in the Lagrangian points of nine known exoplanets. We find no clear evidence of trojans in these nine systems through any of the techniques used down to the precision of the observations. However, this allows us to constrain the presence of any potential trojan in the system, specially in the trojan mass/radius versus libration amplitude plane. In particular, we can set upper mass limits in the super-Earth mass regime for six of the studied systems. ","The TROY project: II. Multi-technique constraints on exotrojans in nine
  planetary systems"
84,1026850918480769024,45357989,Mahdi Nazemi,"['Check out my new paper on optimizing DNN inference through Boolean logic minimization:\n""NullaNet: Training Deep Neural Networks for Reduced-Memory-Access Inference"" <LINK>']",https://arxiv.org/abs/1807.08716,"Deep neural networks have been successfully deployed in a wide variety of applications including computer vision and speech recognition. However, computational and storage complexity of these models has forced the majority of computations to be performed on high-end computing platforms or on the cloud. To cope with computational and storage complexity of these models, this paper presents a training method that enables a radically different approach for realization of deep neural networks through Boolean logic minimization. The aforementioned realization completely removes the energy-hungry step of accessing memory for obtaining model parameters, consumes about two orders of magnitude fewer computing resources compared to realizations that use floatingpoint operations, and has a substantially lower latency. ","NullaNet: Training Deep Neural Networks for Reduced-Memory-Access
  Inference"
85,1023999501651066880,1248692233,Tobias Marriage,['Our new paper! Jesse Rivera studies a galaxy from 11 billion years ago forming 100s of stars per year. (To compare: our Milky Way forms only 1 star per year!) The plot shows how Jesse resolves the flow of gas in this gravitationally lensed galaxy.  <LINK> <LINK>'],https://arxiv.org/abs/1807.08895,"We report Northern Extended Millimeter Array (NOEMA) CO($J = 3 - 2$) observations of the dusty star-forming galaxy ACT-S\,J020941+001557 at $z = 2.5528$, which was detected as an unresolved source in the Atacama Cosmology Telescope (ACT) equatorial survey. Our spatially resolved spectral line data support the derivation of a gravitational lens model from 37 independent velocity channel maps using a pixel-based algorithm, from which we infer a velocity-dependent magnification factor $\mu \approx 7-22$ with a luminosity-weighted mean $\left<\mu\right>\approx 13$. The resulting source-plane reconstruction is consistent with a rotating disk, although other scenarios cannot be ruled out by our data. After correction for lensing, we derive a line luminosity $L^{\prime}_{\rm CO(3-2)}= (5.53\pm 0.69) \times 10^{10}\,{\rm \,K\,km\,s^{-1}\,pc^{2}}$, a cold gas mass $M_{{\rm gas}}= (3.86 \pm 0.33) \times 10^{10}\,M_{\odot}$, a dynamical mass $M_{\rm dyn}\,{\rm sin}^2\,i = 3.9^{+1.8}_{-1.5} \times 10^{10}\,M_{\odot}$, and a gas mass fraction $f_{\rm gas}\,{\rm csc}^2\,i = 1.0^{+0.8}_{-0.4}$. The line brightness temperature ratio of $r_{3,1}\approx 1.6$ relative to a Green Bank Telescope CO($J=1-0$) detection may be elevated by a combination of external heating of molecular clouds, differential lensing, and/or pointing errors. ","The Atacama Cosmology Telescope: CO(J = 3 - 2) mapping and lens modeling
  of an ACT-selected dusty star-forming galaxy"
86,1023217908321804288,335473253,Jason Kalirai,['Our group’s new paper led by @stsci Support Scientist Matteo Correnti on how we can use sensitive IR photometry on #Hubble to better measure the ages of old Milky Way globular clusters. This work paves the way for future #JWST stellar pops research. <LINK> <LINK>'],https://arxiv.org/abs/1807.10142,"Globular Clusters (GCs) in the Milky Way represent the ideal laboratory to establish the age of the oldest stellar populations and to measure the color-magnitude relation of stars. Infrared (IR) photometry of these objects provides a new opportunity to accomplish this task. In particular, at low stellar masses, the stellar main sequence (MS) in an IR color-magnitude diagram (CMD) exhibits a sharp ""kink"" (due to opacity effects in M dwarfs), such that lower mass and cooler dwarfs become bluer in the F110W - F160W color baseline and not redder. This inversion of the color-magnitude relation offers the possibility to fit GC properties using IR imaging, and to reduce their uncertainties. Here, we used the IR channel of the Wide Field Camera 3 onboard the Hubble Space Telescope to obtain new, deep high-resolution photometry of the old metal-poor GC NGC6397. From the analysis of the GC CMD, we revealed below the MS ""kink"" the presence of two MSs with different chemical composition. We derived the cluster fiducial line and we compared it with a grid of isochrones over a large range of parameter space, allowing age, metallicity, distance and reddening to vary freely within reasonable selected ranges. We derived an age of 12.6 Gyr with a random uncertainty sigma ~ 0.7 Gyr. These results confirm that the analysis of the IR color-magnitude of stars provide a valuable tool to measure the GC ages and offers a new venue to determine their absolute age to sub-Gyr accuracy with next generation IR telescopes. ","The Age of the Old Metal-Poor Globular Cluster NGC6397 Using WFC3/IR
  Photometry"
87,1018161901090627584,933084565895286786,Dan Hooper,"['In my new paper with Gordan Krnjaic (@GordanKrnjaic), Andrew Long &amp; Sam McDermott, we identify a class of models in which the particle responsible for inflation is also the dark matter. We call it ""WIMPflation"".\n2 birds with 1 stone!\n<LINK>\n#DarkMatter #cosmology', ""@Aqeelhmed @GordanKrnjaic We worried about this, of course, but found that such potentials can arise in models in which the inflaton has a non-minimal coupling to gravity (ie McDonald and Learner) or in Kallosh and Linde's alpha-attractor scenario, just to name a couple of examples.""]",https://arxiv.org/abs/1807.03308,"We propose a class of models in which a stable inflaton is produced as a thermal relic in the early universe and constitutes the dark matter. We show that inflaton annihilations can efficiently reheat the universe, and identify several examples of inflationary potentials that can accommodate all cosmic microwave background observables and in which the inflaton dark matter candidate has a weak scale mass. As a simple example, we consider annihilations that take place through a Higgs portal interaction, leading to encouraging prospects for future direct detection experiments. ",WIMPflation
88,1017306953490550784,427235323,Ismail Elezi,['Presenting a new brand end-to-end clustering. Paper accepted for presentation in ANNPR 2018. ArXiv preprint: <LINK> …\n\n#annpr #clustering #DeepLearning @thilo_on_data'],https://arxiv.org/abs/1807.04001,"We propose a novel end-to-end neural network architecture that, once trained, directly outputs a probabilistic clustering of a batch of input examples in one pass. It estimates a distribution over the number of clusters $k$, and for each $1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster assignment for each data point. The network is trained in advance in a supervised fashion on separate data to learn grouping by any perceptual similarity criterion based on pairwise labels (same/different group). It can then be applied to different data containing different groups. We demonstrate promising performance on high-dimensional data like images (COIL-100) and speech (TIMIT). We call this ``learning to cluster'' and show its conceptual difference to deep metric learning, semi-supervise clustering and other related approaches while having the advantage of performing learnable clustering fully end-to-end. ",Learning Neural Models for End-to-End Clustering
89,1025202967966822401,2350333891,Andreas Mayer,"['My first paper wholly done since starting as a postdoc in @Princeton now out on Arxiv! <LINK> We propose a simple mechanistic model to explain the surprising finding  that T cell expansion depends as a power law on precursor cell numbers. <LINK>', 'We then show that the same model can also quantitatively fit experimental data on how T cell expansion depends on affinity. https://t.co/tsuQOcZB7g', 'Our model identifies the dynamics of the level of presented antigen as a key natural regulator of T cell expansion. https://t.co/yiUcOLgGlQ', 'For details (and further work on how antigen dosing kinetics affects expansion and who wins when T cells of different affinities compete!) see the preprint!', 'This is joined work with Yaojun Zhang, Alan Perelson and Ned Wingreen!  Let us know if you have any comments. :)']",https://arxiv.org/abs/1807.09481,"An essential feature of the adaptive immune system is the proliferation of antigen-specific lymphocytes during an immune reaction to form a large pool of effector cells. This proliferation must be regulated to ensure an effective response to infection while avoiding immunopathology. Recent experiments in mice have demonstrated that the expansion of a specific clone of T cells in response to cognate antigen obeys a striking inverse power law with respect to the initial number of T cells. Here, we show that such a relationship arises naturally from a model in which T cell expansion is limited by decaying levels of presented antigen. The same model also accounts for the observed dependence of T cell expansion on affinity for antigen and on the kinetics of antigen administration. Extending the model to address expansion of multiple T cell clones competing for antigen, we find that higher affinity clones can suppress the proliferation of lower affinity clones, thereby promoting the specificity of the response. Employing the model to derive optimal vaccination protocols, we find that exponentially increasing antigen doses can achieve a nearly optimized response. We thus conclude that the dynamics of presented antigen is a key regulator of both the size and specificity of the adaptive immune response. ",Regulation of T cell expansion by antigen presentation dynamics
90,1021559079901093888,930764003277643777,Matias Quiroz,"['Want to find out more about our work on using data subsampling to speed up MCMC algorithms? We just released a textbook style easy-to-read review on arXiv: <LINK>. @matvil @robertjk59', 'Just noticed that Figure 3.6 is not rendering properly. Time for some arXiv hacking!', 'The error is now fixed and the new version, with the correct Figure 3.6, should appear on arXiv tomorrow or so.']",https://arxiv.org/abs/1807.08409,"The rapid development of computing power and efficient Markov Chain Monte Carlo (MCMC) simulation algorithms have revolutionized Bayesian statistics, making it a highly practical inference method in applied work. However, MCMC algorithms tend to be computationally demanding, and are particularly slow for large datasets. Data subsampling has recently been suggested as a way to make MCMC methods scalable on massively large data, utilizing efficient sampling schemes and estimators from the survey sampling literature. These developments tend to be unknown by many survey statisticians who traditionally work with non-Bayesian methods, and rarely use MCMC. Our article explains the idea of data subsampling in MCMC by reviewing one strand of work, Subsampling MCMC, a so called pseudo-marginal MCMC approach to speeding up MCMC through data subsampling. The review is written for a survey statistician without previous knowledge of MCMC methods since our aim is to motivate survey sampling experts to contribute to the growing Subsampling MCMC literature. ",Subsampling MCMC - An introduction for the survey statistician
91,1020106065059328000,837133583558987776,Colin Raffel,"['New paper w/ @D_Berthelot_ML Aurko Roy and @goodfellow_ian where we propose an adversarial regularizer for improving interpolation in autoencoders and measure whether it also improves representation learning performance. Paper <LINK>, code <LINK> <LINK>']",http://arxiv.org/abs/1807.07543,"Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can ""interpolate"": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations. ","Understanding and Improving Interpolation in Autoencoders via an
  Adversarial Regularizer"
92,1019784812951814145,133148364,Devendra Chaplot,"['Recently, we formed a working group to study empirical methodology in navigation research and coordinate future research in this area. We just posted a whitepaper on Evaluation of Embodied Navigation Agents. Thanks to Vladlen for leading the effort!\n<LINK> <LINK>']",https://arxiv.org/abs/1807.06757,"Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking. ",On Evaluation of Embodied Navigation Agents
93,1019342561469755398,373457376,Manuel J Marin-Jimenez,"['Given a depth map containing a person, we propose a new method to estimate his/her 3D body pose. Paper: <LINK>\nVideo with results: <LINK> \n#DeepLearning #IngenieriaInteligente #ComputerVision #HumanPose #GitHub']",https://arxiv.org/abs/1807.05389,"Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predefined 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is defined as a ConvNet that computes the specific weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which respectively depict realistic and synthetic samples, defining a new state-of-the-art on them. ","3D human pose estimation from depth maps using a deep combination of
  poses"
94,1019125191207768064,426509606,Yamir Moreno,"['In our last work, <LINK>, we explore the block nature of the matrix representation of multiplex networks\nand study their spectral properties by formulating the corresponding polynomial eigenvalue problem. Work done with @GuiFdeArruda @FranciscoICMC @ecozzo <LINK>', '@HirokiSayama @GuiFdeArruda @FranciscoICMC @ecozzo It is :-) and Thanks!']",https://arxiv.org/abs/1807.05588,"We explore the block nature of the matrix representation of multiplex networks, introducing a new formalism to deal with its spectral properties as a function of the inter-layer coupling parameter. This approach allows us to derive interesting results based on an interpretation of the traditional eigenvalue problem. More specifically, we reduce the dimensionality of our matrices but increase the power of the characteristic polynomial, i.e, a polynomial eigenvalue problem. Such an approach may sound counterintuitive at first glance, but it allows us to relate the quadratic problem for a 2-Layer multiplex system with the spectra of the aggregated network and to derive bounds for the spectra, among many other interesting analytical insights. Furthermore, it also permits to directly obtain analytical and numerical insights on the eigenvalue behavior as a function of the coupling between layers. Our study includes the supra-adjacency, supra-Laplacian, and the probability transition matrices, which enable us to put our results under the perspective of structural phases in multiplex networks. We believe that this formalism and the results reported will make it possible to derive new results for multiplex networks in the future. ",A polynomial eigenvalue approach for multiplex networks
