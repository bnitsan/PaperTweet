,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1503330542510612485,757009671366606848,Luigi Acerbi,"['Ever wanted to perform distributed Bayesian inference on large datasets, e.g. via parallel MCMC? Watch out for *embarrassing failures*!\n\nIn our new paper at @aistats_conf, we explore what can go wrong with popular ""embarrassingly parallel"" methods <LINK> 1/', 'Embarrassingly parallel MCMC works by splitting the data into partitions, sent to different computing nodes. Each node performs MCMC separately and the results are sent back to the main node to be combined. So far so good, right? 2/ https://t.co/NDYwlaeLxS', 'Unfortunately, many things can go wrong!\nWe show how mode collapse, model mismatch, and underrepresented tails *in any of the nodes* can independently produce a catastrophic output in the final combination step, for many commonly used parallel MCMC algorithms. 3/ https://t.co/D5kza84uBV', 'As a solution, we propose Parallel Active Inference (PAI), which uses a mix of (1) surrogate modeling via Gaussian processes; (2) sharing key information across nodes (a single extra step); and (3) active learning to smartly refine the local models. 4/ https://t.co/pEUucFZ0xi', 'We demonstrate how our proposed steps avoid catastrophic failures on several examples - and show why all these steps are necessary via ablation studies. 5/ https://t.co/ldhHza0r8s', 'Check out the paper for more details (https://t.co/pbVcFF8Xta), and the code is available here: https://t.co/QVdqgwJ7x9\n\nWork fantastically led by @spectraldani, with @wkly_infrmtive and @samikaski and the support of @FCAI_fi. 6/6']",https://arxiv.org/abs/2202.11154,"Embarrassingly parallel Markov Chain Monte Carlo (MCMC) exploits parallel computing to scale Bayesian inference to large datasets by using a two-step approach. First, MCMC is run in parallel on (sub)posteriors defined on data partitions. Then, a server combines local results. While efficient, this framework is very sensitive to the quality of subposterior sampling. Common sampling problems such as missing modes or misrepresentation of low-density regions are amplified -- instead of being corrected -- in the combination phase, leading to catastrophic failures. In this work, we propose a novel combination strategy to mitigate this issue. Our strategy, Parallel Active Inference (PAI), leverages Gaussian Process (GP) surrogate modeling and active learning. After fitting GPs to subposteriors, PAI (i) shares information between GP surrogates to cover missing modes; and (ii) uses active sampling to individually refine subposterior approximations. We validate PAI in challenging benchmarks, including heavy-tailed and multi-modal posteriors and a real-world application to computational neuroscience. Empirical results show that PAI succeeds where previous methods catastrophically fail, with a small communication overhead. ",Parallel MCMC Without Embarrassing Failures
1,1503117089778851843,2956121356,Russ Salakhutdinov,"['New #ICLR2022 paper: Learning Weakly-Supervised Contrastive Representations using auxiliary information.\n\nAs for performance, the auxiliary-information-infused self-supervised learning comes closer to supervised learning\n\nPaper <LINK>\nCode <LINK> <LINK>', 'with Yao-Hung Hubert Tsai, Tianqin Li, Weixin Liu, Peiyuan Liao, and Louis-Philippe Morency']",https://arxiv.org/abs/2202.06670,"We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information. The second stage is to learn similar representations within the same cluster and dissimilar representations for data from different clusters. Our empirical experiments suggest the following three contributions. First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals. Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information. Third, we show that our approach also works well with unsupervised constructed clusters (e.g., no auxiliary information), resulting in a strong unsupervised representation learning approach. ",Learning Weakly-Supervised Contrastive Representations
2,1501127175570874369,1176755442,Prof Aimee Morgans,['New paper (resulting from collaboration with @KTHuniversity)!\n\nA model for the aeroacoustic scattering of tube rows in cross-flow\n\nNow out in the Journal of Sound &amp; Vibration: <LINK>\nOA: <LINK> <LINK>'],http://arxiv.org/abs/2202.11538,"Heat exchanger tube rows can influence the thermoacoustic instability behaviour of combustion systems since they act as both acoustic scatterers and unsteady heat sinks. Therefore, with careful tuning of their thermoacoustic properties, heat exchangers have the potential to act as passive control devices. In this work, we focus on (only) the acoustic scattering behaviour of heat exchanger tubes. We present a comparison of existing acoustic models for tube rows and slits, models for the latter having the advantage of incorporating frequency dependence. We then propose a new model that enables the adaptation of slit models for tube rows. This model is validated against experiments and Linearised Navier Stokes Equations (LNSE) predictions for the transmission and reflection coefficients, including phase information. The model predictions show very good agreement with the experimental and numerical validations, especially for low frequencies (Strouhal number < 0.5, based on tube radius and excitation frequency), with mean differences less than 2% for the transmission coefficients (the reflection coefficient errors are somewhat larger since their magnitudes are very close to zero). ","A low frequency model for the aeroacoustic scattering of cylindrical
  tube rows in cross-flow"
3,1500918611258585090,382008009,Miles Cranmer,"['Could machine learning rediscover the law of gravitation simply by observing our solar system?\n\nWith our new approach, the answer is *YES*.\n\nLed by: @PabloLemosP \nWith: @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia\nPaper: <LINK>\nBlog: <LINK> <LINK>', 'Here are two classes of problems which we can already solve:\n\n1. Known law, unknown parameters =&gt; parameter inference.\n\n2. Unknown law, known parameters =&gt; model discovery (eg, https://t.co/bA5NFy4M9P / https://t.co/xEW0hL7ArT)\n\nHere we look at unknown law AND unknown parameters!', 'Summary of the algorithm: \n\n1. Declare unknown physical properties of a system as trainable parameters in a machine learning model.\n\n2. Update these parameters simultaneously with the model weights.\n\n3. Finally, distill the learned model to a set of symbolic rules.', ""After training, we use PySR (https://t.co/FhIXox1InB) to find the following symbolic forms as approximations of our graph neural network's edge function.\n\nThe symbolic rule which best balances accuracy and simplicity is the same as the law of universal gravitation (in teal): https://t.co/lpjPJ8N88x"", ""Let's look at the per-node parameters learned in our model. These are actually strongly correlated with the true masses!\n\nNote that the model's scale is arbitrary, so these are shown normalized to the mass of the sun. We also have to fix F=ma to eliminate a functional degeneracy. https://t.co/BxitaaVb6S"", 'This approach allows us to do model discovery even when missing crucial information about our system! I anticipate this being very useful for model discovery in real-world datasets.\n\nFor more details check out the paper https://t.co/jDeOUt7yFv.', '@SheerPriya The assumption is that you can find F=ma beforehand by observing rigid body motion - no need for solar system data. You require this knowledge before - otherwise, there would be a complete degeneracy: e.g., F=m^3 a (second law) and F=GMm^3/r^2 would be perfectly valid solutions!', '@SheerPriya We put in translational/rotational equivariance since these also seem like reasonable physical assumptions. However, you raise a really great question, and it would be interesting to re-examine the model without such assumptions!', '@TanoojS @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia Good question, that would definitely be interesting to try. In the LNN paper there‚Äôs a GNN model too, so maybe you could use that here. You would still have to learn per-node properties though.\n\n(btw, I‚Äôm the first author of the Lagrangian NNs paper - thanks for reading it!)', ""@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia Do you mean (1) without distances, or (2) to try with a geocentric coordinate system?\n\n(1) - Tycho Brahe's tables (used by Kepler) included 3D positions (even ancient greeks estimated these), so I think inclusion is reasonable\n(2) - this is a great question; I'd be curious too!"", ""@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia Here's a good article - the geometry is quite intense! It would be quite the achievement for an algorithm to figure out distances, unsupervised, from sky observations:\nhttps://t.co/NtF7rF3ljs https://t.co/J1c2ym7Wbd"", ""@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia For posterity, here's the catalog of planetary positions used at the time of Kepler: https://t.co/we4iM5aUoh"", '@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia (Btw - once you know the diameter of a planet, you can get its distance from its angular size on the sky.)', '@TanoojS @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia No worries at all- I was trying to word my tweet in that I am genuinely happy that you remember the main LNN ideas!\n\nLet me know how the project works out, it sounds cool :)', ""@dsivakumar @ChrSzegedy @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia Learning the symbolic law of gravity alone from tabular data even goes back to the 1970s! (Patrick Langley's work).\n\nThe main differences with this paper are:\n1. Don't know the masses a priori\n2. Don't know the individual force contributions; only the net motions\n3. Real data"", ""@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia Actually, that's one way to learn on raw telescope observations: learn the diameter of each planet in addition to its mass! Then, given (ra, dec, apparent_radius), a learned diameter would give you the inferred 3D position of the planet.\n\nIs that what you had in mind?"", '@shoyer @PabloLemosP @Niall_Jeffrey @cosmo_shirley @PeterWBattaglia I have no idea if that would introduce more degenerate solutions or not. Maybe that is completely ill-posed!']",https://arxiv.org/abs/2202.02306,"We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a ""graph neural network"" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery. ",Rediscovering orbital mechanics with machine learning
4,1500814124850286598,1334580500749553665,Miguel Vioque,"['Our paper got accepted for publication in The Astrophysical Journal! We have found 128 new Herbig stars (aka massive forming stars), most of which we have observed for the first time:\n\n<LINK>', 'This increases the number of known objects of the class by ~50%. We find that the new Herbig stars behave similarly to the previously known ones at every mass range (the sample goes from 1.5 to 15 solar masses).', 'We also find evidence for a change in the stellar accretion mechanism at ~4 solar masses. The more massive sources seem to accrete differently. The sample also suggests intense inner-disk photoevaporation for sources with masses above ~7 solar masses.', '@NienkeMarel Hi Nienke. Yes, most of the new Herbig stars are far away. This is one of the reasons why we only have found them now. Only 5 of the new Herbig stars lie within 500 pc, whereas 80% of the sample is beyond 1 kpc.']",https://arxiv.org/abs/2202.01234,"We present optical spectroscopy observations of 145 high-mass pre-main sequence candidates from the catalogue of Vioque et al. (2020). From these, we provide evidence for the Herbig nature of 128 sources. This increases the number of known objects of the class by $\sim50\%$. We determine the stellar parameters of these sources using the spectra and Gaia EDR3 data. The new sources are well distributed in mass and age, with 23 sources between $4$-$8$ M$_{\odot}$ and 32 sources above $8$ M$_{\odot}$. Accretion rates are inferred from H$\alpha$ and H$\beta$ luminosities for 104 of the new Herbigs. These accretion rates, combined with previous similar estimates, allow us to analyze the accretion properties of Herbig stars using the largest sample ever considered. We provide further support to the existence of a break in accretion properties at $\sim3$-$4$ M$_{\odot}$, which was already reported for the previously known Herbig stars. We re-estimate the potential break in accretion properties to be at $3.87^{+0.38}_{-0.96}$ M$_{\odot}$. As observed for the previously known Herbig stars, the sample of new Herbig stars independently suggests intense inner-disk photoevaporation for sources with masses above $\sim7$ M$_{\odot}$. These observations provide robust observational support to the accuracy of the Vioque et al. (2020) catalogue of Herbig candidates. ","Identification and spectroscopic characterization of 128 new Herbig
  stars"
5,1500789496308277249,956539964795301889,Jacopo Bertolotti,"['New paper on #ArXiv!\n""Tracking moving objects through scattering media via speckle correlations""\nWith @YJaureguiS, and Harry Penketh.\n\nShort(?) thread explaining what it is about.\n1/\n<LINK>', ""Scattering is a major problem for imaging, and it doesn't take very much of it before we can't see essentially anything of what it is happening.\nNot surprisingly, imaging in the presence of scattering is a very active field of research.\n2/\nhttps://t.co/XKM56Mvosx"", ""There isn't a single best way on how to deal with scattering, and the answer depends a LOT on how much scattering we are talking about and its properties.\nAs a rule of thumb, the most complicated situation is where all light is multiply scattered.\n3/"", 'That said, linear scattering can\'t destroy the information, it ""just"" scrambles it. The result is that the scattered light forms complex patterns, which might look random but contain a lot of correlations where the information is hidden.\nWhich gives us some hope.\n4/', 'An approach that has attracted some attention is based on a correlation that links the angle of incidence of the incoming wave, with the output angle of the scattered light, called the ""optical memory effect"".\n5/\nhttps://t.co/AyslCXp0hJ', 'To make the long story short, this correlation allows you to measure the convolution between the (unknown) object to be imaged, and the (unknown) speckle pattern generated by the scattering medium.\n6/', ""This by itself is not very useful (too many unknowns) but, as realized by Labeyrie in the '70s, performing an autocorrelation one can separate the information about the object from the information about the speckle.\n7/"", 'The only remaining problem is that we are left with the autocorrelation of the object, not an image of the object itself.\nThis autocorrelation can be numerically inverted to obtain the desired image, but this is a slow and computationally intensive process.\n8/', 'What if things are moving? In principle one can perform an autocorrelation inversion per each frame, but this needs to be done in post-processing (too slow to do otherwise), making real-time tracking complicated.\n9/', 'What if we are more interested in the tracking itself than in the real-time imaging? (i.e. we care more about knowing how the stuff in the scene moved than the precise look of the scene)\nIn that case we show that there is a simple and computationally inexpensive solution!\n10/', 'Instead of autocorrelating the light collected at each frame, we cross-correlate the light detected at different times, and perform the difference between two such correlations (to subtract all the extraneous signal)\n[Showing here a simulation. The experiment is in the paper]\n11/ https://t.co/Anfe8PW0rc', 'This results in a easy-to-compute video where the moving object is fixed in the middle, and the background moves around it, showing clearly what it is happening.\n[Technical note: This allows for tracking well beyond the memory effect range.]\n\nThe end üôÇ\n12/12', '@maartendehaan Thanks üòä']",https://arxiv.org/abs/2202.10804,"Scattering can rapidly degrade our ability to form an optical image, to the point where only speckle-like patterns can be measured. Truly non-invasive imaging through a strongly scattering obstacle is difficult, and usually reliant on a computationally intensive numerical reconstruction. In this work we show that, by combining the cross-correlations of the measured speckle pattern at different times, it is possible to track a moving object with minimal computational effort and over a large field of view. ","Tracking moving objects through scattering media via speckle
  correlations"
6,1499761949688668163,14073989,Gustav Markkula,"['New preprint / position paper: ""How accurate models of human behavior are needed for human-robot interaction? For automated driving?""\n\n<LINK>', 'One main takeaway: Focus modelling on those aspects of human behaviour to which interaction outcome (safety, performance, satisfaction) is most sensitive - but finding out which those aspects are is a research question in its own right - needs addressing also for ML models', 'To be presented next week at the ACM/IEEE HRI 2022 Workshop on Modeling Human Behavior in Human-Robot Interactions\n\nReally stoked about this workshop overall - come listen if you can!\n\nhttps://t.co/hAtpVrjill', '@arashttavakoli Oh glad you liked it! üëç']",https://arxiv.org/abs/2202.06123,"There are many examples of cases where access to improved models of human behavior and cognition has allowed creation of robots which can better interact with humans, and not least in road vehicle automation this is a rapidly growing area of research. Human-robot interaction (HRI) therefore provides an important applied setting for human behavior modeling - but given the vast complexity of human behavior, how complete and accurate do these models need to be? Here, we outline some possible ways of thinking about this problem, starting from the suggestion that modelers need to keep the right end goal in sight: A successful human-robot interaction, in terms of safety, performance, and human satisfaction. Efforts toward model completeness and accuracy should be focused on those aspects of human behavior to which interaction success is most sensitive. We emphasise that identifying which those aspects are is a difficult scientific objective in its own right, distinct for each given HRI context. We propose and exemplify an approach to formulating a priori hypotheses on this matter, in cases where robots are to be involved in interactions which currently take place between humans, such as in automated driving. Our perspective also highlights some possible risks of overreliance on machine-learned models of human behavior in HRI, and how to mitigate against those risks. ","How accurate models of human behavior are needed for human-robot
  interaction? For automated driving?"
7,1499722837258641412,3139883618,Daniel Nevo,"['üì£New preprintüì£\nStandard hazard ratios have no clear causal interpretation. In this paper, led by PhD student @RachelAxelrod2, we developed sensitivity analyses for the causal hazard ratio, a causal estimand inspired by principal stratification. \n<LINK>\n1/2', 'We use ideas from frailty survival models to model the inevitable cross-world dependence and cover randomized and observational studies, using kernel\\Cox-based estimation.\nCheck it out!\n#causaltwitter \n2/2']",https://arxiv.org/abs/2202.12420,"The Hazard Ratio (HR) is often reported as the main causal effect when studying survival data. Despite its popularity, the HR suffers from an unclear causal interpretation. As already pointed out in the literature, there is a built-in selection bias in the HR, because similarly to the truncation by death problem, the HR conditions on post-treatment survival. A recently proposed alternative, inspired by the Survivor Average Causal Effect (SACE), is the causal HR, defined as the ratio between hazards across treatment groups among the study participants that would have survived regardless of their treatment assignment. We discuss the challenge in identifying the causal HR and present a sensitivity analysis identification approach in randomized controlled trials utilizing a working frailty model. We further extend our framework to adjust for potential confounders using inverse probability of treatment weighting. We present a Cox-based and a flexible non-parametric kernel-based estimation under right censoring. We study the finite-sample properties of the proposed estimation method through simulations. We illustrate the utility of our framework using two real-data examples. ","A sensitivity analysis approach for the causal hazard ratio in
  randomized and observational studies"
8,1499358985991958532,1266475355453501440,Frank E. Curtis,['Excited to announce a new paper on how to make effective use of prior function evaluations when solving a sequence of optimization problems in a derivative-free manner...\n\n<LINK>'],https://arxiv.org/abs/2202.12961,"A derivative-free optimization (DFO) algorithm is presented. The distinguishing feature of the algorithm is that it allows for the use of function values that have been made available through prior runs of a DFO algorithm for solving prior related optimization problems. Applications in which sequences of related optimization problems are solved such that the proposed algorithm is applicable include certain least-squares and simulation-based optimization problems. A convergence guarantee of a generic algorithmic framework that exploits prior function evaluations is presented, then a particular instance of the framework is proposed and analyzed. The results of numerical experiments when solving engineering test problems show that the algorithm gains advantage over a state-of-the-art DFO algorithm when prior function values are available. ",Exploiting Prior Function Evaluations in Derivative-Free Optimization
9,1499207309083361285,3061733236,Dong Gong,"['Our new Continual Learning work got accepted by #CVPR2022. We introduced a new Bayesian sparse regularization on the network neurons, with novel memory-based full experience reply. \nLucky to work with a great team making this done! An early-version paper: <LINK> <LINK>', 'The sparse regularization encourages to use less model capacity for each task and thus reserves capacity for the subsequent tasks in the continual learning streams. The hierarchical Bayesian modeling optimizes the sparsity.', 'The full experience reply involves the intermediate features for more effective and flexible knowledge sharing and transferring. It also directly guide sparsity modeling at the intermediate layers.', 'The technical details of the sparse regularization are related to our previous work about Bayesian variational dropout: https://t.co/icdIIF90sE']",http://arxiv.org/abs/2202.10203,"Continual Learning (CL) methods aim to enable machine learning models to learn new tasks without catastrophic forgetting of those that have been previously mastered. Existing CL approaches often keep a buffer of previously-seen samples, perform knowledge distillation, or use regularization techniques towards this goal. Despite their performance, they still suffer from interference across tasks which leads to catastrophic forgetting. To ameliorate this problem, we propose to only activate and select sparse neurons for learning current and past tasks at any stage. More parameters space and model capacity can thus be reserved for the future tasks. This minimizes the interference between parameters for different tasks. To do so, we propose a Sparse neural Network for Continual Learning (SNCL), which employs variational Bayesian sparsity priors on the activations of the neurons in all layers. Full Experience Replay (FER) provides effective supervision in learning the sparse activations of the neurons in different layers. A loss-aware reservoir-sampling strategy is developed to maintain the memory buffer. The proposed method is agnostic as to the network structures and the task boundaries. Experiments on different datasets show that our approach achieves state-of-the-art performance for mitigating forgetting. ","Learning Bayesian Sparse Networks with Full Experience Replay for
  Continual Learning"
10,1499122704833728513,1392820775749623808,Eleni Tzirita Zacharatou,"['New work on efficient spreadsheet parsing for data science, led by (twitter-less) Felix Henze.\n \nWe introduce parallel spreadsheet-specific parsing routines and tightly couple decompression and parsing to reduce the runtime and memory usage. \n\nPaper: <LINK> <LINK>', 'Our work will appear at DOLAP 2022 (co-located with \n@edbticdt2022): https://t.co/7iyhv0fvsg\nThe online attendance is free and open to everyone upon registration: https://t.co/h7M6PDxk5V\n\nJoint work with Haralampos Gavriilidis and Volker Markl.']",http://arxiv.org/abs/2202.13189,"Spreadsheets are widely used for data exploration. Since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. However, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. To make spreasheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. Furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. To evaluate our approach, we implement a prototype for loading Excel spreadsheets into R environments. Our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches. ",Efficient Specialized Spreadsheet Parsing for Data Science
11,1498767764831539200,20865039,Tristan Deleu,"['New paper üìÑ ""Bayesian Structure Learning with Generative Flow Networks"", with @AntGois, @ChrisEmezue, Mansi Rankawat, @SimonLacosteJ,  Stefan Bauer &amp; Yoshua Bengio @Mila_Quebec \n<LINK> <LINK>', 'For that, we use a new class of probabilistic models, called GFlowNets, that induce a probability distribution over discrete &amp; composite objects (which is perfect for graphs!). We wrote a paper about the foundations of these models a few months ago.  https://t.co/PR4iGzxtDm']",https://arxiv.org/abs/2202.13903,"In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference. ",Bayesian Structure Learning with Generative Flow Networks
12,1498749103156740106,21055092,Lucas Hunt,"['Check out this new package called PetroFit, developed by Robel Geda. You can use it to calculate Petrosian profiles and fit galaxy images. You can get photometry and morphological properties in one simple package! Read the paper! Download today!  <LINK>', 'Of course if I were following @robelgeda at the time I would have tagged him! Fantastic work Robel!']",https://arxiv.org/abs/2202.13493,"PetroFit is an open-source Python package, based on Astropy and Photutils, that can calculate Petrosian profiles and fit galaxy images. It offers end-to-end tools for making accurate photometric measurements, estimating morphological properties, and fitting 2D models to galaxy images. Petrosian metric radii can be used for model parameter estimation and aperture photometry to provide accurate total fluxes. Correction tools are provided for improving Petrosian radii estimates affected by galaxy morphology. PetroFit also provides tools for sampling Astropy-based models (including custom profiles and multi-component models) onto image grids and enables PSF convolution to account for the effects of seeing. These capabilities provide a robust means of modeling and fitting galaxy light profiles. We have made the PetroFit package publicly available on GitHub (PetroFit/petrofit) and PyPi (pip install petrofit). ","PetroFit: A Python Package for Computing Petrosian Radii and Fitting
  Galaxy Light Profiles"
13,1498733501750906889,2530947115,Max Tegmark,"[""I'm excited about our new paper: How can your thinking be so reliable when your brain is made of noisy, unreliable neurons? #GridCodes, 1st discovered in mice, allow perfect error correction! We can use this trick to build better #neuromorphic #AI chips.\n<LINK>""]",https://arxiv.org/abs/2202.12887,"It has been an open question in deep learning if fault-tolerant computation is possible: can arbitrarily reliable computation be achieved using only unreliable neurons? In the mammalian cortex, analog error correction codes known as grid codes have been observed to protect states against neural spiking noise, but their role in information processing is unclear. Here, we use these biological codes to show that a universal fault-tolerant neural network can be achieved if the faultiness of each neuron lies below a sharp threshold, which we find coincides in order of magnitude with noise observed in biological neurons. The discovery of a sharp phase transition from faulty to fault-tolerant neural computation opens a path towards understanding noisy analog systems in artificial intelligence and neuroscience. ","Biological error correction codes generate fault-tolerant neural
  networks"
14,1498716828041895939,405026897,Omar Valsson,"['Out now as a preprint, a new paper where we implement and test Daubechies Wavelets as basis functions for Variationally Enhanced Sampling and show that leads to greatly improved performance in enhanced sampling simulations. #compchem\n<LINK>', 'Work done by my PhD student Benjamin Pampel in Mainz that did all the heavy lifting of implementing the wavelets and testing them. Benjamin also handed in his thesis today!', 'The wavelet implementation is already available in the VES module in PLUMED 2.8 which was released yesterday\n@plumed_org \nhttps://t.co/M6xhc5utDu', 'All inputs and simulations data available on the PLUMED-NEST and Zenodo\nhttps://t.co/kVtamSkA1h\nhttps://t.co/iDfoCGgyC4']",https://arxiv.org/abs/2202.13459,"Collective variable-based enhanced sampling methods are routinely used on systems with metastable states, where high free energy barriers impede proper sampling of the free energy landscapes when using conventional molecular dynamics simulations. One such method is variationally enhanced sampling (VES), which is based on a variational principle where a bias potential in the space of some chosen slow degrees of freedom, or collective variables, is constructed by minimizing a convex functional. In practice, the bias potential is taken as a linear expansion in some basis function set. So far, primarily basis functions delocalized in the collective variable space, like plane waves, Chebyshev, or Legendre polynomials, have been used. However, there has not been an extensive study of how the convergence behavior is affected by the choice of the basis functions. In particular, it remains an open question if localized basis functions might perform better. In this work, we implement, tune, and validate Daubechies wavelets as basis functions for VES\@. The wavelets construct orthogonal and localized bases that exhibit an attractive multiresolution property. We evaluate the performance of wavelet and other basis functions on various systems, going from model potentials to the calcium carbonate association process in water. We observe that wavelets exhibit excellent performance and much more robust convergence behavior than all other basis functions, as well as better performance than metadynamics. In particular, using wavelet bases yields far smaller fluctuations of the bias potential within individual runs and smaller differences between independent runs. Based on our overall results, we can recommend wavelets as basis functions for VES. ","Improving the Efficiency of Variationally Enhanced Sampling with
  Wavelet-Based Bias Potentials"
15,1498699496401911815,882257115863187457,Sanjeev Arora,"[""Contrastive learning gives  great data representations. New paper (title is a homage to Zhang et al'16) says understanding requires opening the black box of deep learning). <LINK>\n\n(Note: Lead author Nikunj Saunshi, is on the job market.)""]",https://arxiv.org/abs/2202.14037,"Contrastive learning is a popular form of self-supervised learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs. Recent attempts to theoretically explain the success of contrastive learning on downstream classification tasks prove guarantees depending on properties of {\em augmentations} and the value of {\em contrastive loss} of representations. We demonstrate that such analyses, that ignore {\em inductive biases} of the function class and training algorithm, cannot adequately explain the success of contrastive learning, even {\em provably} leading to vacuous guarantees in some settings. Extensive experiments on image and text domains highlight the ubiquity of this problem -- different function classes and algorithms behave very differently on downstream tasks, despite having the same augmentations and contrastive losses. Theoretical analysis is presented for the class of linear representations, where incorporating inductive biases of the function class allows contrastive learning to work with less stringent conditions compared to prior analyses. ","Understanding Contrastive Learning Requires Incorporating Inductive
  Biases"
16,1498685120827568134,414087321,Pier Luigi Sacco üá∫üá¶,"['Out on @arxivblog today a new paper with @manlius84 @_AlexArenas. We analyze the rich-club structure of the tax avoidance network unveiled by the #PanamaPapers.\n\n<LINK>', 'We find that the rich-club of tax evasion adds new knowledge with respect to previous analyses of this database, and in particular puts under the spotlight one particular tax haven: British Virgin Islands. https://t.co/AbWpAeqj9P', 'Interestingly, this is the tax haven that is renowned to be the key node of the Russian tax avoidance pathway together with Cyprus.\n\nhttps://t.co/uCd42fXloX', ""However, as our analysis shows, all major global powers play a role in the rich-club. The announced crackdown on Russia's offshore money will also inevitably shed new light on the offshore tax avoidance network as a whole."", 'This terrible crisis may therefore also be a major opportunity to promote a broader transition toward fiscal transparency and global social justice. But clearly, this also means that there are strong interests in preventing this from happening.', 'What is at stake is therefore no less than a renegotiation of the deep foundations of the current world order. There can be no credible prospect of a democratic peace without a concurrent commitment to social and fiscal justice. The day of reckoning is round the corner.']",https://arxiv.org/abs/2202.13417,"After the leak of 11.5 million documents from the Panamanian corporation Mossack Fonseca, an intricate network of offshore business entities has been revealed. The emerging picture is that of legal entities, either individuals or companies, involved in offshore activities and transactions with several tax havens simultaneously which establish, indirectly, an effective network of countries acting on tax evasion. The analysis of this network quantitatively uncovers a strongly connected core (a rich-club) of countries whose indirect interactions, mediated by legal entities, form the skeleton for tax evasion worldwide. Intriguingly, the rich-club mainly consists of well-known tax havens such as British Virgin Islands and Hong Kong, and major global powers such as China, Russia, United Kingdom and United States of America. The analysis provides a new way to rank tax havens because of the role they play in this network, and the results call for an international coordination on taxation policies that take into account the complex interconnected structure of tax evaders in a globalized economy. ","The political economy of big data leaks: Uncovering the skeleton of tax
  evasion"
17,1498669784745009158,1047899041311412224,Francois Grondin,"['Our new paper where we introduce SmartBelt, a belt equipped with 8 microphones and 15 haptic motors that provides the user with haptic feedback to indicate the direction of arrival of sound. This could eventually benefit people with hearing impairment.\n\n<LINK>']",https://arxiv.org/abs/2202.13974,"This paper introduces SmartBelt, a wearable microphone array on a belt that performs sound source localization and returns the direction of arrival with respect to the user waist. One of the haptic motors on the belt then vibrates in the corresponding direction to provide useful feedback to the user. We also introduce a simple calibration step to adapt the belt to different waist sizes. Experiments are performed to confirm the accuracy of this wearable sound source localization system, and results show a Mean Average Error (MAE) of 2.90 degrees, and a correct haptic motor selection with a rate of 92.3%. Results suggest the device can provide useful haptic feedback, and will be evaluated in a study with people having hearing impairments. ","SmartBelt: A Wearable Microphone Array for Sound Source Localization
  with Haptic Feedback"
18,1498467101010505728,1138666325897752576,Kimon Fountoulakis,"['New paper ""Graph Attention Retrospective"".  One of the most popular type of models is graph attention networks. These models were introduced to allow a node to aggregate information from the features of neighbor nodes in a non-uniform way <LINK> <LINK>', 'in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem', 'of node classification for a contextual stochastic block model. Here the features of the nodes are obtained from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges are coupled in a natural way.', 'We show that in an ‚Äúeasy‚Äù regime, where the distance between the means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and significantly reduces the weights of the inter-class edges. https://t.co/SeNQYwhFo4', 'This implies perfect node classification independent of the weights of inter-class edges. However, a classical argument shows that in the ‚Äúeasy‚Äù regime, the graph is not needed at all to classify the data with high probability. (in the fig. q is the inter-edge prob) https://t.co/fls3GN8k8M', 'In the ‚Äúhard‚Äù regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. https://t.co/pb9zkISdeN', 'This is joint work with Amit Levi, @shenghao_yang, @aseemrb and Aukosh Jagannath. Also since you reached here, peaceüïäÔ∏è.']",https://arxiv.org/abs/2202.13060,"Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular type of models is graph attention networks. These models were introduced to allow a node to aggregate information from the features of neighbor nodes in a non-uniform way in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here the features of the nodes are obtained from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges are coupled in a natural way. First, we show that in an ""easy"" regime, where the distance between the means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies perfect node classification independent of the weights of inter-class edges. However, a classical argument shows that in the ""easy"" regime, the graph is not needed at all to classify the data with high probability. In the ""hard"" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. We evaluate our theoretical results on synthetic and real-world data. ",Graph Attention Retrospective
19,1498400529843908609,96779364,Arnab Bhattacharyya,"['New paper: <LINK> (Universal 1-Bit Compressive Sensing for Bounded Dynamic Range Signals) with @SidhantBansal6, Anamay Chaturvedi, and @j_m_scarlett.', ""1-bit CS is a variant of compressive sensing where you get only one (!) bit of information per measurement. Here, if x is the unkown signal, a measurement gives you a ¬±1 value that's the sign of &lt;v,x&gt; where v is the sensing vector."", 'You can design the sensing vectors v_1, ..., v_m. The goal is to recover x (upto scaling) while minimizing m. \n\nWe looked at the setting where x has bounded dynamic range. For simplicity here, assume that x is boolean. How many 1-bit measurements do you need to recover x?', ""In our setting, the goal is to fix v_1, ..., v_m once and for all, and then guarantee that they allow you to recover any boolean x.\n\nIt's known that ~k^1.5 measurements suffice where k is the # of nonzeros. You get this by just using iid gaussian sensing vectors."", 'Curiously, we only know the trivial lower bound of k on the number of 1-bit CS measurements needed.\n\nIn this work, we showed that if the sensing vectors are iid gaussian, then with constant probability, you need ~k^1.5 measurements.', 'The message is that either k^1.5 measurements is ~optimal or you need more sophisticated measurement schemes. I have no idea which is the truth.', ""The key tool we used (that I learned from @j_m_scarlett) for the analysis is de Caen's inequality. It's sort of a partial converse to the union bound. It doesn't seem to be all that known in the TCS community but for us here, it was essential. https://t.co/e700TdXNfe"", 'On a personal note, this has been a pet problem for me for 5+ yrs. Satisfying to see some progress finally! @SidhantBansal6 did most of the technical work.\n\nBtw, check out this recent tweet thread on related work: https://t.co/4Cfy8HIRQJ.']",https://arxiv.org/abs/2202.10611,"A {\em universal 1-bit compressive sensing (CS)} scheme consists of a measurement matrix $A$ such that for all signals $x$ belonging to a particular class, $x$ can be approximately recovered from $\textrm{sign}(Ax)$. 1-bit CS models extreme quantization effects where only one bit of information is revealed per measurement. We focus on universal support recovery for 1-bit CS in the case of {\em sparse} signals with bounded {\em dynamic range}. Specifically, a vector $x \in \mathbb{R}^n$ is said to have sparsity $k$ if it has at most $k$ nonzero entries, and dynamic range $R$ if the ratio between its largest and smallest nonzero entries is at most $R$ in magnitude. Our main result shows that if the entries of the measurement matrix $A$ are i.i.d.~Gaussians, then under mild assumptions on the scaling of $k$ and $R$, the number of measurements needs to be $\tilde{\Omega}(Rk^{3/2})$ to recover the support of $k$-sparse signals with dynamic range $R$ using $1$-bit CS. In addition, we show that a near-matching $O(R k^{3/2} \log n)$ upper bound follows as a simple corollary of known results. The $k^{3/2}$ scaling contrasts with the known lower bound of $\tilde{\Omega}(k^2 \log n)$ for the number of measurements to recover the support of arbitrary $k$-sparse signals. ",Universal 1-Bit Compressive Sensing for Bounded Dynamic Range Signals
20,1498323533281251330,48174958,Semon Rezchikov,"['Announcing a new paper:\n""Renormalization Group Flow as Optimal Transport""\n joint with @jordancotler!\n\n<LINK>\n\nWe describe a systematic connection between renormalization and entropy minimization. This opens the door to new machine learning methods in physics.', '{The paper was posted on Thursday, which was a sad day. \n\nScience churns while the world burns... üá∫üá¶}', 'Renormalization is the idea that as we look at a system with different levels of precision, measurements of fundamental parameters (mass, charge) should effectively change. Renormalization group (RG) flow refers to dependence of parameters on scale/precision.', 'The physical idea is that you measure electron mass by shooting something at it and seeing deflection; but a high energy probe produces a virtual particle cloud near the electron, which screens it. Thus mass depends on probe energy.\n\nThis leads to lots of confusing math.', 'A key intuition of our paper is that renormalization ""blurs out"" fields, and /so should increase ""entropy"" of the field theory/. The point of the paper is to present a new perspective on RG in which this intuitive claim can be made rigorous, at the level of physics.', 'This idea has been explored in other contexts (both heuristically and in specialized settings), but it is exciting to make it precise and general.  The key turns out to be connecting RG to optimal transport!', ""A motivation for us was Zamolodchikov's c-theorem, which gives a quantity for 2d field theories that decreases under RG flow. This had lots of impact on conformal field theory. There are other examples of `c-theorems';  it is an active topic."", 'Our paper provides, for conventional field theories in all dimensions and a vast class of RG schemes, a completely different class of monotones based on relative entropies!', ""The basic idea is that RG flow has a formulation as the ``Polchinski equation'' [Polchinski, ‚Äò84], which is formally similar to an infinite-dimensional heat equation. Everyone knows that entropy increases under heat flow."", ""Thus, it is natural to look for an analog of the entropy of the probability functional of the field theory. \n\nThe punchline is that this can be done. This `entropy' is badly divergent, but we provide a natural and general regularization."", 'A more general formulation of RG flow is the Wegner-Morris equation, which parametrizes possible RG schemes in terms of a quantity S_{seed}, the seed action. Taking S_{seed} = 2S_{Free} recovers the Polchinski equation.', 'Our paper shows that we can write RG flow as the *gradient flow* of relative entropy of P with respect to Q_Lambda = e^{-2S_{seed}} / [Normalization], where S_{seed} is the seed action.\n\nThis clarifies the meaning of the seed action, which was previously obscure in general!', 'To take a gradient flow of a function we need a metric. For us it is a field theory analog of the Wasserstein metric from optimal transport (OT). Optimal transport is about how best to transport mass from one distribution to another. The mathematics of OT is still evolving! https://t.co/JVR8KUcjy8', 'Optimal transport is mostly understood in the finite-dimensional setting. Famously, OT can establish the log-Sobolev inequality, which is a *dimension-independent* Sobolev inequality that was originally motivated by constructive quantum field theory! Ideas come full circle.', 'We outline the simplest possible algorithm and already see that a few remarkable simplifications occur to make SGD methods theoretically feasible.\n\nThere is much more structure to be understood!', 'There have been previous attempts to connect the renormalization group and machine learning. We show how to build neural networks which compute RG flows of conventional field theories. This general, systematic connection with conventional field theories is a first in the field.', 'We have been giving talks to physics audiences and there is a lot of excitement. Many connections have been suggested: to AdS-CFT, to optimal transport, and to general relativity, etc.', 'I am most excited about the numerical applications, which I think go far beyond the initial ideas of the paper, and should lead to efficient numerical ansatzae for finding ground states of field theories via variational methods.', ""If you are interested in physics, machine learning, optimal transport, or mathematical physics, reach out and I'd be happy to give a talk, offer references, or share ideas! \n\nI think there is a lot to explore here!"", ""Big thanks to Yair Shenfeld for re-catalyzing our interest in these questions in the fall of 2021 ‚Äì Jordan and I had initially worked on the idea *BACK IN 2018*, but only recently did we pursue the story to its conclusion.\n\nIt's great to see this come to light!"", '@johncarlosbaez @yasamanbb may be interesting']",https://arxiv.org/abs/2202.11737,"We establish that Polchinski's equation for exact renormalization group flow is equivalent to the optimal transport gradient flow of a field-theoretic relative entropy. This provides a compelling information-theoretic formulation of the exact renormalization group, expressed in the language of optimal transport. A striking consequence is that a regularization of the relative entropy is in fact an RG monotone. We compute this monotone in several examples. Our results apply more broadly to other exact renormalization group flow equations, including widely used specializations of Wegner-Morris flow. Moreover, our optimal transport framework for RG allows us to reformulate RG flow as a variational problem. This enables new numerical techniques and establishes a systematic connection between neural network methods and RG flows of conventional field theories. ",Renormalization Group Flow as Optimal Transport
21,1498298166894022657,356223194,Pieter Claeys,"['New paper on arXiv: we further explore the connection between dual-unitarity and random matrix theory, this time combining unitary circuit dynamics with projective measurements. Joint work with @AustenLamacraft (as is tradition). A short thread.\n\n<LINK>', 'Recent works introduced the notion of a projected ensemble: given a quantum state defined on a subsystem+bath we can perform projective measurements on the bath only, returning subsystem states with some fixed probability.', 'Ho and Choi recently showed that after quench dynamics in the dual-unitary kicked Ising model, the resulting projected ensemble quickly becomes indistinguishable from the uniform Haar-random distribution. https://t.co/n2tgf29mkR', 'Averaging over measurement outcomes is here equivalent to averaging over Haar-random states, leading to an exact quantum state design (for an infinite bath, after a surprisingly short time). We wanted to check the role of dual-unitarity, which was not immediately clear to us. https://t.co/waVeNoqVYH', ""Interestingly, dual-unitarity alone does not guarantee such an emergent quantum state design! It needs to be supplemented by a 'solvable measurement scheme', a notion which we here introduce and motivate."", 'For the kicked Ising model, it turns out that measurements in the computational basis form a solvable measurement scheme, but more involved measurement schemes can be found for all dual-unitary models (e.g. measuring Bell pair states).', 'Using MPS calculations we verified that after a time equal to the subsystem size all k-moments of the resulting ensembles collapse to the moments of the Haar-random uniform distribution, and contrast this with generic gates/measurements. https://t.co/KIdX55LAfK', '(As an aside, @Arrr______ recently published a preprint on the emergence of approximate k-designs following thermalization to high temperature. Check it out!)', 'The main message is that dual-unitary circuits, while exactly solvable, can be analytically shown to exhibit the random matrix behavior we expect in chaotic quantum models ‚Äî and which is typically absent in solvable models.', 'As a neat corollary, we also found some new classes of dual-unitary gates that behave in the same way as the kicked Ising model. Comments and feedback welcome! https://t.co/axKnjHH6VL', '@Arrr______ It depends! For KIM-like gates the solvable basis is product states in the computational basis, but this hinges on the special properties of these gates. General dual-unitary gates need more involved measurements, but these can be two-site product states (e.g. Bell states).']",https://arxiv.org/abs/2202.12306,"Recent works have investigated the emergence of a new kind of random matrix behaviour in unitary dynamics following a quantum quench. Starting from a time-evolved state, an ensemble of pure states supported on a small subsystem can be generated by performing projective measurements on the remainder of the system, leading to a projected ensemble. In chaotic quantum systems it was conjectured that such projected ensembles become indistinguishable from the uniform Haar-random ensemble and lead to a quantum state design. Exact results were recently presented by Ho and Choi [Phys. Rev. Lett. 128, 060601 (2022)] for the kicked Ising model at the self-dual point. We provide an alternative construction that can be extended to general chaotic dual-unitary circuits with solvable initial states and measurements, highlighting the role of the underlying dual-unitarity and further showing how dual-unitary circuit models exhibit both exact solvability and random matrix behaviour. Building on results from biunitary connections, we show how complex Hadamard matrices and unitary error bases both lead to solvable measurement schemes. ","Emergent quantum state designs and biunitarity in dual-unitary circuit
  dynamics"
22,1498227425754456067,1159066371319357440,omer goldman,"['üö®New preprint alert!üö®\nTake a break from doomscrolling and follow me.\nWe take morphology to the next level ‚Äì the clause-level!\n<LINK>\n\nIn this paper we (@rtsarfaty and myself) justify the move, provide data for 4 language, derive 3 tasks + baseline results. 1/', 'We all know of multi-lingual morphology tasks, right? Shared tasks with loads of data and much interest from the community in recent years.\nLots of languages + same task = universality. Right? 2/', 'Altho the data is really impressive in the number of languages it has, it still deals with words, and these are canny little bastards! Try to define them in a consistent way across languages and you‚Äôre in for a treat. Many linguists tried. 3/', 'Without a coherent word definition the structure and size of inflection tables vary on the basis of largely arbitrary factors.\nAre auxiliaries part of inflected forms? In a Finnish table ‚Äúolen ajatellut‚Äù is included, but English ‚Äúhave thought‚Äù isn‚Äôt. They mean the same thing! 4/', 'OK then, let‚Äôs remove all auxiliaries. Are we done now? Actually no. Because what if a language just writes auxiliaries without a whitespace? What differentiate an auxiliary from a morpheme? 5/', 'This is particularly tricky for languages that the take a single word for an English sentence. Not talking here about some extreme cases. In highly agglutinative languages the complete pred-arg structure is often expressed in a single word. Georgian, Swahili, Basque etc. etc. 6/', 'Here is an example for a fragment of a Swahili inflection table and its English equivalent.\n*no y‚Äôalls were harmed making this table* 7/ https://t.co/esKASZe8W3', 'Our solution: ignore words!\nWe fixed the set of inflectional features, and defined tables in all languages to include all possible combinations of features. No matter how many whitespaces needed. 8/ https://t.co/jfOH8WBrLF', 'The inflectional features set is the union of all language-specific features used in word-level inflectional data. 9/ https://t.co/xf8hEwPD7f', 'We present Mighty Morph, the first clause-level inflectional morphology dataset! Currently covering 4 languages: English, German, Turkish and Hebrew. We have isolating, fusional, and agglutinative lingos; with many, some, or no ablauts; w/ or w/o word order manipulations. 10/', 'The data reflects its universal definition. No more tiny tables for English and huge ones for Turkish. The number of features used per language and per form also better converge. 11/ https://t.co/DBL9unQNis', 'We defined 3 clause-level morphology tasks: inflection, reinflection and analysis. All modelled after the word-level tasks. We applied word-level models as baselines. And the cherry on top: a pretrained LM! Now we deal with short sentences, so PLMs can deal with them. 12/', 'Results: clause-level tasks are harder (more so for reinflection, less so for analysis that benefits from context). mT5 seems biased towards the Western languages (mostly English), and is generally not a silver bullet. 13/ https://t.co/wpMQnp1VJK', 'So what do we have? Data and tasks that are more universal and theoretically sound, more challenging and difficult, and enables probing of PLMs capabilities with morphologically rich languages. 14/', 'Interested in taking a stab at these tasks? Brace yourselves for a shared-task in the upcoming MRL workshop! With more languages and more fun!\n@_dataman_ @hila_gonen @mirzakhalov98 \n15/15', '@bpopuw @rtsarfaty DMing you, maybe you could help :)', ""@evanmiltenburg @_dataman_ @hila_gonen @mirzakhalov98 This is very interesting! I've heard mostly of distributed morphology (not an expert on that as well).\nIn a way our approach here is the opposite - expanding morphology on the expense of syntax. Lumping together larger pieces of text rather than splitting it.""]",https://arxiv.org/abs/2202.12832,"Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, that arise from the lack of a clear linguistic and operational definition of what is a word, and that severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features homogeneous across languages, that encapsulates all functions realized in a saturated clause. We deliver MightyMorph, a novel dataset for clause-level morphology covering 4 typologically-different languages: English, German, Turkish and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and can be used to probe LMs capacity to encode complex morphology. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphological modeling cross-linguistically. ",Morphology Without Borders: Clause-Level Morphological Annotation
23,1498194067896811522,1120577870403911680,Torben Ferber,"['Energetic neutron identification with pulse shape discrimination in pure CsI crystals is possible - new paper (<LINK>) with a great team at @KITKarlsruhe, @unihh and @desy, made possible with seed funding from @PIERCampus! <LINK>']",https://arxiv.org/abs/2202.12623,"Pulse shape discrimination with pure CsI scintillators is investigated as a method for separating energy deposits by energetic neutrons and photons at particle physics experiments. Using neutron data collected near the European XFEL XS1 beam window the pulse shape discrimination capabilities of pure CsI are studied and compared to CsI(Tl) using near-identical detector setups, which were operated in parallel. The inelastic interactions of 100MeV neutrons are observed to produce a slower scintillation emission in pure CsI relative to energy deposits from cosmic muons. By employing a charge-ratio method for pulse shape characterization, pulse shape discrimination with pure CsI is shown to be effective for identifying energy deposits from neutrons vs. cosmic muons, however, pure CsI was not able resolve the specific type of neutron inelastic interactions as can be done with CsI(Tl). Using pulse shape discrimination, the rate of energetic neutron interactions in a pure CsI detector is measured as a function of time and shown to be correlated with the European XFEL beam power. The results demonstrate that pulse shape discrimination with pure CsI has significant potential to improve electromagnetic vs. hadronic shower identification at future particle physics experiments. ","Energetic neutron identification with pulse shape discrimination in pure
  CsI crystals"
24,1497096758455078914,2390705126,Mausam (IITD),['What happens if two vibrant research groups co-organize a major conference? @k_leyton_brown (@ubc) and I (@iitdelhi) organized #AAAI2021. We developed a brand new reviewer-paper matching system. Read details of system plus loads of analysis on real data.   <LINK> <LINK>'],https://arxiv.org/abs/2202.12273,"This paper studies a novel reviewer-paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted by other conferences including AAAI 2022 and ICML 2022. This approach has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer-paper scores; (2) formulating and solving an optimization problem to find good reviewer-paper matchings; and (3) the introduction of a novel, two-phase reviewing process that shifted reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data -- including a comparison with the matching algorithm used in AAAI's previous (2020) iteration -- and supplements this with additional numerical experimentation. ",Matching Papers and Reviewers at Large Conferences
25,1497056043720798208,258540979,Kevin Leyton-Brown,"[""Our new paper describes &amp; evaluates #AAAI 21's reviewer-paper matching approach, just in time for #AAAI2022! AFAIK, it's one of the most exhaustive, real-data analyses of the optimization + ML algorithms behind a large CS/AI conference, ever. <LINK> @mishumausam"", 'Our approach: (1) combined many signals (pubs, keywords, bids, CoI, ML) to score every reviewer-paper pair; (2) found matchings via a novel MIP formulation; (3) shifted reviewing resources towards papers that needed the attention via a novel, two-phase reviewing process.', ""Exhaustive, real-data experiments evaluate (1) the scoring function's quality and robustness; (2) the MIP's performance, robustness, and comparison to alternatives; and (3) the extent to which the 2-phase design both met its objectives and avoided false negatives."", 'Our software is in use at #ICML2022 (@icmlconf); the two-phase reviewing methodology and various other elements of our design have also been adopted by #AAAI2022 (@RealAAAI) and #IJCAI2022 (@IJCAIconf). Let us know if you want to use it for your own conference!']",https://arxiv.org/abs/2202.12273,"This paper studies a novel reviewer-paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted by other conferences including AAAI 2022 and ICML 2022. This approach has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer-paper scores; (2) formulating and solving an optimization problem to find good reviewer-paper matchings; and (3) the introduction of a novel, two-phase reviewing process that shifted reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data -- including a comparison with the matching algorithm used in AAAI's previous (2020) iteration -- and supplements this with additional numerical experimentation. ",Matching Papers and Reviewers at Large Conferences
26,1497038810357674011,1556664198,Kyle Cranmer,"['The saga continues‚Ä¶ new paper using AI /ML for theoretical nuclear physics with the crew at MIT and @DeepMind. \n‚ÄúFlow-based sampling in the lattice Schwinger model at criticality‚Äù\nüßµ \n\n<LINK> <LINK>', 'We bring together two main threads of recent research. The first has to do with incorporating the symmetries found in fundamental particle physics (Lie groups) into a type of deep generative model called normalizing flows https://t.co/NHVCWgzBCp', 'The second is described in the thread below, where we figured out how to incorporate fermions (eg matter particles like electrons and quarks) https://t.co/JY1KHDFLya', 'Here we bring the two ingredients together to describe a simple model for how matter interacts with a force carrier known as the Schwander Model \nhttps://t.co/7ZJDVjjolD https://t.co/2nJ1s7nFAz', 'Fermions introduce ‚Äúlong range correlations‚Äù and there are different ‚Äúmodes‚Äù to the distribution characterized by a topological charge. It is hard for (Hamiltonian) Monte Carlo to transition between these sectors, but the flow-based sampling is orders of magnitude more efficient! https://t.co/kfK6CmpoRu', 'As always, it‚Äôs been a privilege to work with so many awesome people. I continue to learn a lot about physics &amp; ML. I don‚Äôt have handles for everyone, but a shout-out to @DaniloJRezende  @sracaniere @msalbergo @Julian_Urban @iaifi_news @NYUPhysics @NYUDataScience @DeepMind', 'Oh yeah, and don‚Äôt forget https://t.co/ms77K6ENej']",https://arxiv.org/abs/2202.11712,"Recent results suggest that flow-based algorithms may provide efficient sampling of field distributions for lattice field theory applications, such as studies of quantum chromodynamics and the Schwinger model. In this work, we provide a numerical demonstration of robust flow-based sampling in the Schwinger model at the critical value of the fermion mass. In contrast, at the same parameters, conventional methods fail to sample all parts of configuration space, leading to severely underestimated uncertainties. ",Flow-based sampling in the lattice Schwinger model at criticality
27,1497027555391520774,746440524052082688,Nicolas Delfosse,"['Floquet codes are well suited for Majorana qubits and they outperform surface codes! Check out our new paper with detailed planar layout and numerical simulations:\n<LINK>', 'Thanks a lot to my amazing collaborators with my Adam Paetznick, Christina Knapp, Bela Bauer, Jeongwan Haah, Matt Hastings, Marcus da Silva', 'See also this paper of Gidney and Newman:\nhttps://t.co/SjOqUpFrHn']",https://arxiv.org/abs/2202.11829,"Quantum error correction is crucial for any quantum computing platform to achieve truly scalable quantum computation. The surface code and its variants have been considered the most promising quantum error correction scheme due to their high threshold, low overhead, and relatively simple structure that can naturally be implemented in many existing qubit architectures, such as superconducting qubits. The recent development of Floquet codes offers another promising approach. By going beyond the usual paradigm of stabilizer codes, Floquet codes achieve similar performance while being constructed entirely from two-qubit measurements. This makes them particularly suitable for platforms where two-qubit measurements can be implemented directly, such as measurement-only topological qubits based on Majorana zero modes (MZMs). Here, we explain how two variants of Floquet codes can be implemented on MZM-based architectures without any auxiliary qubits for syndrome measurement and with shallow syndrome extraction sequences. We then numerically demonstrate their favorable performance. In particular, we show that they improve the threshold for scalable quantum computation in MZM-based systems by an order of magnitude, and significantly reduce space and time overheads below threshold. ",Performance of planar Floquet codes with Majorana-based qubits
28,1496907789368795145,45105022,Riccardo Sapienza,"['New paper from the group: ""Saturable time-varying mirror based on an ENZ material"" lead by @RomainTirole with @E_Galiffi @HybridNano @andrea_alu and many others <LINK>']",https://arxiv.org/abs/2202.05937,"We report a switchable time-varying mirror, composed of an ITO-Au stack, which can be efficiently modulated in time with over a ten-fold increase in reflectivity, with a change of 0.6. Upon interacting with the time-varying mirror, the frequency content of a reflected pulse is extended to 31 THz. This originates from the shortening of the response time of the mirror beyond saturation, as confirmed by a time-varying model and by further four-wave mixing experiments. A temporal response unbounded by the pump bandwidth opens new avenues for spectral manipulation from time-varying systems with impact for communication networks, optical switching and computing. ",Saturable time-varying mirror based on an ENZ material
29,1496876933279191041,3423739275,Felix Leditzky,"['A nice open problem that @ChristophHirche and I discuss in our new paper <LINK>: Find a quantum channel N such that both N and its complement N^c are PPT. Since PPT channels have zero quantum cap., such a ""bi-PPT"" channel has zero private capacity by our bound. <LINK>', 'Such ""bi-PPT"" channels would constitute a new class of zero-private-capacity channels, provided they are not also antidegradable. In turn, this could lead to observing super-activation of private capacity!', 'We found some numerical examples of approximately bi-PPT channels, i.e., both channels N and N^c are close in diamond norm to PPT channels. Interestingly, some of these examples are not completely trivial, i.e., they do have (small) positive quantum and private capacity.', '@quantum_graeme @ChristophHirche non-antidegradable =&gt; non-EB, right?', '@LamiLudovico @ChristophHirche We did, and the ones we checked have the complementary channel/state be quite entangled! (As in large log negativity, or distance from set of ppt channels/states.) Will check your channel too once I figure out the parameter choices...', '@LamiLudovico @ChristophHirche I guess the challenge (or way to approach this problem) is to look at some PPT constructions and see whether the constraint on the complementary state/channel can be incorporated. We could chat more about this at @QIPConference ü§ì', '@LamiLudovico @ChristophHirche Yes, applies to your state in (16) as well! Massive dimensions with this one, so only checked log negativity.', ""@markwilde @ChristophHirche I don't fully understand what you wrote down here, and let me know if I'm missing something, but:\na) you want the full state to be pure and rank constraints are non-convex; \nb) the 2-extendibility gives an antidegradable state which we want to avoid.""]",https://arxiv.org/abs/2202.11688,"Quantum capacities are fundamental quantities that are notoriously hard to compute and can exhibit surprising properties such as superadditivity. Thus, a vast amount of literature is devoted to finding tight and computable bounds on these capacities. We add a new viewpoint by giving operationally motivated bounds on several capacities, including the quantum capacity and private capacity of a quantum channel and the one-way distillable entanglement and private key of a quantum state. These bounds are generally phrased in terms of capacity quantities involving the complementary channel or state. As a tool to obtain these bounds, we discuss partial orders on quantum channels and states, such as the less noisy and the more capable order. Our bounds help to further understand the interplay between different capacities, as they give operational limitations on superadditivity and the difference between capacities in terms of the information-theoretic properties of the complementary channel or state. They can also be used as a new approach towards numerically bounding capacities, as discussed with some examples. ",Bounding quantum capacities via partial orders and complementarity
30,1496665656690978825,986718011661762561,Pin-Yu Chen,"['The #AAAI2022 AdvML4Good tutorial recording is available at  <LINK>\n\nAlso, check out the survey paper on model reprogramming, a data-efficient approach to reuse pre-trained models for solving new tasks\n<LINK> \n\n<LINK>']",https://arxiv.org/abs/2202.10629,"In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces the following challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper introduces a new technique called model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use cases, provides a theoretical explanation on the success of model reprogramming, and concludes with a discussion on open-ended research questions and opportunities. A list of model reprogramming studies is actively maintained and updated at this https URL ",Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning
31,1496619931613417479,1217628182611927040,Boaz Barak,"['1/9 New paper on ""pointwise"" approach to analyzing learning <LINK> \n\nWith @GalKaplun @nikhilghosh101  @saurabh_garg67 &amp; @PreetumNakkiran . \n\nThere\'s a web app too on <LINK>\n\nSome thoughts  below (though see paper for details) <LINK>', '2/9 Traditionally, ML classification/regression is considered a special case of statistical estimation. One measures performance of single empirical estimator with respect to population. In this paper we take dual approach: looking at single input wrt collection of estimators.', ""3/9 That is, we consider a collection of models parameterized by increasing resources such as compute (in this work) or data. Generally more resources = better *global* accuracy, but we track a single point's *profile*: how accuracy &amp; softmax probabilities evolve with resources. https://t.co/Z0cPIeYmyq"", ""4/9 This doesn't make much sense in classical statistics. E.g you don't measure opinion polls by how well they predict Joe Smith's vote. But much of modern ML don't make sense from classical statistics viewpoint. Have you ever seen a US opinion poll pretrained on voters in China?"", '5/9 Also, no standard statistical measure of distance can predict out-of-distribution performance of modern ML. \n\nE.g., ImageNet-R (renditions) is completely disjoint from ImageNet and yet Taori et al https://t.co/FoxkOXZwzu showed their performance is strongly correlated https://t.co/qzy8cymgfi', '6/9 We take one step further and remove the distributions from ""distribution shift"". Surprisingly, can extract meaningful signal from a single point. By tracking evolution of accuracy &amp; softmax probabilities as we increase resources, can see what&amp;when models are learning. https://t.co/i2cqZz1kJL', '7/9 Also see barriers for theory, including ""non Bayesian"" behavior such as non monotonicity of accuracy &amp; softmax entropy. In fact, we construct a CIFAR like dataset on which performance is *negatively correlated* with CIFAR performance. https://t.co/1s257lteVR', '8/9 Pre-training significantly ameliorates such non monotonicity, and in general pre-trained models, even at same global accuracy, behave in much more consistent ways, that are more compatible with Bayesian models. See Section 5 of paper for more https://t.co/nFvkzR5IUS https://t.co/2J4JHau2ey', ""9/9 Finally, this is just an initial investigation. We think there are many more insights to be extracted, and invite you to play around with our webapp https://t.co/ctfH0Yl4oD . Upload your own images, and see how models' view of them evolves.""]",https://arxiv.org/abs/2202.09931,"In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ""compatible"" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts ""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021) ",Deconstructing Distributions: A Pointwise Framework of Learning
32,1496321021812649986,1071640880,Petar Veliƒçkoviƒá,"[""New 4-page position paper, in defence of message passing! üï∏Ô∏èüåü\n\ntl;dr: most (if not all) GNNs that go 'beyond MP' can be expressed as MP; you just have to modulate the input graph. It could be trivial, useful, or insightful -- but seems always possible.\n\n<LINK> <LINK>"", ""While co- teaching my GNN course @Cambridge_Uni with @pl219_Cambridge, a point of debate arisen on what it means to go 'beyond message passing'. Lots of discussions later... I realised that using the term could be at best, imprecise; at worst, harmful to newcomers."", 'Hence, I wrote up a 4-page summary of the area in an attempt to seed a broader discussion! Hopefully it is a useful exercise, and hope it brings us new insights, regardless of how we decide to name these methods going forward. (I propose ""augmented MP"" to seed the discussion)', 'No matter how much I stand by this view, it is still a somewhat scary moment, putting it out there for public debate üò≥ I would like to thank all reviewers, students and course TAs for providing me with helpful comments and encouragement throughout writing this!', ""Tagging all of the amazing correspondents and course TAs here: \n@mmbronstein @PeterWBattaglia @andreeadeac22 @crisbodnar @DutaIulia @paulmorio @loukasa_tweet @DobrikG @chaitjo @chrsmrrs @HaggaiMaron @charlieharris01\n\nI hope you'll find it an enjoyable read! :)"", ""@gordic_aleksa Actually not! While MP is quite expressive, it is not always the optimal way to reason about the architecture at hand (though it could reveal useful connections). See my comments in the last subsection, related heavily to @HaggaiMaron's work on k-IGN.""]",https://arxiv.org/abs/2202.11097,"The message passing framework is the foundation of the immense success enjoyed by graph neural networks (GNNs) in recent years. In spite of its elegance, there exist many problems it provably cannot solve over given input graphs. This has led to a surge of research on going ""beyond message passing"", building GNNs which do not suffer from those limitations -- a term which has become ubiquitous in regular discourse. However, have those methods truly moved beyond message passing? In this position paper, I argue about the dangers of using this term -- especially when teaching graph representation learning to newcomers. I show that any function of interest we want to compute over graphs can, in all likelihood, be expressed using pairwise message passing -- just over a potentially modified graph, and argue how most practical implementations subtly do this kind of trick anyway. Hoping to initiate a productive discussion, I propose replacing ""beyond message passing"" with a more tame term, ""augmented message passing"". ",Message passing all the way up
33,1496180361650388992,1860759042,Leon Lang,"['<LINK>\nNew paper together with Pierre Baudot, Rick Quax, and Patrick Forr√©. We take the chain rule of information as a starting point for obtaining information diagrams like this: <LINK>', 'Let‚Äôs unpack what this is about. F1 is a general function satisfying the chain rule of information: https://t.co/jq2pgw9UcA', 'This should remind of the chain rule of Shannon entropy, which usually looks like this: https://t.co/6v25ObDK28', 'What are the inputs of F1?  Simply elements of what‚Äôs called an idempotent, commutative monoid, also known as join-semilattice ‚Äî a convenient generalization of collections of random variables together with their join operation.', 'Where does F1 map to? Into any abelian group ‚Äî the natural generalization of the set of information functions.\n\nWhat is the following notation about? https://t.co/dLkjF4Mw7M', 'This is the action of the monoid on the group ‚Äî the correct generalization of the concept of conditional information, where we condition on X.\n\nAnd what is F2? It is simply given by the following formula: https://t.co/F1cQHB26TG', 'This generalizes the usual construction of mutual information from entropy and conditional entropy: https://t.co/vetiGF2H3i', 'Diagrams such as the above then show how in this fairly general setting, ‚Äúinformation‚Äù as given by F1, F2, and higher generalizations F3, ‚Ä¶ , decomposes ‚Äî it summarizes all summation rules of information content that generally hold.', 'Thereby:\n- variables are replaced by sets\n- joint variables are replaced by unions\n- mutual information/interaction terms are replaced by intersections\n- conditioning is replaced by set difference.\n- summation rules can be read from disjoint unions in Venn diagrams.', 'Does this not just express in abstract language what‚Äôs long known for Shannon entropy? No, it goes beyond since other functions satisfy the pattern of F1 as well: we prove the existence of information diagrams giving rise to decomposition rules for the following functions:', 'Kolmogorov complexity, Tsallis entropy, Kullback-Leibler divergence, Tsallis (alpha)-Kullback-Leibler divergence, cross-entropy, general submodular information functions, and the generalization error from machine learning.', 'For Chaitin‚Äôs prefix-free Kolmogorov complexity, we can show that the higher interaction terms Kcq are in expectation close to Shannon‚Äôs interaction information, which we denote by Iq: https://t.co/miNHeEA5sn', 'Asymptotically, for increasing length of binary sequences and in well-behaved situations, we even obtain an equality of the ‚Äúper-bit‚Äù content of both sides of this equation. This shows a strong bond between algorithmic and classical information theory.', '@threadreaderapp unroll']",https://arxiv.org/abs/2202.09393,"In information theory, one major goal is to find useful functions that summarize the amount of information contained in the interaction of several random variables. Specifically, one can ask how the classical Shannon entropy, mutual information, and higher interaction information functions relate to each other. This is formally answered by Hu's theorem, which is widely known in the form of information diagrams: it relates disjoint unions of shapes in a Venn diagram to summation rules of information functions; this establishes a bridge from set theory to information theory. While a proof of this theorem is known, to date it was not analyzed in detail in what generality it could be established. In this work, we view random variables together with the joint operation as a monoid that acts by conditioning on information functions, and entropy as the unique function satisfying the chain rule of information. This allows us to abstract away from Shannon's theory and to prove a generalization of Hu's theorem, which applies to Shannon entropy of countably infinite discrete random variables, Kolmogorov complexity, Tsallis entropy, (Tsallis) Kullback-Leibler Divergence, cross-entropy, submodular information functions, and the generalization error in machine learning. Our result implies for Chaitin's prefix-free Kolmogorov complexity that the higher-order interaction complexities of all degrees are in expectation close to Shannon interaction information. For well-behaved probability distributions on increasing sequence lengths, this shows that asymptotically, the per-bit expected interaction complexity and information coincide, thus showing a strong bridge between algorithmic and classical information theory. ","Information Decomposition Diagrams Applied beyond Shannon Entropy: A
  Generalization of Hu's Theorem"
34,1496049149212606483,1069045039,daniele marinazzo,"['New paper!\n\nFrontal effective connectivity increases with task demands and time on task: a DCM study on electrocorticogram in macaque monkeys\n\n<LINK>\n\nby @katwegner, in collaboration with @crewilson , E. Procyk, @Frederikvds_ , K. Friston, @dimitrispp <LINK>']",https://arxiv.org/abs/2202.10021,"In this paper, we provide a computational account of changes in synaptic connectivity within two regions of the fronto-parietal network, the dorsolateral prefrontal cortex and the pre-supplementary motor area, applying Dynamic Causal Models to electrocorticogram recordings from two macaque monkeys performing a problem-solving task that engages working memory, and induces time-on-task effects. We find that forward connections between the two regions increased in strength when task demands were increased, and as the experimental session progressed. Similarities in the effects of task demands and time on task allow us to interpret changes in frontal connectivity in terms of increased effort allocation that compensates cognitive fatigue. ","Frontal effective connectivity increases with task demands and time on
  task: a DCM study on electrocorticogram in macaque monkeys"
35,1495857044439826433,106464999,Kat Volk,"['hey, do you need to know the free inclinations of things in the main classical Kuiper belt? (free inclinations are way more useful than ecliptic inclinations!) Check out our new paper led by @yukun_huang that has everything you need! <LINK>', ""I made a simple animation for a review paper to demonstrate the concept of free inclinations (it's based on a simplified analytical theory, so what Yukun did for our above paper is much more accurate!) https://t.co/0SWOEvfPmG"", ""the main concept is that the gravitational influence of the giant planets defines a plane about which a small body's orbit will wobble. The observed inclination relative to our arbitrary choice of the ecliptic as a reference plane will change a lot over time because of this..."", ""but if we calculate that plane set by the influence of the planets (which changes depending on the small body's other orbital parameters!) and remove it, we get an inclination that is much more stable over time and can reflect ...."", 'the original amount of orbital excitation that object experienced in the early solar system! So we can more easily tell the difference between things that have always been on very low-inclination orbits (cold classical Kuiper Belt Objects) and those that have been tossed around!', '@AlphaCatPA @yukun_huang lots and lots of things! and their amount of inclination tells us something about their history!', '@AlphaCatPA @yukun_huang lol, close! The more tipsy ones probably formed closer to the sun and got scattered out and implanted whereas the less tipsy ones probably formed where we see them today!', ""@AlphaCatPA @ItsMeDeaner yes! I'm going to have to look at those to see if they have them online! (Not that I need more pajama pants after my winter lands end spree.....)"", '@yukun_huang Thanks for doing such a great job getting it done and published so quickly!']",https://arxiv.org/abs/2202.09045,"There is a complex inclination structure present in the transneptunian object (TNO) orbital distribution in the main classical belt region (between orbital semimajor axes of 39 and 48 au). The long-term gravitational effects of the giant planets make TNO orbits precess, but non-resonant objects maintain a nearly constant 'free' inclination ($I_\text{free}$) with respect to a local forced precession pole. Because of the likely cosmogonic importance of the distribution of this quantity, we tabulate free inclinations for all main-belt TNOs, each individually computed using barycentric orbital elements with respect to each object's local forcing pole. We show that the simplest method, based on the Laplace-Lagrange secular theory, is unable to give correct forcing poles for objects near the $\nu_{18}$ secular resonance, resulting in poorly conserved $I_\text{free}$ values in much of the main belt. We thus instead implemented an averaged Hamiltonian to obtain the expected nodal precession for each TNO, yielding significantly more accurate free inclinations for non-resonant objects. For the vast majority (96\%) of classical belt TNOs, these $I_\text{free}$ values are conserved to $<1^\circ$ over 4 Gyr numerical simulations, demonstrating the advantage of using this well-conserved quantity in studies of the TNO population and its primordial inclination profile; our computed distributions only reinforce the idea of a very co-planar surviving 'cold' primordial population, overlain by a large $I$-width implanted 'hot' population. ",Free Inclinations for Transneptunian Objects in the Main Kuiper Belt
36,1495751010970705922,26111618,Franz Muheim,"['Paper from @LHCbPhysics submitted to <LINK>.  \nCongratulations to Guillaume @will_pietrak on the most precise measurement of charm mixing. For an explanation, see his thread <LINK>, and new #LHCb public page <LINK> <LINK>']",http://arxiv.org/abs/2202.09106,"A measurement of the ratios of the effective decay widths of $D^0 \to \pi^-\pi^+$ and $D^0 \to K^-K^+$ decays over that of $D^0 \to K^-\pi^+$ decays is performed with the LHCb experiment using proton-proton collisions at a centre-of-mass energy of $13 \, \mathrm{TeV}$, corresponding to an integrated luminosity of $6 \, \mathrm{fb^{-1}}$. These observables give access to the charm mixing parameters $y_{CP}^{\pi\pi} - y_{CP}^{K\pi}$ and $y_{CP}^{KK} - y_{CP}^{K\pi}$, and are measured as $y_{CP}^{\pi\pi} - y_{CP}^{K\pi} = (6.57 \pm 0.53 \pm 0.16) \times 10^{-3}$, $y_{CP}^{KK} - y_{CP}^{K\pi} = (7.08 \pm 0.30 \pm 0.14) \times 10^{-3}$, where the first uncertainties are statistical and the second systematic. The combination of the two measurements is $y_{CP} - y_{CP}^{K\pi} = (6.96 \pm 0.26 \pm 0.13) \times 10^{-3}$, which is four times more precise than the previous world average. ","Measurement of the charm mixing parameter $y_{CP} - y_{CP}^{K\pi}$ using
  two-body $D^0$ meson decays"
37,1495727559480389635,1169196060130123782,Gergely Neu,"['Over the last year, I\'ve spent a fair bit of time thinking about information-theoretic generalization bounds. In particular, what is exactly ""information-theoretic"" about them? \nWe provide some answers in a new paper w/@lugosi_gabor: <LINK>\n1/n <LINK>', 'We approach generalization bounds from the perpective of convex analysis, using the observation that the expected generalization error is a linear function of the joint input-output distribution, so it can be bounded via the Fenchel‚ÄìYoung inequality for any convex function H.\n2/n https://t.co/RgIfw7OljV', 'The main challenge is to understand the conditions for the conjugate H* to decrease with the sample size n. Our main result is identifying a class of dependence measures H satisfying a strong-convexity condition that allows such bounds on H*.\n3/n https://t.co/lNMOuOnVY2', 'This allows us to obtain a new generalization of the classic information-theoretic bounds that replaces the mutual information with a strongly convex dependence measure H. The bound depends on the regularity of the loss function as measured by a norm associated with H.\n4/n https://t.co/1JKBrgVcx4', 'Some obvious examples are the KL divergence (duh) and p-norm divergences. The cool thing about these is that they relax the subgaussianity assumption of the original information-theoretic bounds and continue to work for loss distributions with heavier tails.\n5/n https://t.co/IramxufHpA', 'The true power of our framework starts to show when considering more abstract dependence measures. One curious result that we managed to derive is a generalization bound stated for infinitely smooth loss functions in terms of the Wasserstein-2 distance.\n6/n https://t.co/76JYTLGrhO', ""Another interesting consequence is a generalization bound for SGD that gets rid of some terms that appeared in our COLT'21 paper (w/@HaghifamMahdi @KDziugaite &amp; @roydanroy), although at the price of some stronger assumptions on the loss.\n7/n https://t.co/Ip6lOlmcLp"", 'The proof of the main result is entirely based on elementary convex analysis tools that may be familiar from the modern analyses of online learning algorithms (see, e.g, the awesome lecture notes of @bremen79: https://t.co/Vtvkb4Edsn). There are some subtleties though...\n8/n https://t.co/KDa7D2njru', 'Proving that the linear term on the previous slide is zero is far from trivial. To show this, we needed to introduce a rather tricky construction for the feasible set Delta_n. (This is also what lead to the particular choice of H defined through conditional distributions.)\n9/n https://t.co/QJYX7v4I9o', 'This paper was fun to write, but it feels like just the first step in a new direction. Many questions remain unanswered: How do we find interesting dependence measures that are strongly convex? Can we prove PAC-Bayes-like high-probability bounds? etc..\n10/n', 'We are particularly curious to see how our result on Wasserstein distances fits into the bigger picture. We are not aware of any directly comparable result out there so we would *really* appreciate any feedback on this!!\n11/n https://t.co/gvOlDCxMY4', ""OK I'll stop there, hope some of you will find it interesting. Check out the paper and let us know what you think: https://t.co/5SarLuxvYa\n12/FIN"", '@ccanonne_ haha yes, this thread ended up being so long that i forgot it started with a question by the time i got to the end :D\nand yes, your reading is correct: the standard bounds can be recovered and extended via purely convex-analytic tools without any reference to information theory.', ""@ccanonne_ i wouldn't be able to confidently say that there is nothing information-theoretic about these bounds though, since i really don't know where the boundaries of information theory lie.."", '@ccanonne_ oh wow thank you Cl√©ment!']",https://arxiv.org/abs/2202.04985,"Since the celebrated works of Russo and Zou (2016,2019) and Xu and Raginsky (2017), it has been well known that the generalization error of supervised learning algorithms can be bounded in terms of the mutual information between their input and the output, given that the loss of any fixed hypothesis has a subgaussian tail. In this work, we generalize this result beyond the standard choice of Shannon's mutual information to measure the dependence between the input and the output. Our main result shows that it is indeed possible to replace the mutual information by any strongly convex function of the joint input-output distribution, with the subgaussianity condition on the losses replaced by a bound on an appropriately chosen norm capturing the geometry of the dependence measure. This allows us to derive a range of generalization bounds that are either entirely new or strengthen previously known ones. Examples include bounds stated in terms of $p$-norm divergences and the Wasserstein-2 distance, which are respectively applicable for heavy-tailed loss distributions and highly smooth loss functions. Our analysis is entirely based on elementary tools from convex analysis by tracking the growth of a potential function associated with the dependence measure and the loss function. ",Generalization Bounds via Convex Analysis
38,1495585935643136000,321794593,Jos√© G. Fern√°ndez-Trincado,['Our new accepted paper today on ArXiv by Lian et al. ‚ÄúQuantifying radial migration in the Milky Way: Inefficient over short timescales but essential to the very outer disc beyond ~15 kpc‚Äù üëâüèª<LINK>'],https://arxiv.org/abs/2202.08846,"Stellar radial migration plays an important role in reshaping a galaxy's structure and the radial distribution of stellar population properties. In this work, we revisit reported observational evidence for radial migration and quantify its strength using the age--[Fe/H] distribution of stars across the Milky Way with APOGEE data. We find a broken age--[Fe/H] relation in the Galactic disc at $r>6$ kpc, with a more pronounced break at larger radii. To quantify the strength of radial migration, we assume stars born at each radius have a unique age and metallicity, and then decompose the metallicity distribution function (MDF) of mono-age young populations into different Gaussian components that originated from various birth radii at $r_{\rm birth}<13$ kpc. We find that, at ages of 2 and 3 Gyr, roughly half the stars were formed within 1 kpc of their present radius, and very few stars ($<5$%) were formed more than 4 kpc away from their present radius. These results suggest limited short distance radial migration and inefficient long distance migration in the Milky Way during the last 3 Gyr. In the very outer disc beyond 15~kpc, the observed age--[Fe/H] distribution is consistent with the prediction of pure radial migration from smaller radii, suggesting a migration origin of the very outer disc. We also estimate intrinsic metallicity gradients at ages of 2 and 3 Gyr of $-0.061$ dex kpc$^{-1}$ and $-0.063$ dex kpc$^{-1}$, respectively. ","Quantifying radial migration in the Milky Way: Inefficient over short
  timescales but essential to the very outer disc beyond ~15 kpc"
39,1495425793547251729,1934060341,Jiaoyan Chen,"['A new survey and perspective paper on ""Knowledge Graph Reasoning with Logics and Embeddings: Survey and Perspective"", by @wencolani @ChenJiaoyan1 @jpansw @ChenHuajun etc. <LINK> #KnowledgeGraph #NeuralSymbolic #AI #DeepLearning #Reasoning', '@PMinervini @ejimenez_ruiz @wencolani @jpansw @ChenHuajun Thanks! Since this is not a comprehensive review paper but a survey and perspective paper (with citations limited to 2 pages), we may miss some relevant papers. We will consider this UAI paper if our paper is further extended.']",https://arxiv.org/abs/2202.07412,"Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions. ","Knowledge Graph Reasoning with Logics and Embeddings: Survey and
  Perspective"
40,1495414723096944641,234398193,Hal Tasaki,"['Hal Tasaki ""The Lieb-Schultz-Mattis Theorem: A Topological Point of View""\nMy latest paper is a semi-review (which contains some new results) of the celebrated LSM theorem for spin chains.  The paper is dedicated to Elliott Lieb, whom I sincerely respect.\n<LINK> <LINK>']",https://arxiv.org/abs/2202.06243,"We review the Lieb-Schultz-Mattis theorem and its variants, which are no-go theorems that state that a quantum many-body system with certain conditions cannot have a locally-unique gapped ground state. We restrict ourselves to one-dimensional quantum spin systems and discuss both the generalized Lieb-Schultz-Mattis theorem for models with U(1) symmetry and the extended Lieb-Schultz-Mattis theorem for models with discrete symmetry. We also discuss the implication of the same arguments to systems on the infinite cylinder, both with the periodic boundary conditions and with the spiral boundary conditions. For models with U(1) symmetry, we here present a rearranged version of the original proof of Lieb, Schultz, and Mattis based on the twist operator. As the title suggests we take a modern topological point of view and prove the generalized Lieb-Schultz-Mattis theorem by making use of a topological index (which coincides with the filling factor). By a topological index, we mean an index that characterizes a locally-unique gapped ground state and is invariant under continuous (or smooth) modification of the ground state. For models with discrete symmetry, we describe the basic idea of the most general proof based on the topological index introduced in the context of symmetry-protected topological phases. We start from background materials such as the classification of projective representations of the symmetry group. We also review the notion that we call a locally-unique gapped ground state of a quantum spin system on an infinite lattice and present basic theorems. This notion turns out to be natural and useful from the physicists' point of view. We have tried to make the present article readable and almost self-contained. We only assume basic knowledge about quantum spin systems. ",The Lieb-Schultz-Mattis Theorem: A Topological Point of View
41,1495002350070738953,314171681,Laura Baudis,"['Curios about the radioactivity of a banana and how to properly model its shape in a simulation based on Geant4? See this new paper by the Germanium Material and Meteorite Screening Experiment (GeMSE), operated in a shallow underground lab in Switzerland: <LINK> <LINK>']",https://arxiv.org/abs/2202.06540,"The GeMSE (Germanium Material and meteorite Screening Experiment) facility operates a low-background HPGe crystal in an underground laboratory with a moderate rock overburden of 620$\,$m.w.e. in Switzerland. It has been optimized for continuous remote operation. A multi-layer passive shielding, a muon veto, and a boil-off nitrogen purge line inside the measurement cavity minimize the instrument's background rate, which decreased by 33% to (164$\pm$2)$\,$counts/day (100-2700$\,$keV) after five years of underground operation. This agrees with the prediction based on the expected decay of short-lived isotopes. A fit to the known background components, modeled via a precise simulation of the detector, shows that the GeMSE background is now muon-dominated. We also present updates towards a more accurate detection efficiency calculation for the screened samples: the thickness of the crystal's outer dead-layer is precisely determined and the efficiency can now be easily calculated for any sample geometry. The advantage of this feature is showcased via the determination of the $^{40}$K content in the screening of a complex-shaped object: a banana. ","GeMSE: a Low-Background Facility for Gamma-Spectrometry at Moderate Rock
  Overburden"
42,1494770910762577922,573729628,"Steve Taylor, PhD","['New paper to end the week! Together with some friends and team members, we developed a **parallelized** Bayesian pipeline for GW Background characterization in Pulsar Timing Arrays. (1/3) \n<LINK>', 'This is in preparation for the torrent of new data from international combination efforts, and new data from CHIME and other upcoming radio telescopes. Modular and parallelized techniques help our searches scales much better with all the lovely new data. (2/3)', 'In this plot, the posterior constraints on the GW Background amplitude from many separate pulsars are contrasted against what one gets from a simple combination in post-processing. It agrees super well with the regular PTA likelihood! \n\n""Fix it in post"" as they say in showbiz üòÄ https://t.co/irSD0f0W4P']",https://arxiv.org/abs/2202.08293,"The characterization of nanohertz-frequency gravitational waves (GWs) with pulsar-timing arrays requires a continual expansion of datasets and monitored pulsars. Whereas detection of the stochastic GW background is predicated on measuring a distinctive pattern of inter-pulsar correlations, characterizing the background's spectrum is driven by information encoded in the power spectra of the individual pulsars' time series. We propose a new technique for rapid Bayesian characterization of the stochastic GW background that is fully parallelized over pulsar datasets. This Factorized Likelihood (FL) technique empowers a modular approach to parameter estimation of the GW background, multi-stage model selection of a spectrally-common stochastic process and quadrupolar inter-pulsar correlations, and statistical cross-validation of measured signals between independent pulsar sub-arrays. We demonstrate the equivalence of this technique's efficacy with the full pulsar-timing array likelihood, yet at a fraction of the required time. Our technique is fast, easily implemented, and trivially allows for new data and pulsars to be combined with legacy datasets without re-analysis of the latter. ","A Parallelized Bayesian Approach To Accelerated Gravitational-Wave
  Background Characterization"
43,1494720917288665091,1475191951490174977,Marisa Brienza,['Curious about how the most massive ellipticals of the nearby universe look like at 150 MHz? Check out our new paper!\n<LINK>'],https://arxiv.org/abs/2202.08593,"We study the properties and the origin of the radio emission in the most luminous early-type galaxies (ETGs) in the nearby Universe (MK<-25, recession velocity < 7,500 km/s) as seen by the 150 MHz Low-Frequency ARray (LOFAR) observations. LOFAR images are available for 188 of these giant ETGs (gETGs) and 146 (78%) of them are detected above a typical luminosity of ~10E21 W/Hz. They show a large spread in power, reaching up to ~10E26 W/Hz. We confirm a positive link between the stellar luminosity of gETGs and their median radio power, the detection rate, and the fraction of extended sources. About two-thirds (91) of the detected gETGs are unresolved, with sizes <4 kpc, confirming the prevalence of compact radio sources in local sources. Forty-six gETGs show extended emission on scales ranging from 4 to 340 kpc, at least 80% of which have a FRI class morphology. Based on the morphology and spectral index of the extended sources, ~30% of them might be remnant or restarted sources but further studies are needed to confirm this. Optical spectroscopy (available for 44 gETGs) indicates that for seven of them the nuclear gas is ionized by young stars suggesting a contribution to their radio emission from star forming regions. Their radio luminosities correspond to a star formation rate (SFR) in the range 0.1-8 Msun/yr and a median specific SFR of 0.8x10E-12 yr-1. The gas flowing toward the center of gETGs can accrete onto the supermassive black hole but also stall at larger radii and form new stars, an indication that feedback does not completely quench star formation. The most luminous gETGs (25 galaxies with MK < -25.8) are all detected at 150 MHz however they are not all currently turned on: at least four of them are remnant sources and at least one is likely powered by star formation. ","The LOFAR view of giant, early-type galaxies: radio emission from active
  nuclei and star formation"
44,1494717259171999751,2585907650,Leslie Smith,"['Please check out my new position paper ""General Cyclical Training of Neural Networks""\n<LINK>']",https://arxiv.org/abs/2202.08835,"This paper describes the principle of ""General Cyclical Training"" in machine learning, where training starts and ends with ""easy training"" and the ""hard training"" happens during the middle epochs. We propose several manifestations for training neural networks, including algorithmic examples (via hyper-parameters and loss functions), data-based examples, and model-based examples. Specifically, we introduce several novel techniques: cyclical weight decay, cyclical batch size, cyclical focal loss, cyclical softmax temperature, cyclical data augmentation, cyclical gradient clipping, and cyclical semi-supervised learning. In addition, we demonstrate that cyclical weight decay, cyclical softmax temperature, and cyclical gradient clipping (as three examples of this principle) are beneficial in the test accuracy performance of a trained model. Furthermore, we discuss model-based examples (such as pretraining and knowledge distillation) from the perspective of general cyclical training and recommend some changes to the typical training methodology. In summary, this paper defines the general cyclical training concept and discusses several specific ways in which this concept can be applied to training neural networks. In the spirit of reproducibility, the code used in our experiments is available at \url{this https URL}. ",General Cyclical Training of Neural Networks
45,1494708961517309953,2242856618,Agn√®s Fert√©,['Very excited about our new paper led by @PabloLemosP  and Noah Weaverdycküëè We compared the efficiency and accuracy of samplers for parameter inference and tension metrics. Turns out Multinest can misestimate Bayesian evidence üëâ we recommend Polychord   \n<LINK>'],https://arxiv.org/abs/2202.08233,"Recent cosmological analyses rely on the ability to accurately sample from high-dimensional posterior distributions. A variety of algorithms have been applied in the field, but justification of the particular sampler choice and settings is often lacking. Here we investigate three such samplers to motivate and validate the algorithm and settings used for the Dark Energy Survey (DES) analyses of the first 3 years (Y3) of data from combined measurements of weak lensing and galaxy clustering. We employ the full DES Year 1 likelihood alongside a much faster approximate likelihood, which enables us to assess the outcomes from each sampler choice and demonstrate the robustness of our full results. We find that the ellipsoidal nested sampling algorithm $\texttt{MultiNest}$ reports inconsistent estimates of the Bayesian evidence and somewhat narrower parameter credible intervals than the sliced nested sampling implemented in $\texttt{PolyChord}$. We compare the findings from $\texttt{MultiNest}$ and $\texttt{PolyChord}$ with parameter inference from the Metropolis-Hastings algorithm, finding good agreement. We determine that $\texttt{PolyChord}$ provides a good balance of speed and robustness, and recommend different settings for testing purposes and final chains for analyses with DES Y3 data. Our methodology can readily be reproduced to obtain suitable sampler settings for future surveys. ","Robust sampling for weak lensing and clustering analyses with the Dark
  Energy Survey"
46,1494592079661576231,1439689505850658819,Florian Beutler,['New paper out! \nDo you need the EFTofLSS power spectrum multipoles three orders of magnitude faster than standard codes out there? Have a look at the emulators my student Jamie just created \n<LINK>'],https://arxiv.org/abs/2202.07557,"In this paper we present an extension to the $\texttt{matryoshka}$ suite of neural network based emulators. The new editions have been developed to accelerate EFTofLSS analyses of galaxy power spectrum multipoles in redshift space. They are collectively referred to as the $\texttt{EFTEMU}$. We test the $\texttt{EFTEMU}$ at the power spectrum level and achieve a prediction accuracy of better than 1\% with BOSS-like bias parameters and counterterms on scales $0.001\ h\ \mathrm{Mpc}^{-1} \leq k \leq 0.19\ h\ \mathrm{Mpc}^{-1}$. We also run a series of mock full shape analyses to test the $\texttt{EFTEMU}$ at the inference level. Through these mock analyses we verify that the $\texttt{EFTEMU}$ recovers the true cosmology within $1\sigma$ at several redshifts ($z=[0.38,0.51,0.61]$), and with several noise levels (the most stringent of which being a Gaussian covariance associated with a volume of $5000^3 \ \mathrm{Mpc}^3 \ h^{-3}$). We compare mock inference results from the $\texttt{EFTEMU}$ to those obtained with a fully analytic EFTofLSS model and again find no significant bias, whilst speeding up the inference by three orders of magnitude. The $\texttt{EFTEMU}$ is publicly available as part of the $\texttt{matryoshka}$ $\texttt{Python}$ package this https URL ","$\texttt{matryoshka}$ II: Accelerating Effective Field Theory Analyses
  of the Galaxy Power Spectrum"
47,1494588441765556243,217199322,Pieter Belmans,"['New preprint just out, see <LINK> for the picture version on my blog, and <LINK> for the arXiv version.', ""@westernlands6 Thanks! I think you'll know for which proceedings volume it is :).""]",https://arxiv.org/abs/2202.08601,"The Segre cubic and Castelnuovo-Richmond quartic are two projectively dual hypersurfaces in $\mathbb{P}^4$, with a long and rich history starting in the 19th century. We will explain how Kuznetsov's theory of homological projective duality lifts this projective duality to a relationship between the derived category of a small resolution of the Segre cubic and a small resolution of the Coble fourfold, the double cover of $\mathbb{P}^4$ ramified along the Castelnuovo-Richmond quartic. Homological projective duality then provides a description of the derived categories of linear sections, which we will describe to illustrate the theory. The case of the Segre cubic and Coble fourfold is non-trivial enough to exhibit interesting behavior, whilst being easy enough to explain the general machinery in this special and very classical case. ",Homological projective duality for the Segre cubic
48,1494556915908571140,869876111336914944,Nana Liu,"['Very excited to finally be able to share our new paper `Quantum algorithms for computing observables of nonlinear partial differential equations‚Äô!! üòÄ üòÄ <LINK>', ""@chaoyanglu Thanks Chao-yang! I'm very excited about this new approach! :D"", '@qmisanz Thanks Mikel!!! How are you doing?? üòÑüòÑ', '@QPequi Thanks!', '@QPequi Oh I just realised it‚Äôs you Lucas!! How are you? Long time no see!! üòÑüòÑ', '@QPequi Fantastic to hear you are well Lucas and looking forward to seeing you again too!! üòÑüåà']",https://arxiv.org/abs/2202.07834,"We construct quantum algorithms to compute physical observables of nonlinear PDEs with M initial data. Based on an exact mapping between nonlinear and linear PDEs using the level set method, these new quantum algorithms for nonlinear Hamilton-Jacobi and scalar hyperbolic PDEs can be performed with a computational cost that is independent of M, for arbitrary nonlinearity. Depending on the details of the initial data, it can also display up to exponential advantage in both the dimension of the PDE and the error in computing its observables. For general nonlinear PDEs, quantum advantage with respect to M is possible in the large M limit. ","Quantum algorithms for computing observables of nonlinear partial
  differential equations"
49,1494525180462338048,1069244826,Preetum Nakkiran,"['New paper, short and sweet:\n""Limitations of Neural Collapse for Understanding Generalization in Deep Learning""\n<LINK>\nwith Like Hui, Misha Belkin.\n\nNeural collapse is often claimed to be, in some way, deeply relevant for generalization. But is it? 1/3 <LINK>', 'The literature is muddled because most work does not carefully distinguish between behavior on the train set (an *optimization* property) vs. behavior at test time (a *generation* property).\n\nWe clarify these issues by introducing more precise definitions of ""Neural Collapse"" 2/3 https://t.co/JZsr1bgMau', ""From these definitions, it's clear that Train Collapse may occur, but Test Collapse is often impossible. \n\nThus Neural Collapse is primarily an *optimization* phenomena, with unclear connections to generalization. See paper for more, incl cases where Collapse is undesirable.  3/3 https://t.co/dZqVjhIelz"", '@machinaut Good question, we argue in the paper that the right definition of ""collapse"" should be finite-sample, but infinite-time.\n(Because for infinite both, things just converge to bayes optimal, and are not interesting). https://t.co/xUqI5ySou3', ""@machinaut I don't know what happens to the representation-layer in grokking -- but now that you mention it, this sounds like a great experiment to try."", '@XYHan_ @GalantiTomer @weijie444 Thanks for the nice comments! And for the additional refs‚Äî I‚Äôll take a look.']",https://arxiv.org/abs/2202.08384,"The recent work of Papyan, Han, & Donoho (2020) presented an intriguing ""Neural Collapse"" phenomenon, showing a structural property of interpolating classifiers in the late stage of training. This opened a rich area of exploration studying this phenomenon. Our motivation is to study the upper limits of this research program: How far will understanding Neural Collapse take us in understanding deep learning? First, we investigate its role in generalization. We refine the Neural Collapse conjecture into two separate conjectures: collapse on the train set (an optimization property) and collapse on the test distribution (a generalization property). We find that while Neural Collapse often occurs on the train set, it does not occur on the test set. We thus conclude that Neural Collapse is primarily an optimization phenomenon, with as-yet-unclear connections to generalization. Second, we investigate the role of Neural Collapse in feature learning. We show simple, realistic experiments where training longer leads to worse last-layer features, as measured by transfer-performance on a downstream task. This suggests that neural collapse is not always desirable for representation learning, as previously claimed. Finally, we give preliminary evidence of a ""cascading collapse"" phenomenon, wherein some form of Neural Collapse occurs not only for the last layer, but in earlier layers as well. We hope our work encourages the community to continue the rich line of Neural Collapse research, while also considering its inherent limitations. ","Limitations of Neural Collapse for Understanding Generalization in Deep
  Learning"
50,1494160412669276164,125145954,David Ubilava,"['In this new/updated working paper, we confirm the pro-cyclicality of political violence with agricultural income (via cereal prices) and show that the effect is *seasonal* insomuch as the violence occurs during the first few months following the harvest: <LINK>', 'We arrive to this conclusion by analyzing 24 years of monthly data from January 1997 to December 2020, covering 2538 one-degree grid cells across 51 African economies. https://t.co/NKSQ85XEhY', 'In terms of magnitude, we estimate a three-to-four percent increase in the probability of violence in each of the first three months after harvest, relative to the baseline probability of violence in the cropland. https://t.co/oJU9a4X5Xv', 'Notably, the cumulative effect in the first three months almost adds up to the yearly effect had we not accounted for the seasonality. https://t.co/5R3uyB3D5a', 'Our finding accords with the rapacity mechanism of conflict. This mechanism has been suggested by others (e.g., Berman and Couttenier, 2015; McGuirk and Burke, 2020), though they have not investigated the seasonality of the effect, we do.', 'Agricultural income is seasonal, so the seasonality of violence in largely agrarian regions is expected. To test the mechanism, we interacted the seasonal income shock with the crop growing season weather. Indeed, there is more violence after plausibly richer crop harvest. https://t.co/3NHysPr8bt', 'This finding is important for several reasons, including pointing to a likely temporal displacement of agricultural income-related conflict in the cropland of Africa.', '@grant_mcdermott Thanks, Grant!\n\nAdmittedly, my favourite aspect of the ptoject :)', '@ArielOrtizBobea @grant_mcdermott Thanks, Ariel! Btw, those are done using ggplot... just saying :)']",https://arxiv.org/abs/2202.07863,"We study the seasonality of violence against civilians in the cropland of Africa. We combine 24 years of monthly international cereal prices with one-degree grid-cell level cropland area and harvest data, and nearly 56 thousand unique incidents, to investigate the relationship between seasonal agricultural income shocks and political violence staged by armed groups. We find that a one standard deviation price growth of the major cereal crop results in a monthly four percent increase in violence against civilians by political militias in the cropland of Africa during the early post-harvest season. The effect accrues to more than a ten percent annual increase in violence, nearly all of which is manifested during the first three months of the crop year. Other important actors, state forces and rebel groups in particular, do not appear to be engaging in seasonal conflict related to agricultural income shocks. In further investigating the mechanism, we show that the violence by political militias amplifies after plausibly rich harvest seasons. Because incidents peak during the early post-harvest season when the value of spoils to be appropriated is highest, we suggest rapacity as the key mechanism in the nexus of income shocks and political violence in the cropland of Africa. The study contributes to the growing but ambiguous literature on the economic causes of conflict in agrarian societies with weak institutions, and offers an important temporal nuance to this relationship. ","Agricultural Windfalls and the Seasonality of Political Violence in
  Africa"
51,1494131604809265152,983857052840636417,Rob Corless,"['Ok, as I hinted earlier in the week, we have a new paper up on the arXiv.  Main result: we can explain the fractal edges of eigenvalue density plots of upper Hessenberg Toeplitz Bohemians!  @BohemianMatrix @LeiliRafiee @sigfpe  <LINK> <LINK>']",https://arxiv.org/abs/2202.07769,"A Bohemian matrix family is a set of matrices all of whose entries are drawn from a fixed, usually discrete and hence bounded, subset of a field of characteristic zero. Originally these were integers -- hence the name, from the acronym BOunded HEight Matrix of Integers (BOHEMI) -- but other kinds of entries are also interesting. Some kinds of questions about Bohemian matrices can be answered by numerical computation, but sometimes exact computation is better. In this paper we explore some Bohemian families (symmetric, upper Hessenberg, or Toeplitz) computationally, and answer some open questions posed about the distributions of eigenvalue densities. ",Bohemian Matrix Geometry
52,1494027582525911041,1454174228245266435,Simon Caby,"['""All we need is TIME!""\nOur new paper about unsupervised training of synaptic delays in a SNN\n<LINK>\n(Proudly releasing a 94% accuracy on MNIST in 2022).']",https://arxiv.org/abs/2202.07132,"A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection. ",Memory via Temporal Delays in weightless Spiking Neural Network
53,1494005661071335427,238543224,Dan Wilkins,"['New paper! We took a close look to figure out exactly what happened around a supermassive black hole to cause the bright flares that we saw echo behind the black hole last year. <LINK> (1/4)', 'We used the @NASANuSTAR and @ESA_XMM X-ray telescopes to measure the spectrum of the X-rays that are emitted from close to the black hole, in particular the fraction of those X-rays that we see reflected off of the disc of gas that us falling into the black hole. (2/4)', 'We find that as the flares are launched, the corona (which are the particles around the black hole that get super-heated to produce the X-ray emission) gets launched away from the black hole, and also cools down in the process of the bright flares being launched. (3/4)', 'We‚Äôve seen before that the corona can get accelerated away from the black hole during flares, but now we‚Äôre able to measure how its structure and temperature changes, pointing to how the magnetic fields around the black hole are responsible for the corona and the flares. (4/4)']",http://arxiv.org/abs/2202.06958,"We report on X-ray flares that were observed from the active galactic nucleus I Zwicky 1 (I Zw 1) in 2020 January by the NuSTAR and XMM-Newton observatories. The X-ray spectrum is well-described by a model comprised of the continuum emission from the corona and its reflection from the accretion disc around a rapidly spinning (a > 0.94) black hole. In order to model the broadband spectrum, it is necessary to account for the variation in ionisation across the disc. Analysis of the X-ray spectrum in time periods before, during and after the flares reveal the underlying changes to the corona associated with the flaring. During the flares, the reflection fraction drops significantly, consistent with the acceleration of the corona away from the accretion disc. We find the first evidence that during the X-ray flares, the temperature drops from 140(-20,+100)keV before to 45(-9,+40)keV during the flares. The profile of the iron K line reveals the emissivity profile of the accretion disc, showing it to be illuminated by a compact corona extending no more than 7(-2,+4)rg over the disc before the flares, but with tentative evidence that the corona expands as it is accelerated during the flares. Once the flares subsided, the corona had collapsed to a radius of 6(-2,+2)rg. The rapid timescale of the flares suggests that they arise within the black-hole magnetosphere rather than in the accretion disc, and the variation of the corona is consistent with the continuum arising from the Comptonisation of seed photons from the disc. ","Acceleration and cooling of the corona during X-ray flares from the
  Seyfert galaxy I Zw 1"
54,1493971052187242497,3313806489,Tim Roberts,"[""Busy teaching - so almost missed that it's NEW PAPER DAY.  In which my excellent ex-PhD student @RajAstron shows X-ray and optical emission don't always change  concurrently in a pulsating ULX - so the optical emission could be from the secondary star. \n\n<LINK>""]",https://arxiv.org/abs/2202.06986,"NGC 1313 X-2 is one of the few known pulsating ultraluminous X-ray sources (PULXs), and so is thought to contain a neutron star that accretes at highly super-Eddington rates. However, the physics of this accretion remains to be determined. Here we report the results of two simultaneous XMM-Newton and HST observations of this PULX taken to observe two distinct X-ray behaviours as defined from its Swift light curve. We find that the X-ray spectrum of the PULX is best described by the hard ultraluminous (HUL) regime during the observation taken in the lower flux, lower variability amplitude behaviour; its spectrum changes to a broadened disc during the higher flux, higher variability amplitude epoch. However, we see no accompanying changes in the optical/UV fluxes, with the only difference being a reduction in flux in the near-IR as the X-ray flux increased. We attempt to fit irradiation models to explain the UV/optical/IR fluxes but they fail to provide meaningful constraints. Instead, a physical model for the system leads us to conclude that the optical light is dominated by a companion O/B star, albeit with an IR excess that may be indicative of a jet. We discuss how these results may be consistent with the precession of the inner regions of the accretion disc leading to changes in the observed X-ray properties, but not the optical, and whether we should expect to observe reprocessed emission from ULXs. ","A multi-wavelength view of distinct accretion regimes in the pulsating
  ultraluminous X-ray source NGC 1313 X-2"
55,1493968652940419074,29912873,Mike Brown,"['Cool new paper from my former summer student Anna Simpson (U Mich) on the arXiv yesterday. Basic question we were asking is: can we ever see the insides of Jupiter Trojan asteroids?\n<LINK>', 'What Jupiter Trojans -- those objects that @LucyMission is heading out to! -- are made out of is hard to crack. Their surfaces are dark and bland and all more or less alike (some a little more red, some a little less, but basically boring).', 'Their surfaces have been blasted by sun and solar wind for billions of years, so whatever on the inside is basically hidden under a layer of baked on residue. But what if we could crack them open?', 'Or, OK, what if we could at least find one that was recently cracked open? (Recently = a few tens of millions of years ago). Then the insides might be identifiable before they get cooked.', ""Good news is that asteroids collide and crack open all the time. Bad news is that we don't know of any recent impacts in the Trojans. So our best bet is to look at the smallest objects we can. There are many, so impacts are more frequent. So those were our targets."", 'We observed a set of objects using @almaobs and ZTF at the same time to give an accurate measurement of their size and reflectivity. Most of the ones we looked at were dark and bland. But not all of them!', 'Four (maybe 5) have surfaces that are 2-3 times brighter than all of the other Trojans. Why? Our hypothesis is that we are seeing recently cracked open objects and the interiors are brighter material.', ""So what is it? We can't tell from these observations, so this week t we are at @keckobs looking at these shinier-than-typical Trojans with spectroscopy to see if we can identify what makes them bright. Data are looking good, so stay tuned; we might actually know the answer soon!"", '(also Anna is an awesome student and you should definitely accept her into your program when she applies next year)', '@JonCzerwinski Crating a Trojan would be tough. Definitely cratered!']",https://arxiv.org/abs/2202.07066,"We use ALMA measurements of 870 $\mu$m thermal emission from a sample of mid-sized (15-40 km diameter) Jupiter Trojan asteroids to search for high albedo objects in this population. We calculate the diameters and albedos of each object using a thermal model which also incorporates {contemporaneous} Zwicky Transient Facility photometry to accurately measure the absolute magnitude at the time of the ALMA observation. We find that while many albedos are lower than reported from WISE, several small Trojans have high albedos independently measured both from ALMA and from WISE. The number of these high albedo objects is approximately consistent with expectations of the number of objects that recently have undergone large-scale impacts, suggesting that the interiors of freshly-crated Jupiter Trojans could contain high albedo materials such as ices. ","An ALMA search for high albedo objects among the mid-sized Jupiter
  Trojan population"
56,1493952291359150081,3047079964,Federico Sanchez,"['New paper about tau polarisation in neutrino tau interactions. It is a surprise but it has impact on the physics of the future DUNE experiment. In collaboration with Salamanca, Valencia and Mainz. Cool to  count some theorist friends. @DPNC_Unige @snf_ch <LINK>']",https://arxiv.org/abs/2202.07539,"Since the $\nu_\tau(\bar\nu_\tau) A_Z \to \tau^\mp X$ reaction is notoriously difficult to be directly measured, the information on the dynamics of this nuclear process should be extracted from the analysis of the energy and angular distributions of the tau decay visible products. These distributions depend on the components of the tau-polarization vector. We give, for the first time, the general expression for the outgoing hadron (pion or rho meson) energy and angular differential cross section for the sequential $\nu_\tau A_Z \to \tau^-(\pi^- \nu_\tau, \rho^-\nu_\tau) X$ and $\bar\nu_\tau A_Z \to \tau^+(\pi^+ \bar\nu_\tau, \rho^+ \bar\nu_\tau) X$ reactions. Though all possible nuclear reaction mechanisms contribute to the distribution, it may be possible to isolate/enhance one of them by implementing appropriate selection criteria. For the case of the quasi-elastic reaction off oxygen and neutrino energies below 6 GeV, we show that the pion distributions are sensitive to the details of the tau-polarization components. We find significant differences between the full calculation, where the longitudinal and transverse components of the tau polarization vector vary with the energy and the scattering angle of the produced tau, and the simplified scheme in which the polarizations are set to one and zero (respective asymptotic values in the high energy regime). In addition to its potential impact on neutrino oscillation analyses, this result can be used to further test different nuclear models, since these observables provide complementary information to that obtained by means of the inclusive nuclear weak charged-current differential cross section. We also study the effects on the cross section of the $W_4$, $W_5$ nuclear structure functions, which contributions are proportional to the charged lepton mass, and therefore difficult to constrain in muon and electron neutrino experiments. ","Tau longitudinal and transverse polarizations from visible kinematics in
  (anti-)neutrino nucleus scattering"
57,1493947651573825540,1063396774113628161,Nora Eisner,"['We have an exciting new Planet Hunters TESS paper on arXiv today (<LINK>)! As a heads up, there a zero planets in this system (but there are three stars). Thanks to all the @planethunters who helped identify this fun system.']",https://arxiv.org/abs/2202.06964,"We report the discovery and analysis of a massive, compact, hierarchical triple system (TIC 470710327) initially identified by citizen scientists in data obtained by NASA's Transiting Exoplanet Survey Satellite (TESS). Spectroscopic follow-up observations obtained with the HERMES spectrograph, combined with eclipse timing variations (ETVs), confirm that the system is comprised of three OB stars, with a compact 1.10 d eclipsing binary and a non-eclipsing tertiary on a 52.04 d orbit. Dynamical modelling of the system (from radial velocity and ETVs) reveal a rare configuration wherein the tertiary star (O9.5-B0.5V; 14-17 M$_{\odot}$) is more massive than the combined mass of the inner binary (10.9-13.2 M$_{\odot}$). Given the high mass of the tertiary, we predict that this system will undergo multiple phases of mass transfer in the future, and likely end up as a double neutron star gravitational wave progenitor or an exotic Thorne-Zytkow object. Further observational characterisation of this system promises constraints on both formation scenarios of massive stars as well as their exotic evolutionary end-products. ","Planet Hunters TESS IV: A massive, compact hierarchical triple star
  system TIC 470710327"
58,1493932297409732608,1435758910590394370,Aida Khajavirad,"['Check out our new paper on novel linear programming relaxations for rank-one Boolean Tensor factorization, where we found nice connections between this problem and our beloved polytope, aka the Multilinear polytope\n<LINK>\n@AlbertoDelPia']",https://arxiv.org/abs/2202.07053,"We consider the NP-hard problem of approximating a tensor with binary entries by a rank-one tensor, referred to as rank-one Boolean tensor factorization problem. We formulate this problem, in an extended space of variables, as the problem of minimizing a linear function over a highly structured multilinear set. Leveraging on our prior results regarding the facial structure of multilinear polytopes, we propose novel linear programming relaxations for rank-one Boolean tensor factorization. To analyze the performance of the proposed linear programs, we consider a random corruption model for the input tensor. We first consider the original NP-hard problem and establish information theoretic limits under the random model. Next, we obtain sufficient conditions under which the proposed linear programming relaxations recover the ground truth with high probability. Our theoretical results as well as numerical simulations indicate that certain facets of the multilinear polytope significantly improve the recovery properties of linear programming relaxations for rank-one Boolean tensor factorization. ",Rank-one Boolean tensor factorization and the multilinear polytope
59,1493902830394134528,953383684383608833,Wojciech Samek,['Want to evaluate your explanation?\nCheck out our new Quantus Toolbox.\ncode on github: <LINK>\npaper on arXiv: <LINK>\n@FraunhoferHHI @bifoldberlin'],https://arxiv.org/abs/2202.06861,"The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool exists that exhaustively and speedily allows researchers to quantitatively evaluate explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus - a comprehensive, open-source toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under open source license on PyPi (or on this https URL). ","Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural
  Network Explanations"
60,1493898228042973190,621147651,zpenoyre,"['new paper - <LINK> - a little over 20,000 astrometric binary(+) candidates in the local 100 parsecs <LINK>', 'these are Gaia sources with high astrometric error (specifically UWE) - most ubiquitously caused by binarity - figure from paper I - https://t.co/oRwduTBke8 https://t.co/vpxg9swgc9', ""Gaia is sensitive to binaries with a quite specific range of periods, months to decades - so we're only seeing 20-30% of all expected binary systems https://t.co/dyvunRBvJv"", ""all we have to work with is a single number, the error, not the full astrometric time-series - so the classification is approximate, and in June we'll be able compare our candidates to the few 100,000 non-single stars in Data Release 3 https://t.co/U9Uq1OQbMB https://t.co/aAbkmumP5d""]",https://arxiv.org/abs/2202.06963,"We examine the capacity to identify binary systems from astrometric deviations alone. We apply our analysis to the Gaia eDR3 and DR2 data, specifically the Gaia Catalogue of Nearby Stars. We show we must renormalize (R)UWE over the local volume to avoid biasing local observations, giving a Local Unit Weight Error (LUWE). We use the simple criterion of LUWE>2, along with a handful of quality cuts to remove likely contaminants, to identify unresolved binary candidates. We identify 22,699 binary candidates within 100 pc of the Sun (just under 10% of sources in this volume). We find an astrometric binary candidate fraction of around 20% for giant stars, 10% on the Main Sequence and lower than 1% for White Dwarfs. We also look for Variability Induced Movers, by computing the correlation between photometric variability and astrometric noise -- and show that VIMs may dominate the binary population of sub-Solar mass MS stars. We discuss the possibility and limitations of identifying non-luminous massive companions from astrometry alone, but find that our method is insensitive to these. Finally, we compare the astrometric deviations of MS binaries to the simulated sample from paper I, which show excellent agreement, and compute the astrometric candidate binary fraction as a function of absolute magnitude. ","Astrometric identification of nearby binary stars II: Astrometric
  binaries in the Gaia Catalogue of Nearby Stars"
61,1493896757104762883,271601706,Virginia Dignum,"['Happy to share with you my recent paper: <LINK>\non a new conceptualisation for #AI that focus less on rationalisation and optimisation aspects and more on the relational, societal collective impact.\nComments welcome :)\n#ResponsibleAI <LINK>', '@David_Gunkel @MCoeckelbergh Seems like an opportunity for collaboration :)', '@David_Gunkel @MCoeckelbergh @mitpress look forward to reading it', '@JoshGellers @David_Gunkel @MCoeckelbergh Thanks. In this initial paper, I am proposing a conceptualisation of AI, not AI ethics. I place the work in comparison to the well know rational/human-like classification proposed by Russell&amp;Norvig', '@MCoeckelbergh Will read with interest']",https://arxiv.org/abs/2202.07446,"The impact of Artificial Intelligence does not depend only on fundamental research and technological developments, but for a large part on how these systems are introduced into society and used in everyday situations. Even though AI is traditionally associated with rational decision making, understanding and shaping the societal impact of AI in all its facets requires a relational perspective. A rational approach to AI, where computational algorithms drive decision making independent of human intervention, insights and emotions, has shown to result in bias and exclusion, laying bare societal vulnerabilities and insecurities. A relational approach, that focus on the relational nature of things, is needed to deal with the ethical, legal, societal, cultural, and environmental implications of AI. A relational approach to AI recognises that objective and rational reasoning cannot does not always result in the 'right' way to proceed because what is 'right' depends on the dynamics of the situation in which the decision is taken, and that rather than solving ethical problems the focus of design and use of AI must be on asking the ethical question. In this position paper, I start with a general discussion of current conceptualisations of AI followed by an overview of existing approaches to governance and responsible development and use of AI. Then, I reflect over what should be the bases of a social paradigm for AI and how this should be embedded in relational, feminist and non-Western philosophies, in particular the Ubuntu philosophy. ",Relational Artificial Intelligence
62,1493825141670879232,1485386874013339655,Stephen A Walker,['New paper today! üòÄ\n\n<LINK>'],https://arxiv.org/abs/2202.07056,"We review the latest developments in our X-ray observational and theoretical understanding of the outskirts of galaxy clusters, and their connection to the cosmic web. The faint cluster outskirts are challenging regions to observe in X-rays, requiring highly sensitive telescopes with low and stable background levels. We present our latest understanding of the thermodynamic profiles of clusters in the outskirts, and the biases that gas clumping and non-thermal pressure support can introduce. Features in the outskirts due to merging activity are discussed, along with the chemical enrichment of the outskirts ICM. We describe future prospects for X-ray observations to explore further out in the cluster outskirts and probe their connections to the cosmic web. ",Cluster outskirts and their connection to the cosmic web
63,1493760957658677248,21540543,Grace Lindsay,"['New paper alert‚ö°Ô∏èIn this perspective, I provide an in-depth argument for the claim that we should test tools of neural analysis on artificial neural networks. This will help us be confident they can lead to progress on understanding the brain! <LINK> <LINK>', 'Specifically, I argue that ANNs are suitable as a testing grounds for 3 main reasons: their broad similarity to real neural networks, the fact they are fully observable &amp; perturbable, and the fact that we don\'t understand ""how they work"".', 'These properties makes it possible to quickly (compared to real neural networks!) determine if our analysis tools provide ""experimentally-validated"" understanding: that is, do they lead to insights that lead to experiments that validate those insights. https://t.co/QF5beNQoKV', 'Importantly, this process of tool-vetting would also force an explicit reflection on what we want our tools to do. That is, what kind of understanding are we aiming for? https://t.co/rBtNzRYdHJ', 'To make this argument concrete, I lay out a research plan for performing this testing, which involves applying common analysis tools to a variety of neural network types and noting the extent to which they provide experimentally-validated understanding. https://t.co/PS2vH1pJzN', 'Happy to hear any thoughts! /end', '@khademinori Did you read the paper I just posted?', '@khademinori I think it will be clear then. As an author on both, I can say they make quite different arguments.', ""@KriegeskorteLab @KordingLab I clearly missed an opportunity here https://t.co/JaOV4rSRba\n\n(btw my answer right now would be 'no' üò±)"", '@Isinlor Sorry, what do you mean by noise here? The activity of the neurons?', '@kendmil @VenkRamaswamy @kaymtye @KriegeskorteLab @KordingLab @tyrell_turing @ylecun @TonyZador I have heard this concern come up before with respect to applying the same tools. Here\'s where I address it: (last sentence ends ""...assumptions about our tools explicit, so that their application to real neural data is better understood."") https://t.co/hM1yYl2sM2', '@Isinlor Yea, I think that is a slightly different line of research than a lot of what systems neuroscientists are working towards---that is, sys neuroscis are usually starting with a ""trained"" system and just trying to understand it in that static state.', '@Isinlor But of course plenty of neuroscientists are trying to understand learning and development as well. And as it turns out, there has been working using ANNs to determine how well you can reverse-engineer a learning rule from observing the system: https://t.co/47wHcUl3Z6', '@Isinlor A lot more would need to be done in that direction to really move the needle on understanding biological learning, but it is interesting to see how tackling that question plays out in ANNs']",https://arxiv.org/abs/2202.07035,"Neuroscientists apply a range of common analysis tools to recorded neural activity in order to glean insights into how neural circuits implement computations. Despite the fact that these tools shape the progress of the field as a whole, we have little empirical evidence that they are effective at quickly identifying the phenomena of interest. Here I argue that these tools should be explicitly tested and that artificial neural networks (ANNs) are an appropriate testing grounds for them. The recent resurgence of the use of ANNs as models of everything from perception to memory to motor control stems from a rough similarity between artificial and biological neural networks and the ability to train these networks to perform complex high-dimensional tasks. These properties, combined with the ability to perfectly observe and manipulate these systems, makes them well-suited for vetting the tools of systems and cognitive neuroscience. I provide here both a roadmap for performing this testing and a list of tools that are suitable to be tested on ANNs. Using ANNs to reflect on the extent to which these tools provide a productive understanding of neural systems -- and on exactly what understanding should mean here -- has the potential to expedite progress in the study of the brain. ",Testing the Tools of Systems Neuroscience on Artificial Neural Networks
64,1493721854653046789,739069936019083265,Idan Attias,"['Happy to share with you a new paper with Steve Hanneke and @YishayMansour:\nA Characterization of Semi-Supervised Adversarially-Robust PAC Learnability.\n<LINK>', 'We show that a different combinatorial dimension (d1) controls the sample complexity of semi-supervised robust PAC learning. The dimension that controls the labeled sample complexity is the supervised case (d2) determines the unlabeled sample complexity for semi-supervised.', 'Moreover, d1&lt;=d2 and the gap can be arbitrarily large.\nWe present a generic method that uses an algorithm for partial concept classes and an algorithm for agnostic supervised robust learning.\nWe exhibit different behavior in realizable and agnostic cases.', 'More details are in the paper, feel free to reach out!', '*in the supervised case']",https://arxiv.org/abs/2202.05420,"We study the problem of semi-supervised learning of an adversarially-robust predictor in the PAC model, where the learner has access to both labeled and unlabeled examples. The sample complexity in semi-supervised learning has two parameters, the number of labeled examples and the number of unlabeled examples. We consider the complexity measures, $VC_U \leq dim_U \leq VC$ and $VC^*$, where $VC$ is the standard $VC$-dimension, $VC^*$ is its dual, and the other two measures appeared in Montasser et al. (2019). The best sample bound known for robust supervised PAC learning is $O(VC \cdot VC^*)$, and we will compare our sample bounds to $\Lambda$ which is the minimal number of labeled examples required by any robust supervised PAC learning algorithm. Our main results are the following: (1) in the realizable setting it is sufficient to have $O(VC_U)$ labeled examples and $O(\Lambda)$ unlabeled examples. (2) In the agnostic setting, let $\eta$ be the minimal agnostic error. The sample complexity depends on the resulting error rate. If we allow an error of $2\eta+\epsilon$, it is still sufficient to have $O(VC_U)$ labeled examples and $O(\Lambda)$ unlabeled examples. If we insist on having an error $\eta+\epsilon$ then $\Omega(dim_U)$ labeled examples are necessary, as in the supervised case. The above results show that there is a significant benefit in semi-supervised robust learning, as there are hypothesis classes with $VC_U=0$ and $dim_U$ arbitrary large. In supervised learning, having access only to labeled examples requires at least $\Lambda \geq dim_U$ labeled examples. Semi-supervised require only $O(1)$ labeled examples and $O(\Lambda)$ unlabeled examples. A byproduct of our result is that if we assume that the distribution is robustly realizable by a hypothesis class, then with respect to the 0-1 loss we can learn with only $O(VC_U)$ labeled examples, even if the $VC$ is infinite. ","A Characterization of Semi-Supervised Adversarially-Robust PAC
  Learnability"
65,1493657611362729989,10587552,Michael Bernstein,"[""Whose voice‚Äîwhose labels‚Äîis your AI learning to emulate? Groups often disagree in labeling comment toxicity, medicine, misinfo, design, &amp; others, so it matters whose labels the AI trains on. Enter @mitchellgordon's new CHI paper introducing Jury Learning: <LINK>"", 'Jury learning merges ML with the metaphor of jury selection: you make your values explicit by specifying which groups or intersectional identities, in what proportion, should determine the classifier‚Äôs prediction. https://t.co/EplEH08NRX', ""When given an input, the model samples appropriate jurors from the dataset of annotators, predicts each juror's response, and then aggregates those predictions into a classification, e.g., 7 to 5 in favor."", ""Here's a jury learning model for comment toxicity. Such a model might, for example, centrally feature Black and women jurors, who are commonly targets of harassment online. (In contrast, a large comment toxicity dataset is 74% white annotators.) https://t.co/DYukT32NF1"", 'In an evaluation, we recruited moderators to build jury learning models for their own online communities. Their models drastically increased representational diversity, and this altered *fourteen percent* of classification outcomes.', 'Thanks to @ComputerHistory @PJMFnd @StanfordHAI @Apple for their support of this work! Led by @mitchellgordon with talented coauthors @michelle123lam @joon_s_pk @foil @jeffhancock @tatsu_hashimoto', '@syardi @mitchellgordon Would love any feedback!']",https://arxiv.org/abs/2202.02950,"Whose labels should a machine learning (ML) algorithm learn to emulate? For ML tasks ranging from online comment toxicity to misinformation detection to medical diagnosis, different groups in society may have irreconcilable disagreements about ground truth labels. Supervised ML today resolves these label disagreements implicitly using majority vote, which overrides minority groups' labels. We introduce jury learning, a supervised ML approach that resolves these disagreements explicitly through the metaphor of a jury: defining which people or groups, in what proportion, determine the classifier's prediction. For example, a jury learning model for online toxicity might centrally feature women and Black jurors, who are commonly targets of online harassment. To enable jury learning, we contribute a deep learning architecture that models every annotator in a dataset, samples from annotators' models to populate the jury, then runs inference to classify. Our architecture enables juries that dynamically adapt their composition, explore counterfactuals, and visualize dissent. ","Jury Learning: Integrating Dissenting Voices into Machine Learning
  Models"
66,1493620024535306240,355616314,Murray Shanahan,"[""Just out - new paper with @MelMitchell1 on abstraction for deep reinforcement learning. It's part position statement, part survey.<LINK>""]",https://arxiv.org/abs/2202.05839,"We characterise the problem of abstraction in the context of deep reinforcement learning. Various well established approaches to analogical reasoning and associative memory might be brought to bear on this issue, but they present difficulties because of the need for end-to-end differentiability. We review developments in AI and machine learning that could facilitate their adoption. ",Abstraction for Deep Reinforcement Learning
67,1493545000218599425,106843613,Jacob Haqq Misra,"['Check out my new #technosignatures paper with @ravi_kopparapu\n@ThomasFauchez @AdamFrank4\n@Astro_Wright and Manasvi Lingam\n\n""Detectability of Chlorofluorocarbons in the Atmospheres of Habitable M-dwarf Planets""\n<LINK>']",https://arxiv.org/abs/2202.05858,"The presence of chlorofluorocarbons (CFCs) in Earth's atmosphere is a direct result of technology. Ozone-depleting CFCs have been banned by most countries, but some CFCs have persistent in elevated concentrations due to their long stratospheric lifetimes. CFCs are effective greenhouse gases and could serve as a remotely detectable spectral signature of technology. Here we use a three-dimensional climate model and a synthetic spectrum generator to assess the detectability of CFC-11 and CFC-12 as a technosignature on exoplanets. We consider the case of TRAPPIST-1e as well as a habitable Earth-like planet around a 3300 K M-dwarf star, with CFC abundances ranging from one to five times present-day levels. Assuming an optimistic James Webb Space Telescope (JWST) Mid Infrared Instrument (MIRI) low resolution spectrometer (LRS) noise floor level of 10 ppm to multiple co-added observations, we find that spectral features potentially attributable to present or historic Earth-level CFC features could be detected with a SNR $\ge 3-5$ on TRAPPIST-1e, if present, in $\sim 100$ hours of in-transit time. However, applying a very conservative 50 ppm noise floor to co-added observations, even a 5x Earth-level CFC would not be detectable no matter the observation time. Such observations could be carried out simultaneously and at no additional cost with searches for biosignature gases. Non-detection would place upper limits on the CFC concentration. We find that with the launch of JWST, humanity may be approaching the cusp of being able to detect passive atmospheric technosignatures equal in strength to its own around the nearest stars. ","Detectability of Chlorofluorocarbons in the Atmospheres of Habitable
  M-dwarf Planets"
68,1493496708869005321,400026483,Oem Trivedi,"[""I'm very happy to share that my new paper on Type V singularities with Prof. Maxim Khlopov is now out ! <LINK> We show that the occurence conditions of these singularities are almost the same in loads of non-standard theories as they are in a GR cosmology 1/n"", 'This is very surprising, because usually other types of singularities (Type I-IV) have a significant difference in their occurence conditions in non-standard cosmologies  from the conditions in general relativistic models. However we showed that 2/n', 'for 2 different types of scale factor ansatz, there is no difference in the occurence conditions when one considers an RS-II brane cosmology and particular types of modified area-entropy, generalized uncertainty principle, Chern-Simons and Holographically renormalized theories3/n', 'Only a type of f(R) gravity cosmology displays a bit of departure from a GR cosmology in the occurence conditions for w-singularities, that too only for one form of the scale factor ansatz. This goes to show that w-singularities remain largely untouched by the background theory!', 'Thank you for reading the thread and I hope you like the paper too !üôèüòá']",https://arxiv.org/abs/2202.06093,"Interest in cosmological singularities has remarkably grown in recent times, particularly on future singularities with the discovery of late-time acceleration of the universe and dark energy. Recent work has seen a proper classification of such singularities into strong and weak based on their strength, with weak singularities being the likes of sudden, w and big freeze singularities and strong singularities like the big rip. While there has been an expansive literature which has discussed the occurrence of Type I-IV in many non-standard cosmologies, w-singularities have not yet been explored in exotic cosmological settings. So in this work we pursue the same and discuss the status quo of w-singularities in a variety of non-standard cosmologies. We consider the RS-II Braneworld cosmology, an F(R) gravity cosmology which gives viable late time acceleration. We also consider cosmologies due to modified area-entropy relations, generalized uncertainty principles, holographic renormalization and Chern-Simons gravity( all of which can be coincidentally described by the same form of the modified Friedmann equation). We show that w-singularities will occur in exactly the same conditions in all these vividly different cosmological settings as they do in the usual general relativistic cosmology if one considers a power series expansion ansatz for the scale factor. We also show that if one considers an exponential form of the scale factor then while Type V singularities in the RS-II Braneworld and Chern-Simons cosmologies occur in the same conditions as in the standard cosmology case, there is a significant difference in the conditions when one considers the f(R) gravity case. These results are surprising overall, as one would usually not expect cosmological singularities to occur in almost the same conditions in non-standard cosmologies as they do in the usual standard cosmology. ",Type V singularities in non-standard cosmological backgrounds
69,1493483808036950018,199249094,Paul Villoutreix,"['New paper out! \nWe started with the question: how does the structure of the lymph node affects T-cells exploration? \nWe ended up with a new approach for the study of random walks (RW) on large networks and used it on an actual lymph node. 1/n\n<LINK> <LINK>', 'RW on networks are a widely used to model search strategies, transportation problems, or disease propagation. In this model, random walkers hop from node to node while choosing with an uniform probability the edges on which to travel.', 'The structure of the underlying network, such as its degree distribution and connectivity pattern, will thus determine how the RW evolves over time. Can this connectivity pattern favour the exploration of some nodes over others?', 'We propose a general framework to find network heterogeneities, which we define as connectivity patterns that affect the RW. We propose to characterize and measure these heterogeneities by i) ranking nodes and ii) detecting communities in a way that is RW interpretable. https://t.co/QwrqB7tYfr', 'Moreover, we propose iii) an approximation to accurately and efficiently compute these quantities on large networks.', 'We first applied our method on toy models, in particular, we showed its efficiency at contrasting two highly similar networks (identical degree distribution, same number of nodes) https://t.co/3Wy2HV8qgz', 'Moreover, we showed that none of our computations are redundant with previous centralities or random walk based measures (such as the global mean first passage time) https://t.co/4Ee4hUgjxm', ""Finally, we applied our methodology to characterize an actual lymph node obtained from Kelch et al. 2019. It's a large network containing about 200,000 nodes. So we had to use our approximation which appeared very accurate. https://t.co/AbMAhy0voG"", 'Our analysis suggests that the lymph node conduit network structure is highly homogeneous (a bit like a foam) and therefore promotes a uniform exploration of space by T-cells!', 'We developed an interactive visualisation platform if you want to explore these results \nhttps://t.co/KQKXgTUjaz', 'The credit of this work goes to Sol√®ne Song (@SongSolene), Malek Senoussi (@PtyFilou ) and Paul Escande! https://t.co/prATrApFTs']",https://arxiv.org/abs/2202.06729,"Random walks on networks are widely used to model stochastic processes such as search strategies, transportation problems or disease propagation. A prominent biological example of search by random walkers on a network is the guiding of naive T cells by the lymphatic conduits network in the lymph node. Motivated by this case study, we propose a general framework to find network heterogeneities, which we define as connectivity patterns that affect the random walk. We propose to characterize and measure these heterogeneities by i) ranking nodes and ii) detecting communities in a way that is interpretable in terms of random walk, moreover, we propose iii) an approximation to accurately and efficiently compute these quantities on large networks. The ranking parameter we propose is the probability of presence field, and the community detection method adapts previously defined diffusion coordinates. In addition, we propose an interactive data visualization platform to follow the dynamics of the random walks and their characteristics on our datasets, and a ready-to-use pipeline for other datasets upon download. We first showcase the properties of our method on toy models. We highlight this way the efficiency of our methods at contrasting two highly similar networks (identical degree distribution, same number of nodes). Moreover, we show numerically that the ranking and communities defined in this way are not redundant with any other classical methods (centralities, global mean first passage, louvain, node2vec). We then use our methods to characterize the lymph node conduits network. We show that the lymph node conduits network appears homogeneous and therefore has a global structure that promotes a uniform exploration of space by T-cells. ","Random walk informed community detection reveals heterogeneities in
  large networks"
70,1493433727875043328,119013247,Aditya Vijaykumar,"['New preprint paper on arxiv!\n\n<LINK>\n\nWe know that a class of lensed gravitational wave signals will be significantly distorted away from their un-lensed version. We ask:\n\n- can one measure these distortions, hence conclude lensing from just one image?\n\n(1/2) <LINK>', ""- if we don't account for the lensed effects, will we end up biasing the parameters that we infer from the signal?\n\nComments and questions are welcome!\n\n(2/2)""]",https://arxiv.org/abs/2202.06334,"Strong lensing can produce three types of images, denoted as Type-I, Type-II and Type-III corresponding to the minima, saddle and maxima of the time delay of the lensed images. Type-II images, in particular, receive a non-trivial phase shift of $\pi/2$. This phase shift can introduce additional distortions in the strains produced by the Type-II image of the binary black hole signals depending on the morphology of the signals, e.g., when they have contributions from higher harmonics, precession, eccentricity etc. The optical depth to the Type-II images is nearly the same as the strong lensing (from galaxies) and thus are very likely to be observed in the near future. In this work, we investigate the potential applicability of these distortions in helping identify Type-II signals from a single detection, and also the systematic biases that could arise in the inference of their parameters if they are unknowingly recovered with unlensed gravitational-wave templates. We show that at total network signal-to-noise ratio (SNR) $\rho=20(50)$, individual Type-II images should be identifiable with log Bayes factor $\ln \mathcal{B} > 2$ for inclinations $ \iota > 5 \pi/12 (\pi/3) $. Furthermore, based on the trends we observe in these results we predict that, at high SNRs ($\gtrsim 100$), individual Type-II images would be identifiable even when the inclination angle is much lower ($\sim \pi/6$). We then show that neglecting physical effects arising from these identifiable Type-II images can significantly bias estimates of parameters (such as sky location, distance, inclination, etc.), and sometimes lead to fully inaccurate measurement of these parameters. Thus, in the future, using Type-II lensed template for such signals would be necessary. ","Detection and parameter estimation challenges of Type-II lensed binary
  black hole signals"
71,1493324906443902976,60893773,James Bullock,"['A quick summary of thoughts on our new FIRE et al. paper showing that dark-matter-free low-mass galaxies  arise naturally and fairly frequently around massive galaxies in a cosmological-volume simulation. Paper led by @jorgito__moreno using FIREbox sim\n<LINK> <LINK>', 'van Dokkum and @DanieliShany discovery of low-mass dm-poor galaxies DF2 and DF4 (red bars) v. surprising, since low-mass galaxies are usually DM-dominated.  Remarkably we find several low-mass galaxies (yellow) in these sim have less DM than stars within their stellar radii. https://t.co/h0FF88if6W', 'Every one of them is a satellite of a massive (~1.e11 Mstar) host that is on an orbit that brought it within the core of the galaxy (~10 kpc from the center).  Much more DM lost than stars. https://t.co/P6vyzZsBxy', 'Most of them have very faint tidal features https://t.co/8zlzdGkiKX', 'This work follows a long line of work that has shown that close encounters between low-mass galaxies and massive hosts could do this kind of thing:  Haslbauer et al., Carleton et al., Sales et al., etc.', 'Our sims do a very good job reproducing many properties of DF2 and DF4: velocity dispersion, sizes, etc.  We predict that ~30% of massive hosts should have a satellite that is DM deficient.', 'One thing that still concerns me about our work is that we find our objects are still fairly metal rich compared to DF2 and DF4.  Could be scatter in Fe/H vs. Mstar -- will need to discover more DM-def. galaxies to test these things!', '@azifattahi @jorgito__moreno Yes, basically.  Here is relevant section of table. https://t.co/1TLuruwFFc']",https://arxiv.org/abs/2202.05836,"The standard cold dark matter plus cosmological constant model predicts that galaxies form within dark-matter haloes, and that low-mass galaxies are more dark-matter dominated than massive ones. The unexpected discovery of two low-mass galaxies lacking dark matter immediately provoked concerns about the standard cosmology and ignited explorations of alternatives, including self-interacting dark matter and modified gravity. Apprehension grew after several cosmological simulations using the conventional model failed to form adequate numerical analogues with comparable internal characteristics (stellar masses, sizes, velocity dispersions and morphologies). Here we show that the standard paradigm naturally produces galaxies lacking dark matter with internal characteristics in agreement with observations. Using a state-of-the-art cosmological simulation and a meticulous galaxy-identification technique, we find that extreme close encounters with massive neighbours can be responsible for this. We predict that approximately 30 percent of massive central galaxies (with at least 1e11 solar masses in stars) harbour at least one dark-matter-deficient satellite (with 1e8 - 1e9 solar masses in stars). This distinctive class of galaxies provides an additional layer in our understanding of the role of interactions in shaping galactic properties. Future observations surveying galaxies in the aforementioned regime will provide a crucial test of this scenario. ","Galaxies lacking dark matter produced by close encounters in a
  cosmological simulation"
72,1493250721050996740,741914576,Dr./Prof. Keri Hoadley,"[""After what felt like a very slow 2021 (a new job, a move across the country, feeling burnt out from 2020), it feels so great to see our paper on X-rays detected from the Blue Ring Nebula's central star (and the nebula, too?!) accepted and on arxiv! <LINK>""]",https://arxiv.org/abs/2202.05424,"Tight binary or multiple star systems can interact through mass transfer and follow vastly different evolutionary pathways than single stars. The star TYC 2597-735-1 is a candidate for a recent stellar merger remnant resulting from a coalescence of a low-mass companion with a primary star a few thousand years ago. This violent event is evident in a conical outflow (""Blue Ring Nebula"") emitting in UV light and surrounded by leading shock filaments observed in H$\alpha$ and UV emission. From Chandra data, we report the detection of X-ray emission from the location of TYC 2597-735-1 with a luminosity $\log(L_\mathrm{X}/L_\mathrm{bol})=-5.5$. Together with a previously reported period around 14~days, this indicates ongoing stellar activity and the presence of strong magnetic fields on TYC 2597-735-1. Supported by stellar evolution models of merger remnants, we interpret the inferred stellar magnetic field as dynamo action associated with a newly formed convection zone in the atmosphere of TYC 2597-735-1, though internal shocks at the base of an accretion-powered jet cannot be ruled out. We speculate that this object will evolve into an FK Com type source, i.e. a class of rapidly spinning magnetically active stars for which a merger origin has been proposed but for which no relic accretion or large-scale nebula remains visible. We also detect likely X-ray emission from two small regions close to the outer shock fronts in the Blue Ring Nebula, which may arise from either inhomogenities in the circumstellar medium or in the mass and velocity distribution in the merger-driven outflow. ","X-ray emission from candidate stellar merger remnant TYC 2597-735-1 and
  its Blue Ring Nebula"
73,1493216269016997891,835960362012971009,Michael Vasmer,"['üìúI have a new paper out on arXiv today!üìú\n<LINK>\nWe investigate the effects of Clifford errors in the 3D surface code, finding that they have minimal impact on the error thresholdüòÄ', 'Our simulations include the ""linking charge"" effect that is peculiar to codes with transversal non-Clifford gates, and we also give a simplified proof of its existence in the 3d color code.', ""Many thanks to my collaborators @ProfDanBrowne and Tom Scruby (who did the lion's share of the work on this project)."", '@Perimeter @QuantumIQC', ""@CraigGidney @kenbrownquantum Hi Craig, yes you're right that we don't simulate the CCZ. We get a random pattern of Pauli X operators from the |+&gt; state initialization. The rule for the CCZ is just: apply a Z on qubit q in code 1 if q has X errors in codes 2 and 3."", ""@CraigGidney Yeah good question, I just reran the analysis using bootstrap resampling and the stddevs are slightly larger (x2 in the worst case). Maybe one thing to clarify is that we don't use all the data in Fig6 for the fit; only the data in the vicinity of the threshold.""]",https://arxiv.org/abs/2202.05746,"A powerful feature of stabiliser error correcting codes is the fact that stabiliser measurement projects arbitrary errors to Pauli errors, greatly simplifying the physical error correction process as well as classical simulations of code performance. However, logical non-Clifford operations can map Pauli errors to non-Pauli (Clifford) errors, and while subsequent stabiliser measurements will project the Clifford errors back to Pauli errors the resulting distributions will possess additional correlations that depend on both the nature of the logical operation and the structure of the code. Previous work has studied these effects when applying a transversal $T$ gate to the three-dimensional colour code and shown the existence of a non-local ""linking charge"" phenomenon between membranes of intersecting errors. In this work we generalise these results to the case of a $CCZ$ gate in the three-dimensional surface code and find that many aspects of the problem are much more easily understood in this setting. In particular, the emergence of linking charge is a local effect rather than a non-local one. We use the relative simplicity of Clifford errors in this setting to simulate their effect on the performance of a single-shot magic state preparation process (the first such simulation to account for the full effect of these errors) and find that while they lead to an increase in the logical error rate their effect on the threshold is minimal. ",Non-Pauli Errors in the Three-Dimensional Surface Code
74,1493091160734433280,1078963404356767745,Prof. Jared Cole,"['Exciting new paper from @RMIT_Research and @excitonscience.\n<LINK>\nOptical interaction of the NV- centre in diamond with a plasmonic metal nanoparticle\n\nSummary of the what, why, who üßµ /1 <LINK>', 'Diamond has impurities whose spin state can be controlled with lasers and microwave fields. The most famous of these is the nitrogen-vacancy defect. This defect may be the key to new applications in quantum sensing and computing. /2', '*Very* small spheres of metal respond to lasers very differently than conventional metal.\nIn particular they have ‚Äúplasmonic resonances‚Äù which are particular wavelengths at which the sphere responds really strongly. /3', 'In this paper, we developed a new theoretical approach for describing what happens when you put them close to each other. This theory explains existing experiments, as well as suggests a range of new experiments which can be done with NV centres and metal nanoparticles. /4', 'Over the next year we will be working with experimental colleagues to test the limits of the theory, and to apply the ideas to new technology.\nThis is the excellent work of Dr. Harini Hapuarachchi, with (only a little) assistance from Dr. Francesco Campaioli and myself.\n/end']",https://arxiv.org/abs/2202.04854,"We present a rigorous theoretical model for the optical interaction between a nitrogen-vacancy (NV) centre in diamond and a plasmonic metal nanoparticle (MNP), accompanied by a computationally efficient procedure to solve the evolution. The proposed model enables us to successfully explain existing optical emission measurements of NV centres both in the presence and absence of a metal nanoparticle. We show that the NV-plasmon interaction provides a versatile new avenue to enhance and control the optical emission of an NV centre. Changes to the MNP type and size, NV-MNP centre separation, submerging medium permittivity, and NV orientation with respect to the MNP surface can be exploited to improve a plethora of NV centre based nanodevices. ","Optical interaction of the NV- centre in diamond with a plasmonic metal
  nanoparticle"
75,1492209892316237824,19465243,Hannah Earnshaw,"['New paper alert! üö® Just had a paper accepted into the Journal of Astronomical Telescopes, Instruments, and Systems on the development of a contingency plan should NuSTAR lose one of its metrology lasers. <LINK>', 'Why does NuSTAR need lasers? The telescope achieves its long focal length by being constructed of two benches - one holding the optics, the other holding the X-ray detectors (at the focal plane) - held together by a 10-meter-long mast. https://t.co/7rids9RRw4', ""That mast is made of carbon fiber, and it's pretty stable and rigid, but it's long enough that even very tiny movements over time can cause the image over on the focal plane bench to blur from the motion."", ""So NuSTAR also has two lasers on the optics bench, measured by a pair of detectors on the focal plane bench. How these laser points move over time then allows us to correct for the benches' motion relative to each other. But what happens if one of them fails? https://t.co/RAFgJcRWME"", ""We need two lasers to be able to detangle transverse\xa0(side-to-side or back-and-forth) motion from rotational motion as the mast twists. With only one, we can't fully describe the motion, and would end up with very distorted-looking point sources. üò¨ https://t.co/oEnSCLWxlM"", ""Thankfully, this motion is somewhat predictable - it's caused by thermal flexing as NuSTAR orbits the Earth, passing in and out of its shadow. The flexing also depends on the angle of the spacecraft to the Sun (the Solar aspect angle), affecting how it self-shadows."", ""Using the SAA and the phase of the spacecraft orbit, we're able to apply corrections to our single-laser mast aspect solution. This allows us to make an estimation of the mast motion good enough to make our point sources look like point sources again in most cases! https://t.co/Xrwxs97QZs"", ""We don't expect to be losing either of NuSTAR's lasers any time soon - but if something unexpected happens to one of them, we now have a backup plan. You can find more details in the paper: https://t.co/FuhCr2vSVr"", ""With thanks to @TheBrianGref, @MurrayBrightman and the rest of the @NuSTAR_Science team on and off Twitter! Here's to many more years of successful NuSTAR science!üõ∞Ô∏è""]",https://arxiv.org/abs/2202.04685,"This paper describes a method by which the metrology system of the Nuclear Spectroscopic Telescope Array (NuSTAR) X-ray space observatory, which uses two lasers to characterize the relative motion of the optics and focal plane benches, can be approximated should one laser fail. The two benches are separated by a ten-meter-long rigid mast that undergoes small amounts of thermal flexing which need to be compensated for in order to produce a non-blurred image. We analyze the trends of mast motion by archival observation parameters in order to discover whether the mast motion in future observations can be predicted. We find that, by using the solar aspect angle (SAA), observation date, and orbital phase, we can simulate the motion of one laser by translating the track produced by the other and applying modifications to the resulting mast aspect solution, allowing the reconstruction of a minimally distorted point spread function in most cases. We will implement the generation of simulated mast files alongside the usual NuSTAR data reduction pipeline for contingency purposes. This work has implications for reducing the risk of implementing laser metrology systems on future missions that use deployable masts to achieve the long focal lengths required in high-energy astronomy by mitigating the impact of a metrology laser failure in the extended phase of a mission. ","Reconstruction of the NuSTAR point spread function using single-laser
  metrology"
76,1492205281769631748,1053370661798924294,James Johnson,"['New paper day! We use galactic chemical evolution models to empirically constrain nitrogen yields, finding that net N yields must scale roughly linearly with the initial metallicity of the progenitor star cluster.\n\nThread! üßµ<LINK>', ""If the dependence of the N yield on metallicity Z is non-linear, then key observables like the [N/O]-[O/H] relation just don't come out right. See Fig. 6 in section 4.2: https://t.co/p92MusjO1s"", 'We challenge the popular interpretation that the [N/O]-[O/H] relation is an evolutionary sequence. In our model, it instead arises as a superposition of evolutionary endpoints - reminiscent of previous arguments made regarding the low-alpha sequence. See Fig. 5 in section 4.2: https://t.co/kj8aMUT8hL', 'If we add simple oscillations in either the star formation efficiency or the accretion rates to the model, it reproduces the scatter seen in [N/O] vs. [O/H] trends in MaNGA. See Fig. 10 in section 4.5: https://t.co/Aj82YsiIHF', 'Special thanks to @galaxyhistorian and @EmilyJoGriffith (as well as other twitter-less collaborators) for their insight on this project!']",https://arxiv.org/abs/2202.04666,"We derive empirical constraints on the nucleosynthetic yields of nitrogen by incorporating N enrichment into our previously developed and empirically tuned multi-zone galactic chemical evolution model. We adopt a metallicity-independent (""primary"") N yield from massive stars and a metallicity-dependent (""secondary"") N yield from AGB stars. In our model, galactic radial zones do not evolve along the observed [N/O]-[O/H] relation, but first increase in [O/H] at roughly constant [N/O], then move upward in [N/O] via secondary N production. By $t\approx5$ Gyr, the model approaches an equilibrium [N/O]-[O/H] relation, which traces the radial oxygen gradient. We find good agreement with the [N/O]-[O/H] trend observed in extra-galactic systems if we adopt an IMF-averaged massive star yield $y_\text{N}^\text{CC}=3.6\times10^{-4}$, consistent with predictions for rapidly rotating progenitors, and a fractional AGB yield that is linear in mass and metallicity $y_\text{N}^\text{AGB}=(9\times10^{-4})(M_*/M_\odot)(Z_*/Z_\odot)$. This model reproduces the [N/O]-[O/H] relation found for Milky Way stars in the APOGEE survey, and it reproduces (though imperfectly) the trends of stellar [N/O] with age and [O/Fe]. The metallicity-dependent yield plays the dominant role in shaping the gas-phase [N/O]-[O/H] relation, but the AGB time-delay is required to match the APOGEE stellar age and [O/Fe] trends. If we add $\sim$40\% oscillations to the star formation rate, the model reproduces the scatter in gas-phase [N/O] vs. [O/H] observed in external galaxies by MaNGA. We also construct models using published AGB yields and examine their empirical successes and shortcomings. For all AGB yields we consider, simple stellar populations release half their N after only $\sim$250 Myr. ",Empirical Constraints on the Nucleosynthesis of Nitrogen
77,1492200689409220610,69020466,George King,"['üö® New accepted paper on arXiv! üö®\n\n<LINK>\n\nWork done with @peterwheatley, @astro_fawcett, @nikkehmiller, @eblur27, and Marcel Ag√ºeros.', 'A few years back, the repurposed Kepler mission, K2, discovered a bunch of planets orbiting stars in the open cluster Praesepe, which is ~670 million years old.', 'Using observations from XMM-Newton, we explored how irradiated 4 of those planets are by high-energy light (X-rays &amp; extreme-UV) from their stars. The light heats the upper atmosphere, and drives an outflow of gas off the planet into space. This is a NASA impression of HD 209458b https://t.co/SVUjt4f8vj', ""In the most extreme cases, it's thought that this process can completely strip a planet of its initial hydrogen+helium envelope, leaving at most an atmosphere of heavy elements only."", ""Where it happens, complete stripping is typically assumed to occur very early in a planets' life (the first ~100 Myr), but see also our paper from last year!\nhttps://t.co/5gMWxRubGG"", 'H/He envelope stripping by high energy light may explain two dearths of planets we see in the exoplanet population: the Neptunian desert and the radius-period valley/radius gap. Figures from Owen+Lai 2018 and Fulton et al. 2017. https://t.co/iYKGWVnWyd', 'All four planets we investigated are close to one or both of these features in the radius-period parameter space. (Orange and blue are different estimates for the Neptunian desert boundary, red is an estimate of the radius-period valley). https://t.co/W074aEgGC9', 'These planets are also still relatively young, and as the high-energy emission of a star decreases over its lifetime, one might expect the emission and the resulting outflow to still be relatively high for these planets.', 'Calculating the fluxes from these X-ray spectra show the ratios of the X-ray light to bolometric light (emission across all wavelengths) for these stars are still between 2 and 3 orders of magnitude higher than the much older Sun. https://t.co/FNz3NRqe96', 'From this we used a few different methods of estimating the resulting mass loss rate from the planets, which show that all four planets are likely losing material at 10^9 g/s or greater, if they still retain a H/He envelope. https://t.co/m9OnISDd7S', '10^9 g/s is similar to estimates for GJ 436b, an older planet where gas outflow causes a 56% transit at Ly-alpha, an important spectral line of H. However, the factors determining how the depth of outflow-related transit signals are more complex than just the mass loss rate alone', 'At the end of the paper, we present some illustrative simulations of the possible futures of the 4 planets in the context of mass loss resulting from this high-energy irradiation. Here, for K2-100b. Cases where the radius suddenly increases &amp; the line stops are complete stripping https://t.co/S5rVfA2Y6e', 'This for K2-104b shows the planet is likely already stripped of H/He, or will be soon. 3 of the 4 planets have at least 1 simulation where the planet was fully stripped in the future, though for K2-100b and K2-101b it is very dependent on the current masses, which are uncertain. https://t.co/X7G5iiPR6N', ""That's it! Discovering more planets at a range of ages up to ~1 Gyr (as TESS is doing!) will help us further explore these processes in young planets, determine if high-energy emission is the primary mechanism behind those population features, and the timescale it acts on if so."", 'Much more detail in the paper, of course. Thanks for making it to the end! \n\nNow back to my usual tweets despairing at the England cricket team.']",https://arxiv.org/abs/2202.04750,"We present an analysis of XMM-Newton observations of four stars in the young (670 Myr) open cluster Praesepe. The planets hosted by these stars all lie close in radius-period space to the radius-period valley and/or the Neptunian desert, two features that photoevaporation by X-ray and extreme ultraviolet (EUV) photons could be driving. Although the stars are no longer in the saturated regime, strong X-ray and extreme ultraviolet irradiation is still ongoing. Based on EUV time evolution slopes we derived in a previous paper, in all four cases, two-thirds of their EUV irradiation is still to come. We compare the XMM-Newton light curves to those simultaneously measured with K2 at optical wavelengths, allowing us to search for correlated variability between the X-ray and optical light curves. We find that the X-ray flux decreases and flattens off while the optical flux rises throughout for K2-100, something that could result from active regions disappearing from view as the star spins. Finally, we also investigate possible futures for the four planets in our sample with simulations of their atmosphere evolution still to come, finding that complete photoevaporative stripping of the envelope of three of the four planets is possible, depending on the current planet masses. ",The strongly irradiated planets in Praesepe
78,1492162171299631108,1309406444731731970,Alejandro Sopena,"['Very happy to announce that our new paper on the Algrebaic Bethe Ansatz (ABA) on quantum computers is finally on arXiv!!\n\nRead and enjoy it\n\n[THREAD]\n<LINK>', 'The Bethe Ansatz (BA) is a classical method for exactly solving one-dimensional quantum models.\nHowever, there are some obervables such as long-range correlation functions which are difficult to calculate.\nThis motivates its implementation on a quantum computer.', 'We present a deterministic quantum algorithm for the preparation of BA eigenstates.\nWe build a quantum circuit with no ancillary qubits (Algebraic Bethe Circuit) which allows us to obtain the desired eigenstate as output. https://t.co/nuOmtfqtxr', 'We illustrate our method in the spin- 1/2 XX and XXZ models and we find its application on the XX model is efficient.\nWe run numerical simulations, preparing eigenstates of the XXZ model for systems of up to 24 qubits and 12 magnons. https://t.co/eVlr9IvBCG', 'We run small-scale error-mitigated implementations on the IBM quantum computers, including the preparation of the ground state for the XX and XXZ models in 4 sites. https://t.co/TEgplSz4qo']",https://arxiv.org/abs/2202.04673,"The Algebraic Bethe Ansatz (ABA) is a highly successful analytical method used to exactly solve several physical models in both statistical mechanics and condensed-matter physics. Here we bring the ABA to unitary form, for its direct implementation on a quantum computer. This is achieved by distilling the non-unitary $R$ matrices that make up the ABA into unitaries using the QR decomposition. Our algorithm is deterministic and works for both real and complex roots of the Bethe equations. We illustrate our method in the spin-$\frac{1}{2}$ XX and XXZ models. We show that using this approach one can efficiently prepare eigenstates of the XX model on a quantum computer with quantum resources that match previous state-of-the-art approaches. We run numerical simulations, preparing eigenstates of the XXZ model for systems of up to 24 qubits and 12 magnons. Furthermore, we run small-scale error-mitigated implementations on the IBM quantum computers, including the preparation of the ground state for the XX and XXZ models in $4$ sites. Finally, we derive a new form of the Yang-Baxter equation using unitary matrices, and also verify it on a quantum computer. ",Algebraic Bethe Circuits
79,1492144024072953856,1965093145,Charles Tapley Hoyt,"['What do you need to predict drug-drug interactions or synergy?\n\nSugar, spice, everything nice‚Ä¶ and a little ChemicalX üí•\n\nCheck out our new preprint in collaboration with @AstraZeneca led by @benrozemberczki:\n\nüìú Paper: <LINK>\n\nü§ñ Code: <LINK>']",https://arxiv.org/abs/2202.05240,"In this paper, we introduce ChemicalX, a PyTorch-based deep learning library designed for providing a range of state of the art models to solve the drug pair scoring task. The primary objective of the library is to make deep drug pair scoring models accessible to machine learning researchers and practitioners in a streamlined framework.The design of ChemicalX reuses existing high level model training utilities, geometric deep learning, and deep chemistry layers from the PyTorch ecosystem. Our system provides neural network layers, custom pair scoring architectures, data loaders, and batch iterators for end users. We showcase these features with example code snippets and case studies to highlight the characteristics of ChemicalX. A range of experiments on real world drug-drug interaction, polypharmacy side effect, and combination synergy prediction tasks demonstrate that the models available in ChemicalX are effective at solving the pair scoring task. Finally, we show that ChemicalX could be used to train and score machine learning models on large drug pair datasets with hundreds of thousands of compounds on commodity hardware. ",ChemicalX: A Deep Learning Library for Drug Pair Scoring
80,1492132903572197378,1349798626483187713,Ivan Esteban,"[""Paper day!\nWith Olga Mena and Jordi Salvado, we explore how CMB lensing carries unique information on neutrino properties. Interestingly, new neutrino properties dilute an existing anomaly!\n\nWhat's the anomaly and the physics we explore? See belowüëáüßµ\n\n<LINK>"", 'First, the basics. CMB light is gravitationally lensed by the large-scale structure of the Universe. If we know how the Universe evolves, we can predict how much lensing we should see!\n\nBy looking at ""blurred"" CMB anisotropies, @Planck detected this effect at 40 (!) sigma! https://t.co/po86JNby6U', 'But there is something weird with the data. If we look at the ratio between data and the nominal model, we see some residual wiggles. https://t.co/1xYdSGLyK5', 'These wiggles can be explained if we add extra lensing *by hand*. https://t.co/Mqil7tuGff', 'It seems that our cosmological model cannot self-consistently describe lensing. What is going on? \nThe significance of this anomaly slightly depends on the dataset and likelihood you use, but it is consistently above 2sigma-3sigma. https://t.co/5HPxKj1Biu', 'And this matters! Precisely measuring CMB lensing is the groundwork for precision cosmology in many future observations. \nSo we better understand 1) if the anomaly is there; 2) in case it grows, which physics may be behind. https://t.co/wOjzTeRSS3', 'In our paper, we address point 2). It turns out that, in the standard scenario, neutrinos *reduce* lensing. You can see this from the out-of-phase residuals below üëá https://t.co/uRbs6Rm2xb', '*But* this arises from a competition of lensing-enhancing effects (e.g., massive neutrinos cluster gravitationally), and lensing-suppressing effects (e.g., massive neutrinos increase the energy in the Universe, this enhances the expansion, and structure formation is suppressed)', 'This delicate balance can be broken if we change how neutrinos behave! \nIn particular, we show that modifying the neutrino equation of state can change the picture. See our paper for more details!', 'This modified equation of state naturally arises if new neutrino-neutrino interactions exist. \nIn the plot below, the blue lines are standard equation of states, and the green lines those arising from new interactions. https://t.co/OBsf5X6SlU', 'And if we add this to the CMB anisotropy data... We see that the predictions now match the observed wiggles better, *without* introducing extra lensing by hand! https://t.co/dTCHekJ4zp', 'You can have a look at our paper for the statistics details, but this consistently dilutes the significance of the lensing anomaly.\nEven for datasets as BAO that were challenging to explain in other models addressing this anomaly!', 'The exciting part is that this scenario has a *direct* link to particle physics. We are very excited about prospects not only in cosmology, but also using laboratory or astrophysical neutrino data!', 'Have a look at our paper for other setups (KATRIN, supernovae, solar neutrinos...) where this could show up!\n\nAnd a huge thanks to Olga Mena and Jordi Salvado. They made this project not only interesting, but also super fun to work on!']",https://arxiv.org/abs/2202.04656,"Despite the impressive success of the standard cosmological model, several anomalies defy its triumph. Among them is the so-called lensing anomaly: the Planck satellite observes stronger CMB gravitational lensing than expected. The role of neutrinos in this anomaly has been mostly overlooked, despite their key role in CMB lensing, because in the standard scenario they tend to increase the tension. Here, we show that this strongly depends on the assumed neutrino equation of state. We demonstrate that if neutrinos have yet undiscovered long-range interactions, the lensing pattern is significantly affected, rendering the lensing anomaly as a pure statistical fluctuation. Our results thus open up a window to link anomalous CMB lensing with present and future cosmological, astrophysical, and laboratory measurements of neutrino properties. ",Non-standard neutrino cosmology dilutes the lensing anomaly
81,1492096021157056522,1138012858988617728,Hannes St√§rk,"['Our new paper is out!üß¨\nEquiBind: Geometric Deep Learning for Drug Binding Structure Prediction <LINK>\n\nFast 3D structure predictions in which molecules bind to proteins!\n\nWith the lovely team @octavianEganea @lucky_pattanaik @BarzilayRegina Tommi Jaakkola ü§ó\n1/2 <LINK>', '@octavianEganea has a nice thread describing the paper: https://t.co/TYjKHSj9RD\n\nWhat I wish to add is that the code is available here: https://t.co/5pEcYPJY6E\n\nYou can easily use it to predict the binding structures of your own ligand-protein pairs!\n2/2']",https://arxiv.org/abs/2202.05146,"Predicting how a drug-like molecule binds to a specific protein target is a core problem in drug discovery. An extremely fast computational binding method would enable key applications such as fast virtual screening or drug engineering. Existing methods are computationally expensive as they rely on heavy candidate sampling coupled with scoring, ranking, and fine-tuning steps. We challenge this paradigm with EquiBind, an SE(3)-equivariant geometric deep learning model performing direct-shot prediction of both i) the receptor binding location (blind docking) and ii) the ligand's bound pose and orientation. EquiBind achieves significant speed-ups and better quality compared to traditional and recent baselines. Further, we show extra improvements when coupling it with existing fine-tuning techniques at the cost of increased running time. Finally, we propose a novel and fast fine-tuning model that adjusts torsion angles of a ligand's rotatable bonds based on closed-form global minima of the von Mises angular distance to a given input atomic point cloud, avoiding previous expensive differential evolution strategies for energy minimization. ",EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction
82,1492073626052567059,1257262883207028736,Gerard I. G√°llego,"['This is SHAS ü™ìüí¨, a new Audio Segmentation method for ST, from the @mtupc1 group!\nPaper: <LINK>\nCode: <LINK> <LINK> <LINK>', '@mgaido91 @mtupc1 Thank you Marco, it‚Äôs such an honor to read this from you! üôÇ (cc: @JohnTsiamas )']",https://arxiv.org/abs/2202.04774,"Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages. ",SHAS: Approaching optimal Segmentation for End-to-End Speech Translation
83,1491970884218892290,43691715,M.H. Ansari,"['New paper <LINK> posted on arxiv, on a new 2-qubit gate called PF gate, highly technical as reality matters if we are right! <LINK>']",https://arxiv.org/abs/2202.05208,"We propose a protected 2-qubit gate composed of two sub-gates that switches between idling and entangling modes. These sub-gates are separated by a large energy barrier. In superconducting circuits, this gate is derived by harnessing microwave driving and coupling modulation. The idle mode decouples qubits and leaves them with highly localized wave functions. The entangled mode, however, makes them interact in absence of the adversarial $ZZ$ interactions. Our theoretical results show that this gate is a promising way to switch on and off a fast and high fidelity 2-qubit interaction. ","Parasitic-free gate: A protected switch between idle and entangled
  states"
84,1491894215734743040,66175375,Jason Wang,"['New KPIC paper out showing the ability to measure composition of a benchmark brown dwarf companion from high resolution spectra! The inferred C and O abundances are mostly consistent with its host star, which is good. Led by fellow JWang astronomer Ji Wang <LINK>']",https://arxiv.org/abs/2202.02477,"A benchmark brown dwarf (BD) is a BD whose properties (e.g., mass and chemical composition) are precisely and independently measured. Benchmark BDs are valuable in testing theoretical evolutionary tracks, spectral synthesis, and atmospheric retrievals for sub-stellar objects. Here, we report results of atmospheric retrieval on a synthetic spectrum and a benchmark BD -- HR 7672~B -- with \petit. First, we test the retrieval framework on a synthetic PHOENIX BT-Settl spectrum with a solar composition. We show that the retrieved C and O abundances are consistent with solar values, but the retrieved C/O is overestimated by 0.13-0.18, which is $\sim$4 times higher than the formal error bar. Second, we perform retrieval on HR 7672~B using high spectral resolution data (R=35,000) from the Keck Planet Imager and Characterizer (KPIC) and near infrared photometry. We retrieve [C/H], [O/H], and C/O to be $-0.24\pm0.05$, $-0.19\pm0.04$, and $0.52\pm0.02$. These values are consistent with those of HR 7672~A within 1.5-$\sigma$. As such, HR 7672~B is among only a few benchmark BDs (along with Gl 570~D and HD 3651~B) that have been demonstrated to have consistent elemental abundances with their primary stars. Our work provides a practical procedure of testing and performing atmospheric retrieval, and sheds light on potential systematics of future retrievals using high- and low-resolution data. ","Retrieving the C and O Abundances of HR 7672~AB: a Solar-Type Primary
  Star with a Benchmark Brown Dwarf"
85,1491869877669298177,405026897,Omar Valsson,"['Finally out as a preprint! Our ""living"" review on enhanced sampling methods where we aim to present the unifying principles and differences. Is also my first paper with my new @UNTChemistry affiliation!  \n<LINK>', 'We welcome and need your feedback, so please make comments and suggestions via Github issues! \nhttps://t.co/Bx4BiPdSCe']",http://arxiv.org/abs/2202.04164,"Enhanced sampling algorithms have emerged as powerful methods to extend the potential of molecular dynamics simulations and allow the sampling of larger portions of the configuration space of complex systems. This review aims to present the unifying principles and differences of several computational methods for enhanced sampling in molecular simulations of biomolecules, soft matter and molecular crystals. Indeed, despite the apparent abundance and divergence of such methods, the principles at their core can be boiled down to a few statistical and physical principles. To enable comparisons, the various methods are introduced using the same terminology and notation. We then illustrate in which ways many different methods combine principles from a smaller class of enhanced sampling concepts. This review is intended for scientists with an understanding of the basics of molecular dynamics simulations and statistical physics who want a deeper understanding of the ideas that underlie various enhanced sampling methods and the relationships between them. We expect this to help scientists make informed decisions about which method to use. This ""living"" review is intended to be updated to continue to reflect the wealth of sampling methods as they emerge in the literature. ",Enhanced sampling methods for molecular dynamics simulations
86,1491839463953211394,85640761,Michael Schr√∂der,"['Happy to announce that I\'ll be presenting our vision paper ""Grammars for Free: Toward Grammar Inference for Ad Hoc Parsers"" at the New Ideas track at #icse2022 ü•≥@citostyle \n\nCheck out the pre-print: <LINK>\n\nWhat is an ad hoc parser you ask? üßµüëá', ""Basically any time you do string processing (calling split, trim, substring, looping over characters,...) you're doing some form of parsing. But it's purely ad hoc ‚Äî you're not gonna write down a formal grammar for this parser. But imagine if you could get a grammar ‚ú®for free‚ú® https://t.co/Evvx8HYXRi"", 'If you could infer grammars automatically, you could have a code review bot that\'s able to say ü§ñ""Hey, it looks like you\'re changing the semantics of your code: your function suddenly accepts more/less/different input strings!""', 'With automatic grammar inference, you could also: ‚û°Ô∏ègenerate interactive documentation\n‚û°Ô∏èuse program sketching to synthesize parsers\n‚û°Ô∏èenhance code search and mining\n‚û°Ô∏ègenerate inputs for fuzz testing\n‚û°Ô∏èreason about language-theoretic security properties of your code\n...and more!', ""So how can we infer ad hoc grammars? Our idea is to simplify parser code to an IR whose refinement types model the parser's input language. Infer the types, infer the grammar!\n\nIs this actually possible? Well, that's what I'm hoping to demonstrate during my PhD üòÖü§û https://t.co/ySkoZH0SpP"", 'I\'m really excited about this project and I hope to see a lot of you in person this May @ICSEconf in Pittsburgh so I can bore you with all the details. üë®üèª\u200düè´\n\nLet\'s make ""grammars for free"" a reality! üöÄ']",https://arxiv.org/abs/2202.01021,"Ad hoc parsers are everywhere: they appear any time a string is split, looped over, interpreted, transformed, or otherwise processed. Every ad hoc parser gives rise to a language: the possibly infinite set of input strings that the program accepts without going wrong. Any language can be described by a formal grammar: a finite set of rules that can generate all strings of that language. But programmers do not write grammars for ad hoc parsers -- even though they would be eminently useful. Grammars can serve as documentation, aid program comprehension, generate test inputs, and allow reasoning about language-theoretic security. We propose an automatic grammar inference system for ad hoc parsers that would enable all of these use cases, in addition to opening up new possibilities in mining software repositories and bi-directional parser synthesis. ",Grammars for Free: Toward Grammar Inference for Ad Hoc Parsers
87,1491837547131154437,334231357,Joris Witstok,"[""What is a redshift machine? üßÆ If you're curious, have a look at this paper led by Sander Schouws @UniLeiden, presenting new @almaobs observations of six extremely distant galaxies resulting in the discovery of far-infrared [C II] lines in three of them!\n<LINK>""]",http://arxiv.org/abs/2202.04080,"The [CII]$_{158\mu m}$ line has long been proposed as a promising line to spectroscopically confirm galaxies in the epoch of reionization. In this paper we present the results of new ALMA observations spectral scanning for [CII] in six particularly luminous Lyman Break Galaxies at $z\sim7$. The six sources were drawn from a sample of bright $z\sim7$ galaxies identified using the wide-area optical, near-IR, and Spitzer/IRAC data over the COSMOS/UltraVISTA field and were targeted on the basis of tight constraints on their redshifts from their IRAC [3.6]-[4.5] colors. We detect significant ($>9\sigma$) [CII] lines in three of our six targets ($50\%$) co-spatial with the rest-$UV$ emission from the ground/space-based near-IR imaging. The luminosities of the [CII] lines lie in the range $5.6$ to $8.8\times10^{8}L_{\odot}$, consistent with the local [CII]-SFR relation. Meanwhile, their [CII]/$L_{IR}\sim1-3\times10^{-3}$ ratios are slightly elevated compared to local (U)LIRGS. This could be due to lower dust-to-gas or dust-to-metal ratios. We also find that our sources display a large kinematic diversity, with one source showing signs of rotation, one source a likely major merger and one dispersion dominated source that might contain a bright star-forming clump. Our results highlight the effectiveness of spectral scans with ALMA in spectroscopically confirming luminous galaxies in the epoch of reionization, something that is being be applied on a significantly larger sample in the on-going REBELS large program. ","ALMA as a Redshift Machine: Using [CII] to Efficiently Confirm Galaxies
  in the Epoch of Reionization"
88,1491741211064946688,1364176717477273600,Fergus Cullen,"[""New paper on the arXiv today, lead by PhD student Ryan Begley. In it we estimate the LyC escape fraction in star-forming galaxies at z=3.5, combining data from VANDELS (for robust spec-z's) with the deep VIMOS U-band imaging in CDFS (tracing LyC). <LINK>"", 'Using detailed IGM+CGM modelling to marginalise of the full sightline distribution, we fit the ionizing to non-ionizing flux ratio distribution of our sample and find an average escape fraction of f_esc = 0.07 +/- 0.02. https://t.co/VUxdFV3BXp', ""We also find trends with various galaxy physical properties: galaxies with larger LyA EW's, lower dust and fainter intrinsic UV luminosities have higher f_esc. In contrast, we see no clear stellar mass trend. https://t.co/Bm9WGlpQXE"", ""I think the paper nicely demonstrates that we can measure f_esc from broad-band photometry as long as we have a large enough samples with spec-z's, accurately model the IGM+CGM, and fit the full flux ratio distribution (e.g., rather than stacking images)."", '.. now we just need larger sample sizes so that we can fit more complex underlying f_esc distributions!']",https://arxiv.org/abs/2202.04088,"We present a study designed to measure the average LyC escape fraction ($\langle f_{\rm esc}\rangle$) of star-forming galaxies at z=3.5. We assemble a sample of 148 galaxies from the VANDELS survey at $3.35\leq z_{\rm spec}\leq3.95$, selected to minimize line-of-sight contamination of their photometry. For this sample, we use ultra-deep, ground-based, $U-$band imaging and HST $V-$band imaging to robustly measure the distribution of $\mathcal{R_{\rm obs}}$ $=(L_{\rm LyC}/L_{\rm UV})_{\rm obs}$. We then model the distribution as a function of $\langle f_{\rm esc}\rangle$, carefully accounting for attenuation by dust, and the IGM (and CGM). A maximum likelihood fit to the $\mathcal{R_{\rm obs}}$ distribution returns a best-fitting value of $\langle f_{\rm esc}\rangle =0.07\pm0.02$, a result confirmed using an alternative Bayesian inference technique (both exclude $\langle f_{\rm esc}\rangle=0.0$ at $> 3\sigma$). By splitting our sample in two, we find evidence that $\langle f_{\rm esc}\rangle$ is positively correlated with Ly$\alpha$ equivalent width, with high and low sub-samples returning best fits of $\langle f_{\rm esc}\rangle=0.12^{+0.06}_{-0.04}$ and $\langle f_{\rm esc} \rangle=0.02^{+0.02}_{-0.01}$, respectively. In contrast, we find evidence that $\langle f_{\rm esc}\rangle$ is anti-correlated with intrinsic UV luminosity and UV dust attenuation; with low UV luminosity and dust attenuation sub-samples returning best fits in the range $0.10 \leq \langle f_{\rm esc}\rangle \leq 0.22$. We do not find evidence for a clear correlation between $f_{\rm esc}$ and galaxy stellar mass, suggesting it is not a primary indicator of leakage. Although larger samples are needed to further explore these trends, they suggest that it is entirely plausible that the low dust and metallicity galaxies found at z > 6 will display the $\langle f_{\rm esc}\rangle\geq0.1$ required to drive reionization. ","The VANDELS survey: a measurement of the average Lyman-continuum escape
  fraction of star-forming galaxies at z=3.5"
89,1491728034386395144,1069045039,daniele marinazzo,"['New paper!\n\nA Framework for the Time- and Frequency-Domain Assessment of High-Order Interactions in Brain and Physiological Networks\n\n<LINK>\n\nwith Luca Faes, Gorana Mijatovic,  @SebinoStram, and many others <LINK>', 'Here we extend the elegant and parsimonious O-information framework introduced by @_fernando_rosas and colleagues https://t.co/nHZG3XsNdK\n\nto the frequency domain, and apply it to cardiovascular interactions, and to intracranial recordings in wake and anesthesia.\n\nenjoy! https://t.co/ryXUieiNhP']",https://arxiv.org/abs/2202.04179,"While the standard network description of complex systems is based on quantifying links between pairs of system units, higher-order interactions (HOIs) involving three or more units play a major role in governing the collective network behavior. This work introduces an approach to quantify pairwise and HOIs for multivariate rhythmic processes interacting across multiple time scales. We define the so-called O-information rate (OIR) as a new metric to assess HOIs for multivariate time series, and propose a framework to decompose it into measures quantifying Granger-causal and instantaneous influences, as well as to expand it in the frequency domain. The framework exploits the spectral representation of vector autoregressive and state-space models to assess synergistic and redundant interactions among groups of processes, both in specific bands and in the time domain after whole-band integration. Validation on simulated networks illustrates how the spectral OIR can highlight redundant and synergistic HOIs emerging at specific frequencies but not using time-domain measures. The application to physiological networks described by heart period, arterial pressure and respiration measured in healthy subjects during paced breathing, and to brain networks described by ECoG signals acquired in an animal experiment during anesthesia, document the capability of our approach to identify informational circuits relevant to well-defined cardiovascular oscillations and brain rhythms and related to specific physiological mechanisms of autonomic control and altered consciousness. The proposed framework allows a hierarchically-organized evaluation of time- and frequency-domain interactions in networks mapped by multivariate time series, and its high flexibility and scalability make it suitable to investigate networks beyond pairwise interactions in neuroscience, physiology and other fields. ","A Framework for the Time- and Frequency-Domain Assessment of High-Order
  Interactions in Brain and Physiological Networks"
90,1491721869673869312,1285286172265152519,Gerben Venken,['New paper! <LINK> bounds the D3 tadpole required to get a controlled de Sitter vacuum in LVS from below.'],https://arxiv.org/abs/2202.04087,"The large volume scenario (LVS) for de Sitter compactifications of the type IIB string is, at least in principle, well protected from various unknown corrections. The reason is that, by construction, the Calabi-Yau volume is exponentially large. However, as has recently been emphasised, in practice the most explicit models are rather on the border of parametric control. We identify and quantify parametrically what we believe to be the main issue behind this difficulty. Namely, a large volume implies a shallow AdS minimum and hence a small uplift. The latter, if it relies on an anti-D3 in a throat, requires a large negative tadpole. As our main result, we provide a simple and explicit formula for what this tadpole has to be in order to control the most dangerous corrections. The fundamental ingredients are parameters specifying the desired quality of control. We comment on the interplay between our constraint and the tadpole conjecture. We also discuss directions for future work which could lead to LVS constructions satisfying the tadpole constraint with better control, as well as further challenges that may exist for the LVS. Our formula then represents a very concrete challenge for future searches for and the understanding of relevant geometries. ",The LVS Parametric Tadpole Constraint
91,1491715766147862530,1397091478460116993,Laura Colzi,"['Check out our new paper: Colzi et al. (2022), ApJL in press, arXiv:2202.0411 @L_Colzi @ryvendel  \n<LINK>\nThanks to deuterated molecules we have spotted the presence of two different gas components towards the Galactic Centre source G+0.693-0.027. <LINK>']",https://arxiv.org/abs/2202.04111,"The Central Molecular Zone (CMZ) contains most of the mass of our Galaxy but its star formation rate is one order of magnitude lower than in the Galactic disc. This is likely related to the fact that the bulk of the gas in the CMZ is in a warm ($>$100 K) and turbulent phase with little material in the pre-stellar phase. We present in this Letter observations of deuterium fractionation (D/H ratios) of HCN, HNC, HCO$^{+}$, and N$_{2}$H$^{+}$ towards the CMZ molecular cloud G+0.693-0.027. These observations clearly show, for the first time, the presence of a colder, denser, and less turbulent narrow component, with a line width of $\sim$9 km s$^{-1}$, in addition to the warm, less dense and turbulent broad component with a line width of $\sim$20 km s$^{-1}$. The very low D/H ratio $\le$6$\times$10$^{-5}$ for HCO$^{+}$ and N$_{2}$H$^{+}$, close to the cosmic value ($\sim$2.5$\times$10$^{-5}$), and the high D/H ratios $>$4$\times$10$^{-4}$ for HCN and HNC derived for the broad component, confirm the presence of high-temperatures deuteration routes for nitriles. For the narrow component we have derived D/H ratios $>$10$^{-4}$ and excitation temperatures of $7$ K for all molecules, suggesting kinetic temperatures $\le$30 K and H$_2$ densities $\ge$5$\times$10$^{4}$ cm$^{-3}$, at least one order of magnitude larger than for the broad component. The method presented in this Letter allows to identify clouds on the verge of star formation, i.e. under pre-stellar conditions, towards the CMZ. This method can also be used for the identification of such clouds in external galaxies. ","Deuterium fractionation as a multi-phase component tracer in the
  Galactic Centre"
92,1491699446970789890,1140222123006472194,Kasper Elm Heintz,"['A blast from the past! üí• New paper lead by Andrea Rossi reporting the afterglow properties of the z~6.3 GRB detected last year. Amazing to think that this light has been traveling from a time when the Universe was less than a billion years old. \n\n<LINK>', 'Stay tuned for the next papers in the line reporting the abundances and the IGM neutral fraction in and around this GRB üí•']",https://arxiv.org/abs/2202.04544,"We present the discovery of the very energetic GRB 210905A at the high redshift z=6.312 and its luminous X-ray and optical afterglow. We obtained photometric and spectroscopic follow-up in the optical and near-infrared (NIR), covering both the prompt and afterglow emission from a few minutes up to 7.5 Ms after burst. With an isotropic gamma-ray energy of Eiso=1.27x10^54erg, GRB 210905A lies in the top ~7% GRBs in terms of energy released. Its afterglow is among the most luminous ever observed and, in particular, it is one of the most luminous in the optical at t<0.5 d, in the rest frame. The afterglow starts with a shallow evolution that can be explained by energy injection, and is followed by a steeper decay, while the spectral energy distribution is in agreement with slow cooling in a constant-density environment within the standard fireball theory. A jet break at 39+-21 d has been observed in the X-ray light curve; however, it is hidden in the H-band, potentially due to a constant contribution from an unknown component, most likely a foreground intervening galaxy and/or the host galaxy. We derived a half-opening angle of 7.9+-1.6 degrees, the highest ever measured for a z>~6 burst but within the range covered by closer events. The resulting collimation-corrected gamma-ray energy of 10^52erg is also among the highest ever measured. The moderately large half-opening angle argues against recent claims of an inverse dependence of the half-opening angle on the redshift. The total jet energy is likely too large for a standard magnetar, and suggests that the central engine of this burst was a newly formed black hole. Despite the outstanding energetics and luminosity of both GRB 210905A and its afterglow, we demonstrate that they are consistent within 2swith those of less distant bursts, indicating that the powering mechanisms and progenitors do not evolve significantly with redshift. ",A blast from the infant Universe: the very high-z GRB 210905A
93,1491657125151797249,815941956446470144,Lana Sinapayen,"['In our new paper, we ask: Can measuring ""complexity"" help us when looking for life on faraway planets?\n\nLead by Stuart  Bartlett, in collaboration with experts from many different fields and countries.\n<LINK>\nThis collaboration brought me many excellent memories!', ""An good summary here: https://t.co/X8JMuo87U2\n\nImagine looking at a planet that is SO far away that it's literally just one dot. Wouldn't it be convenient if you could know whether the planet might host alien life, without traveling hundreds of years at the speed of light?"", 'Our (naive! embryonic!) proposal is to measure the fluctuations in the light that we get from that planet, and measure the complexity of these variations.\nWe calculate various measures of complexity, for Earth, Jupiter, and real+simulated data (desert Earth! noisy Earth!) https://t.co/WzlzzYQ92W', ""Some measures seem good at telling whether a planet has a rich biosphere or not; the statistical complexity from Epsilon Machines in particular, is higher for 'complex' worlds, higher for Earth compared to Jupiter, and doesn't mistake noise for complexity.\n(img: phys org) https://t.co/Dd0oOTXqVp"", 'You can\'t really tell if there are aliens on a pixel-sized planet. But this is a modest proposal to filter countless worlds into a manageable number of ""might be promising"" vs many ""probably not that interesting"", given very limited info!', ""The best part is that this method is 'agnostic': it does not require possible alien life to be chemically close to life as we know it. It does not assume DNA, a specific atmosphere, etc.\nOfc, it also means it could be fooled by complexity that is not created by life..."", ""False negatives, false positive remain possible ‚Üì, although we can only make educated guesses as to how likely they are.\nThat is going to be difficult whatever method you use, when you only have one planet where we're *100% certain* that life exists! https://t.co/s1zo5BMSZX"", ""@jobtalle We don't recommend using the Kolmogorov complexity, as it increases with noise (eg white noise), and noise isn't the kind of complexity we'd consider as correlated with the presence of life...""]",https://arxiv.org/abs/2202.03699,"We present a new approach to exoplanet characterisation using techniques from complexity science, with potential applications to biosignature detection. This agnostic method makes use of the temporal variability of light reflected or emitted from a planet. We use a technique known as epsilon machine reconstruction to compute the statistical complexity, a measure of the minimal model size for time series data. We demonstrate that statistical complexity is an effective measure of the complexity of planetary features. Increasing levels of qualitative planetary complexity correlate with increases in statistical complexity and Shannon entropy, demonstrating that our approach can identify planets with the richest dynamics. We also compare Earth time series with Jupiter data, and find that for the three wavelengths considered, Earth's average complexity and entropy rate are approximately 50% and 43% higher than Jupiter's, respectively. The majority of schemes for the detection of extraterrestrial life rely upon biochemical signatures and planetary context. However, it is increasingly recognised that extraterrestrial life could be very different to life on Earth. Under the hypothesis that there is a correlation between the presence of a biosphere and observable planetary complexity, our technique offers an agnostic and quantitative method for the measurement thereof. ","Assessing Planetary Complexity and Potential Agnostic Biosignatures
  using Epsilon Machines"
94,1491637550141739009,2153578286,Matthew Harding üè≥Ô∏è‚Äçüåà,['Can algorithms learn to make the same decisions as human managers? Can they learn how to use both soft data and financial data? What does this tell us about human reasoning? New paper with Gabriel Vasconcelos \n\n<LINK>'],http://arxiv.org/abs/2202.04218,"We use machine learning techniques to investigate whether it is possible to replicate the behavior of bank managers who assess the risk of commercial loans made by a large commercial US bank. Even though a typical bank already relies on an algorithmic scorecard process to evaluate risk, bank managers are given significant latitude in adjusting the risk score in order to account for other holistic factors based on their intuition and experience. We show that it is possible to find machine learning algorithms that can replicate the behavior of the bank managers. The input to the algorithms consists of a combination of standard financials and soft information available to bank managers as part of the typical loan review process. We also document the presence of significant heterogeneity in the adjustment process that can be traced to differences across managers and industries. Our results highlight the effectiveness of machine learning based analytic approaches to banking and the potential challenges to high-skill jobs in the financial sector. ","Managers versus Machines: Do Algorithms Replicate Human Intuition in
  Credit Ratings?"
95,1491557644368293897,789306537781104640,Scott Fleming,"['New paper out today! We studied low-energy, time-resolved UV flares using GALEX/gPhoton data from the GJ 65 system! Shout-outs to my co-authors Chase Million, @rachelosten, Dmitrii Kolotkov, and @cebrasseur. A summary in this thread.\n\nPaper link: <LINK> <LINK>', ""We believe in open data, open software, transparency, and reproducibility. I didn't use showyourwork (timing didn't work out, sorry @rodluger, next time!), but we do have a collection of Python notebooks to recreate figures and tables here: https://t.co/anYuBuMvGj"", 'GALEX raw data is in the form of photon events. The calibrated, delivered mission products at @MAST_News are visit-level images (below) and source catalogs. You can do a ton of science with those, but we want to study variability.\n\n""GALEX is a timeseries mission.""\n-Scott Fleming https://t.co/qC25o52gCL', ""Why look at GALEX data during a visit? Because while most things aren't changing over &lt; 30 minute timescales, lots of things are. White dwarfs, stellar flares, sdB stars, transiting things, eruptive things. We want to take static GALEX data and turn it into dynamic data! https://t.co/BGlWUWwRPE"", ""Why study GJ 65? It's a binary system of two mid-M dwarfs known to be active flare stars, it's quite bright without saturating GALEX detectors, and has a lot of GALEX coverage. So other than the binarity, it's one of the best GALEX sources to study low S:N flares."", ""Disclaimer: We aren't the first to study:\n- things using the GALEX photon events\n- flares using GALEX photon events\n- GJ 65 using GALEX photon events\n\nBut we do think we've done the most comprehensive job of these things to-date. See refs in our paper for previous work!"", ""Back to our study. One big flare was known and well-studied. We found 13 NEW flare events between log(E) 28.5 and 29.5 in the UV. It's hard to get much lower than these, due to S:N concerns. As found by @cebrasseur previous work on UV flares with gPhoton, the shapes can be weird! https://t.co/k4YHb32UNY"", ""One side result: we simulated FRED-like idealized flares that might be concurrent. For a non-FRED shape, we don't know if it's a single flare with a weird shape, or a bunch of FRED-like flares happening concurrently. If the latter, you will under-count and over-estimate energies. https://t.co/u85xjOqlmL"", 'Do we see a number of low energy flares consistent with other flare frequency estimates? Super hard to say: this is just one system (never base off one example), we don\'t know which flare is coming from which star, and it\'s just hard to define what a single ""flare"" is. But, yes? https://t.co/aIjgMxkqQY', 'The big flare was found a while ago by @_BEMILES and @evgenya looking at the visit-level data in 2017. Doyle et al. 2018 time-resolved it with gPhoton in the NUV and reported quasi-periodic pulsations. Our analysis includes the FUV and we confirm QPP for sure! https://t.co/hMDO9QlrgR', 'What\'s next? We\'ve run the entire gPhoton corpus through a variability search on AWS. We\'re going to be finishing work on a catalog of EVERYTHING that goes ""bump in the night"". There\'s flares and eclipses and...things we don\'t know anything about yet...all coming to @MAST_News.', 'There will be plenty of flares in that catalog, but from fainter stars, and due to S:N, these will all be higher energies that are still neat, but are also well-represented in Kepler/TESS/ground data sets. The GJ 65 system is likely going to give us the smallest GALEX flares.', ""One last thing on a personal note. I had a TON of stuff happen in and out of work the past 18 months. I took a full year to respond to a single referee comment b/c I just didn't have it in me for a long time. I want to thank the editor of ApJ and the referee for their patience."", ""And for those who may have similar papers stuck in a limbo: there are a lot of reasons to not be able to finish a paper, but I'm here to say: don't let shame or embarrassment be the only reason. It's not a mark against you, we all suffer from it, even senior astronomers.""]",https://arxiv.org/abs/2202.02861,"Characterizing the distribution of flare properties and occurrence rates is important for understanding habitability of M dwarf exoplanets. The GALEX space telescope observed the GJ 65 system, composed of the active, flaring M stars BL Cet and UV Cet, for 15900 seconds (~4.4 hours) in two ultraviolet bands. The contrast in flux between flares and the photospheres of cool stars is maximized at ultraviolet wavelengths, and GJ 65 is the brightest and nearest flaring M dwarf system with significant GALEX coverage. It therefore represents the best opportunity to measure low energy flares with GALEX. We construct high cadence light curves from calibrated photon events and find 13 new flare events with NUV energies ranging from 10^28.5 - 10^29.5 ergs and recover one previously reported flare with an energy of 10^31 ergs. The newly reported flares are among the smallest M dwarf flares observed in the ultraviolet with sufficient time resolution to discern light curve morphology. The estimated flare frequency at these low energies is consistent with extrapolation from the distributions of higher-energy flares on active M dwarfs measured by other surveys. The largest flare in our sample is bright enough to exceed the local non-linearity threshold of the GALEX detectors, which precludes color analysis. However, we detect quasi-periodic pulsations (QPP) during this flare in both the FUV and NUV bands at a period of ~50 seconds, which we interpret as a modulation of the flare's chromospheric thermal emission through periodic triggering of reconnection by external MHD oscillations in the corona. ","New Time-Resolved, Multi-Band Flares In The GJ 65 System With gPhoton"
96,1491454578461929473,1283077076740759552,Swagat Saurav Mishra,"['One of the highlights of our new paper (with Varun Sahni from IUCAA) has been  illustrated in the fig below,  comparing and contrasting the T-model family with the non-canonical monomial family of potentials.\narXiv link:\n<LINK> <LINK>']",https://arxiv.org/abs/2202.03467,"We discuss implications of the latest BICEP/Keck data release for inflationary models, with particular emphasis on scalar fields with non-canonical Lagrangians of the type ${\cal L} = X^\alpha - V(\phi)$. The observational upper bound on the tensor-to-scalar ratio, $r \leq 0.036$, implies that monotonically increasing convex potentials are now ruled out in the canonical framework. This includes the simplest classic inflationary potentials such as $\frac{1}{2}m^2 \phi^2$ and $\lambda \phi^4$, as well as the whole family of monomial power law potentials $V(\phi) \sim \phi^p$. Instead, the current observations strongly favour asymptotically flat plateau potentials. However, working in the non-canonical framework, we demonstrate that the classic monomial potentials, as well as the Higgs potential with its Standard Model self-coupling, can easily be accommodated by the CMB data. Similarly we show that the inverse power law potential $V(\phi) \sim \phi^{-p}$, which leads to power law inflation in the non-canonical framework, also satisfies the latest CMB bounds. Significantly, $V(\phi)$ can originate from Planck scale initial values $V(\phi) \simeq \, m_p^4$ in non-canonical models while in plateau-like canonical inflation the initial value of the potential is strongly suppressed $V_{\rm plat}(\phi) \leq 10^{-10} \, m_p^4$. This has bearing on the issue of initial conditions for inflation and allows for the equipartition of the kinetic and potential terms in non-canonical models. Equipartition initial conditions can also be accommodated in simple extensions to plateau potentials such as the Margarita model of inflation which is discussed in this paper. ","Canonical and Non-canonical Inflation in the light of the recent
  BICEP/Keck results"
97,1491439202885861376,109603566,Stephen James,"['New paper! Not all reinforcement learning problems are suited for a Gaussian policy parameterization!ü§Ø\nPlan to use 3D rotation or 6D pose as part of an action space? Consider the Bingham distribution! üßµ1/5\n\nPaper: <LINK>\nCode: <LINK>\nw/@pabbeel <LINK>', ""Quaternions are often used as the output rotation\nrepresentation when using deep networks, but due to their antipodal symmetric property, sampling a quaternion from a Gaussian doesn't seem appropriate. Here comes the Bingham distribution to the rescue! üßµ2/5 https://t.co/YE6g66omoV"", ""The Bingham distribution is super cool! It's parameterized by an orthogonal 4x4 matrix M, and a diag matrix Z.\nIntuitively, M holds information regarding the *direction* (akin to mean of Gaussian), while Z controls the *spread* (akin to variance) along the direction. üßµ3/5 https://t.co/JJRgKStxgc"", 'Although unexplored for RL, the Bingham distribution has had success in supervised learning! Two notable works are by @valentinp @domo_mr_roboto (https://t.co/EmplIffFRg) and @igilitschenski (https://t.co/YJqrHgu5os). üßµ4/5', 'The gist of our paper is to show *how* we can leverage the power of Bingham for RL! üí™\nWhen evaluating our approach on the Wahba problem and a set of vision-based robot manipulation tasks from RLBench, we achieve superior performance over a Gaussian parameterization! üßµ5/5 https://t.co/OORSIqRa6y', ""@rndmcnlly @pabbeel I'm not familiar with the *matrix* variant you spoke of, but (correct me if I'm wrong), von Mises-Fisher does not model the antipodally symmetric property of Quaternions; I imagine the *matrix* variant would also have this issue. If true, Bingham seems the better option here.""]",https://arxiv.org/abs/2202.03957,"We propose a new policy parameterization for representing 3D rotations during reinforcement learning. Today in the continuous control reinforcement learning literature, many stochastic policy parameterizations are Gaussian. We argue that universally applying a Gaussian policy parameterization is not always desirable for all environments. One such case in particular where this is true are tasks that involve predicting a 3D rotation output, either in isolation, or coupled with translation as part of a full 6D pose output. Our proposed Bingham Policy Parameterization (BPP) models the Bingham distribution and allows for better rotation (quaternion) prediction over a Gaussian policy parameterization in a range of reinforcement learning tasks. We evaluate BPP on the rotation Wahba problem task, as well as a set of vision-based next-best pose robot manipulation tasks from RLBench. We hope that this paper encourages more research into developing other policy parameterization that are more suited for particular environments, rather than always assuming Gaussian. ","Bingham Policy Parameterization for 3D Rotations in Reinforcement
  Learning"
98,1491437459736911881,1180406227,"Elizabeth A. Barnes, PhD","['New paper by @AntoniosMamala2 , myself and @Iebertu on the fidelity of #XAI methods for CNNs in the geosciences! A great starting point for anyone jumping into XAI for geophysical applications! <LINK> <LINK>']",https://arxiv.org/abs/2202.03407,"Convolutional neural networks (CNNs) have recently attracted great attention in geoscience due to their ability to capture non-linear system behavior and extract predictive spatiotemporal patterns. Given their black-box nature however, and the importance of prediction explainability, methods of explainable artificial intelligence (XAI) are gaining popularity as a means to explain the CNN decision-making strategy. Here, we establish an intercomparison of some of the most popular XAI methods and investigate their fidelity in explaining CNN decisions for geoscientific applications. Our goal is to raise awareness of the theoretical limitations of these methods and gain insight into the relative strengths and weaknesses to help guide best practices. The considered XAI methods are first applied to an idealized attribution benchmark, where the ground truth of explanation of the network is known a priori, to help objectively assess their performance. Secondly, we apply XAI to a climate-related prediction setting, namely to explain a CNN that is trained to predict the number of atmospheric rivers in daily snapshots of climate simulations. Our results highlight several important issues of XAI methods (e.g., gradient shattering, inability to distinguish the sign of attribution, ignorance to zero input) that have previously been overlooked in our field and, if not considered cautiously, may lead to a distorted picture of the CNN decision-making strategy. We envision that our analysis will motivate further investigation into XAI fidelity and will help towards a cautious implementation of XAI in geoscience, which can lead to further exploitation of CNNs and deep learning for prediction problems. ","Investigating the fidelity of explainable artificial intelligence
  methods for applications of convolutional neural networks in geoscience"
99,1491433605834641408,1008856496959049728,"Sierra Grant, PhD","['New paper on the arXiv by Andrea Banzatti on M-band spectroscopy of protoplanetary disks: <LINK>\n\nThis is a magnificent paper with a LOT of information that is useful to those looking to get more involved with spectroscopy at these wavelengths (as I can attest!)', 'This work comes at a time when CRIRES+ joins iSHELL in providing quality high-resolution spectra from 1-5.3 um. We have already learned so much from the M-band spectra of disks, but there is definitely more to come!!']",https://arxiv.org/abs/2202.03438,"We present an overview and first results from a $M$-band spectroscopic survey of planet-forming disks performed with iSHELL on IRTF, using two slits that provide resolving power R $\approx$ 60,000-92,000 (5-3.3 km/s). iSHELL provides a nearly complete coverage at 4.52-5.24 $\mu$m in one shot, covering $>50$ lines from the R and P branches of $^{12}$CO and $^{13}$CO for each of multiple vibrational levels, and providing unprecedented information on the excitation of multiple emission and absorption components. Some of the most notable new findings of this survey are: 1) the detection of two CO Keplerian rings at $<2$ au (in HD 259431), 2) the detection of H${_2}$O ro-vibrational lines at 5 $\mu$m (in AS 205 N), and 3) the common kinematic variability of CO lines over timescales of 1-14 years. By homogeneously analyzing this survey together with a previous VLT-CRIRES survey of cooler stars, we discuss a unified view of CO spectra where emission and absorption components scan the disk surface across radii from a dust-free region within dust sublimation out to $\approx10$ au. We classify two fundamental types of CO line shapes interpreted as emission from Keplerian rings (double-peak lines) and a disk surface plus a low-velocity part of a wind (triangular lines), where CO excitation reflects different emitting regions (and their gas-to-dust ratio) rather than just the irradiation spectrum. A disk+wind interpretation for the triangular lines naturally explains several properties observed in CO spectra, including the line blue-shifts, line shapes that turn into narrow absorption at high inclinations, and the frequency of disk winds as a function of stellar type. ","Scanning disk rings and winds in CO at 0.01-10 au: a high-resolution
  $M$-band spectroscopy survey with IRTF-iSHELL"
100,1491322700828393474,4631815281,Stefano Mandelli,['Peek an eye on our new key paper! LiteBIRD will probe inflation with its compelling design! \n<LINK>'],https://arxiv.org/abs/2202.02773,"LiteBIRD, the Lite (Light) satellite for the study of B-mode polarization and Inflation from cosmic background Radiation Detection, is a space mission for primordial cosmology and fundamental physics. The Japan Aerospace Exploration Agency (JAXA) selected LiteBIRD in May 2019 as a strategic large-class (L-class) mission, with an expected launch in the late 2020s using JAXA's H3 rocket. LiteBIRD is planned to orbit the Sun-Earth Lagrangian point L2, where it will map the cosmic microwave background (CMB) polarization over the entire sky for three years, with three telescopes in 15 frequency bands between 34 and 448 GHz, to achieve an unprecedented total sensitivity of 2.2$\mu$K-arcmin, with a typical angular resolution of 0.5$^\circ$ at 100 GHz. The primary scientific objective of LiteBIRD is to search for the signal from cosmic inflation, either making a discovery or ruling out well-motivated inflationary models. The measurements of LiteBIRD will also provide us with insight into the quantum nature of gravity and other new physics beyond the standard models of particle physics and cosmology. We provide an overview of the LiteBIRD project, including scientific objectives, mission and system requirements, operation concept, spacecraft and payload module design, expected scientific outcomes, potential design extensions and synergies with other projects. ","Probing Cosmic Inflation with the LiteBIRD Cosmic Microwave Background
  Polarization Survey"
101,1491305486020661250,1120577870403911680,Torben Ferber,['New paper together Kai and Camilo from @desy  - a pleasure as always! Belle II potentially has a spectacular reach for long-lived dark photons: <LINK> <LINK>'],https://arxiv.org/abs/2202.03452,"In this letter we point out that the Belle II experiment has a unique sensitivity to visibly decaying long-lived dark photons. Concentrating on the signatures with a single high energy photon in association with a displaced pair of charged particles, we find that Belle II will be able to probe large regions of parameter space that cannot be covered by any other running or proposed experimental facility. While the signature with charged muons or pions in the final state is expected to be background-free after all selections are applied, the case of final state electrons is more involved and requires an in-depth study. We discuss possible ways to further suppress backgrounds and the corresponding experimental prospects. ",Belle II sensitivity to long-lived dark photons
102,1491291811649703940,1191386359707029505,Animesh Mukherjee,"['Releasing our new paper ""Alexa, in you, I trust! Fairness and Interpretability Issues in E-commerce Search through Smart Speakers"" (The WebConf 2022).\n@AbhijnanC @AbhisekFair @kgummadi Saptarshi Ghosh\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2202.03934,"In traditional (desktop) e-commerce search, a customer issues a specific query and the system returns a ranked list of products in order of relevance to the query. An increasingly popular alternative in e-commerce search is to issue a voice-query to a smart speaker (e.g., Amazon Echo) powered by a voice assistant (VA, e.g., Alexa). In this situation, the VA usually spells out the details of only one product, an explanation citing the reason for its selection, and a default action of adding the product to the customer's cart. This reduced autonomy of the customer in the choice of a product during voice-search makes it necessary for a VA to be far more responsible and trustworthy in its explanation and default action. In this paper, we ask whether the explanation presented for a product selection by the Alexa VA installed on an Amazon Echo device is consistent with human understanding as well as with the observations on other traditional mediums (e.g., desktop ecommerce search). Through a user survey, we find that in 81% cases the interpretation of 'a top result' by the users is different from that of Alexa. While investigating for the fairness of the default action, we observe that over a set of as many as 1000 queries, in nearly 68% cases, there exist one or more products which are more relevant (as per Amazon's own desktop search results) than the product chosen by Alexa. Finally, we conducted a survey over 30 queries for which the Alexa-selected product was different from the top desktop search result, and observed that in nearly 73% cases, the participants preferred the top desktop search result as opposed to the product chosen by Alexa. Our results raise several concerns and necessitates more discussions around the related fairness and interpretability issues of VAs for e-commerce search. ","Alexa, in you, I trust! Fairness and Interpretability Issues in
  E-commerce Search through Smart Speakers"
103,1491236909326438405,1444366008,Josh Cooper,"[""Pleased with this new paper written with Erin Hanna and @HaysWhitlatch, mostly drawn from Erin's MS thesis. I love this kind of cross-disciplinary creative exploration. TLDR: semi-definiteness sometimes makes sense over finite fields!\n\n<LINK>"", '@ProfKinyon @HaysWhitlatch Ha, great question!  I have no idea!']",https://arxiv.org/abs/2202.04012,"The study of positive-definite matrices has focused on Hermitian matrices, that is, square matrices with complex (or real) entries that are equal to their own conjugate transposes. In the classical setting, positive-definite matrices enjoy a multitude of equivalent definitions and properties. In this paper, we investigate when a square, symmetric matrix with entries coming from a finite field can be called ""positive-definite"" and discuss which of the classical equivalences and implications carry over. ",Positive-Definite Matrices over Finite Fields
104,1491227700610371584,1283077076740759552,Swagat Saurav Mishra,"[""Our new paper on arXiv today. Got delayed by at least 7 weeks from our original plan. But nevertheless. \nHere's the link :\n<LINK> <LINK>""]",http://arxiv.org/abs/2202.03467,"We discuss implications of the latest BICEP/Keck data release for inflationary models, with particular emphasis on scalar fields with non-canonical Lagrangians of the type ${\cal L} = X^\alpha - V(\phi)$. The observational upper bound on the tensor-to-scalar ratio, $r \leq 0.036$, implies that monotonically increasing convex potentials are now ruled out in the canonical framework. This includes the simplest classic inflationary potentials such as $\frac{1}{2}m^2 \phi^2$ and $\lambda \phi^4$, as well as the whole family of monomial power law potentials $V(\phi) \sim \phi^p$. Instead, the current observations strongly favour asymptotically flat plateau potentials. However, working in the non-canonical framework, we demonstrate that the classic monomial potentials, as well as the Higgs potential with its Standard Model self-coupling, can easily be accommodated by the CMB data. Similarly we show that the inverse power law potential $V(\phi) \sim \phi^{-p}$, which leads to power law inflation in the non-canonical framework, also satisfies the latest CMB bounds. Significantly, $V(\phi)$ can originate from Planck scale initial values $V(\phi) \simeq \, m_p^4$ in non-canonical models while in plateau-like canonical inflation the initial value of the potential is strongly suppressed $V_{\rm plat}(\phi) \leq 10^{-10} \, m_p^4$. This has bearing on the issue of initial conditions for inflation and allows for the equipartition of the kinetic and potential terms in non-canonical models. Equipartition initial conditions can also be accommodated in simple extensions to plateau potentials such as the Margarita model of inflation which is discussed in this paper. ","Canonical and Non-canonical Inflation in the light of the recent
  BICEP/Keck results"
105,1491150637186555905,1023078771073605632,Riku Murai,"[""I'm very excited to share our new work: A Robot Web for Distributed Many-Device Localisation.\nFully decentralised localisation of many robots at a large scale!\n@joeaortiz, @SaeediG, @paulhjkelly, @AjdDavison\nPaper: <LINK> <LINK>"", 'RobotWeb is extremely dynamic and allows any-time addition of new robots. The robots have no prior information about their initial poses. https://t.co/rKRdPvYnrW', 'Our work extends GBP to support Lie Group, which is essential for optimising translation and rotation of the robots. This allows RobotWeb to convergently achieve comparable accuracy as a centralised non-linear factor graph solver such as GTSAM. https://t.co/Y7GKH7EOwQ', 'The windowed optimisation and the sparse communication allow the RobotWeb to scale to over 1000 robots! And the communication bandwidth per robot stays fixed. https://t.co/RsZaOdZ8ka', 'Without RobotWeb and multi-robot optimisation, robots fail to track the circular path! Here, the video shows robots trying to localise against beacons only and then drifting away. https://t.co/SO0dD5mZTP']",https://arxiv.org/abs/2202.03314,"We show that a distributed network of robots or other devices which make measurements of each other can collaborate to globally localise via efficient ad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian Belief Propagation on the fundamental non-linear factor graph describing the probabilistic structure of all of the observations robots make internally or of each other, and is flexible for any type of robot, motion or sensor. We define a simple and efficient communication protocol which can be implemented by the publishing and reading of web pages or other asynchronous communication technologies. We show in simulations with up to 1000 robots interacting in arbitrary patterns that our solution convergently achieves global accuracy as accurate as a centralised non-linear factor graph solver while operating with high distributed efficiency of computation and communication. Via the use of robust factors in GBP, our method is tolerant to a high percentage of faults in sensor measurements or dropped communication packets. ",A Robot Web for Distributed Many-Device Localisation
106,1491110237255847938,2330682822,Ankur Bapna,"['New paper: ""mSLAM: Massively Multilingual Joint Pre-training for Speech and Text""\n\nmSLAM is our new 2B-param speech-text model in 100+ languages, pre-trained on 430k hours of speech and 10+TiB of text.\n \nSOTA on AST(+2.7 BLEU)/ASR, strong on XLU\n\n<LINK> üßµ <LINK>', 'Capable of learning from both speech and text supervision, we fine-tune mSLAM on CoVoST-2 21-&gt;En Speech translation simultaneously with text translation; improving over XLS-R (2B) by 2.7 BLEU and establishing a new state-of-the-art on this task. (2/n) https://t.co/H3YNTAnjVR', 'On MINDS-14 speech intent classification and Fleurs-102 Speech Language Identification, we find significant quality improvements by incorporating text into the pre-training process and  scaling up, compared to models pre-trained only on speech. (3/n) https://t.co/Vncl8rONAH', 'While still being competitive with speech-only pre-training on several multilingual ASR benchmarks (also establishing new state-of-the-art results on Voxpopuli and MLS-10Hr). (4/n) https://t.co/Oywh9BZA9X', 'Moving on to text; on XNLI we observe dual behavior, quality of mSLAM (450M text params) between MT5 300M and 600M on European languages (where we add supervised data with strong CTC alignment losses). (5/n) https://t.co/rt5b48j2dG', 'However, quality rapidly degrades in languages where we don‚Äôt have much supervised data and alignment losses to mitigate cross-modal interference in the model. (6/n) https://t.co/HmF2xiY61q', 'Are representations really aligned? We fine-tune mSLAM on speech-translation and evaluate on text-translation, zero-shot. In figure, speech tuning to speech testing (S-&gt;S) quality in blue, zero-shot speech tuning to text testing (S-&gt;T) in orange. Sadly no good for T-&gt;S :( (7/n) https://t.co/JDduMuwBrD', 'Lots more goodies in the paper (https://t.co/PgR910C1Xs). Joint work with awesome collaborators, @ColinCherry, @yuzhang_xiyi @jiayephy, @melvinjohnsonp, Yong Cheng, @simi_97k, @jasonriesa and @alex_conneau (n/n)']",https://arxiv.org/abs/2202.01374,"We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research. ",mSLAM: Massively multilingual joint pre-training for speech and text
107,1491056222052962308,31386668,Joe Gardiner,['New arXiv pre-print paper from work done with \n@ManolisSamanis as part of his PhD\n providing a taxonomy of free/open source asset discovery tools for ICS.\n<LINK>'],https://arxiv.org/abs/2202.01604,"Asset scanning and discovery is the first and foremost step for organizations to understand what assets they have and what to protect. There is currently a plethora of free and commercial asset scanning tools specializing in identifying assets in industrial control systems (ICS). However, there is little information available on their comparative capabilities and how their respective features contrast. Nor is it clear to what depth of scanning these tools can reach and whether they are fit-for-purpose in a scaled industrial network architecture. We provide the first systematic feature comparison of free-to-use asset scanning tools on the basis of an ICS scanning taxonomy that we propose. Based on the taxonomy, we investigate scanning depths reached by the tools' features and validate our investigation through experimentation on Siemens, Schneider Electric, and Allen Bradley devices in a testbed environment. ","A Taxonomy for Contrasting Industrial Control Systems Asset Discovery
  Tools"
108,1491036828421718021,17520295,Ilija Bogunovic,"['Our new paper presents a Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits.\n\nPaper <LINK>\nWith: Zihan Li, @arkrause, @j_m_scarlett', 'The key idea behind our algorithm is to incorporate a rare switching, along with a novel robust estimator, enlarged confidence bounds, and a minimal number of plays of each selected action.\n\nThis led to significantly tighter regret bounds for several commonly-considered kernels. https://t.co/h039j5HxJn', 'The paper contains the first empirical study of robustness in the corrupted kernelized bandit setting. The key finding is that our algorithm is robust against a variety of adversarial attacks. https://t.co/hbUBnZ17BK']",https://arxiv.org/abs/2202.01850,"We consider the sequential optimization of an unknown, continuous, and expensive to evaluate reward function, from noisy and adversarially corrupted observed rewards. When the corruption attacks are subject to a suitable budget $C$ and the function lives in a Reproducing Kernel Hilbert Space (RKHS), the problem can be posed as corrupted Gaussian process (GP) bandit optimization. We propose a novel robust elimination-type algorithm that runs in epochs, combines exploration with infrequent switching to select a small subset of actions, and plays each action for multiple time instants. Our algorithm, Robust GP Phased Elimination (RGP-PE), successfully balances robustness to corruptions with exploration and exploitation such that its performance degrades minimally in the presence (or absence) of adversarial corruptions. When $T$ is the number of samples and $\gamma_T$ is the maximal information gain, the corruption-dependent term in our regret bound is $O(C \gamma_T^{3/2})$, which is significantly tighter than the existing $O(C \sqrt{T \gamma_T})$ for several commonly-considered kernels. We perform the first empirical study of robustness in the corrupted GP bandit setting, and show that our algorithm is robust against a variety of adversarial attacks. ","A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian
  Process Bandits"
109,1490884280309870594,769369833259479040,Yuto Minami,['Our new paper is available:\n<LINK> <LINK>'],https://arxiv.org/abs/2202.02773,"LiteBIRD, the Lite (Light) satellite for the study of B-mode polarization and Inflation from cosmic background Radiation Detection, is a space mission for primordial cosmology and fundamental physics. The Japan Aerospace Exploration Agency (JAXA) selected LiteBIRD in May 2019 as a strategic large-class (L-class) mission, with an expected launch in the late 2020s using JAXA's H3 rocket. LiteBIRD is planned to orbit the Sun-Earth Lagrangian point L2, where it will map the cosmic microwave background (CMB) polarization over the entire sky for three years, with three telescopes in 15 frequency bands between 34 and 448 GHz, to achieve an unprecedented total sensitivity of 2.2$\mu$K-arcmin, with a typical angular resolution of 0.5$^\circ$ at 100 GHz. The primary scientific objective of LiteBIRD is to search for the signal from cosmic inflation, either making a discovery or ruling out well-motivated inflationary models. The measurements of LiteBIRD will also provide us with insight into the quantum nature of gravity and other new physics beyond the standard models of particle physics and cosmology. We provide an overview of the LiteBIRD project, including scientific objectives, mission and system requirements, operation concept, spacecraft and payload module design, expected scientific outcomes, potential design extensions and synergies with other projects. ","Probing Cosmic Inflation with the LiteBIRD Cosmic Microwave Background
  Polarization Survey"
110,1490714222740885519,1953104810,"Kayse Lee Maass, PhD",['Update! The preprint is now available too! \n\nCheck out @YarenBilgeKaya‚Äôs first authored paper ‚ÄúImproving Access to Housing and Supportive Services for Runaway and Homeless Youth: Reducing Vulnerability to Human Trafficking in New York City‚Äù!\n\n<LINK> <LINK>'],https://arxiv.org/abs/2202.00138,"Recent estimates indicate that there are over 1 million runaway and homeless youth and young adults (RHY) in the United States (US). Exposure to trauma, violence, and substance abuse, coupled with a lack of community support services, puts homeless youth at high risk of being exploited and trafficked. Although access to safe housing and supportive services such as physical and mental healthcare is an effective response to youth s vulnerability towards being trafficked, the number of youth experiencing homelessness exceeds the capacity of available housing resources in most US communities. We undertake an informed, systematic, and data-driven approach to project the collective capacity required by service providers to adequately meet the needs of homeless youth in New York City, including those most at risk of being trafficked. Our approach involves an integer linear programming model that extends the multiple multidimensional knapsack problem and is informed by partnerships with key stakeholders. The mathematical model allows for time-dependent allocation and capacity expansion, while incorporating stochastic youth arrivals and length of stays, services provided in periodic fashion and service delivery time windows. Our RHY and service provider-centered approach is an important step toward meeting the actual, rather than presumed, survival needs of vulnerable youth, particularly those at-risk of being trafficked. ","Improving Access to Housing and Supportive Services for Runaway and
  Homeless Youth: Reducing Vulnerability to Human Trafficking in New York City"
111,1490626410700017671,42837867,Ben Leather,"[""üö® It's new paper day! üö®\n\nAt long last, my first paper has been released!\n\nWe present a new, alternative hyperboloidal method for for frequency-domain self-force calculations.\n\n<LINK>\n\nLet me briefly introduce you to this new approach. üßµ 1/ <LINK>"", 'We consider a scalar field toy-model to demonstrate our approach. This simplified model allows us to understand the problems one might encounter when considering the more mathematically complex, but inherently similar, gravitational problem. 2/', 'In general, the frequency domain wave-equations in the self-force problem have the following form for a single scalar field. 3/ https://t.co/6qwmiAYFah', 'In the paper we detail the specific form of this second-order differential operator and the different types of source one can encounter: 1) distributional, 2) worldtube and 3) unbounded support. 4/ https://t.co/72BaTn831k', 'Traditional frequency domain approaches employ the variation of parameters method and compute the perturbation on standard time slices. 5/ https://t.co/hd7PRpPncm', 'Boundary data is prescribed with asymptotic expansions in order to obtain a solution with the correct behaviour towards the black hole horizon and spatial infinity. 6/ https://t.co/ExFu8EuWwL', 'This approach has been very successful, but boundary conditions calculations can be problematic, and the approach is not well suited to unbounded sources that extend throughout the spacetime. 7/', 'Our paper develops a more favourable, geometric approach building on the work of Freidrich, Andersson, Schmidt and Zenginoƒülu for hyperboloidal methods in black hole spacetimes. 8/ https://t.co/TBnGpVqiqr', 'Avoiding the standard prescription of modelling our perturbations on Cauchy (time) slices, we choose an alternative hyperboloidal foliation with compactification. 9/', 'Hyperboloidal coordinates foliate the (future) event horizon and (future) null infinity rather than intersect the bifurcation sphere and spatial infinity. 10/ https://t.co/V5LBGq7xBx', 'This allows us to include the black hole horizon and null infinity in our computational domain. In doing so, we circumvent the troublesome need to derive boundary conditions from Frobenius or asymptotic series expansions. 11/ https://t.co/k7Dk2gcFaz', 'In fact, if one constructs the hyperboloidal coordinates in such a way that the hyperboloidal time hypersurfaces extend between the black-hole horizon and future null-infinity then the boundary conditions become trivial. 12/', 'Specifically, we employ the following transformation between Schwarzschild coordinates and our chosen hyperboloidal coordinates. \n\nTo make sure our transformed equations are in fact regular we must rescale our scalar field to take into account the asymptotic fall-off. 13/ https://t.co/w8SVPtP0dj', 'We do this by incorporating the conformal factor into our new scalar field. The exponential factor naturally arises from the time transformation but it means we factor out the oscillatory behaviour. 14/', 'This leads to more accurate and efficient solutions since one no longer has to resolve the oscillations. 15/', 'Our new hyperboloidal field satisfies the new equation with a new differential operator, which is degenerate at the boundaries. 16/ https://t.co/9jZSFQBwm4', 'At both boundaries the principal part will vanish hence any regular solution to the differential equation will have the correct physical boundary conditions.\n\nPut simply, the boundary conditions follow directly from the differential equation. 17/', 'There‚Äôs no need to provide data on the grid boundaries because there are no incoming characteristics into the numerical domain. 18/', 'In the paper we also detail how to transform the scalar field equation for each type of source and also discuss how to further transform the equations for the flux and self-force. 19/', 'This is particularly useful for when calculating fluxes towards null infinity as one can directly read-off values for the radiative flux without the need for extrapolation. 20/ https://t.co/wj71I79nu1', 'We solve our hyperboloidal equation using a multi-domain spectral method. Our method works efficiently for all types of sources one encounters in self-force calculations and has distinct advantages over the traditional approach. 21/ https://t.co/SZjdiiPrYc', 'For example, our code finds excellent agreement with regions of the parameter space traditionally not suited to perturbative computations, especially towards the weak-field regime of large orbital radius usually covered by the post-Newtonian theory. 22/ https://t.co/fGvhXD9maO', 'First of all, here is a comparison with the t-component of the self-force calculated from flux balance laws with a scalar field 4PN expression. This plot is a comparison of the t-component of the self-force normalised by the leading PN term. 23/ https://t.co/jHG3Fnr0Pf', 'The blue line is a comparison with the leading PN term and each subsequent line are the residuals after subtracting successive PN terms. 24/', 'This plot is the comparison of the parametric-derivative of the same component and it demonstrates the applicability of the method with non-compact sources that extend throughout the spacetime. 25/ https://t.co/bjfjHMlytE', 'You find similar ‚Äúslow-time‚Äù derivative expressions and calculations in the second-order calculation from the two-timescale approach. See https://t.co/Aem0e6U39N. 26/', 'Secondly, here is a plot of the l-mode contribution to the regularised r-component of the scalar self-force for the mode-sum and effective-source methods. 27/ https://t.co/cygUDJ060d', 'Each line represents a subtraction of analytic regularisation parameters designed to ensure the physical self-force is resolved from the divergent retarded field. 28/', 'The key thing to note here is that typically one is only practically able to resolve up around l ~ 50 modes, whilst our spectral hyperboloidal code can quickly and efficiently manage up to l ~ 100 without too much difficulty for both methods. 29/ https://t.co/W2Ny8c6mNr', 'Even with regularisation parameters, in some cases like close to the light-ring one does indeed require a large l-mode spectrum. 30/', 'We are confident the approach we have developed will play a crucial role in future self-force calculations. 31/ https://t.co/X3AntYxwwJ', ""What's next? Gravitational Perturbations! We are already seeking to replace the traditional variation of parameters method in our second-order calculation for a new second-order Teukolsky calculation.\n\nhttps://t.co/oYJWPtHxNF 32/"", 'This work was done in collaboration with Rodrigo Panesso Macedo, Niels Warburton, Barry Wardell and Anƒ±l Zenginoƒülu. @Niels_JW @barry_wardell @AnilZenginoglu 33/']",https://arxiv.org/abs/2202.01794,"Gravitational self-force theory is the leading approach for modeling gravitational wave emission from small mass-ratio compact binaries. This method perturbatively expands the metric of the binary in powers of the mass ratio. The source for the perturbations depends on the orbital configuration, calculational approach, and the order of the perturbative expansion. These sources fall into three broad classes: (i) distributional, (ii) worldtube, and (iii) unbounded support. The latter, in particular, is important for emerging second-order (in the mass ratio) calculations. Traditional frequency domain approaches employ the variation of parameters method and compute the perturbation on standard time slices with numerical boundary conditions supplied at finite radius from series expansions of the asymptotic behavior. This approach has been very successful, but the boundary conditions calculations are tedious, and the approach is not well suited to unbounded sources where homogeneous solutions must be computed at all radii. This work develops an alternative approach where hyperboloidal slices foliate the spacetime, and compactifying coordinates simplify the boundary treatment. We implement this approach with a multi-domain spectral solver with analytic mesh refinement and use the scalar-field self-force on circular orbits around a Schwarzschild black hole as an example problem. The method works efficiently for all three source classes encountered in self-force calculations and has distinct advantages over the traditional approach. For example, our code efficiently computes the perturbation for orbits with extremely large orbital radii ($r_{p}>10^5M$) or modes with very high spherical harmonic mode index ($\ell \ge 100$). Our results indicate that hyperboloidal methods can play an essential role in self-force calculations. ",Hyperboloidal method for frequency-domain self-force calculations
112,1490609016917172224,2969696397,Ion Nechita,"['New paper <LINK> with @MariaJivulescu and @TeikoHeinosaari about the post-processing order of quantum measurements. We analyze this fundamental partial order, which lies at the heart of incompatibility of quantum measurements', '@TeikoHeinosaari has written a very nice post about it https://t.co/EYAGkY6hUn https://t.co/V2nkfjxb5m']",https://arxiv.org/abs/2202.00725,"We study the partially ordered set of equivalence classes of quantum measurements endowed with the post-processing partial order. The post-processing order is fundamental as it enables to compare measurements by their intrinsic noise and it gives grounds to define the important concept of quantum incompatibility. Our approach is based on mapping this set into a simpler partially ordered set using an order preserving map and investigating the resulting image. The aim is to ignore unnecessary details while keeping the essential structure, thereby simplifying e.g. detection of incompatibility. One possible choice is the map based on Fisher information introduced by Huangjun Zhu, known to be an order morphism taking values in the cone of positive semidefinite matrices. We explore the properties of that construction and improve Zhu's incompatibility criterion by adding a constraint depending on the number of measurement outcomes. We generalize this type of construction to other ordered vector spaces and we show that this map is optimal among all quadratic maps. ",Order preserving maps on quantum measurements
113,1490593473011109890,1392935011,Ole-Chr. Granmo,"['Congratulations to Raihan Seraj at McGill University for his new #ContextualBandit scheme based on the #TsetlinMachine and #ThompsonSampling, outperforming popular algorithms on eight out of nine benchmarks! üéâüéâüéâ Paper: <LINK> Code: <LINK> #ML <LINK>']",https://arxiv.org/abs/2202.01914,"This paper introduces an interpretable contextual bandit algorithm using Tsetlin Machines, which solves complex pattern recognition tasks using propositional logic. The proposed bandit learning algorithm relies on straightforward bit manipulation, thus simplifying computation and interpretation. We then present a mechanism for performing Thompson sampling with Tsetlin Machine, given its non-parametric nature. Our empirical analysis shows that Tsetlin Machine as a base contextual bandit learner outperforms other popular base learners on eight out of nine datasets. We further analyze the interpretability of our learner, investigating how arms are selected based on propositional expressions that model the context. ",Tsetlin Machine for Solving Contextual Bandit Problems
114,1489667082446729216,2377407248,Daniel Whiteson,"['New paper!\n\n‚ÄúResolving Extreme Jet Substructure‚Äù\n\nLed by Alexis Romero and @mfentonHEP together with UCI computer science whizzes:\n\n<LINK>\n\nThread ‚§µÔ∏è', 'There‚Äôs been a lot of study of fat jets with 2 or 3 hard subjets from quarks. https://t.co/AOJ7087M1k', 'Our existing jet substructure variables do a solid job, performing well compared even to deep networks\n\nBut what about jets with four hard subjets? https://t.co/gsy9uoPYbD', 'Or what about six? Or EIGHT?\n\nAt future, higher-energy colliders, the large momentum could lead to such extreme jets. Can our substructure variables handle them? https://t.co/P81AMX4gWl', 'We pitted a network of 136 substructure variables against a Particle Flow Network and a Transformer which used the calorimeter cells. For jets with up to eight hard subjets, the existing variables did very well! https://t.co/x7CNM1CWJ8', 'We expanded the set of variables to try to close the gap (DNN299) and then used LASSO to zoom in on the most important (DNN31). https://t.co/nGsUTWzMeO', 'Here‚Äôs a snapshot of some of the most useful features. https://t.co/D1BG0WjYn1', 'Lots more nuance, topology dependence and stratification, discussed in the paper: https://t.co/G2H2VrSCy5']",https://arxiv.org/abs/2202.00723,"We study the effectiveness of theoretically-motivated high-level jet observables in the extreme context of jets with a large number of hard sub-jets (up to $N=8$). Previous studies indicate that high-level observables are powerful, interpretable tools to probe jet substructure for $N\le 3$ hard sub-jets, but that deep neural networks trained on low-level jet constituents match or slightly exceed their performance. We extend this work for up to $N=8$ hard sub-jets, using deep particle-flow networks (PFNs) and Transformer based networks to estimate a loose upper bound on the classification performance. The high-level jet observables perform well, with an overall classification accuracy of 86.90$\%$, but fall short of the PFN and Transformer models, which reach classification accuracies of 89.19$\%$ and 91.27$\%$ respectively, suggesting that the constituent networks utilize information not captured by the set of high-level observables. We then identify additional high-level observables which are able to narrow this gap, and utilize LASSO regularization for feature selection to identify and rank the most relevant observables and provide further insights into the learning strategies used by the constituent-based neural networks. The final model contains only 31 high-level observables and is able to match the performance of the PFN and approximate the performance of the Transformer model to within 2$\%$. ",Resolving Extreme Jet Substructure
115,1489550342186577924,39688015,Chris Usher,"['New paper out from the galaxy group here in Stockholm, led by PhD student Lorenza Della Bruna, on our study of the nearby spiral galaxy M83 with MUSE. Pictured is our maps of starlight and ionised gas <LINK> <LINK>']",https://arxiv.org/abs/2202.01738,"We present a large VLT/MUSE mosaic (3.8 x 3.8 kpc) of the nearby spiral galaxy M83, with a spatial resolution ~20 pc. We obtained the kinematics of the stars and ionised gas, and compared them with molecular gas kinematics from ALMA CO(2-1). We separated the ionised gas into HII regions and diffuse ionised gas (DIG) and determined the fraction of Ha luminosity originating from the DIG (f_DIG). We observe that both stars and gas trace the galactic disk rotation, as well as a fast-rotating nuclear component, likely connected to secular processes driven by the galactic bar. In the gas kinematics, we observe a stream east of the nucleus, redshifted with respect to the disk. The stream is surrounded by an extended ionised gas region with enhanced velocity dispersion and a high ionisation state, which is largely consistent with being ionised by slow shocks. We interpret this feature as either the superposition of the disk and an extraplanar layer of DIG, or as a bar-driven inflow of shocked gas. A double Gaussian component fit to the Ha line also reveals the presence of a nuclear biconic structure whose axis of symmetry is perpendicular to the bar. The two cones appear blue- and redshifted along the line of sight and stand out for having an Ha emission separated by up to 200 km s-1 from that of the disk, and a high velocity dispersion ~80-200 km s-1. At the far end of the cones, we observe that the gas is consistent with being ionised by shocks. These features had never been observed before in M83; we postulate that they are tracing a starburst-driven outflow shocking into the surrounding ISM. Finally, we obtain f_DIG ~ 13% in our field of view. We inspect the emission of the HII regions and DIG in `BPT' diagrams, finding that in HII regions photoionisation accounts for 99.8% of the Ha flux, whereas the DIG has a mixed contribution from photoionisation (94.9%) and shocks (5.1%). [abridged] ","Stellar feedback in M83 as observed with MUSE -- I. Overview, an
  unprecedented view of the stellar and gas kinematics and evidence of
  outflowing gas"
116,1489529011462156288,3358932880,Joschka Roffe,"['New paper on the arXiv: Bias-tailored quantum LDPC codes, w/ Larry Cohen, @armanda_oq, @daryuschandra and @earltcampbell <LINK> üßµüëá(1/n)', 'Often, in QEC theory, we like to study things in terms of depolarising noise where px=py=pz. However, in the real world, it will usually be the case that the strength of one error types dominates over the others. (2/n)', 'Last year Ataides et al. showed that a modified form of the surface code, the XZZX code, can take advantage of biased-noise to significantly improve code performance https://t.co/tKeA2ktjO3 (3/n)', 'In this project, we set out to extend the idea of bias-tailoring to more general quantum LDPC codes (which promise much lower overheads than the surface code). To do this, we first looked to unpick exactly what allows the XZZX code to respond so well to biased noise... (4/n)', 'Essentially, this boils down to two modifications over the surface code: 1) A redefinition of the stabilisers that ensures the XZZX code decouples to sets of repetition codes in the infinite-bias limit; 2) Twisted boundary checks that lead to longer logical operators... (5/n)', ""We discovered that both characteristics of the XZZX code -- stabiliser redefinition and boundary twists -- arise naturally from a modified form of Pantaleev and Kalachev's lifted product (of recent asymptotically good d~n scaling fame)... (6/n)"", ""This modified form, which we've coined the bias-tailored lifted product, is obtained from the standard lifted product via a Clifford transformation on a subset of the qubits. The bias-tailored lifted product can be applied to any pair of quasi-cyclic LDPC codes to... (7/n)"", 'construct quantum LDPC codes that inherit the high-bias performance of the XZZX code. Pictured is a decoding plot of a bias-tailored lifted product code with parameters [[416,16,d~20]] under increasing X-bias... (8/n) https://t.co/ZvL02A91hL', 'The code construction scripts and decoding simulation routines can be found on the Github repo for this project.\nhttps://t.co/xNJw8sUyYH']",https://arxiv.org/abs/2202.01702,"Bias-tailoring allows quantum error correction codes to exploit qubit noise asymmetry. Recently, it was shown that a modified form of the surface code, the XZZX code, exhibits considerably improved performance under biased noise. In this work, we demonstrate that quantum low density parity check codes can be similarly bias-tailored. We introduce a bias-tailored lifted product code construction that provides the framework to expand bias-tailoring methods beyond the family of 2D topological codes. We present examples of bias-tailored lifted product codes based on classical quasi-cyclic codes and numerically assess their performance using a belief propagation plus ordered statistics decoder. Our Monte Carlo simulations, performed under asymmetric noise, show that bias-tailored codes achieve several orders of magnitude improvement in their error suppression relative to depolarising noise. ",Bias-tailored quantum LDPC codes
117,1489494341550854144,1047808083995578368,Oded Zilberberg Group,"['Interested in nonlinear out-of-equilibrium systems? Check out our new @JuliaLanguage package <LINK> and its white paper @arxiv (<LINK>). \n@ETH_physics, @UniKonstanz, @SFB1432, @NCCR_QSIT. With Jan Ko≈°ata, Javier del Pino, and Toni L. Heugel <LINK>', ""You can also here more about it today in Javier's talk today at @NCCR_QSIT Arosa meeting.""]",http://arxiv.org/abs/2202.00571,"HarmonicBalance.jl is a publicly available Julia package designed to simplify and solve systems of periodic time-dependent nonlinear ordinary differential equations. Time dependence of the system parameters is treated with the harmonic balance method, which approximates the system's behaviour as a set of harmonic terms with slowly-varying amplitudes. Under this approximation, the set of all possible steady-state responses follows from the solution of a polynomial system. In HarmonicBalance.jl, we combine harmonic balance with contemporary implementations of symbolic algebra and the homotopy continuation method to numerically determine all steady-state solutions and their associated fluctuation dynamics. For the exploration of involved steady-state topologies, we provide a simple graphical user interface, allowing for arbitrary solution observables and phase diagrams. HarmonicBalance.jl is a free software available at this https URL ","HarmonicBalance.jl: A Julia suite for nonlinear dynamics using harmonic
  balance"
118,1489462654456508417,1056335529497780226,Kumahlab,"['Our new paper on how atomic positions change as you make a film of  metallic/magnetic oxide SrRuO3 atomically thin  now on arXiV. Visualizing atoms using the @advancedphoton and electron microscopes @NCState allow us to build new functional materials <LINK> <LINK>', 'Work was led by @ncstatephysics   grad student Xuanyi Zhang and @AubreyPenn3 in collaboration with Lindfors-Vrejoiu group at the University of Cologne']",https://arxiv.org/abs/2202.01652,"Due to the strong lattice-property relationships which exist in complex oxide epitaxial layers, their electronic and magnetic properties can be modulated by structural distortions induced at the atomic scale. The modification and control can be affected at coherent heterointerfaces by epitaxial strain imposed by the substrate or by structural modifications to accommodate the film-substrate symmetry mismatch. Often these act in conjunction with a strong dependence on the layer thickness, especially for ultrathin layers. Moreover, as a result of these effects, the temperature dependence of the structure may deviate largely from that of the bulk. The temperature-dependent structure of 3 to 44 unit cell thick ferromagnetic SrRuO$_3$ films grown on Nb-doped SrTiO$_3$ substrates are investigated using a combination of high-resolution synchrotron X-ray diffraction and high-resolution electron microscopy. This aims to shed light on the intriguing magnetic and magnetotransport properties of epitaxial SRO layers, subjected to extensive investigations lately. The oxygen octahedral tilts and rotations are found to be strongly dependent on the temperature, the film thickness, and the distance away from the film-substrate interface. As a striking manifestation of the coupling between magnetic order and lattice structure, the Invar effect is observed below the ferromagnetic transition temperature in epitaxial layers as thin as 8 unit cells, similar to bulk ferromagnetic SrRuO$_3$. ","Thickness and temperature dependence of the atomic-scale structure of
  SrRuO$_3$ thin films"
119,1489295648826564611,883039700,Lenka Zdeborova,"['Dear 2nd referee of our ICML 2019 submission. We finally managed to answer your question about the relation between the Saad&amp;Solla analysis of two-layer neural networks and the one referred to as mean-field/hydrodynamic limit. Please see our new paper: <LINK> <LINK>', 'With @rodsveiga @_brloureiro Ludovic Stephan, and @KrzakalaF  In the figure the axed are scaling exponents of the learning and of the network width with dimension.']",https://arxiv.org/abs/2202.00293,"Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates. ","Phase diagram of Stochastic Gradient Descent in high-dimensional
  two-layer neural networks"
120,1489183631419625476,561167071,Sascha Caron,"['Glad to see a new paper via our collaboration between HEP \u200b\u200band @RUastro via @dark_machines:\n\nAutoSourceID-Light (ASID-L)  uses computer vision  that can  deal with large amounts of data and rapidly localize sources in optical images:\n<LINK>\n\nFactor 100 faster ! <LINK>', 'Main work by Firorenzo Stoppa.']",https://arxiv.org/abs/2202.00489,"$\textbf{Aims}$. With the ever-increasing survey speed of optical wide-field telescopes and the importance of discovering transients when they are still young, rapid and reliable source localization is paramount. We present AutoSourceID-Light (ASID-L), an innovative framework that uses computer vision techniques that can naturally deal with large amounts of data and rapidly localize sources in optical images. $\textbf{Methods}$. We show that the AutoSourceID-Light algorithm based on U-shaped networks and enhanced with a Laplacian of Gaussian filter (Chen et al. 1987) enables outstanding performances in the localization of sources. A U-Net (Ronneberger et al. 2015) network discerns the sources in the images from many different artifacts and passes the result to a Laplacian of Gaussian filter that then estimates the exact location. $\textbf{Results}$. Application on optical images of the MeerLICHT telescope demonstrates the great speed and localization power of the method. We compare the results with the widely used SExtractor (Bertin & Arnouts 1996) and show the out-performances of our method. AutoSourceID-Light rapidly detects more sources not only in low and mid crowded fields, but particularly in areas with more than 150 sources per square arcminute. ","AutoSourceID-Light. Fast Optical Source Localization via U-Net and
  Laplacian of Gaussian"
121,1489172518757969923,166527685,Riccardo Di Clemente,['How can we capture the #European economic Integration to reach global #competitiveness? Check out our new paper: Understanding European Integration with #Bipartite #networks of Comparative Advantage\n<LINK>\nWith \n@blengyelb Lars-Fredrik Andersson Rikard Eriksson <LINK>'],https://arxiv.org/abs/2202.01080,"Core objectives of the European integration are convergence and economic growth, but these are challenged by competition and value chain asymmetries within the common market. A difficult challenge for the EU is how to harmonize specialization of industries and countries to reach global competitiveness, and at the same time bridge productivity differences across more and less developed countries. Here, we develop a novel bipartite network approach and trace pairwise co-specialization, by applying the widely used revealed comparative advantage (RCA) method, within and between EU15 and Central and Eastern European (CEE) member states from 2000. This new co-specialization approach can be used to assess redundancies and division in the system as a whole, and at the level of industries and countries as well. This latter feature enables us to investigate how co-specialization across countries impact economic growth. We find significant overlap of RCA among CEE countries but a diverging RCA structure between EU15 and CEE. Our econometric analysis indicates that productivity increases in those CEE industries that have co-specialized with other CEE countries after EU accession, while co-specialization across CEE and EU15 countries is less related to productivity growth. These results inform European policy that a division of sectoral specialization can lead to productivity convergence between EU15 and CEE member states. ","Understanding European Integration with Bipartite Networks of
  Comparative Advantage"
122,1489159629527404546,1486168076697894916,Seohong Park,"[""Introducing LSD, a new unsupervised skill discovery method that discovers diverse and 'dynamic' skills without external rewards. üòÉ #ICLR2022\n\nPaper: <LINK>\nCode &amp; videos: <LINK>\n\nSummary thread:\n1/ <LINK>"", 'Many previous unsupervised skill discovery methods maximize the mutual information (MI) between skills and states (I(Z; S)), but the MI objective often favors static skills with small state variations.\n2/ https://t.co/1wKpIK7U92', 'Instead of mutual information, LSD maximizes an inner product objective with a Lipschitz constraint so that it encourages more dynamic skills in terms of state variations.\n3/ https://t.co/Sg9HB93IGz', 'On Ant, an LSD agent trained with 16 discrete skills discovered a set of behaviors consisting of 5 locomotion skills, 6 rotation skills, 3 posing skills and 2 flipping skills!\n4/ https://t.co/CaDgGLPjHr', ""Moreover, with LSD's learned representation function and skills, the agent can follow multiple goals in a zero-shot manner.\n5/ https://t.co/XpWuuPqotn"", 'LSD discovers diverse and dynamic behaviors regardless of the random seed.\nCheck out our project page for more videos, results and code: https://t.co/ca4yGL7LYj\n6/ https://t.co/ieFIPK757N', 'Many thanks to the amazing collaborators üòÉ @wookayin @jaekyeom__Kim @honglaklee @gunheekim\n7/']",https://arxiv.org/abs/2202.00914,"We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner - i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at this https URL ",Lipschitz-constrained Unsupervised Skill Discovery
123,1489155948560195587,1236348926153895937,Ricardo Salinas,"['aaand, we have a new paper with Gemini data. This time with an all-star group of stellar abundances experts :) <LINK>']",https://arxiv.org/abs/2202.00849,"Using spectra obtained with the VLT/FORS2 and Gemini-S/GMOS instruments we have investigated carbon, nitrogen and sodium abundances in a sample of red giants in the Small Magellanic Cloud cluster Kron 3. The metallicity and luminosity of this cluster are comparable to those of Galactic globular clusters although with a notably younger age of $\sim$ 6.5 Gyr. Specifically we have investigated the strengths of the CH ($\lambda$ 4300 A) and CN ($\lambda$ 3800, $\lambda$ 4215) molecular bands finding a bimodality of CN band-strengths and a CH/CN anti-correlation. Application of spectrum synthesis techniques reveals a large spread ($\sim$1.2 dex) in nitrogen abundance and a spread in [C/Fe] of $\sim$0.3 dex after applying corrections for evolutionary mixing. We have also estimated sodium abundances from the strengths of the Na D lines finding a range of $\sim$0.8 dex in [Na/Fe] that correlates positively with the N abundances. This is the first star-by-star spectroscopic demonstration of correlated Na, N abundance variations in an intermediate-age star cluster, adding to existing photometric and spectroscopic indications of the presence of multiple populations in such clusters with masses in excess of $\sim 10^5$ solar masses. Our results confirm that the mechanism(s) responsible for the multiple populations observed in globular clusters cannot be an early cosmological effect applying only in old clusters, and provide a key additional factor in the quest to understand the origin of the abundance anomalies. ","Evidence of globular cluster abundance anomalies in the SMC
  intermediate-age cluster Kron 3"
124,1489149555249713152,1015534555384766464,Thomas Van Riet,"['\'You gotta keep em separated\' \n\n<LINK>\n\nNew paper with ""El Risitas"" (@MigMontero), Timm Wrase and our former master student, Fien Apers, who is really behind this all! She is now a PhD student at Oxford, working with  @JosephPConlon', '@Zugzwang_Sor thanks!']",https://arxiv.org/abs/2202.00682,"AdS flux vacua with a parametric separation between the AdS and KK scales have been conjectured to be in the Swampland. We study flux compactifications of massive IIA supergravity with O6 planes which are claimed to allow moduli-stabilised and scale separated AdS$_3$ and AdS$_4$ vacua at arbitrary weak coupling and large volume. A recent refinement of the AdS Distance Conjecture is shown to be inconsistent with the class of AdS$_3$ vacua because the requisite discrete higher form symmetries are absent. We further perform a tree-level study of non-perturbative decays for the nonsupersymmetric versions of the AdS$_3$ solutions, and find that the vacua are stable within this approximation. Finally, we provide an initial investigation of the would-be dual CFT$_2$'s and CFT$_3$'s. We study roughly a dozen different models and find for all AdS$_4$ DGKT-type vacua that the dual operators to the lightest scalars have integer dimensions. For the putative CFT$_2$ dual theories of the AdS$_3$ vacua we find no integer dimensions for the operators. ",Comments on classical AdS flux vacua with scale separation
125,1489086274388459524,990433714948661250,Sergey Levine,"[""Can gradient-based meta-learning be performed entirely online? Jathushan Rajasegaran's new paper proposes a *fully* online version of MAML (FOML), with two parameter vectors that are updated incremental (a prior and posterior): <LINK>\n\nA thread: <LINK>"", 'Standard MAML is batch mode, but in reality we might observe training samples in sequence, and the task might shift, suddenly or gradually. We want to use each datapoint *both* to improve our model *and* to learn how to learn more quickly for when the task changes.', 'In realistic lifelong settings, we might not know when the ""task"" changes (e.g., maybe autonomous car enters a new city, or a rec system model must adapt to a user\'s changing preferences). So we must *constantly* both adapt and meta-train.', 'FOML is an elegant algorithm for this that maintains two parameter vectors: a posterior representing model weights, and a ""prior"" that regularizes those weights to enable fast adaptation. Prior = slow weight, posterior = fast weight. Training is a fully online version of MAML. https://t.co/bSrYWMpUyY', 'We constructed 3 datasets where images are revealed one at a time, and tasks change gradually. FOML adapts quickly, and learns to adapt *even more quickly* as it sees more tasks, outperform even prior methods that get true task boundaries! https://t.co/IceDuwWUlJ', 'This paper is by Jathushan Rajasegaran, @chelseabfinn, and myself, and follows some of our earlier work on online meta-learning: https://t.co/e4bGPL2QLp\n\nCheck out the FOML paper here: https://t.co/9OYgGif7Fu']",https://arxiv.org/abs/2202.00263,"While deep networks can learn complex functions such as classifiers, detectors, and trackers, many applications require models that continually adapt to changing input distributions, changing tasks, and changing environmental conditions. Indeed, this ability to continuously accrue knowledge and use past experience to learn new tasks quickly in continual settings is one of the key properties of an intelligent system. For complex and high-dimensional problems, simply updating the model continually with standard learning algorithms such as gradient descent may result in slow adaptation. Meta-learning can provide a powerful tool to accelerate adaptation yet is conventionally studied in batch settings. In this paper, we study how meta-learning can be applied to tackle online problems of this nature, simultaneously adapting to changing tasks and input distributions and meta-training the model in order to adapt more quickly in the future. Extending meta-learning into the online setting presents its own challenges, and although several prior methods have studied related problems, they generally require a discrete notion of tasks, with known ground-truth task boundaries. Such methods typically adapt to each task in sequence, resetting the model between tasks, rather than adapting continuously across tasks. In many real-world settings, such discrete boundaries are unavailable, and may not even exist. To address these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and stays fully online without resetting back to pre-trained weights. Our experiments show that FOML was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets. ",Fully Online Meta-Learning Without Task Boundaries
126,1488938966485250050,226366834,Hattie Zhou,"['[New paper üö®] You‚Äôve heard of catastrophic forgetting, but have you heard of *fortuitous* forgetting? \n\nIn this paper, we show how forgetting can be a friend of learning in artificial neural nets. With @AnkitKVani, @hugo_larochelle, @AaronCourville (1/6)\n\n<LINK>', 'We introduce ‚Äúforget-and-relearn‚Äù as a powerful paradigm for steering the learning trajectory of NNs. \n\nIn this process, the forgetting step removes undesirable information and the relearning step amplifies features that are useful under different learning conditions. (2/6)', 'We show that many iterative training algos (e.g. LT pruning, knowledge evolution, iterated learning) can be seen as instances of forget-and-relearn, and that their success is proportional to how well the forgetting operations can selectively remove undesirable information\n(3/6)', 'Thus, by designing more targeted forgetting methods, we can significantly improve the performance of these algorithms.\n\nOur analyses also illustrate how the relearning stage amplifies features that are consistently useful, and how this corresponds to better generalization. (4/6)', 'Forget-and-relearn suggests a shift in thinking from ‚Äúlearning desirable information‚Äù to ‚Äúforgetting undesirable information‚Äù. It‚Äôs often easier to suppress unwanted behavior than to delineate good behavior, and we hope this work will inspire new forgetting-based algorithms (5/6)', 'Lastly, I‚Äôm really grateful for my collaborators who made this project super fun to work on, as well as for the friends who offered valuable feedback throughout this work ‚ù§Ô∏è\n\nExcited to present this at #ICLR2022 ü•≥\n\nFeel free to reach out if you want to discuss these ideas! (6/6)']",http://arxiv.org/abs/2202.00155,"Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce ""forget-and-relearn"" as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements. ",Fortuitous Forgetting in Connectionist Networks
127,1488926126017851401,1674028088,Misha Laskin üá∫üá¶,"['New paper on unsupervised skill discovery - Contrastive Intrinsic Control.\n\nTl;dr exploration with contrastive skill learning substantially improves prior skill discovery methods (by 1.8x)! Achieves leading unsupervised RL results. \n\n<LINK>\n\nLearn more üëá\n\n1/N <LINK>', 'When we released the Unsupervised RL Benchmark we noticed that of the three types of unsupervised RL algorithms (competence, knowledge, and data-based) competence-based ones like DIAYN underperform relative to the other ones. Why is that?\n\n2/N https://t.co/jSp3SQlR98', 'First, a primer on competence-based exploration.These algorithms explore by maximizing the mutual information between states and skills (usually a latent vector sampled from noise). \n\n3/N https://t.co/XnaaB2rXFX', 'Mutual information is usually intractable so most algorithms maximize a lower bound. q(z|tau) in the lower bound, called the discriminator, is basically a classifier from behaviors to skills. Like any classifier q(z|tau) needs a lot of data per class to be accurate. \n\n4/N https://t.co/11S1x7l7Fh', 'For this reason dim(z) (i.e. the # of classes) is usually small. Otherwise, you would need an exponentially larger number of data samples to fit q(z|tau) accurately which would make your unsupervised RL algorithm very data inefficient.\n\n5/N', 'This is a big problem if your environment supports many different behaviors, since the discriminator will only learn a small number of skills that are easiest to classify. E.g. here\'s what happens when we train DIAYN on DeepMind Control. The agent gets stuck in ""yoga poses"".\n\n6/N https://t.co/iM61PC2IxL', ""Note: this is not an issue in OpenAI Gym because Gym envs reset early when the agent falls over, so the set of possible behaviors small. If we run DIAYN on OpenAI Gym it does well, but then if we run  it on DeepMind Control it doesn't learn. \n\n7/N https://t.co/VEf6J6FGRK"", 'Main idea of Contrastive Intrinsic Control (CIC) is to use a contrastive estimator for the discriminator. This should allow the discriminator to support larger skills and learn better representations than a simple classifier. \n\n8/N https://t.co/dpqEsTQc6A', 'In this context, there are no data augs as is usual with contrastive learning. Instead, we are doing contrastive learning over skills that are sampled from noise and projected as shown in the diagram below. The intrinsic reward is the entropy over contrastive embeddings.\n\n9/N https://t.co/RpN8rgVQrL', ""And that completes CIC algorithm. Due to better representation learning the CIC discriminator learns a lot of different diverse behaviors. Qualitatively here's what CIC behaviors look like after training with intrinsic rewards. Much more diverse than before!\n\n10/N https://t.co/opUZ2sx2ui"", 'Due to good exploration, CIC adapts quickly to downstream tasks. On the Unsupervised RL Benchmark CIC achieves a 1.79x higher score than prior competence-based algos and a 1.18x higher score than next-best exploration algo.\n\nCIC is the 1st competence algo to lead URLB.\n\n11/N https://t.co/rehWB7ADfa', 'If this was interesting to you, there are many more ablations / experiments in the paper. Had a lot of fun collaborating with @lhaoml @xbpeng4 @denisyarats @aravindr93 and @pabbeel!\n\nPaper: https://t.co/seP9PkQMnw\n\nWebsite &amp; code: https://t.co/dDbBkl1Qxo\n\n12/N END']",https://arxiv.org/abs/2202.00161,"We introduce Contrastive Intrinsic Control (CIC), an algorithm for unsupervised skill discovery that maximizes the mutual information between state-transitions and latent skill vectors. CIC utilizes contrastive learning between state-transitions and skills to learn behavior embeddings and maximizes the entropy of these embeddings as an intrinsic reward to encourage behavioral diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. CIC substantially improves over prior methods in terms of adaptation efficiency, outperforming prior unsupervised skill discovery methods by 1.79x and the next leading overall exploration algorithm by 1.18x. ",CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery
128,1488891517058134016,3276611372,Vidit Nanda,"[""Check out @chadgiusti's monster thread about our new paper on higher-dimensional analogues of iterated integrals and path signatures for mapping spaces!\n\n<LINK> <LINK>"", '@niveknosdunk @chadgiusti Ahhh I just discovered that you were Hain‚Äôs student! I mostly know him as half of the legendary duo who wrote this: \n\nhttps://t.co/XTyvKcuhie']",https://arxiv.org/abs/2202.00491,"A common approach for describing classes of functions and probability measures on a topological space $\mathcal{X}$ is to construct a suitable map $\Phi$ from $\mathcal{X}$ into a vector space, where linear methods can be applied to address both problems. The case where $\mathcal{X}$ is a space of paths $[0,1] \to \mathbb{R}^n$ and $\Phi$ is the path signature map has received much attention in stochastic analysis and related fields. In this article we develop a generalized $\Phi$ for the case where $\mathcal{X}$ is a space of maps $[0,1]^d \to \mathbb{R}^n$ for any $d \in \mathbb{N}$, and show that the map $\Phi$ generalizes many of the desirable algebraic and analytic properties of the path signature to $d \ge 2$. The key ingredient to our approach is topological; in particular, our starting point is a generalisation of K-T Chen's path space cochain construction to the setting of cubical mapping spaces. ",A Topological Approach to Mapping Space Signatures
129,1488880449480966146,1481400147221299201,Livio Nicola Carenza,['Check out our new paper on the relevant symmetries in epithelia! \nWe find that both hexatic and nematic order is needed to describe epithelial tissues with the former dominant at the small scales and the latter at larger ones.@LeidenPhysics\n <LINK>'],https://arxiv.org/abs/2202.00668,"Biological processes such as embryogenesis, wound healing and cancer progression, crucially rely on the ability of epithelial cells to coordinate their mechanical activity over length scales order of magnitudes larger than the typical cellular size. While regulated by signalling pathways, such as YAP (yes-associated protein), MAPK (mitogen-activated protein kinase) and Wnt, this behavior is believed to additionally hinge on a minimal toolkit of physical mechanisms, of which liquid crystal order is the most promising candidat. Yet, experimental and theoretical studies have given so far inconsistent results in this respect: whereas nematic order is often invoked in the interpretation of experimental data, computational models have instead suggested that hexatic order could in fact emerge in the biologically relevant region of parameter space. In this article we resolve this dilemma. Using a combination of in vitro experiments on Madin-Darby canine kidney cells (MDCK), numerical simulations and analytical work, we demonstrate that both nematic and hexatic order is in fact present in epithelial layers, with the former being dominant at large length scales and the latter at small length scales. In MDCK GII cells on uncoated glass, these different types of liquid crystal order crossover at $34 \mu$m, corresponding approximatively to clusters of $21$ cells. Our work sheds light on the emergent organization of living matter, provides a new set of tools for analyzing the structure of epithelia and paves the way toward a comprehensive and predictive mesoscopic theory of tissues. ",Epithelia are multiscale active liquid crystals
130,1488869144002969608,2930047588,Andrew Vanderburg,"[""I'm super excited today about this new paper from MIT freshman undergraduate Sam Christian (@SamChristian27) working with me and the @TESSatMIT team!  In it, Sam shows that the orbits of wide binary stars are often aligned with close-in planets! <LINK>"", 'There are two kinds of planets in binaries: Tatooine-like circumbinary planets (where the planet orbits both stars) and s-type systems (where the planet orbits one star, and the other star orbits around the them both). Sam focused on the s-type systems https://t.co/d5VPDFXS3R', ""Kepler taught us a lot about these systems - like that they common, but if the stars are too close together, planets may not form- but we didn't really know anything about the orbits of those binary stars other than the approximate separation. https://t.co/tfveR6J1fl"", 'But now, the Gaia mission has given us a new opportunity to study these systems! Gaia can measure the motions of stars so precisely that we can use it to learn about the binary star orbits! @loganpearce has a great package for estimating the orbits: https://t.co/vDyNvqaPhU https://t.co/6HSSZsNZ6Z', ""Sam studied the orbits of binary stars that happen to have transiting exoplanets. He matched @kjb_astro's catalog of wide binary stars with the @TESSatMIT  planet candidates, and constrained their orbits with @loganpearce's LOFTI package and ended up constraining 67 systems. https://t.co/vNCO2Apfne"", 'Because these are _transiting_ planets, we know their inclination is close to 90 degrees (edge-on orbits). LOFTI gives us the orbital inclinations of the binary stars, so we can compare the two and see if the planet and binary orbits could be aligned. https://t.co/yTayrbGKlG', ""Note: you can't do this for just a single system because there is another dimension besides inclination (longitude of ascending node) in which the two orbits could be misaligned. But if you have a large sample like Sam, you can see statistical evidence for alignment."", ""And that's exactly what Sam found! The difference in inclination between planet orbits and binary orbits showed significantly more systems consistent with alignment than we get from a randomized control sample! The difference is about 3 sigma significant. https://t.co/MQHUQNly7M"", 'Interestingly, the alignment is mostly present at binary orbital separation less than 700 AU. The more distant binaries (out to separations of almost 10,000 AU) are much more randomized, although some appear to be aligned as well. https://t.co/MXy34juL1B', ""What could be causing this alignment and the break at 700 AU? Maybe the binary stars could be torquing protoplanetary disks into alignment. We (@jcbastro)  calculated that close binaries can do this before disks dissipate, but binaries with semimajor axes &gt;700 AU generally can't. https://t.co/aVq71ntur5"", ""So we think we have learned something new and interesting about planets in binaries! We look forward to more TESS data and new planets to study. Also check out @TrentDupuy's paper last night, which comes to a similar conclusion using Kepler planets! https://t.co/s0OwvokHGU"", 'And finally, I want to emphasize what an impressive accomplishment this is for @SamChristian27! We started this project and did most of the work when he was still in high school @lasahighschool! I look forward to seeing his next great discoveries!', ""@PlavchanPeter @SamChristian27 I know right? Yeah, there's some occurrence rate work from Kepler (https://t.co/sRCDobaBRZ) but it's pre-Gaia and there's probably a lot we could do now that we couldn't do back then!""]",https://arxiv.org/abs/2202.00042,"Astronomers do not have a complete picture of the effects of wide-binary companions (semimajor axes greater than 100 AU) on the formation and evolution of exoplanets. We investigate these effects using new data from Gaia EDR3 and the TESS mission to characterize wide-binary systems with transiting exoplanets. We identify a sample of 67 systems of transiting exoplanet candidates (with well-determined, edge-on orbital inclinations) that reside in wide visual binary systems. We derive limits on orbital parameters for the wide-binary systems and measure the minimum difference in orbital inclination between the binary and planet orbits. We determine that there is statistically significant difference in the inclination distribution of wide-binary systems with transiting planets compared to a control sample, with the probability that the two distributions are the same being 0.0037. This implies that there is an overabundance of planets in binary systems whose orbits are aligned with those of the binary. The overabundance of aligned systems appears to primarily have semimajor axes less than 700 AU. We investigate some effects that could cause the alignment and conclude that a torque caused by a misaligned binary companion on the protoplanetary disk is the most promising explanation. ","A Possible Alignment Between the Orbits of Planetary Systems and their
  Visual Binary Companions"
131,1488788646966800385,1141006043218108419,Clara Isabel Meister,"['Neural language models are really good at explaining held-out data. So when we sample from them, why do they yield dull and degenerate text? \n\nOur paper analyzes this behavior using information theory, and corrects for it with a new sampling principle: <LINK> <LINK>', 'Humans use natural language as a vehicle for communication‚Äîexactly the concept info theory studies. Results from psycholinguistics tell us we use it in an efficient and somewhat predictable way. One consequence is that we pack an expected amount of information into each word.', 'Can we decode from a probabilistic language generator in a manner that mimics this process?', ""Yes! Our paper proposes a new decoding principle: Instead of sampling only high probability words, we should instead sample from the set of words whose information content is close to the expected information content, i.e., the model's conditional entropy."", 'We find that our decoding method leads to fewer repetitions than nucleus or top-k sampling while also performing strongly in quality ratings.\n\nJoint work with @tpimentelms @ryandcotterell and Gian! https://t.co/R2XO2usU0g']",https://arxiv.org/abs/2202.00666,"Despite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\`a la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions. ",Typical Decoding for Natural Language Generation
132,1488724141591379970,173043666,takuma akimoto,['[2202.00274] Infinite ergodic theory for three heterogeneous stochastic models with application to subrecoil laser cooling\n\nnew paper on infinite ergodic theory <LINK>'],https://arxiv.org/abs/2202.00274,"We compare ergodic properties of the kinetic energy for three stochastic models of subrecoil-laser-cooled gases. One model is based on a heterogeneous random walk (HRW), another is an HRW with long-range jumps (the exponential model), and the other is a mean-field-like approximation of the exponential model (the deterministic model). All the models show an accumulation of the momentum at zero in the long-time limit, and a formal steady state cannot be normalized, i.e., there exists an infinite invariant density. We obtain the exact form of the infinite invariant density and the scaling function for the exponential and deterministic models and devise a useful approximation for the momentum distribution in the HRW model. While the models are kinetically non-identical, it is natural to wonder whether their ergodic properties share common traits, given that they are all described by an infinite invariant density. We show that the answer to this question depends on the type of observable under study. If the observable is integrable, the ergodic properties such as the statistical behavior of the time averages are universal as they are described by the Darling-Kac theorem. In contrast, for non-integrable observables, the models in general exhibit non-identical statistical laws. This implies that focusing on non-integrable observables, we discover non-universal features of the cooling process, that hopefully can lead to a better understanding of the particular model most suitable for a statistical description of the process. This result is expected to hold true for many other systems, beyond laser cooling. ","Infinite ergodic theory for three heterogeneous stochastic models with
  application to subrecoil laser cooling"
133,1500847572360970246,762359343656361984,James Zou,"[""Worse #AI can ‚û°Ô∏è better human decisions. \n\nOur new paper shows this in 1000s of human-AI experiments: making AI's output to users overconfident can improve human decisions &gt; using calibrated AI. \n\nWe also propose framework for training AI to opt human perf <LINK> <LINK>"", 'Intuition is that human users are not calibrated so AI needs to un-calibrate itself to help more. \n\nThis study shows the importance of training AI to opt for the overall human perf, rather than standard AI metrics (esp when AI is one part of a team).  \n\nGreat work led by Kailas!']",https://arxiv.org/abs/2202.05983,"In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of ""confidence"" that the human can use to calibrate how much they depend on or trust the advice. In this paper, we demonstrate that presenting AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first learn a model for how humans incorporate AI advice using data from thousands of human interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks -- dealing with images, text and tabular data -- involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of and a framework for jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone. ",Uncalibrated Models Can Improve Human-AI Collaboration
134,1499586599314669568,1472501532,Vasileios Maroulas,['New paper joint with @gsiopsis and Bernardo Ameneyro. We develop a quantum algorithm for computing persistent Betti numbers. Thank you @NSF  for supporting of our work. @MathDeptUTK @ResearchUTK <LINK>'],https://arxiv.org/abs/2202.12965,"Persistent homology is a powerful mathematical tool that summarizes useful information about the shape of data allowing one to detect persistent topological features while one adjusts the resolution. However, the computation of such topological features is often a rather formidable task necessitating the subsampling the underlying data. To remedy this, we develop an efficient quantum computation of persistent Betti numbers, which track topological features of data across different scales. Our approach employs a persistent Dirac operator whose square yields the persistent combinatorial Laplacian, and in turn the underlying persistent Betti numbers which capture the persistent features of data. We also test our algorithm on point cloud data. ",Quantum Persistent Homology
135,1499489792857296896,218250514,Heiko Hamann,"[""fulltext of our new paper available now\n\n‚ÄúIf you could see me through my eyes‚Äù: \nPredicting Pedestrian Perception\n\n@Julian_Petzold et al.\n\nhere: modeling #pedestrian perception\nnext: autonomous cars taking pedestrians' perspectives\n\n@w9hby #ai #ml \n<LINK> <LINK>""]",https://arxiv.org/abs/2202.13981,"Pedestrians are particularly vulnerable road users in urban traffic. With the arrival of autonomous driving, novel technologies can be developed specifically to protect pedestrians. We propose a machine learning toolchain to train artificial neural networks as models of pedestrian behavior. In a preliminary study, we use synthetic data from simulations of a specific pedestrian crossing scenario to train a variational autoencoder and a long short-term memory network to predict a pedestrian's future visual perception. We can accurately predict a pedestrian's future perceptions within relevant time horizons. By iteratively feeding these predicted frames into these networks, they can be used as simulations of pedestrians as indicated by our results. Such trained networks can later be used to predict pedestrian behaviors even from the perspective of the autonomous car. Another future extension will be to re-train these networks with real-world video data. ","""If you could see me through my eyes"": Predicting Pedestrian Perception"
136,1499200262937845760,206026629,Richard Sutton,"['If you take all the fields that study intelligent decision making‚Äîfrom neuroscience to AI, psychology to control theory, economics to operations research‚Äîdo their theories have much in common? I think so, as I explain in this new short paper: <LINK>']",https://arxiv.org/abs/2202.13252,"The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the ""common model of the intelligent agent"". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent. ",The Quest for a Common Model of the Intelligent Decision Maker
137,1497285329925316608,825579580597628930,Soumyabrata Pal,"['New paper (<LINK>) titled ‚ÄúOn Learning Mixture Models with Sparse Parameters‚Äù is accepted to AISTATS 2022! This is a joint work with Arya Mazumdar (@MountainOfMoon ).', 'Overall, the main technical contribution in this paper is to introduce a very novel and general framework for support recovery in a variety of mixture models where we reduce the support recovery problem to estimating certain statistics of the indices. Below, is a detailed thread:', 'Suppose we obtain samples from a uniform mixture model with unknown parameters (high dimensional). Examples include mixtures of Distributions (MD) with unknown means, mixtures of linear regressions (MLR)/classifiers (MLC)  with unknown weights, and Gaussian covariates.', 'We assume that the parameter vectors are sparse and formulate the objective of finding the support of the unknown parameter vectors. Support recovery is useful as a pre-processing step for feature selection and can significantly speed up subsequent learning algorithms.', 'In MD setting, we provide sample complexity guarantees that hold for most component distributions. In particular, our results for support recovery hold for mixtures of Gaussians, mixtures of Poisson, mixtures of Laplacian among many others.', 'Importantly, our sample complexity guarantees scale logarithmically with the dimension.  We use the canonical method of moments for estimating the parameters where we show an upper bound on the sufficient number of moments via an interesting application of Newton‚Äôs identities.', 'In MLR/MLC, we provide similar sample complexity guarantees for support recovery under certain assumptions. Although sometimes, it is trivial to find the union of support and then apply known parameter estimation algorithms, we show that our results improve on these baselines.']",https://arxiv.org/abs/2202.11940,"Mixture models are widely used to fit complex and multimodal datasets. In this paper we study mixtures with high dimensional sparse latent parameter vectors and consider the problem of support recovery of those vectors. While parameter learning in mixture models is well-studied, the sparsity constraint remains relatively unexplored. Sparsity of parameter vectors is a natural constraint in variety of settings, and support recovery is a major step towards parameter estimation. We provide efficient algorithms for support recovery that have a logarithmic sample complexity dependence on the dimensionality of the latent space. Our algorithms are quite general, namely they are applicable to 1) mixtures of many different canonical distributions including Uniform, Poisson, Laplace, Gaussians, etc. 2) Mixtures of linear regressions and linear classifiers with Gaussian covariates under different assumptions on the unknown parameters. In most of these settings, our results are the first guarantees on the problem while in the rest, our results provide improvements on existing works. ",On Learning Mixture Models with Sparse Parameters
138,1497105584017256453,57294647,Arindam Khan,"['New Arxiv paper!\nWhile packing for traveling, we have many items to pack into a limited-space suitcase. This is a variant of 3-D knapsack. \nWe give tight approximation algorithm when items are cubes, for any d-D, settling a decade-long open conjecture. \n\n<LINK> <LINK>']",https://arxiv.org/abs/2202.11902,"We study the d-dimensional hypercube knapsack problem where we are given a set of d-dimensional hypercubes with associated profits, and a knapsack which is a unit d-dimensional hypercube. The goal is to find an axis-aligned non-overlapping packing of a subset of hypercubes such that the profit of the packed hypercubes is maximized. For this problem, Harren (ICALP'06) gave an algorithm with an approximation ratio of (1+1/2^d+epsilon). For d=2, Jansen and Solis-Oba (IPCO'08) showed that the problem admits a polynomial-time approximation scheme (PTAS); Heydrich and Wiese (SODA'17) further improved the running time and gave an efficient polynomial-time approximation scheme (EPTAS). Both the results use structural properties of 2-D packing, which do not generalize to higher dimensions. For d>2, it remains open to obtain a PTAS, and in fact, there has been no improvement since Harren's result. We settle the problem by providing a PTAS. Our main technical contribution is a structural lemma which shows that any packing of hypercubes can be converted into another structured packing such that a high profitable subset of hypercubes is packed into a constant number of special hypercuboids, called V-Boxes and N-Boxes. As a side result, we give an almost optimal algorithm for a variant of the strip packing problem in higher dimensions. This might have applications for other multidimensional geometric packing problems. ",A PTAS for Packing Hypercubes into a Knapsack
139,1496119951068913664,1203290389307891713,Saugata Basu,"['New paper on the arXiv. ""Persistent homology of semi-algebraic sets"" (with Negin Karisani). Continuous to finite filtrations, barcodes of non-proper maps, algorithm for computing barcodes of proper semi-algebraic maps.\n<LINK> <LINK>']",https://arxiv.org/abs/2202.09591,"We give an algorithm with singly exponential complexity for computing the barcodes up to dimension $\ell$ (for any fixed $\ell \geq 0$) of the filtration of a given semi-algebraic set by the sub-level sets of a given polynomial. Our algorithm is the first algorithm for this problem with singly exponential complexity, and generalizes the corresponding results for computing the Betti numbers up to dimension $\ell$ of semi-algebraic sets with no filtration present. ",Persistent homology of semi-algebraic sets
140,1495806164856102917,2751320737,Amir Barati,['Our new paper on #molecular #machinelearning for self supervised learning: iMolCLR: Improving Molecular #Contrastive #Learning via Faulty Negative Mitigation: <LINK>. Pushing #SOTA on some more molecular #ML tasks. <LINK>'],https://arxiv.org/abs/2202.09346,"Deep learning has been a prevalence in computational chemistry and widely implemented in molecule property predictions. Recently, self-supervised learning (SSL), especially contrastive learning (CL), gathers growing attention for the potential to learn molecular representations that generalize to the gigantic chemical space. Unlike supervised learning, SSL can directly leverage large unlabeled data, which greatly reduces the effort to acquire molecular property labels through costly and time-consuming simulations or experiments. However, most molecular SSL methods borrow the insights from the machine learning community but neglect the unique cheminformatics (e.g., molecular fingerprints) and multi-level graphical structures (e.g., functional groups) of molecules. In this work, we propose iMolCLR: improvement of Molecular Contrastive Learning of Representations with graph neural networks (GNNs) in two aspects, (1) mitigating faulty negative contrastive instances via considering cheminformatics similarities between molecule pairs; (2) fragment-level contrasting between intra- and inter-molecule substructures decomposed from molecules. Experiments have shown that the proposed strategies significantly improve the performance of GNN models on various challenging molecular property predictions. In comparison to the previous CL framework, iMolCLR demonstrates an averaged 1.3% improvement of ROC-AUC on 7 classification benchmarks and an averaged 4.8% decrease of the error on 5 regression benchmarks. On most benchmarks, the generic GNN pre-trained by iMolCLR rivals or even surpasses supervised learning models with sophisticated architecture designs and engineered features. Further investigations demonstrate that representations learned through iMolCLR intrinsically embed scaffolds and functional groups that can reason molecule similarities. ","Improving Molecular Contrastive Learning via Faulty Negative Mitigation
  and Decomposed Fragment Contrast"
141,1495702821395652609,307526089,Guillaume Pietrzyk üá∫üá¶üá™üá∫,"[""Hi everyone, I have extremely happy today because the paper of the analysis I've been working on in the last years has been submitted to arXiv (<LINK>)! I think it's time to do a small thread to explain a bit this new charm result! <LINK>"", 'This paper is the subject of my PhD thesis and focuses on charm mixing! Flavour mixing is what allows a particle to oscillate to its antimatter counterpart. This is an amazing quantum phenomenon described by Pais and Gell-Mann in 1955! https://t.co/7R134B10e5', 'Back in the days, Pais and Gell-Mann described the behavior of Œ∏ and Œ∏bar, now called K0 and K0bar, and explained that the two particles should not be seen as independent entities but as a two-state system! They can therefore oscillate over time from one to another. https://t.co/rqjTMNfNgx', 'Just like kaons, charm mesons can oscillate, and they do it very slowly! Technically, this is because their mass eigenstates (blue) are different from their flavour eigenstates (red), allowing them to change from one flavour to another over time. https://t.co/JWudJVUnFw', 'The probability to go from a D0 to a D0bar is describred by this mathematical expression. Where the two important parameters are x and y, probing respectively the mass and width difference between the D1 and D2 states. See that the bigger x is, the fastest are oscillations! https://t.co/u2dJVPumYE', 'This expression is actually the same for the beauty and kaon sectors. This plot shows the oscillations of the different neutral charm meson systems. See how different the dynamics are! https://t.co/pJETvPJOFw', ""A the bottom right of the plot above, you can see the Bs system, where the oscillations are extremely fast (we're very good at measuring that at LHCb!), this is because x (or Œîms) is very large in the Bs system."", 'Whereas at the top right, you can see that the charm system shows very slow oscillations, since x is very small! The oscillation period is over one hundred times larger than the average lifetime of a D0 meson. Nice to see that LHCb studies two very different systems! https://t.co/BTdZd8YIRf', 'Very recently, LHCb measured x to be significantly different from zero through an amazing measurement studying D0-&gt;KsœÄœÄ decays!\nhttps://t.co/psPYJIuUkV', ""Now, let's switch to the measurement of the paper submitted today. It does not focus on x, but on y, probing the width difference (1/lifetime) between the D1 and D2 states. https://t.co/4Xkwvozro8"", ""At LHCb, we're very good at analysing some golden two-body D0 decay modes, the D0-&gt;KœÄ, D0-&gt;KK, and D0-&gt;œÄœÄ. We can produce and reconstruct tens of millions of these decay modes, and with very low background! https://t.co/ohWA8hISkn"", ""Because of mixing, the D0-&gt;KœÄ decay will have a slightly larger lifetime than D0-&gt;KK and D0-&gt;œÄœÄ. Studying this difference of lifetimes can tell us a lot about charm mixing (especially y), but I won't go too much into the mathematical details!"", 'The slight offset between the D0-&gt;KœÄ  and D0-&gt;f (f=KK,œÄœÄ) lifetimes is studied through the observable yCP-yCPKœÄ, which has kind of a complicated name (in charm we tend to have complicated names for lots of stuff...) This is what this analysis measures! https://t.co/Kv09G5lB9x', 'Long story short, if you measure precisely yCP-yCPKœÄ, you constrain heavily y, since yCP-yCPKœÄ and y are linear combinations of one another! So we can really improve the knowledge of charm physics with this measurement!', 'How precise would a new measurement of yCP-yCPKœÄyou be you might ask? Four times more precise than the current world average value! This is because we have a lot of data at LHCb.', 'However! This measurement is challenging! This is because you want to study the lifetime difference between decays with two different final states. Therefore, inside LHCb they interact differently with the trigger selection and the components of the detector.', 'This may be a bit technical, but what we do is to build what is called a decay-time observable, being the ratio of the decay time distributions of D0-&gt;KK over D0-&gt;ŒöœÄ. What you need to see is that by building this ratio you can measure yCP-yCPKœÄ with an exponential fit. https://t.co/EYq3fB1Igl', 'However, you can see in green what I talked to you about, being different efficiencies for the two different final states, which can destroy your measurement!', 'You need equal efficiencies for the two decays to be able to measurement yCP-yCPKœÄ, because you want them to cancel out in the ratio! https://t.co/y8xxDZADC4', 'This is clearly the biggest challenge, and what made me bang my head against the walls of @EPFL, and why many physicsts at LHCb thought that this measurement could not be done! https://t.co/i4fEouEWrl', 'What we came up with was a method to transform D0-&gt;KK decays so that they pretend to be a D0-&gt;KœÄ (this is called a kinematic matching). This allows to equalise these efficiencies and it seems to work very well! https://t.co/uDDwR24GwV', 'So long story short, we worked very hard to treat all the complicated experimental effects of this measurement! Lots of LHCb physicists helped us a lot to improve everything and we managed to measurement yCP-yCPKœÄ, using both KK and œÄœÄ final states! https://t.co/BGZuMQO4US', 'You can then combine the yCPKK-yCPKœÄ and yCPœÄœÄ-yCPKœÄ measurements to get what you want, yCP-yCPKœÄ.\nThis is the red data on this plot, where the inner plot is a zoom of the whole plot. You can see how much more precise it is than the previous measurements. https://t.co/1DhrUlYHYs', 'You can plug in this measurement in charm global fits (to get the improvement of the charm world). Here is the y versus x plot, with blue before and orange after the introduction of this new measurement! We improve y by more than a factor of two! üòç https://t.co/mz7uU4wxhC', 'I am extremely happy that this result came out! And I would like to thank the whole @LHCbExperiment  collaboration for its help. LHCb is an amazing experiment, everybody was super nice with me and it really helped me perform this work in the best environment possible!', ""In Run 3 (starting this year), we'll have even more data, allowing to improve further more the knowledge of the charm world! https://t.co/rN2KP9zeFZ"", 'Thanks a lot if you read this thread. It is quite funny to describe a thesis in tweets. https://t.co/QjI3YF66CR', '@DrNaikIsAwesome Thanks!!', 'I like this gif with all plots of the paper! https://t.co/T2nyjOEE3y']",https://arxiv.org/abs/2202.09106,"A measurement of the ratios of the effective decay widths of $D^0 \to \pi^-\pi^+$ and $D^0 \to K^-K^+$ decays over that of $D^0 \to K^-\pi^+$ decays is performed with the LHCb experiment using proton-proton collisions at a centre-of-mass energy of $13 \, \mathrm{TeV}$, corresponding to an integrated luminosity of $6 \, \mathrm{fb^{-1}}$. These observables give access to the charm mixing parameters $y_{CP}^{\pi\pi} - y_{CP}^{K\pi}$ and $y_{CP}^{KK} - y_{CP}^{K\pi}$, and are measured as $y_{CP}^{\pi\pi} - y_{CP}^{K\pi} = (6.57 \pm 0.53 \pm 0.16) \times 10^{-3}$, $y_{CP}^{KK} - y_{CP}^{K\pi} = (7.08 \pm 0.30 \pm 0.14) \times 10^{-3}$, where the first uncertainties are statistical and the second systematic. The combination of the two measurements is $y_{CP} - y_{CP}^{K\pi} = (6.96 \pm 0.26 \pm 0.13) \times 10^{-3}$, which is four times more precise than the previous world average. ","Measurement of the charm mixing parameter $y_{CP} - y_{CP}^{K\pi}$ using
  two-body $D^0$ meson decays"
142,1494362438434795521,874210693,Alex Amon,"['New paper out!üéâLed with the fab @NaomiR_astro  (fellow tiny-islanderüáπüáπüè¥\U000e0067\U000e0062\U000e0073\U000e0063\U000e0074\U000e007f - we‚Äôve come a long way from all of our first conference together - Crete 2016!). We have analysed the clustering and  lensing signals from BOSS with KiDS-1000, DES-Y3, and HSC-Y1: <LINK> <LINK>', 'This work was made possible by a wonderfully open and collaborative spirit between the lensing teams: KiDS, DES and HSC, as well as the patient mentoring of @AstroRoyalScot, @RisaWechsler and other wise folk! üôèüèΩ']",https://arxiv.org/abs/2202.07440,"We evaluate the consistency between lensing and clustering probes of large-scale structure based on measurements of projected galaxy clustering from BOSS combined with overlapping galaxy-galaxy lensing from three surveys: DES Y3, HSC Y1, and KiDS-1000. An intra-lensing-survey study finds good agreement between these lensing data. We model the observations using the Dark Emulator and fit the data at two fixed cosmologies: Planck, with $S_8=0.83$, and a Lensing cosmology with $S_8=0.76$. For a joint analysis limited to scales with $R>5.25h^{-1}$Mpc, we find that both cosmologies provide an acceptable fit to the data. Full utilisation of the small-scale clustering and lensing measurements is hindered by uncertainty in the impact of baryon feedback and assembly bias, which we account for with a reasoned theoretical error budget. We incorporate a systematic scaling parameter for each redshift bin, $A$, that decouples the lensing and clustering to capture any inconsistency. When a wide range of scales ($0.15<R<60h^{-1}$Mpc) are incorporated, we find different results for the consistency of clustering and lensing between the two cosmologies. Limiting the analysis to the bins for which the impact of the selection of the lens sample is expected to be minimal, for the low-$S_8$ Lensing cosmology, the measurements are consistent with $A$=1; $A=0.91\pm0.04$ using DES+KiDS and $A=0.97\pm0.06$ using HSC. For the Planck cosmology case, we find a discrepancy: $A=0.79\pm0.03$ using DES+KiDS and $A=0.84\pm0.05$ using HSC. We demonstrate that a kSZ-based estimate for baryonic effects alleviates some of the discrepancy in the Planck cosmology. This analysis demonstrates the statistical power of these small-scale measurements, but also indicates that caution is still warranted given current uncertainties in modelling baryonic effects, assembly bias, and selection effects in the foreground sample. ","Consistent lensing and clustering in a low-$S_8$ Universe with BOSS, DES
  Year 3, HSC Year 1 and KiDS-1000"
143,1494361834932953103,2701532126,Berivan Isik,"['Check out our new paper titled ‚ÄúLearning under Storage and Privacy Constraints‚Äù. We propose a novel data pre-processing framework, LCoN, which simultaneously boosts data efficiency, privacy, accuracy, and robustness. 1/4\n\n<LINK>\n\n#compression #privacy #learning <LINK>', 'Our framework comprises noise injection followed by lossy compression. The noise injection step prevents user information from being leaked during learning, while lossy compression reduces the cost of storing/transmitting the data. 2/4', 'We show that, when appropriately matching the lossy compression to the distribution of the added noise, the compressed examples converge, in distribution, to that of the noise-free training data. 3/4', 'With this, we guarantee that the utility of the data for learning is essentially maintained while reducing storage and privacy leakage by quantifiable amounts. The improved robustness against adversarial data is a welcome additional feature we observed empirically. 4/4']",https://arxiv.org/abs/2202.02892,"Storage-efficient privacy-guaranteed learning is crucial due to enormous amounts of sensitive user data required for increasingly many learning tasks. We propose a framework for reducing the storage cost while at the same time providing privacy guarantees, without essential loss in the utility of the data for learning. Our method comprises noise injection followed by lossy compression. We show that, when appropriately matching the lossy compression to the distribution of the added noise, the compressed examples converge, in distribution, to that of the noise-free training data. In this sense, the utility of the data for learning is essentially maintained, while reducing storage and privacy leakage by quantifiable amounts. We present experimental results on the CelebA dataset for gender classification and find that our suggested pipeline delivers in practice on the promise of the theory: the individuals in the images are unrecognizable (or less recognizable, depending on the noise level), overall storage of the data is substantially reduced, with no essential loss of the classification accuracy. As an added bonus, our experiments suggest that our method yields a substantial boost to robustness in the face of adversarial test data. ",Learning under Storage and Privacy Constraints
144,1494217753347215362,218250514,Heiko Hamann,['a new paper\n\nEfficient quantitative assessment of robot swarms: coverage and targeting #Levy strategies\n\nby Duncan et al.\n\nsome quite heavy theory for #foraging supported also by simulation results\n\n#swarm #robotics #modeling\n<LINK>'],https://arxiv.org/abs/2202.06931,"Biologically inspired strategies have long been adapted to swarm robotic systems, including biased random walks, reaction to chemotactic cues and long-range coordination. In this paper we apply analysis tools developed for modeling biological systems, such as continuum descriptions, to the efficient quantitative characterization of robot swarms. As an illustration, both Brownian and L\'{e}vy strategies with a characteristic long-range movement are discussed. As a result we obtain computationally fast methods for the optimization of robot movement laws to achieve a prescribed collective behavior. We show how to compute performance metrics like coverage and hitting times, and illustrate the accuracy and efficiency of our approach for area coverage and search problems. Comparisons between the continuum model and robotic simulations confirm the quantitative agreement and speed up of our approach. Results confirm and quantify the advantage of L\'{e}vy strategies over Brownian motion for search and area coverage problems in swarm robotics. ","Efficient quantitative assessment of robot swarms: coverage and
  targeting L\'{e}vy strategies"
145,1494147269796540418,1128095824473276418,Kai Shu,"['Identifying disinformation in a new domain is harder than an existing domain? Our recent accepted @TheWebConf paper on detecting cross-domain fake news with reinforcement learning.\nW/ Ahmadreza Mosallanezhad, @Mansoureh_K Michelle V. Mancenido, @liuhuan \n<LINK>']",https://arxiv.org/abs/2202.08159,"With social media being a major force in information consumption, accelerated propagation of fake news has presented new challenges for platforms to distinguish between legitimate and fake news. Effective fake news detection is a non-trivial task due to the diverse nature of news domains and expensive annotation costs. In this work, we address the limitations of existing automated fake news detection models by incorporating auxiliary information (e.g., user comments and user-news interactions) into a novel reinforcement learning-based model called \textbf{RE}inforced \textbf{A}daptive \textbf{L}earning \textbf{F}ake \textbf{N}ews \textbf{D}etection (REAL-FND). REAL-FND exploits cross-domain and within-domain knowledge that makes it robust in a target domain, despite being trained in a different source domain. Extensive experiments on real-world datasets illustrate the effectiveness of the proposed model, especially when limited labeled data is available in the target domain. ",Domain Adaptive Fake News Detection via Reinforcement Learning
146,1493632604981317634,57294647,Arindam Khan,"['In guillotine strip packing, a set of rectangles needs to be packed into a unit-width strip of minimum height s.t. each packed rectangle can be separated out using a sequence of guillotine (end-to-end) cuts. \n\nOur new paper on arxiv settles the problem!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2202.05989,"In the Strip Packing problem (SP), we are given a vertical half-strip $[0,W]\times[0,\infty)$ and a set of $n$ axis-aligned rectangles of width at most $W$. The goal is to find a non-overlapping packing of all rectangles into the strip such that the height of the packing is minimized. A well-studied and frequently used practical constraint is to allow only those packings that are guillotine separable, i.e., every rectangle in the packing can be obtained by recursively applying a sequence of edge-to-edge axis-parallel cuts (guillotine cuts) that do not intersect any item of the solution. In this paper, we study approximation algorithms for the Guillotine Strip Packing problem (GSP), i.e., the Strip Packing problem where we require additionally that the packing needs to be guillotine separable. This problem generalizes the classical Bin Packing problem and also makespan minimization on identical machines, and thus it is already strongly NP-hard. Moreover, due to a reduction from the Partition problem, it is NP-hard to obtain a polynomial-time $(3/2-\varepsilon)$-approximation algorithm for GSP for any $\varepsilon>0$ (exactly as Strip Packing). We provide a matching polynomial time $(3/2+\varepsilon)$-approximation algorithm for GSP. Furthermore, we present a pseudo-polynomial time $(1+\varepsilon)$-approximation algorithm for GSP. This is surprising as it is NP-hard to obtain a $(5/4-\varepsilon)$-approximation algorithm for (general) Strip Packing in pseudo-polynomial time. Thus, our results essentially settle the approximability of GSP for both the polynomial and the pseudo-polynomial settings. ","Tight Approximation Algorithms for Two Dimensional Guillotine Strip
  Packing"
147,1492926466040135682,57294647,Arindam Khan,['New paper on Arxiv: \nGives improved approximation (from log n to log log n) for ROUND-UFP (a generalization of bin packing) and shows the problem to be APX-hard. \n\n<LINK> <LINK>'],https://arxiv.org/abs/2202.03492,"We study ROUND-UFP and ROUND-SAP, two generalizations of the classical BIN PACKING problem that correspond to the unsplittable flow problem on a path (UFP) and the storage allocation problem (SAP), respectively. We are given a path with capacities on its edges and a set of tasks where for each task we are given a demand and a subpath. In ROUND-UFP, the goal is to find a packing of all tasks into a minimum number of copies (rounds) of the given path such that for each copy, the total demand of tasks on any edge does not exceed the capacity of the respective edge. In ROUND-SAP, the tasks are considered to be rectangles and the goal is to find a non-overlapping packing of these rectangles into a minimum number of rounds such that all rectangles lie completely below the capacity profile of the edges. We show that in contrast to BIN PACKING, both the problems do not admit an asymptotic polynomial-time approximation scheme (APTAS), even when all edge capacities are equal. However, for this setting, we obtain asymptotic $(2+\varepsilon)$-approximations for both problems. For the general case, we obtain an $O(\log\log n)$-approximation algorithm and an $O(\log\log\frac{1}{\delta})$-approximation under $(1+\delta)$-resource augmentation for both problems. For the intermediate setting of the no bottleneck assumption (i.e., the maximum task demand is at most the minimum edge capacity), we obtain absolute $12$- and asymptotic $(16+\varepsilon)$-approximation algorithms for ROUND-UFP and ROUND-SAP, respectively. ",Approximation Algorithms for ROUND-UFP and ROUND-SAP
148,1491479630326337536,1115847756,Daniel Tompkins,"['Happy to share our new paper accepted at #icassp2022! We achieve a new state-of-the-art score on the ESC-50 dataset (95.8%) using an Xception model with knowledge transfer, pretraining, and data augmentation. <LINK>']",https://arxiv.org/abs/2202.03514,"An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50 dataset for audio event detection through knowledge transfer from ImageNet weights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline. This paper presents an ablation study that analyzes which components contribute to the boost in performance and training time. A smaller Xception model is also presented which nears SOTA performance with almost a third of the parameters. ","Maximizing Audio Event Detection Model Performance on Small Datasets
  Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation
  Study"
149,1491441919565901826,1002014071,Frank Wilczek,"[""New paper <LINK> posted to arXiv on observable manifestations of mathematical singularities in spin liquid theory.  Highly technical, and in a way that's the point: reality cares! (If we're right ...) <LINK>""]",https://arxiv.org/abs/2202.03445,"We show that interactions in quantum spin liquids can result in non-Hermitian phenomenology that differs qualitatively from mean-field expectations. We demonstrate this in two prominent cases through the effects of phonons and disorder on a Kitaev honeycomb model. Using analytic and numerical calculations, we show the generic appearance of exceptional points and rings depending on the symmetry of the system. Their existence is reflected in dynamical observables including the dynamic structure function measured in neutron scattering. The results point to new phenomenological features in realizable spin liquids that must be incorporated into the analysis of experimental data. ",Exceptional dynamics of interacting spin liquids
150,1491368303939715075,2434570915,Alejandro Font√°n,"['our paper in #RAL2022, #ICRA2022 ! we model a new term based on Perspective Deformation to better estimate residual covariances and boost both direct and feature-based SLAM! Thanks LauraOliva, @jcivera, RudolphTriebel\npaper: <LINK>\nvideo: <LINK>']",https://arxiv.org/abs/2202.00765,"In this work, we derive a model for the covariance of the visual residuals in multi-view SfM, odometry and SLAM setups. The core of our approach is the formulation of the residual covariances as a combination of geometric and photometric noise sources. And our key novel contribution is the derivation of a term modelling how local 2D patches suffer from perspective deformation when imaging 3D surfaces around a point. Together, these add up to an efficient and general formulation which not only improves the accuracy of both feature-based and direct methods, but can also be used to estimate more accurate measures of the state entropy and hence better founded point visibility thresholds. We validate our model with synthetic and real data and integrate it into photometric and feature-based Bundle Adjustment, improving their accuracy with a negligible overhead. ","A Model for Multi-View Residual Covariances based on Perspective
  Deformation"
151,1490725739930562569,2180768821,Erik Hoel,"[""1/ How can we identify and agree upon cases of emergence in complex systems? A new paper by myself and @renzocom is out today, showing that across measures of causation there is widespread agreement over what's emergent and what's not. <LINK> <LINK>"", '2/ This is critically important because we need to move beyond metaphysical debates about emergence and instead develop a science of emergence analogous to that of complex systems science. https://t.co/MhUMTOCoah', '3/ Specifically, the theory of causal emergence states that macroscales can minimize noise in causal relations and increase their strength. It turns out that this effect of macroscale causation via error-correction is supported by every measure of causation we examined. https://t.co/MyxuQfdfEk', '4/ Note that the ubiquity of causal emergence holds true across background assumptions (like what intervention distributions you use) and individual instances of causal emergence vanish in *no* conditions in *any* measure.', '5/ Additionally, we show that measures of causation are *consilient* in that they rely on the same small number of fundamental terms: ""causal primitives."" Causal primitives themselves can be greater at the macroscale (green is greater at macroscale), ensuring the measures are https://t.co/H0HVszrMm2', '6/ This consilience is why people keep rediscovering measures of causation! We show that, for instance, Judea Pearl ended up rediscovering a measure first proposed by David Lewis. But it also gives us a family of measures that generally agree (so objectivity about causes).', ""7/ There's a lot of really cool extra stuff in here. It's worth checking out for the menagerie of measures of causation alone. I think it's the most comprehensive in the literature, demonstrating they are all related and behave quite similarly."", 'Some people (in no particular order) who might be interested @anilkseth @_fernando_rosas @PedroMediano @adambarrett81 @RCarhartHarris @DanielBor @Sara_Imari @GaneshNatesh @DrYohanJohn etc', 'Also @joe_dewhurst', ""@GKBesterfriend @renzocom I don't think Aristotle's concepts map cleanly onto modern concepts without some bending. Probably counterfactuals are closest to formal causes, but you could argue efficient/material causes too."", '@NiethammerLab ""Strong"" vs. ""weak"" emergence, for instance. Various notions of downward causation that don\'t work when you model them out. Etc.']",https://arxiv.org/abs/2202.01854,"Causal emergence is the theory that macroscales can reduce the noise in causal relationships, leading to stronger causes at the macroscale. First identified using the effective information and later the integrated information in model systems, causal emergence has been analyzed in real data across the sciences since. But is it simply a quirk of these original measures? To answer this question we examined over a dozen popular measures of causation, all independently developed and widely used, and spanning different fields from philosophy to statistics to psychology to genetics. All showed cases of causal emergence. This is because, we prove, measures of causation are based on a small set of related ""causal primitives."" This consilience of independently-developed measures of causation shows that macroscale causation is a general fact about causal relationships, is scientifically detectable, and is not a quirk of any particular measure of causation. This finding sets the science of emergence on firmer ground, opening the door for the detection of intrinsic scales of function in complex systems, as well as assisting with scientific modeling and experimental interventions. ",Causal emergence is widespread across measures of causation
152,1489346318480904197,907346058207936512,Andy Zeng,"['Exciting new ICRA paper led by @WiYoungsun!\n\nVIRDO uses neural fields to predict how an object will deform, given visual-tactile sensing w/ partial point clouds + forces &amp; contact \n<LINK> \n\nw/ @NimaFazeli7‚Äôs robotics lab at UMich, @peteflorence <LINK>']",https://arxiv.org/abs/2202.00868,"Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO:an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts.Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations,and iii) state-estimation with partial visio-tactile feedback ",VIRDO: Visio-tactile Implicit Representations of Deformable Objects
153,1489321348237119496,577537524,Pete Florence,"['New ü§ñ paper led by the awesome @WiYoungsun!  <LINK>\n\nThe paper is essentially ""using the Force* to deform neural fields"" (In this case, DeepSDF-style representations.)\n\nA cool thing here is that robots can have tactile (e.g., force-torque) sensing... <LINK>', 'So we can do perception üëÄ that fuses shape knowledge together with tactile sensing üëè, even for deformable objects.\n\nThis can help do things like predict full shape deformation state, with only partial information. https://t.co/VAk2mACO24', 'Note these are *multi-modal (multi-sensory)*--input neural fields, capable of fusing both:\n- visual data (in this case point cloud data), and\n- tactile data (in this case force data) https://t.co/oqD23UGWYG', ""If you've ever thought about how good humans are at fusing shape information and touch sensing, including of objects that are deforming...\n\nthis work takes some initial steps towards that direction. https://t.co/JRvIXSHZpV"", ""Just accepted to ICRA.\n\nSuper happy to see @WiYoungsun's hard work pay off on the project.\n\nLed out of @NimaFazeli7's lab at Michigan, where they know a lot about tactile stuff :) .  @andyzengtweets and I were lucky to be able to help out. https://t.co/7ITlV88Q5K"", ""Also @WiYoungsun just made her Twitter account like yesterday.... if you're looking for cool new researchers to follow, she is awesome."", '* yes not actually the Force from Star Wars.\n\n(Btw, is it just me or are somehow Episodes 5 and 6 of Boba Fett just way better than the previous episodes?)']",https://arxiv.org/abs/2202.00868,"Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO:an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts.Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations,and iii) state-estimation with partial visio-tactile feedback ",VIRDO: Visio-tactile Implicit Representations of Deformable Objects
154,1488850789372243968,206818334,Ivan Oseledets,['We present the new approximate backward functions  which reduce the memory for activations. Interesting math behind! \nPaper about activation functions:\n<LINK>\nAbout linear layers: \n<LINK>\n–°ode:\n<LINK>'],https://arxiv.org/abs/2202.00441,"Memory footprint is one of the main limiting factors for large neural network training. In backpropagation, one needs to store the input to each operation in the computational graph. Every modern neural network model has quite a few pointwise nonlinearities in its architecture, and such operation induces additional memory costs which -- as we show -- can be significantly reduced by quantization of the gradients. We propose a systematic approach to compute optimal quantization of the retained gradients of the pointwise nonlinear functions with only a few bits per each element. We show that such approximation can be achieved by computing optimal piecewise-constant approximation of the derivative of the activation function, which can be done by dynamic programming. The drop-in replacements are implemented for all popular nonlinearities and can be used in any existing pipeline. We confirm the memory reduction and the same convergence on several open benchmarks. ","Few-Bit Backward: Quantized Gradients of Activation Functions for Memory
  Footprint Reduction"
155,1501393377610407946,1055883888159932416,Jason Lee,"['What is the analog of ERM for offline RL? We propose primal dual regularized offline rl (PRO-RL), which has many of the properties that makes ERM so successful. <LINK>', '1) For general function classes on the value/density ratios, the algorithm allows for agnostic learning, 2) does not require completeness (closure under Bellman operator), and 3) can compete with the best covered policy. https://t.co/JaRkGjMfyD', 'With Wenhao Zhan (https://t.co/bi5IdnhFNb) , Baihe Huang, Audrey Huang, and @nanjiang_cs', ""@yuxiangw_cs So my viewpoint is more about 'what is the right loss' so that it has all the benefits erm in stat learning has (agnostic, any func class). If you do ope for all \\pi in the FA setting, almost certainly you will have an assumption that is for all \\pi Blah needs to hold."", '@yuxiangw_cs I am also abusing the term erm here. All the losses are empirical risks, but the question is more which risk to use']",https://arxiv.org/abs/2202.04634,"Sample-efficiency guarantees for offline reinforcement learning (RL) often rely on strong assumptions on both the function classes (e.g., Bellman-completeness) and the data coverage (e.g., all-policy concentrability). Despite the recent efforts on relaxing these assumptions, existing works are only able to relax one of the two factors, leaving the strong assumption on the other factor intact. As an important open problem, can we achieve sample-efficient offline RL with weak assumptions on both factors? In this paper we answer the question in the positive. We analyze a simple algorithm based on the primal-dual formulation of MDPs, where the dual variables (discounted occupancy) are modeled using a density-ratio function against offline data. With proper regularization, we show that the algorithm enjoys polynomial sample complexity, under only realizability and single-policy concentrability. We also provide alternative analyses based on different assumptions to shed light on the nature of primal-dual algorithms for offline RL. ","Offline Reinforcement Learning with Realizability and Single-policy
  Concentrability"
156,1500216350257598465,2892508670,Guillermo Lorenzo,"['New preprint out ! üìÑ \nSolving spatiotemporal PDE models of solid tumor growth can be a computationally expensive task. In this work, we propose to use Dynamic Mode Decomposition to address this long-standing challenge.\n<LINK>']",https://arxiv.org/abs/2202.13860,"The computer simulation of organ-scale biomechanistic models of cancer personalized via routinely collected clinical and imaging data enables to obtain patient-specific predictions of tumor growth and treatment response over the anatomy of the patient's affected organ. These patient-specific computational forecasts have been regarded as a promising approach to personalize the clinical management of cancer and derive optimal treatment plans for individual patients, which constitute timely and critical needs in clinical oncology. However, the computer simulation of the underlying spatiotemporal models can entail a prohibitive computational cost, which constitutes a barrier to the successful development of clinically-actionable computational technologies for personalized tumor forecasting. To address this issue, here we propose to utilize Dynamic-Mode Decomposition (DMD) to construct a low-dimensional representation of cancer models and accelerate their simulation. DMD is an unsupervised machine learning method based on the singular value decomposition that has proven useful in many applications as both a predictive and a diagnostic tool. We show that DMD may be applied to Fisher-Kolmogorov models, which constitute an established formulation to represent untreated solid tumor growth that can further accommodate other relevant cancer phenomena. Our results show that a DMD implementation of this model over a clinically-relevant parameter space can yield impressive predictions, with short to medium-term errors remaining under 1% and long-term errors remaining under 20%, despite very short training periods. We posit that this data-driven approach has the potential to greatly reduce the computational overhead of personalized simulations of cancer models, thereby facilitating tumor forecasting, parameter identification, uncertainty quantification, and treatment optimization. ","Data-driven simulation of Fisher-Kolmogorov tumor growth models using
  Dynamic Mode Decomposition"
157,1499290288115826690,948995274961309697,Andrea Botteon,"['‚ÄºÔ∏èThe @Planck clusters in the @LOFAR sky‚ÄºÔ∏è\nThis is a huge project where we used 309 (!!) PSZ2 clusters to study diffuse radio sources in the ICM (halos and relics).\n\nPaper I (accepted in @AandA_journal) -&gt; <LINK>\nProject website -&gt; <LINK>\n\n1/13 <LINK>', 'We investigated all the Planck PSZ2 clusters belonging to the 5634 deg^2 area covered by the new LoTSS-DR2 (https://t.co/5iYnKQtdW2). With 309 clusters, this is the largest statistical cluster sample used to date to search for and study diffuse synchrotron sources in the ICM 2/13 https://t.co/5eaVT16F36', 'These 309 clusters span a wide range of of redshifts and masses, allowing us to study cluster diffuse sources in the poorly explored regimes of high-z and low-mass systems 3/13 https://t.co/gKCJDDWYro', 'All clusters were further processed to get the best image quality as possible. For each target, we produced a set of images at different resolution, with/without discrete radio sources removed, and overlays with @PanSTARRS1 + @chandraxray and @ESA_XMM (when available) 4/13 https://t.co/jRi61Pb1vE', 'The *thousands* of images produced were visually inspected to classify the diffuse emission (if present). In this long process, a decision tree was created to guide the classification and to keep the classification reproducible and as consistent as possible 5/13 https://t.co/9FgPY8galb', 'Classification is not always easy as our clusters show a large variety of radio structures. If you want to have a quick look at the full sample at once, check this high-resolution mosaic: https://t.co/9vUlHhs1V7 6/13 https://t.co/fncZuiTvIr', 'Overall, we found 83 clusters hosting a halo and 26 clusters hosting one or more radio relics (including candidates). 47 clusters show diffuse sources with uncertain emission. Half of the radio halos and relics reported are new discoveries! 7/13 https://t.co/8LqG7AkcPY', 'For halos and relics, we measured important quantities of the emission that are used in the subsequent statistical work. For example, all halos were fitted with an exponential model (https://t.co/tqJEcGb4e0) to obtain the radio power, central brightness and e-folding radius 8/13 https://t.co/rvYKzDunGt', 'These are the mass and redshift distributions of the diffuse sources found in our sample. In particular note that we have a decent number of halos with masses &lt;5e14 Msun and at redshifts &gt;0.6. Until recent, only a handful of halos were known in these regimes 9/13 https://t.co/rCPag33fqC', 'Extrapolating the number of detections in our sample, we anticipate that LoTSS at its completion will detect 251\\pm92 clusters with a halo and 83\\pm50 clusters with one or more relics from PSZ2 clusters alone 10/13', 'In the project website (https://t.co/SzENoWOHNq) we made available all the products of our work, that are: tables with classifications, radio halos and relics powers and other properties, and X-ray morphological parameters of the ICM + images (PNG and FITS) for each cluster 11/13 https://t.co/bkRt2on0Ve', 'Note that we have a number of papers underway that will focus on the statistical analysis of halos and relics, methods to derive upper limits to the presence of diffuse radio emission, and study of X-ray properties of the clusters. So stay tuned for more! 12/13', 'This large project is the result of years of collaborative work with many persons. So..thanks and kudos to all of them! @roxycas @fabiogasta @mcrossetti_twit @HirokiAkamatsu @Duy__Hoang @a_ignesti @AnnalisBonafede @UniLeiden @ASTRON_NL @IRA_INAF @HambObs @SRON_Space @Unibo+ 13/13']",https://arxiv.org/abs/2202.11720,"Relativistic electrons and magnetic fields permeate the intra-cluster medium (ICM) and manifest themselves as diffuse sources of synchrotron emission observable at radio wavelengths, namely radio halos and radio relics. Although there is broad consensus that the formation of these sources is connected to turbulence and shocks in the ICM, the details of the required particle acceleration, the strength and morphology of the magnetic field in the cluster volume, and the influence of other sources of high-energy particles are poorly known. Sufficiently large samples of radio halos and relics, which would allow us to examine the variation among the source population and pinpoint their commonalities and differences, are still missing. At present, large numbers of these sources are easiest to detect at low radio frequencies, where they shine brightly. We examined the low-frequency radio emission from all 309 clusters in the second catalog of Planck Sunyaev Zel'dovich detected sources that lie within the 5634 deg$^2$ covered by the Second Data Release of the LOFAR Two-meter Sky Survey (LoTSS-DR2). We produced LOFAR images at different resolutions, with and without discrete sources subtracted, and created overlays with optical and X-ray images before classifying the diffuse sources in the ICM, guided by a decision tree. Overall, we found 83 clusters that host a radio halo and 26 that host one or more radio relics (including candidates). About half of them are new discoveries. The detection rate of clusters hosting a radio halo and one or more relics in our sample is $30\pm11$% and $10\pm6$%, respectively. Extrapolating these numbers, we anticipate that once LoTSS covers the entire northern sky it will provide the detection of $251\pm92$ clusters hosting a halo and $83\pm50$ clusters hosting at least one relic from Planck clusters alone. ","The Planck clusters in the LOFAR sky. I. LoTSS-DR2: new detections and
  sample overview"
158,1499097939435667457,1010536067886387200,Ananya Kumar,"['How should you fine-tune a large pretrained model (CLIP, SimCLR) robustly? We find that standard fine-tuning can do poorly out-of-distribution (test data ‚â† fine-tuning data). Our analysis leads to a simple fix, higher accuracy on 10 datasets. <LINK> (ICLR Oral) <LINK>', '(2/n) Joint work with Aditi Raghunathan, @rmjones96, and my advisors @tengyuma and @percyliang', '(3/n) We find that full fine-tuning (updating all model parameters) can be worse than linear probing (updating only the last layer) on out-of-distribution test examples, when the distribution shift is large and the pretrained features are good', '(4/n) We prove theoretically that this phenomenon arises even in simple and natural settings. One line explanation: while full fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features', ""(5/n) This suggests the easy two-step strategy of linear probing then full fine-tuning (LP-FT). Intuition: head doesn't change as much, so features get distorted less"", '(6/n) LP-FT gives large gains OOD: 10% better OOD, 1% better ID than full fine-tuning. Also outperforms linear probing both ID and OOD', '(7/n) Caption for Figure in Tweet 1/n: (a) full fine-tuning does better in-distribution (ID), (b) linear probing can do better out-of-distribution (OOD), (c) LP-FT does better on both, especially OOD', '(8/n) This work is part of a broader trend (e.g., prompt tuning, composed fine-tuning, prefix tuning), where tuning a small part of a pretrained model can be better than full fine-tuning, especially for robustness', '@CyrusMaher Yup! And to clarify we cited this and other papers, and mention in our abstract + intro that LP-FT is sometimes used as a fine-tuning heuristic (though not for robustness). Hopefully our analysis popularizes it, and explains when it can be particularly useful (OOD)']",http://arxiv.org/abs/2202.10054,"When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the ""head""). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\to$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning). ","Fine-Tuning can Distort Pretrained Features and Underperform
  Out-of-Distribution"
159,1499057946847809536,1176009385790844929,Chen Yanover,"['Can we estimate the performance of predictive models on external, inaccessible data using only limited statistical characteristics?\n\nA new study (w/ @TalElHay, @ResearchKi), accepted to #CHIL2022 (preprint: <LINK>) &amp; featured today on #OHDSISocialShowcase\n\nüßµ\n1/8 <LINK>', 'In the #healthcare domain, we typically train #predictive models (or risk scores) on a few internal, patient-level data sources, then expect them to work well in external environments, which may vary in their population characteristics, clinical settings, and policies. üëé\n\n2/8', 'Some studies directly validate model performance on external environments, but this requires access to additional sources and is often lengthy and costly. Can we, instead, estimate the external performance using limited external statistics (+ patient-level internal data)? \n\n3/8', 'Briefly, our algorithm reweights individuals in an internal sample to match external statistics (e.g., reported in preceding publications); then estimates model performance on the external sample using the reweighted internal one (for technical details, see the preprint)\n\n4/8', 'We tested the proposed algorithm on: \n1‚É£simulated data (üëá graphical representation of the data-generating model)\n\n2‚É£predictive models of complications for #ulcerative #colitis patients in internal-external synthetic splits of primary care EMR data\n\n5/8 https://t.co/ANhLk6sH7e', '3‚É£3 stroke risk scores in 7 external resources, using the entire EMR data, and compared to actual performance, as reported in https://t.co/6KgxGwzjQ8 @OHDSI \n\n6/8', 'For almost all cases, the estimated external performance is closer to the actual one than the internal (‚Äúbaseline‚Äù) performance\n\nCompare, for example, the actual performance (üîµ) to the external estimated (‚ô¶Ô∏è) and internal (solid line) ones for the ATRIA #stroke risk score\n\n7/8 https://t.co/XH4vmQIF91', 'Our proposed method may be an important building block in training robust models and detecting potential model failures in external environments. \n\n8/8']",https://arxiv.org/abs/2202.13683,"Methods that address data shifts usually assume full access to multiple datasets. In the healthcare domain, however, privacy-preserving regulations as well as commercial interests limit data availability and, as a result, researchers can typically study only a small number of datasets. In contrast, limited statistical characteristics of specific patient samples are much easier to share and may be available from previously published literature or focused collaborative efforts. Here, we propose a method that estimates model performance in external samples from their limited statistical characteristics. We search for weights that induce internal statistics that are similar to the external ones; and that are closest to uniform. We then use model performance on the weighted internal sample as an estimation for the external counterpart. We evaluate the proposed algorithm on simulated data as well as electronic medical record data for two risk models, predicting complications in ulcerative colitis patients and stroke in women diagnosed with atrial fibrillation. In the vast majority of cases, the estimated external performance is much closer to the actual one than the internal performance. Our proposed method may be an important building block in training robust models and detecting potential model failures in external environments. ","Estimating Model Performance on External Samples from Their Limited
  Statistical Characteristics"
160,1498722631226470408,1284197401,John ZuHone,['Very happy to be on this great work by @UChadayammuri doing a detailed study of the merging galaxy cluster A2146 using MHD simulations. Shows the challenges and the promise of using observations to probe ICM plasma physics and how we can do better. <LINK>'],https://arxiv.org/abs/2202.13430,"Kelvin-Helmholtz Instabilities (KHI) along contact discontinuities in galaxy clusters have been used to constrain the strength of magnetic fields in galaxy clusters, following the assumption that, as magnetic field lines drape around the interface between the cold and hot phases, their magnetic tension resists the growth of perturbations. This has been observed in simulations of rigid objects moving through magnetised media and sloshing galaxy clusters, and then applied in interpreting observations of merger cold fronts. Using a suite of MHD simulations of binary cluster mergers, we show that even magnetic field strengths stronger than yet observed ($\beta = P_{\rm th}/P_B = 50$) show visible KHI features. This is because our initial magnetic field is tangled, producing Alfven waves and associated velocity fluctuations in the ICM; stronger initial fields therefore seed larger fluctuations, so that even a reduced growth rate due to magnetic tension produces a significant KHI. The net result is that a stronger initial magnetic field produces more dramatic fluctuations in surface brightness and temperature, not the other way around. We show that this is hard to distinguish from the evolution of turbulent perturbations of the same initial magnitude. Therefore, in order to use observations of KHI in the ICM to infer magnetic field strengths by comparing to idealized simulations, the perturbations which seed the KHI must be well-understood and (if possible) carefully controlled. ","Turbulent magnetic fields in merging clusters: A case study of Abell
  2146"
161,1498711423714832390,890379031,Carlos Riquelme,"['Vision Transformers can drop many tokens (say, background, redundant, noise, etc) &amp; still succeed at image classification. We propose an extremely simple way to merge tokens and save up to 40-50% training cost in huge models while keeping their performance <LINK> <LINK>', 'if you are training VIT models, consider using this simple trick to greatly speed up all your experiments!\njoint work with @rengglic @ASusanoPinto @neilhoulsby @_basilM @joapuipe', 'Next step: can we do this *adaptively*? i.e., different amounts of compression per input depending on its difficulty or intrinsic properties? moreover, can we implement it in a hardware-friendly and practical way?', 'Our work is really close in spirit to the TokenLearner https://t.co/d3UKOhWKm7 by @ryoo_michael AJ Piergiovanni @anuragarnab @m__dehghani Anelia Angelova. You can read more in its @GoogleAI blogpost! https://t.co/QLKsfQwbg5']",https://arxiv.org/abs/2202.12015,"Transformers are widely applied to solve natural language understanding and computer vision tasks. While scaling up these architectures leads to improved performance, it often comes at the expense of much higher computational costs. In order for large-scale models to remain practical in real-world systems, there is a need for reducing their computational overhead. In this work, we present the PatchMerger, a simple module that reduces the number of patches or tokens the network has to process by merging them between two consecutive intermediate layers. We show that the PatchMerger achieves a significant speedup across various model sizes while matching the original performance both upstream and downstream after fine-tuning. ",Learning to Merge Tokens in Vision Transformers
162,1498367180370587651,577235600,Matt Groh üêô,"['üö® New arXiv pre-print on detecting multimedia misinformation <LINK> üö®\n\nAre deepfake videos of political speeches more believable than transcripts of the same speeches?\n\nWe (@menaminiki, @RosalindPicard and I) conducted an experiment to find out. Thread üëá', ""If we want to understand why people fall for misinfo, it's helpful to understand how people discern whether what they read, hear, and see is likely to be real or fake and how people weigh what is said (content) vs. how it is said (audiovisual) when assessing credibility."", ""When it comes to real events, people are more likely to believe a real event occurred after watching a video than reading a description of the event (see https://t.co/beFalVYXEY)\n\nBut when it comes to fake events, there's little empirical work."", 'When it comes to communication theory, the realism heuristic predicts ‚Äúpeople are more likely to trust audiovisual modality because its content has a higher resemblance to the real world"" https://t.co/V8onQWWgiK. But does this heuristic apply when people know things may be faked?', '@menamika created 16 deepfakes and curated another 16 real ones of political speeches (see https://t.co/kinxn3rXDu) and we hosted these on https://t.co/5t9CaGBI7F and ran an experiment to figure out how audiovisual info influences the credibility of real and fabricated speeches.', 'We showed 5k participants 32 different speeches by US presidents. Half were deepfakes, half real.\n\nWe randomly assigned speeches to appear as permutations of text, audio, and video e.g., text only, video + text, ...\n\nWe asked, ""Did he say that?"" https://t.co/hxhuSngYuH', 'See how you would do by playing this video. And check out how people who only saw a subset of the communication modalities perform on this video.\n\nVideo + audio: 84% accuracy\nVideo + subtitles: 72% \nVideo only: 53% \nText only: 46%\nAudio only: 81%\n\nhttps://t.co/Q65tkniTUo', ""That video is a deepfake. It's a lip sync manipulation based on wav2lip combined with a voice actors impression of Trump. \n\nWe examine how 5k participants perform across 32 speeches. Here are the results across the modality conditions. https://t.co/tz8K30fy2L"", 'We find that more information via additional communication modalities enables people to make more accurate judgments about which speeches are real and which ones are fabricated.\n\nThere\'s more nuances to visual misinformation than the ""seeing is believing"" adage would suggest.', 'Our preprint has more on this analysis, further examination of heterogeneous effects of con/discordant content, and a discussion of the limitations of this experiment.\n\nThanks for reading this thread! If you read https://t.co/LJkFzBHlf5, comments/critiques are much appreciated!']",https://arxiv.org/abs/2202.12883,"Recent advances in technology for hyper-realistic visual effects provoke the concern that deepfake videos of political speeches will soon be visually indistinguishable from authentic video recordings. Yet there exists little empirical research on how audio-visual information influences people's susceptibility to fall for political misinformation. The conventional wisdom in the field of communication research predicts that people will fall for fake news more often when the same version of a story is presented as a video as opposed to text. However, audio-visual manipulations often leave distortions that some but not all people may pick up on. Here, we evaluate how communication modalities influence people's ability to discern real political speeches from fabrications based on a randomized experiment with 5,727 participants who provide 61,792 truth discernment judgments. We show participants soundbites from political speeches that are randomly assigned to appear using permutations of text, audio, and video modalities. We find that communication modalities mediate discernment accuracy: participants are more accurate on video with audio than silent video, and more accurate on silent video than text transcripts. Likewise, we find participants rely more on how something is said (the audio-visual cues) rather than what is said (the speech content itself). However, political speeches that do not match public perceptions of politicians' beliefs reduce participants' reliance on visual cues. In particular, we find that reflective reasoning moderates the degree to which participants consider visual information: low performance on the Cognitive Reflection Test is associated with an underreliance on visual cues and an overreliance on what is said. ","Human Detection of Political Deepfakes across Transcripts, Audio, and
  Video"
163,1498334321869934595,928289778960707584,Sewon Min,"['LMs can learn via inference alone through demonstrations -- but how does it work?\n\nWe find that LMs do not really need correct input-output pairs. Randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 models.\n\n<LINK> <LINK>', 'Then, how do demonstrations lead to performance gains?\n\nWe find that (1) gains mainly come from (independent) specification of the input space and the label space, and (2) models still sometimes retain ~95% of gains with either inputs only or labels only *given the right format.*', 'Further analysis provides a new way of understanding the role of the demonstrations and what we can say about the model ""learning at test time"". More discussion in the paper!\n\nWork with Xinxi Lyu, @universeinanegg, @artetxem, @ml_perception, @HannaHajishirzi, @LukeZettlemoyer']",http://arxiv.org/abs/2202.12837,"Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone. ","Rethinking the Role of Demonstrations: What Makes In-Context Learning
  Work?"
164,1497149816933027847,1024520749258817537,Jan A. Krzywda,"['23.2.22 #arXivaria \n<LINK>\nTurning stationary qubit into flying one will allow for coherent #quantum link between arrays of Si spin qubits. Here we propose scalable method of coherent Spin Qubit Shuttle (SQuS) in realistic Si/SiGe device (length~10mum, v~10m/s) <LINK>']",http://arxiv.org/abs/2202.11793,"Silicon spin qubits stand out due to their very long coherence times, compatibility with industrial fabrication, and prospect to integrate classical control electronics. To achieve a truly scalable architecture, a coherent mid-range link between qubit registers has been suggested to solve the signal fan-out problem. Here we present a blueprint of such a $\sim 10\,\mu$m long link, called a spin qubit shuttle, which is based on connecting an array of gates into a small number of sets. The number of these gate sets and thus the number of required control signal is independent of the link distance to coherently shuttle the electron qubit. We discuss two different operation modes for the spin qubit shuttle: A qubit conveyor, i.e. a potential minimum that smoothly moves laterally, and a bucket brigade, in which the electron is transported through a series of tunnel-coupled quantum dots by adiabatic passage. We find the former approach more promising considering a realistic Si/SiGe device including potential disorder from the charged defects at the Si/SiO$_2$ layer, as well as typical charge noise. Focusing on the qubit transfer fidelity of the conveyor shuttling mode, motional narrowing, the interplay between orbital and valley excitation and relaxation in presence of $g$-factors that depend on orbital and valley state of the electron, and effects from spin-hotspots are discussed in detail. We find that a transfer fidelity of 99.9 \% is feasible in Si/SiGe at a speed of $\sim$10 m/s, if the average valley splitting and its inhomogeneity stay within realistic bounds. Operation at low global magnetic field $\approx 20$ mT and material engineering towards high valley splitting is favourable to reach high transfer fidelities. ","Blueprint of a scalable spin qubit shuttle device for coherent mid-range
  qubit transfer in disordered Si/SiGe/SiO$_2$"
165,1496777934299926532,1257389298145529856,Daniel Franco-Barranco,"['Check out our last paper in domain adaptation for EM volumes: ""Deep learning based domain adaptation for mitochondria segmentation on EM volumes"". We study three unsupervised strategies: 1) style transfer, 2) SSL and 3) multitask architectures.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2202.10773,"Accurate segmentation of electron microscopy (EM) volumes of the brain is essential to characterize neuronal structures at a cell or organelle level. While supervised deep learning methods have led to major breakthroughs in that direction during the past years, they usually require large amounts of annotated data to be trained, and perform poorly on other data acquired under similar experimental and imaging conditions. This is a problem known as domain adaptation, since models that learned from a sample distribution (or source domain) struggle to maintain their performance on samples extracted from a different distribution or target domain. In this work, we address the complex case of deep learning based domain adaptation for mitochondria segmentation across EM datasets from different tissues and species. We present three unsupervised domain adaptation strategies to improve mitochondria segmentation in the target domain based on (1) state-of-the-art style transfer between images of both domains; (2) self-supervised learning to pre-train a model using unlabeled source and target images, and then fine-tune it only with the source labels; and (3) multi-task neural network architectures trained end-to-end with both labeled and unlabeled images. Additionally, we propose a new training stopping criterion based on morphological priors obtained exclusively in the source domain. We carried out all possible cross-dataset experiments using three publicly available EM datasets. We evaluated our proposed strategies on the mitochondria semantic labels predicted on the target datasets. The methods introduced here outperform the baseline methods and compare favorably to the state of the art. In the absence of validation labels, monitoring our proposed morphology-based metric is an intuitive and effective way to stop the training process and select in average optimal models. ","Deep learning based domain adaptation for mitochondria segmentation on
  EM volumes"
166,1496646110919553025,1069244826,Preetum Nakkiran,"['One lesson for theory: instead of trying to ""understand generalization"", we should be trying to understand *pointwise* learning. The object we propose studying is the ""learning profile"" (basically Figure 1 in the paper; also done formally in Def. 2.1: <LINK>) <LINK>', '@de_JQK Our claim is that we should use pointwise perf as an analytic tool: even if we only have one model (that we use for all inputs), we should consider its behavior separately on each input.']",https://arxiv.org/abs/2202.09931,"In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ""compatible"" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts ""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021) ",Deconstructing Distributions: A Pointwise Framework of Learning
167,1496492656398118919,151193108,Mert R. Sabuncu,['Hot off the press. We propose a method to Compute Multiple Image Reconstructions with a Single Hypernetwork. \u2066Work with \u2066@AdrianDalca\u2069 and Alan Wang <LINK>'],https://arxiv.org/abs/2202.11009,"Deep learning based techniques achieve state-of-the-art results in a wide range of image reconstruction tasks like compressed sensing. These methods almost always have hyperparameters, such as the weight coefficients that balance the different terms in the optimized loss function. The typical approach is to train the model for a hyperparameter setting determined with some empirical or theoretical justification. Thus, at inference time, the model can only compute reconstructions corresponding to the pre-determined hyperparameter values. In this work, we present a hypernetwork-based approach, called HyperRecon, to train reconstruction models that are agnostic to hyperparameter settings. At inference time, HyperRecon can efficiently produce diverse reconstructions, which would each correspond to different hyperparameter values. In this framework, the user is empowered to select the most useful output(s) based on their own judgement. We demonstrate our method in compressed sensing, super-resolution and denoising tasks, using two large-scale and publicly-available MRI datasets. Our code is available at this https URL ",Computing Multiple Image Reconstructions with a Single Hypernetwork
168,1496439472468209665,572479189,Manlio De Domenico,"['In our latest work w/ @ArshamGhavasieh &amp; @GiuliaTtt we study how structural robustness cannot be used as the only indicator of network health/function. \n\nWe inspect how information flow is globally affected by localized perturbations. A thread 1/\n\nPaper üëâ<LINK> <LINK>', 'Usually, network damage is modeled by progressive removal of components or connections. \n\nConsequently, robustness is assessed in terms of how fast their structure fragments into disconnected sub-systems.\n\nBut does this capture pathological yet structurally integrated systems?\n2/', 'We attack the problem by using our statistical field theory of information dynamics. For background information check the updated thread below and this review paper (https://t.co/0EtLPsf7LI)\n\n3/\n\nhttps://t.co/qt6jEkPcbq https://t.co/T4FD65fMpV', 'We have also analyzed the role of the latent diffusion geometry and how it can help to gain insights about the propagation of signals from out-of-equilibrium to equilibrium.\n\nSome new indicators are introduced for this purpose\n\n4/ https://t.co/gGyzd0NT2c', ""1. The average received flow per node (ARI)\n2. The average squared diffusion distance (ASDD)\n\nThey capture distinct functional feature of a network: informational/entropic and geometric (yes, that's very cool)\n\n5/ https://t.co/wCBCUb92YP"", 'As usual, we have analyzed the response to (structural &amp; functional) attacks as measured by these new measures, for a variety of synthetic networks with distinct topological features, such as connectivity heterogeneity, mesoscale organization, small-worldness.\n\n6/ https://t.co/jRg3K78M58', 'In a nutshell, information flow might not be really affected when attacks rely on some structural descriptors, but it can be really impaired by functional ones, such as entanglement centrality.\n\nWhat is that? Check its thread below\n\n7/\n\nhttps://t.co/XOjPh7bl9Q', 'Does it work in practice for real networks? Yes.\n\nIn the human brain, not only the network topology shapes flows in the brain, but also different communications dynamics do, since the creation of each synaptic connection involves a trade-off between costs &amp; benefits...\n\n8/ https://t.co/Lp4KBOHnQa', '... and removing highly entangled nodes, may not disrupt the network topologically but may impact brain functions.\n\nWe confirmed analytically and numerically that only at low time scales functional analysis (dominated by short-ranges) coincides w/ structural one\n\n9/ https://t.co/POcM4GnqxI', 'Analyses of the Chilean power grid, the EU air traffic network and other systems confirm our theoretical expectations and the goodness of ARI and ASDD in capturing informational/entropic &amp; geometric aspects of functional robustness.\n\nPaper üëâhttps://t.co/iUSqs914z5\n\n/end https://t.co/BISjcFFEXt']",https://arxiv.org/abs/2202.09692,"Microscopic structural damage, such as lesions in neural systems or disruptions in urban transportation networks, can impair the dynamics crucial for systems' functionality, such as electrochemical signals or human flows, or any other type of information exchange, respectively, at larger topological scales. Damage is usually modeled by progressive removal of components or connections and, consequently, systems' robustness is assessed in terms of how fast their structure fragments into disconnected sub-systems. Yet, this approach fails to capture how damage hinders the propagation of information across scales, since system function can be degraded even in absence of fragmentation -- e.g., pathological yet structurally integrated human brain. Here, we probe the response to damage of dynamical processes on the top of complex networks, to study how such an information flow is affected. We find that removal of nodes central for network connectivity might have insignificant effects, challenging the traditional assumption that structural metrics alone are sufficient to gain insights about how complex systems operate. Using a damaging protocol explicitly accounting for flow dynamics, we analyze synthetic and empirical systems, from biological to infrastructural ones, and show that it is possible to drive the system towards functional fragmentation before full structural disintegration. ",Dismantling the information flow in complex interconnected systems
169,1496352164054454274,71214185,Thomas Busch,['We put together a state-of-the-area review and guide to the future of small and strongly correlated quantum gases. Learn about a beautiful system to study few-body quantum dynamics: <LINK>'],https://arxiv.org/abs/2202.11071,"Cold atomic gases have become a paradigmatic system for exploring fundamental physics, while at the same time allowing to explore applications in quantum technologies. The accelerating developments in the field have led to a highly advanced set of engineering techniques that, among others, can tune interactions, shape the external geometry, select among a large set of atomic species with different properties, as well as control the number of atoms. In particular, it is possible to operate in lower dimensions and drive atomic systems in the strongly correlated regime. In this review, we discuss recent advances in few-body cold atom systems confined in low dimensions from a theoretical viewpoint. We mainly focus on bosonic systems in one dimension and provide an introduction to the well-known static properties before we discuss the state-of-the-art quantum dynamical processes stimulated by the presence of correlations. Besides describing the fundamental physical phenomena arising in these systems, we also provide an overview of the tools and methods that are commonly used, thus delivering a balanced and comprehensive overview of the field. We conclude by giving an outlook on possible future directions that are interesting to explore in these correlated systems. ",Cold atoms in low dimensions -- a laboratory for quantum dynamics
170,1496140531004583943,1149494387182907392,Stephanie Milani,"['Check out our survey on explainable reinforcement learning! We propose a novel taxonomy for organizing the existing XRL literature, summarize current techniques, and discuss some interesting directions for future work: <LINK> <LINK>', 'w/ Nicholay Topin (equal contribution), Manuela Veloso, and Fei Fang']",https://arxiv.org/abs/2202.08434,"Explainable reinforcement learning (XRL) is an emerging subfield of explainable machine learning that has attracted considerable attention in recent years. The goal of XRL is to elucidate the decision-making process of learning agents in sequential decision-making settings. In this survey, we propose a novel taxonomy for organizing the XRL literature that prioritizes the RL setting. We overview techniques according to this taxonomy. We point out gaps in the literature, which we use to motivate and outline a roadmap for future work. ",A Survey of Explainable Reinforcement Learning
171,1496130189113315338,1202375676004667392,Tianyu Pang,"['Are robustness and accuracy inherently at odds? The answer may be NOT --- We propose Self-COnsistent Robust Error (SCORE), bringing us new insights on overfitting and semantic gradients phenomena, with top-rank performance on RobustBench (no extra data). <LINK> <LINK>', ""Taking a simple step beyond Madry's definition of robust error makes robustness and accuracy reconcilable. @trustworthy_ml""]",https://arxiv.org/abs/2202.10103,"The trade-off between robustness and accuracy has been widely studied in the adversarial literature. Although still controversial, the prevailing view is that this trade-off is inherent, either empirically or theoretically. Thus, we dig for the origin of this trade-off in adversarial training and find that it may stem from the improperly defined robust error, which imposes an inductive bias of local invariance -- an overcorrection towards smoothness. Given this, we advocate employing local equivariance to describe the ideal behavior of a robust model, leading to a self-consistent robust error named SCORE. By definition, SCORE facilitates the reconciliation between robustness and accuracy, while still handling the worst-case uncertainty via robust optimization. By simply substituting KL divergence with variants of distance metrics, SCORE can be efficiently minimized. Empirically, our models achieve top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides instructive insights for explaining the overfitting phenomenon and semantic input gradients observed on robust models. ",Robustness and Accuracy Could Be Reconcilable by (Proper) Definition
172,1496030888207003648,1430890835902562313,Silvia Galli,"['We find hints of Early Dark Energy in @Planck (excising small scale temperature), @ACT_Pol and @SPTelescope <LINK>! From these CMB data, 3.3sigma hint for EDE and a Hubble constant of 74.2+-2, in agreement with supernovae SH0ES value!Systematics?#neucosmos 1/3', 'Caveats to our results:\n1) Adding the small scale temperature Planck data, hints drastically reduce.\n2) The slight preference of Planck polarization for EDE depends on the choice of polarization efficiencies, a known possible systematic effect in Planck polarization data. 2/3', 'Disantangling between new physics, systematics or statistical fluctuations is  tricky! Eagerly waiting for new data to shed light on this problem! Huge thanks to first author Tristan Smith @160GHz and all other co-authors including @VivPoulin. @ERC_Research @astroIAP  3/3']",https://arxiv.org/abs/2202.09379,"We investigate constraints on early dark energy (EDE) using ACT DR4, SPT-3G 2018, Planck polarization, and restricted Planck temperature data (at $\ell<650$), finding a $3.3\sigma$ preference ($\Delta\chi^2=-16.2$ for 3 additional degrees of freedom) for EDE over $\Lambda$CDM. The EDE contributes a maximum fractional energy density of $f_{\rm EDE}(z_c)=0.163^{+0.047}_{-0.04}$ at a redshift $z_c=3357\pm200$ and leads to a CMB inferred value of the Hubble constant $H_0=74.2^{+1.9}_{-2.1}$ km/s/Mpc. We find that Planck and ACT DR4 data provide the majority of the improvement in $\chi^2$, and that the inclusion of SPT-3G pulls the posterior of $f_{\rm EDE}(z_c)$ away from $\Lambda$CDM. This is the first time that a moderate preference for EDE has been reported for these three combined CMB data sets. We find that including measurements of supernovae luminosity distances and the baryon acoustic oscillation standard ruler only minimally affects the preference ($3.0\sigma$), while measurements that probe the clustering of matter at late times - the lensing potential power spectrum from Planck and $f \sigma_8$ from BOSS - decrease the significance of the preference to 2.6$\sigma$. Conversely, adding a prior on the $H_0$ value as reported by the SH0ES collaboration increases the preference to the $4-5\sigma$ level. In the absence of this prior, the inclusion of Planck TT data at $\ell>1300$ reduces the preference from $3.0\sigma$ to $2.3\sigma$ and the constraint on $f_{\rm EDE}(z_c)$ becomes compatible with $\Lambda$CDM at $1\sigma$. We explore whether systematic errors in the Planck polarization data may affect our conclusions and find that changing the TE polarization efficiencies significantly reduces the Planck preference for EDE. More work will be necessary to establish whether these hints for EDE within CMB data alone are the sole results of systematic errors or an opening to new physics. ","Hints of Early Dark Energy in Planck, SPT, and ACT data: new physics or
  systematics?"
173,1495957750219739136,1238890699111751680,Anirudha Majumdar,"['New preprint:\nComparing the Complexity of Robotic Tasks\n<LINK>\nWe propose a definition of reduction between robotic tasks in order to capture when one task is harder than another, and also define a measure that quantifies the relative complexity between tasks.', 'Work led by Michelle Ho (undergraduate student in the group) and Alec Farid.']",https://arxiv.org/abs/2202.09892,"We are motivated by the problem of comparing the complexity of one robotic task relative to another. To this end, we define a notion of reduction that formalizes the following intuition: Task 1 reduces to Task 2 if we can efficiently transform any policy that solves Task 2 into a policy that solves Task 1. We further define a quantitative measure of the relative complexity between any two tasks for a given robot. We prove useful properties of our notion of reduction (e.g., reflexivity, transitivity, and antisymmetry) and relative complexity measure (e.g., nonnegativity and monotonicity). In addition, we propose practical algorithms for estimating the relative complexity measure. We illustrate our framework for comparing robotic tasks using (i) examples where one can analytically establish reductions, and (ii) reinforcement learning examples where the proposed algorithm can estimate the relative complexity between tasks. ",Comparing the Complexity of Robotic Tasks
174,1495806095205584896,795089712864051200,Barret Zoph,"['Interested in using sparse expert models, but find they are unstable, hard to design or don‚Äôt fine-tune well?\n\nWe address these key issues and train 269B param MoE model (w/ FLOPs of 32B dense model) that improves SOTA on NLP benchmarks liked SuperGLUE.\n\n<LINK> <LINK>', 'We do a large-scale study of the quality-stability trade-offs of stability techniques\n\nWe observe that our router z-loss fixes stability, while slightly improving quality\n\nThe router z-loss is an auxiliary loss that makes the logits of the router smaller for numerical stability https://t.co/yP8Idybzjo', 'We study the fine-tuning of sparse vs dense models\n\nThe optimal batch sizes and learning rates for sparse vs dense models are very different\n\nIn certain scenarios wrong values masked any of the pre-training performance improvements of sparse models over the dense models https://t.co/zbMWHfZ5G1', 'We also find sparse models to be incredibly robust to ‚Äúdropping‚Äù tokens during fine-tuning. \n\nSparse models have a fixed batch size ahead of time, so if there is overflow to a specific expert, the token is ‚Äúdropped‚Äù and passed to the next layer unchanged. https://t.co/HQwSPufNo2', 'For two models with the same FLOPs per token, we find sparse models to outperform their dense counterpart on a large suite of fine-tuning tasks. https://t.co/DrBLzX5Ij7', 'We also propose a few heuristics and architectural modifications to design Pareto efficient architectures. https://t.co/VIfsTpSEN6', 'We study how the experts specialize on different tokens and find that they end up semantically specializing to different categories such as punctuation, verbs and proper names. https://t.co/aYXT3M5ZY6', 'We finally combine our improvements and train a sparse model with 269B parameters (FLOP matched to a 32B dense model).\n\nThis model achieve SOTA on a wide range of NLP tasks: SuperGLUE, XSum, CNN-DM, ANLI R3, ARC-Easy/Challenge, CB WebQA, CB NatQA. https://t.co/XBxeXzcNWU']",http://arxiv.org/abs/2202.08906,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3). ",ST-MoE: Designing Stable and Transferable Sparse Expert Models
175,1495803324926664707,1176581066863153153,Vishvak Murahari,"['Data Multiplexing for Neural NetworksüîÄ\nCan neural networks process multiple instances simultaneously as a single mixed input, similar to how radio channels can share bandwidth to carry multiple signals? Surprisingly, we find they can indeed!! \n\n<LINK>  \nüìú [1/6] <LINK>', 'We propose DataMUX, a novel data multiplexing method that enables networks to process multiple inputs simultaneously using a single compact representation, while ensuring accurate predictions. This enables massive gains in throughput with minimal model size/compute overhead\n[2/6]', ""We introduce three modules: 1) a multiplexing layer that performs a linear transformation to each input before merging them 2) a demultiplexing layer that converts the base network's output into independent representations 3) an auto-encoding retrieval task to aid training.\n[3/6] https://t.co/jv0GJF7FWG"", 'Multiplexed Transformers (T-MUX) can process 20x/40x inputs across tasks, achieving 11x/18x gain in throughput with minimal absolute performance drops of 2% and 4% respectively on MNLI. We also explore DataMUX for MLPs and CNNs on image classification to a limited extent.\n[4/6] https://t.co/aYgWdlexF7', 'Our findings have connections to recent work around overparameterization in neural networks (Allen-Zhu et al.), lottery ticket hypothesis (Frankle and Carbin et al.) and works like Mixup(Zhang et al.) that train multiple sub-networks to improve robustness (Havasi et al.).\n[5/6]', 'For more details, check out:\nData Multiplexing for Neural Networks\nBy @VishvakM @_carlosejimenez @RunzheYang @karthik_r_n \nPaper: https://t.co/OZR1BE1LWQ \nCode: https://t.co/F6r5FwDAAc\nProject Page: https://t.co/Wva2jYaBMR\n[6/6]']",https://arxiv.org/abs/2202.09318,"In this paper, we introduce data multiplexing (DataMUX), a technique that enables deep neural networks to process multiple inputs simultaneously using a single compact representation. DataMUX demonstrates that neural networks are capable of generating accurate predictions over mixtures of inputs, resulting in increased throughput with minimal extra memory requirements. Our approach uses two key components -- 1) a multiplexing layer that performs a fixed linear transformation to each input before combining them to create a mixed representation of the same size as a single input, which is then processed by the base network, and 2) a demultiplexing layer that converts the base network's output back into independent representations before producing predictions for each input. We show the viability of DataMUX for different architectures (Transformers, and to a lesser extent MLPs and CNNs) across six different tasks spanning sentence classification, named entity recognition and image classification. For instance, DataMUX for Transformers can multiplex up to $20$x/$40$x inputs, achieving $11$x/$18$x increase in throughput with minimal absolute performance drops of $<2\%$ and $<4\%$ respectively on MNLI, a natural language inference task. We also provide a theoretical construction for multiplexing in self-attention networks and analyze the effect of various design elements in DataMUX. ",DataMUX: Data Multiplexing for Neural Networks
176,1495157288914538500,253939924,√ñmer Faruk Tuna,['<LINK>\n\nCheck out our new study. We propose a simple modification to standard dnn based classifiers which helps to zero out gradients for both targeted and untargeted attacks without impairing ability to learn. #deeplearning #adversarialml'],https://arxiv.org/abs/2202.07342,"In standard Deep Neural Network (DNN) based classifiers, the general convention is to omit the activation function in the last (output) layer and directly apply the softmax function on the logits to get the probability scores of each class. In this type of architectures, the loss value of the classifier against any output class is directly proportional to the difference between the final probability score and the label value of the associated class. Standard White-box adversarial evasion attacks, whether targeted or untargeted, mainly try to exploit the gradient of the model loss function to craft adversarial samples and fool the model. In this study, we show both mathematically and experimentally that using some widely known activation functions in the output layer of the model with high temperature values has the effect of zeroing out the gradients for both targeted and untargeted attack cases, preventing attackers from exploiting the model's loss function to craft adversarial samples. We've experimentally verified the efficacy of our approach on MNIST (Digit), CIFAR10 datasets. Detailed experiments confirmed that our approach substantially improves robustness against gradient-based targeted and untargeted attack threats. And, we showed that the increased non-linearity at the output layer has some additional benefits against some other attack methods like Deepfool attack. ",Unreasonable Effectiveness of Last Hidden Layer Activations
177,1495130648448700416,4786381520,üå¥Muhao Chenüå¥,['#TACL <LINK> In this work we studied how indirect supervision from NLI led to both precise and generalizable (ultra-fine) entity typing. <LINK>'],https://arxiv.org/abs/2202.06167,"The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types. ","Ultra-fine Entity Typing with Indirect Supervision from Natural Language
  Inference"
178,1494683502251495424,2930047588,Andrew Vanderburg,"[""Congrats to Maura Lally @mauralally9 on her first first-author paper! In it, we study the variability in HAT-P-7 b's atmosphere and show that it may not be as clear as previously believed:  <LINK>"", ""We study the atmospheres of hot, close-in exoplanets by measuring how the total brightness of the system changes throughout the planet's orbit. When the planet is on the far side of its orbit, it both reflects and emits light in our direction, so we measure more total light. https://t.co/lTLKzAOtRs"", 'Making these measurements, called ""phase curves"", has been done since the 2000s using telescopes like Spitzer, @KeplerGO , and @NASAHubble. This technique will only increase in importance now that @NASAWebb has launched. https://t.co/bsC48Jbr14', 'This is challenging in and of itself, but measuring small changes in the atmospheres over time -- weather on exoplanets -- is even harder. A major result came in 2016, when variations were found in the atmosphere of HAT-P-7 b in four years of Kepler data: https://t.co/h47cuXm9pb https://t.co/6VUboqWlnc', 'This is a great paper and an extremely impressive result, so I was excited to see if we could make similar measurements for other planets! So when Maura came to @UTAstronomy for a summer REU in 2019, we decided to try replicating HAT-P-7 b result and look at other Kepler systems.', ""Maura built a pipeline to download/pre-process the Kepler data and use MCMC to measure the phase curve parameters over time.  Here's an example phase curve fit for a single 22-day time period. From each fit, we extracted the amplitude and orbital phase of max brightness. https://t.co/QfUY5HOmA5"", 'We then did this for the entire 4-year Kepler light curve. Our measurements are the dark points, and the original result is in faint points. Note the original paper has more points because they plot overlapping segments as well, but overall the results are very similar. https://t.co/T7Q2OqV1Is', 'But in the process, we noticed some odd results that made us a bit nervous. For example, when we used different methods (https://t.co/6MLJtA5vFO) to remove long-term trends from the light curves, we could get significantly different results: https://t.co/qNEBNjjpRG', 'We decided to investigate further. When we injected un-changing phase curves into similar Kepler stars and measured phase curve parameters, we still recovered apparently statistically significant variability. So apparently something else can cause variability like we see. https://t.co/uAjYKHSnMO', ""What could be causing these variations? We looked at the power spectrum of HAT-P-7's light curve. There are signals due to granulation and p-mode oscillations, as well as supergranulation (horizontal stochastic flows on ~day timescales) and a possible rotation period at 1.7 days. https://t.co/ra5xezFtZU"", 'After some investigation, we concluded that the unmodeled red noise in the light curve due to supergranulation is the most likely culprit. We simulated the spurious phase curve variations we would see in the presence of the measured supergranulation and found a very close match. https://t.co/xU96y9xzSr', ""So we think it's possible that the measured phase curve variations in HAT-P-7 b's atmosphere are really due to the host star's variability! This underscores how hard it is to measure atmospheric variability, and how important careful modeling of stellar noise will be for JWST."", 'Finally, I want to emphasize what impressive work this was from @mauralally9! Her summer project turned into almost three years of work on an extremely challenging paper, but it has really paid off in the end! I think we can expect great things from her in grad school @Cornell!']",https://arxiv.org/abs/2202.08279,"We reassess the claimed detection of variability in the atmosphere of the hot Jupiter HAT-P-7 b, reported by Armstrong et al. (2016). Although astronomers expect hot Jupiters to have changing atmospheres, variability is challenging to detect. We looked for time variation in the phase curves of HAT-P-7 b in Kepler data using similar methods to Armstrong et al. (2016), and identified apparently significant variations similar to what they found. Numerous tests show the variations to be mostly robust to different analysis strategies. However, when we injected unchanging phase curve signals into the light curves of other stars and searched for variability, we often saw similar levels of variations as in the HAT-P-7 light curve. Fourier analysis of the HAT-P-7 light curve revealed background red noise from stellar supergranulation on timescales similar to the planet's orbital period. Tests of simulated light curves with the same level of noise as HAT-P-7's supergranulation show that this effect alone can cause the amplitude and phase offset variability we detect for HAT-P-7 b. Therefore, the apparent variations in HAT-P-7 b's atmosphere could instead be caused by non-planetary sources, most likely photometric variability due to supergranulation on the host star. ","Reassessing the Evidence for Time Variability in the Atmosphere of the
  Exoplanet HAT-P-7 b"
179,1494676841126416393,1327363823817412608,Maura Lally,"['Browsing arXiv? Check out my first first-author paper, with @amvanderburg: we reassess the claimed detection of atmospheric variability on exoplanet HAT-P-7b, &amp; find that the apparent variations could instead be caused by supergranulation on the host star. <LINK>']",https://arxiv.org/abs/2202.08279,"We reassess the claimed detection of variability in the atmosphere of the hot Jupiter HAT-P-7 b, reported by Armstrong et al. (2016). Although astronomers expect hot Jupiters to have changing atmospheres, variability is challenging to detect. We looked for time variation in the phase curves of HAT-P-7 b in Kepler data using similar methods to Armstrong et al. (2016), and identified apparently significant variations similar to what they found. Numerous tests show the variations to be mostly robust to different analysis strategies. However, when we injected unchanging phase curve signals into the light curves of other stars and searched for variability, we often saw similar levels of variations as in the HAT-P-7 light curve. Fourier analysis of the HAT-P-7 light curve revealed background red noise from stellar supergranulation on timescales similar to the planet's orbital period. Tests of simulated light curves with the same level of noise as HAT-P-7's supergranulation show that this effect alone can cause the amplitude and phase offset variability we detect for HAT-P-7 b. Therefore, the apparent variations in HAT-P-7 b's atmosphere could instead be caused by non-planetary sources, most likely photometric variability due to supergranulation on the host star. ","Reassessing the Evidence for Time Variability in the Atmosphere of the
  Exoplanet HAT-P-7 b"
180,1494636885674799107,1247125153110204416,Roland Szakacs,"['Paper day! ""The Column Densities of Molecular Gas across Cosmic Time: Bridging Observations and Simulations"" is accepted for publication in #MNRAS! In this paper we study the evolution of the molecular gas column density distribution in sim. and obs.: <LINK>']",https://arxiv.org/abs/2202.08777,"Observations of the cosmic evolution of different gas phases across time indicate a marked increase in the molecular gas mass density towards $z\sim 2-3$. Such a transformation implies an accompanied change in the global distribution of molecular hydrogen column densities ($N_{\rm{H_2}}$). Using observations by PHANGS-ALMA/SDSS and simulations by GRIFFIN/IllustrisTNG we explore the evolution of this H$_2$ column density distribution function [$f(N_{\rm{H}_2})$]. The H$_2$ (and HI) column density maps for TNG50 and TNG100 are derived in post-processing and are made available through the IllustrisTNG online API. The shape and normalization of $f(N_{\rm{H}_2})$ of individual main-sequence star-forming galaxies are correlated with the star formation rate (SFR), stellar mass (${M_*}$), and H$_2$ mass ($M_{\rm{H}_2}$) in both observations and simulations. TNG100, combined with H$_2$ post-processing models, broadly reproduces observations, albeit with differences in slope and normalization. Also, an analytically modelled $f(N)$, based on exponential gas disks, matches well with the simulations. The GRIFFIN simulation gives first indications that the slope of $f(N_{\rm{H}_2})$ might not majorly differ when including non-equilibrium chemistry in simulations. The $f(N_{\rm{H}_2})$ by TNG100 implies that higher molecular gas column densities are reached at $z=3$ than at $z=0$. Further, denser regions contribute more to the molecular mass density at $z=3$. Finally, H$_2$ starts dominating compared to HI only at column densities above log($N_{\rm{H}_2} / \rm{cm}^{-2}) \sim 21.8-22$ at both redshifts. These results imply that neutral atomic gas is an important contributor to the overall cold gas mass found in the ISM of galaxies including at densities typical for molecular clouds at $z=0$ and $z=3$. ","The Column Densities of Molecular Gas across Cosmic Time: Bridging
  Observations and Simulations"
181,1494490728738996225,1331065469151764483,Amit Seta,"['Usually, the turbulent dynamo is explored in an isothermal gas. We study it in a non-isothermal setting (motivation: the multiphase ISM). We find that the non-isothermal dynamo is less efficient than its isothermal counterpart... details below!\n\n<LINK> <LINK>', '@juandiegosoler @PaolaDom1 Yes, the growth of the field by the dynamo action in the colder, supersonic gas (Mach ~ 5) is an order of magnitude weaker than in the warmer, transsonic (Mach ~ 1) region... But the field in the colder gas is quite strong compared to the RMS value due to gas compression (Fig. 6)']",https://arxiv.org/abs/2202.08324,"Magnetic fields are a dynamically important component of the turbulent interstellar medium (ISM) of star-forming galaxies. These magnetic fields are due to a dynamo action, which is a process of converting turbulent kinetic energy to magnetic energy. A dynamo that acts at scales less than the turbulent driving scale is known as the turbulent dynamo. The ISM is a multiphase medium and observations suggests that the properties of magnetic fields differ with the phase. Here, we aim to study how the properties of the turbulent dynamo depend on the phase. We simulate the non-isothermal turbulent dynamo in a two-phase medium (most previous work assumes an isothermal gas). We find that the growth rate of magnetic fields in the exponentially growing stage is similar in both the phases, and this is because of a roughly equal amount of vorticity being generated in each phase. We further compute each term responsible for amplification and destruction of vorticity and show that the amplification of vorticity by turbulent motions is a dominant term (similar in both the phases) followed by the baroclinic term (only present in non-isothermal gases, higher in the warm phase) and the term for viscous interactions in the presence of logarithmic density gradients (higher in the cold phase). We find that the final ratio of magnetic to turbulent kinetic energy is lower due to a stronger Lorentz force. We find that the non-isothermal turbulent dynamo is less efficient than its isothermal counterpart. ",Turbulent dynamo in the two-phase interstellar medium
182,1493965067804626950,23237589,Tanya Urrutia,"['Oh man, I almost missed that Josie finally put this on ArXiv (<LINK>). I love this paper. First, we find ~20% of Lyman Alpha Emitters have Equivalent Widths &gt;240 Angstrom, that is really hard to explain with ""normal"" star formation.', 'Then the distribution of equivalent width histograms are not the same for MUSE-Wide and MUSE-Deep, with Wide showing a propensity for higher EW objects. It is a bit hard to explain, but it is really a product of the Lyman Alpha Luminosity function.', ""We reproduce the anti-correlation between effective UV continuum radius and EW, but other UV and spectral properties do not show clear correlations with EW. The highes EW LAE doesn't show a halo, though!""]",https://arxiv.org/abs/2202.06642,"The aim of this study is to better understand the connection between the Lyman $\alpha$ rest-frame equivalent width (EW$_0$) and spectral properties as well as ultraviolet (UV) continuum morphology by obtaining reliable EW$_0$ histograms for a statistical sample of galaxies and by assessing the fraction of objects with large equivalent widths. We used integral field spectroscopy from MUSE combined with broad-band data from the Hubble Space Telescope (HST) to measure EW$_0$. We analysed the emission lines of $1920$ Lyman $\alpha$ emitters (LAEs) detected in the full MUSE-Wide (one hour exposure time) and MUSE-Deep (ten hour exposure time) surveys and found UV continuum counterparts in archival HST data. We fitted the UV continuum photometric images using the Galfit software to gain morphological information on the rest-UV emission and fitted the spectra obtained from MUSE to determine the double peak fraction, asymmetry, full-width at half maximum, and flux of the Lyman $\alpha$ line. The two surveys show different histograms of Lyman $\alpha$ EW$_0$. In MUSE-Wide, $20\%$ of objects have EW$_0 > 240$ \r{A}, while this fraction is only $11\%$ in MUSE-Deep and $\approx 16\%$ for the full sample. This includes objects without HST continuum counterparts (one-third of our sample), for which we give lower limits for EW$_0$. The object with the highest securely measured EW$_0$ has EW$_0=589 \pm 193$ \r{A} (the highest lower limit being EW$_0=4464$ \r{A}). We investigate the connection between EW$_0$ and Lyman $\alpha$ spectral or UV continuum morphological properties. The survey depth has to be taken into account when studying EW$_0$ distributions. We find that in general, high EW$_0$ objects can have a wide range of spectral and UV morphological properties, which might reflect that the underlying causes for high EW$_0$ values are equally varied. (abridged) ",Equivalent widths of Lyman $\alpha$ emitters in MUSE-Wide and MUSE-Deep
183,1493859421423915012,223144852,Suchita Kulkarni,"['Today on arXiv, looking forward at @FCC_study , we try to understand what can we learn about neutrino mass mechanisms specifically in a given model. <LINK>\n\nTogether with my long-standing collaborators F. Deppisch, W. Liu. Wei was really the driving force.', ""@1cRebeca @xmpierinix @FCC_study I don't necessarily want to see it myself. That's why these studies, I just want to know what could the very forward possible future might hold. A theorist can dream? üôÇ\n\nOTOH I do want to make sure that our planet lasts for that long to have even a chance to make it happen."", '@mktraly @FCC_study thank you, I have no email from you, I checked.']",https://arxiv.org/abs/2202.07310,"We investigate the potential of the 100 TeV future circular collider (FCC-hh) to probe heavy neutrinos. We concentrate in particular on heavy neutrino production via a $U(1)_{B-L}$ $Z'$ gauge boson and contrast the resulting limits with that mediated by Standard Model weak currents. We consider heavy neutrino decays to semi-leptonic as well as fully leptonic final states, particularly with muon flavour, and we show the importance of considering searches both in prompt and displaced decays of the heavy neutrinos. For prompt final states, semi-leptonic modes are more promising due to smaller background and larger yields, and TeV-scale heavy neutrinos with active-sterile mixing compatible with light neutrino mass generation in a seesaw scenario can be probed for a 5 TeV $Z'$ and gauge coupling as low as $g_{B-L} = 10^{-2}$. Displaced vertex searches can extend this range to heavy neutrino masses as low as 10 GeV. ",Heavy Neutrinos at the FCC-hh in the $U(1)_{B-L}$ Model
184,1493516961124392964,1364749022,Haitham Bou Ammar,"['üö® 3 steps to provable almost sure safety in RL (can do model-based and model-free)üö®\nAiming to find a simple and effective solution for safe RL, we present SAUTE RL. üëâüëâIt seriously requires minor changes to your gym env üëàüëà Check it out: <LINK> <LINK>', '3 steps to provable almost sure safety in #reinforcementlearning  (can do model-based and model-free). #MachineLearning #DeepLearning https://t.co/hpBh9uoJVb']",https://arxiv.org/abs/2202.06558,"Satisfying safety constraints almost surely (or with probability one) can be critical for deployment of Reinforcement Learning (RL) in real-life applications. For example, plane landing and take-off should ideally occur with probability one. We address the problem by introducing Safety Augmented (Saute) Markov Decision Processes (MDPs), where the safety constraints are eliminated by augmenting them into the state-space and reshaping the objective. We show that Saute MDP satisfies the Bellman equation and moves us closer to solving Safe RL with constraints satisfied almost surely. We argue that Saute MDP allows to view Safe RL problem from a different perspective enabling new features. For instance, our approach has a plug-and-play nature, i.e., any RL algorithm can be ""sauteed"". Additionally, state augmentation allows for policy generalization across safety constraints. We finally show that Saute RL algorithms can outperform their state-of-the-art counterparts when constraint satisfaction is of high importance. ","SAUTE RL: Almost Surely Safe Reinforcement Learning Using State
  Augmentation"
185,1493483808036950018,199249094,Paul Villoutreix,"['New paper out! \nWe started with the question: how does the structure of the lymph node affects T-cells exploration? \nWe ended up with a new approach for the study of random walks (RW) on large networks and used it on an actual lymph node. 1/n\n<LINK> <LINK>', 'RW on networks are a widely used to model search strategies, transportation problems, or disease propagation. In this model, random walkers hop from node to node while choosing with an uniform probability the edges on which to travel.', 'The structure of the underlying network, such as its degree distribution and connectivity pattern, will thus determine how the RW evolves over time. Can this connectivity pattern favour the exploration of some nodes over others?', 'We propose a general framework to find network heterogeneities, which we define as connectivity patterns that affect the RW. We propose to characterize and measure these heterogeneities by i) ranking nodes and ii) detecting communities in a way that is RW interpretable. https://t.co/QwrqB7tYfr', 'Moreover, we propose iii) an approximation to accurately and efficiently compute these quantities on large networks.', 'We first applied our method on toy models, in particular, we showed its efficiency at contrasting two highly similar networks (identical degree distribution, same number of nodes) https://t.co/3Wy2HV8qgz', 'Moreover, we showed that none of our computations are redundant with previous centralities or random walk based measures (such as the global mean first passage time) https://t.co/4Ee4hUgjxm', ""Finally, we applied our methodology to characterize an actual lymph node obtained from Kelch et al. 2019. It's a large network containing about 200,000 nodes. So we had to use our approximation which appeared very accurate. https://t.co/AbMAhy0voG"", 'Our analysis suggests that the lymph node conduit network structure is highly homogeneous (a bit like a foam) and therefore promotes a uniform exploration of space by T-cells!', 'We developed an interactive visualisation platform if you want to explore these results \nhttps://t.co/KQKXgTUjaz', 'The credit of this work goes to Sol√®ne Song (@SongSolene), Malek Senoussi (@PtyFilou ) and Paul Escande! https://t.co/prATrApFTs']",https://arxiv.org/abs/2202.06729,"Random walks on networks are widely used to model stochastic processes such as search strategies, transportation problems or disease propagation. A prominent biological example of search by random walkers on a network is the guiding of naive T cells by the lymphatic conduits network in the lymph node. Motivated by this case study, we propose a general framework to find network heterogeneities, which we define as connectivity patterns that affect the random walk. We propose to characterize and measure these heterogeneities by i) ranking nodes and ii) detecting communities in a way that is interpretable in terms of random walk, moreover, we propose iii) an approximation to accurately and efficiently compute these quantities on large networks. The ranking parameter we propose is the probability of presence field, and the community detection method adapts previously defined diffusion coordinates. In addition, we propose an interactive data visualization platform to follow the dynamics of the random walks and their characteristics on our datasets, and a ready-to-use pipeline for other datasets upon download. We first showcase the properties of our method on toy models. We highlight this way the efficiency of our methods at contrasting two highly similar networks (identical degree distribution, same number of nodes). Moreover, we show numerically that the ranking and communities defined in this way are not redundant with any other classical methods (centralities, global mean first passage, louvain, node2vec). We then use our methods to characterize the lymph node conduits network. We show that the lymph node conduits network appears homogeneous and therefore has a global structure that promotes a uniform exploration of space by T-cells. ","Random walk informed community detection reveals heterogeneities in
  large networks"
186,1493482976319004674,826482000597024770,Deividas Sabonis,"['Two new preprint today @arxiv.\n\nIn the first one, we study parity switching (quasiparticle poisoning) in a hybrid superconductor-semiconductor qubit.\n\n<LINK>', 'In the second one, we show the electrostatic control of transition/poisoning rates in a superconducting island  (by several orders of magnitude).\n\nhttps://t.co/h0kwBv7SAP']",https://arxiv.org/abs/2202.05974,"The rate of charge-parity switching in a full-shell superconductor-semiconductor nanowire qubit is measured by directly monitoring the dispersive shift of a readout resonator. At zero magnetic field, the measured switching time scale $T_P$ is on the order of 100 ms. Two-tone spectroscopy data post-selected on charge-parity is demonstrated. With increasing temperature or magnetic field, TP is at first constant, then exponentially suppressed, consistent with a model that includes both non-equilibrium and thermally activated quasiparticles. As TP is suppressed, qubit lifetime T1 also decreases. The long $T_P\sim 0.1$ s at zero field is promising for future development of qubits based on hybrid nanowires. ","Parity switching in a full-shell superconductor-semiconductor nanowire
  qubit"
187,1493308664219934729,1316746180085403655,Pierre Colombo,"['Super excited to share our new work on Evaluation of NLP system: <LINK> ! When using large benchmark we often try to answer to the question: ¬´ what are the best systems? ¬ª. We study alternative such as using ranking ideas instead of considering the mean!', ""Joint work with @nathan_noiry, Ekhine and Stephan! CLI interface is available at https://t.co/uzKUGS8e5O‚Ä¶. We thanks @seb_ruder for it's inspirational blog post as well: https://t.co/EQvXmfMayf‚Ä¶.""]",https://arxiv.org/abs/2202.03799,"In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in (i) assessing the progress of new methods along different axes and (ii) selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (e.g. GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust. ",What are the best systems? New perspectives on NLP Benchmarking
188,1493251761515089921,352690320,Christopher Jung,['Excited about this new work with Pranjal Awasthi and @jamiemorgenste1 (my google internship projectüòÄ): <LINK>. \n\nWe study how to combine a labeled dataset and unlabeled dataset which may also have additional auxiliary features not present in the first dataset.'],https://arxiv.org/abs/2202.05797,"Suppose we are given two datasets: a labeled dataset and unlabeled dataset which also has additional auxiliary features not present in the first dataset. What is the most principled way to use these datasets together to construct a predictor? The answer should depend upon whether these datasets are generated by the same or different distributions over their mutual feature sets, and how similar the test distribution will be to either of those distributions. In many applications, the two datasets will likely follow different distributions, but both may be close to the test distribution. We introduce the problem of building a predictor which minimizes the maximum loss over all probability distributions over the original features, auxiliary features, and binary labels, whose Wasserstein distance is $r_1$ away from the empirical distribution over the labeled dataset and $r_2$ away from that of the unlabeled dataset. This can be thought of as a generalization of distributionally robust optimization (DRO), which allows for two data sources, one of which is unlabeled and may contain auxiliary features. ",Distributionally Robust Data Join
189,1492214859655569408,348159742,Blair Bilodeau,"['Is it possible to efficiently identify the optimal intervention while remaining agnostic to assumptions about the causal structure?\n\nIn new work with Linbo Wang and @roydanroy, we study adapting to the presence of a d-separator using multi-armed bandits.\n\n<LINK> <LINK>', 'Causal assumptions provide guarantees to more efficiently identify the optimal intervention. However, it can be expensive or impossible to verify such assumptions. Ideal algorithms should do as well as if they knew whether the assumptions hold, without requiring this knowledge.', 'We provide a new algorithm that (a) achieves optimal regret when given access to a d-separator, beating algos like UCB, and (b) significantly improves on causal bandit algos when no d-separator is observed. We require no prior knowledge of whether a d-separator is observed.', 'We prove that when d-separation is incorrectly assumed, existing causal bandit algos can incur linear regret (worst possible). We also prove optimal adaptivity is *impossible*: no algorithm can enjoy the benefits of causal structure while never paying for it in the worst-case.', 'Our new algorithm (HAC-UCB) always gets sublinear regret, and in certain settings where existing causal bandit algos get linear regret, we prove HAC-UCB only incurs sqrt(T) regret. We also see this improvement in simulations. https://t.co/xyYBeDLRQL', 'Our adaptivity is wrt a novel condition for bandits. It reduces to observing a d-separator when actions are all do interventions, and is implied by the front-door criterion without the null intervention. Next steps: apply our framework for adapting to other causal structures.', 'This is a growing area of research that has generated lots of exciting work in the last few years. I‚Äôll end by plugging next week‚Äôs workshop @SimonsInstitute, with speakers who will be discussing many related ideas.\nhttps://t.co/B1SzXNSJ4v', '@aminkarbasi @roydanroy Thanks!']",https://arxiv.org/abs/2202.05100,"Multi-armed bandit problems provide a framework to identify the optimal intervention over a sequence of repeated experiments. Without additional assumptions, minimax optimal performance (measured by cumulative regret) is well-understood. With access to additional observed variables that d-separate the intervention from the outcome (i.e., they are a d-separator), recent causal bandit algorithms provably incur less regret. However, in practice it is desirable to be agnostic to whether observed variables are a d-separator. Ideally, an algorithm should be adaptive; that is, perform nearly as well as an algorithm with oracle knowledge of the presence or absence of a d-separator. In this work, we formalize and study this notion of adaptivity, and provide a novel algorithm that simultaneously achieves (a) optimal regret when a d-separator is observed, improving on classical minimax algorithms, and (b) significantly smaller regret than recent causal bandit algorithms when the observed variables are not a d-separator. Crucially, our algorithm does not require any oracle knowledge of whether a d-separator is observed. We also generalize this adaptivity to other conditions, such as the front-door criterion. ",Adaptively Exploiting d-Separators with Causal Bandits
190,1492151791928098818,1471512679655231493,Matt Turner,"['Proud to present my first academic paper, in collaboration with Karsten Matthies and Johannes Nordstr√∂m, where we find some particularly elusive G2-instantons \n<LINK>']",https://arxiv.org/abs/2202.05028,We construct SU(2)^2xU(1)-invariant G_2-instantons on the asymptotically conical limit of the C7 family of G_2-metrics. The construction uses a dynamical systems approach involving perturbations of an abelian solution and a solution on the G_2-cone. From this we obtain a 1-parameter family of invariant instantons with gauge group SU(2) and bounded curvature. ,SU(2)^2xU(1)-invariant G_2-instantons on the AC limit of the C7 family
191,1492049003957075986,809336237194706944,Rico Sennrich,"['Neural MT metrics show high correlation with human judgments, but can we safely optimize towards them? Study by @chantalamrhein using MBR decoding with COMET as utility function shows marked increase in wrong translations of numbers and names. 1/2\n\n<LINK>', 'The paper is also a call for developers of neural metrics: high correlation with human judgments is great, but if you can identify and eliminate such blind spots during development, this will make resulting metrics even more useful for the community.\n2/2']",https://arxiv.org/abs/2202.05148,"Neural metrics have achieved impressive correlation with human judgements in the evaluation of machine translation systems, but before we can safely optimise towards such metrics, we should be aware of (and ideally eliminate) biases towards bad translations that receive high scores. Our experiments show that sample-based Minimum Bayes Risk decoding can be used to explore and quantify such weaknesses. When applying this strategy to COMET for en-de and de-en, we find that COMET models are not sensitive enough to discrepancies in numbers and named entities. We further show that these biases cannot be fully removed by simply training on additional synthetic data. ","Identifying Weaknesses in Machine Translation Metrics Through Minimum
  Bayes Risk Decoding: A Case Study for COMET"
192,1491557644368293897,789306537781104640,Scott Fleming,"['New paper out today! We studied low-energy, time-resolved UV flares using GALEX/gPhoton data from the GJ 65 system! Shout-outs to my co-authors Chase Million, @rachelosten, Dmitrii Kolotkov, and @cebrasseur. A summary in this thread.\n\nPaper link: <LINK> <LINK>', ""We believe in open data, open software, transparency, and reproducibility. I didn't use showyourwork (timing didn't work out, sorry @rodluger, next time!), but we do have a collection of Python notebooks to recreate figures and tables here: https://t.co/anYuBuMvGj"", 'GALEX raw data is in the form of photon events. The calibrated, delivered mission products at @MAST_News are visit-level images (below) and source catalogs. You can do a ton of science with those, but we want to study variability.\n\n""GALEX is a timeseries mission.""\n-Scott Fleming https://t.co/qC25o52gCL', ""Why look at GALEX data during a visit? Because while most things aren't changing over &lt; 30 minute timescales, lots of things are. White dwarfs, stellar flares, sdB stars, transiting things, eruptive things. We want to take static GALEX data and turn it into dynamic data! https://t.co/BGlWUWwRPE"", ""Why study GJ 65? It's a binary system of two mid-M dwarfs known to be active flare stars, it's quite bright without saturating GALEX detectors, and has a lot of GALEX coverage. So other than the binarity, it's one of the best GALEX sources to study low S:N flares."", ""Disclaimer: We aren't the first to study:\n- things using the GALEX photon events\n- flares using GALEX photon events\n- GJ 65 using GALEX photon events\n\nBut we do think we've done the most comprehensive job of these things to-date. See refs in our paper for previous work!"", ""Back to our study. One big flare was known and well-studied. We found 13 NEW flare events between log(E) 28.5 and 29.5 in the UV. It's hard to get much lower than these, due to S:N concerns. As found by @cebrasseur previous work on UV flares with gPhoton, the shapes can be weird! https://t.co/k4YHb32UNY"", ""One side result: we simulated FRED-like idealized flares that might be concurrent. For a non-FRED shape, we don't know if it's a single flare with a weird shape, or a bunch of FRED-like flares happening concurrently. If the latter, you will under-count and over-estimate energies. https://t.co/u85xjOqlmL"", 'Do we see a number of low energy flares consistent with other flare frequency estimates? Super hard to say: this is just one system (never base off one example), we don\'t know which flare is coming from which star, and it\'s just hard to define what a single ""flare"" is. But, yes? https://t.co/aIjgMxkqQY', 'The big flare was found a while ago by @_BEMILES and @evgenya looking at the visit-level data in 2017. Doyle et al. 2018 time-resolved it with gPhoton in the NUV and reported quasi-periodic pulsations. Our analysis includes the FUV and we confirm QPP for sure! https://t.co/hMDO9QlrgR', 'What\'s next? We\'ve run the entire gPhoton corpus through a variability search on AWS. We\'re going to be finishing work on a catalog of EVERYTHING that goes ""bump in the night"". There\'s flares and eclipses and...things we don\'t know anything about yet...all coming to @MAST_News.', 'There will be plenty of flares in that catalog, but from fainter stars, and due to S:N, these will all be higher energies that are still neat, but are also well-represented in Kepler/TESS/ground data sets. The GJ 65 system is likely going to give us the smallest GALEX flares.', ""One last thing on a personal note. I had a TON of stuff happen in and out of work the past 18 months. I took a full year to respond to a single referee comment b/c I just didn't have it in me for a long time. I want to thank the editor of ApJ and the referee for their patience."", ""And for those who may have similar papers stuck in a limbo: there are a lot of reasons to not be able to finish a paper, but I'm here to say: don't let shame or embarrassment be the only reason. It's not a mark against you, we all suffer from it, even senior astronomers.""]",https://arxiv.org/abs/2202.02861,"Characterizing the distribution of flare properties and occurrence rates is important for understanding habitability of M dwarf exoplanets. The GALEX space telescope observed the GJ 65 system, composed of the active, flaring M stars BL Cet and UV Cet, for 15900 seconds (~4.4 hours) in two ultraviolet bands. The contrast in flux between flares and the photospheres of cool stars is maximized at ultraviolet wavelengths, and GJ 65 is the brightest and nearest flaring M dwarf system with significant GALEX coverage. It therefore represents the best opportunity to measure low energy flares with GALEX. We construct high cadence light curves from calibrated photon events and find 13 new flare events with NUV energies ranging from 10^28.5 - 10^29.5 ergs and recover one previously reported flare with an energy of 10^31 ergs. The newly reported flares are among the smallest M dwarf flares observed in the ultraviolet with sufficient time resolution to discern light curve morphology. The estimated flare frequency at these low energies is consistent with extrapolation from the distributions of higher-energy flares on active M dwarfs measured by other surveys. The largest flare in our sample is bright enough to exceed the local non-linearity threshold of the GALEX detectors, which precludes color analysis. However, we detect quasi-periodic pulsations (QPP) during this flare in both the FUV and NUV bands at a period of ~50 seconds, which we interpret as a modulation of the flare's chromospheric thermal emission through periodic triggering of reconnection by external MHD oscillations in the corona. ","New Time-Resolved, Multi-Band Flares In The GJ 65 System With gPhoton"
193,1491465210414178310,1169242545139986432,Martin Lefebvre,"[""It's preprint time ! üòÄ\nCheck out our latest work with Prof. David Bol, in which we propose a family of simple current references, requiring a limited silicon area while ensuring robustness against supply voltage and temperature variations. üëá\n<LINK>"", 'These references consist of a two-transistor voltage reference, buffered onto a voltage-to-currrent converter by a single transistor.\nWe proposed two novel topologies, a nA-range proportional-to-absolute-temperature (PTAT) one and a ¬µA-range constant-with-temperature (CWT) one.', 'We fabricated and measured both current references in a 0.18-¬µm partially-depleted silicon-on-insulator (PDSOI) technology, demonstrating functionality on silicon in real-world conditions.', 'First, the PTAT reference is obtained by biasing a self-cascode MOSFET with a PTAT voltage. It generates a 0.096-nA current, consumes 0.28 nW at 0.55V, and only requires 7 transistors, thus occupying a silicon area of 8700 ¬µm^2.', 'Second, the CWT reference is obtained by biasing a polysilicon resistor with a CWT voltage. It generates a 1.09-¬µA current with a temperature coefficient (TC) of 38 ppm/¬∞C, and only requires 4 transistors and resistor, occupying a silicon area of 4300 ¬µm^2.', 'Finally, we demonstrated the portability of the references to common scaled technologies, such as 65-nm bulk and 28-nm fully-depleted SOI, by proposing techniques to deal with their analog non-idealites and by simulating the references post-layout in these two technologies.']",https://arxiv.org/abs/2202.01751,"The robustness of current and voltage references to process, voltage and temperature (PVT) variations is paramount to the operation of integrated circuits in real-world conditions. However, while recent voltage references can meet most of these requirements with a handful of transistors, current references remain rather complex, requiring significant design time and silicon area. In this paper, we present a family of simple current references consisting of a two-transistor (2T) ultra-low-power voltage reference, buffered onto a voltage-to-current converter by a single transistor. Two topologies are fabricated in a 0.18-$\mu$m partially-depleted silicon-on-insulator (SOI) technology and measured over 10 dies. First, a 7T nA-range proportional-to-absolute-temperature (PTAT) reference intended for constant-$g_m$ biasing of subthreshold operational amplifiers demonstrates a 0.096-nA current with a line sensitivity (LS) of 1.48 %/V, a temperature coefficient (TC) of 0.75 %/$^\circ$C, and a variability $(\sigma/\mu)$ of 1.66 %. Then, two 4T+1R $\mu$A-range constant-with-temperature (CWT) references with (resp. without) TC calibration exhibit a 1.09-$\mu$A (resp. 0.99-$\mu$A) current with a 0.21-%/V (resp. 0.20-%/V) LS, a 38-ppm/$^\circ$C (resp. 290-ppm/$^\circ$C) TC, and a 0.87-% (resp. 0.65-%) $(\sigma/\mu)$. In addition, portability to common scaled CMOS technologies, such as 65-nm bulk and 28-nm fully-depleted SOI, is discussed and validated through post-layout simulations. ","A Family of Current References Based on 2T Voltage References:
  Demonstration in 0.18-$\mu$m with 0.1-nA PTAT and 1.1-$\mu$A CWT
  38-ppm/$^\circ$C Designs"
194,1491200767428280320,53393389,Woo-Sung Jung,"['We study knowledge synchronization based #wikipedia and some #facebook data (Social Connectedness index). Investigating the path of knowledge diffusion using Wiki, we suggest 21st knowledge ""Silk Road"". @WikiResearch @Meta  <LINK> <LINK>']",https://arxiv.org/abs/2202.01466,"Humans acquire and accumulate knowledge through language usage and eagerly exchange their knowledge for advancement. Although geographical barriers had previously limited communication, the emergence of information technology has opened new avenues for knowledge exchange. However, it is unclear which communication pathway is dominant in the 21st century. Here, we explore the dominant path of knowledge diffusion in the 21st century using Wikipedia, the largest communal dataset. We evaluate the similarity of shared knowledge between population groups, distinguished based on their language usage. When population groups are more engaged with each other, their knowledge structure is more similar, where engagement is indicated by socioeconomic connections, such as cultural, linguistic, and historical features. Moreover, geographical proximity is no longer a critical requirement for knowledge dissemination. Furthermore, we integrate our data into a mechanistic model to better understand the underlying mechanism and suggest that the knowledge ""Silk Road"" of the 21st century is based online. ",Quantifying knowledge synchronisation in the 21st century
195,1489702326071808001,524971431,William Cai,"['Paper! with @rosemariesays , Bobbie Chern, @scorbettdavies , @mbogen , @tvr2c , @5harad. We propose a strategy to construct more equitable datasets for training ML models.\nPaper: <LINK>\nThread: 1/', 'Machine learning models have exhibited disparities in a wide variety of domains, often underperforming for members of traditionally underserved groups. One contributing factor to this is lack of representation in training data.', 'In response to this, there have been many calls to make datasets more diverse, including from policymakers, but it is unclear exactly how a model-builder should operationalize this.', 'Concretely, we consider a model-builder with a fixed budget who must allocate their budget to obtain training data over a variety of groups, who wants to construct a model with equitable performance across all groups.', 'We show that two heuristic strategies, representative and equal sampling, where the number of samples per group is proportional to its share in the overall population or equal, can lead to quite different downstream models and resulting outcomes.', 'Further, these heuristics do not consider many potentially relevant factors, including differences in cost or intrinsic challenges of data collection or effect on model performance between groups, which can lead to suboptimal results.', 'We present a framework for constructing broadly equitable datasets in this setting, which separates the problem into 2 key components:', ""1) The 'learning curves', which map allocations of budgets to resulting group-level model performances, and 2) specifying the model-builder‚Äôs preferences over resulting group-level model performances into a utility function."", 'Further, we present an adaptive sampling strategy which sequentially samples data while taking these factors into account.', 'We apply our framework and adaptive sampling strategy in both a synthetic example and a simulation environment for constructing polygenic risk scores.', 'Polygenic risk scores are a next-generation health tool for risk stratification via genomic data, which have traditionally suffered from poor performance in individuals of African descent due to lack of training data.', 'We show that in both settings, our adaptive sampling strategy obtains near-optimal policies, while allowing model-builders to efficiently prioritize traditionally underserved groups and avoid unintended consequences of heuristics such as representative and equal sampling.', 'We hope our work will be of use to practitioners constructing datasets to train models, and broadly support ongoing efforts to make statistical models more equitable.']",https://arxiv.org/abs/2202.01327,"In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data -- an application domain that often suffers from non-representative data collection. We find that our adaptive sampling strategy outperforms several common data collection heuristics, including equal and proportional sampling, demonstrating the value of strategic dataset design for building equitable models. ",Adaptive Sampling Strategies to Construct Equitable Training Datasets
196,1489330392242999299,1932483559,Akshat Shrivastava,"['I am excited to present our latest work on efficient and scalable semantic parsing: <LINK> . We propose a new parsing task: scenario based TOP and a neural retrieval model RAF with strong improvements on generalization despite being non-autoregressive. 1/ <LINK>', 'We propose a variation to traditional semantic parsing. We provide our model with a bank of all possible ‚Äúscenarios‚Äù (parses without any slots filled in). As data collection tends to start with scenarios, our intuition is to provide this explicitly to our model to reason over 2/ https://t.co/W3avL95B75', 'In order to effectively and efficiently solve scenario based parsing, we propose RAF (Retrieve-and-Fill) a coarse-to-fine parser that retrieves a scenario and fills in the slots. RAF allows us to learn scenarios effectively with the principle: modular yet differentiable.  3/ https://t.co/3TPRZhuiJD', 'Prior work in retrieval tends to focus on text ‚Üí text retrieval however in our case we are retrieving structures (scenarios). In order to improve our model we explore various representations to convert scenarios to text and combine them. 4/ https://t.co/fAzusbinx3', 'Despite leveraging a base model as the encoder and non-autoregressive decoding, our scenario modeling formulation along with the RAF architecture showcases strong improvements to generalization across new domains and new languages. 5/ https://t.co/Yb1p4av2k7', 'As we introduce several components, we also include ablations of each of our decisions, and further analysis on how RAF handles scenarios without training support, OOD, retrieval vs filling, and producing N-best lists. 6/', 'Due to our metric learning centric approach, RAF enables direct visualizations of the model internals through inspecting projections of the shared retrieval space. This enables us to further understand domain development process and see how adding data refines the space 7/ https://t.co/ONt7oYu1gj', 'And how we can leverage this space to understand model errors more effectively. 8/ https://t.co/xVwPGl0xuW', 'This is joint work alongside amazing co-authors: @shreydesai, Anchit Gupta, Ali Elkahky, @vl4g,  \n@AlexZotov, and @Ahhegazy77 ! 9/']",https://arxiv.org/abs/2202.00901,"Task-oriented semantic parsing models have achieved strong results in recent years, but unfortunately do not strike an appealing balance between model size, runtime latency, and cross-domain generalizability. We tackle this problem by introducing scenario-based semantic parsing: a variant of the original task which first requires disambiguating an utterance's ""scenario"" (an intent-slot template with variable leaf spans) before generating its frame, complete with ontology and utterance tokens. This formulation enables us to isolate coarse-grained and fine-grained aspects of the task, each of which we solve with off-the-shelf neural modules, also optimizing for the axes outlined above. Concretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a retrieval module which ranks the best scenario given an utterance and (2) a filling module which imputes spans into the scenario to create the frame. Our model is modular, differentiable, interpretable, and allows us to garner extra supervision from scenarios. RAF achieves strong results in high-resource, low-resource, and multilingual settings, outperforming recent approaches by wide margins despite, using base pre-trained encoders, small sequence lengths, and parallel decoding. ",Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing
197,1489255149432307715,1225430594504531973,Andreas Haahr Larsen,"['Happy to present our review on how to use MD to find specific interactions between peripheral membrane proteins and lipids. Preprint now on arXiv: \n<LINK>\nWe hope it is useful to some of you\nWith Laura John, @RobinCorey1, @mark_sansom', '@RobinCorey1 @mark_sansom ... including a lot on free energy calculations for the interactions...']",https://arxiv.org/abs/2202.00492,"Peripheral membrane proteins can reversibly and specifically bind to biological membranes to carry out functions such as cell signalling, enzymatic activity, or membrane remodelling. Structures of these proteins and of their lipid-binding domains are typically solved in a soluble form, sometimes with a lipid or lipid headgroup at the binding site. To provide a detailed molecular view of peripheral membrane protein interactions with the membrane, computational methods such as molecular dynamics (MD) simulations can be applied. Here, we outline recent attempts to characterise these binding interactions, focusing on both intracellular proteins such as PIP-binding domains, and on extracellular proteins such as glycolipid-binding bacterial exotoxins. We compare methods to identify and analyse lipid binding sites from simulation data. We highlight recent work characterising the energetics of these interactions using free energy calculations. We describe how improvements in methodologies and computing power will help MD simulations to continue to contribute to this field in the future. ","Specific interactions of peripheral membrane proteins with lipids: what
  can molecular simulations show us?"
198,1488950496727429121,882511862524465152,Aleksander Madry,"['Can we cast ML predictions as simple functions of individual training inputs? Yes! w/ @andrew_ilyas @smsampark @logan_engstrom @gpoleclerc, we introduce datamodels (<LINK>), a framework to study how data + algs -&gt; predictions.  Blog: <LINK> (1/6) <LINK>', 'We trained *hundreds of thousands* of models on random subsets of computer vision datasets using our library FFCV (https://t.co/QWUdL50g9i). We then used this data to fit *linear* models that can successfully predict model outputs. (2/6) https://t.co/ITedgWiCQi', 'We then use datamodels to: (1) Predict data counterfactuals (i.e., what if I remove subset R from the train set?) and find that you can flip model predictions for *over 50%* of test examples on CIFAR-10 by removing only 200 (target-specific) training images (0.4% of total)  (3/6) https://t.co/SbkaAGJffl', '(2) Identify similar training examples to a given test example and use this to find (a non-trivial amount of) train-test leakage in both CIFAR-10 and FMoW datasets. [Some (of many) examples images from the same scene are below.] (4/6) https://t.co/919Ls0xlyJ', '(3) Use datamodels as *feature representation* and employ these embeddings to perform clustering that pinpoints *model-driven* data subpopulations (see a PCA-driven example below) and unlock a suite of graph algorithms for analyzing complex datasets. (5/6) https://t.co/Grbfy3ghc5', 'Overall, datamodels are a versatile tool for *model-driven* understanding of the data. The fact that a simple linear model can predict outputs of end-to-end model training suggests there is much to be learned about generalization of DNNs here too. (See Sec. 7 of the paper.) (6/6)']",http://arxiv.org/abs/2202.00622,"We present a conceptual framework, datamodeling, for analyzing the behavior of a model class in terms of the training data. For any fixed ""target"" example $x$, training set $S$, and learning algorithm, a datamodel is a parameterized function $2^S \to \mathbb{R}$ that for any subset of $S' \subset S$ -- using only information about which examples of $S$ are contained in $S'$ -- predicts the outcome of training a model on $S'$ and evaluating on $x$. Despite the potential complexity of the underlying process being approximated (e.g., end-to-end training and evaluation of deep neural networks), we show that even simple linear datamodels can successfully predict model outputs. We then demonstrate that datamodels give rise to a variety of applications, such as: accurately predicting the effect of dataset counterfactuals; identifying brittle predictions; finding semantically similar examples; quantifying train-test leakage; and embedding data into a well-behaved and feature-rich representation space. Data for this paper (including pre-computed datamodels as well as raw predictions from four million trained deep neural networks) is available at this https URL . ",Datamodels: Predicting Predictions from Training Data
199,1488880449480966146,1481400147221299201,Livio Nicola Carenza,['Check out our new paper on the relevant symmetries in epithelia! \nWe find that both hexatic and nematic order is needed to describe epithelial tissues with the former dominant at the small scales and the latter at larger ones.@LeidenPhysics\n <LINK>'],https://arxiv.org/abs/2202.00668,"Biological processes such as embryogenesis, wound healing and cancer progression, crucially rely on the ability of epithelial cells to coordinate their mechanical activity over length scales order of magnitudes larger than the typical cellular size. While regulated by signalling pathways, such as YAP (yes-associated protein), MAPK (mitogen-activated protein kinase) and Wnt, this behavior is believed to additionally hinge on a minimal toolkit of physical mechanisms, of which liquid crystal order is the most promising candidat. Yet, experimental and theoretical studies have given so far inconsistent results in this respect: whereas nematic order is often invoked in the interpretation of experimental data, computational models have instead suggested that hexatic order could in fact emerge in the biologically relevant region of parameter space. In this article we resolve this dilemma. Using a combination of in vitro experiments on Madin-Darby canine kidney cells (MDCK), numerical simulations and analytical work, we demonstrate that both nematic and hexatic order is in fact present in epithelial layers, with the former being dominant at large length scales and the latter at small length scales. In MDCK GII cells on uncoated glass, these different types of liquid crystal order crossover at $34 \mu$m, corresponding approximatively to clusters of $21$ cells. Our work sheds light on the emergent organization of living matter, provides a new set of tools for analyzing the structure of epithelia and paves the way toward a comprehensive and predictive mesoscopic theory of tissues. ",Epithelia are multiscale active liquid crystals
200,1488812546165514240,1432777256682860547,Lucas Siemons,"['I always felt like the bee‚Äôs knees researching biology to gain medical insights, but why do it if we can‚Äôt live on Earth? Huge respect for Nick‚Äôs study looking for green alternatives to our energy needs. Congats on your first pre-print! <LINK> #BroScience']",https://arxiv.org/abs/2202.00337,"Exchanging hydrophobic alkyl-based side chains to hydrophilic glycol-based side chains is a widely adopted method for improving mixed-transport device performance, despite the impact on solid state packing and polymer-electrolyte interactions being poorly understood. Presented here is a Molecular Dynamics (MD) force field for modelling alkoxylated and glycolated polythiophenes. The force field is validated against known packing motifs for their monomer crystals. MD simulations, coupled with X-ray Diffraction (XRD), show that alkoxylated polythiophenes will pack with a `tilted stack' and straight interdigitating side chains, whilst their glycolated counterpart will pack with a `deflected stack' and an s-bend side chain configuration. MD simulations reveal water penetration pathways into the alkoxylated and glycolated crystals - through the {\pi}-stack and through the lamellar stack respectively. Finally, the two distinct ways tri-ethylene glycol polymers can bind to cations are revealed, showing the formation of a meta-stable single bound state, or an energetically deep double bound state, both with a strong side chain length dependance. The minimum energy pathways for the formation of the chelates are identified, showing the physical process through which cations can bind to one or two side chains of a glycolated polythiophene, with consequences for ion transport in bithiophene semiconductors. ","Impact of Side Chain Hydrophilicity on Packing, Swelling and Ion
  Interactions in Oxy-bithiophene Semiconductors"
201,1500847572360970246,762359343656361984,James Zou,"[""Worse #AI can ‚û°Ô∏è better human decisions. \n\nOur new paper shows this in 1000s of human-AI experiments: making AI's output to users overconfident can improve human decisions &gt; using calibrated AI. \n\nWe also propose framework for training AI to opt human perf <LINK> <LINK>"", 'Intuition is that human users are not calibrated so AI needs to un-calibrate itself to help more. \n\nThis study shows the importance of training AI to opt for the overall human perf, rather than standard AI metrics (esp when AI is one part of a team).  \n\nGreat work led by Kailas!']",https://arxiv.org/abs/2202.05983,"In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of ""confidence"" that the human can use to calibrate how much they depend on or trust the advice. In this paper, we demonstrate that presenting AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first learn a model for how humans incorporate AI advice using data from thousands of human interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks -- dealing with images, text and tabular data -- involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of and a framework for jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone. ",Uncalibrated Models Can Improve Human-AI Collaboration
202,1496494302029041665,1091045790242533376,Mariya Toneva,"['Why are huge parts of üß† predicted well by the same DNN representation? Do these parts really process similar things?\n\nWe propose a causal framework to infer the effect of complex stimuli on multiple brain zones.\n\n<LINK>\nTo appear @ Causal Learning &amp; Reasoning\nüßµ <LINK>', 'This work was a true team effort together with @Jenn_L_Williams, Anand Bollu, @chrodan, and Leila Wehbe.\n\nWe are excited to present this work @CLeaR_2022!\n\n2/n', 'To map info processing in the brain, researchers often use encoding models to evaluate if stimulus properties predict brain data.\n\nWe can further increase our understanding of info processing by asking whether stimulus properties affect multiple brain zones in the same way.\n\n3/n', 'This is not easy to do with an encoding model. \n\nIn settings with complex stimuli (including naturalistic stimuli), it becomes hard to infer if the stimulus affects different zones in the same way as there are multivariate and often high-dimensional aspects to the stimulus.\n\n4/n', 'If the stimulus-representation contains multiple types of information, different zones can be predicted using different types of information.\n\nIn fact, this is why it remains an open question if Language Network ROIs are affected in the same way by naturalistic language.\n\n5/n', ""Especially important since it's increasingly popular to use naturalistic stimuli @LibertySays @alex_ander @samnastase \n \n&amp; DNN representations in encoding models @shaileeejain @alex_ander @c_caucheteux @jrking0 @martin_schrimpf @neuranna @ev_fedorenko @ArielYGoldstein\n+ more\n\n6/n"", 'To infer whether a stimulus affects two brain zones in the same way, we present a framework that includes two new metrics.\n\nWe validate this both in simulation and in 2 naturalistic fMRI datasets. Many thanks to @HumanConnectome and @CNeuromod for sharing the datasets!\n\n7/n', 'Metric #1, zone generalization, captures how similarly 2 zones are affected by stimulus properties in a stim-representation.\n\nMetric #2, zone residuals, captures any stimulus effect that is not shared between 2 zones (even due to stim. properties not in stim-representation).\n\n8/n', 'The simulations show that these metrics provide new insights beyond current brain mapping techniques.\n\nThe fMRI results are consistent across the 2 naturalistic datasets, which are acquired from different subjects, labs, stimuli and scanning parameters.\n\n9/n', 'Example inferences for language ROI:\n- left PTG &amp; AG affected differently by stim. properties captured by ELMo\n- left ATG &amp; PTG affected similarly, but ELMo missing some properties that affect the zones differently\n- left &amp; right PTG affected similarly by stim. properties\n\n10/n', ""If you'd like to explore this framework, you can get started here:\nhttps://t.co/HIb67vF57o\n \nLots more to do to delve deeper into the similarity between representations in üß† &amp; üñ•Ô∏è! I‚Äôm excited to use this framework to make progress towards this important question.\n \nn/n""]",https://arxiv.org/abs/2202.10376,"To study information processing in the brain, neuroscientists manipulate experimental stimuli while recording participant brain activity. They can then use encoding models to find out which brain ""zone"" (e.g. which region of interest, volume pixel or electrophysiology sensor) is predicted from the stimulus properties. Given the assumptions underlying this setup, when stimulus properties are predictive of the activity in a zone, these properties are understood to cause activity in that zone. In recent years, researchers have used neural networks to construct representations that capture the diverse properties of complex stimuli, such as natural language or natural images. Encoding models built using these high-dimensional representations are often able to significantly predict the activity in large swathes of cortex, suggesting that the activity in all these brain zones is caused by stimulus properties captured in the representation. It is then natural to ask: ""Is the activity in these different brain zones caused by the stimulus properties in the same way?"" In neuroscientific terms, this corresponds to asking if these different zones process the stimulus properties in the same way. Here, we propose a new framework that enables researchers to ask if the properties of a stimulus affect two brain zones in the same way. We use simulated data and two real fMRI datasets with complex naturalistic stimuli to show that our framework enables us to make such inferences. Our inferences are strikingly consistent between the two datasets, indicating that the proposed framework is a promising new tool for neuroscientists to understand how information is processed in the brain. ",Same Cause; Different Effects in the Brain
203,1494437438856572932,1863618842,Jean Kaddour,"['Flat minima often generalize better than sharp ones due to robustness against loss shifts between train and test set. What‚Äôs the best way to find them? We compare two popular methods, SWA and SAM, across 42 deep learning tasks (CV, NLP, GRL): <LINK>\n1/7 <LINK>', 'To gain some intuition, let‚Äôs compare them on a toy surface with one sharp valley and a flatter one. Training/test losses are blue/red and slightly shifted. We add Gaussian noise to the gradients to simulate mini-batch stochasticity. 2/7 https://t.co/whijrKliw5', 'SWA averages SGD iterates to produce a solution pulled towards flatter regions inside a valley.\n‚úÖ it finds a flatter region within the SGD valley.\n‚ùå it is limited to the valley found by SGD.\n3/7 https://t.co/erpxSwmiw1', 'SAM smoothes the loss landscape by minimizing the maximum loss around a neighborhood of the current iterate.\n‚úÖ it finds a flatter valley.\n‚ùå inside the flatter valley, it oscillates around the minimum, just like SGD. 4/7 https://t.co/8ful8zTjXF', 'These observations lead to a simple idea: averaging SAM iterates (WASAM) should further improve generalization. In our paper, we verify that WASAM improves over either flat-minima approach in 39 out of 42 cases. 5/7 https://t.co/fb4E3yH7hc', 'Our large-scale study consists of image classification, self-supervised learning, open-domain question answering, natural language understanding, and node/graph/link property prediction tasks, using CNN, ViT, MLP-Mixer, FiD, RoBERTa, and GNN architectures. 6/7 https://t.co/CObies6ml1', 'Joint work @ai_ucl with @likicode, Ricardo Silva, and Matt J. Kusner! 7/7', '@CsabaSzepesvari Thanks a lot for your question! Most recently, Petzka et al theoretically connect a notion of representative data with a notion of feature robustness and a novel measure of the flatness of the loss surface.""https://t.co/8pYxoMQVwT @MichaelKamp7 @LinaraAdylova @CSminchisescu 1/3', '@CsabaSzepesvari @MichaelKamp7 @LinaraAdylova @CSminchisescu The authors of SAM, @Foret_p @TheGradient @bneyshabur, state a PAC-Bayes generalization bound based in terms of neighborhood-wise training loss sharpness: \nhttps://t.co/FZR4HM3Zvg\n@KDziugaite and @roydanroy discuss similar connections in https://t.co/fqXx1CR9Io 2/3', '@CsabaSzepesvari @MichaelKamp7 @LinaraAdylova @CSminchisescu @Foret_p @TheGradient @bneyshabur @KDziugaite @roydanroy Tsuzuku et al. provide ""PAC-Bayesian generalization error bounds using scale-invariant loss curvature metrics"" and ""a novel normalized loss curvature metric that has a close connection to the bounds"".\nhttps://t.co/jnAOPe1MpX\n3/3', '@MateuszOnAI @CsabaSzepesvari Great find, thank you!! This paper suggests that classical flatness measures are not invariant to linear reparameterizations and hence, cannot theoretically be related to generalization. However, there are also other connections: https://t.co/m35nZKtenZ', '@PreetumNakkiran @jeremyphoward @CsabaSzepesvari What is missing in our empirical study and other previous work (SWA &amp; SAM papers, etc.) to convince you that it is useful and practical?']",https://arxiv.org/abs/2202.00661,"For training neural networks, flat-minima optimizers that seek to find parameters in neighborhoods having uniformly low loss (flat minima) have been shown to improve upon stochastic and adaptive gradient-based methods. Two methods for finding flat minima stand out: 1. Averaging methods (i.e., Stochastic Weight Averaging, SWA), and 2. Minimax methods (i.e., Sharpness Aware Minimization, SAM). However, despite similar motivations, there has been limited investigation into their properties and no comprehensive comparison between them. In this work, we investigate the loss surfaces from a systematic benchmarking of these approaches across computer vision, natural language processing, and graph learning tasks. The results lead to a simple hypothesis: since both approaches find different flat solutions, combining them should improve generalization even further. We verify this improves over either flat-minima approach in 39 out of 42 cases. When it does not, we investigate potential reasons. We hope our results across image, graph, and text data will help researchers to improve deep learning optimizers, and practitioners to pinpoint the optimizer for the problem at hand. ",Questions for Flat-Minima Optimization of Modern Neural Networks
204,1494361834932953103,2701532126,Berivan Isik,"['Check out our new paper titled ‚ÄúLearning under Storage and Privacy Constraints‚Äù. We propose a novel data pre-processing framework, LCoN, which simultaneously boosts data efficiency, privacy, accuracy, and robustness. 1/4\n\n<LINK>\n\n#compression #privacy #learning <LINK>', 'Our framework comprises noise injection followed by lossy compression. The noise injection step prevents user information from being leaked during learning, while lossy compression reduces the cost of storing/transmitting the data. 2/4', 'We show that, when appropriately matching the lossy compression to the distribution of the added noise, the compressed examples converge, in distribution, to that of the noise-free training data. 3/4', 'With this, we guarantee that the utility of the data for learning is essentially maintained while reducing storage and privacy leakage by quantifiable amounts. The improved robustness against adversarial data is a welcome additional feature we observed empirically. 4/4']",https://arxiv.org/abs/2202.02892,"Storage-efficient privacy-guaranteed learning is crucial due to enormous amounts of sensitive user data required for increasingly many learning tasks. We propose a framework for reducing the storage cost while at the same time providing privacy guarantees, without essential loss in the utility of the data for learning. Our method comprises noise injection followed by lossy compression. We show that, when appropriately matching the lossy compression to the distribution of the added noise, the compressed examples converge, in distribution, to that of the noise-free training data. In this sense, the utility of the data for learning is essentially maintained, while reducing storage and privacy leakage by quantifiable amounts. We present experimental results on the CelebA dataset for gender classification and find that our suggested pipeline delivers in practice on the promise of the theory: the individuals in the images are unrecognizable (or less recognizable, depending on the noise level), overall storage of the data is substantially reduced, with no essential loss of the classification accuracy. As an added bonus, our experiments suggest that our method yields a substantial boost to robustness in the face of adversarial test data. ",Learning under Storage and Privacy Constraints
205,1493636204645994509,560983300,Briland Hitaj,"['Hello World! Happy to share with you our recent work: Tattooed, where we propose a new strategy to watermark deep neural networks! <LINK> <LINK>']",https://arxiv.org/abs/2202.06091,"The proliferation of deep learning applications in several areas has led to the rapid adoption of such solutions from an ever-growing number of institutions and companies. These entities' deep neural network (DNN) models are often trained on proprietary data. They require powerful computational resources, with the resulting DNN models being incorporated in the company's work pipeline or provided as a service. Being trained on proprietary information, these models provide a competitive edge for the owner company. At the same time, these models can be attractive to competitors (or malicious entities), which can employ state-of-the-art security attacks to obtain and use these models for their benefit. As these attacks are hard to prevent, it becomes imperative to have mechanisms that enable an affected entity to verify the ownership of its DNN with high confidence. This paper presents TATTOOED, a robust and efficient DNN watermarking technique based on spread-spectrum channel coding. TATTOOED has a negligible effect on the performance of the DNN model and is robust against several state-of-the-art mechanisms used to remove watermarks from DNNs. Our results show that TATTOOED is robust to such removal techniques even in extreme scenarios. For example, if the removal techniques such as fine-tuning and parameter pruning change as much as 99% of the model parameters, the TATTOOED watermark is still present in full in the DNN model and ensures ownership verification. ","TATTOOED: A Robust Deep Neural Network Watermarking Scheme based on
  Spread-Spectrum Channel Coding"
206,1493503680079106048,776107104180600832,Reza Shokri,"['We study the question ""What Does it Mean for a Language Model to Preserve Privacy?"" <LINK> in a great collaboration with wonderful Hannah, Katherine, Fatemeh, and Florian @Hannah_Aught @katherine1ee @limufar @florian_tramer We discuss the mismatch between the 1/3 <LINK>', 'narrow assumptions made by data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for 2/3', 'language models. We conclude that ""data protection is not equivalent to privacy protection for natural language\ndata"", and language models should be trained on text data which was explicitly produced for public use. 3/3']",https://arxiv.org/abs/2202.05520,"Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use. ",What Does it Mean for a Language Model to Preserve Privacy?
