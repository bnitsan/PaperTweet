,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,820391792361308164,2956121356,Russ Salakhutdinov,"['New paper on ""Using Knowledge Graphs for Image Classification"" under few-shot setting, with K. Marino and A. Gupta.\n<LINK> <LINK>']",https://arxiv.org/abs/1612.04844,"One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification. ",The More You Know: Using Knowledge Graphs for Image Classification
1,815811124033515520,3808273878,Andrew Lehmann,"[""New paper on dusty shocks listed in today's astro.ph <LINK>""]",https://arxiv.org/abs/1612.09383,"The key role that dust plays in the interstellar medium has motivated the development of numerical codes designed to study the coupled evolution of dust and gas in systems such as turbulent molecular clouds and protoplanetary discs. Drift between dust and gas has proven to be important as well as numerically challenging. We provide simple benchmarking problems for dusty gas codes by numerically solving the two-fluid dust-gas equations for steady, plane-parallel shock waves. The two distinct shock solutions to these equations allow a numerical code to test different forms of drag between the two fluids, the strength of that drag and the dust to gas ratio. We also provide an astrophysical application of J-type dust-gas shocks to studying the structure of accretion shocks onto protoplanetary discs. We find that two-fluid effects are most important for grains larger than 1 um, and that the peak dust temperature within an accretion shock provides a signature of the dust-to-gas ratio of the infalling material. ","Two-fluid dusty shocks: simple benchmarking problems and applications to
  protoplanetary discs"
2,814226535884488705,529438517,Aaron Clauset,"['New paper! When in a career are scientists most productive? Answers within, w/ @samfway allie morgan @DanLarremore <LINK> <LINK>']",https://arxiv.org/abs/1612.08228,"A scientist may publish tens or hundreds of papers over a career, but these contributions are not evenly spaced in time. Sixty years of studies on career productivity patterns in a variety of fields suggest an intuitive and universal pattern: productivity tends to rise rapidly to an early peak and then gradually declines. Here, we test the universality of this conventional narrative by analyzing the structures of individual faculty productivity time series, constructed from over 200,000 publications and matched with hiring data for 2453 tenure-track faculty in all 205 Ph.D-granting computer science departments in the U.S. and Canada. Unlike prior studies, which considered only some faculty or some institutions, or lacked common career reference points, here we combine a large bibliographic dataset with comprehensive information on career transitions that covers an entire field of study. We show that the conventional narrative confidently describes only one fifth of faculty, regardless of department prestige or researcher gender, and the remaining four fifths of faculty exhibit a rich diversity of productivity patterns. To explain this diversity, we introduce a simple model of productivity trajectories, and explore correlations between its parameters and researcher covariates, showing that departmental prestige predicts overall individual productivity and the timing of the transition from first- to last-author publications. These results demonstrate the unpredictability of productivity over time, and open the door for new efforts to understand how environmental and individual factors shape scientific productivity. ","The misleading narrative of the canonical faculty productivity
  trajectory"
3,813632727178055680,320769143,Amin Beheshti,['Are you interested in open Data Curation APIs to turn your raw data into contextualized data? Read our new paper..\n\n<LINK>'],https://arxiv.org/abs/1612.03277,"Understanding and analyzing big data is firmly recognized as a powerful and strategic priority. For deeper interpretation of and better intelligence with big data, it is important to transform raw data (unstructured, semi-structured and structured data sources, e.g., text, video, image data sets) into curated data: contextualized data and knowledge that is maintained and made available for use by end-users and applications. In particular, data curation acts as the glue between raw data and analytics, providing an abstraction layer that relieves users from time consuming, tedious and error prone curation tasks. In this context, the data curation process becomes a vital analytics asset for increasing added value and insights. In this paper, we identify and implement a set of curation APIs and make them available (on GitHub) to researchers and developers to assist them transforming their raw data into curated data. The curation APIs enable developers to easily add features - such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string, number, date and time data; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data - into their applications. ",Data Curation APIs
4,813317069307604992,40639812,Colin Cotter,"['New paper submitted with @attmcrae and Chris Budd on optimally transported mesh adaptivity.<LINK>', 'We had a lot of fun finding a Monge-Ampere-like equation on the sphere and solving it numerically with finite element methods.', 'All implemented using @firedrakeFEM of course.']",https://arxiv.org/abs/1612.08077,"In moving mesh methods, the underlying mesh is dynamically adapted without changing the connectivity of the mesh. We specifically consider the generation of meshes which are adapted to a scalar monitor function through equidistribution. Together with an optimal transport condition, this leads to a Monge-Amp\`ere equation for a scalar mesh potential. We adapt an existing finite element scheme for the standard Monge-Amp\`ere equation to this mesh generation problem; this is a mixed finite element scheme, in which an extra discrete variable is introduced to represent the Hessian matrix of second derivatives. The problem we consider has additional nonlinearities over the basic Monge-Amp\`ere equation due to the implicit dependence of the monitor function on the resulting mesh. We also derive the equivalent Monge-Amp\`ere-like equation for generating meshes on the sphere. The finite element scheme is extended to the sphere, and we provide numerical examples. All numerical experiments are performed using the open-source finite element framework Firedrake. ","Optimal-transport-based mesh adaptivity on the plane and sphere using
  finite elements"
5,812347233286791169,333069274,sukanya chakrabarti,['check out our new paper on dwarf galaxies around the Milky Way: <LINK>'],https://arxiv.org/abs/1612.07325,"The dwarf galaxies around the Milky Way are distributed in a so-called vast polar structure (VPOS) that may be in conflict with Lambda CDM simulations. Here, we seek to determine if the VPOS poses a serious challenge to the Lambda cold dark matter paradigm on galactic scales. Specifically, we investigate if the VPOS remains coherent as a function of time. Using the measured Hubble Space Telescope (HST) proper motions and associated uncertainties, we integrate the orbits of the classical Milky Way satellites backwards in time and find that the structure disperses well before a dynamical time. We also examine in particular Leo I and Leo II using their most recent proper motion data, both of which have extreme kinematic properties, but these satellites do not appear to drive the polar fit that is seen at the present day. We have studied the effect of the uncertainties on the HST proper motions on the coherence of the VPOS as a function of time. We find that 8 of the 11 classical dwarfs have reliable proper motions; for these 8, the VPOS also loses significance in less than a dynamical time, indicating that the VPOS is not a dynamically stable structure. Obtaining more accurate proper motion measurements of Ursa Minor, Sculptor, and Carina would bolster these conclusions. ","Is the vast polar structure of dwarf galaxies a serious problem for
  lambda cold dark matter?"
6,812333372148215811,3199605543,Afonso S. Bandeira,['New paper on Statistical limits of spiked tensor models available at <LINK>'],https://arxiv.org/abs/1612.07728,"We study the statistical limits of both detecting and estimating a rank-one deformation of a symmetric random Gaussian tensor. We establish upper and lower bounds on the critical signal-to-noise ratio, under a variety of priors for the planted vector: (i) a uniformly sampled unit vector, (ii) i.i.d. $\pm 1$ entries, and (iii) a sparse vector where a constant fraction $\rho$ of entries are i.i.d. $\pm 1$ and the rest are zero. For each of these cases, our upper and lower bounds match up to a $1+o(1)$ factor as the order $d$ of the tensor becomes large. For sparse signals (iii), our bounds are also asymptotically tight in the sparse limit $\rho \to 0$ for any fixed $d$ (including the $d=2$ case of sparse PCA). Our upper bounds for (i) demonstrate a phenomenon reminiscent of the work of Baik, Ben Arous and P\'ech\'e: an `eigenvalue' of a perturbed tensor emerges from the bulk at a strictly lower signal-to-noise ratio than when the perturbation itself exceeds the bulk; we quantify the size of this effect. We also provide some general results for larger classes of priors. In particular, the large $d$ asymptotics of the threshold location differs between problems with discrete priors versus continuous priors. Finally, for priors (i) and (ii) we carry out the replica prediction from statistical physics, which is conjectured to give the exact information-theoretic threshold for any fixed $d$. Of independent interest, we introduce a new improvement to the second moment method for contiguity, on which our lower bounds are based. Our technique conditions away from rare `bad' events that depend on interactions between the signal and noise. This enables us to close $\sqrt{2}$-factor gaps present in several previous works. ",Statistical limits of spiked tensor models
7,812332688061399041,781309796741947396,EEMS Group @ MIT (PI: Vivienne Sze),['New paper on “Hardware for Machine Learning: Challenges and Opportunities” available on arXiv: <LINK>'],https://arxiv.org/abs/1612.07625,"Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors). ",Hardware for Machine Learning: Challenges and Opportunities
8,811973314113654785,26716739,Mattias Villani,"[""New paper 'Bayesian Non-Central Chi Regression For Neuroimaging' with @wandedob <LINK> <LINK>""]",https://arxiv.org/abs/1612.07034,"We propose a regression model for non-central $\chi$ (NC-$\chi$) distributed functional magnetic resonance imaging (fMRI) and diffusion weighted imaging (DWI) data, with the heteroscedastic Rician regression model as a prominent special case. The model allows both parameters in the NC-$\chi$ distribution to be linked to explanatory variables, with the relevant covariates automatically chosen by Bayesian variable selection. A highly efficient Markov chain Monte Carlo (MCMC) algorithm is proposed for simulating from the joint Bayesian posterior distribution of all model parameters and the binary covariate selection indicators. Simulated fMRI data is used to demonstrate that the Rician model is able to localize brain activity much more accurately than the traditionally used Gaussian model at low signal-to-noise ratios. Using a diffusion dataset from the Human Connectome Project, it is also shown that the commonly used approximate Gaussian noise model underestimates the mean diffusivity (MD) and the fractional anisotropy (FA) in the single-diffusion tensor model compared to the theoretically correct Rician model. ",Bayesian Non-Central Chi Regression For Neuroimaging
9,811847982337249280,101898541,Chao-Yang Lu,['put a new paper on arxiv on my 34th birthday <LINK> <LINK>'],https://arxiv.org/abs/1612.06956,"Boson sampling is considered as a strong candidate to demonstrate the quantum computational supremacy over classical computers. However, previous proof-of-principle experiments suffered from small photon number and low sampling rates owing to the inefficiencies of the single-photon sources and multi-port optical interferometers. Here, we develop two central components for high-performance boson sampling: robust multi-photon interferometers with 0.99 transmission rate, and actively demultiplexed single-photon sources from a quantum-dot-micropillar with simultaneously high efficiency, purity and indistinguishability. We implement and validate 3-, 4-, and 5-photon boson sampling, and achieve sampling rates of 4.96 kHz, 151 Hz, and 4 Hz, respectively, which are over 24,000 times faster than the previous experiments, and over 220 times faster than obtaining one sample through calculating the matrices permanent using the first electronic computer (ENIAC) and transistorized computer (TRADIC) in the human history. Our architecture is feasible to be scaled up to larger number of photons and with higher rate to race against classical computers, and might provide experimental evidence against the Extended Church-Turing Thesis. ",Multi-photon boson-sampling machines beating early classical computers
10,811541700950786048,3099782033,Lisa Steinborn,['The paper about our new cosmological web portal is now on ArXiv: <LINK>'],https://arxiv.org/abs/1612.06380v1,"This article describes a virtual observatory hosting a web portal for accessing and sharing the output of large, cosmological, hydro-dynamical simulations with a broad scientific community. It also allows users to receive related scientific data products by directly processing the raw simulation data on a remote computing cluster. The virtual observatory is a multi-layer structure: a web portal, a job control layer, a computing cluster and a HPC storage system. The outer layer enables users to choose an object from the simulations. Objects can be selected by visually inspecting 2D maps of the simulation data, by performing highly compounded and elaborated queries or graphically from plotting arbitrary combinations of properties. The user can apply several services to a chosen object. These services allow users to run analysis tools on the raw simulation data. The job control layer is responsible for handling and performing the analysis jobs, which are executed on a computing cluster. The inner most layer is formed by a HPC storage system which host the large, raw simulation data. The virtual observatory provides the following services for the users: (I) ClusterInspect visualizes properties of member galaxies of a selected galaxy cluster; (II) SimCut returns the raw data of a sub-volume around a selected object from a simulation, containing all the original, hydro-dynamical quantities; (III) Smac creates idealised 2D maps of various, physical quantities and observable of a selected object; (IV) Phox generates virtual X-ray observations with specifications of various current and upcoming instruments. ","] An online theoretical virtual observatory for hydrodynamical,
  cosmological simulations"
11,811218093607452672,14386507,Giulio Ruffini,['New Luminous paper on the making on Kolmogorov Complexity &amp; ML: <LINK> … Check this out too: <LINK>'],https://arxiv.org/abs/1612.05627,"I aim to show that models, classification or generating functions, invariances and datasets are algorithmically equivalent concepts once properly defined, and provide some concrete examples of them. I then show that a) neural networks (NNs) of different kinds can be seen to implement models, b) that perturbations of inputs and nodes in NNs trained to optimally implement simple models propagate strongly, c) that there is a framework in which recurrent, deep and shallow networks can be seen to fall into a descriptive power hierarchy in agreement with notions from the theory of recursive functions. The motivation for these definitions and following analysis lies in the context of cognitive neuroscience, and in particular in Ruffini (2016), where the concept of model is used extensively, as is the concept of algorithmic complexity. ","Models, networks and algorithmic complexity"
12,811213735004082176,628967454,Bruce Desmarais,"['new paper on learning from prediction with Skyler Cranmer, forthcoming @polanalysis, preprints <LINK> #PredictiveAnalytics']",https://arxiv.org/abs/1612.05844,"The large majority of inferences drawn in empirical political research follow from model-based associations (e.g. regression). Here, we articulate the benefits of predictive modeling as a complement to this approach. Predictive models aim to specify a probabilistic model that provides a good fit to testing data that were not used to estimate the model's parameters. Our goals are threefold. First, we review the central benefits of this under-utilized approach from a perspective uncommon in the existing literature: we focus on how predictive modeling can be used to complement and augment standard associational analyses. Second, we advance the state of the literature by laying out a simple set of benchmark predictive criteria. Third, we illustrate our approach through a detailed application to the prediction of interstate conflict. ",What can we Learn from Predictive Modeling?
13,811047737395318784,304654146,Jeremy Sumner,['New paper on efficient calculation of maximum likelihood distances for genome rearrangement models. @arkfrancis \n<LINK>'],http://arxiv.org/abs/1612.06035,"In the context of bacteria and models of their evolution under genome rearrangement, we explore a novel application of group representation theory to the inference of evolutionary history. Our contribution is to show, in a very general maximum likelihood setting, how to use elementary matrix algebra to sidestep intractable combinatorial computations and convert the problem into one of eigenvalue estimation amenable to standard numerical approximation techniques. ","A representation-theoretic approach to the calculation of evolutionary
  distance in bacteria"
14,811039425123651586,460069521,Andrew Francis,"['New paper out with @jezlurch and Peter Jarvis, using representation theory to calc inversion distance in bacteria!\n<LINK>', 'Very nice to be able to use the group algebra in algebraic biology!  And some irreducible characters, thanks to power work by @jezlurch']",https://arxiv.org/abs/1612.06035,"In the context of bacteria and models of their evolution under genome rearrangement, we explore a novel application of group representation theory to the inference of evolutionary history. Our contribution is to show, in a very general maximum likelihood setting, how to use elementary matrix algebra to sidestep intractable combinatorial computations and convert the problem into one of eigenvalue estimation amenable to standard numerical approximation techniques. ","A representation-theoretic approach to the calculation of evolutionary
  distance in bacteria"
15,810983536618508290,2294472165,Edgar Martínez-Moro,['My new paper at #Arxiv  On Counting Subring-Subcodes of Free Linear Codes Over Finite Principal  Ideal Rings <LINK>'],https://arxiv.org/abs/1612.02213,"Let $R$ be a finite principal ideal ring and $S$ the Galois extension of $R$ of degree $m$. For $k$ and $k_0$, positive integers we determine the number of free $S$-linear codes $B$ of length $l$ with the property $k = rank_S(B)$ and $k_0 = rank_R (B\cap R^l)$. This corrects a wrong result which was given in the case of finite fields. ","On Counting Subring-Subcodes of Free Linear Codes Over Finite Principal
  Ideal Rings"
16,809807737873797121,21611239,Sean Carroll,"['Eternal inflation relies on ""quantum fluctuations,"" by which you really should mean ""decoherence."" New paper: <LINK>']",https://arxiv.org/abs/1612.04894,"Slow-roll inflation can become eternal if the quantum variance of the inflaton field around its slowly rolling classical trajectory is converted into a distribution of classical spacetimes inflating at different rates, and if the variance is large enough compared to the rate of classical rolling that the probability of an increased rate of expansion is sufficiently high. Both of these criteria depend sensitively on whether and how perturbation modes of the inflaton interact and decohere. Decoherence is inevitable as a result of gravitationally-sourced interactions whose strength are proportional to the slow-roll parameters. However, the weakness of these interactions means that decoherence is typically delayed until several Hubble times after modes grow beyond the Hubble scale. We present perturbative evidence that decoherence of long-wavelength inflaton modes indeed leads to an ensemble of classical spacetimes with differing cosmological evolutions. We introduce the notion of per-branch observables---expectation values with respect to the different decohered branches of the wave function---and show that the evolution of modes on individual branches varies from branch to branch. Thus single-field slow-roll inflation fulfills the quantum-mechanical criteria required for the validity of the standard picture of eternal inflation. For a given potential, the delayed decoherence can lead to slight quantitative adjustments to the regime in which the inflaton undergoes eternal inflation. ",How Decoherence Affects the Probability of Slow-Roll Eternal Inflation
17,808976512556867584,26004943,Ben Slater,"[""We have a new paper on arXiv - Lowenstein's rule can be overcome in zeolites <LINK>. A challenge to synthetic chemists!""]",https://arxiv.org/abs/1612.04162,"Zeolites, microporous aluminosilicates, are amongst the most widely used catalysts in the petrochemical industry. Zeolite catalytic functionality is coupled to the distribution of tetrahedral alumina (AlO4-) and associated counter-cations throughout the aluminosilicate framework, yet little is definitively known about the factors that govern framework aluminium arrangement. It is generally accepted that all zeolites obey Lowensteins rule of aluminium avoidance and that Al-O-Al linkages are forbidden. Here, we describe the unprecedented screening of aluminium distribution in catalytically active zeolite SSZ-13 in both its protonated and sodium containing forms, H-SSZ-13 and Na-SSZ-13, using density functional theory. We predict violations of Lowensteins rule in high and low silica H-SSZ-13 and other protonated frameworks considered in this investigation, H-LTA, H-RHO and H-ABW. The synthetic realisation of these zeolites could spur the development of new catalytic routes and materials, and the optimisation of existing zeolite catalysts. ","Violations of L\""owensteins rule in zeolites"
18,808609631975178241,2307019063,Jonathan Jogenfors,"['New #quantum paper on the #arXiv: ""High-Visibility Time-Bin Entanglement for Testing Chained Bell  Inequalities""\n<LINK>']",https://arxiv.org/abs/1612.03602,"The violation of Bell's inequality requires a well-designed experiment to validate the result. In experiments using energy-time and time-bin entanglement, initially proposed by Franson in 1989, there is an intrinsic loophole due to the high postselection. To obtain a violation in this type of experiment, a chained Bell inequality must be used. However, the local realism bound requires a high visibility in excess of 94.63 percent in the time-bin entangled state. In this work, we show how such a high visibility can be reached in order to violate a chained Bell inequality with 6, 8 and 10 terms. ","High-Visibility Time-Bin Entanglement for Testing Chained Bell
  Inequalities"
19,808502386830544897,187780472,Blake C. Stacey,['Bittersweet new paper dance <LINK>'],https://arxiv.org/abs/1612.03234,"We reconstruct quantum theory starting from the premise that, as Asher Peres remarked, ""Unperformed experiments have no results."" The tools of modern quantum information theory, and in particular the symmetric informationally complete (SIC) measurements, provide a concise expression of how exactly Peres's dictum holds true. That expression is a constraint on how the probability distributions for outcomes of different, mutually exclusive experiments mesh together, a type of constraint not foreseen in classical thinking. Taking this as our foundational principle, we show how to reconstruct the formalism of quantum theory in finite-dimensional Hilbert spaces. Along the way, we derive a condition for the existence of a d-dimensional SIC. ",Introducing the Qplex: A Novel Arena for Quantum Theory
20,808485076338143232,156804540,Francisco Rodrigues,['Our new paper on arxiv: Complex networks for image segmentation.\n<LINK> <LINK>'],https://arxiv.org/abs/1612.03705,"Image segmentation has many applications which range from machine learning to medical diagnosis. In this paper, we propose a framework for the segmentation of images based on super-pixels and algorithms for community identification in graphs. The super-pixel pre-segmentation step reduces the number of nodes in the graph, rendering the method the ability to process large images. Moreover, community detection algorithms provide more accurate segmentation than traditional approaches, such as those based on spectral graph partition. We also compare our method with two algorithms: a) the graph-based approach by Felzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez. Results have shown that our method provides more precise segmentation and is faster than both of them. ","Segmentation of large images based on super-pixels and community
  detection in graphs"
21,808301311875813377,2546724918,Francesc Vilardell,['New paper published. Discovery of XO-6b: a hot Jupiter transiting a fast rotating F5 star on  an oblique orbit. <LINK>'],https://arxiv.org/abs/1612.02776,"Only a few hot Jupiters are known to orbit around fast rotating stars. These exoplanets are harder to detect and characterize and may be less common than around slow rotators. Here, we report the discovery of the transiting hot Jupiter XO-6b, which orbits a bright, hot, and fast rotating star: V = 10.25, Teff = 6720 +/- 100 K, v sin i = 48 +/- 3 km/s. We detected the planet from its transits using the XO instruments and conducted a follow-up campaign. Because of the fast stellar rotation, radial velocities taken along the orbit do not yield the planet's mass with a high confidence level, but we secure a 3-sigma upper limit Mp < 4.4 MJup. We also obtain high resolution spectroscopic observations of the transit with the SOPHIE spectrograph at the 193-cm telescope of the Observatoire de Haute-Provence and analyze the stellar lines profile by Doppler tomography. The transit is clearly detected in the spectra. The radii measured independently from the tomographic analysis and from the photometric lightcurves are consistent, showing that the object detected by both methods is the same and indeed transits in front of XO-6. We find that XO-6b lies on a prograde and misaligned orbit with a sky-projected obliquity lambda = -20.7 +/- 2.3 deg. The rotation period of the star is shorter than the orbital period of the planet: Prot < 2.12 days, Porb = 3.77 days. Thus, this system stands in a largely unexplored regime of dynamical interactions between close-in giant planets and their host stars. ","Discovery of XO-6b: a hot Jupiter transiting a fast rotating F5 star on
  an oblique orbit"
22,807256610276016128,2956121356,Russ Salakhutdinov,['New paper on Spatially Adaptive Computation Time for Residual Networks\nwith Michael Figurnov et al. \n<LINK>'],https://arxiv.org/abs/1612.02297,"This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions. ",Spatially Adaptive Computation Time for Residual Networks
23,807221561933627393,314014164,Adrian Price-Whelan,"['Some neat things about our new paper using @esagaia DR1 (<LINK>):', '1) We identify stars consistent w/ having same 3D velocity- not same proper motion! -using a probabilistic model w/ PMs, parallaxes, errors', '(stars with the same 3D velocity will have *different* proper motions if angular separation is large because spherical geometry)', '2) We find a huge number (&gt;10,000) of co-moving pairs of stars with physical separations up to 10 pc (our self-imposed limit for this work)', 'Stars w/ sep &gt;~ 0.1 pc not bound: dissolving clusters / disrupting wide binaries (predicted but never before seen https://t.co/X1GCoYpQ2A)', '3) We also find some new co-moving networks of pairs - likely star cluster or moving groups - at dists ~200-300 pc', 'The increase in proper motion and parallax precision in TGAS (vs. Tyc2) is *hugely* important for this work - thanks @esagaia and DPAC!', ""All of these stars have mag &lt;12.5 -- i.e. don't have many low-mass dwarfs in input sample -- in DR2+ we expect the num of pairs to skyrocket""]",https://arxiv.org/abs/1612.02440v1,"The primary sample of the Gaia Data Release 1 is the Tycho-Gaia Astrometric Solution (TGAS): $\approx$ 2 million Tycho-2 sources with improved parallaxes and proper motions relative to the initial catalog. This increased astrometric precision presents an opportunity to find new binary stars and moving groups. We search for high-confidence co-moving pairs of stars in TGAS by identifying pairs of stars consistent with having the same 3D velocity using a marginalized likelihood ratio test to discriminate candidate co-moving pairs from the field population. Although we perform some visualizations using (bias-corrected) inverse-parallax as a point-estimate of distance, the likelihood ratio is computed with a probabilistic model that includes the covariances of parallax and proper motions, and marginalizes the (unknown) true distances and 3D velocities of the stars. We find 13,085 co-moving star pairs among 10,606 unique stars with separations as large as 10 pc (our search limit). Some of these pairs form larger groups through mutual co-moving neighbors: many of these pair networks correspond to known open clusters and OB associations, but we also report the discovery of several new co-moving groups. Most surprisingly, we find a large number of very wide ($>1$ pc) separation co-moving star pairs, the number of which increases with increasing separation and cannot be explained purely by false-positive contamination. Our key result is a catalog of high-confidence co-moving pairs of stars in TGAS. We discuss the utility of this catalog for making dynamical inferences about the Galaxy, testing stellar atmosphere models, and validating chemical abundance measurements. ","] Co-moving stars in Gaia DR1: An abundance of very wide separation
  co-moving pairs"
24,806873059328200705,42853583,Michael Figurnov,['Check out our new paper called Spatially Adaptive Computation Time for Residual Networks! <LINK>'],https://arxiv.org/abs/1612.02297,"This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions. ",Spatially Adaptive Computation Time for Residual Networks
25,806829929602158592,280083723,Yoh Tanimoto,['our new paper~ <LINK>'],https://arxiv.org/abs/1612.02073,"In the bootstrap approach to integrable quantum field theories in the (1+1)-dimensional Minkowski space, one conjectures the two-particle S-matrix and tries to study local observables. The massless sine-Gordon model is conjectured to be equivalent to the Thirring model, and its breather-breather S-matrix components (where the first breather corresponds to the scalar field of the sine-Gordon model) are closed under fusion. Yet, the residues of the poles in this breather-breather S-matrix have wrong signs and cannot be considered as a separate model. We find CDD factors which adjust the signs, so that the breather-breather S-matrix alone satisfies reasonable assumptions. Then we propose candidates for observables in wedge-shaped regions and prove their commutativity in the weak sense. ",Wedge-local observables in the deformed sine-Gordon model
26,806306846764105728,1087183776,Jordan Ellenberg,"['New (very short) paper, also a tetradactyl:  ""Sumsets as unions of sumsets of subsets"" <LINK>']",https://arxiv.org/abs/1612.01929,"Let $S$ and $T$ be subsets of $\mathbf{F}_q^n$. We show there are subsets $S'$ of $S$ and $T'$ of $T$ such that $S+T$ is the union of $S+T'$ and $S'+T$, with $|S'| + |T'|$ bounded by $c^n$ with $c < q$. The proof relies on the method of Croot-Lev-Pach and Ellenberg-Gijswijt on the cap set problem, together with a result of Meshulam on linear spaces of low-rank matrices. The result is a modest generalization of the recent bounds on (single-colored and multi-colored) sum-free sets by the author and others. ",Sumsets as unions of sumsets of subsets
27,806077823349915649,267909797,Alexandre Drouin,['New paper on arXiv: Large scale modeling of antimicrobial resistance with interpretable classifiers. <LINK> #nips2016 #ml4h'],http://arxiv.org/abs/1612.01030,"Antimicrobial resistance is an important public health concern that has implications in the practice of medicine worldwide. Accurately predicting resistance phenotypes from genome sequences shows great promise in promoting better use of antimicrobial agents, by determining which antibiotics are likely to be effective in specific clinical cases. In healthcare, this would allow for the design of treatment plans tailored for specific individuals, likely resulting in better clinical outcomes for patients with bacterial infections. In this work, we present the recent work of Drouin et al. (2016) on using Set Covering Machines to learn highly interpretable models of antibiotic resistance and complement it by providing a large scale application of their method to the entire PATRIC database. We report prediction results for 36 new datasets and present the Kover AMR platform, a new web-based tool allowing the visualization and interpretation of the generated models. ","Large scale modeling of antimicrobial resistance with interpretable
  classifiers"
28,805947778887249920,83601067,Frédéric Raymond,['New paper on arxiv: Large scale modeling of antimicrobial resistance with interpretable classifiers. <LINK>'],http://arxiv.org/abs/1612.01030,"Antimicrobial resistance is an important public health concern that has implications in the practice of medicine worldwide. Accurately predicting resistance phenotypes from genome sequences shows great promise in promoting better use of antimicrobial agents, by determining which antibiotics are likely to be effective in specific clinical cases. In healthcare, this would allow for the design of treatment plans tailored for specific individuals, likely resulting in better clinical outcomes for patients with bacterial infections. In this work, we present the recent work of Drouin et al. (2016) on using Set Covering Machines to learn highly interpretable models of antibiotic resistance and complement it by providing a large scale application of their method to the entire PATRIC database. We report prediction results for 36 new datasets and present the Kover AMR platform, a new web-based tool allowing the visualization and interpretation of the generated models. ","Large scale modeling of antimicrobial resistance with interpretable
  classifiers"
29,805719672150691840,487990723,Gianfranco Bertone,"['In our new paper today on arXiv, we search for primordial black holes in radio &amp; X-rays, and argue PBHs≠dark matter  <LINK> <LINK>']",https://arxiv.org/abs/1612.00457,"We model the accretion of gas onto a population of massive primordial black holes in the Milky Way, and compare the predicted radio and X-ray emission with observational data. We show that under conservative assumptions on the accretion process, the possibility that ${\cal O}(10) \, M_\odot$ primordial black holes can account for all of the dark matter in the Milky Way is excluded at $5\sigma$ by a comparison with a VLA radio catalog at $1.4$ GHz, and at $\simeq 40\sigma$ by a comparison with a Chandra X-ray catalog ($0.5 - 8$ keV). We argue that this method can be used to identify such a population of primordial black holes with more sensitive future radio and X-ray surveys. ",Searching for Primordial Black Holes in the radio and X-ray sky
30,808601641150717952,1069045039,daniele marinazzo,"[""We use public data (thanks!). We find partitions characterised by different levels of @russpoldrack's fatigue scores\n<LINK> <LINK>"", ""@ChrisFiloG @russpoldrack that's why he scored mostly low on those! But when he scored high, the connectome ended up in a separate community""]",https://arxiv.org/abs/1612.03760,"A novel approach rooted on the notion of consensus clustering, a strategy developed for community detection in complex networks, is proposed to cope with the heterogeneity that characterizes connectivity matrices in health and disease. The method can be summarized as follows: (i) define, for each node, a distance matrix for the set of subjects by comparing the connectivity pattern of that node in all pairs of subjects; (ii) cluster the distance matrix for each node; (iii) build the consensus network from the corresponding partitions; (iv) extract groups of subjects by finding the communities of the consensus network thus obtained. Differently from the previous implementations of consensus clustering, we thus propose to use the consensus strategy to combine the information arising from the connectivity patterns of each node. The proposed approach may be seen either as an exploratory technique or as an unsupervised pre-training step to help the subsequent construction of a supervised classifier. Applications on a toy model and two real data sets, show the effectiveness of the proposed methodology, which represents heterogeneity of a set of subjects in terms of a weighted network, the consensus matrix. ",Consensus clustering approach to group brain connectivity matrices
