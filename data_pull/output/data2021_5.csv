,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1404497507774865409,1014263782493818880,Pierre Ablin,"['New paper: Kernel Stein Discrepancy Descent !\n\nA method to sample from unnormalized densities by optimization of the Kernel Stein Discrepancy (KSD)üéØüéØüéØ\n\nPaper: <LINK>\n\nWith @Korba_Anna, P-C Aubin &amp; @sjm_majewski \n\nAccepted for a long talk @icmlconf üçæ\n\n1/8üßµüëá <LINK>', 'Kernel Stein Discrepancy is a measure of distances between densities. It is a Maximum Mean Discrepancy (MMD) for a special Kernel, the Stein Kernel.\n\n2/8 https://t.co/B9AZGpafys', ""The Stein Kernel only involves the score s (derivative of the log) of the target distribution.\n\nWe can therefore compute the KSD in the formula above using only the score of the target: unlike most MMD's, we do not need samples.\n\n3/8 https://t.co/4tYJfGAk5v"", 'As a consequence, it is easy to evaluate the KSD between the target and a discrete measure of particles. In order to sample from the target, we can simply optimize this cost function w.r.t. the positions of the particles\n\n4/8 https://t.co/DkeDjpP5ok', 'We then have a simple objective function of the positions of the samples, which can then be minimized using (stochastic) gradient descent or the fast and robust algorithm L-BFGS.\n\nHere, particles reach an equilibrium on a simple Gaussian problem\n\n5/8 https://t.co/ZUEIF7ShxS', 'Yet, we also uncover bad behaviors when the problem is not log-concave : some particles tend to get stuck in spurious local minima, for instance here on a mixture of Gaussian with low variances\n\n6/8 https://t.co/7MMd8DsQvc', 'We give a mathematical analysis for these phenomena, and also show surprising results like lack of exponential convergence.\n\n7/8', 'If you want to try it yourself, we have made a python package:\n\npip install ksddescent\n\nSite: https://t.co/ckGZtfGYWN\nSource: https://t.co/rrrwBE67Ew\n\nIt also features pytorch/numpy code for the awesome SVGD algorithm (https://t.co/L9mJFciENx)\n\n8/8', ""@umutsimsekli @Korba_Anna @sjm_majewski @icmlconf Of course it's here ! \n\nFor Bayesian ICA, KSD descent does not really work, because it is a hard non convex problem with many spurious local minima https://t.co/BB5nfDdJ5l""]",https://arxiv.org/abs/2105.09994,"Among dissimilarities between probability distributions, the Kernel Stein Discrepancy (KSD) has received much interest recently. We investigate the properties of its Wasserstein gradient flow to approximate a target probability distribution $\pi$ on $\mathbb{R}^d$, known up to a normalization constant. This leads to a straightforwardly implementable, deterministic score-based method to sample from $\pi$, named KSD Descent, which uses a set of particles to approximate $\pi$. Remarkably, owing to a tractable loss function, KSD Descent can leverage robust parameter-free optimization schemes such as L-BFGS; this contrasts with other popular particle-based schemes such as the Stein Variational Gradient Descent algorithm. We study the convergence properties of KSD Descent and demonstrate its practical relevance. However, we also highlight failure cases by showing that the algorithm can get stuck in spurious local minima. ",Kernel Stein Discrepancy Descent
1,1403673720259031042,28734416,Sebastian Risi,"['New paper ""Hybrid Encoding For Generating Large Scale Game Level Patterns With Local Variations Using a GAN"". \n\nIdea: First define latent vectors of a GAN with an evolving CPPN to provide overall geometry then evolve latent vectors further to fine-tune.\n<LINK> <LINK>', 'A fun collaboration lead by Jacob Schrum w/ Benjamin Capps, Kirby Steckel, and Vanessa Volz (@CIGbalance)\n\nCode: https://t.co/EDu8G1OkXR', 'This new hybrid approach (CPPNThenDirect2GAN) can cover areas of the space of possible levels that neither the indirect nor direct approach can cover. https://t.co/crYlVqHlck', ""Does it also work for AI researchers' favorite game Mario you ask? Yes, yes it does. https://t.co/qQXcmDNs6h"", '@LouisKirschAI Thanks Louis!']",https://arxiv.org/abs/2105.12960,"Generative Adversarial Networks (GANs) are a powerful indirect genotype-to-phenotype mapping for evolutionary search. Much previous work applying GANs to level generation focuses on fixed-size segments combined into a whole level, but individual segments may not fit together cohesively. In contrast, segments in human designed levels are often repeated, directly or with variation, and organized into patterns (the symmetric eagle in Level 1 of The Legend of Zelda, or repeated pipe motifs in Super Mario Bros). Such patterns can be produced with Compositional Pattern Producing Networks (CPPNs). CPPNs define latent vector GAN inputs as a function of geometry, organizing segments output by a GAN into complete levels. However, collections of latent vectors can also be evolved directly, producing more chaotic levels. We propose a hybrid approach that evolves CPPNs first, but allows latent vectors to evolve later, combining the benefits of both approaches. These approaches are evaluated in Super Mario Bros. and The Legend of Zelda. We previously demonstrated via a Quality-Diversity algorithm that CPPNs better cover the space of possible levels than directly evolved levels. Here, we show that the hybrid approach (1) covers areas that neither of the other methods can, and (2) achieves comparable or superior QD scores. ","Hybrid Encoding For Generating Large Scale Game Level Patterns With
  Local Variations"
2,1402550648000839684,807327556072402945,Machel Reid,"['Our new #ACL2021NLP Findings paper ""LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer"" (w/@hllo_wrld) proposes LEWIS, an edit-based model to transfer style/domains of text in an unsupervised manner.\n\nPaper: <LINK>\nThread: 1/7', ""For many style/domain transfer tasks, we don't have any parallel source/target pairs. How can we overcome this?\n\nPart 2/7"", 'We do so by synthesising data using classifier attention to identify style-specific tokens and a domain-specific generator to replace those SLOTs with domain-specific text.\n\nPart 3/7 https://t.co/2yvoQbW2Ca', 'Using this synthesized data, we learn to edit text using the Levenshtein edit operations with a coarse-grain edit tagger and a fine-grained tagger as shown below:\n\nPart 4/7 https://t.co/b3grbygE1K', 'The full process is shown below:\n\nPart 5/7 https://t.co/GNv1ag1KWG', 'With our method, we show large gains over previous work:\n\nPart 6/8 https://t.co/wSrdaFewc0', 'We also find that editing outperforms generation when given the same synthetic data, however, training with synthetic data using a simple MT model outperforms previous unsupervised methods by a large margin.\n\nPart 7/8 https://t.co/CCkQ94KkY3', 'Finally thank you to @gabriel_ilharco, Edison Marrese-Taylor, @_julianmichael_, @tongshuangwu, @_yutaroyamada, and @LukeZettlemoyer for helpful discussions and feedback!\n\nPart 8/8', '*fine-grained generator']",https://arxiv.org/abs/2105.08206,"Many types of text style transfer can be achieved with only small, precise edits (e.g. sentiment transfer from I had a terrible time... to I had a great time...). We propose a coarse-to-fine editor for style transfer that transforms text using Levenshtein edit operations (e.g. insert, replace, delete). Unlike prior single-span edit methods, our method concurrently edits multiple spans in the source text. To train without parallel style text pairs (e.g. pairs of +/- sentiment statements), we propose an unsupervised data synthesis procedure. We first convert text to style-agnostic templates using style classifier attention (e.g. I had a SLOT time...), then fill in slots in these templates using fine-tuned pretrained language models. Our method outperforms existing generation and editing style transfer methods on sentiment (Yelp, Amazon) and politeness (Polite) transfer. In particular, multi-span editing achieves higher performance and more diverse output than single-span editing. Moreover, compared to previous methods on unsupervised data synthesis, our method results in higher quality parallel style pairs and improves model performance. ",LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer
3,1402382668910796810,20593623,Maggie Makar,"['[1/n] New arXiv paper alert! ML models can take shortcuts, which makes them perform poorly under distribution shift. We used some ideas from causality to get models to use the right signal. W @davisblalock, @packer_ben, Yoni Halpern, &amp; @alexdamour. <LINK> üßµ <LINK>', '[2/n] Example (based on work by @shiorisagawa, @pangweikoh, et al https://t.co/DwBES7bvzO): Train a ResNet50 to classify water and land birds using a training distribution where most land birds appear on land backgrounds and vice versa. https://t.co/yoinuq4FnG', '[3/n] The bird in the foreground is sufficient to classify the images perfectly, but the model learns to use the background as a shortcut. If the foreground/background correlation changes at test time, performance deteriorates. https://t.co/ux5kRGdoA0', ""[4/n] Addressing this problem automatically is hard. But if you have labels for the shortcut feature at training time, we show that you can enforce certain causal invariances in the model's representation, using weights and an invariance penalty (based on MMD). It works! https://t.co/p3JHWQCXWq"", ""[5/n] Importantly, the penalty reduces the complexity of the model's function space without inducing bias (since we know the causal invariances are true).  In a simple linear case, the invariance penalty limits the weights of the ‚Äúshortcut‚Äù factors https://t.co/jBCqg3lYbd"", '[6/n] Thus, even when there is no shortcut, the method is more data efficient, achieving better performance on balanced data. https://t.co/Q3lPPrPGD3', ""[7/n] We're very excited about this work, but it's just the beginning. Lots of opportunities to use causal formalism to import domain knowledge into ML to improve robustness, efficiency, and trustworthiness! Code coming soon!"", '[7.5/n] work also with Dan Moldovan! #errata']",https://arxiv.org/abs/2105.06422,"Shortcut learning, in which models make use of easy-to-represent but unstable associations, is a major failure mode for robust machine learning. We study a flexible, causally-motivated approach to training robust predictors by discouraging the use of specific shortcuts, focusing on a common setting where a robust predictor could achieve optimal \emph{iid} generalization in principle, but is overshadowed by a shortcut predictor in practice. Our approach uses auxiliary labels, typically available at training time, to enforce conditional independences implied by the causal graph. We show both theoretically and empirically that causally-motivated regularization schemes (a) lead to more robust estimators that generalize well under distribution shift, and (b) have better finite sample efficiency compared to usual regularization schemes, even when no shortcut is present. Our analysis highlights important theoretical properties of training techniques commonly used in the causal inference, fairness, and disentanglement literatures. Our code is available at this https URL ",Causally motivated Shortcut Removal Using Auxiliary Labels
4,1400827497646751747,1400815831018213376,Meng-Hao Guo,['Our new paper uses two linear layers to replace self-attention and builds an all-MLP architecture name EAMLP.  EAMLP achieved 79.4% accuracy on ImageNet and surpassed Performer in some settings! Results and visualizations are as follows. \n\nArXiv: <LINK>. <LINK>'],https://arxiv.org/abs/2105.02358,"Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs. ","Beyond Self-attention: External Attention using Two Linear Layers for
  Visual Tasks"
5,1400607468191944705,83017144,Santosh Nagarakatte,"['Our Linux kernel patch with a new algorithm for sound and precise multiplication of tnums (an abstract domain in the ebpf verifier) has been upstreamed. See paper at <LINK>. \nSee patch at <LINK>\nJoint work with Hari, Matan, and @ngsrinivas #ebpf']",https://arxiv.org/abs/2105.05398,"Extended Berkeley Packet Filter (BPF) is a language and run-time system that allows non-superusers to extend the Linux and Windows operating systems by downloading user code into the kernel. To ensure that user code is safe to run in kernel context, BPF relies on a static analyzer that proves properties about the code, such as bounded memory access and the absence of operations that crash. The BPF static analyzer checks safety using abstract interpretation with several abstract domains. Among these, the domain of tnums (tristate numbers) is a key domain used to reason about the bitwise uncertainty in program values. This paper formally specifies the tnum abstract domain and its arithmetic operators. We provide the first proofs of soundness and optimality of the abstract arithmetic operators for tnum addition and subtraction used in the BPF analyzer. Further, we describe a novel sound algorithm for multiplication of tnums that is more precise and efficient (runs 33% faster on average) than the Linux kernel's algorithm. Our tnum multiplication is now merged in the Linux kernel. ","Sound, Precise, and Fast Abstract Interpretation with Tristate Numbers"
6,1400548790575124485,4552714514,Kerrie Mengersen,"['Dive into arxiv for our new paper on cool ways to cluster compositional data, applied to the Great Barrier Reef.\n<LINK>\nLed by the talented Luiza Piancastelli, co-authors @NialFriel, @JVercelloni, @MiraAntonietta']",https://arxiv.org/abs/2105.02140,"Relative abundance is a common metric to estimate the composition of species in ecological surveys reflecting patterns of commonness and rarity of biological assemblages. Measurements of coral reef compositions formed by four communities along Australia's Great Barrier Reef (GBR) gathered between 2012 and 2017 are the focus of this paper. We undertake the task of finding clusters of transect locations with similar community composition and investigate changes in clustering dynamics over time. During these years, an unprecedented sequence of extreme weather events (cyclones and coral bleaching) impacted the 58 surveyed locations. The dependence between constituent parts of a composition presents a challenge for existing multivariate clustering approaches. In this paper, we introduce a finite mixture of Dirichlet distributions with group-specific parameters, where cluster memberships are dictated by unobserved latent variables. The inference is carried in a Bayesian framework, where MCMC strategies are outlined to sample from the posterior model. Simulation studies are presented to illustrate the performance of the model in a controlled setting. The application of the model to the 2012 coral reef data reveals that clusters were spatially distributed in similar ways across reefs which indicates a potential influence of wave exposure at the origin of coral reef community composition. The number of clusters estimated by the model decreased from four in 2012 to two from 2014 until 2017. Posterior probabilities of transect allocations to the same cluster substantially increase through time showing a potential homogenization of community composition across the whole GBR. The Bayesian model highlights the diversity of coral reef community composition within a coral reef and rapid changes across large spatial scales that may contribute to undermining the future of the GBR's biodiversity. ","A Bayesian latent allocation model for clustering compositional data
  with application to the Great Barrier Reef"
7,1400342381367541762,153384371,Jonas Schuett,"['I\'m super excited to share with you my new paper ""AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries"" that I wrote together with my amazing colleagues Peter Cihon (@pcihon), Moritz Kleinaltenkamp, and Seth Baum (@SethBaum).\n<LINK>', 'Based on a review of the management literature on certification, we show how AI certification can reduce information asymmetries and incentivize the adoption of AI ethics principles.', 'We also survey the current landscape of AI certification schemes and briefly discuss what all this could mean for future AI systems.', 'The article will appear in IEEE Transactions on Technology and Society: https://t.co/2BKuZeelgC']",https://arxiv.org/abs/2105.10356,"As artificial intelligence (AI) systems are increasingly deployed, principles for ethical AI are also proliferating. Certification offers a method to both incentivize adoption of these principles and substantiate that they have been implemented in practice. This paper draws from management literature on certification and reviews current AI certification programs and proposals. Successful programs rely on both emerging technical methods and specific design considerations. In order to avoid two common failures of certification, program designs should ensure that the symbol of the certification is substantially implemented in practice and that the program achieves its stated goals. The review indicates that the field currently focuses on self-certification and third-party certification of systems, individuals, and organizations - to the exclusion of process management certifications. Additionally, the paper considers prospects for future AI certification programs. Ongoing changes in AI technology suggest that AI certification regimes should be designed to emphasize governance criteria of enduring value, such as ethics training for AI developers, and to adjust technical criteria as the technology changes. Overall, certification can play a valuable mix in the portfolio of AI governance tools. ","AI Certification: Advancing Ethical Practice by Reducing Information
  Asymmetries"
8,1400253722073079808,2909643908,Dan Zhang,"['I wrote a paper with a few Google colleagues about FAST, a new technique to build new specialized ML hardware accelerators able to improve computer vision inference performance by up to 6x relative to TPU-v3!\n\n<LINK> (1/5)', 'FAST extends previous work by expanding the design exploration space to up to 10^2300, covering not just the hardware datapath, but also software scheduling and compiler decisions including padding and fusion. Fusion is key since it addresses memory bandwidth bottlenecks. (2/5)', 'Accelerating ML inference is important because fast ML inference latency/throughput is required to launch models in production at scale. If an application is sufficiently important and with enough volume, it can make sense to customize a chip for this purpose. (3/5)', 'A key benefit of the work is that our specialized accelerators can still run other ML workloads - just not as efficiently. Relative to building a chip that can only handle a single workload, this gives engineers the flexibility to still change the model in production. (4/5)', 'Finally, techniques like this are important because they can be used to accelerate the chip design process from years to potentially months. By increasing the workload set targeted by FAST, FAST can also be used to automatically design general-purpose ML accelerators. (5/5)', 'TLDR: we made specialized hardware chips that can do machine learning faster :)', ""@selynnasun Thanks! We're super excited about the work!""]",https://arxiv.org/abs/2105.12842,"The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7x on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4x on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments. ","A Full-Stack Search Technique for Domain Optimized Deep Learning
  Accelerators"
9,1399938708804018178,1321933647021133824,Ma√Øssa Salama,"['#PaperDay! After several years of work, from installing @robo_ao (on 2 diff telescopes!) &amp; adding a new IR SAPHIRA camera to long observing runs... my paper is finally finished &amp; accepted!! The first results of the LASSO survey can be found here: <LINK> 1/11 <LINK>', 'The goal of  LASSO (Large Adaptive optics Survey for Substellar Objects) is to detect wide-orbit (&gt;50 AU) substellar companions. These massive substellar companions generate many Qs about their formation mechanisms. High-mass end of planets? Low-mass end of stars? Both? New? 2/11', 'We directly imaged 427 young (&lt;300 Myrs), nearby (&lt;100 pc), low-mass (0.1-0.8 M_Sun) stars taken from the Cool Dwarf Catalog, cross-matched with GALEX to apply UV cuts to select for youth, and cross-matched with Gaia DR2 to select stars within 100 pc. 3/11 https://t.co/kjj2xXY0dv', 'We observed with @robo_ao, a robotic laser adaptive optics instrument, first on the 2.1-m telescope @KittPeakNatObs, then on the UH 2.2-m telescope @maunakeaobs. We installed a new SAPHIRA detector, an IR avalanche photodiode array &amp; simultaneously imaged in the visible &amp; IR 4/11 https://t.co/DAOBfN2pCw', 'Our sensitivity estimates, generated by injecting &amp; recovering fake companions. Sensitive to substellar masses for 50% of our observations in the 50-450 AU range. We also observed a set of Sco-Cen targets, more sensitive to substellar objects because of the younger ages 5/11 https://t.co/EfqArwyX7a', 'We detected 121 companion candidates: 62 are physically associated, 14 are background objects, 45 still require confirmation. 4 triple systems in hierarchical structures. 6/11 https://t.co/ylBJYmCmK7', 'To determine physical association, we searched for the companions in @ESAGaia DR2 for matching proper motion and parallax measurements with the primary star. 7/11 https://t.co/d3WNdapz1O', 'Having optical and IR images allowed us to do initial characterization of companion detections. Most are stellar. We are currently following-up the ones that are the faintest and reddest to determine if they are substellar. 2 previously reported 43MJup &amp; 81MJup also detected 8/11 https://t.co/N37vo35gXK', 'Astrometrically accelerating stars allow us to calculate companion dynamical masses &amp; orbital info. 34 of our observed stars were found in the HGCA catalog. The majority of those with companion detections had significant accelerations and correlated with separation. 9/11 https://t.co/mWGX174bq1', 'Summary: We directly imaged 427 young, nearby, low-mass stars w/ Robo-AO, detected 121 comp candidates: 62 physically associated, 14 background, 45 TBD. 4 triple systems, one 43 MJup BD, one 81 MJup, and 4-5 being followed-up with potential for being substellar if young. 10/11', 'Finally, none of this work would have been possible without the many people who contributed along the way. From my advisors @CBaranec &amp; Mike Liu to collaborators James Ou, @bpbowler, @astrozjzhang, the Robo-AO team &amp; SAPHIRA camera team. Thank you all! 11/11', '@SuchiNarayanan Aww thank you so much!!! And hopefully in person!!ü§û']",https://arxiv.org/abs/2105.13364,"We present results from the Large Adaptive optics Survey for Substellar Objects (LASSO), where the goal is to directly image new substellar companions (<70 M$_{Jup}$) at wide orbital separations ($\gtrsim$50 AU) around young ($\lesssim$300 Myrs), nearby (<100 pc), low-mass ($\approx$0.1-0.8 M$_{\odot}$) stars. We report on 427 young stars imaged in the visible (i') and near-infrared (J or H) simultaneously with Robo-AO on the Kitt Peak 2.1-m telescope and later the Maunakea University of Hawaii 2.2-m telescope. To undertake the observations, we commissioned a new infrared camera for Robo-AO that uses a low-noise high-speed SAPHIRA avalanche photodiode detector. We detected 121 companion candidates around 111 stars, of which 62 companions are physically associated based on Gaia DR2 parallaxes and proper motions, another 45 require follow-up observations to confirm physical association, and 14 are background objects. The companion separations range from 2-1101 AU and reach contrast ratios of 7.7 magnitudes in the near infrared compared to the primary. The majority of confirmed and pending candidates are stellar companions, with ~5 being potentially substellar and requiring follow-up observations for confirmation. We also detected a 43$\pm$9 M$_{Jup}$ and an 81$\pm$5 M$_{Jup}$ companion that were previously reported. We found 34 of our targets have acceleration measurements detected using Hipparcos-Gaia proper motions. Of those, 58$^{+12}_{-14}$% of the 12 stars with imaged companion candidates have significant accelerations ($\chi^2 >11.8$), while only 23$^{+11}_{-6}$% of the remaining 22 stars with no detected companion have significant accelerations. The significance of the acceleration decreases with increasing companion separation. These young accelerating low-mass stars with companions will eventually yield dynamical masses with future orbit monitoring. ","Large Adaptive Optics Survey for Substellar Objects (LASSO) Around
  Young, Nearby, Low-mass Stars with Robo-AO"
10,1399900430927233029,941131462744539137,Liangming Pan,"['Hi all, our new #ACL2021 paper on ""Zero-shot Fact Verification by Claim Generation"" is now at <LINK> #NLProc. We propose a framework based on question generation to automatically generate (evidence, claim) pairs to train the fact verification model. <LINK>']",https://arxiv.org/abs/2105.14682,"Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available. ",Zero-shot Fact Verification by Claim Generation
11,1399879973964435456,186701821,Aldo Pacchiano,"['New paper with \n@niladrichat \n, Peter Bartlett and Michael Jordan called ""On the Theory of Reinforcement Learning with Once-per-Episode Feedback"": <LINK>. It was very interesting for us to think about non-Markovian reward models for reinforcement learning!! <LINK>']",http://arxiv.org/abs/2105.14363,"We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner's complete trajectory was either ""good"" or ""bad,"" but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sub-linear regret. ",On the Theory of Reinforcement Learning with Once-per-Episode Feedback
12,1399879536947331074,363058714,Niladri Chatterji,"['New paper with @aldopacchiano, Peter Bartlett and Michael Jordan called ""On the Theory of Reinforcement Learning with Once-per-Episode Feedback"": <LINK>. It was very interesting for us to think about non-Markovian reward models for reinforcement learning (1/2.)', 'Very eager to hear any comments and criticism about our setting and results! (2/2.)']",http://arxiv.org/abs/2105.14363,"We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner's complete trajectory was either ""good"" or ""bad,"" but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sub-linear regret. ",On the Theory of Reinforcement Learning with Once-per-Episode Feedback
13,1399791291337035777,490621486,Dr. Malena Espanol,"['Our new paper ""An lp Variable Projection Method for Large-Scale Separable Nonlinear Inverse Problems"" with Mirjeta Pasha is now in arxiv. <LINK>']",https://arxiv.org/abs/2105.14155,"The variable projection (VarPro) method is an efficient method to solve separable nonlinear least squares problems. In this paper, we propose a modified VarPro for large-scale separable nonlinear inverse problems that promotes edge-preserving and sparsity properties on the desired solution and enhances the convergence of the parameters that define the forward problem. We adopt a majorization minimization method that relies on constructing a quadratic tangent majorant to approximate a general $\ell_p$ regularized problem by an $\ell_2$ regularized problem that can be solved by the aid of generalized Krylov subspace methods at a relatively low cost compared to the original unprojected problem. In addition, we can use more potential general regularizers including total variation (TV), framelet, and wavelets operators. The regularization parameter can be defined automatically at each iteration by means of generalized cross validation. Numerical examples on large-scale two-dimensional imaging problems arising from blind deconvolution are used to highlight the performance of the proposed approach in both quality of the reconstructed image as well as the reconstructed forward operator. ","An $\ell_p$ Variable Projection Method for Large-Scale Separable
  Nonlinear Inverse Problems"
14,1399750404179697664,1115817574078582785,Prithviraj Ammanabrolu,"['New #SIGDIAL2021 paper on #DnD style storytelling through multi-user dialogue! Predicting relationship types between characters via sentiment while learning to talk helps #AI models be better DnD players!!\nPaper: <LINK>\nüßµüëá1/3 <LINK>', 'Tl;dr we automatically learn to augment a dataset of #DnD Dialogue transcripts from @CriticalRole with pairwise character relation info (friend/enemy/neutral). Multi tasking to predict relations significantly improves dialogue based storytelling.\nw/ Raymond Si, @mark_riedl', ""Of course, we're pretty far away from having real AI DnD players or DMs. But just trying to emulate the sheer complexity of all the things that happen in our head while playing DnD helps us build better language based AI. 3/3""]",https://arxiv.org/abs/2105.15054,"This paper explores character-driven story continuation, in which the story emerges through characters' first- and second-person narration as well as dialogue -- requiring models to select language that is consistent with a character's persona and their relationships with other characters while following and advancing the story. We hypothesize that a multi-task model that trains on character dialogue plus character relationship information improves transformer-based story continuation. To this end, we extend the Critical Role Dungeons and Dragons Dataset (Rameshkumar and Bailey, 2020) -- consisting of dialogue transcripts of people collaboratively telling a story while playing the role-playing game Dungeons and Dragons -- with automatically extracted relationships between each pair of interacting characters as well as their personas. A series of ablations lend evidence to our hypothesis, showing that our multi-task model using character relationships improves story continuation accuracy over strong baselines. ","Telling Stories through Multi-User Dialogue by Modeling Character
  Relations"
15,1399731788793008133,28840722,Phil Long,"['New paper called ""Properties of the After Kernel"": <LINK>.  Code: <LINK>.  (The after kernel is like the neural tangent kernel, but after training.)']",https://arxiv.org/abs/2105.10585,"The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined using neural networks at initialization, whose embedding is the gradient of the output of the network with respect to its parameters. We study the ""after kernel"", which is defined using the same embedding, except after training, for neural networks with standard architectures, on binary classification problems extracted from MNIST and CIFAR-10, trained using SGD in a standard way. For some dataset-architecture pairs, after a few epochs of neural network training, a hard-margin SVM using the network's after kernel is much more accurate than when the network's initial kernel is used. For networks with an architecture similar to VGG, the after kernel is more ""global"", in the sense that it is less invariant to transformations of input images that disrupt the global structure of the image while leaving the local statistics largely intact. For fully connected networks, the after kernel is less global in this sense. The after kernel tends to be more invariant to small shifts, rotations and zooms; data augmentation does not improve these invariances. The (finite approximation to the) conjugate kernel, obtained using the last layer of hidden nodes, sometimes, but not always, provides a good approximation to the NTK and the after kernel. Training a network with a larger learning rate (while holding the training error constant) produces a better kernel, as measured by the test error of a hard-margin SVM. The after kernels of networks trained with larger learning rates tend to be more global, and more invariant to small shifts, rotations and zooms. ",Properties of the After Kernel
16,1399718457764200459,626009011,Zachary Nasipak,['New paper out today! We calculate the scalar self-force experienced by (scalar-charged) EMRIs as they pass through r-theta resonances and test how resonant self-force effects impact the leading-order orbital evolution of EMRIs.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2105.15188,"Extreme-mass ratio inspirals (EMRIs), compact binaries with small mass ratios $\epsilon\ll 1$, will be important sources for low-frequency gravitational wave detectors. Almost all EMRIs will evolve through important transient orbital $r\theta$ resonances, which will enhance or diminish their gravitational wave flux, thereby affecting the phase evolution of the waveforms at $O(\epsilon^{1/2})$ relative to leading order. While modeling the local gravitational self-force (GSF) during resonances is essential for generating accurate EMRI waveforms, so far the full GSF has not been calculated for an $r\theta$-resonant orbit owing to computational demands of the problem. As a first step we employ a simpler model, calculating the scalar self-force (SSF) along $r\theta$-resonant geodesics in Kerr spacetime. We demonstrate two ways of calculating the $r\theta$-resonant SSF (and likely GSF), with one method leaving the radial and polar motions initially independent as if the geodesic is nonresonant. We illustrate results by calculating the SSF along geodesics defined by three $r\theta$-resonant ratios (1:3, 1:2, 2:3). We show how the SSF and averaged evolution of the orbital constants vary with the initial phase at which an EMRI enters resonance. We then use our SSF data to test a previously proposed integrability conjecture, which argues that conservative effects vanish at adiabatic order during resonances. We find prominent contributions from the conservative SSF to the secular evolution of the Carter constant, $\langle \dot{\mathcal{Q}}\rangle$, but these nonvanishing contributions are on the order of, or less than, the estimated uncertainties of our self-force results. The uncertainties come from residual, incomplete removal of the singular field in the regularization process. Higher order regularization parameters, once available, will allow definitive tests of the integrability conjecture. ","Resonant self-force effects in extreme-mass-ratio binaries: A scalar
  model"
17,1399691813095718914,839820726777544705,Alexia Jolicoeur-Martineau,"['New paper is out! üòª We show how to generate high-quality data as fast as possible with score-based (diffusion) models! üèÉüèªüí®üí®\n\nBlog: <LINK>\nPaper: <LINK>\nCode: <LINK>\n\nWork with @KL_Div @Remi29048827 @TalKachman  @bouzoukipunks', 'Tagging a few people working in the field who might be interested: @arankomatsuzaki @unixpickle @prafdhar @YSongStanford @pabbeel']",https://arxiv.org/abs/2105.14080,"Score-based (denoising diffusion) generative models have recently gained a lot of success in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data to noise and generate data by reversing it (thereby going from noise to data). Unfortunately, current score-based models generate data very slowly due to the sheer number of score network evaluations required by numerical SDE solvers. In this work, we aim to accelerate this process by devising a more efficient SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which uses a fixed step size. We found that naively replacing it with other SDE solvers fares poorly - they either result in low-quality samples or become slower than EM. To get around this issue, we carefully devise an SDE solver with adaptive step sizes tailored to score-based generative models piece by piece. Our solver requires only two score function evaluations, rarely rejects samples, and leads to high-quality samples. Our approach generates data 2 to 10 times faster than EM while achieving better or equal sample quality. For high-resolution images, our method leads to significantly higher quality samples than all other methods tested. Our SDE solver has the benefit of requiring no step size tuning. ",Gotta Go Fast When Generating Data with Score-Based Models
18,1399666122140626947,2872853608,Paula Soares,"['New paper! In <LINK> we showed for the first time that Gaussian Process Regression (GPR) can be used as a foreground removal technique in single-dish, low redshift 21cm intensity mapping. With @CunningtonSD, @CatAstro_Phy, and @AlkistisPou. [thread]', 'We also released some user-friendly code for running GPR as a foreground removal technique, which has a lot of introductory Jupyter notebooks and easy to follow examples: https://t.co/jT5EU3V41c', 'Astrophysical foregrounds dominate over the cosmological 21cm signal so need to be removed. GPR has already been successfully used as a foreground cleaning technique in the context of EoR (see e.g. arXiv:1711.10834), so we wanted to try it in the case of large-scale structure!', 'We used MeerKAT-like simulations of foregrounds, 21cm signal and instrumental noise, and compared the performance of GPR to another widely used foreground cleaning technique: Principal Component Analysis (PCA).', 'We find that GPR is very good at recovering the 21cm power spectrum, especially the radial power spectrum. It is in many cases better suited to recover the power spectrum than PCA, especially on small scales. Check out how well it performs when there is no polarisation leakage: https://t.co/jgNmuRPhz8', 'We also tested the case of including polarisation leakage, trimming the bandwidth, removing frequency channels, and tested if it is possible to implement a foreground transfer function with GPR (spoiler: it is!)', ""In summary: it works!! And you can use our code to try it out yourself üôÇ we also describe how GPR works in detail in the paper, so if you're interested check that out too!"", '@AmelieSaintonge @CunningtonSD @CatAstro_Phy @AlkistisPou Thank you Am√©lie! üôÇ', '@guadalupecah Thank you!! üôÇ']",http://arxiv.org/abs/2105.12665,"We apply for the first time Gaussian Process Regression (GPR) as a foreground removal technique in the context of single-dish, low redshift HI intensity mapping, and present an open-source Python toolkit for doing so. We use MeerKAT and SKA1-MID-like simulations of 21cm foregrounds (including polarisation leakage), HI cosmological signal and instrumental noise. We find that it is possible to use GPR as a foreground removal technique in this context, and that it is better suited in some cases to recover the HI power spectrum than Principal Component Analysis (PCA), especially on small scales. GPR is especially good at recovering the radial power spectrum, outperforming PCA when considering the full bandwidth of our data. Both methods are worse at recovering the transverse power spectrum, since they rely on frequency-only covariance information. When halving our data along frequency, we find that GPR performs better in the low frequency range, where foregrounds are brighter. It performs worse than PCA when frequency channels are missing, to emulate RFI flagging. We conclude that GPR is an excellent foreground removal option for the case of single-dish, low redshift HI intensity mapping in the absence of missing frequency channels. Our Python toolkit gpr4im and the data used in this analysis are publicly available on GitHub. ","Gaussian Process Regression for foreground removal in HI intensity
  mapping experiments"
19,1399633850138046465,202069135,laurent besacier,"['Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads? We investigate this Q for a strong MNMT model, in our new ACL 2021 (Findings) paper: <LINK>. Joint work with Zae-Myung Kim, Vassilina Nikoulina and @didier_schwab !']",https://arxiv.org/abs/2105.14940,"Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of language-independent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a ""black-box"" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as ""variance"" or ""confidence"", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly. ","Do Multilingual Neural Machine Translation Models Contain Language Pair
  Specific Attention Heads?"
20,1399567901896503300,1147473161950134273,Akinori F. Ebihara,"['Êàë„ÄÖ„ÅÆÊñ∞„Åó„ÅÑË´ñÊñá„ÅåÂÖ¨Èñã„Åï„Çå„Åæ„Åó„ÅüÔºÅICML2021„Å´„Ç¢„ÇØ„Çª„Éó„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nOur new paper was accepted to International Conference on Machine Learning (ICML) 2021!\n\n<LINK>\n\n<LINK>', '„Åì„ÅÆË´ñÊñá„ÅØICLR2021„Å´Âá∫„Åó„ÅüÂØÜÂ∫¶ÊØîÊé®ÂÆö„Å®SPRT„ÅÆË´ñÊñá„ÅÆÁ∂ö„Åç„Åß„ÄÅÂÆÆÂ∑ù„Åï„ÇìÔºà@kanaheinousagiÔºâ„ÅåÔºàÁßÅ„ÅåICLR„Åß„Éê„Çø„Éê„Çø„Åó„Å¶„ÅÑ„Çã„ÅÜ„Å°„Å´Ôºâ„Åó„Å£„Åã„Çä„Å®ÈÄ≤„ÇÅ„Å¶„Åè„Çå„Åæ„Åó„Åü„ÄÇÂØÜÂ∫¶ÊØîÊé®ÂÆöloss„ÅÆ„Ç¢„ÉÉ„Éó„Ç∞„É¨„Éº„Éâ„ÅåÁõÆÁéâ„Åß„Åô„Åå„ÄÅËß£Ë™¨„ÅØÂΩº„ÅÆ„ÉÑ„Ç§„Éº„Éà„Å´Ë≠≤„Çä„Åæ„Åô„ÅÆ„Åß„Å©„ÅÜ„ÅûÂæ°Ë¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ']",https://arxiv.org/abs/2105.13636,"We propose a model for multiclass classification of time series to make a prediction as early and as accurate as possible. The matrix sequential probability ratio test (MSPRT) is known to be asymptotically optimal for this setting, but contains a critical assumption that hinders broad real-world applications; the MSPRT requires the underlying probability density. To address this problem, we propose to solve density ratio matrix estimation (DRME), a novel type of density ratio estimation that consists of estimating matrices of multiple density ratios with constraints and thus is more challenging than the conventional density ratio estimation. We propose a log-sum-exp-type loss function (LSEL) for solving DRME and prove the following: (i) the LSEL provides the true density ratio matrix as the sample size of the training set increases (consistency); (ii) it assigns larger gradients to harder classes (hard class weighting effect); and (iii) it provides discriminative scores even on class-imbalanced datasets (guess-aversion). Our overall architecture for early classification, MSPRT-TANDEM, statistically significantly outperforms baseline models on four datasets including action recognition, especially in the early stage of sequential observations. Our code and datasets are publicly available at: this https URL ","The Power of Log-Sum-Exp: Sequential Density Ratio Matrix Estimation for
  Speed-Accuracy Optimization"
21,1399567699257212929,862031958284652544,Weiming Xiang,"['New paper accepted by IEEE Control Systems Letters, proposing two equivalent non-conservative stability criteria for switched systems with ranged dwell time. Preprint: <LINK>']",https://arxiv.org/abs/2105.14113,"This paper deals with the stability analysis problem of discrete-time switched linear systems with ranged dwell time. A novel concept called L-switching-cycle is proposed, which contains sequences of multiple activation cycles satisfying the prescribed ranged dwell time constraint. Based on L-switching-cycle, two sufficient conditions are proposed to ensure the global uniform asymptotic stability of discrete-time switched linear systems. It is noted that two conditions are equivalent in stability analysis with the same $L$-switching-cycle. These two sufficient conditions can be viewed as generalizations of the clock-dependent Lyapunov and multiple Lyapunov function methods, respectively. Furthermore, it has been proven that the proposed L-switching-cycle can eventually achieve the nonconservativeness in stability analysis as long as a sufficiently long L-switching-cycle is adopted. A numerical example is provided to illustrate our theoretical results. ","Necessary and Sufficient Conditions for Stability of Discrete-Time
  Switched Linear Systems with Ranged Dwell Time"
22,1399566968684912641,4741698912,Linzi Xing,"['Excited to share our new #ACL2021NLP paper ‚ÄúDemoting the Lead Bias in News Summarization via Alternating Adversarial Learning‚Äù (w/ @wendyxiao06091  and Prof. Giuseppe Carenini). \nArXiv: <LINK>\nCode is coming soon.', 'We propose an effective strategy to demote the lead bias learned by the neural extractive summarizer from news data with such bias (e.g., CNN/DM). https://t.co/n76gPwpqUZ', ""In real-world scenario, we may have numerous news data (like CNN/DM) to train a summarizer, but when applying the learned summarizer to an unseen news, it's hard to tell if the model is reliable cuz its prediction can be heavily impacted by the lead bias covered in training data."", 'To demote the lead bias learned by the summarizer and encourage it to focus more on the content semantics, we develop an alternating adversarial learning method with a position prediction component as adversary, and optimize it along with the summarizer in an alternating manner. https://t.co/9zHGSdwMge', ""Experiments show that our proposal improves the summarizer‚Äôs generality on out-of-distribution data, with little to no performance loss on in-distribution data. It's noteworthy that our proposal is model-independent and only requires one type of news dataset as training input. https://t.co/qyW6AbHGP7""]",https://arxiv.org/abs/2105.14241,"In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers, severely limiting their performance on data with different or even no bias. In this paper, we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model's learned lead bias and improve its generality on out-of-distribution data, with little to no performance loss on in-distribution data. ","Demoting the Lead Bias in News Summarization via Alternating Adversarial
  Learning"
23,1399434343148396550,90655851,David Picard,"['Cool new paper with Victor Besnier and Alexandre Briot. We learn to predict uncertainty by (self-sup) training 2 CNNs to output diverging distributions.\n""Learning Uncertainty For Safety-Oriented Semantic Segmentation In Autonomous Driving""\narxiv: <LINK> <LINK>']",https://arxiv.org/abs/2105.13688v1,"In this paper, we show how uncertainty estimation can be leveraged to enable safety critical image segmentation in autonomous driving, by triggering a fallback behavior if a target accuracy cannot be guaranteed. We introduce a new uncertainty measure based on disagreeing predictions as measured by a dissimilarity function. We propose to estimate this dissimilarity by training a deep neural architecture in parallel to the task-specific network. It allows this observer to be dedicated to the uncertainty estimation, and let the task-specific network make predictions. We propose to use self-supervision to train the observer, which implies that our method does not require additional training data. We show experimentally that our proposed approach is much less computationally intensive at inference time than competing methods (e.g. MCDropout), while delivering better results on safety-oriented evaluation metrics on the CamVid dataset, especially in the case of glare artifacts. ","] Learning Uncertainty For Safety-Oriented Semantic Segmentation In
  Autonomous Driving"
24,1399321796000849922,428355146,Karin Sevegnani,"[""You can find the pre-print for the OTTers paper here: <LINK>\nThis work introduces a new task: how to transition between topics in an open-domain conversation.\nWe consider as 'topic' the entity mentioned in each utterance, which is further grounded on a 1/3 <LINK>"", 'commonsense knowledge graph.\nThe goal of the task is to generate a transition utterance that shifts topics by mentioning a connecting entity.\nWe present a dataset for the task (https://t.co/mWEvfPYc0W) and tested a couple of SOTA models to see how they perform for such 2/3', 'a task. Finally, we analyze possible improvements that could be used to achieve better performance. 3/3']",https://arxiv.org/abs/2105.13710,"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a ""bridging"" utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we call OTTers. We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. ",OTTers: One-turn Topic Transitions for Open-Domain Dialogue
25,1399318979244707842,563821235,Verena Rieser,['Most open-domain #ConvAI systems are purely reactive. How can a system introduce new topics without sounding abrupt or incoherent? Check-out our new paper to appear @aclmeeting with @KarinSevegnani @sinantie @_dmh  <LINK>'],https://arxiv.org/abs/2105.13710,"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a ""bridging"" utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we call OTTers. We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. ",OTTers: One-turn Topic Transitions for Open-Domain Dialogue
26,1399130286835781632,77815209,Brian Skinner,"[""1/ The hard part of being a researcher is penetrating the dense layer of jargon required to properly pose a problem.\n\nBut once you're there you get the fun part: the logic puzzle or game waiting to be solved\n\nüßµ about the game at the heart of our new paper\n<LINK>"", '2/ Here\'s the game:\nImagine that you have a finite 1D grid, and at each site in the grid you can place a ""charge"" of either 0, +1, or -1.\n\nOnce you\'ve made an initial placement, the charges evolve in the following way ...', '3/\n1. Randomly pick a set of three neighboring sites \n2. Alter the charges on those 3 sites in some (randomly-chosen) way, but you must preserves the total charge and the total ""dipole moment"" P, where\nP = sum( position * charge )\n3. Repeats steps 1-2 many times', '4/ The interesting question is: what is the average charge on a given site after a long time?\n\n(This question is secretly a question about many-particle quantum mechanics. But that\'s the less fun ""dense jargon"" part of the project)', '5/ I\'ll spare you a little thinking and tell you that there are only four operations allowed by the ""preserve charge and dipole moment"" rule. Here they are: https://t.co/5aLIP3GJbF', ""6/ The answer to the interesting question (where is the charge after a long time?) depends on the initial state.\n\nHere's a simple one to start thinking about: \n... 0000+0000 ...\n(all 0's except for one + somewhere in the middle)"", '7/ And here\'s what you get when you go through the time evolution.\n\nThat initial charge ""spreads out"" at first, but then after a long time it sticks (and remains stuck) to the very edges of the grid!\n\nCan you figure out why? https://t.co/YhbB0OL0iW', '8/ For us, the key was to stop thinking about the charge itself, and to start thinking about the _integral_ of the charge.\nThat is, define a ""height field"" h(x), such that h(x) corresponds to the sum of all charges to the left of point x.', '9/ The two end points of the height field are always fixed:\nh(0) = 0\nand h(last site) = (total charge) = 1\n\nThe charge, correspondingly, is the (discrete) derivative of the height field:\ncharge(x) = h(x+1) - h(x)', '10/ In this language, + charges are upward jumps in the height field, and - charges are downward jumps.\n\nBut notice from the last few rows of this table that every legal operation can be described in a simple visual way:\nit\'s a ""block"" sliding either left or right by one space https://t.co/hCXoHMukK0', ""11/ So now the game is simple: put down a bunch of blocks on the right, then randomly start sliding them around.\n(Under the constraint that blocks can't slide through each other) https://t.co/JhyqHtTlhh"", ""12/ It's pretty clear that the blocks get uniformly distributed around the system after a long time. So the average height field takes a constant value, and it only jumps at the two edges [h(0)=0, h(end)=1].\nErgo, all the charge is concentrated in two peaks at the end points. üòé"", '13/ This language of the ""game of sliding blocks"" generalizes pretty nicely to figuring out lots of different initial states (for example, any state that has alternating + and - charges can similarly be described as a single row of sliding blocks).', ""14/ But the case of an initial state ...00+00...00+00... is different, and more interesting. Here's what you get: https://t.co/l8dqdzdplH"", ""15/ In this case the height field has two levels rather than one. If you think about it enough you can figure out how the game works, and why there's that peak in charge density in the middle.\n\nBut I'll leave you to ponder it yourself, or to read the paper if you're curious. https://t.co/O1qVDuwgVD"", '16/16 This is what I love about my job. No matter how far down the rabbit hole of technical and specialized jargon I progress, each piece of research always involves posing and solving a simple kind of puzzle or game.\n\nPretty happy that I get to spend my life playing.', ""Congratulations to Xiaozhou Feng for a very nice piece of work.\n\nHere's the link again:\nhttps://t.co/mbxLlRYb6U"", '@BSeradjeh It looks nice (and also smells like KPZ?)\nBut I have to say that your graphic design sensibilities have improved quite a bit in the last 20 years. :)']",https://arxiv.org/abs/2105.11465,"Fracton systems exhibit restricted mobility of their excitations due to the presence of higher-order conservation laws. Here we study the time evolution of a one-dimensional fracton system with charge and dipole moment conservation using a random unitary circuit description. Previous work has shown that when the random unitary operators act on four or more sites, an arbitrary initial state eventually thermalizes via a universal subdiffusive dynamics. In contrast, a system evolving under three-site gates fails to thermalize due to strong ""fragmentation"" of the Hilbert space. Here we show that three-site gate dynamics causes a given initial state to evolve toward a highly nonthermal state on a time scale consistent with Brownian diffusion. Strikingly, the dynamics produces an effective attraction between isolated fractons or between a single fracton and the boundaries of the system, in analogy with the Casimir effect in quantum electrodynamics. We show how this attraction can be understood by exact mapping to a simple classical statistical mechanics problem, which we solve exactly for the case of an initial state with either one or two fractons. ","Hilbert space fragmentation produces a ""fracton Casimir effect"""
27,1398625287122964482,1047659881,Dario Izzo,"['Introducing GeodesyNets, a new #DeepLearning model learning the density distribution of irregular celestial from their gravitational signature. \nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/2105.13031,"We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity. ",Geodesy of irregular small bodies via neural density fields: geodesyNets
28,1398348151661211648,1184198482455945218,Yuber F Perez-G,"['New paper day üòÄ! Thanks to @ivanj_ms and Manibrata for the excellent collaboration! \n\nWe explored the possibility that neutrinos are pseudo-Dirac particles by looking at the data from the Supernova SN1987A. Please, take a look\n\n<LINK>\n\n1/n', 'But, how are we testing the pseudo-Dirac scenario? Well, if neutrinos are pseudo-Dirac, it means that they are, in fact, Majorana but behave in all practical aspects as Dirac... However, oscillations between active and sterile states, spanning huge distances, are possible\n\n2/n', 'Searching for tiny mass splittings is rather hard; we would need low energy neutrinos with large baselines \n\n3/n https://t.co/mJjHFXp9oW', 'So, one of the best candidates to test the Pseudo-Dirac hypothesis are supernova neutrinos... and we saw neutrinos in 1987 (before any of us was born üòÜ) from a supernova in the Large Magellanic Cloud\n\n4/n', 'We fitted the data and found a mild preference for oscillations... also, and maybe more interestingly, we are able to put one of the first constraints in tiny mass differences, as low as 2 * 10^-19 eV^2\n\n5/n https://t.co/gWrSo3GqEF', 'If we observe a supernova in the future, we expect to have a huge number of neutrino events. So, we computed the sensitivity of future experiments like DUNE and HK, finding that they can explore a large portion of the parameter space. \n\n6/n https://t.co/zx03Y6ArIp', 'Perhaps, some supernova neutrinos are already on their way to the Earth, expecting to be detected! So, we just need to wait patiently for the next galactic supernova üòÄ\n\n7/n']",https://arxiv.org/abs/2105.12736,"Ever since the discovery of neutrinos, one question has daunted us, are neutrinos their own antiparticles? One remarkable possibility is that neutrinos have a pseudo-Dirac nature, truly Majorana neutrinos which behave, for all practical purposes, as Dirac fermions, only distinguishable by tiny mass-squared differences. Such mass differences would induce oscillations that could only be conspicuous over astrophysical baselines. We analyze the neutrino data from SN1987A in the light of these active-sterile oscillations and find a mild preference ($\Delta\chi^2\approx 3$) for a non-zero quadratic mass difference $\delta m^2=6.31\times 10^{-20}~{\rm eV}^2$. Notably, the same data is able to exclude $\delta m^2\sim [2.55,3.01]\times 10^{-20}~{\rm eV}^2$ with $\Delta\chi^2> 9$, the tiniest mass differences constrained so far. We further consider the future sensitivity of next-generation experiments like the Deep Underground Neutrino Experiment (DUNE) and Hyper-Kamiokande (HK) and demonstrate that, for a future galactic SN occurring at $10~{\rm kpc}$, mass-squared differences as small as $\sim 10^{-20}~{\rm eV}^2$ could be explored. ",SN1987A still shining: A Quest for Pseudo-Dirac Neutrinos
29,1398315622954582021,610427323,Desika Narayanan,"['Hey Astro-verse - today we have a new paper up led by awesome undergraduate @hollisakins (with other notable notables like @astrowhit, @RBezanson, @es_em_ell and others) on how quenching galaxies evolve in UVJ color-color space!\n\n<LINK>\n\n[1/', ""Hollis came to us from @GrinnellCollege as a part of our @NSF funded REU program at UF in 2020, and participated in our first year of REU operations in a *fully remote environment*  (i.e, I've only ever met Hollis on zoom!). \n\n[2/"", ""In this theoretical study, we paired cosmological galaxy formation simulations with dust radiative transfer methods we've developed to simulate their mock observable colors [3/"", 'Observationally, @astrowhit and many other observers have determined that you can separate galaxies into star-forming and quenched based on their U, V and J colors. This pedagogical example shows how our model galaxy mock images and SEDs appear in UVJ space [4/ https://t.co/gmCkttOZSq', ""Because these are fake galaxies, we have the ability to create all sorts of universes -- ones where there's no dust, ones where stars are binaries (shout out to @astro_jje ), ones where stars rotate etc.  \n\n[5/]"", 'All in all, we find that the simba cosmological simulation is able to reproduce the bulk distributions of galaxies in UVJ space.  This said, like most others, we have a ton of trouble producing dusty enough galaxies (the top right of the diagram.)   Why is this?\n\n[6/]', 'It turns out that for the most massive galaxies in our simulations, attenuation curves begin to flatten, and we have relatively little reddening for these massive systems.   These complexities in the attenuation law blur inferred observational trends (i.e. with sSFR):\n\n[7/] https://t.co/VCTrIe7KRU', 'Maybe most interestingly: we find no correlation between quenching timescale and the path a galaxy takes in UVJ space.  This evolution is almost entirely dependent on the time since the most recent burst of star formation. \n\n[8/', 'Stay tuned as our group dives further and further into the wild and wooly world of quenched galaxies!  In the mean time, shout out to the @NSF REU program for funding this research!\n\n[9/9]', ""Also to add: there's amazing work by @SPACEbeverage and company on massive galaxies up today as well!\n\nhttps://t.co/Tcrz1WCxPI""]",https://arxiv.org/abs/2105.12748,"Over the past decade, rest-frame color-color diagrams have become popular tools for selecting quiescent galaxies at high redshift, breaking the color degeneracy between quiescent and dust-reddened star-forming galaxies. In this work, we study one such color-color selection tool -- the rest-frame $U-V$ vs. $V-J$ diagram -- by employing mock observations of cosmological galaxy formation simulations. In particular, we conduct numerical experiments assessing both trends in galaxy properties in UVJ space and the color-color evolution of massive galaxies as they quench at redshifts $z\sim 1$--$2$. We find that our models broadly reproduce the observed UVJ diagram at $z=1$--$2$, including (for the first time in a cosmological simulation) reproducing the population of extremely dust-reddened galaxies in the top right of the UVJ diagram. However, our models primarily populate this region with low-mass galaxies and do not produce as clear a bimodality between star-forming and quiescent galaxies as is seen in observations. The former issue is due to an excess of dust in low-mass galaxies and relatively gray attenuation curves in high-mass galaxies, while the latter is due to the overpopulation of the green valley in SIMBA. When investigating the time evolution of galaxies on the UVJ diagram, we find that the quenching pathway on the UVJ diagram is independent of the quenching timescale, and instead dependent primarily on the average specific star formation rate in the 1 Gyr prior to the onset of quenching. Our results support the interpretation of different quenching pathways as corresponding to the divergent evolution of post-starburst and green valley galaxies. ",Quenching and the UVJ diagram in the SIMBA cosmological simulation
30,1398300354807226370,750727628294848512,Malena Rice,"['Exciting new coauthor paper on arxiv today, led by Songhu Wang. High obliquities of giant exoplanet host stars are almost exclusively associated with wide-separation planets (a/R* &gt; 10) or hot stars, providing evidence for tidal damping of hot Jupiters <LINK>']",https://arxiv.org/abs/2105.12902,"Measuring the obliquity distribution of stars hosting warm Jupiters may help us to understand the formation of close-orbiting gas giants. Few such measurements have been performed due to practical difficulties in scheduling observations of the relatively infrequent and long-duration transits of warm Jupiters. Here, we report a measurement of the Rossiter-McLaughlin effect for K2-232b, a warm Jupiter (M_P=0.39 M_Jup) on an 11.17-day orbit with an eccentricity of 0.26. The data were obtained with the Automated Planet Finder during two separate transits. The planet's orbit appears to be well-aligned with the spin axis of the host star, with a projected spin-orbit angle of lambda = -11.1+/-6.6 deg. Combined with the other available data, we find that high obliquities are almost exclusively associated with planets that either have an orbital separation greater than 10 stellar radii or orbit stars with effective temperatures hotter than 6,000K. This pattern suggests that the obliquities of the closest-orbiting giant planets around cooler stars have been damped by tidal effects. ",The Aligned Orbit of the Eccentric Warm Jupiter K2-232b
31,1398037941532778497,501353692,Travis Metcalfe,"['PAPER DAY! Work led by Tim Brown identifies a new proxy for magnetic activity (the Mid-Frequency Continuum) that shows a familiar dependence on Rossby number. Easy to measure from Kepler/K2/TESS time series, and probably related to supergranulation. <LINK> <LINK>']",https://arxiv.org/abs/2105.12231,"We analyze space-based time series photometry of Sun-like stars, mostly in the Pleiades, but also field stars and the Sun itself. We focus on timescales between roughly 1 hour and 1 day. In the corresponding frequency band these stars display brightness fluctuations with a decreasing power-law continuous spectrum. K2 and Kepler observations show that the RMS flicker due to this Mid-Frequency Continuum (MFC) can reach almost 1%, approaching the modulation amplitude from active regions. The MFC amplitude varies by a factor up to 40 among Pleiades members with similar Teff, depending mainly on the stellar Rossby number Ro. For Ro<0.04, the mean amplitude is roughly constant at about 0.4%; at larger Ro the amplitude decreases rapidly, shrinking by about two orders of magnitude for Ro~1. Among stars, the MFC amplitude correlates poorly with that of modulation from rotating active regions. Among field stars observed for 3 years by Kepler, the quarterly average modulation amplitudes from active regions are much more time-variable than the quarterly MFC amplitudes. We argue that the process causing the MFC is largely magnetic in nature, and that its power-law spectrum comes from magnetic processes distinct from the star's global dynamo, with shorter timescales. By analogy with solar phenomena, we hypothesize that the MFC arises from a (sometimes energetic) variant of the solar magnetic network, perhaps combined with rotation-related changes in the morphology of supergranules. ","Brightness Fluctuation Spectra of Sun-Like Stars. I. The Mid-Frequency
  Continuum"
32,1397908669501632519,1220766363549126656,Alain Rossier,"['New paper ""Scaling Properties of Deep Residual Networks"" has been accepted for publication as a conference paper at ICML 2021! @alainsamjr @icmlconf \nCollaboration with @instadeepai \n<LINK>\n\n1/n <LINK>', 'As you may know, residual network (ResNet) is a rockstar among Deep Learning architectures. It leads to huge improvements over feedforward networks for computer vision and speech recognition.\n\n2/n https://t.co/B5yKkhLeDh', 'The forward rule of a ResNet (top) looks oddly familiar to the Euler scheme of a differential equation (bottom). This insight culminated in the now famous Neural ODE model @RickyTQChen that uses various ODE schemes to train their model.\n\n3/n https://t.co/bbMbF8Gawp', 'For those of you familiar with numerical scheme of ODEs, the link between the two equations only holds under the particular conditions below... and they may not be satisfied in practice.\n\n4/n https://t.co/Sx6VEtn26E', 'We introduce a more general framework for the trained weights of ResNets that admits the Neural ODE assumptions as a special case. We test the framework on synthetic and standardized datasets (coming at you, CIFAR 10).\n\n5/n', ""We found that depending on the activation function, the shape of the trained weights for an absurdly deep network (L ~ 10'000) looks drastically different!\nleft: sigma=tanh;  right: sigma=relu\n\n6/n https://t.co/otS7NZlW2R"", ""For those of you who know about stochastic calculus, the image on the right looks pretty much like white noise. So let's integrate it... We get a process that looks oddly familiar with a diffusion process (think: Brownian motion)!\n\n7/n https://t.co/2OAta45u0R"", 'In our paper, we lay down two general assumptions that describe the behaviour of trained weights of ResNets. We prove the convergence of their limiting equations, which can be either the Neural ODE, a linear ODE or a linear-quadratic SDE (the interesting stuff below).\n\n8/n https://t.co/Lg0W74Xs86', 'What‚Äôs the point of all this? Well, knowing the scaling limit of trained weights will help you choose your initialization strategy, as well as directly train the scaling limit in the Neural ODE fashion.\n\n9/n', 'Is this the end of the story? Of course not! Our results hold for fully-connected networks, but we observed that convolutional networks have different scaling behaviour (sparse limit?)... stay tuned for the next episode!\nhttps://t.co/9PXGsuObLp\n\n10/10, like this thread']",https://arxiv.org/abs/2105.12245,"Residual networks (ResNets) have displayed impressive results in pattern recognition and, recently, have garnered considerable theoretical interest due to a perceived link with neural ordinary differential equations (neural ODEs). This link relies on the convergence of network weights to a smooth function as the number of layers increases. We investigate the properties of weights trained by stochastic gradient descent and their scaling with network depth through detailed numerical experiments. We observe the existence of scaling regimes markedly different from those assumed in neural ODE literature. Depending on certain features of the network architecture, such as the smoothness of the activation function, one may obtain an alternative ODE limit, a stochastic differential equation or neither of these. These findings cast doubts on the validity of the neural ODE model as an adequate asymptotic description of deep ResNets and point to an alternative class of differential equations as a better description of the deep network limit. ",Scaling Properties of Deep Residual Networks
33,1397840987913863169,1061375141274439685,Xabier Cid Vidal üõ∞Ô∏è,"['So, when you publish at a certain rate per year the excitement of each new paper decreases to some extent‚Ä¶ However, I‚Äôm super excited this time, we‚Äôre unleashing the full power of LHCb to probe Stealth New Physics!\n<LINK>\n(thread)', 'It\'s been a while since several of us at LHCb realised that our wonderful detector could be exploited much more, providing sensitivity to physics beyond the SM in many unexpected ways. We encompassed all this signatures with the tag ""Stealth physics""', 'This refers to dynamics beyond the SM, not including searches that focus on energetic objects or precision measurements of known processes. Examples of Stealth physics include  long-lived particles and light resonances  produced very rarely or together with huge backgrounds', ""So, @MartinoBorsato, Yuhsin Tsai, @CharlesTheVS, Jose Zurita and me came to the conclusion that we needed to make an effort to collect all the theory and experimental efforts in this regard in a single place. It's been quite a ride, but truly worth it!"", ""The first step was organizing a workshop in Santiago last year (https://t.co/YVar3Fw8S3). It was the last one before the pandemics, and looking at the outcome, I'm convinced it was very useful, setting the stage for what it was about to come."", 'Now we have our document in the arxiv (hopefully to be published soon), as a useful guide for theorists and experimentalists who want to tackle Stealth physics at LHCb. There are more ideas that could have probably been included,', ""but I ensure you'll find plenty of exciting scientific ideas. It is a very nice effort, with a lot of work and a powerful list of authors behind it, which to be honest was a pleasure to coordinate on our side. Now, we really hope you enjoy the read!""]",https://arxiv.org/abs/2105.12668,"In this paper, we describe the potential of the LHCb experiment to detect Stealth physics. This refers to dynamics beyond the Standard Model that would elude searches that focus on energetic objects or precision measurements of known processes. Stealth signatures include long-lived particles and light resonances that are produced very rarely or together with overwhelming backgrounds. We will discuss why LHCb is equipped to discover this kind of physics at the Large Hadron Collider and provide examples of well-motivated theoretical models that can be probed with great detail at the experiment. ",Unleashing the full power of LHCb to probe Stealth New Physics
34,1397817330986459137,762420558,Ciaran O'Hare,"['New paper out yesterday: I have continued my (so far) career-long obsession with directional detection by studying in detail one of craziest concepts proposed a few years ago by Drukier, Freese and others: detecting particles with DNA <LINK> <LINK>', 'Turns out it might not be that crazy after all and there are many interesting avenues for future experimental work. https://t.co/gyKAojPEXr', ""I should say that this study wouldn't have been possible without the work of several excellent @sydney_physics undergraduates""]",https://arxiv.org/abs/2105.11949,"We present the first proof-of-concept simulations of detectors using biomaterials to detect particle interactions. The essential idea behind a ""DNA detector"" involves the attachment of a forest of precisely-sequenced single or double-stranded nucleic acids from a thin holding layer made of a high-density material. Incoming particles break a series of strands along a roughly co-linear chain of interaction sites and the severed segments then fall to a collection area. Since the sequences of base pairs in nucleic acid molecules can be precisely amplified and measured using polymerase chain reaction (PCR), the original spatial position of each broken strand inside the detector can be reconstructed with nm precision. Motivated by the potential use as a low-energy directional particle tracker, we perform the first Monte Carlo simulations of particle interactions inside a DNA detector. We compare the track topology as a function of incoming direction, energy, and particle type for a range of ionising particles. While particle identification and energy reconstruction might be challenging without a significant scale-up, the excellent potential angular and spatial resolution ($\lesssim 25^\circ$ axial resolution for a keV-scale particles and nm-scale track segments) are clear advantages of this concept. We conclude that a DNA detector could be a cost-effective, portable, and powerful new particle detection technology. We outline the outstanding experimental challenges, and suggest directions for future laboratory tests. ",Particle detection and tracking with DNA
35,1397720678107803649,1119656379243884545,Yuichiro Mori,"[""Our new paper \n'Weak value amplification and the lifetime of decaying particle‚Äô is released.\n\n<LINK>""]",https://arxiv.org/abs/2105.12349,"We study the possibility of varying the measured lifetime of a decaying particle based on the technique of weak value amplification in which an additional filtering process called postselection is performed. Our analysis made in a direct measurement scheme presented here shows that, for simple two-level systems, the lifetime may be prolonged more than three times compared to the original one, while it can also be shortened arbitrarily by a proper choice of postselection. This result is consistent with our previous analysis on the possible prolongation of the lifetime of B mesons that may be observed in laboratories, and suggests room for novel applications of weak value amplification beyond precision measurement conventionally considered. ",Weak value amplification and the lifetime of decaying particle
36,1397600373372772357,219545939,Zoe Zontou,"[""New science alert! Everyone should check out my awesome co-supervisor's (@astro_pwise) paper on SNe Ia rates &amp; delay times in @theDESurvey ! So excited to be a part of this (my first co-authored paper!!!) with the rest of our stellar group @UoS_SNe! ‚ú®\n\n<LINK>""]",https://arxiv.org/abs/2105.11954,"We use a sample of 809 photometrically classified type Ia supernovae (SNe Ia) discovered by the Dark Energy Survey (DES) along with 40415 field galaxies to calculate the rate of SNe Ia per galaxy in the redshift range $0.2 < z <0.6$. We recover the known correlation between SN Ia rate and galaxy stellar mass across a broad range of scales $8.5 \leq \log(M_*/\mathrm{M}_{\odot}) \leq 11.25$. We find that the SN Ia rate increases with stellar mass as a power-law with index $0.63 \pm 0.02$, which is consistent with previous work. We use an empirical model of stellar mass assembly to estimate the average star-formation histories (SFHs) of galaxies across the stellar mass range of our measurement. Combining the modelled SFHs with the SN Ia rates to estimate constraints on the SN Ia delay time distribution (DTD), we find the data are fit well by a power-law DTD with slope index $\beta = -1.13 \pm 0.05$ and normalisation $A = 2.11 \pm0.05 \times 10^{-13}~\mathrm{SNe}~{\mathrm{M}_{\odot}}^{-1}~\mathrm{yr}^{-1}$, which corresponds to an overall SN Ia production efficiency $N_{\mathrm{Ia}}/M_* = 0.9~_{-0.7}^{+4.0} \times 10^{-3}~\mathrm{SNe}~\mathrm{M}_{\odot}^{-1}$. Upon splitting the SN sample by properties of the light curves, we find a strong dependence on DTD slope with the SN decline rate, with slower-declining SNe exhibiting a steeper DTD slope. We interpret this as a result of a relationship between intrinsic luminosity and progenitor age, and explore the implications of the result in the context of SN Ia progenitors. ",Rates and delay times of type Ia supernovae in the Dark Energy Survey
37,1397572145178365955,77234797,Jesper Agdakx,"[""I've been working working with Artem Shinkarov on a new paper on how to use Agda's reflection machinery to extract shallowly encodings from Agda. You can get it from Arxiv at <LINK>. Any comments or suggestions you have would be very welcome!""]",https://arxiv.org/abs/2105.10819,"Dependently-typed host languages empower users to verify a wide range of properties of embedded languages and programs written in them. Designers of such embedded languages are faced with a difficult choice between using a shallow or a deep embedding. The former is easier to use because the entire infrastructure of the host langauge is immediately available. Meanwhile, the latter gives full access to the structure of embedded programs, but is difficult to use in practice, especially when the embedded language is itself dependently typed. The main insight presented in this paper is that the choice between shallow and deep embedding can be eliminated by working in a host language with reflection capabilities: we start from a shallow embedding that can use all libraries and tools of the host language, and later use reflection to expose the deep structure of the embedded programs. Concretely, we apply this technique to embed three programming languages -- Kaleidoscope, SaC, and (a subset of) APL -- into the dependently typed theorem prover Agda, using dependent types to statically enforce several properties of interest. We then use Agda's reflection capabilities to extract the embedded programs back into the original language, so that the existing toolchain can be leveraged. In this process, statically verified properties of the host language are mapped onto runtime checks in the target language, allowing extracted programs to interact safely with existing code. Finally, we demonstrate the feasibility of our approach with the implementation and extraction of a convolutional neural network in our embedding of APL.\@ ","Choosing is Losing: How to combine the benefits of shallow and deep
  embeddings through reflection"
38,1397534289324134402,4871769136,Michael Franke,"['Interested in learning about probabilistic pragmatics?\n\nWe have a new paper to accompany the interactive web-book <LINK> (w/ Greg Scontras &amp; @mhtessler):\n<LINK>\n\nPaper adds overview for web-book, technical details, literature review etc.']",https://arxiv.org/abs/2105.09867,"Recent advances in computational cognitive science (i.e., simulation-based probabilistic programs) have paved the way for significant progress in formal, implementable models of pragmatics. Rather than describing a pragmatic reasoning process in prose, these models formalize and implement one, deriving both qualitative and quantitative predictions of human behavior -- predictions that consistently prove correct, demonstrating the viability and value of the framework. The current paper provides a practical introduction to and critical assessment of the Bayesian Rational Speech Act modeling framework, unpacking theoretical foundations, exploring technological innovations, and drawing connections to issues beyond current applications. ",A practical introduction to the Rational Speech Act modeling framework
39,1397499375430082560,890595415669837824,Luan M. Ver√≠ssimo,"['Check out our new  paper x)\n\nTangential finite-size scaling of the Gaussian topological transition in the quantum spin-1 anisotropic chain\n\n<LINK>', '@marimonteiro04 O Marcelo sabe escolher bem os t√≠tulos hahaha']",https://arxiv.org/abs/2105.11846,"Scaling aspects of Gaussian topological phase-transitions in quantum spin chains are investigated using the prototypical one-dimensional spin-1 XXZ Heisenberg model with uniaxial single-ion anisotropy $D$. This model presents a critical line separating the gaped Haldane and large-$D$ phases, with the relevant energy gap closing at the transition point. We show that a proper tangential finite-size scaling analysis is able to accurately locate the Gaussian critical line and to probe the continuously varying set of correlation length critical exponents. The specific features of the tangential scaling are highlighted in contrast with the standard scaling holding in the Ising-like transition between the gapless AF-N\'eel and gaped Haldane phases. Our results are compared with field-theoretic predictions and available high-accuracy data for specific points along the Gaussian line. ","Tangential finite-size scaling of the Gaussian topological transition in
  the quantum spin-1 anisotropic chain"
40,1397475649716166656,67936420,Rahul,"['Super excited to share our new @aclmeeting paper on promoting faithfulness and diversity in abstractive summarization. <LINK> (1/4)', 'We introduce Focus Attention Mechanism (FAMe) which performs source-side planning and biases the decoder to focus more on tokens that are supported by or are topically similar to the input document. (2/4) https://t.co/IILRDWa8gd', 'This leads to summaries that are more faithful to the input when compared to other SOTA models. We also introduce Focus Sampling as an alternative to top-k and nucleus sampling which enhances the diversity of the generated summaries while maintaining its faithfulness. (3/4) https://t.co/1w40L0Clo8', 'This is joint work with Shashi Narayan, Joshua Maynez, Sascha Rothe, and Ryan McDonald done during my internship at @GoogleAI (4/4)']",https://arxiv.org/abs/2105.11921,"Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on \rouge and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-$k$ or nucleus sampling-based decoding methods. ",Focus Attention: Promoting Faithfulness and Diversity in Summarization
41,1397292339907993601,1556664198,Kyle Cranmer,"['New paper!\nIt was presented by Sebastian at vCHEP last week &amp; it gives an overview for physicists of several recent works where we reframe jet physics in probabilistic terms &amp; connect to RL, hierarchical clustering, etc. @NYUPhysics @NYUDataScience\n<LINK> <LINK>', 'I wrote about these connections before in this tweet thread about the work using Reinforcement Learning\nhttps://t.co/6JeKWBOoEB', 'and also here when talking about probabilistic treatment of hierarchical clustering \nhttps://t.co/tUPmai5ogy', 'This paper also includes work by Matthew Drnevich using probabilistic programming to guide/control the parton shower so that it produces jets that satisfy some condition. Here we give a simple example, but could be used to efficiently populate signal-like regions of phase space. https://t.co/Y0a1m2RVWT']",https://arxiv.org/abs/2105.10512,"We reframe common tasks in jet physics in probabilistic terms, including jet reconstruction, Monte Carlo tuning, matrix element - parton shower matching for large jet multiplicity, and efficient event generation of jets in complex, signal-like regions of phase space. We also introduce Ginkgo, a simplified, generative model for jets, that facilitates research into these tasks with techniques from statistics, machine learning, and combinatorial optimization. We review some of the recent research in this direction that has been enabled with Ginkgo. We show how probabilistic programming can be used to efficiently sample the showering process, how a novel trellis algorithm can be used to efficiently marginalize over the enormous number of clustering histories for the same observed particles, and how dynamic programming, A* search, and reinforcement learning can be used to find the maximum likelihood clustering in this enormous search space. This work builds bridges with work in hierarchical clustering, statistics, combinatorial optmization, and reinforcement learning. ",Reframing Jet Physics with New Computational Methods
42,1397239818636169224,1243544508983279617,Horng Sheng Chia,['New paper led by Javier!\n\n<LINK>\n\nWe found no evidence for binary black holes with spins that are anti-aligned with the orbit in the current BBH population. This suggests that BBHs cannot be formed solely through dynamical capture in dense stellar environments. <LINK>'],https://arxiv.org/abs/2105.10580,"The distribution of effective spin $\chi_{\rm eff}$, a parameter that encodes the degree of spin-orbit alignment in a binary system, has been widely regarded as a robust discriminator between the isolated and dynamical formation pathways for merging binary black holes. Until the recent release of the GWTC-2 catalog, such tests have yielded inconclusive results due to the small number of events with measurable nonzero spins. In this work, we study the $\chi_{\rm eff}$ distribution of the binary black holes detected in the LIGO-Virgo O1-O3a observing runs. Our focus is on the degree to which the $\chi_{\rm eff}$ distribution is symmetric about $\chi_{\rm eff} = 0$ and whether the data provides support for a population of negative-$\chi_{\rm eff}$ systems. We find that the $\chi_{\rm eff}$ distribution is asymmetric at 95% credibility, with an excess of aligned-spin binary systems ($\chi_{\rm eff}>0$) over anti-aligned ones. Moreover, we find that there is no evidence for negative-$\chi_{\rm eff}$ systems in the current population of binary black holes. Thus, based solely on the $\chi_{\rm eff}$ distribution, dynamical formation is disfavored as being responsible for the entirety of the observed merging binary black holes, while isolated formation remains viable. We also study the mass distribution of the current binary black hole population, confirming that a single truncated power law distribution in the primary source-frame mass, $m_1^{\rm src}$, fails to describe the observations. Instead, we find that the preferred models have a steep feature at $m_1^{\rm src} \sim 40 \,\rm M_\odot$ consistent with a step and an extended, shallow tail to high masses. ","Distribution of Effective Spins and Masses of Binary Black Holes from
  the LIGO and Virgo O1-O3a Observing Runs"
43,1397238218110685189,113264888,Aaswath Raman,"[""New preprint with perhaps the best title of a paper I've been on: Relativistic lightsails need to billow. Led by Matthew Campbell and Igor Bargatin, with contributions from John Brewer in our group and \n@deep29jariwala. Funding from Breakthrough Starshot. <LINK> <LINK>""]",https://arxiv.org/abs/2105.10849,"We argue that light sails that are rapidly accelerated to relativistic velocities by lasers must be significantly curved in order to reduce their mechanical stresses and avoid tears. Using an integrated opto-thermo-mechanical model, we show that the diameter and radius of curvature of a circular light sail should be comparable in magnitude, both on the order of a few meters in optimal designs for gram-scale payloads. Moreover, when sufficient laser power is available, a sail's acceleration length decreases and its chip payload capacity increases as its curvature increases. Our findings provide guidance for emerging light sail design programs, which herald a new era of interstellar space exploration. ",Relativistic light sails need to billow
44,1397097306617430018,17239073,Atabey Kaygun,"['New paper on the arXiv: ""Birational Equivalences and Kac-Moody Algebras"" <LINK>']",https://arxiv.org/abs/2105.11360,We show that every Kac-Moody algebra is birationally equivalent to a smash biproduct of two copies of a Weyl algebra together with a polynomial algebra. We also show that the same is true for quantized Kac-Moody algebras where one replaces Weyl algebras with their quantum analogues. ,Birational Equivalences and Kac-Moody Algebras
45,1396999496350027777,1929084218,Lev Guzman,"[""Interested in multifractality of complex time series?. Our new paper about a generalization of Higuchi's fractal dimension is out on @arxiv_org. Great collab with Carlos Carrizales and Reik Donner.  <LINK>""]",https://arxiv.org/abs/2105.11055,"We introduce a generalization of Higuchi's estimator of the fractal dimension as a new way to characterize the multifractal spectrum of univariate time series. The resulting multifractal Higuchi dimension analysis (MF-HDA) method considers the order-$q$ moments of the partition function provided by the length of the time series graph at different levels of subsampling. The results obtained for different types of stochastic processes as well as real-world examples of word length series from fictional texts demonstrate that MF-HDA provides a reliable estimate of the multifractal spectrum already for moderate time series lengths. Practical advantages as well as disadvantages of the new approach as compared to other state-of-the-art methods of multifractal analysis are discussed, highlighting the particular potentials of MF-HDA to distinguish mono- from multi-fractal dynamics based on relatively short time series. ","Generalization of Higuchi's fractal dimension for multifractal analysis
  of time series with limited length"
46,1396947295132950535,1371751675145416705,Robin Henry,"[""We have just released a new preprint in which we discuss how Gym-ANM can be used to facilitate collaboration between the reinforcement learning and power system research communities.\n\n+ it's a great teaching tool\n\nPaper: <LINK>\nGitHub: <LINK> <LINK>""]",https://arxiv.org/abs/2105.08846,"Gym-ANM is a Python package that facilitates the design of reinforcement learning (RL) environments that model active network management (ANM) tasks in electricity networks. Here, we describe how to implement new environments and how to write code to interact with pre-existing ones. We also provide an overview of ANM6-Easy, an environment designed to highlight common ANM challenges. Finally, we discuss the potential impact of Gym-ANM on the scientific community, both in terms of research and education. We hope this package will facilitate collaboration between the power system and RL communities in the search for algorithms to control future energy systems. ","Gym-ANM: Open-source software to leverage reinforcement learning for
  power system management in research and education"
47,1396918737320742914,132958051,Emily Sandford,"['New paper today with @david_kipping and Michael Collins! What if we thought of planets in their systems like words in a sentence? What information belongs to the arrangement of the planets, or in the relationship of each planet to its surrounding context? <LINK>']",https://arxiv.org/abs/2105.09966,"A planetary system consists of a host star and one or more planets, arranged into a particular configuration. Here, we consider what information belongs to the configuration, or ordering, of 4286 Kepler planets in their 3277 planetary systems. First, we train a neural network model to predict the radius and period of a planet based on the properties of its host star and the radii and period of its neighbors. The mean absolute error of the predictions of the trained model is a factor of 2.1 better than the MAE of the predictions of a naive model which draws randomly from dynamically allowable periods and radii. Second, we adapt a model used for unsupervised part-of-speech tagging in computational linguistics to investigate whether planets or planetary systems fall into natural categories with physically interpretable ""grammatical rules."" The model identifies two robust groups of planetary systems: (1) compact multi-planet systems and (2) systems around giant stars ($\log{g} \lesssim 4.0$), although the latter group is strongly sculpted by the selection bias of the transit method. These results reinforce the idea that planetary systems are not random sequences -- instead, as a population, they contain predictable patterns that can provide insight into the formation and evolution of planetary systems. ",On planetary systems as ordered sequences
48,1396912380039286791,2693638267,Arjun (Raj) Manrai,"['Deep learning approaches for semantic image segmentation are often data hungry or difficult to train. Thrilled to share our new approach called PixMatch, our #CVPR21 paper now on arXiv. Led by the singular @lukemelas:\n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'PixMatch is a new approach to unsupervised domain adaptation for semantic segmentation. It exploits the idea that in order to perform well on the target domain, a model‚Äôs output should be consistent with respect to small perturbations of inputs in the target domain.', ""Read more here: https://t.co/bMqw14GiAZ and keep an eye on @lukemelas üöÄ, a wonderful member of my group and a truly exceptional scientist. Check out some of @lukemelas' other work here: https://t.co/gFywTBOLsV\nhttps://t.co/epQJt3paZZ""]",https://arxiv.org/abs/2105.08128,"Unsupervised domain adaptation is a promising technique for semantic segmentation and other computer vision tasks for which large-scale data annotation is costly and time-consuming. In semantic segmentation, it is attractive to train models on annotated images from a simulated (source) domain and deploy them on real (target) domains. In this work, we present a novel framework for unsupervised domain adaptation based on the notion of target-domain consistency training. Intuitively, our work is based on the idea that in order to perform well on the target domain, a model's output should be consistent with respect to small perturbations of inputs in the target domain. Specifically, we introduce a new loss term to enforce pixelwise consistency between the model's predictions on a target image and a perturbed version of the same image. In comparison to popular adversarial adaptation methods, our approach is simpler, easier to implement, and more memory-efficient during training. Experiments and extensive ablation studies demonstrate that our simple approach achieves remarkably strong results on two challenging synthetic-to-real benchmarks, GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes. Code is available at: this https URL ","PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency
  Training"
49,1396734429029179392,2352149191,Mimmo Nardiello,"[""12th paper as 1st author, the 4th of the PATHOS series: <LINK>\nNew candidate exoplanets around open clusters' members, their frequency and Rp-age distribution. Results obtained thanks @NASA_TESS data. Light curves soon available at @MAST_News <LINK>"", 'Here you can find the light curves already published : https://t.co/tGwboKyTfg']",https://arxiv.org/abs/2105.09952,"The knowledge of the ages of stars hosting exoplanets allows us to obtain an overview on the evolution of exoplanets and understand the mechanisms affecting their life. The measurement of the ages of stars in the Galaxy is usually affected by large uncertainties. An exception are the stellar clusters: for their coeval members, born from the same molecular cloud, ages can be measured with extreme accuracy. In this context, the project PATHOS is providing candidate exoplanets orbiting members of stellar clusters and associations through the analysis of high-precision light curves obtained with cutting-edge tools. In this work, we exploited the data collected during the second year of the TESS mission. We extracted, analysed, and modelled the light curves of $\sim 90000$ stars in open clusters located in the northern ecliptic hemisphere in order to find candidate exoplanets. We measured the frequencies of candidate exoplanets in open clusters for different orbital periods and planetary radii, taking into account the detection efficiency of our pipeline and the false positive probabilities of our candidates. We analysed the Age--$R_{\rm P}$ distribution of candidate and confirmed exoplanets with periods $<100$ days and well constrained ages. While no peculiar trends are observed for Jupiter-size and (super-)Earth-size planets, we found that objects with $4\,R_{\rm Earth} \lesssim R_{\rm P} \lesssim 13\,R_{\rm Earth}$ are concentrated at ages $\lesssim 200$ Myr; different scenarios (atmospheric losses, migration, etc.) are considered to explain the observed age-$R_{\rm P}$ distribution. ","A PSF-based Approach to TESS High quality data Of Stellar clusters
  (PATHOS) -- IV. Candidate exoplanets around stars in open clusters: frequency
  and age-planetary radius distribution"
50,1395818146955730956,2872569532,Alejandro S. Borlaff,"['First paper at part of @NASA! And in a new field to me (the magnetic field)! \n\nCheck out the first paper of the @SOFIAtelescope Legacy Program for Extra-galactic Magnetism, presenting a deep study of the spiral magnetic field of the Whirlpool Galaxy M51! <LINK> <LINK>', 'And if you are interested in knowing ***how*** is it possible to detect magnetic fields on deep space, check out this thread!  \n\nhttps://t.co/qFsRCEoGsr']",http://arxiv.org/abs/2105.09315,"The recent availability of high-resolution far-infrared (FIR) polarization observations of galaxies using HAWC+/SOFIA has facilitated studies of extragalactic magnetic fields in the cold and dense molecular disks.We investigate if any significant structural differences are detectable in the kpc-scale magnetic field of the grand design face-on spiral galaxy M51 when traced within the diffuse (radio) and the dense and cold (FIR) interstellar medium (ISM). Our analysis reveals a complex scenario where radio and FIR polarization observations do not necessarily trace the same magnetic field structure. We find that the magnetic field in the arms is wrapped tighter at 154um than at 3 and 6 cm; statistically significant lower values for the magnetic pitch angle are measured at FIR in the outskirts (R > 7 kpc) of the galaxy. This difference is not detected in the interarm region. We find strong correlations of the polarization fraction and total intensity at FIR and radio with the gas column density and 12CO(1-0) velocity dispersion. We conclude that the arms show a relative increase of small-scale turbulent B-fields at regions with increasing column density and dispersion velocities of the molecular gas. No correlations are found with HI neutral gas. The star formation rate shows a clear correlation with the radio polarized intensity, which is not found in FIR, pointing to a small-scale dynamo-driven B-field amplification scenario. This work shows that multi-wavelength polarization observations are key to disentangling the interlocked relation between star formation, magnetic fields, and gas kinematics in the multi-phase ISM. ","Extragalactic Magnetism with SOFIA (Legacy Program) -- I: The magnetic
  field in the multi-phase interstellar medium of M51"
51,1395816182763098115,1350544263071883267,Julia Harz,['First tweet - new paper out this week! Curious about the perspectives to probe baryogenesis with neutron-antineutron oscillations? üëâ<LINK> Fun work within my  group with Chandan Hati (@ChandanHati6) and K√•re Fridell! <LINK>'],https://arxiv.org/abs/2105.06487,"In the near future, the Deep Underground Neutrino Experiment and the European Spallation Source aim to reach unprecedented sensitivity in the search for neutron-antineutron ($n\text{-}\bar{n}$) oscillations, whose observation would directly imply $|\Delta B| = 2$ violation and hence might hint towards a close link to the mechanism behind the observed baryon asymmetry of the Universe. In this work, we explore the consequences of such a discovery for baryogenesis first within a model-independent effective field theory approach. We then refine our analysis by including a source of CP violation and different hierarchies between the scales of new physics using a simplified model. We analyse the implication for baryogenesis in different scenarios and confront our results with complementary experimental constraints from dinucleon decay, LHC, and meson oscillations. We find that for a small mass hierarchy between the new degrees of freedom, an observable rate for $n\text{-}\bar{n}$ oscillation would imply that the washout processes are too strong to generate any sizeable baryon asymmetry, even if the CP violation is maximal. On the other hand, for a large hierarchy between the new degrees of freedom, our analysis shows that successful baryogenesis can occur over a large part of the parameter space, opening the window to be probed by current and future colliders and upcoming $n\text{-}\bar{n}$ oscillation searches. ",Probing baryogenesis with neutron-antineutron oscillations
52,1395793478274199565,1189941003479855105,Robin Kaarsgaard,"['New paper by @drmathys_ and yours truly, and as is traditional, it comes with a #memeabstract. Paper üëâ <LINK>, memeüëá <LINK>']",https://arxiv.org/abs/2105.09929,"Reversible computing is a computational paradigm in which computations are deterministic in both the forward and backward direction, so that programs have well-defined forward and backward semantics. We investigate the formal semantics of the reversible functional programming language Rfun. For this purpose, we introduce join inverse rig categories, the natural marriage of join inverse categories and rig categories, which we show can be used to model the language Rfun, under reasonable assumptions. These categories turn out to be a particularly natural fit for reversible computing as a whole, as they encompass models for other reversible programming languages, notably Theseus and reversible flowcharts. This suggests that join inverse rig categories really are the categorical models of reversible computing. ","Join Inverse Rig Categories for Reversible Functional Programming, and
  Beyond"
53,1395665967800360962,1141279096065929216,Martha Hilton,['New #Charm paper on @arxiv today: Search for time-dependent CP Violation in D0 -&gt; KK and D0 -&gt; pipi decays. Results consistent with CP symmetry but the result improves the precision by a factor of 2\n<LINK> <LINK>'],https://arxiv.org/abs/2105.09889,"A search for time-dependent violation of the charge-parity symmetry in $D^0 \to K^+ K^-$ and $D^0 \to \pi^+ \pi^-$ decays is performed at the LHCb experiment using proton-proton collision data recorded from 2015 to 2018 at a centre-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 6 fb$^{-1}$. The $D^0$ meson is required to originate from a $D^*(2010)^+ \to D^0 \pi^+$ decay, such that its flavour at production is identified by the charge of the accompanying pion. The slope of the time-dependent asymmetry of the decay rates of $D^0$ and $\bar{D}^0$ mesons into the final states under consideration is measured to be $\Delta Y_{K^+ K^-} = (-2.3 \pm 1.5 \pm 0.3) \times 10^{-4}$, $\Delta Y_{\pi^+ \pi^-} = (-4.0 \pm 2.8 \pm 0.4)\times 10^{-4}$, where the first uncertainties are statistical and the second are systematic. These results are compatible with the conservation of the charge-parity symmetry at the level of 2 standard deviations and improve the precision by nearly a factor of two. ","Search for time-dependent $CP$ violation in $D^0 \to K^+ K^-$ and $D^0
  \to \pi^+ \pi^-$ decays"
54,1395547761815216129,20041912,Chentao Yang | Êù®Ëæ∞Ê∂õ,['Check out our new paper of an ALMA survey of a sample of radio AGN hosts at high redshifts <LINK> <LINK>'],https://arxiv.org/abs/2105.09895,"Radio-emitting jets might be one of the main ingredients shaping the evolution of massive galaxies in the Universe since early cosmic times. However, identifying early radio active galactic nuclei (AGN) and confirming this scenario has been hard to accomplish, with studies of samples of radio AGN hosts at z>2 becoming routinely possible only recently. With the above in mind, we have carried out a survey with the Atacama Compact Array (ACA, or Morita Array) at 1.3 mm (rms=0.15 mJy) of 36 high-redshift radio AGN candidates found within 3.9deg2 in the ELAIS-S1 field. The work presented here describes the survey and showcases a preliminary set of results. The selection of the sample was based on three criteria making use of infrared (IR) and radio fluxes only. The criterion providing the highest selection rate of high-redshift sources (86% at z>0.8) is one combining an IR colour cut and radio flux cut (S(5.8um)/S(3.6um)>1.3 and S(1.4GHz)>1mJy). Among the sample of 36 sources, 16 show a millimetre (mm) detection. In eight of these cases, the emission has a non-thermal origin. A zsp=1.58 object, with a mm detection of non-thermal origin, shows a clear spatial offset between the jet-dominated mm continuum emission and that of the host's molecular gas, as traced by serendipitously detected CO(5-4) emission. Among the objects with serendipitous line detections there is a source with a narrow jet-like region, as revealed by CS(6-5) emission stretching 20kpc out of the host galaxy. ","An ACA 1mm survey of HzRGs in the ELAIS-S1: survey description and first
  results"
55,1395543016018952194,36438289,Dr. Patr√≠cia,"[""My new paper is on Arxiv:\n\n<LINK>\n\npublished in MNRAS: <LINK>\n\nIt's about another Milky Way morphological twin: NGC 2442. It has an low-luminosity Compton-thick AGN! \n\n#AGN #astronomy #astrophysics #MNRAS \n\nHere is a nice pic of the nucleus: <LINK>"", 'There is a star-forming ring around the nuclear region and at the center you can see an arched emission that is actually the wall the ionization cone of the AGN.']",http://arxiv.org/abs/2105.09420,"The detailed study of nuclear regions of galaxies is important because it can help understanding the active galactic nucleus (AGN) feedback mechanisms, the connections between the nuclei and their host galaxies, and ultimately the galaxy formation processes. We present the analysis of an optical data cube of the central region of the galaxy NGC 2442, obtained with the integral field unit (IFU) of the Gemini Multi-Object Spectrograph (GMOS). We also performed a multiwavelength analysis, with Chandra data, XMM--Newton and NuSTAR spectra, and Hubble Space Telescope (HST) images. The analysis revealed that the nuclear emission is consistent with a Low Ionization Nuclear Emission-line Region (LINER) associated with a highly obscured compact hard X-ray source, indicating a Compton-thick AGN. The HST image in the F658N filter (H$\alpha$) reveals an arched structure corresponding to the walls of the ionization cone of the AGN. The gas kinematic pattern and the high gas velocity dispersion values in the same region of the ionization cone suggest an outflow emission. The stellar archaeology results indicate the presence of only old stellar populations ($\sim$ 10 Gyr), with high metallicity (z = 0.02 and 0.05), and the absence of recent star formation in the central region of NGC 2442, which is possibly a consequence of the AGN feedback, associated with the detected outflow, shutting off star formation. NGC 2442 is a late-type galaxy similar to the Milky Way, and comparisons show that the main difference between them is the presence of a low-luminosity AGN. ",The nuclear environment of NGC 2442: a Compton-thick low-luminosity AGN
56,1395500630614106114,2818695390,Sasho Nikolov,"['New paper up, with amazing U of T student Deepanshu Kush and Haohua Tang (who was an undergrad while working on this): Near Neighbor Search via Efficient Average Distortion Embeddings <LINK>. üßµ [1/9]', 'The goal in approximate near neighbor search (ANN): preprocess n points P (in some metric space) into a small data structure s.t. queries can be answered quickly: given a new point q, if P has some x* that‚Äôs close to q, return a point x in P that‚Äôs approximately close to q. [2/9]', 'We want to answer a query in less time than it takes to scan P, while keeping space and preprocessing time low. Approximation is necessary for this, but how much? It depends on how we measure distance. With Euclidean or Manhattan distance, constant approx is possible. [3/9]', 'What about other distances? You can approximate your favorite metric by a subset of Euclidean/Manhattan space, i.e., find a low distortion embedding. You can get lots of non-trivial results like this, but you are stuck with bad approximation poly(d) even for ell_p^d spaces [4/9]', 'In papers with Andoni, Naor, @ilyaraz2, and Waingarten we showed that you can get very good approximation poly(log d) for *any* distance given by a norm, much better than embeddings (@QuantaMagazine article: https://t.co/KYqJqt3sjP) [5/9]', 'Big caveat: the data structures require exponential preprocessing. This is what the new results improve: for ell_p and other spaces, we give data structures with the same approx as ANNRW but efficient preprocessing, via a new general method that could work for all norms. [6/9]', 'The idea is to use an embedding into Euclidean space with a much weaker guarantee: low *average* distortion, rather than low distortion. I.e. the embedding should not expand distances, but it can contract them, as long as the average distance between pts in P stays large. [7/9]', 'Turns out that such embeddings are enough to get good random hash functions, and from them good data structures. We also construct low avg distortion embeddings for ell_p and other spaces. Finally, a fascinating open problem: [8/9]', 'Find an efficiently computable embedding of (the 1/2-snowflake) of any d-dimensional norm into Euclidean space with avg distortion sqrt(log d). Naor proved (https://t.co/p63oPk5NV9) these exist, but did not give an explicit embedding, let alone an algorithm to compute it. [9/9]']",https://arxiv.org/abs/2105.04712,"A recent series of papers by Andoni, Naor, Nikolov, Razenshteyn, and Waingarten (STOC 2018, FOCS 2018) has given approximate near neighbour search (NNS) data structures for a wide class of distance metrics, including all norms. In particular, these data structures achieve approximation on the order of $p$ for $\ell_p^d$ norms with space complexity nearly linear in the dataset size $n$ and polynomial in the dimension $d$, and query time sub-linear in $n$ and polynomial in $d$. The main shortcoming is the exponential in $d$ pre-processing time required for their construction. In this paper, we describe a more direct framework for constructing NNS data structures for general norms. More specifically, we show via an algorithmic reduction that an efficient NNS data structure for a given metric is implied by an efficient average distortion embedding of it into $\ell_1$ or into Euclidean space. In particular, the resulting data structures require only polynomial pre-processing time, as long as the embedding can be computed in polynomial time. As a concrete instantiation of this framework, we give an NNS data structure for $\ell_p$ with efficient pre-processing that matches the approximation factor, space and query complexity of the aforementioned data structure of Andoni et al. On the way, we resolve a question of Naor (Analysis and Geometry in Metric Spaces, 2014) and provide an explicit, efficiently computable embedding of $\ell_p$, for $p \ge 2$, into $\ell_2$ with (quadratic) average distortion on the order of $p$. We expect our approach to pave the way for constructing efficient NNS data structures for all norms. ",Near Neighbor Search via Efficient Average Distortion Embeddings
57,1395288908989091841,1224994747846098944,Diego Calder√≥n,"['New paper on wind-reprocessed transients with a new radiation module for the moving-mesh hydro code JET. Simulations cover up to 11 orders of magnitude in space and time. We validated previous analytical models, and simulated anisotropic irradiation cases.\n<LINK> <LINK>']",https://arxiv.org/abs/2105.08735,"Motivated by recent theoretical work on tidal disruption events and other peculiar transients, we present moving-mesh radiation-hydrodynamic simulations of radiative luminosity emitted by a central source being reprocessed by a wind-like outflow. We couple the moving-mesh hydrodynamic code JET with our newly-developed radiation module based on mixed-frame grey flux-limited diffusion with implicit timestep update. This allows us to study the self-consistent multi-dimensional radiation-hydrodynamic evolution over more than ten orders of magnitude in both space and time in a single run. We simulate an optically-thick spherical wind with constant or evolving mass-loss rate, which is irradiated by a central isotropic or angularly-dependent radiation source. Our spherically-symmetric simulations confirm previous analytic results by identifying different stages of radiation reprocessing: radiation trapped in the wind, diffusing out through the wind, and reaching constant maximum attenuation. We find that confining the central radiation source in a cone with moderate opening angles decrease up to one order of magnitude the early flux along sightlines oriented away from the direction of radiation injection but that the reprocessed radiation becomes isotropic roughly after one lateral diffusion time through the ejecta. We discuss further applications and guidelines for the use of our novel radiation-hydrodynamics tool in the context of transient modelling. ","Moving-mesh radiation-hydrodynamic simulations of wind-reprocessed
  transients"
58,1395274157114413057,1246739202164895744,Thomas Pasini,"['My new paper is out! We focused on the kinematic properties of radio galaxies in groups, finding different behaviours for BGGs and satellites, as well as relevant dissimilarities with simulations. Check it out on <LINK> \n\n@HambObs @max_gasp @fradega  #paperday <LINK>']",https://arxiv.org/abs/2105.08727,"We investigate the kinematic properties of a large (N=998) sample of COSMOS spectroscopic galaxy members distributed among 79 groups. We identify the Brightest Group Galaxies (BGGs) and cross-match our data with the VLA-COSMOS Deep survey at 1.4 GHz, classifying our parent sample into radio/non-radio BGGs and radio/non-radio satellites. The radio luminosity distribution spans from $L_R\sim2\times10^{21}$ W Hz$^{-1}$ to $L_R\sim3\times$10$^{25}$ W Hz$^{-1}$. A phase-space analysis, performed by comparing the velocity ratio (line-of-sight velocity divided by the group velocity dispersion) with the galaxy-group centre offset, reveals that BGGs (radio and non-radio) are mostly ($\sim$80\%) ancient infallers. Furthermore, the strongest ($L_R>10^{23}$ W Hz$^{-1}$) radio galaxies are always found within 0.2$R_{\rm vir}$ from the group centre. Comparing our samples with HORIZON-AGN, we find that the velocities and offsets of simulated galaxies are more similar to radio BGGs than to non-radio BGGs, albeit statistical tests still highlight significant differences between simulated and real objects. We find that radio BGGs are more likely to be hosted in high-mass groups. Finally, we observe correlations between the powers of BGG radio galaxies and the X-ray temperatures, $T_{\rm x}$, and X-ray luminosities, $L_{\rm x}$, of the host groups. This supports the existence of a link between the intragroup medium and the central radio source. The occurrence of powerful radio galaxies at group centres can be explained by Chaotic Cold Accretion, as the AGN can feed from both the galactic and intragroup condensation, leading to the observed positive $L_{\rm R}-T_{\rm x}$ correlation. ","Radio galaxies in galaxy groups: kinematics, scaling relations and AGN
  feedback"
59,1395250551097593860,1334580500749553665,Miguel Vioque,"['We have a new paper out! <LINK>\n\n""First detection of a disk free of volatile elements around a young A-type star: A sign of collisions between rocky planets?""', 'We conclude that HD 152384 is surrounded by a tenuous circumstellar disk. We suggest that this disk may be due to collisions in a newly formed planetary system.']",https://arxiv.org/abs/2105.08327,"Aims. We present the first detailed analysis of the astrophysical parameters of the poorly studied Sco-Cen member HD 152384 and its circumstellar environment. Methods. We analyze newly obtained optical-near-IR XSHOOTER spectra, as well as archival TESS data, of HD 152384. In addition, we use literature photometric data to construct a detailed spectral energy distribution (SED) of the star. Results. The photospheric absorption lines in the spectrum of HD 152384 are characteristic of a A0 V star, for which we derive a stellar mass of 2.1 +/- 0.1 M_sun and a stellar age > 4.5 Myr. Superimposed on the photospheric absorption, the optical spectrum also displays double-peaked emission lines of Ca II, Fe I, Mg I and Si I, typical of circumstellar disks. Notably, all Hydrogen and Helium lines appear strictly in absorption. A toy model shows that the observed emission line profiles can be reproduced by emission from a compact (radius < 0.3 au) disk seen at an inclination of ~24 degrees. Further evidence for the presence of circumstellar material comes from the detection of a moderate infrared excess in the SED, similar to those found in extreme debris disk systems. Conclusions. We conclude that HD 152384 is surrounded by a tenuous circumstellar disk which, although rich in refractory elements, is highly depleted of volatile elements. To the best of our knowledge such a disk is unique within the group of young stars. However, it is reminiscent of the disks seen in some white dwarfs, which have been attributed to the disruption of rocky planets. We suggest that the disk around HD 152384 may have a similar origin and may be due to collisions in a newly formed planetary system. ","First detection of a disk free of volatile elements around a young
  A-type star: A sign of collisions between rocky planets?"
60,1395187722529034244,11419652,Artem Chernikov,"['üó£Ô∏è My new preprint is finally out! \nJoint with Ehud Hrushovski, Alex Kruckman, Krzysztof Krupinski, Slavko Moconja, Anand Pillay and @nickramsey (a record on the number of co-authors in a model theory paper?).\n<LINK>\n\nA short thread explaining the result üëá 1/n <LINK>', 'Amenability is the key concept behind the famous Banach-Tarski paradox (see my older thread about it below). A group is amenable if there exists a finitely additive probability measure on the Boolean algebra of its subsets invariant under translation. 2/n\nhttps://t.co/A4Zh5uYvnh', 'A group is paradoxical if it is possible to partition it into finitely many sets so that by translating them we can get two copies of the same group. Amenability would rule this out since the total measure has to be preserved. Tarski proved: G is amenable iff non-paradoxical. 3/n', 'For example, the free group on two generators is paradoxical (see below), and can be embedded into the group of Euclidean motions in R^3 - which with some further use of the axiom of choice allows to double multiply balls and other things like that.\n4/n https://t.co/igZE5YhnCp', 'If the group comes equipped with additional structure, it might be natural to restrict the Boolean algebra of sets in the definition of amenability. E.g., given a topological group one might restrict to Borel sets, a complex algebraic group - to constructible sets, or more\n5/n', ""generally and naturally for a model theorist,given a definable group in some structure,restrict to its definable subsets. This gives the notion of definable amenability. Of course,if a group is amenable as an abstract group,it's def. am. no matter the choice of the language.\n6/n"", 'But it is often easier to be amenable since most sets might not be definable.For example,F_2 is non-amen. as we saw,but it is def. amenable viewed as a first-order structure (F_2, *) in the pure group language. In fact,it has an invariant 0-1 measure on all definable subsets!\n7/n', ""Which is moreover unique such. This follows from Zlil Sela's solution of the Tarski's problem demonstrating some level of quantifier elimination for non-abelian free groups and their _stability_ - an important model theoretic notion of tameness. \n8/n"", 'An explicit combinatorial description of which sets are negligible with respect to this measure is tricky - it was announced by Bestvina and Feighn, but I think is still unpublished? (paging @SklinosR). See e.g. page 22 in these notes of Pillay https://t.co/PwTxiesqcI.\n9/n', ""For a model theorist, it's natural to study def. amenability not only for particular structures,but for groups def. in classes of structures satisfying certain tameness assumptions on their def. sets: stability,o-minimality,NIP - see @GabeConant's map https://t.co/oKK5fAuIvs\n10/n https://t.co/blbIpCIqyc"", ""By now there is a rather developed theory of definable amenability in NIP groups (closely related to tame dynamical systems of Glasner and Megrelishvili), starting with the work of Hrushovski, Peterzil, Pillay on Pillay's conjectures, and culminating in my paper with Simon.\n11/n https://t.co/UtugpfkGVJ"", 'However, the situation in the orthogonal class of the so-called simple structures (which are quite far from simple) is much less understood.Some examples: \n1)Groups definable in pseudofinite fields (e.g.algebraic groups over ultraproducts of finite fields) -all def. amenable 12/n', 'as witnessed by the ultralimits of the corresponding finite counting measures.\n2) Groups definable in ACFA (algebraically closed fields with generic automorphism) are essentially solvable, hence amenable even as abstract groups.\nThen the natural question asked by Pillay - \n13/n', 'are all groups in simple theories definably amenable?\nIn this paper we give first counterexamples: there exists a simple theory expanding algebraically closed fields in which the group SL_2 is not definably amenable!\n14/n', ""In fact, the method is more general and can be adapted to show the following: given a definable group G in a simple theory T containing a (not necessarily definable) non-abelian free group, there exists a simple theory T' expanding T in which G is not definably amenable.\n15/n"", 'This expansion is obtained by adding a ""generic"" paradoxical partition of the group and demonstrating that this preserves simplicity (and forking). Some curious tree combinatorics is involved in showing that the model companion of an expansion by a partition exists.\n16/n', 'Here is @nickramsey explaining some of that tree combinatorics -see the video of his talk https://t.co/H5stxgudOF!\nWe also obtain some positive results for the so-called ""small"" groups and show non-triviality of graded Grothendieck rings of small first-order structures!\n17/17 https://t.co/cV5Kgladwe', '@Category_Fury Thank you! Though who knows - so many times things that are far turn out to be so close!', '@JonSchw73589513 @nickramsey Thank you!']",https://arxiv.org/abs/2105.07281,"We give examples of (i) a simple theory with a formula (with parameters) which does not fork over the empty set but has mu measure 0 for every automorphism invariant Keisler measure mu, and (ii) a definable group G in a simple theory such that G is not definably amenable, i.e. there is no translation invariant Keisler measure on G We also discuss paradoxical decompositions both in the setting of discrete groups and of definable groups, and prove some positive results about small theories, including the definable amenability of definable groups, and nontriviality of the graded Grothendieck ring. ",Invariant measures in simple and in small theories
61,1395182893102624772,2377407248,Daniel Whiteson,"['New paper with Aishik Ghosh and @BPNachman about how networks learn when your training samples have uncertainties:\n\n<LINK>\n\nWhat if you are learning the wrong thing? It‚Äôs a big deal.', 'What if your data is controlled by some unknown nuisance parameter. You might train on simulated data like this (dots) and learn a NN (shading): https://t.co/ha16BptgyF', 'But what if the real data are actually like this? https://t.co/tEeJILWY02', 'Then you have the wrong NN. You want this one: https://t.co/DBfrHtwDAy', 'Recently many people approach this by trying to make your NN insensitive to the unknown parameter. That doesn‚Äôt work here.  There‚Äôs no network that works for all rotations.', 'In our paper, we use a parameterized network, where the network output is a function of the unknown parameter z.  Then we can treat it statistically and profile over it. https://t.co/K6qR0uPsN8', 'This gives the optimal performance! https://t.co/ZrATUCaTaL', '@kratsg At training, you give it examples with true values of z.  \n\nAt inference, you give it data and get a value as a function of z.', ""@kratsg Technically you just treat z as an additional input, so to evaluate the NN, you need to specify z.  I'll find @Aishik_Ghosh_'s code and post a link."", '@HEPfeickert @kratsg @Aishik_Ghosh_ Absolutely.']",https://arxiv.org/abs/2105.08742,"Machine learning techniques are becoming an integral component of data analysis in High Energy Physics (HEP). These tools provide a significant improvement in sensitivity over traditional analyses by exploiting subtle patterns in high-dimensional feature spaces. These subtle patterns may not be well-modeled by the simulations used for training machine learning methods, resulting in an enhanced sensitivity to systematic uncertainties. Contrary to the traditional wisdom of constructing an analysis strategy that is invariant to systematic uncertainties, we study the use of a classifier that is fully aware of uncertainties and their corresponding nuisance parameters. We show that this dependence can actually enhance the sensitivity to parameters of interest. Studies are performed using a synthetic Gaussian dataset as well as a more realistic HEP dataset based on Higgs boson decays to tau leptons. For both cases, we show that the uncertainty aware approach can achieve a better sensitivity than alternative machine learning strategies. ",Uncertainty Aware Learning for High Energy Physics
62,1395095368770080771,3252140220,priyanka nanayakkara,"['Curious about how AI researchers are thinking about the societal consequences of their work?\n\nWe (@JessicaHullman @ndiakopoulos) read through hundreds of @NeurIPSConf 2020 broader impact statements to find out. Our results are in a new @AIESConf paper (<LINK>) üßµ', 'Authors vary in how they describe impacts in terms of \n\n* valence (pos vs neg consequences)\n* orientation (technical or society-facing outcomes)\n* specificity (highly contextual to general)\n* uncertainty (in terms of future outcomes)', 'They discuss impacts falling under several categories: https://t.co/SVvM4M5iCS', 'Some authors say that due to the theoretical nature of their work, there are no foreseeable, ethical, or negative societal consequences (9% of statements in our sample). Others say/imply the opposite (10%).', 'Authors also recommend ways of mitigating negative consequences and achieving the following outcomes:\n\n* safe and effective use of AI (21% of statements in our sample)\n* ensure ""fair"" outcomes (6%)\n* protect privacy (5%)\n* reduce environmental impact (1%)', 'For more details (including *who* authors say might be impacted and *who* is responsible for future action), please see one of our blogposts or paper: \n1) https://t.co/1H3xBjgl0e\n2) https://t.co/s1xJGCYdUM (by @JessicaHullman)\n3) the full paper!!! ‚ú®https://t.co/HklogslAVU‚ú®', 'Very curious to hear your thoughts on these findings! For example, are there any impact areas that you find surprising? Any that you think are missing? Any other impressions around how the #NeurIPS2020 broader impact statement requirement went?', ""@DiverseInAI @NeurIPSConf @icmlconf Interesting, bias in particular came up quite a bit, and I think could definitely be interesting to further investigate in these statements. \nAlso, here's our dataset: https://t.co/sG9RacEDlL\nWe extracted these statements randomly from PDFs here: https://t.co/intNxuw2od""]",https://arxiv.org/abs/2105.04760,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals. ","Unpacking the Expressed Consequences of AI Research in Broader Impact
  Statements"
63,1395037023006826501,1092693586263457792,Greg Yang,"['Architectural universality of neural tangent training dynamics <LINK>. Last year I showed NTK has ‚àû-width limit for any arch at init - this paper now covers training! This is enabled by a new graphical form of Tensor Programs (eg üëá). ICML 2021 w/ Etai Littwin <LINK>', 'To me, this paper demonstrates the power of Tensor Programs as it now complete subsumes both NTK and mean field theories, in so far as infinite-width limits are concerned, as well as automatically generalize them to arbitrary architectures.', ""And in the end it's all quite simple: if you can write down the neural computation in a Tensor Program, then *rigorously* it has an infinite-width limit that can be calculated according to a recipe. You can then analyze this limit however you want."", ""So this is one for the textbooks. But I'm moving beyond the kernel limit now! Plowing full steam toward understanding the feature learning limit of NNs https://t.co/vhvvXylq58! Can we train them practically? Can they inform our practice of finite neural networks? Stay tuned! &lt;3 https://t.co/0c6Fi3cM0C""]",https://arxiv.org/abs/2105.03703,"Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at initialization has an infinite-width limit for a large class of architectures including modern staples such as ResNet and Transformers. However, their analysis does not apply to training. Here, we show the same neural networks (in the so-called NTK parametrization) during training follow a kernel gradient descent dynamics in function space, where the kernel is the infinite-width NTK. This completes the proof of the *architectural universality* of NTK behavior. To achieve this result, we apply the Tensor Programs technique: Write the entire SGD dynamics inside a Tensor Program and analyze it via the Master Theorem. To facilitate this proof, we develop a graphical notation for Tensor Programs. ","Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel
  Training Dynamics"
64,1395033709644947460,94412971,Wojciech Kry≈õci≈Ñski,"['Excited to share our new paper ‚ÄúBookSum: A Collection of Datasets for Long-form Narrative Summarization"" üìöüìñü§ñ\n\nw/ @nazneenrajani, @jigsaw2212, @CaimingXiong, and Dragomir Radev\n\nPaper: <LINK>\nCode: <LINK>\n\nThread üßµ:', 'BookSum includes documents from the literature domain aligned with human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. https://t.co/LT00UQnnyf', 'The domain of the data helps create unique challenges for summarization systems: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures.', 'Through quantitative analysis we show that source data included in BookSum is less prone to layout biases in comparison to existing datasets and the associated summaries are highly abstractive. https://t.co/iSrh69fGX3', 'The hierarchical structure of the data and lengthy source documents make BookSum a good candidate to facilitate research in single- and multi-document summarization, as well as hierarchical and sparse-attention models. https://t.co/0GQXbWJSM3', 'To benchmark the performance of existing models, we trained and evaluated multiple extractive and abstractive summarization baselines for our dataset. https://t.co/S3wvPKKwBw', '#NLProc #NLP #Summarization']",https://arxiv.org/abs/2105.08209,"The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations of text summarization systems. We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization. Our dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset. ",BookSum: A Collection of Datasets for Long-form Narrative Summarization
65,1395009341569310722,1246943809,Philipp Moesta,"['New group paper: First paper led by PhD student Swapnil Shankar at @uva_api and @GRAPPAinstitute  <LINK> <LINK>', 'This is our first attempt at end-to-end modelling of type Ic-bl supernova. We extract the engine dynamics from a full 3D GRMHD CCSN simulation performed with https://t.co/m9JeRT3bki and use the resulting parameters as engine models for full-star jet-breakout simulations with JET.', 'Finally we perform radiation-transport calculation with SEDONA to get lightcurves and spectras for these events.\nWe find that the engine dynamics from 3D CCSN jet formation simulations are consistent with type Ic-bl supernova lightcurves and spectra. https://t.co/QdQlPf3teH', 'Lightcurves and spectra also seem robust with respect to uncertainties in engine parameter extraction. This is very encouraging to keep improving our end-to-end modelling approach. The most important step will be to do full 3D jet-propagation simulations in the future.', 'Huge congratulations to Swapnil for carrying out this project and working with multiple different codes and datasets. Such a pleasure to be working with him. Also, many thanks to collaborators Jennifer Barnes, Paul Duffel and Dan Kasen whose work we relied on heavily.']",https://arxiv.org/abs/2105.08092,"A subset of type Ic supernovae (SNe Ic), broad-lined SNe Ic (SNe Ic-bl), show unusually high kinetic energies ($\sim 10^{52}$ erg) which cannot be explained by the energy supplied by neutrinos alone. Many SNe Ic-bl have been observed in coincidence with long gamma-ray bursts (GRBs) which suggests a connection between SNe and GRBs. A small fraction of core-collapse supernovae (CCSNe) form a rapidly-rotating and strongly-magnetized protoneutron star (PNS), a proto-magnetar. Jets from such magnetars can provide the high kinetic energies observed in SNe Ic-bl and also provide the connection to GRBs. In this work we use the jetted outflow produced in a 3D CCSN simulation from a consistently formed proto-magnetar as the central engine for full-star explosion simulations. We extract a range of central engine parameters and find that the extracted engine energy is in the range of $6.231 \times 10^{51}-1.725 \times 10^{52}$ erg, the engine time-scale in the range of $0.479-1.159$ s and the engine half-opening angle in the range of $\sim 9-19^{\circ}$. Using these as central engines, we perform 2D special-relativistic (SR) hydrodynamic (HD) and radiation transfer simulations to calculate the corresponding light curves and spectra. We find that these central engine parameters successfully produce SNe Ic-bl which demonstrates that jets from proto-magnetars can be viable engines for SNe Ic-bl. We also find that only the central engines with smaller opening angles ($\sim 10^{\circ}$) form a GRB implying that GRB formation is likely associated with narrower jet outflows and Ic-bl's without GRBs may be associated with wider outflows. ","Proto-magnetar jets as central engines for broad-lined type Ic
  supernovae"
66,1394921931380559872,1238481001304686594,Pablo Mart√≠nez-Mirav√©,"['New paper today! With @MariamTortola @spastorcarpi @PFdeSalas and Stefano Gariazzo\n\n""Cosmological radiation density with non-standard neutrino-electron interactions""\n\n<LINK>\n\nWe study how NSI with electrons alter the picture of neutrino decoupling <LINK>', 'We address the variation on the effective number of neutrinos in the presence of NSI, including the effect in oscillations, annihilation and scattering between neutrinos and electrons and positrons. https://t.co/41Ff3ttmZC', 'We also show that future cosmological data would complement terrestrial experiments (and even provide competitive constraints in some of the couplings)!\n\nAnd most importantly, I really enjoyed learning and working with these great collaborators!\n\n üòÉüòÉüòÉüòÉüòÉ']",https://arxiv.org/abs/2105.08168,"Neutrino non-standard interactions (NSI) with electrons are known to alter the picture of neutrino decoupling from the cosmic plasma. NSI modify both flavour oscillations through matter effects, and the annihilation and scattering between neutrinos and electrons and positrons in the thermal plasma. In view of the forthcoming cosmological observations, we perform a precision study of the impact of non-universal and flavour-changing NSI on the effective number of neutrinos, $N_{eff}$. We present the variation of $N_{eff}$ arising from the different NSI parameters and discuss the existing degeneracies among them, from cosmology alone and in relation to the current bounds from terrestrial experiments. Even though cosmology is generally less sensitive to NSI than these experiments, we find that future cosmological data would provide competitive and complementary constraints for some of the couplings and their combinations. ","Cosmological radiation density with non-standard neutrino-electron
  interactions"
67,1394914104763113473,3213868013,German Sborlini,['When quantum computers meets geometry...ü§Ø\n\nNEW PAPER!!! Quantum algorithms applied to Feynman integrals: <LINK>\n\n@GermanRodrigoC2 @AerOlivo @RamirezSelomit @PARTICLEFACE #physics #quantum #technology #Science <LINK>'],https://arxiv.org/abs/2105.08703,"We present a novel benchmark application of a quantum algorithm to Feynman loop integrals. The two on-shell states of a Feynman propagator are identified with the two states of a qubit and a quantum algorithm is used to unfold the causal singular configurations of multiloop Feynman diagrams. To identify such configurations, we exploit Grover's algorithm for querying multiple solutions over unstructured datasets, which presents a quadratic speed-up over classical algorithms when the number of solutions is much smaller than the number of possible configurations. A suitable modification is introduced to deal with topologies in which the number of causal states to be identified is nearly half of the total number of states. The output of the quantum algorithm in \emph{IBM Quantum} and \emph{QUTE Testbed} simulators is used to bootstrap the causal representation in the loop-tree duality of representative multiloop topologies. The algorithm may also find application and interest in graph theory to solve problems involving directed acyclic graphs. ",Quantum algorithm for Feynman loop integrals
68,1394836962272587776,976155561522794497,Juliano C√©sar Silva Neves,"['My new paper with my colleague, R. V. Maluf, is about anisotropies in cosmology. A different physics in a given direction could be an effect of a weird field, the bumblebee field. #cosmology\n<LINK>']",https://arxiv.org/abs/2105.08659,"In this work, a bumblebee field is adopted in order to generate cosmological anisotropies. For that purpose, we assume a Bianchi I cosmology, as the background geometry, and a bumblebee field coupled to it. Bumblebee models are examples of a mechanism for the Lorentz symmetry violation by assuming a nonzero vacuum expectation value for the bumblebee field. When coupled to the Bianchi I geometry, which is not in agreement with a cosmological principle, the bumblebee field plays the role of a source of anisotropies and produces a preferred axis. Thus, a fraction of the cosmic anisotropies would come from the Lorentz symmetry violation. In the last part of the article, we try to assume an upper bound on the bumblebee field using the quadrupole and octopole moments of the cosmic microwave background radiation. ",Bumblebee field as a source of cosmological anisotropies
69,1394768628059361282,1251423297209761793,Yun Zhang,['Our new paper about the fate of residual small bodies around white dwarfs is out: <LINK>  It‚Äôs exciting to link the clues from extra solar system small bodies to those in our own system :)'],http://arxiv.org/abs/2105.06475,"A significant fraction of white dwarfs (WDs) exhibit signs of ongoing accretion of refractory elements at rates $\sim10^3$--$10^7$ kg s$^{-1}$, among which, 37 WDs were detected to harbor dusty debris disks. Such a concurrence requires not only fertile reservoirs of planetary material, but also a high duty cycle of metal delivery. It has been commonly suggested that this material could be supplied by Solar System analogs of Main Belt asteroids or Kuiper Belt objects. Here we consider the primary progenitors of WD pollutants as a population of residual high-eccentricity planetesimals, de-volatilized during the stellar giant phases. Equivalent to the Solar System's long-period comets, they are scattered to the proximity of WDs by perturbations from remaining planets, Galactic tides, passing molecular clouds, and nearby stars. These objects undergo downsizing when they venture within the tidal disruption limit. We show quantitatively how the breakup condition and fragment sizes are determined by material strength and gravity. Thereafter, the fragments' semi-major axes need to decay by at least $\sim$6 orders of magnitude before their constituents are eventually accreted onto the surface of WDs. We investigate the orbital evolution of these fragments around WDs and show that WDs' magnetic fields induce an Alfv\'en-wave drag during their periastron passages and rapidly circularize their orbits. This process could be responsible for the observed accretion rates of heavy-elements and the generation of circum-WD debris disks. A speculative implication is that giant planets may be common around WDs' progenitors and they may still be bound to some WDs today. ","Orbital migration and circularization of tidal debris by Alfv\'en-wave
  drag: circumstellar debris and pollution around white dwarfs"
70,1394749999418773504,145986026,Erdem Bƒ±yƒ±k,"['Decentralized RL agents reach Nash equilibrium (if they converge). When multiple Nash exist, they often end up at risk-dominant equ due to the uncertainty about others. In our new IJCAI paper, we show ""gifting"" helps agents reach more prosocial equilibrium <LINK> <LINK>', 'Simply put: no need for reward shaping! Just allow agents to give away their rewards to others and they will learn not to do that over time. But while this transient behavior is happening, they will move towards the optimum that is more prosocial.', 'w/ @woodywang153 , Mark Beliaev (kudos for the comic above! üòÑ), Daniel A. Lazar, Ramtin Pedarsani and @DorsaSadigh', '@kulaksor Just to clarify, when an agent gives away some reward, it loses its own reward. So the sum of rewards never changes with a gifting action. In the example in the paper, the prosocial equilibrium is also the Pareto efficient one.']",https://arxiv.org/abs/2105.06593,"Coordination is often critical to forming prosocial behaviors -- behaviors that increase the overall sum of rewards received by all agents in a multi-agent game. However, state of the art reinforcement learning algorithms often suffer from converging to socially less desirable equilibria when multiple equilibria exist. Previous works address this challenge with explicit reward shaping, which requires the strong assumption that agents can be forced to be prosocial. We propose using a less restrictive peer-rewarding mechanism, gifting, that guides the agents toward more socially desirable equilibria while allowing agents to remain selfish and decentralized. Gifting allows each agent to give some of their reward to other agents. We employ a theoretical framework that captures the benefit of gifting in converging to the prosocial equilibrium by characterizing the equilibria's basins of attraction in a dynamical system. With gifting, we demonstrate increased convergence of high risk, general-sum coordination games to the prosocial equilibrium both via numerical analysis and experiments. ",Emergent Prosociality in Multi-Agent Games Through Gifting
71,1394748134773956612,19510090,Julian Togelius,"['Can you use deep learning to model nonlinear partial differential equations? In a new paper led by @ruben_torrado of @AiOrigen, with contributions from @Bumblebor and me, we show that we can model the Buckley-Leverett equation using attention mechanisms. \n<LINK> <LINK>', 'The Buckley-Leverett equation, which describes liquid flow through a porous medium, is hugely important to many engineering equations in e.g. reservoir modeling. It is also expensive to approximate. Until now, no-one has effectively modeled it with deep learning.', 'The trick here is to use an attention mechanism, which allows the network to effectively model the discontinuity of the shock that travels through the medium. In our experiments, we could see the attention mechanism tracking the shock front, yielding a physical interpretation. https://t.co/Hl1Pyb1BBh', 'If you are interested in this research, and want to contribute to pushing the limits of modeling physical systems with deep learning, @AiOrigen is currently hiring deep learning researchers.\nhttps://t.co/0pCKHMfGnz', 'Read more about what the company does here:\nhttps://t.co/yV1HpoSobH']",https://arxiv.org/abs/2105.07898,"Physics-Informed Neural Networks (PINNs) have enabled significant improvements in modelling physical processes described by partial differential equations (PDEs). PINNs are based on simple architectures, and learn the behavior of complex physical systems by optimizing the network parameters to minimize the residual of the underlying PDE. Current network architectures share some of the limitations of classical numerical discretization schemes when applied to non-linear differential equations in continuum mechanics. A paradigmatic example is the solution of hyperbolic conservation laws that develop highly localized nonlinear shock waves. Learning solutions of PDEs with dominant hyperbolic character is a challenge for current PINN approaches, which rely, like most grid-based numerical schemes, on adding artificial dissipation. Here, we address the fundamental question of which network architectures are best suited to learn the complex behavior of non-linear PDEs. We focus on network architecture rather than on residual regularization. Our new methodology, called Physics-Informed Attention-based Neural Networks, (PIANNs), is a combination of recurrent neural networks and attention mechanisms. The attention mechanism adapts the behavior of the deep neural network to the non-linear features of the solution, and break the current limitations of PINNs. We find that PIANNs effectively capture the shock front in a hyperbolic model problem, and are capable of providing high-quality solutions inside and beyond the training set. ","Physics-informed attention-based neural network for solving non-linear
  partial differential equations"
72,1394703439960043520,1912298966,Dr. L. C. Mayorga,"['üö®New paperüö® with some suspicious characters üòâ\n<LINK>\nI was visiting @astromarkmarley to wrap up a paper and we chatted about my aspirations to observe eccentric planets and I said, Hey can we trick EGP to model them by running different orbit positions? 1/7 <LINK>', 'See GCMs are very resource hungry, and takes weeks to months to spin up and look at a planet. So to study long period planets and figure out what parameters are important in causing variations in a planet atmosphere we need something fast. 2/7 https://t.co/MNTxcjGmNx', 'We can do better! Robinson et al. 2014 had previously worked on a creating a time-stepping version of EGP, which is a radiative-convective equilibrium model that can self-consistently include clouds, to model changing internal heat flux inside a solo brown dwarf. 3/7 https://t.co/SZ7OyUjGac', 'We took this version updated it to the half python wrapped version of EGP and turned the star on and moved the planet around and made some sick animations of the temperature pressure profile of the atmospheres of our proof-of-concept planets. 4/7 https://t.co/iGvr36WmIn', 'The strength of this code is our ability to track chemical changes (and eventually cloud changes) throughout the orbit, so you‚Äôll be able to see relative to your observation window, what species will dominate the spectra, how long they are visible, etc. 5/7 https://t.co/HeLDJJP12s', 'While 3D GCMs are necessary for looking at 3D and 2D winds and energy transport, our parameterizations to simplify to 1D mean we can rapidly (hrs and days instead of weeks) predict what an eccentric planet might look like and predict spectra, light curves, etc. 6/7 https://t.co/IFPZ2Hzy5O', 'If you‚Äôve got an eccentric planet you want to model, I can currently run you a cloudless model in a day or so. We‚Äôre working on getting the clouds in there next so you‚Äôll see paper 2 later this year! Please see the paper for more details and model/data intercomparisons! 7/7 https://t.co/tEio4JarGa', ""@aussiastronomer @astromarkmarley Sorry I put it two tweets in. It's our/Mark's radiative-convective equilibrium model that does self-consistent clouds. Started in the Solar System, then expanded to BD and Extrasolar Giant Planets, but we've abandoned the acronym as it's expanding to terrestrials these days."", ""Shoutout to @_astronomay (who was the shadow PI for my internal funding for this when the deadline arrived and I hadn't quite arrived yet) and @kevinbstevenson"", ""@astronomerslc25 @jackacton96 I'll let you know when I have it working properly! I also have a quick and dirty way of doing it now if you need something fast. [begin typical side effect speech common in American advertisements for drugs]""]",http://arxiv.org/abs/2105.08009,"Exoplanets on eccentric orbits experience an incident stellar flux that can be markedly larger at periastron versus apoastron. This variation in instellation can lead to dramatic changes in atmospheric structure in regions of the atmosphere where the radiative and advective heating/cooling timescales are shorter than the orbital timescale. To explore this phenomenon, we develop a sophisticated one-dimensional (vertical) time-stepping atmospheric structure code, EGP+, capable of simulating the dynamic response of atmospheric thermal and chemical structure to time-dependent perturbations. Critically, EGP+ can efficiently simulate multiple orbits of a planet, thereby providing new opportunities for exoplanet modeling without the need for more computationally-expensive models. We make the simplifying assumption of cloud-free atmospheres, and apply our model to HAT-P-2b, HD~17156b, and HD~80606b, which are known to be on higher-eccentricity orbits. We find that for those planets which have Spitzer observations, our planet-to-star ratio predictions are roughly consistent with observations. However, we are unable to reproduce the observed peak offsets from periastron passage. Finally, we discuss promising pathways forward for adding new model complexity that would enable more detailed studies of clear and cloudy eccentric planets as well as worlds orbiting active host stars. ",Variable Irradiation on 1D Cloudless Eccentric Exoplanet Atmospheres
73,1394634958182903811,1473984524,Keisuke Okumura,"['Our new paper ""Offline Time-Independent Multi-Agent Path Planning‚Äù (OTIMAPP), w/Bonnet-sensei @y7amura D√©fago-sensei, is out on arXiv! Throughout the paper, we address a bunch of questions ‚Äì both theory and practice.\n<LINK>\n\nvideo üé•\n<LINK>']",https://arxiv.org/abs/2105.07132,"This paper studies a novel planning problem for multiple agents moving on graphs that we call offline time-independent multi-agent path planning (OTIMAPP). The motivation is to overcome time uncertainties in multi-agent scenarios where we cannot expect agents to act perfectly following timed plans, e.g., executions with mobile robots. For this purpose, OTIMAPP abandons all timing assumptions; it is offline planning that assumes event-driven executions without or less run-time effort. The problem is finding plans to be terminated correctly in any action orders of agents, i.e., guaranteeing that all agents eventually reach their destinations. We address a bunch of questions for this problem: required conditions for feasible solutions, computational complexity, comparison with well-known other multi-agent problems, construction of solvers, effective relaxation of a solution concept, and how to implement the plans by actual robots. Throughout the paper, we establish the foundation of OTIMAPP and demonstrate its utility. A video is available at this https URL ",Offline Time-Independent Multi-Agent Path Planning
74,1394615124338565127,109255123,Danny Caballero üá≤üáΩ,['New submitted paper on computational thinking written by a teamed up postdoc (Dan) and undergrad researcher (Theo). Some really complete work on their part. Proud they got this done. <LINK>'],http://arxiv.org/abs/2105.07981,"Computational thinking has been a recent focus of education research within the sciences. However, there is a dearth of scholarly literature on how best to teach and to assess this topic, especially in disciplinary science courses. Physics classes with computation integrated into the curriculum are a fitting setting for investigating computational thinking. In this paper, we lay the foundation for exploring computational thinking in introductory physics courses. First, we review relevant literature to synthesize a set of potential learning goals that students could engage in when working with computation. The computational thinking framework that we have developed features 14 practices contained within 6 different categories. We use in-class video data as existence proofs of the computational thinking practices proposed in our framework. In doing this work, we hope to provide ways for teachers to assess their students' development of computational thinking, while also giving physics education researchers some guidance on how to study this topic in greater depth. ","Developing a learning goal framework for computational thinking in
  computationally integrated physics classrooms"
75,1394614280478240771,23980621,"Brett Morris, PhD","['New @ESA_CHEOPS GO program paper by me, @KevinHeng1, @42Lendl and more: hunting for planetesimals orbiting white dwarfs!\n\n<LINK> <LINK>', 'Back in 2019, @KevinHeng1 and I started talking about our mutual interest in white dwarfs. White dwarfs are the white-hot embers of stellar cores that remain into the stellar afterlife, after a star swells into a giant and blows off its outer layers.', 'One of my favorite parts of this job is using astrophysics as a time machine. By looking at young stars we can ""go back in time"" to see what the young Sun might have been like. But we can also use this trick to see the future: the Sun one day will leave behind a white dwarf. https://t.co/da9ZkY7cEx', ""Since the stars which become white dwarfs often have planets, we can use space telescopes like @ESA_CHEOPS to take precise photometry to hunt for transits of the planetary remnants orbiting white dwarfs, and foretell our Solar System's fate. https://t.co/2AiF72gu5r"", 'We observed six white dwarfs for 24 hours each, taking images of the WDs once per minute. @ESA_CHEOPS measured the brightness of the white dwarfs precisely enough to see changes of only two parts per thousand. And, we found... *drumroll please*... https://t.co/ArOWIPyVQZ', ""...nothing! The white dwarfs didn't show any transit events, which could mean one of a few things: either they don't have transiting material on short orbital periods of a few hours, that material is quite small, or that material is inclined so we can't see it."", 'We would have been most sensitive to material orbiting on periods of 3-8 hours, with significant drops in detection efficiency near multiples of the @ESA_CHEOPS orbital period of 100 minutes... https://t.co/f9gMLYHb83', ""...and we would have been sensitive to incredibly small planetesimals, with great detection efficiency over objects 1000 km or larger: about half the size of the Earth's Moon! üåí https://t.co/3bKOOCnBZW"", ""Though we didn't catch anything, we're super thrilled and grateful for the opportunity to go fishing with this exquisite photometric machine. \n\nAnd of course, lots more @ESA_CHEOPS results are in the pipeline from the GTO side, so get ready, here they come! https://t.co/vEdtpVsLJk"", '@dstndstn @ESA_CHEOPS @KevinHeng1 @42Lendl I love this rendering too üòÖ']",https://arxiv.org/abs/2105.07987,"White dwarf spectroscopy shows that nearly half of white dwarf atmospheres contain metals that must have been accreted from planetary material that survived the red giant phases of stellar evolution. We can use metal pollution in white dwarf atmospheres as flags, signalling recent accretion, in order to prioritize an efficient sample of white dwarfs to search for transiting material. We present a search for planetesimals orbiting six nearby white dwarfs with the CHEOPS spacecraft. The targets are relatively faint for CHEOPS, $11$ mag $< G < 12.8$ mag. We use aperture photometry data products from the CHEOPS mission as well as custom PSF photometry to search for periodic variations in flux due to transiting planetesimals. We detect no significant variations in flux that cannot be attributed to spacecraft systematics, despite reaching a photometric precision of $<2$ ppt in 60 s exposures on each target. We simulate observations to show that the small survey is sensitive primarily to Moon-sized transiting objects with periods $3$ hr $< P < 10$ hr, with radii $R \gtrsim 1000$ km. ",A CHEOPS White Dwarf Transit Search
76,1394538581520224256,1133734070293323776,Hammer Lab ML,"['New paper alertü•≥ Our work on ""Convex optimization for actionable &amp; plausible #CounterfactualExplanations"" by Andr√© Artelt and Barbara Hammer is now available as a preprint on #arXiv - check it out: <LINK>']",http://arxiv.org/abs/2105.07630,"Transparency is an essential requirement of machine learning based decision making systems that are deployed in real world. Often, transparency of a given system is achieved by providing explanations of the behavior and predictions of the given system. Counterfactual explanations are a prominent instance of particular intuitive explanations of decision making systems. While a lot of different methods for computing counterfactual explanations exist, only very few work (apart from work from the causality domain) considers feature dependencies as well as plausibility which might limit the set of possible counterfactual explanations. In this work we enhance our previous work on convex modeling for computing counterfactual explanations by a mechanism for ensuring actionability and plausibility of the resulting counterfactual explanations. ","Convex optimization for actionable \& plausible counterfactual
  explanations"
77,1394475305054531588,1367634215320252417,Kenji Itao / ÊùøÂ∞æÂÅ•Âè∏,"['Our new paper is available at arXiv. Using multi-level evolutionary simulation, we show that various kinship structures and descent systems spontaneously emerge by considering social ties and competition between families owing to marriage relationships. \n<LINK>', 'We also explain simulation results analytically, and conduct empirical data analyses to support them. We believe such collaboration between theoretical and empirical approaches will unveil universal features in anthropology.']",https://arxiv.org/abs/2105.08014,"In many indigenous societies, people are categorised into several cultural groups, or clans, within which they believe to share ancestors. Clan attributions provide certain rules for marriage and descent. Such rules between clans constitute kinship structures. Anthropologists have revealed several kinship structures. Here, we propose an agent-based model of indigenous societies to reveal the evolution of kinship structures. In the model, several societies compete. Societies themselves comprise multiple families with parameters for cultural traits and mate preferences. These values determine with whom each family cooperates and competes and are transmitted to a new generation with mutation. The growth rate of each family is determined by the number of cooperators and competitors. Through this multi-level evolution, family traits and preferences diverge to form clusters that can be regarded as clans. Subsequently, kinship structures emerge, including dual organisation and generalised or restricted exchange, as well as patrilineal, matrilineal, and double descent systems. These structures emerge depending on the necessity of cooperation and the strength of mating competition. Their dependence is also estimated analytically. Finally, statistical analysis using the Standard Cross-Cultural Sample, a global ethnographic database, empirically verified theoretical results. Such collaboration between theoretical and empirical approaches will unveil universal features in anthropology. ","Emergence of Kinship Structures and Descent Systems: Multi-level
  Evolutionary Simulation and Empirical Data Analysis"
78,1394470774086328324,593407733,Sayak Paul,"['New work w/ @pinyuchenTW - ""Vision Transformers are Robust Learners""\n\nWith self-attention and other design choices, can vision transformers provide improved robustness to common corruptions, perturbations, etc.?\n\nPaper: <LINK>\nCode: <LINK>\n\n1/n', 'We build on top of existing works &amp; investigate the robustness aspects of ViT.\n\nThrough a series of six systematically designed experiments, we present analyses that provide both quantitative &amp; qualitative indications to explain why ViTs are indeed more robust learners. \n\n2/n https://t.co/xNBvxHMGWJ', 'The framework of our experiments is extensible and flexible. We hope future work will be able to build on top of it and further foster the body of work on the intriguing properties of ViTs.\n\n3/n', 'Huge thanks to @pinyuchenTW for being patient and for providing an immense amount of support from all fronts. \n\nVery thankful to @GoogleDevExpert program for GCP credit support. @ksoonson &amp; @kweinmeister you folks rock!']",https://arxiv.org/abs/2105.07581,"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at this https URL ",Vision Transformers are Robust Learners
79,1394459262118682624,2427351685,Rick Betzel,['Second new paper tweet of the day. This time a brand new review article from newly-minted PhD @joshfasky and @spornslab | Edges in Brain Networks: Contributions to Models of Structure and Function | <LINK> <LINK>'],https://arxiv.org/abs/2105.07069,"Network models describe the brain as sets of nodes and edges that represent its distributed organization. So far, most discoveries in network neuroscience have prioritized insights that highlight distinct groupings and specialized functional contributions of network nodes. Importantly, these functional contributions are determined and expressed by the web of their interrelationships, formed by network edges. Here, we underscore the important contributions made by brain network edges for understanding distributed brain organization. Different types of edges represent different types of relationships, including connectivity and similarity among nodes. Adopting a specific definition of edges can fundamentally alter how we analyze and interpret a brain network. Furthermore, edges can associate into collectives and higher-order arrangements, describe time series, and form edge communities that provide insights into brain network topology complementary to the traditional node-centric perspective. Focusing on the edges, and the higher-order or dynamic information they can provide, discloses previously underappreciated aspects of structural and functional network organization. ","Edges in Brain Networks: Contributions to Models of Structure and
  Function"
80,1394355141432709129,3100596960,Walter Scheirer,"['Lots of projects (especially in the DH world) can benefit from good automated handwriting analysis. A major confound (surprise) is when new things are encountered. My lab takes a look at this in our ICDAR 2021 paper ""Handwriting Recognition with Novelty""\n\n<LINK> <LINK>', 'Here we formalize the problem, introduce an agent-centric approach as a baseline solution, and introduce a new dataset and evaluation protocols. This is work coming out of the @DARPA SAIL-ON program, which has a terrific stable of problem domains with aspects of novelty.']",https://arxiv.org/abs/2105.06582,"This paper introduces an agent-centric approach to handle novelty in the visual recognition domain of handwriting recognition (HWR). An ideal transcription agent would rival or surpass human perception, being able to recognize known and new characters in an image, and detect any stylistic changes that may occur within or across documents. A key confound is the presence of novelty, which has continued to stymie even the best machine learning-based algorithms for these tasks. In handwritten documents, novelty can be a change in writer, character attributes, writing attributes, or overall document appearance, among other things. Instead of looking at each aspect independently, we suggest that an integrated agent that can process known characters and novelties simultaneously is a better strategy. This paper formalizes the domain of handwriting recognition with novelty, describes a baseline agent, introduces an evaluation protocol with benchmark data, and provides experimentation to set the state-of-the-art. Results show feasibility for the agent-centric approach, but more work is needed to approach human-levels of reading ability, giving the HWR community a formal basis to build upon as they solve this challenging problem. ",Handwriting Recognition with Novelty
81,1394297402983653377,1243544508983279617,Horng Sheng Chia,"['New paper today!\n\n<LINK>\n\nWe found that GW151226, the 2nd binary black hole detected by @LIGO, could have highly-unequal BH masses, with the heavier BH spinning very rapidly and tilted away from the orbit. These properties are rare among the known BBH population <LINK>', '@cplberry @DD_Baumann @LIGO Hi @cplberry! Indeed we plan to contact the waveform developers soon. From a quick read of their paper, it seems that they have reweighted samples from a flat-in-q prior to the flat-in-component-masses prior that we also use. We suspect this is the source of tension, but TBC!', '@samayanissanke @LIGO Thanks @samayanissanke and hope all is well!', '@cplberry A related comment is that, empirically, we found that the low-q mode occupies a smaller volume in likelihood space than the high-q mode. This means we had to use a large no. of live points in the sampler in order for the low-q mode, which has a larger log likelihood, to emerge!', '@cplberry That‚Äôs because in the plots, we have dialed the live points to such a large value that the low-q mode dominates over the high-q mode, and the posterior distributions do not change appreciably as more live points are adopted :)']",https://arxiv.org/abs/2105.06486,"We present a reanalysis of GW151226, the second binary black hole merger discovered by the LIGO-Virgo Collaboration. Previous analysis showed that the best-fit waveform for this event corresponded to the merger of a $\sim 14 \, M_\odot$ black hole with a $\sim 7.5 \, M_\odot$ companion. In this work, we perform parameter estimation using a waveform model that includes the effects of orbital precession and higher-order radiative multipoles, and find that the mass and spin parameters of GW151226 have bimodal posterior distributions. The two modes are separated in mass ratio, $q$: the high-$q$ mode ($0.4 \lesssim q < 1$) is consistent with the results reported in the literature. On the other hand, the low-$q$ mode ($q \lesssim 0.4$), which describes a binary with component masses of $\sim 29 \, M_\odot$ and $\sim \, 4.3 M_\odot$, is new. The low-$q$ mode has several interesting properties: (a) the secondary black hole mass may fall in the lower mass gap of astrophysical black hole population; and (b) orbital precession is driven by the primary black hole spin, which has a dimensionless magnitude as large as $\sim 0.88$ and is tilted away from the orbital angular momentum at an angle of $\sim 47^\circ$. The new low-$q$ mode has a log likelihood that is about six points higher than that of the high-$q$ mode, and can therefore affect the astrophysical interpretation of GW151226. Crucially, we show that the low-$q$ mode disappears if we neglect either higher multipoles or orbital precession in the parameter estimation. More generally, this work highlights how incorporating additional physical effects into waveform models used in parameter estimations can alter the interpretation of gravitational-wave sources. ","Boxing Day Surprise: Higher Multipoles and Orbital Precession in
  GW151226"
82,1394264179993808907,369569444,Takahiro TERADA (ÂØ∫Áî∞ ÈöÜÂ∫É),"['Our new paper on ""Massless Preheating and Electroweak Vacuum Metastability"" <LINK>\nPrecise measurements of Standard Model parameters suggest that the electroweak vacuum is metastable. We need to ensure the (meta)stability throughout cosmological history.', 'Quantum fluctuations of the Higgs field during cosmic inflation and parametric/tachyonic instability after that might destabilize the electroweak vacuum. We can stabilize the Higgs by introducing an effective mass term in the form of Higgs-inflaton and/or gravitational couplings.', 'However, the stability during and after inflation is typically in a trade-off relation. It is important to study the condition that ensures stability throughout these epochs. https://t.co/yXRKN1rELr', 'In our work, we focus on (quasi) scale-invariant models with quartic potentials and non-minimal gravitational couplings.  (Why scale invariance?  1. It can explain the flatness of the inflaton potential consistently with observations.  2. It might explain the hierarchy problem.) https://t.co/PX66DX1WKU', 'Naively, scale invariance implies unimpeded growth of resonance and hence unavoidable destabilization of the vacuum, which is a cosmological catastrophe. However, we find this is not the case taking into account the perturbative Higgs decay and backreaction of produced particles. https://t.co/onEH7axRcP', 'We find nontrivial and dynamical interplays between the effects of quartic and curvature couplings, which can partially cancel each other in some cases. We finally find disjoint ""islands of (meta)stability"" in the couplings parameter space. https://t.co/7SnuXWK6NK']",http://arxiv.org/abs/2105.06939,"Current measurements of Standard Model parameters suggest that the electroweak vacuum is metastable. This metastability has important cosmological implications, because large fluctuations in the Higgs field could trigger vacuum decay in the early universe. For the false vacuum to survive, interactions which stabilize the Higgs during inflation -- e.g., inflaton-Higgs interactions or non-minimal couplings to gravity -- are typically necessary. However, the post-inflationary preheating dynamics of these same interactions could also trigger vacuum decay, thereby recreating the problem we sought to avoid. This dynamics is often assumed catastrophic for models exhibiting scale invariance since these generically allow for unimpeded growth of fluctuations. In this paper, we examine the dynamics of such ""massless preheating"" scenarios and show that the competing threats to metastability can nonetheless be balanced to ensure viability. We find that fully accounting for both the backreaction from particle production and the effects of perturbative decays reveals a large number of disjoint ""islands of (meta)stability"" over the parameter space of couplings. Ultimately, the interplay among Higgs-stabilizing interactions plays a significant role, leading to a sequence of dynamical phases that effectively extend the metastable regions to large Higgs-curvature couplings. ",Massless Preheating and Electroweak Vacuum Metastability
83,1394196453929603078,1316512308466647040,Christoph Bergmeir,"['We have put together a new time series data repository for forecasting. It is dedicated to sets of series for cross-learning/global modelling, and to single but very long series, find it here:\nPaper link: <LINK>\nWebsite: <LINK>']",https://arxiv.org/abs/2105.06643,"Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms. ",Monash Time Series Forecasting Archive
84,1393516337318797317,1316746180085403655,Pierre Colombo,"['Our paper <LINK> proposing a new estimator of Mutual Information for Text data is accepted at @aclmeeting . \nWe use it to learn better-disentangled representations and apply it to fair classification and style transfert. Joint work with @PiantanidaPablo', 'This estimator is really easy to use and with it, you can obtain better control than just training an adversary or using existing MI estimators such as CLUB.\n@IBMResearch']",https://arxiv.org/abs/2105.02685,"Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data {either rely} on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code {or rely on minimising variational bounds of the mutual information between latent code and the value attribute}. {However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement.} {In contrast to} {adversarial methods}, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi's divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement {than state-of-the-art methods proposed for textual data}. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence. ","A Novel Estimator of Mutual Information for Learning to Disentangle
  Textual Representations"
85,1393323170032193536,2889619139,liubov üá∫üá¶ üè¥‚Äç‚ò†Ô∏èü§çüíô‚ù§,"['#arXiv  time: our new paper on delay propagation in networks, where we study perturbation spreading in Belgian train networks\n<LINK> #mobility #delay \nwork was started at wwcs2019 @complex_warsaw   and during @criparis @CriResearch 3 years almost‚ö°Ô∏è']",https://arxiv.org/abs/2105.06111,"Railway systems form an important means of transport across the world. However, congestions or disruptions may significantly decrease these systems' efficiencies, making predicting and understanding the resulting train delays a priority for railway organisations. Delays are studied in a wide variety of models, which usually simulate trains as discrete agents carrying delays. In contrast, in this paper, we define a novel model for studying delays, where they spread across the railway network via a diffusion-like process. This type of modelling has various advantages such as quick computation and ease of applying various statistical tools like spectral methods, but it also comes with limitations related to the directional and discrete nature of delays and the trains carrying them. We apply the model to the Belgian railways and study its performance in simulating the delay propagation in severely disrupted railway situations. In particular, we discuss the role of spatial aggregation by proposing to cluster the Belgian railway system into sets of stations and adapt the model accordingly. We find that such aggregation significantly increases the model's performance. For some particular situations, a non-trivial optimal level of spatial resolution is found on which the model performs best. Our results show the potential of this type of delay modelling to understand large-scale properties of railway systems. ",Modelling railway delay propagation as diffusion-like spreading
86,1393295327680110595,273200904,Sean Kross,"['I am so excited to announce that my new paper ‚ÄúOrienting, Framing, Bridging, Magic, and Counseling: How Data Scientists Navigate the Outer Loop of Client Collaborations in Industry and Academia‚Äù has been accepted to #CSCW2021! You can read it here: <LINK> <LINK>', 'We detail the relationships that data scientists must manage every day. We explore how data scientists build trust with clients, the ways work comes to data scientists, how they ask questions to understand their clients‚Äô problems, and how they help clients cope with results. https://t.co/EqiSWrW0Nz']",https://arxiv.org/abs/2105.05849,"Data scientists often collaborate with clients to analyze data to meet a client's needs. What does the end-to-end workflow of a data scientist's collaboration with clients look like throughout the lifetime of a project? To investigate this question, we interviewed ten data scientists (5 female, 4 male, 1 non-binary) in diverse roles across industry and academia. We discovered that they work with clients in a six-stage outer-loop workflow, which involves 1) laying groundwork by building trust before a project begins, 2) orienting to the constraints of the client's environment, 3) collaboratively framing the problem, 4) bridging the gap between data science and domain expertise, 5) the inner loop of technical data analysis work, 6) counseling to help clients emotionally cope with analysis results. This novel outer-loop workflow contributes to CSCW by expanding the notion of what collaboration means in data science beyond the widely-known inner-loop technical workflow stages of acquiring, cleaning, analyzing, modeling, and visualizing data. We conclude by discussing the implications of our findings for data science education, parallels to design work, and unmet needs for tool development. ","Orienting, Framing, Bridging, Magic, and Counseling: How Data Scientists
  Navigate the Outer Loop of Client Collaborations in Industry and Academia"
87,1393137321705447432,2176486874,Steven Thomson,"['Another new preprint out today!\n\n""Finding the phase diagram of strongly-correlated disordered bosons using quantum quenches"" by L. Villa, S. J. Thomson &amp; L. Sanchez-Palencia: <LINK>\n\n(This is a companion paper to <LINK> which came out yesterday!) <LINK>', 'In this longer paper, we develop the technique of quench spectroscopy further, and map out the zero temperature phase diagram of disordered bosons at unit filing using both spatially-resolved and momentum-resolved quench spectroscopy. üòÖ https://t.co/hOJ7u3aqMy', 'Our main result is that quench spectroscopy provides a new way to explore the zero-temperature phases of quantum matter with what we hope is a simplified experimental protocol, compared to established methods. üòÉ\n\nHoping to see some experimental groups try it in the near future!', ""If you missed my thread on yesterday's paper where I outlined the technique in more detail, you can find it here: üëá\n\nhttps://t.co/9qHJIcEeiI\n\nPlease do check out both of our papers - all comments and constructive criticism welcome! üòÉ""]",https://arxiv.org/abs/2105.06396,"The question of how the low-energy properties of disordered quantum systems may be connected to exotic localization phenomena at high energy is a key open question in the context of quantum glasses and many-body localization. In arXiv:2105.05774 we have shown that key features of the excitation spectrum of a disordered system can be efficiently probed from out-of-equilibrium dynamics following a quantum quench, providing distinctive signatures of the various phases. Here, we extend this work by providing a more in-depth study of the behavior of the quench spectral functions associated to different observables and investigating an extended parameter regime. We provide a detailed introduction to quench spectroscopy for disordered systems and show how spectral properties can be probed using both local operators and two-point correlation functions. We benchmark the technique using the one-dimensional Bose-Hubbard model in the presence of a random external potential, focusing on the low-lying excitations, and demonstrate that quench spectroscopy can distinguish the Mott insulator, superfluid, and Bose glass phases. We then explicitly reconstruct the zero-temperature phase diagram of the disordered Bose-Hubbard at fixed filling using two independent methods, both experimentally accessible via time-of-flight imaging and quantum gas microscopy respectively, and demonstrate that quench spectroscopy can give valuable insights as to the distribution of rare regions within disordered systems. ","Finding the phase diagram of strongly-correlated disordered bosons using
  quantum quenches"
88,1393117510183100422,328430286,Jad C. Halimeh,"['In our new paper <LINK>, we detect equilibrium and dynamical phase transitions through single-site observables. For critical quenches, we extract a truly out-of-equilibrium critical exponent.\n#OutOfEquilibriumCriticality\n#DynamicalPhaseTransitions\n@HaukeGroup <LINK>']",https://arxiv.org/abs/2105.05986,"Extracting critical behavior in the wake of quantum quenches has recently been at the forefront of theoretical and experimental investigations in condensed matter physics and quantum synthetic matter, with particular emphasis on experimental feasibility. Here, we investigate the potential of single-site observables in probing equilibrium phase transitions and dynamical criticality in short-range transverse-field Ising chains. For integrable and near-integrable models, our exact and mean-field-theory analyses reveal a truly out-of-equilibrium universal scaling exponent in the vicinity of the transition that is independent of the initial state and the location of the probe site so long as the latter is sufficiently close to the edge of the chain. Signature of a dynamical crossover survives when integrability is strongly broken. Our work provides a robust scheme for the experimental detection of quantum critical points and dynamical scaling laws in short-range interacting models using modern ultracold-atom setups. ","Probing Equilibrium and Dynamical Criticality Through Single-Site
  Observables"
89,1393111641248632835,2813229562,Ivan Vovk,"['1/n. Happy to announce that my team presents our new paper ‚ÄúGrad-TTS: A Diffusion Probabilistic Model for Text-to-Speech‚Äù which has been accepted to ICML 2021! Check it out: <LINK>. DEMO: <LINK>. The code will also be released shortly.', '2/n. Inspired by vocoder success of WaveGrad (@cnxhk, @heiga_zen et al) and DiffWave (@ZhifengKong, @wpingnet et al) as well as of the recent advances in score-based diffusion modeling (@YSongStanford et al), we designed a novel TTS approach for SOTA mel-spectrogram generation.', '3/n. Equipped with score-based decoder modeled through SDEs, fully-parallel Grad-TTS produces extremely natural mel-spectrograms from encoded and aligned text sequence resulting in speech of great quality, outperforming even autoregressive Tacotron2 by both MOS and RTF.', '4/n. (!!!) Besides yielding SOTA results in TTS with DPM, for ensembling purposes we introduced generalized forward and reverse SDEs for Gaussian prior with arbitrary mean and diagonal covariance matrix.', '5/n. It naturally helps to parameterize Grad-TTS decoder‚Äôs prior with text encoder outputs, which are forced to be as close as possible to the target. That means that instead of running backward ODE solver from pure noise, we start decoding from some initial mel reconstruction.', '6/n. As a consequence, it helps to reduce the overall number of ODE solver steps necessary for high-quality generation of Grad-TTS. It is sufficient to run as few as 10 iterations for excellent speech synthesis without any proper noise schedule search.', '7/n. Another motivation for it is that this setup helped us to adopt Monotonic Alignment Search algorithm from Glow-TTS, so Grad-TTS also learns phoneme durations in fully-unsupervised way, doesn‚Äôt need any external aligner, and can synthesize text of arbitrary length.', '8/n. Interestingly, Grad-TTS achieves much higher log-likelihood than Glow-TTS even though the latter has a decoder with 3x larger capacity and was trained to maximize the exact data likelihood. Similar phenomena were observed by @YSongStanford et al in the image generation task.', '9/n. It emphasizes the true power of diffusion probabilistic models. Personally I am still shocked how good this concept already works.', '10/n. Finally, I want to send love to all my co-authors: Vadim Popov, Vladimir Gogoryan (@vsgogoryan), Tasnima Sadekova, Mikhail Kudinov. Infinitely happy to be a part of our team.']",https://arxiv.org/abs/2105.06337,"Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly. ",Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech
90,1392969925241630725,990344816587309057,Dhruba Dutta Chowdhury,"[""New paper on today's arXiv with @DokkumPieter, Frank van den Bosch, and other collaborators on the dynamics of nuclear objects such as dense star clusters (SCs) or supermassive black holes (SMBHs) in a Fuzzy Dark Matter (FDM) halo: <LINK>"", 'We simulate a ~ 6 x 10^9 M_sun FDM halo for a boson mass of 8 x 10^-23 eV. It consists of a soliton (ground state, indicated by the red nugget) surrounded by a granular envelope (interfering excited states). The azimuthally averaged density is still NFW-like at large radii. https://t.co/52joCXmwry', 'The excited states also interfere with the soliton, causing it to oscillate and execute a random walk with respect to the halo center-of-mass. Soliton oscillations are shown below. Left: halo density at different times. Right: peak density evolution, corresponding power spectrum. https://t.co/9WQtebhf4R', 'The soliton random walk is highlighted here. The images show the offset of the soliton center from the halo center-of-mass at four different times. The maximum offset, as can be seen from the associated plot, is ~ 2 kpc, which is the same as the soliton radius. https://t.co/NwxVrVisqB', 'The wobbling, oscillating soliton perturbs point particles (representing dense SCs or SMBHs) initially placed at rest at the soliton center, causing them to diffuse outwards. Scattering by the halo-granules also contributes to this effect. Orbit of a 10^5 M_sun particle is shown. https://t.co/tMY79aA5rO', 'Results from statistical ensembles of massless and different mass particles. The outward diffusion for m &lt;= 10^6 M_sun is almost identical to that of the massless particles, indicating that they experience negligible dynamical friction within the runtime of the simulations. https://t.co/QSxMtCIkZk', 'In contrast, for m&gt;=5 x 10^6 M_sun, dynamical friction is important, leading to a reduced outward diffusion. For m=10^7 M_sun, the median particle radius stalls around ~ 1 kpc after ~ 1 Gyr, indicating a rough balance between the ensemble-averaged heating and cooling rates.', 'To summarize - in an FDM halo, nuclear objects will diffuse outwards, until the heating and cooling rates balance. Therefore, off-centered nuclear SCs and/or AGNs can be used to constrain the boson mass.', 'However, the specific results presented in this paper are only valid for the assumed halo and boson mass and will be generalized in a follow-up work.']",http://arxiv.org/abs/2105.05268,"Fuzzy Dark Matter (FDM), consisting of ultralight bosons ($m_{\rm b} \sim 10^{-22}\ \rm eV$), is an intriguing alternative to Cold Dark Matter. Numerical simulations that solve the Schr\""odinger-Poisson (SP) equation show that FDM halos consist of a central solitonic core, which is the ground state of the SP equation, surrounded by an envelope of interfering excited states. These excited states also interfere with the soliton, causing it to oscillate and execute a confined random walk with respect to the halo center of mass. Using high-resolution numerical simulations of a $6.6 \times 10^9 M_{\odot}$ FDM halo with $m_{\rm b} = 8 \times 10^{-23}\ \rm eV$ in isolation, we demonstrate that the wobbling, oscillating soliton gravitationally perturbs nuclear objects, such as supermassive black holes or dense star clusters, causing them to diffuse outwards. In particular, we show that, on average, objects with mass $\lesssim 0.3 \%$ of the soliton mass ($M_{\rm sol}$) are expelled from the soliton in $\sim 3\ \rm Gyr$, after which they continue their outward diffusion due to gravitational interactions with the soliton and the halo granules. More massive objects ($\gtrsim 1 \% M_{\rm sol}$), while executing a random walk, remain largely confined to the soliton due to dynamical friction. We also present an effective treatment of the diffusion, based on kinetic theory, that accurately reproduces the outward motion of low mass objects and briefly discuss how the observed displacements of star clusters and active galactic nuclei from the centers of their host galaxies can be used to constrain FDM. ",On the Random Motion of Nuclear Objects in a Fuzzy Dark Matter Halo
91,1392951171636154376,1068545181576773632,Kenneth Brown,"['Our new paper ""Optimizing Stabilizer Parities for Improved Logical Qubit Memories"" was posted to the arXiv yesterday \n<LINK> . Joint work with Chris Monroe\'s group @JQInews @DukeEngineering #DukeQuantumCenter', 'My latest PhD graduate @DriptoDebroy (now @GoogleQuantumAI ) and I have been thinking about coherent rotation errors on quantum error codes for the past year and a bit. We noticed that for Shor codes you could reduce this error on one axis by changing the sign of the stabilizers.', 'For even-distance Shor codes you can make the error vanish.  This is an example of a broader set of codes developed by our colleagues, Jingzhen Hu, Qingzhong Liang, @nrenga92, and Robert Calderbank https://t.co/Z0hYvVWQmq #DukeQuantumCenter', 'These codes are an example of weak collective decoherence free subspaces https://t.co/VHDc6r1dui, but are not concatenations of DFS with stabilizer codes like https://t.co/SppPLCHQUG. The DFS is part of the code.', 'Our codes are not decoherence free subspaces just decoherence reducing. From a physics standpoint, we replace the very phase sensitive GHZ states (|000&gt;+|111&gt;) with the less phase sensitive states (|010&gt;+|101&gt;).', 'Our experimental collaborators (Laird Egan, @crystalMIT13 , Andrew Risinger, Daiwei Zhu , Debopriyo Biswas, Marko Cetina, and Chris Monroe), tested it on 9-qubit states and saw a 4-fold increase in the logical T_2.', 'For the weight-2 Z checks, changing the stabilizer sign is equivalent to switching from stabilizing ferromagnetic states to stabilizing antiferromagnetic states. In the appendix, we show numerically that switching the signs for weight-6 Z checks on the dual Shor code also helps.', ""More broadly, for any stabilizer code, there are n-k subspaces that could be the code space. For independent random Pauli errors, it doesn't matter which subspace you use for your code. For correlated noise, it can make a huge difference."", ""@cgranade @nrenga92 Yes. Exactly. I think what's cool about Jingzhen's et al. construction is that they arrived at it from a totally different direction and the DFS is just part of the code."", 'edit: should be 2^(n-k) subspaces, where n is the number of data qubits and k is the number of logical qubits.']",http://arxiv.org/abs/2105.05068,"We study variants of Shor's code that are adept at handling single-axis correlated idling errors, which are commonly observed in many quantum systems. By using the repetition code structure of the Shor's code basis states, we calculate the logical channel applied to the encoded information when subjected to coherent and correlated single qubit idling errors, followed by stabilizer measurement. Changing the signs of the stabilizer generators allows us to change how the coherent errors interfere, leading to a quantum error correcting code which performs as well as a classical repetition code of equivalent distance against these errors. We demonstrate a factor of 4 improvement of the logical memory in a distance-3 logical qubit implemented on a trapped-ion quantum computer. Even-distance versions of our Shor code variants are decoherence-free subspaces and fully robust to identical and independent coherent idling noise. ",Optimizing Stabilizer Parities for Improved Logical Qubit Memories
92,1392874668344840200,1197450600004411392,Alessandro Cheli,['New paper is out on @arxiv about Symbolics.jl and its code generation capabilities. I worked with Shashi Gowda on integrating Metatheory.jl to optimize an ODE model of B Cell Antigen Signaling with a 2x speedup! <LINK>'],https://arxiv.org/abs/2105.03949,"As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. We demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We showcase an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation. ",High-performance symbolic-numerics via multiple dispatch
93,1392810927918141441,1002606609527443462,Dr. Angela Collier,"[""Our new paper on the arXiv yesterday: <LINK>\n\nI was down from my second Pfizer so I will post about it when it's published on ApJ.""]",https://arxiv.org/abs/2105.04698,"Resonant torques couple stellar bars to dark matter halos. Here we use high-resolution numerical simulations to demonstrate long-term angular momentum transfer between stellar bars and dark matter orbits of varying orientation. We show that bar-driven reversals of dark matter orbit orientations can play a surprisingly large role in the evolution of the bar pattern speed. In predominantly prograde (co-rotating) halos, dark matter orbits become trapped in the stellar bar forming a parallel dark matter bar. This dark matter bar reaches more than double the vertical height of the stellar bar. In halos dominated by retrograde orbits, a dark matter wake forms oriented perpendicular to the stellar bar. These dark matter over-densities provide a novel space to look for dark matter annihilation or decay signals. % We predict that the Milky Way hosts a dark matter bar aligned with the stellar bar as well as a dark matter wake the near-side of which should extend from Galactic center to a galactic longitude of $l \approx 323^\circ$. ",The Coupling of Galactic Dark Matter Halos with Stellar Bars
94,1392738370783166465,128206668,Alfred Castro,['Finally! New paper yesterday exploring the Milky Way spiral structure with open clusters @McMillan_Astro @xavier_luri @merce_mrg @CantatGaudin @frediferente available on <LINK> <LINK>'],https://arxiv.org/abs/2105.04590,"Context. The physical processes driving the formation of Galactic spiral arms are still under debate. Studies using open clusters favour the description of the Milky Way spiral arms as long-lived structures following the classical density wave theory. Current studies comparing the Gaia DR2 field stars kinematic information of the Solar neighbourhood to simulations, find a better agreement with short-lived arms with a transient behaviour. Aims. Our aim is to provide an observational, data-driven view of the Milky Way spiral structure and its dynamics using open clusters as the main tracers, and to contrast it with simulation-based approaches. We use the most complete catalogue of Milky Way open clusters, with astrometric Gaia EDR3 updated parameters, estimated astrophysical information and radial velocities, to re-visit the nature of the spiral pattern of the Galaxy. Methods. We use a Gaussian mixture model to detect overdensities of open clusters younger than 30 Myr that correspond to the Perseus, Local, Sagittarius and Scutum spiral arms, respectively. We use the birthplaces of the open cluster population younger than 80 Myr to trace the evolution of the different spiral arms and compute their pattern speed. We analyse the age distribution of the open clusters across the spiral arms to explore the differences in the rotational velocity of stars and spiral arms. Results. We are able to increase the range in Galactic azimuth where present-day spiral arms are described, better estimating its parameters by adding 264 young open clusters to the 84 high-mass star-forming regions used so far, thus increasing by a 314% the number of tracers. We use the evolution of the open clusters from their birth positions to find that spiral arms nearly co-rotate with field stars at any given radius, discarding a common spiral pattern speed for the spiral arms explored. [abridged] ",On the Milky Way spiral arms from open clusters in Gaia EDR3
95,1392737009601503235,2640805367,Milan Gritta üá¨üáß üá∏üá∞ üá∫üá¶,"[""Hey! New paper with @iiacobacNLP called 'XeroAlign: Zero-Shot Cross-lingual Transformer Alignment' to be published in Findings of ACL 2021 :) Read the preprint at <LINK> #huawei #NLProc <LINK>"", 'Use this simple technique to align the representations of XLM-RoBERTa (or other pretrained #multilingual transformers) across languages for strong zero-shot transfer. #noahsArkLab #Huawei https://t.co/vIZ7iLZ6fC', 'XeroAlign achieves SOTA scores on 3 cross-lingual task-oriented natural language understanding datasets. It also works well for text classification tasks like paraphrase detection. Simple and effective! #ACL2021 https://t.co/XqYqUsmk9i']",https://arxiv.org/abs/2105.02472,"The introduction of pretrained cross-lingual language models brought decisive improvements to multilingual NLP tasks. However, the lack of labelled task data necessitates a variety of methods aiming to close the gap to high-resource languages. Zero-shot methods in particular, often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign, a simple method for task-specific alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows strong improvements over the baseline models to achieve state-of-the-art zero-shot results on three multilingual natural language understanding tasks. XLM-RA's text classification accuracy exceeds that of XLM-R trained with labelled data and performs on par with state-of-the-art models on a cross-lingual adversarial paraphrasing task. ",XeroAlign: Zero-Shot Cross-lingual Transformer Alignment
96,1392642314905001988,1001049754787368960,Dr. Yu-Dai Tsai,"['Our new paper is out: <LINK>!\nI thank my collaborators Patrick Fitzpatrick, @hongwan_liu, and Tracy Slatyer for this very nice project üôÇ']",https://arxiv.org/abs/2105.05255,"We examine the vector-portal inelastic dark matter (DM) model with DM mass $m_\chi$ and dark photon mass $m_{A'}$, in the `forbidden dark matter' regime where $1 \lesssim m_{A'}/m_\chi \lesssim 2$, carefully tracking the dark sector temperature throughout freezeout. The inelastic nature of the dark sector relaxes the stringent cosmic microwave background (CMB) and self-interaction constraints compared to symmetric DM models. We determine the CMB limits on both annihilations involving excited states and annihilation into $e^+e^-$ through initial-state-radiation of an $A'$, as well as limits on the DM self-scattering, which proceeds at the one-loop level. The unconstrained parameter space serves as an ideal target for accelerator $A'$ searches, and provides a DM self-interaction cross section that is large enough to observably impact small-scale structure. ",New Thermal Relic Targets for Inelastic Vector-Portal Dark Matter
97,1392549336073003010,1200141005233836032,Pedro Fernandes,"['My new paper is out! <LINK>\nI generalize GR with a conformally coupled scalar field in a natural way, incorporating a very interesting scalar-Gauss-Bonnet sector in the theory. Closed-form BH solutions are found, along with a set of modified Friedmann equations!', 'The phenomenology is very similar to that of higher-dimensional GB theory. Particular cases of my model appear in various contexts, namely in trace anomalies and dimensional regularizations of GB gravity. It is a good setup to probe in-vogue scalar-GB effects analytically.']",https://arxiv.org/abs/2105.04687,"We naturally extend the theory of gravity with a conformally coupled scalar field by only requiring conformal invariance of the scalar field equation of motion and not of the action. The classically extended theory incorporates a scalar-Gauss-Bonnet sector and has second-order equations of motion, belonging to the Horndeski class. Remarkably, the theory features a purely geometrical field equation that allows for closed-form black hole solutions and cosmologies to be easily found. These solutions permit investigations of in-vogue scalar-Gauss-Bonnet corrections to the gravitational action without the need of resorting to approximations or numerical methods. We discuss on the connection to the recently formulated 4D Einstein-Gauss-Bonnet theory of gravity. ",Gravity with a generalized conformal scalar field: Theory and solutions
98,1392487027355099137,3053460216,Matthias Niessner,"['Check out 4DComplete &amp; the DeformableThings4D dataset, which is now publicly available!\n\n-&gt; estimate non-rigid motion from partial scans\n-&gt; new dataset with 1,972 non-rigid sequences\n\nVideo: <LINK>\nPaper: <LINK>\nData: <LINK> <LINK>']",https://arxiv.org/abs/2105.01905,"Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and incompleteness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences. ",4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface
99,1392448222392733697,335920279,Ben Burningham,"['Here\'s a thread about our new paper: ""Cloudbusting: enstatite and quartz clouds in 2M2224-0158"". This paper is a deep dive into the clouds of an L dwarf, using data-driven retrievals to explore its atmosphere and determine what its clouds are made of <LINK>\n1/11', 'Huge thanks and heartfelt appreciation to co-authors @jfaherty,@EGonzales788, @astromarkmarley, @ChannonVisscher, @Josefine_Gaarn, @mfbieger, Richard Freedman and Didier Saumon for their support and patience in helping it see the light of day - it has been a lot of work \n2/11', 'In this paper we have used the ""Brewster"" retrieval framework to test a HUGE number of atmospheric parameterisations against spectroscopy spanning 1-15um for a red L dwarf, 2M2224-0158. Thanks to @UnioHerts High Performance Computing Cluster for making this possible \n3/11', 'Our retrieval model is able to fit these data spectacularly, including the ""silicate cloud"" absorption feature at around 9um, which hasn\'t been successfully fit before. 4/n https://t.co/YbJ2QTS0b7', 'Now, ""everyone knows"" L dwarfs are cloudy, and lots of work has been done modelling which clouds are expected to form, and at what pressures and so on. But, what has been lacking are data-driven insights as to which clouds we actually see, until now... \n5/n', 'We threw the kitchen sink of cloud species and arrangements at this object, and found that the data point to cloud opacity from small grains of quartz and enstatite at low-pressures (&lt;0.1bar) with a deeper cloud deck, extending beyond 10bar, dominated by iron particles. \n6/11', ""Finding quartz with enstatite and NO forsterite is a really interesting result. Quartz is predicted by Helling's microphysical cloud models, but not expected from phase equilibrium models. And both phase equilibrium and microphysical models predict forsterite!\n7/11"", ""But - all cloud models (so far) have assumed solar composition. Solar composition has Mg / Si = 1.05, and the minerals we're talking about are SiO2(quartz), MgSiO3(enstatite), and Mg2SiO4(forsterite). Varying Mg/Si either side of 1 can have a big impact on which clouds form!\n8/11"", 'Clouds are often seen as obstacles that hinder characterisation of alien atmospheres. But, analysis like ours can use clouds to decode even more information, including hard to access mineral ratios like Mg/Si, which are of such interest for exoplanets \n9/11', ""This is the first time we've been able to test the predictions of cloud models against data like this. But, it won't be the last - this is really laying the groundwork for what we'll be able to do with #JWST for all sorts of brown dwarfs and exoplanets. \n10/11"", 'Anyhow - we also found a bunch of other interesting stuff in this analysis so check out the paper!\n11/11', '@paulmolli Cheers! I hope it was worth the wait!', '@WhataMergerJr https://t.co/x9zVw0cUIe']",https://arxiv.org/abs/2105.04268,"We present the most detailed data-driven exploration of cloud opacity in a substellar object to-date. We have tested over 60 combinations of cloud composition and structure, particle size distribution, scattering model, and gas phase composition assumptions against archival $1-15 {\rm \mu m}$ spectroscopy for the unusually red L4.5~dwarf 2MASSW~J2224438-015852 using the Brewster retrieval framework. We find that, within our framework, a model that includes enstatite and quartz cloud layers at shallow pressures, combined with a deep iron cloud deck fits the data best. This models assumes a Hansen distribution for particle sizes for each cloud, and Mie scattering. We retrieved particle effective radii of $\log_{10} a {\rm (\mu m)} = -1.41^{+0.18}_{-0.17}$ for enstatite, $-0.44^{+0.04}_{-0.20}$ for quartz, and $-0.77^{+0.05}_{-0.06}$ for iron. Our inferred cloud column densities suggest ${\rm (Mg/Si)} = 0.69^{+0.06}_{-0.08}$ if there are no other sinks for magnesium or silicon. Models that include forsterite alongside, or in place of, these cloud species are strongly rejected in favour of the above combination. We estimate a radius of $0.75 \pm 0.02$ Rjup, which is considerably smaller than predicted by evolutionary models for a field age object with the luminosity of 2M2224-0158. Models which assume vertically constant gas fractions are consistently preferred over models that assume thermochemical equilibrium. From our retrieved gas fractions we infer ${\rm [M/H]} = +0.38^{+0.07}_{-0.06}$ and ${\rm C/O} = 0.83^{+0.06}_{-0.07}$. Both these values are towards the upper end of the stellar distribution in the Solar neighbourhood, and are mutually consistent in this context. A composition toward the extremes of the local distribution is consistent with this target being an outlier in the ultracool dwarf population. ","Cloud busting: enstatite and quartz clouds in the atmosphere of
  2M2224-0158"
100,1392439468032921600,256442599,Patrick Fernandes,"['Recent work has tried to incorporate context into MT, but are models actually using the context at translation time?\n\nIn our new #ACL2021NLP paper (w/ @kayo_yin @gneubig @andre_t_martins) we attempt to measure and increase context usage by MT models\n\n<LINK>\n\n1/6 <LINK>', 'Typically work on context-aware MT relies on standard metrics (BLEU) and contrastive datasets to evaluate context usage\n\nHowever, increases in BLEU are hard to tie to context usage and contrastive datasets only provide a limited measure of it on specific discourse phenomena\n\n2/6', 'Inspired by @ebugliarello et al work, we propose conditional cross mutual information (CXMI), to measure how much probabilistic MT models use context\n\nInformally, CXMI measures the information gained when a model is given the context vs when it is only given the source\n\n3/6 https://t.co/77Eqt6htTI', 'Using CXMI, we find that MT models use target context more than source context and there are diminishing returns in including larger contexts\n\nThis applies to both smaller and more powerful (pretrained) models!\n\n4/6 https://t.co/HQMazpX4Jx', 'We then propose a simple regularization technique, CoWord dropout, to incentivize models to leverage context more\n\nWe find the CoWord dropout not only increases context usage according to CXMI but also improves translation quality and performance on contrastive datasets\n\n5/6 https://t.co/gbopoqTJ8p', ""I would like to thank my amazing collaborators for all the help! Any feedback is welcome and the camera-ready will include some more experiments and corrections\n\nThis was work done in the scope of the @CMUPortugal's MAIA project\n\nCode: https://t.co/u18kUf8K7e\n\n7/7""]",http://arxiv.org/abs/2105.03482,"Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context -- context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify the usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that conditioning on a longer context has a diminishing effect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method increases context usage and that this reflects on the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets. ","Measuring and Increasing Context Usage in Context-Aware Machine
  Translation"
101,1392430171240677376,216729597,Marcel S. Pawlowski,"['New paper: ‚ÄúOn the absence of backsplash analogues to NGC 3109 in the ŒõCDM framework‚Äù lead by Indranil Banik, with @benfamaey. We looked at NGC3109, which was suggested to be a backsplash galaxy (entered the virial radius of the MW, but then left it again) <LINK> <LINK>', 'I have been interested in the NGC 3109 association since @DudeDarkmatter and I wrote a paper on it a while back (https://t.co/EzmaTh4RsI). NGC 3109 and some other dwarf galaxies at similar distance form a peculiar linear arrangement and seem to recede too fast from the Milky Way. https://t.co/McfCdclSf6', 'Back then I already suspected that it is unlikely to get backsplash galaxies to appear at such distances within a LCDM framework. However, that was just based on a few dark-matter-only simulations, and didn‚Äôt consider the rather high mass of NGC 3109. https://t.co/zaEOweHFLr', 'In the new paper, we make use of the superior statistics of the Illustris TNG-300 simulation. We found that backsplash galaxies as massive (‚â•4.0 √ó 10^10 Msun) and as distant (‚â•1.2 Mpc) as NGC 3109 are extremely rare. https://t.co/EGaOrQXjQq', 'Yet, NGC3109 has an additional peculiarity: it flies away too fast, the Galactiocentric velocity is 170 km/s. While the Hubble flow predicts some receding velocity, taking into account Local Group masses with a timing argument analysis still implies a velocity excess of 105 km/s.', 'To explain the rather high receding velocity of NGC 3109, one would like a backsplash galaxy to gain energy in the encounter with the host. That doesn‚Äôt happen often in the simulation, but it does happen sometimes (e.g. the black line in the plot). https://t.co/RyCzbHgn67', 'Yet, none of the simulated NGC 3109 analogs that are as massive and as distant as the observed one have gained any energy in the encounter. That‚Äôs probably because dynamical friction acts stronger on more massive galaxies like NGC 3109. https://t.co/f6UIYFy8wz', 'So, it is really unlikely to find a galaxy as massive as NGC 3109, at a similar distance, and with an as high velocity in this LCDM simulation. We confirmed that the same applies to the dark-matter-only run. It is thus unlikely NGC 3109 is a backsplash galaxy if LCDM is correct. https://t.co/0LSuUirGHH', ""In MOND dynamical friction is of less concern. The timing argument also mandates a past encounter between the MW and M31 (known baryonic masses + stronger acceleration, plot from https://t.co/JyewImIvoE). That could've flung out dwarf galaxies to large distances with high speeds. https://t.co/dU2aOYK8vL"", ""NGC3109 might be backsplash in that scenario, though MOND is so different from LCDM that I wonder whether the term even makes sense. The galaxy could've also formed during the encounter or have been a bound satellite before being expelled. Things to maybe look into in the future."", 'By the way, Indranil will move from Bonn to another postdoc position in Saint Andrews in September with his former PhD Hongsheng Zhao, where he will mainly work on testing gravity with wide binary stars.', ""@deniserkal There's some scatter of about 25 km/s around the expected radial velocity. So pushing the excess from 100 km/s to 50 km/s would help quite a bit.\nNow, if your next question is on the MW's reflex motion due to the LMC: that's already included in calculating the velocity excess. üòâ"", '@deniserkal What I mean comes from the series of Banik &amp; Zhao papers (2016, 2017, 2018). Figs. 2 &amp; 3 are just straight velocities of galaxies out of the simulation catalog, but yes, in your scenario there would be a shift to the left in those.', '@deniserkal Fig. 3 shows the distribution of velocities of simulated dwarf galaxies relative to the mean radial velocity of all dwarfs around this host, considering only systems with at least five nearby dwarf galaxies.', '@deniserkal The LMC is only taken into account for the observed NGC 3109 galaxy, in the timing-argument analysis. It is not there in the simulated systems, for which only dwarfs at a distance comparable to NGC 3109 are considered. The averages are only made for those distant dwarfs.', '@deniserkal In other words, the question we ask in Figs 2&amp;3 is: how many dwarfs, that are at a comparable distance from their host as NGC3109 is from the MW, would show a similar high velocity excess.', ""@maximetrebitsch Good point, but unfortunately no. The higher-res sims simply don't have enough analogs to compare to.\nMy gut feeling is that DF might act more strongly with better resolution, in which case backsplash would be slowed down more. Definitely something worth looking into."", ""@deniserkal Hm, yeah, difficult to know its orbit at that distance. Sure, one could probably make a few reasonable assumptions, but if NGC 3109 ever came close to the MW in the past, that would've been way before the LMC was around. So maybe this much detail isn't even necessary?"", '@maximetrebitsch You could of course relax the requirement for the mass of NGC3109 to get better statistics, but that in turn reduces the dynamical friction effect.\nYes, I was going to say that, too. Idealized experiments should be ok, at least to assess how much of an effect the resolution has.', ""@maximetrebitsch Now where do we find someone who's willing to do that work ‚Ä¶"", ""@maximetrebitsch Oh, and shouldn't the stripping be reduced for higher resolutions, too? Which in turn results in more massive dwarfs and thus again in more DF ‚Ä¶\n\nOk, I'll stop speculating now.""]",https://arxiv.org/abs/2105.04575,"The dwarf galaxy NGC 3109 is receding 105 km/s faster than expected in a $\Lambda$CDM timing argument analysis of the Local Group and external galaxy groups within 8 Mpc (Banik \& Zhao 2018). If this few-body model accurately represents long-range interactions in $\Lambda$CDM, this high velocity suggests that NGC 3109 is a backsplash galaxy that was once within the virial radius of the Milky Way and was slingshot out of it. Here, we use the Illustris TNG300 cosmological hydrodynamical simulation and its merger tree to identify backsplash galaxies. We find that backsplashers as massive ($\geq 4.0 \times 10^{10} M_\odot$) and distant ($\geq 1.2$ Mpc) as NGC 3109 are extremely rare, with none having also gained energy during the interaction with their previous host. This is likely due to dynamical friction. Since we identified 13225 host galaxies similar to the Milky Way or M31, we conclude that postulating NGC 3109 is a backsplash galaxy causes $>3.96\sigma$ tension with the expected distribution of backsplashers in $\Lambda$CDM. We show that the dark matter only version of TNG300 yields much the same result, demonstrating its robustness to how the baryonic physics is modelled. If instead NGC 3109 is not a backsplasher, consistency with $\Lambda$CDM would require the 3D timing argument analysis to be off by 105 km/s for this rather isolated dwarf, which we argue is unlikely. We discuss a possible alternative scenario for NGC 3109 and the Local Group satellite planes in the context of MOND, where the Milky Way and M31 had a past close flyby $7-10$ Gyr ago. ","On the absence of backsplash analogues to NGC 3109 in the $\Lambda$CDM
  framework"
102,1392411727434715137,1279014829718679552,cigdem.beyan,['Wanna be a ‚Äúgood‚Äù public speaker? üòç\nCheck out our new paperüëáthat introduces the most fine-grained presentation competence instrument so far and extensive analyses regarding the effectiveness of several nonverbal behaviors.\n<LINK>\n#publicspeaking #AI <LINK>'],https://arxiv.org/abs/2105.02636,"Public speaking and presentation competence plays an essential role in many areas of social interaction in our educational, professional, and everyday life. Since our intention during a speech can differ from what is actually understood by the audience, the ability to appropriately convey our message requires a complex set of skills. Presentation competence is cultivated in the early school years and continuously developed over time. One approach that can promote efficient development of presentation competence is the automated analysis of human behavior during a speech based on visual and audio features and machine learning. Furthermore, this analysis can be used to suggest improvements and the development of skills related to presentation competence. In this work, we investigate the contribution of different nonverbal behavioral cues, namely, facial, body pose-based, and audio-related features, to estimate presentation competence. The analyses were performed on videos of 251 students while the automated assessment is based on manual ratings according to the T\""ubingen Instrument for Presentation Competence (TIP). Our classification results reached the best performance with early fusion in the same dataset evaluation (accuracy of 71.25%) and late fusion of speech, face, and body pose features in the cross dataset evaluation (accuracy of 78.11%). Similarly, regression results performed the best with fusion strategies. ","Estimating Presentation Competence using Multimodal Nonverbal Behavioral
  Cues"
103,1392402393149677576,52147188,Guadalupe Ca√±as Herrera,"['New paper alert <LINK>‚ÄºÔ∏èüì¢\n\n@ValeriVardanyan, Omar Contigiani and I have focused on studying correlations between GW mergers and galaxies üî≠\n\nThe goal: reconstruct the Luminosity Distance and the bias ratios between both populations, using Gaussian Processes üßë\u200düíª <LINK>']",https://arxiv.org/abs/2105.04262,"Soon, the combination of electromagnetic and gravitational signals will open the door to a new era of gravitational-wave (GW) cosmology. It will allow us to test the propagation of tensor perturbations across cosmic time and study the distribution of their sources over large scales. In this work, we show how machine learning techniques can be used to reconstruct new physics by leveraging the spatial correlation between GW mergers and galaxies. We explore the possibility of jointly reconstructing the modified GW propagation law and the linear bias of GW sources, as well as breaking the slight degeneracy between them by combining multiple techniques. We show predictions roughly based on a network of Einstein Telescopes combined with a high-redshift galaxy survey ($z\lesssim3$). Moreover, we investigate how these results can be re-scaled to other instrumental configurations. In the long run, we find that obtaining accurate and precise luminosity distance measurements (extracted directly from the individual GW signals) will be the most important factor to consider when maximizing constraining power. ","Learning how to surf: Reconstructing the propagation and origin of
  gravitational waves with Gaussian Processes"
104,1392376737359699969,1707692827,Paul McMillan,"[""New paper on the Milky Way's spiral arms lead by @alfredcas - <LINK> <LINK>"", 'This paper started life when Alfred came for a 6 week visit at the end of February last year - as you can imagine this got cut short by circumstances beyond our control...', '... but we/he persevered, and now we have this great paper to show for it!']",https://arxiv.org/abs/2105.04590,"Context. The physical processes driving the formation of Galactic spiral arms are still under debate. Studies using open clusters favour the description of the Milky Way spiral arms as long-lived structures following the classical density wave theory. Current studies comparing the Gaia DR2 field stars kinematic information of the Solar neighbourhood to simulations, find a better agreement with short-lived arms with a transient behaviour. Aims. Our aim is to provide an observational, data-driven view of the Milky Way spiral structure and its dynamics using open clusters as the main tracers, and to contrast it with simulation-based approaches. We use the most complete catalogue of Milky Way open clusters, with astrometric Gaia EDR3 updated parameters, estimated astrophysical information and radial velocities, to re-visit the nature of the spiral pattern of the Galaxy. Methods. We use a Gaussian mixture model to detect overdensities of open clusters younger than 30 Myr that correspond to the Perseus, Local, Sagittarius and Scutum spiral arms, respectively. We use the birthplaces of the open cluster population younger than 80 Myr to trace the evolution of the different spiral arms and compute their pattern speed. We analyse the age distribution of the open clusters across the spiral arms to explore the differences in the rotational velocity of stars and spiral arms. Results. We are able to increase the range in Galactic azimuth where present-day spiral arms are described, better estimating its parameters by adding 264 young open clusters to the 84 high-mass star-forming regions used so far, thus increasing by a 314% the number of tracers. We use the evolution of the open clusters from their birth positions to find that spiral arms nearly co-rotate with field stars at any given radius, discarding a common spiral pattern speed for the spiral arms explored. [abridged] ",On the Milky Way spiral arms from open clusters in Gaia EDR3
105,1392322029030973440,3250001664,Colin Hill,"['New paper led by Leander Thiele + @yilun_guan, w/ A. Kosowsky + @DavidSpergel: <LINK>  We show that small-scale baryon clumping models (e.g., primordial magnetic fields) aiming to resolve the H0 tension are highly constrained by @ACT_Pol data (+ @Planck) #H0 #CMB', 'Key result in Fig. 2: https://t.co/ykUj0NRtuD', ""@nu_phases Planck does do quite well on its own. ACT's most important role here is as an independent test of the model; ACT's constraining power comes from very different range in TT+TE+EE than Planck. If the model were true, one would expect shift in ACT params w.r.t. Planck. But not seen"", '@nu_phases (and yes, adding ACT does also further cut the high-H0 tail of the posteriors)']",https://arxiv.org/abs/2105.03003,"Small-scale inhomogeneities in the baryon density around recombination have been proposed as a solution to the tension between local and global determinations of the Hubble constant. These baryon clumping models make distinct predictions for the cosmic microwave background anisotropy power spectra on small angular scales. We use recent data from the Atacama Cosmology Telescope to test these predictions. No evidence for baryon clumping is found, assuming a range of parameterizations for time-independent baryon density probability distribution functions. The inferred Hubble constant remains in significant tension with the SH0ES measurement. ","Can small-scale baryon inhomogeneities resolve the Hubble tension? An
  investigation with ACT DR4"
106,1392318699282079744,18850305,Zachary Lipton,"['Hey @UCSDJacobs, @ucsd_cse, @HDSIUCSD friends, I\'ll be ""at"" UCSD Thursday to talk about label shift &amp; label noise, inc new paper on spiking training sets w unlabeled data (with random labels assigned) to guarantee generalization (<LINK>).\n\n<LINK>', '@limufar @UCSDJacobs @ucsd_cse @HDSIUCSD There\'s a ""website"" link on the linked page.']",https://arxiv.org/abs/2105.00303,"To assess generalization, machine learning scientists typically either (i) bound the generalization gap and then (after training) plug in the empirical risk to obtain a bound on the true risk; or (ii) validate empirically on holdout data. However, (i) typically yields vacuous guarantees for overparameterized models. Furthermore, (ii) shrinks the training set and its guarantee erodes with each re-use of the holdout set. In this paper, we introduce a method that leverages unlabeled data to produce generalization bounds. After augmenting our (labeled) training set with randomly labeled fresh examples, we train in the standard fashion. Whenever classifiers achieve low error on clean data and high error on noisy data, our bound provides a tight upper bound on the true risk. We prove that our bound is valid for 0-1 empirical risk minimization and with linear classifiers trained by gradient descent. Our approach is especially useful in conjunction with deep learning due to the early learning phenomenon whereby networks fit true labels before noisy labels but requires one intuitive assumption. Empirically, on canonical computer vision and NLP tasks, our bound provides non-vacuous generalization guarantees that track actual performance closely. This work provides practitioners with an option for certifying the generalization of deep nets even when unseen labeled data is unavailable and provides theoretical insights into the relationship between random label noise and generalization. ",RATT: Leveraging Unlabeled Data to Guarantee Generalization
107,1392301806739345408,1191056593476915200,Ray Bai,"['New preprint w/ @BalocchiCecilia, @MaryRBoland, @spcanelon, @chenyong1203, Ed George, &amp; Jessica Liu! ""A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of Adverse Pregnancy Outcomes.""  Read our paper here:  <LINK> <LINK>', 'In this work, we develop geospatial mixed effects logistic regression models for adverse pregnancy outcomes that account for spatial autocorrelation and heterogeneity between neighborhoods. 1/2', 'We identify several informative patient-level and neighborhood-level covariates for stillbirth and preterm birth. We identify the neighborhoods in the city of Philadelphia at greatest risk of these adverse pregnancy outcomes. 2/2']",https://arxiv.org/abs/2105.04981,"Studying the determinants of adverse pregnancy outcomes like stillbirth and preterm birth is of considerable interest in epidemiology. Understanding the role of both individual and community risk factors for these outcomes is crucial for planning appropriate clinical and public health interventions. With this goal, we develop geospatial mixed effects logistic regression models for adverse pregnancy outcomes. Our models account for both spatial autocorrelation and heterogeneity between neighborhoods. To mitigate the low incidence of stillbirth and preterm births in our data, we explore using class rebalancing techniques to improve predictive power. To assess the informative value of the covariates in our models, we use posterior distributions of their coefficients to gauge how well they can be distinguished from zero. As a case study, we model stillbirth and preterm birth in the city of Philadelphia, incorporating both patient-level data from electronic health records (EHR) data and publicly available neighborhood data at the census tract level. We find that patient-level features like self-identified race and ethnicity were highly informative for both outcomes. Neighborhood-level factors were also informative, with poverty important for stillbirth and crime important for preterm birth. Finally, we identify the neighborhoods in Philadelphia at highest risk of stillbirth and preterm birth. ","A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of
  Adverse Pregnancy Outcomes"
108,1392301382397353992,762420558,Ciaran O'Hare,"[""New paper out now: A cookbook for setting limits on dark photon dark matter. <LINK>\n\nWe attempt to address the complexity involved in translating between axion signals and dark photon signals. Namely, how to deal with the dark photon's polarisation <LINK>"", 'The DP polarisation leads to strong direction and time-dependence that people seemed to have not really investigated much until now. But accounting for it is easy and can improve limits https://t.co/f4XetOduyn', 'We also make some suggestions for how future experiments could enhance their sensitivity. This leads to some amusingly specific recommendations like the cardinal direction that antennae should be pointed, and the best latitudes to do an experiment https://t.co/VyFP4AGhLZ', 'The code for this paper is rather inelegantly distributed across two separate github pages: https://t.co/DdniHEenXj and https://t.co/dPYRK1jQ1x']",https://arxiv.org/abs/2105.04565,"The dark photon is a massive hypothetical particle that interacts with the Standard Model by kinetically mixing with the visible photon. For small values of the mixing parameter, dark photons can evade cosmological bounds to be a viable dark matter candidate. Due to the similarities with the electromagnetic signals generated by axions, several bounds on dark photon signals are simply reinterpretations of historical bounds set by axion haloscopes. However, the dark photon has a property that the axion does not: an intrinsic polarisation. Due to the rotation of the Earth, accurately accounting for this polarisation is nontrivial, highly experiment-dependent, and depends upon assumptions about the dark photon's production mechanism. We show that if one does account for the DP polarisation, and the rotation of the Earth, an experiment's discovery reach can be enhanced by over an order of magnitude. We detail the strategies that would need to be taken to properly optimise a dark photon search. These include judiciously choosing the location and orientation of the experiment, as well as strategically timing any repeated measurements. Experiments located at $\pm$35$^\circ$ or $\pm$55$^\circ$ latitude, making three observations at different times of the sidereal day, can achieve a sensitivity that is fully optimised and insensitive to the dark photon's polarisation state, and hence its production mechanism. We also point out that several well-known searches for axions employ techniques for testing signals that preclude their ability to set exclusion limits on dark photons, and hence should not be reinterpreted as such. ",Dark photon limits: a handbook
109,1392187171319205890,857151967055015937,Vincent Dutordoir,"['New paper out where we show how to make a forward pass through a Deep GP equivalent to a ReLU DNN. Another step towards unifying DGPs and DNNs.\n \n<LINK>', '@jameshensman and @lawrennd (2014) pointed out the strong similarity between DNNs and DGPs posteriors, but the missing ingredients were fast inference and GP basis functions that induce neural network style activation functions. https://t.co/xPyCnsaUeB', 'In Dutordoir et al. (2020), we published an important building block to accommodate for this: showing how zonal kernels on the hypersphere have a Mercer decomposition comprising spherical harmonics - giving rise to its RKHS.\n\nhttps://t.co/WZ77rhatLH', 'In this paper, we use this RKHS to construct an interdomain inducing variable that leads to GP basis functions c(.) that are approximately identical to the activation functions in neural nets.\n\nLeft: standard RBF basis functions\nRight: our ReLU basis functions https://t.co/awPZPZTQM6', 'Stacking these GP layers on top of each other gives a Deep GP for which the approximate posterior mean is equivalent to a DNN - allowing us to initialise a DGP with a point estimate from training a normal DNN, and speeding up the overall training time.', 'We provide a consistent way to initialise a Deep GP by using a DNN as opposed to a single layer as in Sun et al. (2020). We also focus on activation functions that are commonly used in DNNs and show the necessity of using the ArcCosine kernel to avoid a mismatch in the spectrum. https://t.co/tn68s10ni4', 'Very fortunate to have worked on this with @jameshensman, @markvanderwilk, @carlhenrikek, @ZoubinGhahrama1 and @NicolasDurrande', '@jordigraumo Thanks Jordi']",https://arxiv.org/abs/2105.04504,"Neural networks and Gaussian processes are complementary in their strengths and weaknesses. Having a better understanding of their relationship comes with the promise to make each method benefit from the strengths of the other. In this work, we establish an equivalence between the forward passes of neural networks and (deep) sparse Gaussian process models. The theory we develop is based on interpreting activation functions as interdomain inducing features through a rigorous analysis of the interplay between activation functions and kernels. This results in models that can either be seen as neural networks with improved uncertainty prediction or deep Gaussian processes with increased prediction accuracy. These claims are supported by experimental results on regression and classification datasets. ",Deep Neural Networks as Point Estimates for Deep Gaussian Processes
110,1391957647348310017,2569631268,Daniel Huber,"['Check out our latest paper on the arxiv tonight, led by @UHIfA graduate student Jingwen Zhang, presenting a new misaligned multiplanet system with a long-period Jovian perturber discovered using @keckobservatory. The #Kepler mission keeps on giving! <LINK>']",https://arxiv.org/abs/2105.03446,"We present the discovery of Kepler-129 d ($P_{d}=7.2^{+0.4}_{-0.3}$ yr, $m\sin i_{d}=8.3^{+1.1}_{-0.7}\ \rm M_{Jup}$, $ e_{d}=0.15^{+0.07}_{-0.05} $) based on six years of radial velocity (RV) observations from Keck/HIRES. Kepler-129 also hosts two transiting sub-Neptunes: Kepler-129 b ($P_{b}=15.79$ days, $r_{b}=2.40\pm{0.04}\ \rm{R_{\oplus}}$) and Kepler-129 c ($P_{c}=82.20$ days, $r_{c}=2.52\pm{0.07}\ \rm{R_{\oplus}}$) for which we measure masses of $m_{b}<20\ \rm{M_{\oplus}}$ and $m_{c}=43^{+13}_{-12}\ \rm{M_{\oplus}}$. Kepler-129 is an hierarchical system consisting of two tightly-packed inner planets and an external companion whose mass is close to the deuterium burning limit. In such a system, two inner planets precess around the orbital normal of the outer companion, causing their inclinations to oscillate with time. Based on an asteroseismic analysis of Kepler data, we find tentative evidence that Kepler-129 b and c are misaligned with stellar spin axis by $\gtrsim 38$ deg, which could be torqued by Kepler-129 d if it is inclined by $\gtrsim 19$ deg relative to inner planets. Using N-body simulations, we provide additional constraints on the mutual inclination between Kepler-129 d and inner planets by estimating the fraction of time during which two inner planets both transit. The probability that two planets both transit decreases as their misalignment with Kepler-129 d increases. We also find a more massive Kepler-129 c enables the two inner planets to become strongly coupled and more resistant to perturbations from Kepler-129 d. The unusually high mass of Kepler-129 c provides a valuable benchmark for both planetary dynamics and interior structure, since the best-fit mass is consistent with this $\rm{2.5\ R_{\oplus}}$ planet having a rocky surface. ","Long Period Jovian Tilts the Orbits of Two sub-Neptunes Relative to
  Stellar Spin Axis in Kepler-129"
111,1391829045889863683,32713500,Rob Nowak,"['What kinds of functions do deep neural networks learn?  Now we know!\n<LINK>\nOur paper presents a new representer theorem for deep ReLU networks and provides theoretical insights into  weight decay, sparsity, skip connections, and low-rank weight matrices.', '@roydanroy The main result says that a solution to a variational problem in a certain Banach space (compositional Radon BV) is a deep ReLU network having widths proportional to sample size, and that this optimization is equivalent to one over NNs that is min loss+ridge penalty on weights.', '@roydanroy It really falls together seamlessly.  I think these are the ‚Äúright‚Äù spaces to be working with, at least for multilayer relu nets.']",https://arxiv.org/abs/2105.03361,"We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines. ","What Kinds of Functions do Deep Neural Networks Learn? Insights from
  Variational Spline Theory"
112,1391795594335866882,1319101874532978690,jason wei,"['For years, I\'ve wanted to read a survey on data augmentation for NLP. Alas, @stevenyfeng and @VarunGangal made it happen.\n\nCheck out our new survey paper ""A Survey of Data Augmentation Approaches for NLP"", to appear in ACL Findings:\n\n<LINK>']",http://arxiv.org/abs/2105.03075,"Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at this https URL ",A Survey of Data Augmentation Approaches for NLP
113,1391697628015087616,481539448,Richard Alexander,"['New paper, led by @ajwinter93. We analysed the observed architectures of known exoplanet systems, and found that things might actually be simpler than we thought.\n\n<LINK>', ""This is a follow-up to Andrew's recent paper showing there are significant differences between exoplanet systems in different environments. Many exoplanet systems have been perturbed, &amp; the architectures of those systems may not reflect their formation. \n\nhttps://t.co/rWB3UG7Wqc"", 'In this new paper we focus on the systems which have probably *not* been perturbed: Kepler multiples, and RV systems in low-density environments. The orbital periods and radii/masses of these planets are consistent with a simple model where there is an upper limit on planet mass. https://t.co/uyf9xdPMnS', 'This upper limit - surprisingly - has the same functional form as the isolation mass for planets forming in a canonical protoplanetary disc. This means that unperturbed planetary systems can in principle be explained by a simple model of in situ growth up to the isolation mass.', ""I was not expecting this result - it suggests that planet migration (and various other processes) may be far less important than we thought. To be honest, I'm still not entirely sure what this result is telling us. However..."", ""...if a very simple model can explain the observations, then we should treat more complex models/analyses with caution. And it's clear that if we really want to link observed exoplanets to their formation, we need to focus on systems which have not been externally perturbed."", '@nomadastro @ajwinter93 For individual systems you can\'t know for sure (it\'s a statistical analysis), and some ""unperturbed"" systems may indeed not have pristine dynamical histories. But the perturbed systems have very different architectures, so should be treated separately in, e.g., population models.']",https://arxiv.org/abs/2105.02907,"The exotic range of known planetary systems has provoked an equally exotic range of physical explanations for their diverse architectures. However, constraining formation processes requires mapping the observed exoplanet population to that which initially formed in the protoplanetary disc. Numerous results suggest that (internal or external) dynamical perturbation alters the architectures of some exoplanetary systems. Isolating planets that have evolved without any perturbation can help constrain formation processes. We consider the Kepler multiples, which have low mutual inclinations and are unlikely to have been dynamically perturbed. We apply a modelling approach similar to that of Mulders et al. (2018), additionally accounting for the two-dimensionality of the radius ($R =0.3-20\,R_\oplus$) and period ($P= 0.5-730$ days) distribution. We find that an upper limit in planet mass of the form $M_{\rm{lim}} \propto a^\beta \exp(-a_{\rm{in}}/a)$, for semi-major axis $a$ and a broad range of $a_{\rm{in}}$ and $\beta$, can reproduce a distribution of $P$, $R$ that is indistinguishable from the observed distribution by our comparison metric. The index is consistent with $\beta= 1.5$, expected if growth is limited by accretion within the Hill radius. This model is favoured over models assuming a separable PDF in $P$, $R$. The limit, extrapolated to longer periods, is coincident with the orbits of RV-discovered planets ($a>0.2$ au, $M>1\,M_{\rm{J}}$) around recently identified low density host stars, hinting at isolation mass limited growth. We discuss the necessary circumstances for a coincidental age-related bias as the origin of this result, concluding that such a bias is possible but unlikely. We conclude that, in light of the evidence that some planetary systems have been dynamically perturbed, simple models for planet growth during the formation stage are worth revisiting. ",An upper limit for the growth of inner planets?
114,1391665648418410497,14326341,Amedeo Balbi,['New research paper: Feasibility of Detecting Interstellar Panspermia in Astrophysical Environments <LINK>'],https://arxiv.org/abs/2105.03295,"The proposition that life can spread from one planetary system to another (interstellar panspermia) has a long history, but this hypothesis is difficult to test through observations. We develop a mathematical model that takes parameters such as the microbial survival lifetime, the stellar velocity dispersion, and the dispersion of ejecta into account in order to assess the prospects for detecting interstellar panspermia. We show that the correlations between pairs of life-bearing planetary systems (embodied in the pair-distribution function from statistics) may serve as an effective diagnostic of interstellar panspermia, provided that the velocity dispersion of ejecta is greater than the stellar dispersion. We provide heuristic estimates of the model parameters for various astrophysical environments, and conclude that open clusters and globular clusters appear to represent the best targets for assessing the viability of interstellar panspermia. ","Feasibility of Detecting Interstellar Panspermia in Astrophysical
  Environments"
115,1391631234279694341,16079444,Ying-Jer Kao,['New paper with @MasakiOshikawa and Pochung Chen on  Two-wire Junction of Inequivalent Tomonaga-Luttinger Liquids\n\n<LINK>'],https://arxiv.org/abs/2105.03104,"We develop two novel numerical schemes to study the conductance of the two-wire junction of inequivalent Tomonaga-Luttinger Liquids. In the first scheme we use the static current-current correlation function across the junction to extract the linear conductance through a relation that is derived via the bosonization method. In the second scheme we apply a bias and evaluate the time-dependent current across the junction to obtain the current-voltage characteristic. The conductance is then extracted from the small bias result within the linear response regime. Both schemes are based on the infinite size matrix product state to minimize the finite-size effects. Due to the lack of the translational invariance, we focus on a finite-size window containing the junction. For time-independent calculations, we use infinite boundary conditions to evaluate the correlations within the window. For time-dependent calculations, we use the window technique to evaluate the local currents within the window. The numerical results obtained by both schemes show excellent agreement with the analytical predictions. ",Two-wire Junction of Inequivalent Tomonaga-Luttinger Liquids
116,1391353801953746951,1215058162551873537,Elizabeth Bondi-Kelly,"['Very excited to share a new paper, entitled ""Envisioning Communities: A Participatory Approach Towards AI for Social Good,"" at @AIESConf / #AIES2021. Thanks to my amazing co-authors @lilyxu0, @dacostanavas, @JacksonAKillian. Preprint posted at <LINK> (1/4) <LINK>', 'In our paper, we present PACT, a framework for including community members as partners throughout an AI for social good project, including to define ""good"" in their context.  (2/4)', 'We use the capabilities and participatory approaches as conceptual tools. The capabilities approach focuses\non the notion that all human beings should have a set of substantive liberties that allow them to function in society in the ways they choose. (3/4)', 'We also provide guiding questions for AI researchers implementing this framework. We, as AI researchers dedicated to the advancement of social good, must make a PACT with communities to find our way forward together. (4/4)']",http://arxiv.org/abs/2105.01774,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good. ","Envisioning Communities: A Participatory Approach Towards AI for Social
  Good"
117,1390723037419409410,22148802,Leo C. Stein ü¶Å,"['üö® New paper day! üö® This one was led by @KeefeMitman, a grad student at Caltech in the @SXSProject. This paper is on making a unique choice for the ""frame"" of gravitational wave predictions coming from numerical relativity simulations.\n<LINK>\nüßµ 1/ <LINK>', 'In numerical relativity, we predict waveforms seen by a family of observers ""infinitely"" far away from a source (for example, merging black holes). We imagine putting a family of observers on an enormous sphere surrounding the source. But there is still coordinate freedom!\n2/ https://t.co/RB3YccpdD9', 'A basic tenet of general relativity is that you should be able to use any coordinate system you want. But when you change coordinate systems, the gravitational-wave predictions change. In the 1960s, a bunch of folks figured out how much freedom there is, infinitely far away.\n3/', 'This freedom is called the ""BMS group"", and we need to make a choice of ""BMS frame"" to be precise about our predictions. This includes things like where is the center of mass of the binary, how fast it\'s moving, and more subtle quantities, all relative to our giant sphere.\n4/', ""Before, we were measuring the center of mass motion using coordinate trajectories of the black holes within the spacetime‚Äîwhich doesn't mean much, because coordinates don't mean much!\n\nNow we have a much better ways.\n5/ https://t.co/lJi8GS5k6z"", 'The algebra of the BMS group gives us ""charge integrals"" that we can compute that tell us, from the asymptotic waveforms themselves, where is the center of mass, what is the momentum, angular momentum, even something called ""supermomentum"".\n6/ https://t.co/gf74vNvV9s', 'We want to make these center-of-mass charges oscillate around 0 early in the simulations. Here you can see that without correction, and with our old approach to choosing the BMS frame, there was a drift over time, because the center of mass was drifting.\n7/ https://t.co/kmKH44eQvs', 'By using the waveform itself, and an iterative procedure, we can find the translation and Lorentz boost to get rid of this drift. Œ±, Œ≤: translation and boost parameters. As we iterate, we get closer and closer to a final Poincar√© transformation.\n8/ https://t.co/YR2cEstIwC', 'But that\'s not all we have to nail down. We also have to nail down ""supertranslations"", which correspond to angle-dependent time delays for our family of observers on the giant sphere. If we don\'t fix supertranslations, then some of the (2,2) mode mixes into the (2,1) mode\n9/ https://t.co/3GdYi8Bs9k', 'For this particular system, the prediction is a vanishing (2,1) mode. It\'s ‚â†0 for various reasons, among them the fact that we\'re in the ""wrong"" BMS frame. When we fix the frame, two things happen:\ni) the amplitude goes down by 1000, and\nii) the frequency gets halved. Why?\n10/ https://t.co/dP5Z07IZJD', 'The post-Newtonian prediction for the (2,1) mode for this system is that it oscillates at the orbital frequency, while the dominant (2,2) mode oscillates at *twice* the orbital frequency. The wrong frame is mixing the bigger (2,2) mode into the (2,1) mode.\n11/', 'When we fix the supertranslation, we can remove the unwanted 2*œâ contribution from the (2,2) mode, leaving only the slower oscillation at œâ, which is 1000 times smaller in amplitude!\n12/', ""Here's a summary quantifying how good this procedure is, by matching to post-Newtonian waveforms. If we do nothing, we have ~10% errors between NR and PN (in a window 4 orbits long). If we only do a global time+phase alignment, it goes down a touch. When we fix the BMS frame,\n13/ https://t.co/zAgAzNLX8o"", 'when we fix the BMS frame, our matching between PN and NR improves by a factor of 10^4 or 10^5 for nonprecessing systems!\n14/ https://t.co/WFpltyhvd5', ""There's more in this paper‚Äîif you're into GR waveform modeling, please check it out. https://t.co/76xIEy79yx\nAlso if you need more relativity experts in your feed, follow @KeefeMitman and send him any questions you have about relativity/GWs/BHs!\n15/15"", '""a much better ways"" ü§¶üèª I need to learn to proofread', 'Erratum for the thread! (Can we make this a thing?) @KeefeMitman corrects my bad memory:\nhttps://t.co/T9oEbLrwuU']",https://arxiv.org/abs/2105.02300,"Understanding the Bondi-Metzner-Sachs (BMS) frame of the gravitational waves produced by numerical relativity is crucial for ensuring that analyses on such waveforms are performed properly. It is also important that models are built from waveforms in the same BMS frame. Up until now, however, the BMS frame of numerical waveforms has not been thoroughly examined, largely because the necessary tools have not existed. In this paper, we show how to analyze and map to a suitable BMS frame for numerical waveforms calculated with the Spectral Einstein Code (SpEC). However, the methods and tools that we present are general and can be applied to any numerical waveforms. We present an extensive study of 13 binary black hole systems that broadly span parameter space. From these simulations, we extract the strain and also the Weyl scalars using both SpECTRE's Cauchy-characteristic extraction module and also the standard extrapolation procedure with a displacement memory correction applied during postprocessing. First, we show that the current center-of-mass correction used to map these waveforms to the center-of-mass frame is not as effective as previously thought. Consequently, we also develop an improved correction that utilizes asymptotic Poincar\'e charges instead of a Newtonian center-of-mass trajectory. Next, we map our waveforms to the post-Newtonian (PN) BMS frame using a PN strain waveform. This helps us find the unique BMS transformation that minimizes the $L^{2}$ norm of the difference between the numerical and PN strain waveforms during the early inspiral phase. We find that once the waveforms are mapped to the PN BMS frame, they can be hybridized with a PN strain waveform much more effectively than if one used any of the previous alignment schemes, which only utilize the Poincar\'e transformations. ",Fixing the BMS Frame of Numerical Relativity Waveforms
118,1390709745909211138,785763100712665088,Yaroslav Ganin,"['New paper day.\n\nOur take on generation of structured objects:\nProtocol Buffers + Transformers + Pointer Nets\n\nWe showcase the method on 2D CAD sketches (geometric primitives &amp; relations between them). Should work for other domains too.\n\n<LINK>\n\nMandatory samples: <LINK>', 'A close-up view for those who got dizzy https://t.co/myjvPfhO2s', 'Look at this guy - he is so happy to get vectorized! (we do bitmap to sketch translation too). This one is out of distribution - I just doodled it in an app (see the image in the bottom left corner) https://t.co/CsGlVLa8qv', '(joint work w/ @sbos, @liyuajia, Ethan Keller and Stefano Saliceti)', '1/ Some details. Thread\n\n2D sketches are at the heart of mechanical CAD. Each sketch is a collections of entities (lines, arcs, splines) and constraints (""this line is parallel to that"", ""this point and that point are coincident"" and so on). The latter defines the design intent. https://t.co/cPCg6QODBg', ""2/ Both entities and constraints are structured objects and can be described using JSON, XML or Protocol Buffers (the path we take). Below are two examples. Since constraints are applied to entities they employ pointers to refer their arguments (that's why we need Pointer Nets). https://t.co/a9hjG5fk6f"", '3/ Our approach is to let an external interpreter handle the structure and use Transformers only to generate missing bits (e.g., the field values). We encode each missing bit as a triplet: (discrete value, continuous value, boolean flag). The flag is needed to handle loops. https://t.co/2t231CdtXq', ""4/ This Transformer+Interpreter tandem is flexible enough to generate pretty much anything that we can represent as a PB message. That's why we can synthesize both entities and constraints in the same sequence without resorting to using specialized multi-stage architectures.""]",http://arxiv.org/abs/2105.02769,"Computer-Aided Design (CAD) applications are used in manufacturing to model everything from coffee mugs to sports cars. These programs are complex and require years of training and experience to master. A component of all CAD models particularly difficult to make are the highly structured 2D sketches that lie at the heart of every 3D construction. In this work, we propose a machine learning model capable of automatically generating such sketches. Through this, we pave the way for developing intelligent tools that would help engineers create better designs with less effort. Our method is a combination of a general-purpose language modeling technique alongside an off-the-shelf data serialization protocol. We show that our approach has enough flexibility to accommodate the complexity of the domain and performs well for both unconditional synthesis and image-to-sketch translation. ",Computer-Aided Design as Language
119,1390704157808414724,1375527360666107904,John Martyn,"['Excited to share our new paper ‚ÄúA Grand Unification of Quantum Algorithms‚Äù! \n\nWe survey the quantum singular value transformation and showcase how it encompasses the three major quantum algorithms (search, simulation, and phase estimation): <LINK> <LINK>']",http://arxiv.org/abs/2105.02859,"Quantum algorithms offer significant speedups over their classical counterparts for a variety of problems. The strongest arguments for this advantage are borne by algorithms for quantum search, quantum phase estimation, and Hamiltonian simulation, which appear as subroutines for large families of composite quantum algorithms. A number of these quantum algorithms were recently tied together by a novel technique known as the quantum singular value transformation (QSVT), which enables one to perform a polynomial transformation of the singular values of a linear operator embedded in a unitary matrix. In the seminal GSLW'19 paper on QSVT [Gily\'en, Su, Low, and Wiebe, ACM STOC 2019], many algorithms are encompassed, including amplitude amplification, methods for the quantum linear systems problem, and quantum simulation. Here, we provide a pedagogical tutorial through these developments, first illustrating how quantum signal processing may be generalized to the quantum eigenvalue transform, from which QSVT naturally emerges. Paralleling GSLW'19, we then employ QSVT to construct intuitive quantum algorithms for search, phase estimation, and Hamiltonian simulation, and also showcase algorithms for the eigenvalue threshold problem and matrix inversion. This overview illustrates how QSVT is a single framework comprising the three major quantum algorithms, thus suggesting a grand unification of quantum algorithms. ",A Grand Unification of Quantum Algorithms
120,1390656236333457410,72781449,Nikos Aletras,['New #ACL2021NLP findings paper w/ @DTsarapatsanis  on the ethical limits of legal #nlproc adding to the debate the importance of academic freedom and the diversity of ethical and legal norms (<LINK>) <LINK>'],https://arxiv.org/abs/2105.02751,"Natural language processing (NLP) methods for analyzing legal text offer legal scholars and practitioners a range of tools allowing to empirically analyze law on a large scale. However, researchers seem to struggle when it comes to identifying ethical limits to using NLP systems for acquiring genuine insights both about the law and the systems' predictive capacity. In this paper we set out a number of ways in which to think systematically about such issues. We place emphasis on three crucial normative parameters which have, to the best of our knowledge, been underestimated by current debates: (a) the importance of academic freedom, (b) the existence of a wide diversity of legal and ethical norms domestically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. ",On the Ethical Limits of Natural Language Processing on Legal Text
121,1390643200910573569,2790406081,D.Tsarapatsanis,['A new paper written jointly with @nikaletras on the ethics of legal #NLP placing emphasis on #academic_freedom has been accepted to appear in Findings of the upcoming #ACL2021NLP conference. Pre-print available here: <LINK>'],https://arxiv.org/abs/2105.02751,"Natural language processing (NLP) methods for analyzing legal text offer legal scholars and practitioners a range of tools allowing to empirically analyze law on a large scale. However, researchers seem to struggle when it comes to identifying ethical limits to using NLP systems for acquiring genuine insights both about the law and the systems' predictive capacity. In this paper we set out a number of ways in which to think systematically about such issues. We place emphasis on three crucial normative parameters which have, to the best of our knowledge, been underestimated by current debates: (a) the importance of academic freedom, (b) the existence of a wide diversity of legal and ethical norms domestically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. ",On the Ethical Limits of Natural Language Processing on Legal Text
122,1390593190173024256,738769492122214400,Johannes Lischner,"['In our new paper (accepted to @CommsPhys, <LINK>) we discover that photoemission satellites are a sensitive probe for interlayer hybridization of the hole wavefunction in PdCoO2. Great collab with @advlightsource! #xps #compchem <LINK>']",https://arxiv.org/abs/2105.02455,"When a three-dimensional material is constructed by stacking different two-dimensional layers into an ordered structure, new and unique physical properties can emerge. An example is the delafossite PdCoO2, which consists of alternating layers of metallic Pd and Mott-insulating CoO2 sheets. To understand the nature of the electronic coupling between the layers that gives rise to the unique properties of PdCoO2, we revealed its layer-resolved electronic structure combining standing-wave X-ray photoemission spectroscopy and ab initio many-body calculations. Experimentally, we have decomposed the measured valence band spectrum into contributions from Pd and CoO2 layers. Computationally, we find that many-body interactions in Pd and CoO2 layers are highly different. Holes in the CoO2 layer interact strongly with charge-transfer excitons in the same layer, whereas holes in the Pd layer couple to plasmons in the Pd layer. Interestingly, we find that holes in states hybridized across both layers couple to both types of excitations (charge-transfer excitons or plasmons), with the intensity of photoemission satellites being proportional to the projection of the state onto a given layer. This establishes satellites as a sensitive probe for inter-layer hybridization. These findings pave the way towards a better understanding of complex many-electron interactions in layered quantum materials. ","Layer-Resolved Many-Electron Interactions in Delafossite PdCoO2 from
  Standing-Wave Photoemission Spectroscopy"
123,1390580017403412480,2270640056,Henry Legg,"[""Correlated zero-bias peaks? A gap opening and closing measured via local and non-local conductance? \n\nCAUTION: These are not the Majorana bound states you're looking for! (they're trivial Andreev bound states)\n\nRead more in our new paper:\n<LINK> <LINK>"", ""@DmitryPesin @spinespresso Wait you're meant to find them? I thought the field of not finding Majoranas had been around since at least 2012üòã"", ""@spinespresso @DmitryPesin Ahh now I finally understand your talk earlier this week: It's important to know how one might accidentally find them in order to avoid killing the field üòâ https://t.co/UCjSH5oZGc"", ""@spinespresso @DmitryPesin If you think the fields have a finite overlap how can you be sure it's not just one field with a finite support at the other end? Did you even check that they're quantised in the fundamental quantum of fieldness?!?"", '@spinespresso @DmitryPesin I will not have an *experimentalist* lecture me on field theory! üò°üò°üò°', '@spinespresso @DmitryPesin Got it: AÃ∂nÃ∂dÃ∂rÃ∂eÃ∂eÃ∂vÃ∂ Ã∂bÃ∂oÃ∂uÃ∂nÃ∂dÃ∂ Ã∂sÃ∂tÃ∂aÃ∂tÃ∂eÃ∂sÃ∂ ""Majorana"" bound states.']",https://arxiv.org/abs/2105.02791,"We analyze Andreev bound states (ABSs) that form in normal sections of a Rashba nanowire that is only partially covered by a superconducting layer. These ABSs are localized close to the ends of the superconducting section and can be pinned to zero energy over a wide range of magnetic field strengths even if the nanowire is in the non-topological regime. For finite-size nanowires (typically $\lesssim 1$ $\mu$m in current experiments), the ABS localization length is comparable to the length of the nanowire. The probability density of an ABS is therefore non-zero throughout the nanowire and differential-conductance calculations reveal a correlated zero-bias peak (ZBP) at both ends of the nanowire. When a second normal section hosts an additional ABS at the opposite end of the superconducting section, the combination of the two ABSs can mimic the closing and reopening of the bulk gap in local and non-local conductances accompanied by the appearance of the ZBP. These signatures are reminiscent of those expected for Majorana bound states (MBSs) but occur here in the non-topological regime. Our results demonstrate that conductance measurements of correlated ZBPs at the ends of a typical superconducting nanowire or an apparent closing and reopening of the bulk gap in the local and non-local conductance are not conclusive indicators for the presence of MBSs. ","Local and non-local quantum transport due to Andreev bound states in
  finite Rashba nanowires with superconducting and normal sections"
124,1390533715726458888,1228949827213348864,Thrupthi Ann John,['Check out our new paper:\nCanonical Saliency Maps: Decoding Deep Face Models\n\nVideo: <LINK>\nPaper: <LINK>'],https://arxiv.org/abs/2105.01386,"As Deep Neural Network models for face processing tasks approach human-like performance, their deployment in critical applications such as law enforcement and access control has seen an upswing, where any failure may have far-reaching consequences. We need methods to build trust in deployed systems by making their working as transparent as possible. Existing visualization algorithms are designed for object recognition and do not give insightful results when applied to the face domain. In this work, we present 'Canonical Saliency Maps', a new method that highlights relevant facial areas by projecting saliency maps onto a canonical face model. We present two kinds of Canonical Saliency Maps: image-level maps and model-level maps. Image-level maps highlight facial features responsible for the decision made by a deep face model on a given image, thus helping to understand how a DNN made a prediction on the image. Model-level maps provide an understanding of what the entire DNN model focuses on in each task and thus can be used to detect biases in the model. Our qualitative and quantitative results show the usefulness of the proposed canonical saliency maps, which can be used on any deep face model regardless of the architecture. ",Canonical Saliency Maps: Decoding Deep Face Models
125,1390533430677540866,1113856096119197699,Lucas Lamata,"['New paper today! A rewarding collaboration among three nuclear physicists and one quantum technology theorist. Four Andalusian scientists, three from \n@unisevilla and one from @UniHuelva . Looking forward to further works along these lines!\n<LINK> <LINK>']",https://arxiv.org/abs/2105.02834,"A digital quantum simulation of the Agassi model from nuclear physics is proposed and analyzed. The proposal is worked out for the case with four different sites. Numerical simulations and analytical estimations are presented to illustrate the feasibility of this proposal with current technology. The proposed approach is fully scalable to a larger number of sites. The use of a quantum correlation function as a probe to explore the quantum phases by quantum simulating the time dynamics, with no need of computing the ground state, is also studied. Evidence is given showing that the amplitude of the time dynamics of a correlation function in this quantum simulation is linked to the different quantum phases of the system. This approach establishes an avenue for the digital quantum simulation of useful models in nuclear physics. ",A digital quantum simulation of the Agassi model
126,1390521280936435713,2541230860,Dom Rowan,"['Our new paper on the search for ellipsoidal variables in ASAS-SN is now on astro-ph:\n<LINK>\n\nsee below for a brief description', 'We typically observe stellar-mass black holes in X-ray binary systems or gravitational wave mergers. Here we are searching for the large but less studied population of non-interacting black holes.', 'The search for non-interacting black holes often starts with radial velocity observations by identifying stars with large binary mass functions. We instead start from the photometric side of things by looking for ellipsoidal variables (ELLs).', 'ELLs are tidally distorted by their companion star such that they have a teardrop shape. By modeling ELL light curves, we can place constraints on the mass ratio of the binary and identify massive compact object companions.', 'We identified &gt;350 ELLs in ASAS-SN using an analytic model for ELL light curves. We combine this model with estimates of the mass and radius of the primary to calculate the minimum companion mass. Those with the highest mass companions are excellent targets for RV follow-up!']",https://arxiv.org/abs/2105.02242,"The majority of non-merging stellar mass black holes are discovered by observing high energy emission from accretion processes. Here we pursue the large, but still mostly unstudied population of non-interacting black holes and neutron stars by searching for the tidally-induced ellipsoidal variability of their stellar companions. We start from a sample of about 200,000 rotational variables, semi-regular variables, and eclipsing binary stars from the All-Sky Automated Survey for Supernovae (ASAS-SN). We use a $\chi^2$ ratio test followed by visual inspection to identify 369 candidates for ellipsoidal variability. We also discuss how to combine the amplitude of the variability with mass and radius estimates for observed stars to calculate a minimum companion mass, identifying the most promising candidates for high mass companions. ",High Tide: A Systematic Search for Ellipsoidal Variables in ASAS-SN
127,1390480406995165185,1000911088538419201,Gabe Schamberg,"['<LINK>  Excited about a new preprint written with Praveen Venkatesh @praveen_ven about the (bivariate) partial information decomposition of multivariate Gaussians. The paper has two main points... <LINK>', 'First, we show that well-known results in network information theory answer a question posed in Barrett\'s 2015 paper [https://t.co/E0bH1JrYVz]: When is the MMI-PID the ""right"" PID for multivariate Gaussians? (He showed the answer is always for univariate)', 'The answer is not often. It turns out, the conditions when MMI is ""right"" are the same ones that characterize stochastic degradation of broadcast channels. It\'s been shown that these channels occur w.p. zero (in certain conditions) [https://t.co/97W9Lsg4Z8]', 'So how should we compute PIDs of multivariate Gaussians? As a first step, we provide a convex relaxation of a deficiency-based PID. We show that our approach will provide a lower bound on the true deficiency-based synergy and redundancy.', 'Finally, we conduct simulations that suggest our approach for computing the PID satisfies some other nice properties. In case you want to compute PIDs on your Gaussian data, all the code to do so is available at https://t.co/6Cw0BAqvO4. Enjoy!', ""Truth be told, the strongest result was getting to write a paper w/ Praveen. We've discussed stuff like this for years now, so it's great to finally work together. Looking forward to more work on this topic and hearing your thoughts.""]",https://arxiv.org/abs/2105.00769,"Partial information decompositions (PIDs) extend the concept of mutual information to three (or more) random variables: they decompose the mutual information between a ""message"" and two other random variables into information components that are unique to each variable, redundantly present in both, and synergistic. This paper focuses on the PID of three jointly Gaussian random vectors. Barrett (2015) previously characterized the Gaussian PID for a scalar message in closed form - we examine the case where the message is a vector. Specifically, we provide a necessary and sufficient condition for the existence of unique information in the fully multivariate Gaussian PID. We do this by drawing a connection between the notion of Blackwell sufficiency from statistics and the idea of stochastic degradedness of broadcast channels from communication theory. Our first result shows that Barrett's closed form expression extends to the case of vector messages only very rarely. To compute the Gaussian PID in all other cases, we provide a convex optimization approach for approximating the PID, analyze its properties, and evaluate it empirically on randomly generated Gaussian systems. ","Partial Information Decomposition via Deficiency for Multivariate
  Gaussians"
128,1390354252673978372,1353754791881674753,Lorena Acu√±a,"['New paper on <LINK> led by #LAM postdoc Sergio Hoyer: TOI-220 b: a warm sub-Neptune discovered by TESS\n<LINK>\nSergio has given me his blessing to open a thread about it, so here I go. 1/n', 'We report the discovery of TOI-220 b by TESS transit photometry, and confirm it with radial velocity follow-up with the HARPS spectrograph. From the data reduction and analysis of these two methods we obtain the radius and mass of the planet, respectively. 2/n', ""If we look at TOI-220 b's position in the mass-radius diagram, we find that it is above the 100% (ice) water relationship. Given that this planet receives a high irradiation from its host star, it is most likely rich in volatiles. 3/n https://t.co/f5t53dZbTS"", 'We perform an interior-atmosphere Bayesian analysis to estimate the mass fraction of the core (CMF) and the volatile layer with their uncertainties. We assume that the volatile layer is dominated by water, and use stellar Fe/Si and Mg/Si abundances to help constrain the CMF. 4/n', 'TOI-220 b is consistent with a bulk less enriched in Fe than Earth and a water mass fraction of at least 50%. Water can be present in steam or supercritical phases. Its atmosphere could also be rich in H/He: confirming its composition would need atmospheric characterisation. 5/n https://t.co/kY3r4ogxrq']",https://arxiv.org/abs/2105.01944,"In this paper we report the discovery of TOI-220 $b$, a new sub-Neptune detected by the Transiting Exoplanet Survey Satellite (TESS) and confirmed by radial velocity follow-up observations with the HARPS spectrograph. Based on the combined analysis of TESS transit photometry and high precision radial velocity measurements we estimate a planetary mass of 13.8 $\pm$ 1.0 M$_{Earth}$ and radius of 3.03 $\pm$ 0.15 R$_{Earth}$, implying a bulk density of 2.73 $\pm$ 0.47 $\textrm{g cm}^{-3}$. TOI-220 $b$ orbits a relative bright (V=10.4) and old (10.1$\pm$1.4 Gyr) K dwarf star with a period of $\sim$10.69 d. Thus, TOI-220 $b$ is a new warm sub-Neptune with very precise mass and radius determinations. A Bayesian analysis of the TOI-220 $b$ internal structure indicates that due to the strong irradiation it receives, the low density of this planet could be explained with a steam atmosphere in radiative-convective equilibrium and a supercritical water layer on top of a differentiated interior made of a silicate mantle and a small iron core. ",TOI-220 $b$: a warm sub-Neptune discovered by TESS
129,1390254233371488260,137108766,Andrew Dwyer,"[""üìú New Paper | Don't forget your classics: Systematizing 45 years of Ancestry for Security API Usability Recommendations\n\nIt's now on arXiv, written with colleagues from Bristol!\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2105.02031,"Producing secure software is challenging. The poor usability of security APIs makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries and APIs; rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers spanning 45 years, offering a total of 883 recommendations.We undertake a thematic analysis to identify 7 core ways to improve usability of APIs. We find that most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand. There is less focus, however, on documentation, writing requirements, code quality assessment and the impact of organizational software development practices. By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time.We find evidence that less than a quarter of all API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind in this regard. ","Don't forget your classics: Systematizing 45 years of Ancestry for
  Security API Usability Recommendations"
130,1390237906384609280,2999702157,Anton Ilderton,"['New paper on the arXiv by Robin Ekman, Tom Heinzl and... me! #resummation #radiationreaction\n<LINK>\n#AcademicChatter #AcademicTwitter \n@plym_math @PlymUni @HiggsCentre <LINK>', 'And today we picked up our first citation! Thanks Greger!\nhttps://t.co/d8z7jdPZC4 https://t.co/f9XouNAp59', 'Ah, the feeling when two groups independently get the sequence \n{2, -20, 328, -7024, 179264, -5204416, 167270400....}\nBingo!']",https://arxiv.org/abs/2105.01640,"The Landau-Lifshitz equation is the first in an infinite series of approximations to the Lorentz-Abraham-Dirac equation obtained from `reduction of order'. We show that this series is divergent, predicting wildly different dynamics at successive perturbative orders. Iterating reduction of order ad infinitum in a constant crossed field, we obtain an equation of motion which is free of the erratic behaviour of perturbation theory. We show that Borel-Pad\'e resummation of the divergent series accurately reproduces the dynamics of this equation, using as little as two perturbative coefficients. Comparing with the Lorentz-Abraham-Dirac equation, our results show that for large times the optimal order of truncation typically amounts to using the Landau-Lifshitz equation, but that this fails to capture the resummed dynamics over short times. ","Reduction of order, resummation and radiation reaction"
131,1390108346544447490,1252017674651144193,√Ålvaro Lozano-Robledo,"['New paper!! üìÉ ""Towards a classification of entanglements of Galois representations attached to elliptic curves"" with Harris Daniels and Jackson Morrow. üéâüéäüéà <LINK>']",https://arxiv.org/abs/2105.02060,"Let $E/\mathbb{Q}$ be an elliptic curve, let $\overline{\mathbb{Q}}$ be a fixed algebraic closure of $\mathbb{Q}$, and let $G_{\mathbb{Q}}=\text{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ be the absolute Galois group of $\mathbb{Q}$. The action of $G_{\mathbb{Q}}$ on the adelic Tate module of $E$ induces the adelic Galois representation $\rho_E\colon G_{\mathbb{Q}} \to \text{GL}(2,\widehat{\mathbb{Z}}).$ The goal of this paper is to explain how the image of $\rho_E$ can be smaller than expected. To this end, we offer a group theoretic categorization of different ways in which an entanglement between division fields can be explained and prove several results on elliptic curves (and more generally, principally polarized abelian varieties) over $\mathbb{Q}$ where the entanglement occurs over an abelian extension. ","Towards a classification of entanglements of Galois representations
  attached to elliptic curves"
132,1390106806983163904,315718949,Cl√©ment Canonne,"['A new paper with Karl Wimmer, on ""Identity testing under label mismatch."" Warning: it\'s short!\n<LINK>\n\nYou have some discrete data, and you *know* it is distributed as your model q... sort of. You may have labelled the domain elements wrong. Oopsies.\n\n1/2 <LINK>', ""Phrased differently, you have i.i.d. samples from some distribution p, promised to be equal to q *up to a permutation of the domain.* \n\nYour goal? Test if p=q (you relabelled the domain correctly!) or if p is far from q (you really screwed the pooch there, didn't you?).\n\n2/3"", 'We consider testing (p=q vs. p,q far) and tolerant testing (p,q close vs. p,q far).\n\nFun: w/o the promise of permutation, on a domain of size n the # samples would be ‚àön and n/log n, w/ q=uniform being the hardest.\n\nBut under this promise, q=uniform is.. absolutely trivial.\n\n3/4', 'We prove tight or nearly tight bounds on this new problem, which we call ""identity testing under promise of permutation."" Spoiler alert: the sample complexities are... surprising. Goodbye, ‚àön. ü§Ø\n\nHope you enjoy the question, and paper. Comments welcome!\n\n4/4 https://t.co/pP2d0tjHZL', ""Haha, I really got the numbering of that thread wrong, didn't I? How fitting. Anyways, here's the link to the paper again, to make up for it: https://t.co/NRWQAHnKGu"", 'To elaborate on that: it becomes trivial (0 samples needed) since, if p is promised to be equal to uniform ""up to relabeling of the domain,"" then it must be the uniform distribution. You permute the uniform distribution, you get the unif distribution, so nothing to test here.']",https://arxiv.org/abs/2105.01856,"Testing whether the observed data conforms to a purported model (probability distribution) is a basic and fundamental statistical task, and one that is by now well understood. However, the standard formulation, identity testing, fails to capture many settings of interest; in this work, we focus on one such natural setting, identity testing under promise of permutation. In this setting, the unknown distribution is assumed to be equal to the purported one, up to a relabeling (permutation) of the model: however, due to a systematic error in the reporting of the data, this relabeling may not be the identity. The goal is then to test identity under this assumption: equivalently, whether this systematic labeling error led to a data distribution statistically far from the reference model. ",Identity testing under label mismatch
133,1390006566796107777,1091042849561473031,Alexander Kolesnikov üá∫üá¶,"['MLP-Mixer (a new vision architecture based on MLP only) code and pretrained models are now available: <LINK>. \nLooking forward to community contributions that will shed some light on how Mixer works and how to make it even better.\n\npaper: <LINK>. <LINK>', 'Notably, full model code is extremely compact and simple; and with large-scale pretraining it gets to 87.78% on ImageNet. https://t.co/swbQfciZvQ', 'Work done with @tolstikhini, @neilhoulsby, @giffmana, @XiaohuaZhai, @TomUnterthiner, @JessicaYung17, @keysers, @kyosu, @MarioLucic_, Alexey Dosovitskiy. And special thanks to Andreas Steiner for outstanding support with opensourcing.']",https://arxiv.org/abs/2105.01601,"Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. ""mixing"" the per-location features), and one with MLPs applied across patches (i.e. ""mixing"" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers. ",MLP-Mixer: An all-MLP Architecture for Vision
134,1390006271890247681,1224468727611056128,Kayla Jane Rodriguez,"['Very excited to announce my group has a new paper out! I‚Äôm no longer on this experiment, but I built a lot of the laser system currently used for spectroscopy on AlCl. Check it out my friends! <LINK>']",https://arxiv.org/abs/2105.01211,"Aluminum monochloride (AlCl) has been proposed as an excellent candidate for laser cooling. Here we present absorption spectroscopy measurements on the $A^1\Pi \leftarrow X^1\Sigma^+$ transition in AlCl inside a cryogenic helium buffer-gas beam cell. The high resolution absorption data enables a rigorous, quantitative comparison with our high-level ab initio calculations of the electronic and rovibronic energies, providing a comprehensive picture of the AlCl quantum structure. The combination of high resolution spectral data and theory permits the evaluation of spectroscopic constants and associated properties, like equilibrium bond length, with an order of magnitude higher precision. Based on the measured molecular equilibrium constants of the $A^1\Pi$ state, we estimate a Franck-Condon factor of the $A^1\Pi \leftarrow X^1\Sigma^+$ of 99.88%, which confirms that AlCl is amenable to laser cooling. ","Spectroscopy on the $A^1\Pi \leftarrow X^1\Sigma^+$ Transition of
  Buffer-Gas Cooled AlCl"
135,1389916459598589954,60995841,Federica Tarsitano,['Curious about new methods for galaxy images classification? Check out my paper adopting AutoML algorithms designed through @Modulos_ai : <LINK>. Special thanks to @kevinschawinski and @cbruderer . Data from @theDESurvey ‚ú® #AI #ML #astronomy @ETH_physics @ETH_en'],https://arxiv.org/abs/2105.01070,"In this work we explore the possibility of applying machine learning methods designed for one-dimensional problems to the task of galaxy image classification. The algorithms used for image classification typically rely on multiple costly steps, such as the Point Spread Function (PSF) deconvolution and the training and application of complex Convolutional Neural Networks (CNN) of thousands or even millions of parameters. In our approach, we extract features from the galaxy images by analysing the elliptical isophotes in their light distribution and collect the information in a sequence. The sequences obtained with this method present definite features allowing a direct distinction between galaxy types, as opposed to smooth S\'ersic profiles. Then, we train and classify the sequences with machine learning algorithms, designed through the platform Modulos AutoML, and study how they optimize the classification task. As a demonstration of this method, we use the second public release of the Dark Energy Survey (DES DR2). We show that by applying it to this sample we are able to successfully distinguish between early-type and late-type galaxies, for images with signal-to-noise ratio greater then 300. This yields an accuracy of $86\%$ for the early-type galaxies and $93\%$ for the late-type galaxies, which is on par with most contemporary automated image classification approaches. Our novel method allows for galaxy images to be accurately classified and is faster than other approaches. Data dimensionality reduction also implies a significant lowering in computational cost. In the perspective of future data sets obtained with e.g. Euclid and the Vera Rubin Observatory (VRO), this work represents a path towards using a well-tested and widely used platform from industry in efficiently tackling galaxy classification problems at the peta-byte scale. ","Image feature extraction and galaxy classification: a novel and
  efficient approach with automated machine learning"
136,1389892003123679236,1298963141745930242,Javier Nistal,['New paper submitted to WASPAA21!\n\nVQCPC-GAN: Variable-length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding\n\nüìë<LINK>\nüíª<LINK> (soon)\nüîä<LINK>\n\n@cyranaouameur  @deeplearnmusic @RichardGal8'],http://arxiv.org/abs/2105.01531,"Influenced by the field of Computer Vision, Generative Adversarial Networks (GANs) are often adopted for the audio domain using fixed-size two-dimensional spectrogram representations as the ""image data"". However, in the (musical) audio domain, it is often desired to generate output of variable duration. This paper presents VQCPC-GAN, an adversarial framework for synthesizing variable-length audio by exploiting Vector-Quantized Contrastive Predictive Coding (VQCPC). A sequence of VQCPC tokens extracted from real audio data serves as conditional input to a GAN architecture, providing step-wise time-dependent features of the generated content. The input noise z (characteristic in adversarial architectures) remains fixed over time, ensuring temporal consistency of global features. We evaluate the proposed model by comparing a diverse set of metrics against various strong baselines. Results show that, even though the baselines score best, VQCPC-GAN achieves comparable performance even when generating variable-length audio. Numerous sound examples are provided in the accompanying website, and we release the code for reproducibility. ","VQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using
  Vector-Quantized Contrastive Predictive Coding"
137,1389878084543860736,513464916,Carlos S√°nchez Mu√±oz,"['New paper on the arxiv today, in colaboration with G. Frascella and Frank Schlawin:\n\n""Quantum metrology of two-photon absorption""\n<LINK>\n\nWhat is this about? A short thread üëáüßµ', 'The simultaneous absorption of 2 photons by a quantum system is very important tool for spectroscopy and microscopy. E.g., 2-photon microscopy in life sciences allows to get images with higher spatial resolution, deeper tissue penetration, and less damage to the sample! üí•üí• https://t.co/RnXXtp6OrN', 'Here we tackle the following question: if we use light to shine a system that absorbs photons in pairs, and analyse the resulting state of the light, how much can we learn about that system?  Is our learning much better if we drive the system with *quantum* states of light? https://t.co/fR8lNrjk0x', 'To answer this, we computed the precision of estimation of two-photon absorption cross sections. In the limit of very small cross sections, we find that squeezed states have no fundamental limits to the precision you can achieve! ü§Ø', '(In more technical words, the quantum Fisher information diverges). We can‚Äôt get too excited about this though, since the measurement one would need to do to achieve this precision is not realizable in practice, at least not easily üòÖ', 'Nevertheless, looking at the precision you achieve by standard homodyne measurements, we find that squeezed yield an inverse quadratic scaling of precision with photon number 1/N¬≤! (some might call this super-Heisenberg scaling). üöÄüöÄ https://t.co/zW3fVFGC5h', 'This is much better than coherent states, that scale as 1/N^(3/2) (which is not bad either).']",https://arxiv.org/abs/2105.01561,"Two-photon absorption (TPA) is of fundamental importance in super-resolution imaging and spectroscopy. Its nonlinear character allows for the prospect of using quantum resources, such as entanglement, to improve measurement precision or to gain new information on, e.g., ultrafast molecular dynamics. Here, we establish the metrological properties of nonclassical squeezed light sources for precision measurements of TPA cross sections. We find that there is no fundamental limit for the precision achievable with squeezed states in the limit of very small cross sections. Considering the most relevant measurement strategies -- namely photon counting and quadrature measurements -- we determine the quantum advantage provided by squeezed states as compared to coherent states. We find that squeezed states outperform the precision achievable by coherent states when performing quadrature measurements, which provide improved scaling of the Fisher information with respect to the mean photon number $\sim n^4$. Due to the interplay of the incoherent nature and the nonlinearity of the TPA process, unusual scaling can also be obtained with coherent states, which feature a $\sim n^3$ scaling in both quadrature and photon-counting measurements. ",Quantum metrology of two-photon absorption
138,1389876960067735553,1141279096065929216,Martha Hilton,['New @LHCbPhysics #charm paper on @arxiv Measurement of CP asymmetry in D0 -&gt; KSKS. Very impressive result in a promising channel for CP violation and charm physics! \n\n<LINK> <LINK>'],https://arxiv.org/abs/2105.01565,"A measurement of the $CP$ asymmetry in $D^0 \to K^0_S K^0_S$ decays is reported, based on a data sample of proton-proton collisions collected by the LHCb experiment from 2015 to 2018, corresponding to an integrated luminosity of 6 fb$^{-1}$. The flavor of the $D^0$ candidate is determined using the charge of the $D^{*\pm}$ meson, from which the decay is required to originate. The $D^0 \to K^+ K^-$ decay is used as a calibration channel. The time-integrated $CP$ asymmetry for the $D^0 \to K^0_S K^0_S$ mode is measured to be: $$A^{CP}(D^0 \to K^0_S K^0_S) = (-3.1\pm 1.2\pm 0.4 \pm 0.2)\%, $$ where the first uncertainty is statistical, the second is systematic, and the third is due to the uncertainty on the $CP$ asymmetry of the calibration channel. This is the most precise determination of this quantity to date. ",Measurement of $CP$ asymmetry in $D^0 \to K^0_S K^0_S$ decays
139,1389859898947620868,9888672,Peter Rohde,"['Twitter-thread version of our new paper: <LINK>\n\n‚ÄúQuNet: Cost vector analysis &amp; multi-path entanglement routing in quantum networks‚Äù with @HudsonTLeone1 @Deepesh__Singh @nklangford @UTS_QSI @SydneyQuantum #QuantumComputing \n\nGithub: <LINK>', 'Goal: efficient simulation of multi-user entanglement distribution networks using cost-vector analysis. ‚ÄòCosts‚Äô are arbitrary properties that accumulate additively as qubits traverse networks. We can express loss, dephasing &amp; depolarising channels, and monetary cost in this form.', 'Classical networks rely on path-finding algorithms (e.g shortest path a la Dijkstra) for optimal packet routing. Quantum networks can employ multi-path routing, whereby multiple independently routed Bell pairs are purified into one of higher fidelity. https://t.co/2SMMBTkEKc', 'Primitive operations in quantum networks include entanglement swapping (for extending entanglement links), and entanglement purification (for boosting fidelity). https://t.co/Eu2fmcnc9V', 'These primitives provide simple substitution rules for graph reduction. https://t.co/h90acZIj0o', 'Here Alice &amp; Bob have the option of communicating via:\n‚Ä¢ A static ground-based fibre link.\n‚Ä¢ A LEO satellite passing overhead through atmospheric free-space channels, which dynamically update.\n‚Ä¢ Exploiting both and purifying them together (multi-path routing). https://t.co/GS0NDFrZqu', 'This is the QuNet code in Julia that creates that network. Julia modules can be called from Python or run in Jupyter notebooks too. You can learn more about Julia at https://t.co/yKCkkMF3Ym. https://t.co/wbVMBPPWYc', 'We accommodate for quantum memories by treating them as temporal channels between the respective nodes of identical copies of the underlying graph, where each layer represents the network at a particular point in time. https://t.co/JAKVO5oBeh', 'The incrementally weighted asynchronous nodes guide the routing algorithm to preference earlier times, thereby temporally compressing multi-user routing, and providing a temporal routing queue.', 'The compression ratio is the ratio between routing time with and without memories. Here we show the temporal compression ratio of our algorithm against increasing network congestion. https://t.co/6Vi2Xh2d0M', 'Our greedy multi-path routing algorithm allows multi-user routing with congestion mitigation via quantum memories, with algorithmic efficiency O(M^3 V^2), for M user-pairs on a V-vertex graph, and is therefore highly scalable and efficient in both users and network size.', 'Here‚Äôs a multi-user network with 3 users (colour coded) and multi-path routing (maximum 3 paths per user). The stacked layers represent time. https://t.co/jwJb70EDrW', 'Here we consider a grid network with edge percolations, showing the likelihood of users utilising different path numbers as the network becomes increasingly disconnected. https://t.co/TDptKULJvk', 'This heat map shows the fidelity/efficiency trade off for random user pairs on a square lattice network. The distinct heat curves correspond to different numbers of paths utilised. Superimposed contours show achievable per-user E91 QKD secret key rates for the network. https://t.co/nb9RDnDnmQ', 'Next stage of research is applying QuNet to distributed quantum computing. Entanglement links can be used to fuse together geographically separated graph states, facilitating distributed quantum computation exponentially more powerful than the sum of the parts. https://t.co/HLLk9Sa5dD', 'Consider a distributed computer with N nodes, each with n bits/qubits, and a scaling function that indicates classical-equivalent compute power (classically this is linear, for quantum computers super-linear). The computational gain achieved by unifying remote devices is: https://t.co/15lLbHhoVV', 'Through unification of remote computational assets:\n‚Ä¢ Classical computers, Œª=1. There is no computational enhancement.\n‚Ä¢ Quantum computers Œª&gt;1, in the best case Œª=exp(N). We achieve exponential computational enhancement.', 'The big question is ‚Äúin a future world with scalable quantum computers, is it economically justified to network them together‚Äù. The quantum networking infrastructure will be expensive, but the computational gains enormous. This is the question we hope to answer.', 'The answers have not only major economic implications, but enormous political and geo-strategic ones too, with the potential to play a major role in future technological diplomacy.', 'Our vision for the quantum internet is presented in my upcoming book:\n\n‚ÄúThe Quantum Internet‚Äù https://t.co/K2SYW78eO5', 'Ping: @rdviii @StephWehner @UTSEngage', 'Thanks @HudsonTLeone1 for pushing this to the arXiv while I had better things to do‚Ä¶ https://t.co/TrUyNgpRMK']",https://arxiv.org/abs/2105.00418,"Entanglement distribution will form the backbone of many future distributed quantum technologies, especially the quantum internet. The act of purifying multiple noisy entangled states into a single one of higher quality has no analogue in classical networking and as such, this transforms the way in which we will consider future algorithms for routing entanglement. We outline the differences that arise because of this, demonstrate some elementary formalisms for `multi-path entanglement routing', and discuss the philosophical differences that arise when comparing this regime to conventional digital network theory. We also present a software package, QuNet, that uses novel `quantum cost-vector analysis' to simulate and benchmark routing in multi-user entanglement networks in a way that is is highly scalable in network size and the number of competing users. Our software accommodates both ground- and space-based networks, and implements efficient multi-user time-optimisation for mitigating congestion when quantum memories are available. ","QuNet: Cost vector analysis & multi-path entanglement routing in quantum
  networks"
140,1389822601741144066,560668519,Neil Houlsby,"['New paper from Brain Zurich and Berlin!\n\nWe try a conv and attention free vision architecture: MLP-Mixer (<LINK>)\n\nSimple is good, so we went as minimalist as possible (just MLPs!) to see whether modern training methods &amp; data is sufficient... <LINK>', 'Mixer consists of MLPs applied in alternation to the image patch embeddings (tokens) and feature channels.\n\nWith  pre-training, Mixer learns really good features, and works well for transfer (e.g. 87.9% ImageNet), sitting alongside even the best CNNs and Vision Transformers! ...', 'Looking at the learned weights can be pretty fun too... https://t.co/JxaKH4a3sK', 'This continues our team‚Äôs work in vision, transfer, scalability, and architectures. As always, pleasure to work with a great team: @tolstikhini ,  @__kolesnikov__ , @giffmana , @XiaohuaZhai , @TomUnterthiner , @JessicaYung17 , @keysers , @kyosu ,@MarioLucic_ , Alexey Dosovitskiy', '@karpathy One small, but curious, difference with depthwise conv is parameter tying across channels. Slightly surprising (to me at least) that one can get away with it, but a really useful memory saving due to the large (entire image) receptive field.']",https://arxiv.org/abs/2105.01601,"Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. ""mixing"" the per-location features), and one with MLPs applied across patches (i.e. ""mixing"" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers. ",MLP-Mixer: An all-MLP Architecture for Vision
141,1389775359747448832,1348851291313889280,Rajdeep Dasgupta,"['New paper accepted in ApJ: Led by @izidorocosta @CLEVER_Planets - tests the effects of a pressure bump in the outer disk on the inner solar system formation. \n\nHighlight: terrestrial embryos formed by accreting planetesimals than by accreting pebbles.\n\n<LINK>', '@CrustalEvo @izidorocosta @CLEVER_Planets The planetesimals form from the pebbles themselves. But the exact composition of pebble population can vary as a function of time and locally. I will let @izidorocosta answer in more detail.']",https://arxiv.org/abs/2105.01101,"Mass-independent isotopic anomalies of carbonaceous and non-carbonaceous meteorites show a clear dichotomy suggesting an efficient separation of the inner and outer solar system. Observations show that ring-like structures in the distribution of mm-sized pebbles in protoplanetary disks are common. These structures are often associated with drifting pebbles being trapped by local pressure maxima in the gas disk. Similar structures may also have existed in the sun's natal disk, which could naturally explain the meteorite/planetary isotopic dichotomy. Here, we test the effects of a strong pressure bump in the outer disk (e.g. $\sim$5~au) on the formation of the inner solar system. We model dust coagulation and evolution, planetesimal formation, as well as embryo's growth via planetesimal and pebble accretion. Our results show that terrestrial embryos formed via planetesimal accretion rather than pebble accretion. In our model, the radial drift of pebbles foster planetesimal formation. However, once a pressure bump forms, pebbles in the inner disk are lost via drift before they can be efficiently accreted by embryos growing at $\gtrapprox$1~au. Embryos inside $\sim$0.5-1.0au grow relatively faster and can accrete pebbles more efficiently. However, these same embryos grow to larger masses so they should migrate inwards substantially, which is inconsistent with the current solar system. Therefore, terrestrial planets most likely accreted from giant impacts of Moon to roughly Mars-mass planetary embryos formed around $\gtrapprox$1.0~au. Finally, our simulations produce a steep radial mass distribution of planetesimals in the terrestrial region which is qualitatively aligned with formation models suggesting that the asteroid belt was born low-mass. ","The effect of a strong pressure bump in the Sun's natal disk:
  Terrestrial planet formation via planetesimal accretion rather than pebble
  accretion"
142,1389748127897317377,313814795,M. Sohaib Alam,"['Verifying entanglement can be hard. Our new paper with the @NASA QuAIL team shows that for QAOA-MaxCut (and perhaps other) states, you could verify N-partite entanglement with just 3 bases measurements, and poly(N) terms to estimate.\n\n<LINK>', '@dinunno @NASA Haha thanks, I pretty much just ever log in here to catch some conversations on the latest quantum news/papers anyway, so citation requests are totally fair game imo lol']",https://arxiv.org/abs/2105.01639,"In order to assess whether quantum resources can provide an advantage over classical computation, it is necessary to characterize and benchmark the non-classical properties of quantum algorithms in a practical manner. In this paper, we show that using measurements in no more than 3 out of the possible $3^N$ bases, one can not only reconstruct the single-qubit reduced density matrices and measure the ability to create coherent superpositions, but also possibly verify entanglement across all $N$ qubits participating in the algorithm. We introduce a family of generalized Bell-type observables for which we establish an upper bound to the expectation values in fully separable states by proving a generalization of the Cauchy-Schwarz inequality, which may serve of independent interest. We demonstrate that a subset of such observables can serve as entanglement witnesses for QAOA-MaxCut states, and further argue that they are especially well tailored for this purpose by defining and computing an entanglement potency metric on witnesses. A subset of these observables also certify, in a weaker sense, the entanglement in GHZ states, which share the $\mathbb{Z}_2$ symmetry of QAOA-MaxCut. The construction of such witnesses follows directly from the cost Hamiltonian to be optimized, and not through the standard technique of using the projector of the state being certified. It may thus provide insights to construct similar witnesses for other variational algorithms prevalent in the NISQ era. We demonstrate our ideas with proof-of-concept experiments on the Rigetti Aspen-9 chip for ansatze containing up to 24 qubits. ","Practical Verification of Quantum Properties in Quantum Approximate
  Optimization Runs"
143,1389743925120798720,2203468841,Dr Jade Powell,['We have a new paper on arXiv today about GW signals from 2D SN models with rotation and magnetic fields <LINK>'],https://arxiv.org/abs/2105.01315,"We investigate the impact of rotation and magnetic fields on the dynamics and gravitational wave emission in 2D core-collapse supernova simulations with neutrino transport. We simulate 17 different models of $15\,M_\odot$ and $39\,M_\odot$ progenitor stars with various initial rotation profiles and initial magnetic fields strengths up to $10^{12}\, \mathrm{G}$, assuming a dipolar field geometry in the progenitor. Strong magnetic fields generally prove conducive to shock revival, though this trend is not without exceptions. The impact of rotation on the post-bounce dynamics is more variegated, in line with previous studies. A significant impact on the time-frequency structure of the gravitational wave signal is found only for rapid rotation or strong initial fields. For rapid rotation, the angular momentum gradient at the proto-neutron star surface can appreciably affect the frequency of the dominant mode, so that known analytic relations for the high-frequency emission band no longer hold. In case of two magnetorotational explosion models, the deviation from these analytic relations is even more pronounced. One of the magnetorotational explosions has been evolved to more than half a second after the onset of the explosion and shows a subsidence of high-frequency emission at late times. Its most conspicuous gravitational wave signature is a high-amplitude tail signal. We also estimate the maximum detection distances for our waveforms. The magnetorotational models do not stick out for higher detectability during the post-bounce and explosion phase. ","Gravitational Wave Signals from Two-Dimensional Core-Collapse Supernova
  Models with Rotation and Magnetic Fields"
144,1389686831525076998,3250001664,Colin Hill,"[""New paper w/ @suzIQUV @brandonshensley + Chang-Goo Kim: <LINK>  We show that 'parity-violating' Galactic dust TB and EB signals are driven by misalignment of dust filaments with the plane-of-sky magnetic field, w/ implications for cosmic birefringence searches"", 'What causes the misalignment?  Still an open question! #dust #CMB #Planck #HI']",https://arxiv.org/abs/2105.00120,"Recent measurements of Galactic polarized dust emission have found a nonzero $TB$ signal, a correlation between the total intensity and the $B$-mode polarization component. We present evidence that this parity-odd signal is driven by the relative geometry of the magnetic field and the filamentary interstellar medium in projection. Using neutral hydrogen morphology and Planck polarization data, we find that the angle between intensity structures and the plane-of-sky magnetic field orientation is predictive of the signs of Galactic $TB$ and $EB$. Our results suggest that magnetically misaligned filamentary dust structures introduce nonzero $TB$ and $EB$ correlations in the dust polarization, and that the intrinsic dust $EB$ can be predicted from measurements of dust $TB$ and $TE$ over the same sky mask. We predict correlations between $TE$, $TB$, $EB$, and $EE/BB$, and confirm our predictions using synthetic dust polarization maps from magnetohydrodynamic simulations. We introduce and measure a scale-dependent effective magnetic misalignment angle, $\psi_\ell^{dust} \sim 5^\circ$ for $100 \lesssim \ell \lesssim 500$, and predict a positive intrinsic dust $EB$ with amplitude $\left<D_\ell^{EB}\right> \lesssim 2.5~\mu\mathrm{K^2_{CMB}}$ for the same multipole range at 353 GHz over our sky mask. Both the sign and amplitude of the Galactic $EB$ signal can change with the sky area considered. Our results imply that searches for parity violation in the cosmic microwave background must account for the nonzero Galactic $EB$ and $TB$ signals, necessitating revision of existing analyses of the evidence for cosmic birefringence. ","The Origin of Parity Violation in Polarized Dust Emission and
  Implications for Cosmic Birefringence"
145,1389603802144288771,73389062,S Ramamoorthy,['New self-supervised training paper with @mgb_infers: \n\nLearning data association without data association: An EM approach to neural assignment prediction. <LINK> \n\nWork done as part of @turinginst project at @InfAtEd @EDINrobotics <LINK>'],https://arxiv.org/abs/2105.00369,"Data association is a fundamental component of effective multi-object tracking. Current approaches to data-association tend to frame this as an assignment problem relying on gating and distance-based cost matrices, or offset the challenge of data association to a problem of tracking by detection. The latter is typically formulated as a supervised learning problem, and requires labelling information about tracked object identities to train a model for object recognition. This paper introduces an expectation maximisation approach to train neural models for data association, which does not require labelling information. Here, a Sinkhorn network is trained to predict assignment matrices that maximise the marginal likelihood of trajectory observations. Importantly, networks trained using the proposed approach can be re-used in downstream tracking applications. ","Learning data association without data association: An EM approach to
  neural assignment prediction"
146,1389591056140886025,3881712928,Valentina,"['New paper today <LINK> with Ernest Ma (University of California, Riverside). We study light freeze-in dark matter in an extension of the singlet majoron model with a Majorana fermion singlet with L=2 and one dark complex scalar singlet with L=1. <LINK>']",https://arxiv.org/abs/2105.00552,"The singlet majoron model of seesaw neutrino mass is appended by one dark Majorana fermion singlet $\chi$ with $L=2$ and one dark complex scalar singlet $\zeta$ with $L=1$. This simple setup allows $\chi$ to obtain a small radiative mass anchored by the same heavy right-handed neutrinos, whereas the one-loop decay of the standard-model Higgs boson to $\chi \chi + \bar{\chi} \bar{\chi}$ provides the freeze-in mechanism for $\chi$ to be the light dark matter of the Universe. ",Radiative Seesaw Dark Matter
147,1389586850994663429,3883277674,Barak Shoshany ‚ö°,"['New paper out! ""A C++17 Thread Pool for High-Performance Scientific Computing"". It\'s my first computer science paper on the arXiv.\n\nCheck it out here:\n<LINK>']",https://arxiv.org/abs/2105.00613,"We present a modern C++17-compatible thread pool implementation, built from scratch with high-performance scientific computing in mind. The thread pool is implemented as a single lightweight and self-contained class, and does not have any dependencies other than the C++17 standard library, thus allowing a great degree of portability. In particular, our implementation does not utilize OpenMP or any other high-level multithreading APIs, and thus gives the programmer precise low-level control over the details of the parallelization, which permits more robust optimizations. The thread pool was extensively tested on both AMD and Intel CPUs with up to 40 cores and 80 threads. This paper provides motivation, detailed usage instructions, and performance tests. ",A C++17 Thread Pool for High-Performance Scientific Computing
148,1404120361579462657,704799523,francesca dominici,['New paper posted <LINK>\nGaussian process to estimate a causal exposure-response curve.'],https://arxiv.org/abs/2105.03454,"Motivated by environmental health research on air pollution, we address the challenge of estimation and uncertainty quantification of causal exposure-response function (CERF). The CERF describes the relationship between a continuously varying exposure (or treatment) and its causal effect on a outcome. We propose a new Bayesian approach that relies on a Gaussian process (GP) model to estimate the CERF. We parametrize the covariance (kernel) function of the GP to mimic matching via a Generalized Propensity Score (GPS). The tuning parameters of the matching function are chosen to optimize covariate balance. Our approach achieves automatic uncertainty evaluation of the CERF with high computational efficiency, enables change point detection through inference on derivatives of the CERF, and yields the desired separation of design and analysis phases for causal estimation. We provide theoretical results showing the correspondence between our Bayesian GP framework and traditional approaches in causal inference for estimating causal effects of a continuous exposure. We apply the methods to 520,711 ZIP-code-level observations to estimate the causal effect of long-term exposures to PM2.5 on all-cause mortality among Medicare enrollees in the United States. ","Bayesian Modeling for Exposure Response Curve via Gaussian Processes:
  Causal Effects of Exposure to Air Pollution on Health Outcomes"
149,1400516353686188043,373525906,Weijie Su,"['This new paper (<LINK>) proposes a new weighted training algorithm to improve the sample efficiency of learning from cross-task signals. To the best of our knowledge, it is the first weighted algorithm for cross-task learning with theoretical guarantees. <LINK>']",https://arxiv.org/abs/2105.14095,"In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning. ",Weighted Training for Cross-Task Learning
150,1400106602737381382,1158385581476515840,Allyson Ettinger,"[""New paper with @langyu94: Previously we found transformer LMs fail to show intelligent phrase meaning composition <LINK> -- now we find that models fine-tuned on tasks seemingly promising for composition (PAWS, SST) don't cut it either <LINK> 1/"", 'In particular, we find often detrimental influence from PAWS fine-tuning -- and follow-up analyses turn up a spurious cue of word swapping distance that may be undermining focus on actual sentence meaning when training on PAWS 2/', 'SST fares a bit better, with (very) localized improvements on phrase composition tests. We speculate that training on such fine-grained phrase labeling may be promising for learning composition -- but ideally labels should involve something more meaning-rich than sentiment 3/', 'This paper is now up on arXiv, and coming out in ACL Findings 4/']",https://arxiv.org/abs/2010.03763,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models. ",Assessing Phrasal Representation and Composition in Transformers
151,1399556975168421891,872594072,Luca Maestrini,['Glad to announce a new paper or arXiv <LINK> coauthored with @kddang91.\nWe explore the use of variational approximations and bootstrap for structural equation models.'],https://arxiv.org/abs/2105.15036,"Structural equation models are commonly used to capture the relationship between sets of observed and unobservable variables. Traditionally these models are fitted using frequentist approaches but recently researchers and practitioners have developed increasing interest in Bayesian inference. In Bayesian settings, inference for these models is typically performed via Markov chain Monte Carlo methods, which may be computationally intensive for models with a large number of manifest variables or complex structures. Variational approximations can be a fast alternative; however, they have not been adequately explored for this class of models. We develop a mean field variational Bayes approach for fitting elemental structural equation models and demonstrate how bootstrap can considerably improve the variational approximation quality. We show that this variational approximation method can provide reliable inference while being significantly faster than Markov chain Monte Carlo. ",Fitting Structural Equation Models via Variational Approximations
152,1398669064126537729,947153529789018114,Mubarak Shah,['Check our new paper: \nUnsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities <LINK>    state of art results on Breakfast and 50Salads datasets.'],https://arxiv.org/abs/2105.00067,"Action recognition and detection in the context of long untrimmed video sequences has seen an increased attention from the research community. However, annotation of complex activities is usually time consuming and challenging in practice. Therefore, recent works started to tackle the problem of unsupervised learning of sub-actions in complex activities. This paper proposes a novel approach for unsupervised sub-action learning in complex activities. The proposed method maps both visual and temporal representations to a latent space where the sub-actions are learnt discriminatively in an end-to-end fashion. To this end, we propose to learn sub-actions as latent concepts and a novel discriminative latent concept learning (DLCL) module aids in learning sub-actions. The proposed DLCL module lends on the idea of latent concepts to learn compact representations in the latent embedding space in an unsupervised way. The result is a set of latent vectors that can be interpreted as cluster centers in the embedding space. The latent space itself is formed by a joint visual and temporal embedding capturing the visual similarity and temporal ordering of the data. Our joint learning with discriminative latent concept module is novel which eliminates the need for explicit clustering. We validate our approach on three benchmark datasets and show that the proposed combination of visual-temporal embedding and discriminative latent concepts allow to learn robust action representations in an unsupervised setting. ","Unsupervised Discriminative Embedding for Sub-Action Learning in Complex
  Activities"
153,1398184832136531969,738769492122214400,Johannes Lischner,"['In our new paper (<LINK>), we explore the electronic properties of graphene trilayers with an additional twisted monolayer on top. These systems feature flat bands and interesting magnetic ground states. #graphene #twistronics <LINK>']",https://arxiv.org/abs/2105.12641,"Starting with twisted bilayer graphene, graphene-based moir\'e materials have recently been established as a new platform for studying strong electron correlations. In this paper, we study twisted graphene monolayers on trilayer graphene and demonstrate that this system can host flat bands when the twist angle is close to the magic-angle of 1.16$^\circ$. When monolayer graphene is twisted on ABA trilayer graphene, the flat bands are not isolated, but are intersected by a Dirac cone with a large Fermi velocity. In contrast, graphene twisted on ABC trilayer graphene (denoted AtABC) exhibits a gap between flat and remote bands. Since ABC trilayer graphene and twisted bilayer graphene are known to host broken-symmetry phases, we further investigate the ostensibly similar magic angle AtABC system. We study the effect of electron-electron interactions in AtABC using both Hartree theory and an atomic Hubbard theory to calculate the magnetic phase diagram as a function of doping, twist angle, and perpendicular electric field. Our analysis reveals a rich variety of magnetic orderings, including ferromagnetism and ferrimagnetism, and demonstrates that a perpendicular electric field makes AtABC more susceptible to magnetic ordering. ","Flat bands, electron interactions and magnetic order in magic-angle
  mono-trilayer graphene"
154,1397069963404484614,1200309798601973761,üòé Avi Mohan üåû,"['Excited to share ü•≥our new paper on Decentralized Hybrid MAC protocols for the #InternetOfThings \nLink <LINK>\nWe use Partially Observable MDPs #POMDP to optimize packet delay in constrained queueing networks.\nReally happy with this paper in particular cuz\n1/3 <LINK>', 'I had the chance to work on ALL aspects of WSNs: building theory -&gt; protocol design -&gt; tailoring to latest #iot standards like #6tisch -&gt; implementing the protocol on a test bed (CC2420 Crossbow #telosB motes) -&gt; testing our MAC work in real time! üòÉ\nTheorist or Practitioner\n2/3', ""this paper has something very interesting for you.\nEASTER EGG: there's one place in the paper where we use the Foster-#Lyapunov theorem in the exact OPPOSITE direction in which it is usually invoked.\nCan you find it? üòâüòè""]",https://arxiv.org/abs/2105.11213,"We consider a system of several collocated nodes sharing a time slotted wireless channel, and seek a MAC that (i) provides low mean delay, (ii) has distributed control (i.e., there is no central scheduler), and (iii) does not require explicit exchange of state information or control signals. The design of such MAC protocols must keep in mind the need for contention access at light traffic, and scheduled access in heavy traffic, leading to the long-standing interest in hybrid, adaptive MACs. We first propose EZMAC, a simple extension of an existing decentralized, hybrid MAC called ZMAC. Next, motivated by our results on delay and throughput optimality in partially observed, constrained queuing networks, we develop another decentralized MAC protocol that we term QZMAC. A method to improve the short-term fairness of QZMAC is proposed and analysed, and the resulting modified algorithm is shown to possess better fairness properties than QZMAC. The theory developed to reduce delay is also shown to work %with different traffic types (batch arrivals, for example) and even in the presence of transmission errors and fast fading. Extensions to handle time critical traffic (alarms, for example) and hidden nodes are also discussed. Practical implementation issues, such as handling Clear Channel Assessment (CCA) errors, are outlined. We implement and demonstrate the performance of QZMAC on a test bed consisting of CC2420 based Crossbow telosB motes, running the 6TiSCH communication stack on the Contiki operating system over the 2.4GHz ISM band. Finally, using simulations, we show that both protocols achieve mean delays much lower than those achieved by ZMAC, and QZMAC provides mean delays very close to the minimum achievable in this setting, i.e., that of the centralized complete knowledge scheduler. ","Decentralized, Hybrid MAC Design with Reduced State Information Exchange
  for Low-Delay IoT Applications"
155,1395766500477177863,824426964140249089,Hao Sheng,"['[1/6] I\'m excited to share our new @AIESConf paper ""Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data"" w/ @KenielYao and @5harad <LINK> üßµ', ""[2/6] Rapid advances in face-recognition technology have made the privacy implications of surveillance cameras more acute. But it's hard to know where or how many cameras there are."", '[3/6] We built a computer vision model to detect cameras from 1.6 million street-view images sampled from 16 cities around the world. Human experts then verified positive model detections to ensure near-perfect precision. https://t.co/masqfxgO03', '[4/6] We found camera density varies widely: there are 4x as many cameras per km in Boston and NYC than in Seattle and LA. https://t.co/mOwye4yPyH', '[5/6] We also found more cameras in neighborhoods with higher proportions of non-white residents. This concentration of cameras in majority-minority neighborhoods points to the potential disparate impacts of surveillance technology on communities of color. https://t.co/jaxiUVmaZP', '[6/6] Check out our website for more, including maps of the locations of verified cameras, our computer vision model, and data to use in your own analysis. https://t.co/tshGui6W5W']",https://arxiv.org/abs/2105.01764,"The use of video surveillance in public spaces -- both by government agencies and by private citizens -- has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here, we combine computer vision, human verification, and statistical analysis to estimate the spatial distribution of surveillance cameras. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.2 (in Los Angeles) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents -- a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color. ","Surveilling Surveillance: Estimating the Prevalence of Surveillance
  Cameras with Street View Data"
156,1395721790064926720,1238126323714801665,Dr. Shweta Dalal,"['New paper on arXiv today -<LINK>\nWe detect 6 cool Jupiters, 3 brown dwarfs, and 16 low-mass stars with the radial velocity method using the SOPHIE spectrograph at OHP. We also perform astrometry analyses using Hipparcos and Gaia data! @FlavienKiefer <LINK>', 'One of the new cool Jupiters, BD+631405 b (msini = 3.96 Mj and e= 0.88), adds to the population of highly eccentric cool Jupiters, and it is presently the most massive member. https://t.co/nl2aSPruuE', 'Another interesting star is HD 205521, which has a companion with msini ~ 26 Mj but our analyses of the Hipparcos and Gaia astrometric data both find that the orbit is close to face-on and that the companion is actually a star. https://t.co/lhcLplsUEP']",https://arxiv.org/abs/2105.09741,"Distinguishing classes within substellar objects and understanding their formation and evolution need larger samples of substellar companions such as exoplanets, brown dwarfs, and low-mass stars. In this paper, we look for substellar companions using radial velocity surveys of FGK stars with the SOPHIE spectrograph at the Observatoire de Haute-Provence. We assign here the radial velocity variations of 27 stars to their orbital motion induced by low-mass companions. We also constrained their plane-of-the-sky motion using HIPPARCOS and Gaia Data Release 1 measurements, which constrain the true masses of some of these companions. We report the detection and characterization of six cool Jupiters, three brown dwarf candidates, and 16 low-mass stellar companions. We additionally update the orbital parameters of the low-mass star HD 8291 B, and we conclude that the radial velocity variations of HD 204277 are likely due to stellar activity despite resembling the signal of a giant planet. One of the new giant planets, BD+631405 b, adds to the population of highly eccentric cool Jupiters, and it is presently the most massive member. Two of the cool Jupiter systems also exhibit signatures of an additional outer companion. The orbital periods of the new companions span 30 days to 11.5 years, their masses 0.72 Jupiter mass to 0.61 Solar mass, and their eccentricities 0.04 to 0.88. These discoveries probe the diversity of substellar objects and low-mass stars, which will help constrain the models of their formation and evolution. ","The SOPHIE search for northern extrasolar planets -- XVII. A wealth of
  new objects: Six cool Jupiters, three brown dwarfs, and 16 low-mass binary
  stars"
157,1395536919774121984,68538286,Dan Hendrycks,"[""Can Transformers crack the coding interview? We collected 10,000 programming problems to find out. GPT-3 isn't very good, but new models like GPT-Neo are starting to be able to solve introductory coding challenges.\n\npaper: <LINK>\ndataset: <LINK> <LINK>""]",https://arxiv.org/abs/2105.09938,"While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements. ",Measuring Coding Challenge Competence With APPS
158,1394911374212583425,494870213,Thomas Haworth,"['New paper that I was involved with out today\n\nFirst detection of a disk free of volatile elements around a young A-type star: A sign of collisions between rocky planets?\n\n<LINK>', 'In this paper we find odd material around a young star. It has lots of refractories elements (stuff usually in rocks) but no volatiles (stuff we usually find as gas in discs around stars). \n\nThe only other place we see this is planetary debris around much older white dwarfs', 'so what has happened? Its still somewhat uncertain, though one compelling possibility is that two young planets have collided and what we are seeing is the debris from this', '@davecl42 How had I not thought of that before!']",https://arxiv.org/abs/2105.08327,"Aims. We present the first detailed analysis of the astrophysical parameters of the poorly studied Sco-Cen member HD 152384 and its circumstellar environment. Methods. We analyze newly obtained optical-near-IR XSHOOTER spectra, as well as archival TESS data, of HD 152384. In addition, we use literature photometric data to construct a detailed spectral energy distribution (SED) of the star. Results. The photospheric absorption lines in the spectrum of HD 152384 are characteristic of a A0 V star, for which we derive a stellar mass of 2.1 +/- 0.1 M_sun and a stellar age > 4.5 Myr. Superimposed on the photospheric absorption, the optical spectrum also displays double-peaked emission lines of Ca II, Fe I, Mg I and Si I, typical of circumstellar disks. Notably, all Hydrogen and Helium lines appear strictly in absorption. A toy model shows that the observed emission line profiles can be reproduced by emission from a compact (radius < 0.3 au) disk seen at an inclination of ~24 degrees. Further evidence for the presence of circumstellar material comes from the detection of a moderate infrared excess in the SED, similar to those found in extreme debris disk systems. Conclusions. We conclude that HD 152384 is surrounded by a tenuous circumstellar disk which, although rich in refractory elements, is highly depleted of volatile elements. To the best of our knowledge such a disk is unique within the group of young stars. However, it is reminiscent of the disks seen in some white dwarfs, which have been attributed to the disruption of rocky planets. We suggest that the disk around HD 152384 may have a similar origin and may be due to collisions in a newly formed planetary system. ","First detection of a disk free of volatile elements around a young
  A-type star: A sign of collisions between rocky planets?"
159,1394609051519102981,187447619,Alessandro,"['New paper arXived, check this out at <LINK>. Joint work with @AndreaCappozzo  and @michael_fop, a crucial proof demonstrating that informal and pub-inspired evening zoom meetings might turn out to be productive! <LINK>']",https://arxiv.org/abs/2105.07935,"Finite Gaussian mixture models provide a powerful and widely employed probabilistic approach for clustering multivariate continuous data. However, the practical usefulness of these models is jeopardized in high-dimensional spaces, where they tend to be over-parameterized. As a consequence, different solutions have been proposed, often relying on matrix decompositions or variable selection strategies. Recently, a methodological link between Gaussian graphical models and finite mixtures has been established, paving the way for penalized model-based clustering in the presence of large precision matrices. Notwithstanding, current methodologies implicitly assume similar levels of sparsity across the classes, not accounting for different degrees of association between the variables across groups. We overcome this limitation by deriving group-wise penalty factors, which automatically enforce under or over-connectivity in the estimated graphs. The approach is entirely data-driven and does not require additional hyper-parameter specification. Analyses on synthetic and real data showcase the validity of our proposal. ",Group-wise shrinkage estimation in penalized model-based clustering
160,1392399251716595718,3313806489,Tim Roberts,"[""*New paper klaxon*\n\n<LINK>\n\nIn which @BIGfalke and other colleagues conclude that the rate at which an Ultraluminous X-ray source (NGC 7793 P13) pulses is increasing; and better constrain various periodicities of uncertain origin.\n\nIt's a cracking read!""]",https://arxiv.org/abs/2105.04229,"Ultra-luminous X-ray pulsars (ULXPs) provide a unique opportunity to study super-Eddington accretion. We present the results of a monitoring campaign of ULXP NGC 7793 P13. Over our four-year monitoring campaign with Swift, XMM-Newton, and NuSTAR, we measured a continuous spin-up with $\dot P$ ~ -3.8e-11 s/s. The strength of the spin-up is independent of the observed X-ray flux, indicating that despite a drop in observed flux in 2019, accretion onto the source has continued at largely similar rates. The source entered an apparent off-state in early 2020, which might have resulted in a change in the accretion geometry as no pulsations were found in observations in July and August 2020. We used the long-term monitoring to update the orbital ephemeris and the periodicities seen in both the observed optical/UV and X-ray fluxes. We find that the optical/UV period is very stable over the years, with $P_\text{UV}$ = 63.75 (+0.17, -0.12) d. The best-fit orbital period determined from our X-ray timing results is 64.86 +/- 0.19 d, which is almost a day longer than previously implied, and the X-ray flux period is 65.21+/- 0.15 d, which is slightly shorter than previously measured. The physical origin of these different flux periods is currently unknown. We study the hardness ratio to search for indications of spectral changes. We find that the hardness ratios at high energies are very stable and not directly correlated with the observed flux. At lower energies we observe a small hardening with increased flux, which might indicate increased obscuration through outflows at higher luminosities. We find that the pulsed fraction is significantly higher at low fluxes. This seems to imply that the accretion geometry already changed before the source entered the deep off-state. We discuss possible scenarios to explain this behavior, which is likely driven by a precessing accretion disk. ","Long-term pulse period evolution of the ultra-luminous X-ray pulsar NGC
  7793 P13"
161,1392101086786461697,227062815,Kenny Joseph,"['1/2 ""New"" paper with Jon Morgan, out soon in J. of Math Sociology. ""New"" b/c I started it six years ago. We define the task of predicting which identity a person will be labeled with, and present a new theoretical model for it.\n<LINK>', ""Our model is tightly connected to other social psych models of this process. I'm excited the paper is finally out, and b/c I think the idea has implications for the study of bias in NLP and for theories of identity that I am looking forward to better understanding."", '@arxivabs damn it. I even meant to link the abstract. thanks abstract bot.']",https://arxiv.org/abs/2105.04462,"We introduce the identity labeling problem - given an individual in a social situation, can we predict what identity(ies) they will be labeled with by someone else? This problem remains a theoretical gap and methodological challenge, evidenced by the fact that models of social-cognition often sidestep the issue by treating identities as already known. We build on insights from existing models to develop a new framework, entitled Latent Cognitive Social Spaces, that can incorporate multiple social cues including sentiment information, socio-demographic characteristics, and institutional associations to estimate the most culturally expected identity. We apply our model to data collected in two vignette experiments, finding that it predicts identity labeling choices of participants with a mean absolute error of 10.9%, a 100% improvement over previous models based on parallel constraint satisfaction and affect control theory. ","Friend or Foe: A Review and Synthesis of Computational Models of the
  Identity Labeling Problem"
162,1392057546567991296,915988922831863808,Ron Litman,"['Happy to share our new paper named ‚ÄúTextAdaIN: Fine-Grained AdaIN for Robust Text Recognition‚Äù. \n\nThis is a join work with Oren Nuriel and Sharon Fogel. \n\nSee more details in our paper: <LINK> <LINK>', 'In this paper we reveal that state-of-the-art text recognizers are prone to overly rely on local image statistics. We propose a simple normalization-based technique for moderating the reliance on local statistics, which enhances the performance of text recognizers.']",https://arxiv.org/abs/2105.03906,"Leveraging the characteristics of convolutional layers, neural networks are extremely effective for pattern recognition tasks. However in some cases, their decisions are based on unintended information leading to high performance on standard benchmarks but also to a lack of generalization to challenging testing conditions and unintuitive failures. Recent work has termed this ""shortcut learning"" and addressed its presence in multiple domains. In text recognition, we reveal another such shortcut, whereby recognizers overly depend on local image statistics. Motivated by this, we suggest an approach to regulate the reliance on local statistics that improves text recognition performance. Our method, termed TextAdaIN, creates local distortions in the feature map which prevent the network from overfitting to local statistics. It does so by viewing each feature map as a sequence of elements and deliberately mismatching fine-grained feature statistics between elements in a mini-batch. Despite TextAdaIN's simplicity, extensive experiments show its effectiveness compared to other, more complicated methods. TextAdaIN achieves state-of-the-art results on standard handwritten text recognition benchmarks. Additionally, it generalizes to multiple architectures and to the domain of scene text recognition. Furthermore, we demonstrate that integrating TextAdaIN improves robustness towards more challenging testing conditions. ",TextAdaIN: Paying Attention to Shortcut Learning in Text Recognizers
163,1392000179394420736,1002942405157507073,Avik Pal,"['New ICML Paper: Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics w/ @YingboMa1 @Viral_B_Shah @ChrisRackauckas \n\nArxiv: <LINK>\n\n@JuliaLanguage / @SciML_Org code: <LINK>\n\n[1/4] <LINK>', 'We propose a generally applicable method to force the neural differential equation training process to choose the least expensive option. Using numerical heuristics baked inside sophisticated solvers we get the cheapest equations without requiring extra computation.\n\n[2/4] https://t.co/4yL9lJd0Nw', ""Since Local Error Estimates and Stiff Estimates are computed by default in standard explicit RK methods, we don't incur any penalty during the forward pass.\n\n[3/4] https://t.co/2oILP08yVW"", 'Finally, our method is trivial to implement using the Discrete Adjoints and Callback Mechanism provided in https://t.co/cwxUlD4L5b &amp; https://t.co/UHxZ4X9CtB\n\n[4/4]']",https://arxiv.org/abs/2105.03918,"Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver's algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning. ","Opening the Blackbox: Accelerating Neural Differential Equations by
  Regularizing Internal Solver Heuristics"
164,1390131400435699715,837412258150035457,Dr. Burcin Mutlu-Pakdil,"['Check out our new paper: ‚ÄúResolved dwarf galaxy searches within ‚àº5 Mpc with @VRubinObs and #SubaruTelescope HSC. We performed image-level simulations of faint dwarf galaxies and rigorously quantified their detectabilities. <LINK> <LINK>', 'We showed that near-future surveys will be able to probe at least ‚àº4.5 mag below the tip of the red giant branch (TRGB) for a distance of up to 1.5 Mpc, and ‚àº2 mag below the TRGB at 5 Mpc. https://t.co/wRE2Vgzcf0', 'This will push the discovery frontier for resolved dwarf galaxies to fainter magnitudes, lower surface brightnesses, and larger distances. We show secure census of dwarf galaxies down to Mv‚âà-5, ‚àí7, ‚àí8, will be soon within reach, out to 1.5 Mpc, 3.5 Mpc, and 5 Mpc, respectively https://t.co/hpzHWhBK9T', 'These are truly exciting times for #dwarfgalaxy studies ü§©ü•≥']",https://arxiv.org/abs/2105.01658,"We present a preview of the faint dwarf galaxy discoveries that will be possible with the Vera C. Rubin Observatory and Subaru Hyper Suprime-Cam in the next decade. In this work, we combine deep ground-based images from the Panoramic Imaging Survey of Centaurus and Sculptor (PISCeS) and extensive image simulations to investigate the recovery of faint, resolved dwarf galaxies in the Local Volume with a matched-filter technique. We adopt three fiducial distances - 1.5, 3.5, 5 Mpc, and quantitatively evaluate the effects on dwarf detection of varied stellar backgrounds, ellipticity, and Milky Way foreground contamination and extinction. We show that our matched-filter method is powerful for identifying both compact and extended systems, and near-future surveys will be able to probe at least ~4.5 mag below the tip of the red giant branch (TRGB) for a distance of up to 1.5 Mpc, and ~2 mag below the TRGB at 5 Mpc. This will push the discovery frontier for resolved dwarf galaxies to fainter magnitudes, lower surface brightnesses, and larger distances. Our simulations show the secure census of dwarf galaxies down to $M_{V}$$\approx$-5, -7, -8, will be soon within reach, out to 1.5 Mpc, 3.5 Mpc, and 5 Mpc, respectively, allowing us to quantify the statistical fluctuations in satellite abundances around hosts, and parse environmental effects as a function of host properties. ","Resolved Dwarf Galaxy Searches within ~5 Mpc with the Vera Rubin
  Observatory and Subaru Hyper Suprime-Cam"
165,1390127257478434822,104529881,Diogo Souto,"['New paper on @arXiver!\nIn this work we used the @APOGEEsurvey spectra to determine metallicities for a sample of FGKM dwarfs stars from the Coma Berenices open cluster. This is the first APOGEE study determining detailed metallicities of Mdwarfs from an OC <LINK> <LINK>', 'We also observe the signature of atomic diffusion operating in the warmer stars from the cluster, as can be seen in the Figure above. Enjoy the reading!']",https://arxiv.org/abs/2105.01667,"We present a study of metallicities in a sample of main sequence stars with spectral types M, K, G and F ($T_{\rm eff}$ $\sim$ 3200 -- 6500K and log $g$ $\sim$ 4.3 -- 5.0 dex) belonging to the solar neighborhood young open cluster Coma Berenices. Metallicities were determined using the high-resolution (R=$\lambda$/$\Delta$ $\lambda$ $\sim$ 22,500) NIR spectra ($\lambda$1.51 -- $\lambda$1.69 $\mu$m) of the SDSS-IV APOGEE survey. Membership to the cluster was confirmed using previous studies in the literature along with APOGEE radial velocities and Gaia DR2. An LTE analysis using plane-parallel MARCS model atmospheres and the APOGEE DR16 line list was adopted to compute synthetic spectra and derive atmospheric parameters ($T_{\rm eff}$ and log $g$) for the M dwarfs and metallicities for the sample. The derived metallicities are near solar and are homogeneous at the level of the expected uncertainties, in particular when considering stars from a given stellar class. The mean metallicity computed for the sample of G, K, and M dwarfs is $\langle$[Fe/H]$\rangle$ = +0.04 $\pm$ 0.02 dex; however, the metallicities of the F-type stars are slightly lower, by about 0.04 dex, when compared to cooler and less massive members. Models of atomic diffusion can explain this modest abundance dip for the F dwarfs, indicating that atomic diffusion operates in Coma Berenices stars. The [Fe/H] dip occurs in nearly the same effective temperature range as that found in previous analyses of the lithium and beryllium abundances in Coma Berenices. ","A metallicity study of F, G, K and M dwarfs in the Coma Berenices open
  cluster from the APOGEE survey"
166,1389671819767123970,2826142023,Carsten Binnig,"['In our most recent paper,  we (@BHilprecht\n@DMTUDA) discuss the vision of zero-shot learning for databases which is a new learning approach for database components. If you want to know more and see our first very promising results, then see  <LINK> <LINK>']",https://arxiv.org/abs/2105.00642,"In this paper, we present our vision of so called zero-shot learning for databases which is a new learning approach for database components. Zero-shot learning for databases is inspired by recent advances in transfer learning of models such as GPT-3 and can support a new database out-of-the box without the need to train a new model. Furthermore, it can easily be extended to few-shot learning by further retraining the model on the unseen database. As a first concrete contribution in this paper, we show the feasibility of zero-shot learning for the task of physical cost estimation and present very promising initial results. Moreover, as a second contribution we discuss the core challenges related to zero-shot learning for databases and present a roadmap to extend zero-shot learning towards many other tasks beyond cost estimation or even beyond classical database systems and workloads. ",One Model to Rule them All: Towards Zero-Shot Learning for Databases
167,1402207895991820290,151193108,Mert R. Sabuncu üá∫üá¶,['In our recent pre-print we propose a new building block called hyper-conv which is an implicit representation of a conv kernel. Hyper-conv enables decoupling the kernel size from the number of learnable parameters in a network. <LINK>'],https://arxiv.org/abs/2105.10559,"The convolution operation is a central building block of neural network architectures widely used in computer vision. The size of the convolution kernels determines both the expressiveness of convolutional neural networks (CNN), as well as the number of learnable parameters. Increasing the network capacity to capture rich pixel relationships requires increasing the number of learnable parameters, often leading to overfitting and/or lack of robustness. In this paper, we propose a powerful novel building block, the hyper-convolution, which implicitly represents the convolution kernel as a function of kernel coordinates. Hyper-convolutions enable decoupling the kernel size, and hence its receptive field, from the number of learnable parameters. In our experiments, focused on challenging biomedical image segmentation tasks, we demonstrate that replacing regular convolutions with hyper-convolutions leads to more efficient architectures that achieve improved accuracy. Our analysis also shows that learned hyper-convolutions are naturally regularized, which can offer better generalization performance. We believe that hyper-convolutions can be a powerful building block in future neural network architectures solving computer vision tasks. ",Hyper-Convolution Networks for Biomedical Image Segmentation
168,1400821352353910792,1169582693438361600,Eugenio Petrovich,['Acknowledgments data are becoming increasingly available on multidisciplinary databasesüñ•Ô∏è. Can they be used to study informal collaboration patterns in #science? üë©\u200düî¨‚ÜîÔ∏èüë®\u200düî¨ We examine the case of Top5 Journals in #economics üîçüìä<LINK> @albertobaccini'],https://arxiv.org/abs/2105.12988,"Two alternative accounts can be given of the information contained in the acknowledgments of academic publications. According to the mainstream normative account, the acknowledgments serve to repay debts towards formal or informal collaborators. According to the strategic account, by contrast, the acknowledgments serve to increase the perceived quality of papers by associating the authors with influential scholars. The two accounts are assessed by analyzing the acknowledgments indexed in Web of Science of 1218 articles published in the ""top-five journals"" of economics for the years 2015-2019. The analysis is focused on six dimensions: (i) the style of acknowledging texts, (ii) the distribution of mentions, (iii) the identity of the most mentioned acknowledgees, (iv) the shares of highly and lowly mentioned acknowledgees, (v) the hierarchy of the acknowledgment network, and (vi) the correlation at a paper level between intellectual similarity, measured by common references, and social similarity, measured by common acknowledges. Results show that the normative and the strategic account should be considered as valid but partial explanations of acknowledging behavior. Hence, acknowledgments should be used with extreme caution for investigating collaboration practices and they should not be used to produce acknowledgments-based metrics of scholars for evaluative purposes. ","Normative versus strategic accounts of acknowledgment data: the case of
  the top-five journals of economics"
169,1399900430927233029,941131462744539137,Liangming Pan,"['Hi all, our new #ACL2021 paper on ""Zero-shot Fact Verification by Claim Generation"" is now at <LINK> #NLProc. We propose a framework based on question generation to automatically generate (evidence, claim) pairs to train the fact verification model. <LINK>']",https://arxiv.org/abs/2105.14682,"Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available. ",Zero-shot Fact Verification by Claim Generation
170,1399834114841317381,2852215580,Ferdinando Fioretto,"['Very excited to share ‚ÄúDecision Making with Differential Privacy (DP) under a Fairness Lens‚Äù accepted at #IJCAI21 @IJCAIconf \nTL;DR: We study why decisions that take as input DP data may disproportionately impact some groups over others. \nPaperüëâ <LINK>\n\nA üßµ1/8', '(2/n) It was recently observed that DP may induce fairness issues in downstream decision processes. \n\nSee the amazing work by Satya Kuppam, @RyanBMcKenna, David Pujol, @michaelghay, @machanavajjhala, and @geromiklau\nPaperüëâ https://t.co/DRcF7mjLMV', '(3/8) Our work builds on these observations and provides a step towards understanding the fairness issues arising when differentially private data is used as input to several allocation problems P. \nSee setting below. https://t.co/BwSLgEQ9dM', '(4/8) We first show that there is a relationship between fairness and the difference in the problem P local curvatures on any pair of entities (i.e., individuals or groups). See Thm 3.\nThis implies that (perfect) fairness can be achieved when P is a linear function.', '(5/8) Next, we bound the fairness violations of a class of indicator functions, called thresholding (see Thm 4) and discuss the loss of fairness caused by the composition of boolean predicates (see Thms 5 and 6). Both are recurrent features in decision rules.', '(6/8) Then, we analyze the unfairness resulting from post-processing DP data release, in particular, to ensure non-negativity. These results connect to our #AAAI21 paper üëâ https://t.co/WouETGav3A in which we analyzed the bias of several DP projections programs.', '(7/8) Next, we analyze the reasons for disparity errors arising in two Census-motivated problems: \n- Title I of the Elementary and Secondary Edu Act that allots funds to qualifying school districts. \n- Minority language voting rights, granted to qualifying voting jurisdictions', '(8/8) Finally, we discuss several guidelines that may be adopted to mitigate the unfairness effects presented in the study. \n\nJoint work with @CuongTr95450563 and  @PVanHentenryck']",https://arxiv.org/abs/2105.07513,"Agencies, such as the U.S. Census Bureau, release data sets and statistics about groups of individuals that are used as input to a number of critical decision processes. To conform to privacy and confidentiality requirements, these agencies are often required to release privacy-preserving versions of the data. This paper studies the release of differentially private data sets and analyzes their impact on some critical resource allocation tasks under a fairness perspective. {The paper shows that, when the decisions take as input differentially private data}, the noise added to achieve privacy disproportionately impacts some groups over others. The paper analyzes the reasons for these disproportionate impacts and proposes guidelines to mitigate these effects. The proposed approaches are evaluated on critical decision problems that use differentially private census data. ",Decision Making with Differential Privacy under a Fairness Lens
171,1399733287422103557,3127541301,Marco Cerezo,"['üî•New manuscript: We use tools from Quantum Optimal Control to study barren plateaus in periodic-structured problem-inspired ansatzes (QAOA, HVA,...)üî•\n\n<LINK>\n\nIn collaboration with @MartinLaroo, @czarnik_piotr, @kunal_phy, @wgopu, and @ColesQuantum.\n\nSee thread <LINK>', 'Tldr1: Problem inspired ansatzes are NOT immune to barren plateaus. One needs to be careful using some proposed Periodic Structure Ansatz (PSA) (HVA, adapt-QAOA, quantum optimal control inspired ansatz) as these can lead to barren plateaus due to the controllability of the ansatz https://t.co/a00Nr7N7Vy', 'Tldr2: Our results provide a framework for trainability-aware ansatz design strategies that do not come at the cost of extra quantum resources. See more details below.', '1) Variational Quantum Algorithms (VQA) and Quantum Optimal Control (QOC) can be regarded as two different levels of a theory that manipulate the evolution of a quantum system by training sets of parameters governing the system‚Äôs dynamical evolution. https://t.co/mxr2unkwBq', '2) Using this connection, we employ tools from QOC to analyze the trainability of VQAs and show that the gradient scaling of the VQA depends on the controllability of the system, and hence can be diagnosed trough the dynamical Lie algebra arising from the generators of the ansatz https://t.co/HTHH8sOt65', '3) We prove that controllable systems will have barren plateaus. Then, in systems with symmetries we show that the input state to the VQA can determine its trainability, with some states leading to barren plateaus while others avoiding them. https://t.co/mgUewGOlpj', '4) We finally link the dimension of the Dynamical Lie Algebra to the scaling of the variance function of the cost partial derivative. This observation can lead to simpler trainability analysis. https://t.co/cW2unzSQDd', '5) We hope that our work helps improve the design of better problem-inspired trainability-aware ansatzes (what a mouthful) and further understand how barren plateaus can arise in VQAs.', '@quantumVerd Indeed, and we show that the expressibility can be seen trough the controllability. For instance, if you were to do QAOA with Ising problem Hamiltonian and X mixer, this is all cool but if you do a mixer X+Z, then you have a barren plateau (the latter is controllable).', '@quantumVerd Sort-of, if you are in an exponentially large subspace you might not be able to train. And breaking the symmetry can make things worse (now you are in an even larger subspace).', '@quantumVerd This for me was perhaps some of the most surprising results, making small (and almost innocuous looking) changes to an ansatz can straight up lead to an untrainable cost.']",https://arxiv.org/abs/2105.14377,"Variational Quantum Algorithms (VQAs) have received considerable attention due to their potential for achieving near-term quantum advantage. However, more work is needed to understand their scalability. One known scaling result for VQAs is barren plateaus, where certain circumstances lead to exponentially vanishing gradients. It is common folklore that problem-inspired ansatzes avoid barren plateaus, but in fact, very little is known about their gradient scaling. In this work we employ tools from quantum optimal control to develop a framework that can diagnose the presence or absence of barren plateaus for problem-inspired ansatzes. Such ansatzes include the Quantum Alternating Operator Ansatz (QAOA), the Hamiltonian Variational Ansatz (HVA), and others. With our framework, we prove that avoiding barren plateaus for these ansatzes is not always guaranteed. Specifically, we show that the gradient scaling of the VQA depends on the controllability of the system, and hence can be diagnosed trough the dynamical Lie algebra $\mathfrak{g}$ obtained from the generators of the ansatz. We analyze the existence of barren plateaus in QAOA and HVA ansatzes, and we highlight the role of the input state, as different initial states can lead to the presence or absence of barren plateaus. Taken together, our results provide a framework for trainability-aware ansatz design strategies that do not come at the cost of extra quantum resources. Moreover, we prove no-go results for obtaining ground states with variational ansatzes for controllable system such as spin glasses. We finally provide evidence that barren plateaus can be linked to dimension of $\mathfrak{g}$. ",Diagnosing barren plateaus with tools from quantum optimal control
172,1399585478546845698,1199782808501153793,Andrew Lampinen,"['How can RL agents recall the past in detail, in order to behave appropriately in the present? In our new preprint ""Towards mental time travel: A hierarchical memory for RL agents"" (<LINK>) we propose a memory architecture that steps in this direction.', 'We draw inspiration from the idea that human memory is like ""mental time travel""‚Äîwe can recall a specific event in the past, and relive it in some sequential detail, with relatively little interference from other events. This ability is key to our goal-directed use of memory.', 'By contrast, RL agent memories generally lack either this sparsity (attending to one or a few events in the past) or this detail (replaying an event in sequence, rather than just recalling a single vector from each event). This limits their ability to learn from their memory.', 'We therefore propose a Hierarchical Transformer Memory (HTM). HTM stores chunks of the past, together with a summary for each chunk. It recalls hierarchically: first attending over summaries to identify relevance, then time-traveling to each relevant chunk to attend in detail. https://t.co/zvdt6iINgm', 'We show that HTM allows RL agents to excel across 6 varied memory domains, such as recalling a ballet, maintaining object permanence, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning new nouns and recalling them after distractor tasks. https://t.co/YImnZeZh9K', 'Agents with HTM can extrapolate to recalling knowledge after an order of magnitude(!) more distractor phases than they were trained on, and can even generalize zero-shot from training within single episodes to evaluation of knowledge from several episodes before.', 'They also achieve near-optimal performance on One-Shot StreetLearn (https://t.co/YIHT26FFOn), a challenging domain which requires recombining memories of prior paths in order to plan new routes. HTM matches the performance of a memory specifically designed for this setting.', 'We think these results highlight the value of hierarchy, sparsity, and attention in RL agent memories. Combining these allows agents to effectively ""mental time travel"" into relevant memories, which allows them to better learn from the past to achieve their present goals.', 'We also think our work highlights some generally interesting ideas, including 1) hierarchical attention where the top level determines what deserves detailed attention and 2) key-value memories where the values are not just vectors, but more general structures (e.g. sequences).', 'Interested to hear your thoughts! Thanks to my awesome coauthors @scychan_brains, Andrea Banino, and @FelixHill84, as well as all our colleagues who provided amazing suggestions and support while working on this paper!', '@anirudhg9119 @rosemary_ke Thanks for sharing, will check it out!']",https://arxiv.org/abs/2105.14039,"Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore ""mentally time-travel"" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments. ","Towards mental time travel: a hierarchical memory for reinforcement
  learning agents"
173,1399321796000849922,428355146,Karin Sevegnani,"[""You can find the pre-print for the OTTers paper here: <LINK>\nThis work introduces a new task: how to transition between topics in an open-domain conversation.\nWe consider as 'topic' the entity mentioned in each utterance, which is further grounded on a 1/3 <LINK>"", 'commonsense knowledge graph.\nThe goal of the task is to generate a transition utterance that shifts topics by mentioning a connecting entity.\nWe present a dataset for the task (https://t.co/mWEvfPYc0W) and tested a couple of SOTA models to see how they perform for such 2/3', 'a task. Finally, we analyze possible improvements that could be used to achieve better performance. 3/3']",https://arxiv.org/abs/2105.13710,"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a ""bridging"" utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we call OTTers. We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. ",OTTers: One-turn Topic Transitions for Open-Domain Dialogue
174,1399226015151497223,149652410,Daniel M√©ndez,"['New print of our manuscript (to appear in TSE) where we study the extent to which RE-related standards are known and used by practitioners! In collaboration with @FranchXavier, M. Glinz, and N. Seyff \n\nüëâ preprint: <LINK>\nüëâ data: <LINK>']",https://arxiv.org/abs/2105.13961,"Context: The use of standards is considered a vital part of any engineering discipline. So one could expect that standards play an important role in Requirements Engineering (RE) as well. However, little is known about the actual knowledge and use of RE-related standards in industry. Objective: In this article, we investigate to which extent standards and related artifacts such as templates or guidelines are known and used by RE practitioners. Method: To this end, we have conducted a questionnaire-based online survey. We could analyze the replies from 90 RE practitioners using a combination of closed and open-text questions. Results: Our results indicate that the knowledge and use of standards and related artifacts in RE is less widespread than one might expect from an engineering perspective. For example, about 47% of the respondents working as requirements engineers or business analysts do not know the core standard in RE, ISO/IEC/IEEE 29148. Participants in our study mostly use standards by personal decision rather than being imposed by their respective company, customer, or regulator. Beyond insufficient knowledge, we also found cultural and organizational factors impeding the widespread adoption of standards in RE. Conclusions: Overall, our results provide empirically informed insights into the actual use of standards and related artifacts in RE practice and - indirectly - about the value that the current standards create for RE practitioners. ","A Study about the Knowledge and Use of Requirements Engineering
  Standards in Industry"
175,1398083554127400962,747383862154559488,Kuniyuki Takahashi,"['We have published an article and a video on quantitative grasping of food to be presented at ICRA2021. We propose a method to produce stable results with small dataset by taking uncertainty into account using self-supervised learning.\n<LINK>', 'R&amp;D is in progress after submission of the paper. Now, using the training data of 20-100 grasps of a random quantity, the success rate of a grasp with an error of ¬±5% from an arbitrary target quantity is about 100%.\nPlease look forward to further updates!\nhttps://t.co/SMic5HqcMk']",https://arxiv.org/abs/2105.12946,"Food packing industry workers typically pick a target amount of food by hand from a food tray and place them in containers. Since menus are diverse and change frequently, robots must adapt and learn to handle new foods in a short time-span. Learning to grasp a specific amount of granular food requires a large training dataset, which is challenging to collect reasonably quickly. In this study, we propose ways to reduce the necessary amount of training data by augmenting a deep neural network with models that estimate its uncertainty through self-supervised learning. To further reduce human effort, we devise a data collection system that automatically generates labels. We build on the idea that we can grasp sufficiently well if there is at least one low-uncertainty (high-confidence) grasp point among the various grasp point candidates. We evaluate the methods we propose in this work on a variety of granular foods -- coffee beans, rice, oatmeal and peanuts -- each of which has a different size, shape and material properties such as volumetric mass density or friction. For these foods, we show significantly improved grasp accuracy of user-specified target masses using smaller datasets by incorporating uncertainty. ",Uncertainty-Aware Self-Supervised Target-Mass Grasping of Granular Foods
176,1398070485167095808,236102260,Kostas Pelechrinis,"['We did a thing a few years back with @kirkgoldsberry on corner 3s but now we got it published @IJCAIconf workshop on AI for #sportsanalytics. You can find details in the paper (<LINK>) but here is a quick thread w the things we looked: (1/5)', 'Q1: why are C3s so efficient? Shorter distance? Certainly helps but major driver seems to be assists - corroborated by analyzing data from FIBA competitions as well where distance is more or less the same (2/5) https://t.co/HFhVZyBLlX', 'Q2: how are they generated? With clustering on tracking data we find that almost half of them involve a shooter anchored at the corner for at least 4 seconds prior the shot. (3/5) https://t.co/PFluOCmulb', 'Q3: how to defend them? Using a simplified zero-sum game for drive-b-kick we show that the Nash equilibrium involves either committing to the corner or the drive. In our data players lingered between the two options (4/5) https://t.co/ta8TAz2xFo', 'Some potential things to explore on why players linger between the two options involve potential cognitive biases on how we assess spatial risk. Interesting direction (imo) that of course might lead nowhere, but behavioral (decision) science should be part of the picture (5/5) https://t.co/DonfTFVxT0']",https://arxiv.org/abs/2105.12785,"Modern basketball is all about creating efficient shots, that is, shots with high payoff. This is not necessarily equivalent to creating looks with the highest probability of success. In particular, the two most efficient shots in the NBA - which are shots from the paint, i.e., extremely close to the basket, and three-point shots from the corner, i.e., at least 22 feet apart - have completely different spatial profiles when it comes to their distance from the basket. The latter also means that they are pretty much at the opposing ends of the spectrum when it comes to their probability of being made. Due to their efficiency, these are the most sought after shots from the offense, while the defense is trying to contain them. However, in order to contain them one needs to first understand what makes them efficient in the first place and how they are generated. In this study we focus on the corner three point shots and using player tracking data we show that the main factor for their efficiency - contrary to the belief from the sports mass media - is not the shorter distance to the basket compared to three-point shots above the break, but rather the fact that they are assisted at a very high rate (more than 90\%). Furthermore, we analyze the movement of the shooter and his defender and find that more than half of these shots involve a shooter anchored at the corner waiting for the kick out pass. We finally define a simplified game between the offense and defense in these situation and we find that the Nash Equilibrium supports either committing to the corner shooter or to the drive to the basket, and not lingering between the two, which is what we observed from the defenses in our dataset. ","The Anatomy of Corner 3s in the NBA: What makes them efficient, how are
  they generated and how can defenses respond?"
177,1397943597694083080,951610042737885184,L√©onard H.,"['Ever wondered how to be better than Mr. Bean at selecting hyperparameters when doing Imitation Learning without using the *unknown* reward function?\n\nCheck out our ICML 2021 paper! \n\nWe discuss different solutions and compare them in a large-scale study!\n\n<LINK> <LINK>', 'It is called ""Hyperparameter Selection for Imitation Learning"" and is a joint work with @robdadashi @RaphaelMarinier @sabelaraga @OlivierBachem and many others at @GoogleAI ! https://t.co/t7WzU5SM7S']",https://arxiv.org/abs/2105.12034,"We address the issue of tuning hyperparameters (HPs) for imitation learning algorithms in the context of continuous-control, when the underlying reward function of the demonstrating expert cannot be observed at any time. The vast literature in imitation learning mostly considers this reward function to be available for HP selection, but this is not a realistic setting. Indeed, would this reward function be available, it could then directly be used for policy training and imitation would not be necessary. To tackle this mostly ignored problem, we propose a number of possible proxies to the external reward. We evaluate them in an extensive empirical study (more than 10'000 agents across 9 environments) and make practical recommendations for selecting HPs. Our results show that while imitation learning algorithms are sensitive to HP choices, it is often possible to select good enough HPs through a proxy to the reward function. ",Hyperparameter Selection for Imitation Learning
178,1397549887521271810,1285216628884602881,Santi √Åvila,"['Paper Alert! <LINK>\n\nWe study the clustering of HI gas as observed by simulated intensity mapping (HI) experiments. We investigate the effect that the telescope beam and the foreground cleaning have on the observed 2-point correlation function (2PCF). <LINK>', 'For that, we use the UNITsims (https://t.co/teSuxE3yKd) coupled to the Semi-Analytic Galaxy Evolution code (presented in https://t.co/9Axs4ijsCR). \n\nWe obtain HI from the cold gas, following 2 different prescriptions. We study the HI to halo mass relation and total HI abundace.', 'We create IM pixels. \nThe telescope beam is simulated by a Gaussian smoothing on the angular coordinates. \nThe Foreground cleaning algorithms remove any smooth signal with frecuency, resulting in an exponential damping on the radial cosmological signal.', 'We study the observational effects on different types of 2PCF: \n- Anisotropic (r_per, r_parallel)\n- mu-wedges\n- Radial\n\nWe find that the BAO is still visible for a SKA-like survey!', 'with @CunningtonSD and others']",https://arxiv.org/abs/2105.10454,"We study the clustering of HI intensity maps produced from simulations with a focus on baryonic acoustic oscillations (BAO) and the effects induced by telescope beam smoothing and foreground cleaning. We start by creating a HI catalogue at $z=1.321$ based on the Semi-Analytic Galaxy Evolution (SAGE) model applied to the UNIT simulations. With this catalogue we investigate the relation between model HI and the dark matter haloes and we also study the abundance of HI, $\Omega_{\rm HI}$, predicted by this model. We then create synthetic HI intensity maps with a Nearest-Grid-Point approach. In order to simulate the telescope beam effect, a Gaussian smoothing is applied on the plane perpendicular to the line of sight. The effect of foreground removal methods is simulated by exponentially damping the largest wavelength Fourier modes on the radial direction. We study the anisotropic 2-point correlation function (2PCF) $\xi(r_\perp,r_\parallel)$ and how it is affected by the aforementioned observational effects. In order to better isolate the BAO signal, we study several 2PCF $\mu$-wedges (with a restricted range of orientations $\mu$) tailored to address the systematics effects and we compare them with different definitions of radial 2PCFs. Finally, we discuss our findings in the context of an SKA-like survey, finding a clear BAO signal in most of the estimators here proposed. ","HI intensity mapping correlation function from UNIT simulations: BAO and
  observationally induced anisotropy"
179,1397476734417522691,12778712,Gwendal Simon,"[""Our paper accepted at SEC'21 is out on Arxiv <LINK> in-router deep-learning traffic classification at line-rate #dpdk we extract per-flow features and infer a classification. We studied device archi w/ TPU and GPU. Perf includes classification rate, energy.""]",https://arxiv.org/abs/2105.11738,"Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance the processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations. In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both adversarial and realistic network conditions. Our analysis shows that FENXI can sustain 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions. ",FENXI: Deep-learning Traffic Analytics at the Edge
180,1396763078738554883,1032962818453188608,Emma Dodd,"['My first paper day! <LINK>  With Gaia EDR3 data we study the substructure of the local Helmi streams stars, the debris from an accreted dwarf Galaxy ~5-8 Gyr ago. w/ @amina_helmi  and Helmer Koppelman. <LINK>', 'In the previous tweet, we show these stars in angular momentum space, the space in which the Helmi streams are commonly selected. We find that the debris splits into two clumps, which we call clump hiL and loL, and we investigate the origin of this substructure.', 'Both clumps populate the negative and positive Vz streams of the debris and they have consistent colour-absolute magnitude diagrams. Also, they both show similar metallicity distributions that peak at [Fe/H] ~ -1.5. This all points to that these stars are of a common origin. https://t.co/yF9r5StJT9', 'We then integrate the orbits of the stars in several Milky Way potentials and calculate the orbital frequencies in order to find the cause of this substructure. Looking at these frequencies we see that the stars split across several orbital families. https://t.co/x5eVw62CsE', 'Almost 40% of stars are on a tight Omega_z : Omega_R = 1:2 resonance, while the remainder form a broad distribution close to Omega_z / Omega_R = 0.7. However, ALL of the stars in the higher Lperp clump populate a resonance in Omega_phi : Omega_z close to the 1:1. https://t.co/QtHxCY8fSH', 'Therefore we conclude that the Omega_phi : Omega_z resonance is the cause of the substructure in angular momentum space. Questions remain on what causes this resonance and why is not exactly 1:1? Understanding these resonances is important for interpreting accreted substructure.', 'Furthermore, since angular momentum is an observable that does not depend on the choice of potential, but the time evolution of this does, then measuring whether these two clumps and the gap remain long-lived is a possible way in which we could constrain the potential locally.', 'A summary and some more visualisations can be found at https://t.co/v19s8gnWig.', '@joannasakowska Thank you so much! üòä', '@rajkpradh Thank you! üòä', '@Tugbaastro Thank you! ‚ò∫Ô∏èüéâ']",https://arxiv.org/abs/2105.09957,"The local stellar halo of the Milky Way contains the debris from several past accretion events. Here we study in detail the structure and properties of nearby debris associated with the Helmi streams, originally identified as an overdensity in integrals of motion space. We use 6D phase-space information from Gaia EDR3 combined with spectroscopic surveys, and we analyse the orbits and frequencies of the stars in the streams using various Galactic potentials. We also explore how the Helmi streams constrain the flattening q, of the Galactic dark matter halo. We find that the streams are split into substructures in integrals of motion space, most notably into two clumps in angular momentum space. The clumps have consistent metallicity distributions and stellar populations, supporting a common progeny. In all the realistic Galactic potentials explored, the Helmi streams stars depict a diffuse distribution close to Omega_z /Omega_R ~0.7. At the same time, the reason for the substructure in angular momentum space appears to be a Omega_z : Omega_phi resonance close to the 1:1. This resonance is exactly the 1:1 in the case that the (density) flattening of the dark halo is q = 1.2. For this halo shape the substructure in angular momenta is also long lasting. Our findings suggest that the structure of the Galactic potential leaves a clear imprint on the properties of phase-mixed debris streams. ","Substructures, Resonances and debris streams"
181,1396465087175593996,2941377939,Ida Momennejad,"['Excited to share new work w @katjahofmann @smdvln @ralgeorgescu @JarekRzepecki Evelyn Zuniga &amp; colleagues! \n\nNavigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation\n<LINK>\naccepted@ICML\n\nWe propose a method to evaluate human-like navigation\nüßµ1/n <LINK>', ""Many algorithms pass benchmarks, like navigation from a given location to a goal location in 3D games. \n\nBut passing benchmarks doesn't guarantee human-like navigation behavior nor cognitively or neurally plausible human-like algorithms/representations. This matters whether...2/n https://t.co/0PM2yhvLWW"", '...the goal is to use the algorithm to understand human behavior or cognition, as in cog neuro,\nor to design agents that generate human-like behavior in XBoX games so humans can play w agents as a team.\n\nWould pursuing these goals simultaneously accelerate achieving both?\n3/n https://t.co/0WJQhWrHbG', 'We used a modified version of a 4X4 player game by Ninja Theory, Bleeding Edge, for solo navigation.\n\nThe 3rd person perspective (camera following agent) allows us to observe how different algorithms learn to navigate to 16 goals &amp; judge the human-likeness of their navigation\n4/n https://t.co/q3VNfHd5uw', 'Inspired by \nAlonso et al (2020) Deep Reinforcement Learning for Navigation in AAA Video Games https://t.co/VRTfLVx8nc \n\nIn this paper we compared 2 baseline architectures: \n- symbolic agent, w only access to x y z, angle, distance \n- hybrid agent, w symbolic &amp; vision layers\n5/n https://t.co/LbXe6hGKkO', ""Navigation Turing Test (NTT)\nOnce the agents were sufficiently trained, we designed 2 NTT studies in which we showed pairs of videos of agent vs. human navigation or pairs of 2 agents navigating the game.\n\nParticipants judged which of each pair's videos was played by a human\n6/n https://t.co/3ZxdPfFxcx"", 'Across both studies\n- no agent passed Turing test (humans were ~80% accurate in discerning human players)\n- the hybrid agent was judged more human-like\n- but many were uncertain\n- Among those who confused humans, some reported assuming humans will be worse at navigation!\n7/n https://t.co/RAPOm5j570', ""Eventually, we'd want algorithms that can judge human-likeness of artificial behavior rather than asking human participants, &amp; one day meta-cognitively optimize behavior w these judgments.\nSo we compared a number of different architectures &amp;  inputs for artificial NTT judges\n8/n https://t.co/fyzJjUz8O0"", 'Comparing 6 artificial NTT judges (some discriminator of adversarial imitation learning, some CNNs) we found:\n- Symbolic input ~ highest mean accuracy \n- Symbolic &amp; topdown trajectory most correlated with human ranking \n- All models perform poorly at differentiating 2 agents\n9/n https://t.co/8Kk6kgMOJt', ""Lessons:\n- just passing navigation benchmarks doesn't lead to human-like nav\nnor human-like algorithms or representations\n- automated assessment of human-likeness of human vs agent easier than agent vs agent\n- future work can study key behavioral features of human-like nav\n10/n https://t.co/GjLpyJgRwe"", 'Why would this matter?\nML: Eventually we want machines that are not data hungry for performing every task, that can generalize &amp; adapt human-likeness\n\nCog neuro: cognitively plausible algorithms for human-like navigation &amp; judgment can advance comp cog neuro theories of nav\n11/n https://t.co/BLEeyUZPs3', 'Toward these goals &amp; inspired by meta-cognition in humans a future direction is to add in-built artificial judges of human-likeness that can flexibly modify and adapt parameters of the agent. \nBuilding machines that learn to evaluate human-likeness can advance ML &amp; cog neuro\n12/n https://t.co/59muMH5NX6', ""This assumes that such metacognition has enabled us humans to observe our own behavior, compare it with others' behavior, &amp; flexibly adapt/adjust common human behaviors.\nSpeculation: could lead to RL/ML approaches to metacognition &amp; social/cultural cog, &amp; advance all 3.\nn/n https://t.co/uVzmTEJZn5""]",https://arxiv.org/abs/2105.09637,"A key challenge on the path to developing agents that learn complex human-like behavior is the need to quickly and accurately quantify human-likeness. While human assessments of such behavior can be highly accurate, speed and scalability are limited. We address these limitations through a novel automated Navigation Turing Test (ANTT) that learns to predict human judgments of human-likeness. We demonstrate the effectiveness of our automated NTT on a navigation task in a complex 3D environment. We investigate six classification models to shed light on the types of architectures best suited to this task, and validate them against data collected through a human NTT. Our best models achieve high accuracy when distinguishing true human and agent behavior. At the same time, we show that predicting finer-grained human assessment of agents' progress towards human-like behavior remains unsolved. Our work takes an important step towards agents that more effectively learn complex human-like behavior. ",Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation
182,1396102295515852802,29274213,Chaz Firestone,"[""Here we go! Up today at #VSS2021 is @mattgroh (+ @_ziv_e @RosalindPicard), sharing results from a massive  online study on human vs machine deepfake detection. VSS link here (<LINK>), &amp; there's a preprint too! (<LINK>)\n\n11:45am ET in Talk Room 2! <LINK> <LINK>""]",https://arxiv.org/abs/2105.06496,"The recent emergence of machine-manipulated media raises an important societal question: how can we know if a video that we watch is real or fake? In two online studies with 15,016 participants, we present authentic videos and deepfakes and ask participants to identify which is which. We compare the performance of ordinary human observers against the leading computer vision deepfake detection model and find them similarly accurate while making different kinds of mistakes. Together, participants with access to the model's prediction are more accurate than either alone, but inaccurate model predictions often decrease participants' accuracy. To probe the relative strengths and weaknesses of humans and machines as detectors of deepfakes, we examine human and machine performance across video-level features, and we evaluate the impact of pre-registered randomized interventions on deepfake detection. We find that manipulations designed to disrupt visual processing of faces hinder human participants' performance while mostly not affecting the model's performance, suggesting a role for specialized cognitive capacities in explaining human deepfake detection performance. ","Deepfake Detection by Human Crowds, Machines, and Machine-informed
  Crowds"
183,1395724577200869376,426509606,Yamir Moreno,"['In our last work, we study the effects\nof different social distancing schemes on the large scale spreading of diseases via metapopulation models. Check it out at: <LINK>. Great work by @paulocv92 @SrAleta &amp; @FranciscoICMC. <LINK>']",https://arxiv.org/abs/2105.09697,"To contain the propagation of emerging diseases that are transmissible from human to human, non-pharmaceutical interventions (NPIs) aimed at reducing the interactions between humans are usually implemented. One example of the latter kind of measures is social distancing, which can be either policy-driven or can arise endogenously in the population as a consequence of the fear of infection. However, if NPIs are lifted before the population reaches herd immunity, further re-introductions of the pathogen would lead to secondary infections. Here we study the effects of different social distancing schemes on the large scale spreading of diseases. Specifically, we generalize metapopulation models to include social distancing mechanisms at the subpopulation level and model short- and long-term strategies that are fed with local or global information about the epidemics. We show that different model ingredients might lead to very diverse outcomes in different subpopulations. Our results suggest that there is not a unique answer to the question of whether contention measures are more efficient if implemented and managed locally or globally and that model outcomes depends on how the full complexity of human interactions is taken into account. ","Modeling the effects of social distancing on the large-scale spreading
  of diseases"
184,1395683669554122753,195773271,Marios Fournarakis,"['Quantized training can pave the way for efficient on-device training. In our latest work with @mnagel87, we propose ""In-hindsight quantization range estimation"" (<LINK>) to enable fast and efficient gradient quantization.\n(accepted at EDLCV Workshop @CVPR 2021) <LINK>']",https://arxiv.org/abs/2105.04246,"Quantization techniques applied to the inference of deep neural networks have enabled fast and efficient execution on resource-constraint devices. The success of quantization during inference has motivated the academic community to explore fully quantized training, i.e. quantizing back-propagation as well. However, effective gradient quantization is still an open problem. Gradients are unbounded and their distribution changes significantly during training, which leads to the need for dynamic quantization. As we show, dynamic quantization can lead to significant memory overhead and additional data traffic slowing down training. We propose a simple alternative to dynamic quantization, in-hindsight range estimation, that uses the quantization ranges estimated on previous iterations to quantize the present. Our approach enables fast static quantization of gradients and activations while requiring only minimal hardware support from the neural network accelerator to keep track of output statistics in an online fashion. It is intended as a drop-in replacement for estimating quantization ranges and can be used in conjunction with other advances in quantized training. We compare our method to existing methods for range estimation from the quantized training literature and demonstrate its effectiveness with a range of architectures, including MobileNetV2, on image classification benchmarks (Tiny ImageNet & ImageNet). ",In-Hindsight Quantization Range Estimation for Quantized Training
185,1395278587629035522,410981632,Robin Haunschild,['We published a #bibliometric study on #QuantumTechnology: <LINK>'],https://arxiv.org/abs/2105.08750,"The second quantum technological revolution started around 1980 with the control of single quantum particles and their interaction on an individual basis. These experimental achievements enabled physicists and engineers to utilize long-known quantum features - especially superposition and entanglement of single quantum states - for a whole range of practical applications. We use a publication set of 54,598 papers from the Web of Science published between 1980 and 2018 to investigate the time development of four main subfields of quantum technology in terms of numbers and shares of publication as well as the occurrence of topics and their relation to the 25 top contributing countries. Three successive time periods are distinguished in the analyses by their short doubling times in relation to the whole Web of Science. The periods can be characterized by the publication of pioneering works, the exploration of research topics, and the maturing of quantum technology, respectively. Compared to the US, China has a far over proportional contribution to the worldwide publication output, but not in the segment of highly-cited papers. ","Quantum technology 2.0 -- topics and contributing countries from 1980 to
  2018"
186,1395193882510495751,941435998239711232,Juan Irving,"['We have released our latest research on coverage path planning as #arxiv preprint: <LINK>\nIn it, we propose a sprinkler-aware method for 2d area disinfection using micro aerial vehicles.']",https://arxiv.org/abs/2105.08743,"The pandemic by COVID-19 is causing a devastating effect on the health of global population. There are several efforts to prevent the spread of the virus. Among those efforts, cleaning and disinfecting public areas have become important tasks. In order to contribute in this direction, this paper proposes a coverage path planning algorithm for a spraying drone, a micro aerial vehicle that has mounted a sprayer/sprinkler system, to disinfect areas. In contrast with planners in the state-of-the-art, this proposal presents i) a new sprayer/sprinkler model that fits a more realistic coverage volume to the drop dispersion and ii) a planning algorithm that efficiently restricts the flight to the region of interest avoiding potential collisions in bounded scenes. The drone with the algorithm has been tested in several simulation scenes, showing that the algorithm is effective and covers more areas with respect to other approaches in the literature. Note that the proposal is not limited to disinfection applications, but can be applied to other ones, such as painting or precision agriculture. ",Coverage Path Planning for Spraying Drones
187,1395095368770080771,3252140220,priyanka nanayakkara,"['Curious about how AI researchers are thinking about the societal consequences of their work?\n\nWe (@JessicaHullman @ndiakopoulos) read through hundreds of @NeurIPSConf 2020 broader impact statements to find out. Our results are in a new @AIESConf paper (<LINK>) üßµ', 'Authors vary in how they describe impacts in terms of \n\n* valence (pos vs neg consequences)\n* orientation (technical or society-facing outcomes)\n* specificity (highly contextual to general)\n* uncertainty (in terms of future outcomes)', 'They discuss impacts falling under several categories: https://t.co/SVvM4M5iCS', 'Some authors say that due to the theoretical nature of their work, there are no foreseeable, ethical, or negative societal consequences (9% of statements in our sample). Others say/imply the opposite (10%).', 'Authors also recommend ways of mitigating negative consequences and achieving the following outcomes:\n\n* safe and effective use of AI (21% of statements in our sample)\n* ensure ""fair"" outcomes (6%)\n* protect privacy (5%)\n* reduce environmental impact (1%)', 'For more details (including *who* authors say might be impacted and *who* is responsible for future action), please see one of our blogposts or paper: \n1) https://t.co/1H3xBjgl0e\n2) https://t.co/s1xJGCYdUM (by @JessicaHullman)\n3) the full paper!!! ‚ú®https://t.co/HklogslAVU‚ú®', 'Very curious to hear your thoughts on these findings! For example, are there any impact areas that you find surprising? Any that you think are missing? Any other impressions around how the #NeurIPS2020 broader impact statement requirement went?', ""@DiverseInAI @NeurIPSConf @icmlconf Interesting, bias in particular came up quite a bit, and I think could definitely be interesting to further investigate in these statements. \nAlso, here's our dataset: https://t.co/sG9RacEDlL\nWe extracted these statements randomly from PDFs here: https://t.co/intNxuw2od""]",https://arxiv.org/abs/2105.04760,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals. ","Unpacking the Expressed Consequences of AI Research in Broader Impact
  Statements"
188,1394921931380559872,1238481001304686594,Pablo Mart√≠nez-Mirav√©,"['New paper today! With @MariamTortola @spastorcarpi @PFdeSalas and Stefano Gariazzo\n\n""Cosmological radiation density with non-standard neutrino-electron interactions""\n\n<LINK>\n\nWe study how NSI with electrons alter the picture of neutrino decoupling <LINK>', 'We address the variation on the effective number of neutrinos in the presence of NSI, including the effect in oscillations, annihilation and scattering between neutrinos and electrons and positrons. https://t.co/41Ff3ttmZC', 'We also show that future cosmological data would complement terrestrial experiments (and even provide competitive constraints in some of the couplings)!\n\nAnd most importantly, I really enjoyed learning and working with these great collaborators!\n\n üòÉüòÉüòÉüòÉüòÉ']",https://arxiv.org/abs/2105.08168,"Neutrino non-standard interactions (NSI) with electrons are known to alter the picture of neutrino decoupling from the cosmic plasma. NSI modify both flavour oscillations through matter effects, and the annihilation and scattering between neutrinos and electrons and positrons in the thermal plasma. In view of the forthcoming cosmological observations, we perform a precision study of the impact of non-universal and flavour-changing NSI on the effective number of neutrinos, $N_{eff}$. We present the variation of $N_{eff}$ arising from the different NSI parameters and discuss the existing degeneracies among them, from cosmology alone and in relation to the current bounds from terrestrial experiments. Even though cosmology is generally less sensitive to NSI than these experiments, we find that future cosmological data would provide competitive and complementary constraints for some of the couplings and their combinations. ","Cosmological radiation density with non-standard neutrino-electron
  interactions"
189,1394919311169495041,1151073402188267521,Jaedong Hwang,"['#CVPR2021 Exemplar-Based Open-Set Panoptic Segmentation Network.\n\ntl;dr: We propose a new task, open-set panoptic segmentation with a baseline network inspired by exemplar theory.\n\narXiv: <LINK>\ncode: <LINK> <LINK>']",https://arxiv.org/abs/2105.08336,"We extend panoptic segmentation to the open-world and introduce an open-set panoptic segmentation (OPS) task. This task requires performing panoptic segmentation for not only known classes but also unknown ones that have not been acknowledged during training. We investigate the practical challenges of the task and construct a benchmark on top of an existing dataset, COCO. In addition, we propose a novel exemplar-based open-set panoptic segmentation network (EOPSN) inspired by exemplar theory. Our approach identifies a new class based on exemplars, which are identified by clustering and employed as pseudo-ground-truths. The size of each class increases by mining new exemplars based on the similarities to the existing ones associated with the class. We evaluate EOPSN on the proposed benchmark and demonstrate the effectiveness of our proposals. The primary goal of our work is to draw the attention of the community to the recognition in the open-world scenarios. The implementation of our algorithm is available on the project webpage: this https URL ",Exemplar-Based Open-Set Panoptic Segmentation Network
190,1394811787879452672,15163166,Sherri Rose,['New preprint on identifying undercompensated groups defined by multiple attributes (w/Anna Zink) <LINK>\n\nWe construct a group importance measure &amp; find previously unidentified groups at risk of discrimination in the healthcare system\n\nCode <LINK> <LINK>'],https://arxiv.org/abs/2105.08493,"Risk adjustment in health care aims to redistribute payments to insurers based on costs. However, risk adjustment formulas are known to underestimate costs for some groups of patients. This undercompensation makes these groups unprofitable to insurers and creates incentives for insurers to discriminate. We develop a machine learning method for ""group importance"" to identify unprofitable groups defined by multiple attributes, improving on the arbitrary nature of existing evaluations. This procedure was designed to evaluate the risk adjustment formulas used in the U.S. health insurance Marketplaces as well as Medicare. We find that a number of previously unidentified groups with multiple chronic conditions are undercompensated in the Marketplaces risk adjustment formula, while groups without chronic conditions tend to be overcompensated in the Marketplaces. The magnitude of undercompensation when defining groups with multiple attributes is larger than with single attributes. No complex groups were found to be consistently under- or overcompensated in the Medicare risk adjustment formula. Our work provides policy makers with new information on potential targets of discrimination in the health care system and a path towards more equitable health coverage. ","Identifying Undercompensated Groups Defined By Multiple Attributes in
  Risk Adjustment"
191,1394196453929603078,1316512308466647040,Christoph Bergmeir,"['We have put together a new time series data repository for forecasting. It is dedicated to sets of series for cross-learning/global modelling, and to single but very long series, find it here:\nPaper link: <LINK>\nWebsite: <LINK>']",https://arxiv.org/abs/2105.06643,"Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms. ",Monash Time Series Forecasting Archive
192,1393999020662542336,101810581,Animesh Garg,"['Ever wondered if we solved RL in continuous time, would it be better for robotics. We find in many cases, Yes!\n\n\nValue Iteration in Continuous Actions, States, and Time \n<LINK>\n\n@_mlutter @MannorShie @Jan_R_Peters Dieter Fox\n@icmlconf #ICML2021 @NVIDIAAI <LINK>', 'Continuous-Time Fitted Value Iteration (cFVI) enables dynamic programming with a known approximate model for problems with continuous states and actions without discretization.\n\ncFVI results in much smoother value and policy functions as compared to Deep RL methods: SAC &amp; PPO https://t.co/YfBlvw3KHx', 'This method works for both offline RL (DP-cFVI) and online RL (RTDP-cFVI)\n\ncFVI has a similar performance to deep RL methods in sim but excels in sim2real transfer!\n\nHere is a SAC simulation policy transferred to real env! https://t.co/WhlhxfKkkZ', 'And on-policy PPO is not necessarily much better either. \n\nFurther, the continuous-time policy can be used for intermittent control (event-based control) where interacting with the system occurs at irregular time-steps and each interaction is associated with a cost! https://t.co/SisQjVqvcT', ""Let's first look into what Continuous-Time Formulation looks like.\n\nDiscrete version can be solved with Value Iteration! (iteratively updates the value of each state using the Bellman optimality)\nHowever, VI is impractical for larger MDPs due to as the computational complexity. https://t.co/Akccp9Ro8j"", 'Often, Robot dynamics models are naturally expressed in the continuous-time formulation. \n\nIn fact, many are control problems are affine in dynamics and the reward is separable into a non-linear state reward\nand the action cost. https://t.co/S6H0jE34ln', 'To extend value iteration to continuous actions, we show that one can solve this maximization analytically for control affine dynamics!\n\nthis avoids the policy optimization used by the usual actor-critic approaches or discretization in classical methods. https://t.co/Ykd9uxT9cd', 'The inner part is the direction of the steepest ascent &amp; the action cost rescales this direction.\n- a zero action cost for actions makes the optimal policy a bang-bang controller\n- A quadratic action cost linearly rescales the gradient\n- A barrier-shaped cost clips the gradients https://t.co/LI1IGa4E0I', 'While the Cart-pole and Furuta pendulum might feel simplistic to a non-expert, they highlight that even simple setups with non-linear dynamics can have hard-to-model dynamics effects such as backlash and control lag!\n\nCheck out this SAC model on Furuta Pendulum https://t.co/C2LI8eFdfi', 'And here is the PPO on the same task!\n\nThese experiments show that while Deep RL methods may work with domain randomization but with a lot of domain-specific tricks!\n\nWe need better controllers that work without as much DR https://t.co/6dKxVQQtwR', 'Check out more papers on RL both in Machine Learning and Robotics\nhttps://t.co/UOIu7RF368', '@yisongyue @_mlutter @MannorShie @Jan_R_Peters @icmlconf @NVIDIAAI Thanks \n@yisongyue\n this looks very neat and clearly has a bigger more realistic inverted pendulum than ours üòâ\nWe will be sure to cite this in the updated version &amp; learn from it in our follow-ups!']",https://arxiv.org/abs/2105.04682,"Classical value iteration approaches are not applicable to environments with continuous states and actions. For such environments, the states and actions are usually discretized, which leads to an exponential increase in computational complexity. In this paper, we propose continuous fitted value iteration (cFVI). This algorithm enables dynamic programming for continuous states and actions with a known dynamics model. Leveraging the continuous-time formulation, the optimal policy can be derived for non-linear control-affine dynamics. This closed-form solution enables the efficient extension of value iteration to continuous environments. We show in non-linear control experiments that the dynamic programming solution obtains the same quantitative performance as deep reinforcement learning methods in simulation but excels when transferred to the physical system. The policy obtained by cFVI is more robust to changes in the dynamics despite using only a deterministic model and without explicitly incorporating robustness in the optimization. Videos of the physical system are available at \url{this https URL}. ","Value Iteration in Continuous Actions, States and Time"
193,1393323170032193536,2889619139,liubov üá∫üá¶ üè¥‚Äç‚ò†Ô∏èü§çüíô‚ù§,"['#arXiv  time: our new paper on delay propagation in networks, where we study perturbation spreading in Belgian train networks\n<LINK> #mobility #delay \nwork was started at wwcs2019 @complex_warsaw   and during @criparis @CriResearch 3 years almost‚ö°Ô∏è']",https://arxiv.org/abs/2105.06111,"Railway systems form an important means of transport across the world. However, congestions or disruptions may significantly decrease these systems' efficiencies, making predicting and understanding the resulting train delays a priority for railway organisations. Delays are studied in a wide variety of models, which usually simulate trains as discrete agents carrying delays. In contrast, in this paper, we define a novel model for studying delays, where they spread across the railway network via a diffusion-like process. This type of modelling has various advantages such as quick computation and ease of applying various statistical tools like spectral methods, but it also comes with limitations related to the directional and discrete nature of delays and the trains carrying them. We apply the model to the Belgian railways and study its performance in simulating the delay propagation in severely disrupted railway situations. In particular, we discuss the role of spatial aggregation by proposing to cluster the Belgian railway system into sets of stations and adapt the model accordingly. We find that such aggregation significantly increases the model's performance. For some particular situations, a non-trivial optimal level of spatial resolution is found on which the model performs best. Our results show the potential of this type of delay modelling to understand large-scale properties of railway systems. ",Modelling railway delay propagation as diffusion-like spreading
194,1393268069703639040,713117884239753216,Xiaolong Wang,"['Cycle comes again: we propose contrastive learning with cross-video cycle-consistency. Instead of learning by augmenting a single image, our method forms a cycle across different videos to provide positive training pairs from different instances. \n\n<LINK>\n(1/n) <LINK>', 'Start from one frame in a video, we find its soft nearest neighbor from other videos as a forward step. The cycle-consistency is achieved when the soft nearest neighbor finds its closest frame back within the same video as the start frame. \n\n(2/n) https://t.co/HYxWYKAcki', 'The self-supervised image representation can be generalized to multiple downstream tasks beyond action recognition in videos, including image classification and object tracking. \n\nProject page: https://t.co/Fd4lOmC97M‚Ä¶\n\nJoint work with @Happy_Wu\n\n(3/n)', '@kevin_zakka It needs to have two videos from the same action class to form the cycle. So in the sense of knowing the two videos are from the same action class, it is labeled.', '@kevin_zakka But this is just stating the fact in the TCC experiments. If you are using PennAction category labels to get the pairs, it is ""require human annotators to provide grouth-truth pairs"" no? We are not trying to downgrade TCC, it is great work, and the statement is also neutral.', '@kevin_zakka And I also think it makes perfect sense for TCC to align two videos from the same category since it is performing a frame-level alignment. It will not make sense for TCC to apply on aligning two completely different action videos. For us, we are not trying to align two videos.']",https://arxiv.org/abs/2105.06463,"Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learning. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, representations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Unlike intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learning. This allows to collect positive sample pairs across different video instances, which we hypothesize will lead to higher-level semantics. We validate our method by transferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at this https URL ","Contrastive Learning of Image Representations with Cross-Video
  Cycle-Consistency"
195,1393203111452348417,857263993207099392,Ori Fox,"['It‚Äôs a BIG paper day! (i.e., one of my own) \n\nINFRARED obs of Type Ia thermonuclear supernovae out to z=0.1. \n\nOffering the low-z anchor for #jwst and @NASARoman measurements of DARK ENERGY.\n\nWe find NO evidence for a ``mass-step‚Äô‚Äô! \n\n#astrospax #ratir \n<LINK> <LINK>']",https://arxiv.org/abs/2105.06236,"We present optical and near-infrared (NIR, $YJH$-band) observations of 42 Type Ia supernovae (SNe Ia) discovered by the untargeted intermediate Palomar Transient Factory (iPTF) survey. This new data-set covers a broad range of redshifts and host galaxy stellar masses, compared to previous SN Ia efforts in the NIR. We construct a sample, using also literature data at optical and NIR wavelengths, to examine claimed correlations between the host stellar masses and the Hubble diagram residuals. The SN magnitudes are corrected for host galaxy extinction using either a global total-to-selective extinction ratio, $R_V$=2.0 for all SNe, or a best-fit $R_V$ for each SN individually. Unlike previous studies which were based on a narrower range in host stellar mass, we do not find evidence for a ""mass-step"", between the color- and stretch-corrected peak $J$ and $H$ magnitudes for galaxies below and above $\log(M_{*}/M_{\odot}) = 10$. However, the mass-step remains significant ($3\sigma$) at optical wavelengths ($g,r,i$) when using a global $R_V$, but vanishes when each SN is corrected using their individual best-fit $R_V$. Our study confirms the benefits of the NIR SN Ia distance estimates, as these are largely exempted from the empirical corrections dominating the systematic uncertainties in the optical. ","Near-IR Type Ia SN distances: host galaxy extinction and mass-step
  corrections revisited"
196,1393134095530696704,754948023382310912,Niclas Rieger,['You have some big üåèdata &amp; want to find possible time lags between your variables for each location?\n\nGive complex MCA a try! We applied it on SST üåä &amp; precipitation üåßÔ∏è #ERA5 to identify lagged teleconnectionsüîóüïô\n\narXiv‚û°Ô∏è <LINK>\n#openaccess #openscience <LINK>'],https://arxiv.org/abs/2105.04618,"A proper description of ocean-atmosphere interactions is key for a correct understanding of climate evolution. The interplay among the different variables acting over the climate is complex, often leading to correlations across long spatial distances (teleconnections). In some occasions, those teleconnections occur with quite significant temporal shifts that are fundamental for the understanding of the underlying phenomena but which are poorly captured by standard methods. Applying orthogonal decomposition such as Maximum Covariance Analysis (MCA) to geophysical data sets allows to extract common dominant patterns between two different variables, but generally suffers from (i) the non-physical orthogonal constraint as well as (ii) the consideration of simple correlations, whereby temporally offset signals are not detected. Here we propose an extension, complex rotated MCA, to address both limitations. We transform our signals using the Hilbert transform and perform the orthogonal decomposition in complex space, allowing us to correctly correlate out-of-phase signals. Subsequent Varimax rotation removes the orthogonal constraints, leading to more physically meaningful modes of geophysical variability. As an example of application, we have employed this method on sea surface temperature and continental precipitation; our method successfully captures the temporal and spatial interactions between these two variables, namely for (i) the seasonal cycle, (ii) canonical ENSO, (iii) the global warming trend, (iv) the Pacific Decadal Oscillation, (v) ENSO Modoki and finally (vi) the Atlantic Meridional Mode. The complex rotated modes of MCA provide information on the regional amplitude, and under certain conditions, the regional time lag between changes on ocean temperature and land precipitation. ","Lagged teleconnections of climate variables identified via complex
  rotated Maximum Covariance Analysis"
197,1392834854014816258,55522623,Avi Sil,['#ACL2021NLP with @elgreco_winter @HaoyangWen @hansolosan ! We propose VAULT( VAriable Unified Long Text Representation for Machine Reading Comprehension) : a novel model which improves runtime performance by over 16x without sacrificing F1.\nPdf: <LINK>\n#NLProc <LINK>'],https://arxiv.org/abs/2105.03229,"Existing models on Machine Reading Comprehension (MRC) require complex model architecture for effectively modeling long texts with paragraph representation and classification, thereby making inference computationally inefficient for production use. In this work, we propose VAULT: a light-weight and parallel-efficient paragraph representation for MRC based on contextualized representation from long document input, trained using a new Gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth. We validate our VAULT architecture showing experimental results on two benchmark MRC datasets that require long context modeling; one Wikipedia-based (Natural Questions (NQ)) and the other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain -- TechQA -- with large improvement over a model fine-tuned on a previously published large PLM. ","VAULT: VAriable Unified Long Text Representation for Machine Reading
  Comprehension"
198,1392714026455535617,422672164,Dr Michael Reidinger,"['Neutrino constraints to scotogenic dark matter interacting in the Sun\n\nHere we present a study of neutrino signals from the annihilation of dark matter particles which have been gravitationally captured in the Sun, in the framework of the scotogenic model.\n<LINK>']",https://arxiv.org/abs/2105.05613,"Radiative seesaw models have the attractive property of providing dark matter candidates in addition to generation of neutrino masses. Here we present a study of neutrino signals from the annihilation of dark matter particles which have been gravitationally captured in the Sun, in the framework of the scotogenic model. We compute expected event rates in the IceCube detector in its 86-string configuration. As fermionic dark matter does not accumulate in the Sun, we study the case of scalar dark matter, with a scan over the parameter space. Due to a naturally small mass splitting between the two neutral scalar components, inelastic scattering processes with nucleons can occur. We find that for small mass splittings, the model yields very high event rates. If a detailed analysis at IceCube can exclude these parameter points, our findings can be translated into a lower limit on one of the scalar couplings in the model. For larger mass splittings only the elastic case needs to be considered. We find that in this scenario the XENON1T limits exclude all points with sufficiently large event rates. ",Neutrino constraints to scotogenic dark matter interacting in the Sun
199,1392520079234019334,926129519827697664,Aleksandra Urman,['We find that people click on the very first (!) web search result over 50% of the time; the first result page gets over 97% of user clicks. This and more about web search behaviour based on web tracking data from üá©üá™ andüá®üá≠ in our new preprint <LINK>'],https://arxiv.org/abs/2105.04961,"We conduct a comparative analysis of desktop web search behaviour of users from Germany (n=558) and Switzerland (n=563) based on a combination of web tracking and survey data. We find that web search accounts for 13% of all desktop browsing, with the share being higher in Switzerland than in Germany. We find that in over 50% of cases users clicked on the first search result, with over 97% of all clicks being made on the first page of search outputs. Most users rely on Google when conducting searches, and users preferences for other engines are related to their demographics. We also test relationships between user demographics and daily number of searches, average share of search activities among tracked events by user as well as the tendency to click on higher- or lower-ranked results. We find differences in such relationships between the two countries that highlights the importance of comparative research in this domain. Further, we observe differences in the temporal patterns of web search use between women and men, marking the necessity of disaggregating data by gender in observational studies regarding online information behaviour. ","You Are How (and Where) You Search? Comparative Analysis of Web Search
  Behaviour Using Web Tracking Data"
200,1392511253877297157,882511862524465152,Aleksander Madry,['How can we build deep networks that are easier to debug? With @RICEric22 and @ShibaniSan we find that fitting a sparse linear decision layer on top of model features gets you surprisingly far. Blogs: <LINK> &amp; <LINK> Paper: <LINK> <LINK>'],https://arxiv.org/abs/2105.04857,"We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at this https URL ",Leveraging Sparse Linear Layers for Debuggable Deep Networks
201,1392453704029196294,1835092896,Dr. Teresa Lynn,"['Our Shared Task Survey article (preprint) is now online: <LINK>\n\nThanks to 175 reponses from our #NLProc community on Shared Task issues (e.g. transparency, data, fairness), we propose a checklist for STs going forward #EthNLP\n\nw @CParraEsc @Jossmo  @janiedunne', 'https://t.co/zEP7bU8l75']",https://arxiv.org/abs/2105.05020,"This article reports on a survey carried out across the Natural Language Processing (NLP) community. The survey aimed to capture the opinions of the research community on issues surrounding shared tasks, with respect to both participation and organisation. Amongst the 175 responses received, both positive and negative observations were made. We carried out and report on an extensive analysis of these responses, which leads us to propose a Shared Task Organisation Checklist that could support future participants and organisers. The proposed Checklist is flexible enough to accommodate the wide diversity of shared tasks in our field and its goal is not to be prescriptive, but rather to serve as a tool that encourages shared task organisers to foreground ethical behaviour, beginning with the common issues that the 175 respondents deemed important. Its usage would not only serve as an instrument to reflect on important aspects of shared tasks, but would also promote increased transparency around them. ",Towards transparency in NLP shared tasks
202,1392285789652926467,130881465,Alex Nitz,"[""In work led by @CollinCapano we find support for a sought after sub-dominant mode in the 'ringdown' of GW190521. This enables a test of the nature of the final black hole and suggests limits on the mass ratio of the initial binary.\n\n<LINK> <LINK>"", ""General relativity predicts that for a Kerr black hole (spinning, no charge) if it is perturbed, such as when resulting from the merger of two progenitor black holes, it will emit gravitational radiation during this 'ringdown' with a predictable discrete spectrum of frequencies."", 'A key principle is that spectrum of these modes only depends on the total mass and spin of the black hole. (the relative amplitudes may however depend on the initial system).', ""By comparing the observed modes and the prediction from General relativity, we can test the nature of the final black hole and look for signs of new physics. We find the observed signal is very consistent with what you'd expect from a perturbed Kerr black hole. https://t.co/EQAHsF6AGa"", 'The presence of one particular mode also indicates that the original system was unlikely to have been made of equal mass black holes (NR simulations suggest that unequal masses are required to excite this mode). This allows one to predict what the mass ratio may have been. https://t.co/JvZoDqpflt', 'GW190521 has been somewhat of mystery for the gravitational-wave community. It is the highest mass merger observed to date, there is a possible EM flare observed in coincidence, and several studies have suggested the merger may be eccentric. Likely mysteries remain!']",https://arxiv.org/abs/2105.05238,"When two black holes merge, the late stage of gravitational wave emission is a superposition of exponentially damped sinusoids. According to the black hole no-hair theorem, this ringdown spectrum depends only on the mass and angular momentum of the final black hole. An observation of more than one ringdown mode can test this fundamental prediction of general relativity. Here we provide strong observational evidence for a multimode black hole ringdown spectrum using the gravitational wave event GW190521, with a Bayes factor of $\sim 40$ preferring two fundamental modes over one. The dominant mode is the $\ell=m=2$ harmonic, and the sub-dominant mode corresponds to the $\ell=m=3$ harmonic. We estimate the redshifted mass and dimensionless spin of the final black hole as $330^{+30}_{-40}\,\mathrm{M}_\odot$ and $0.87^{+0.05}_{-0.10}$, respectively. The detection of the two modes disfavors a binary progenitor with equal masses; the mass ratio is constrained to $0.4^{+0.2}_{-0.3}$. We find that the final black hole is consistent with the no hair theorem and constrain the fractional deviation from general relativity of the sub-dominant mode's frequency to be $-0.01^{+0.07}_{-0.11}$. ","Observation of a multimode quasi-normal spectrum from a perturbed black
  hole"
203,1392177740342652929,774170436057731073,Alexis Conneau,"[""üì¢ New study: 'Larger-Scale Transformers for Multilingual Masked Language Modeling' üö®\n\n <LINK>\n\nWe study and release two larger-scale XLM-R models: \n- XLM-R XL (3.5B params) \n- XLM-R XXL (10.7B params)\n\nModels and code will be publicly available soon\n\n1/5"", 'This simple study shows that with higher capacity:\n\n- multilingual models get much better performance on cross-lingual tasks (e.g. +2.4% acc on XNLI, +4.1 F1 on MLQA) \n\n- while exhibiting very strong performance on high-resource languages (+0.3% on GLUE over RoBERTa-Large)\n\n2/5', 'The training settings (data, updates, parameters) are very different compared to mT5, differences are explained in the paper. Models are competitive however although the XXL model is a bit below.\n\n3/5', ""We hope these encoder-only alternatives will be useful for your research in multilingual NLP, and in high- or low-resource language understanding.\n\nConsider giving them a try even if the language you're interested in is not covered in the 100 languages.\n\n4/5"", 'Work done with the great @NamanGoyal21, @JefferyDuu,  @myleott and Giri Anantharaman.\n\n5/5', ""@_lewtun Thanks! We haven't evaluated on CoNLL tasks. Would be interesting to get those results indeed. When the model gets open-sourced in a few days and land on the HF platform, it should make things simple for everyone to apply these models to any NLU task."", ""@_lewtun Cool, thanks :). I'm optimistic results should get better on sequence labeling tasks too.""]",https://arxiv.org/abs/2105.00572,"Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available. ",Larger-Scale Transformers for Multilingual Masked Language Modeling
204,1392041955023073280,298064479,Hermann Blum,['The next step for machine learning on robots: We propose to combine continual learning and self-supervision such that robots perpetually learn and improve during deployment.\n<LINK> <LINK>'],http://arxiv.org/abs/2105.01595,"We propose a novel robotic system that can improve its perception during deployment. Contrary to the established approach of learning semantics from large datasets and deploying fixed models, we propose a framework in which semantic models are continuously updated on the robot to adapt to the deployment environments. By combining continual learning with self-supervision, our robotic system learns online during deployment without external supervision. We conduct real-world experiments with robots localising in 3D floorplans. Our experiments show how the robot's semantic perception improves during deployment and how this translates into improved localisation, even across drastically different environments. We further study the risk of catastrophic forgetting that such a continuous learning setting poses. We find memory replay an effective measure to reduce forgetting and show how the robotic system can improve even when switching between different environments. On average, our system improves by 60% in segmentation and 10% in localisation accuracy compared to deployment of a fixed model, and it maintains this improvement while adapting to further environments. ",Self-Improving Semantic Perception for Indoor Localisation
205,1391813238921236482,2361934191,Louigi Addario-Berry,"['Anna Brandenberger (@a__bra), Jad Hamdan, C√©line Kerriou and I just posted a new article, \n""Universal height and width bounds for random trees"". \n\nWe prove some (I think) nice results &amp; also propose a change to the field\'s nomenclature (see Tweet 18).\n<LINK>\n1/23', 'A plane tree is a connected acyclic graph with a distinguished root vertex, where the children of each vertex have a left-to-right order (as occurs when the tree is drawn in the plane, say with the root at the bottom of the page and children above their parents). 2/23 https://t.co/kTjOQJHenq', 'The number of plane trees with n edges (and so n+1 vertices) is given by the Catalan number C_n= (2n choose n)/(n+1). Here are the plane trees with 4 edges. \n3/23 https://t.co/PkTSvok5TH', 'The (out)degree of a node v in a plane tree T is its number of children. The degree statistics of T is the vector (n(c),c &gt;= 0), where n(c) is the number of nodes with exactly c children. \n4/23', ""The number of plane trees with *fixed* degree statistics is given by a formula similar to Cayley's formula: the # of trees with degree statistics (n(c),c&gt;= 0) is \n\n(1/n)(n choose n(c),c &gt;= 0) = (1/n) * n!/(product_c n(c)!) \n[That formula is also the alt text for the image.]\n5/23 https://t.co/07rwLeNbRN"", 'Our paper studies random trees with fixed degree statistics; for any degree statistics the # of such trees is finite, so you can think of just drawing all the trees with given degree statistics, putting them in a bag, mixing them up and picking one out uniformly at random. \n6/23', 'We specifically study the height (maximum distance of any node from the root) and the width (greatest number of nodes at a given level) of such trees. \n7/23', 'We show that for a random tree T with degree statistics n=(n(c),c &gt;= 0) typical nodes are found at distance |n|_1/|n|_2 from the root, where |n|_p = (sum_c c^p n(c))^{1/p}. \n8/23', 'This formula isn\'t quite true when n(1) is very close to n=|n|_1, because nodes with exactly one child cause ""a vertical stretch"" of the tree. In full generality the denominator |n|_2 should be replaced by (|n|_2^2 - n(1))^{1/2}. \n9/23', 'We also prove stretched exponential tail bounds on the distance of typical nodes from the root, and use them to deduce similar tail bounds on the height of the whole tree. If the height is small the width must be large, so this also gives us lower tail bounds on the width. \n10/23', 'The nice thing about this model is that it is a ""refinement"" of multiple other models: on the probabilistic side it refines (single-type) branching process models, and on the combinatorial side it refines ""simply generated trees"" models. \n11/23', 'What ""refines"" means is that for those models, if you condition on the degree statistics, you get uniformly random trees with those statistics, so you can recover results about those models from our results by averaging out over the degree statistics.\n12/23', ""This allows us to use our results to prove some conjectures about simply generated trees from this 2012 paper of Svante Janson. (Svante has written 96 other papers since then; if you're looking for open problems, you might try looking in those!)  https://t.co/4oCaREZLuW\n13/23"", 'Our main tool is a new algorithm for sampling a sequence of degrees whose distribution is the same as that the degrees along the path to a random node in a random tree. You can think of this as an algorithm for sampling a node in the tree without exploring the whole tree. \n14/23', ""The algorithm is *almost* the following: list the degrees of the tree in size-biased random order as D=(D_1,...,D_n). Then let tau be a stopping time for D, defined by the fact that the prob. of stopping at time t, given we haven't already stopped, is just (D_1+...+D_t)/n.\n15/23"", 'Then D_1,...,D_T is (almost) distributed as the degrees along the path to a random node in a tree with vertex degrees D_1,...,D_n. The ""almost"" is because the formula (D_1+...+D_t)/n is a slight simplification - but the story I\'m telling here doesn\'t lie in its essence.\n16/23', 'This allows us to access information about the height of a random node in a random tree by studying a simple stopping time for a size-biased random sequence, and is what unlocks essentially all the results of our paper. \n17/23', 'The change in nomenclature that we propose is this: the mathematical+statistical community should stop calling the family trees of branching process ""Galton-Watson trees"", and start calling them ""Bienaym√© trees.\n18/23', 'The logic behind this is twofold. First, Bienaym√© introduced these trees before Galton and Watson, and with greater mathematical rigor (and fewer mistakes!). This fact was already known, but we provide references to both original and contemporary sources in the literature.\n19/23', 'Second, Francis Galton was one of the founders of the eugenics movement - he literally coined the term ""eugenics"". He described the meaning of his new term as follows:\n20/23', '""We greatly want a brief word to express the science of improving stock, which is by no means confined to questions of judicious mating, but which, especially in the case of man, \n(contd.)\n21/23', '...takes cognisance of all influences that tend in however remote a degree to give to the more suitable races or strains of blood a better chance of prevailing speedily over the less suitable than they otherwise would have had.""\n22/23', 'We prefer not to honour the founder of the eugenics movement by continuing to attach his name to these beautiful mathematical objects. \n\nBut even if one ""only cares about the science"", the logic for changing the name to ""Bienaym√© trees"" is clear: Bienaym√© got there first.\n\n23/23']",https://arxiv.org/abs/2105.03195,"We prove non-asymptotic stretched exponential tail bounds on the height of a randomly sampled node in a random combinatorial tree, which we use to prove bounds on the heights and widths of random trees from a variety of models. Our results allow us to prove a conjecture and settle an open problem of Janson (this https URL), and nearly prove another conjecture and settle another open problem from the same work (up to a polylogarithmic factor). The key tool for our work is an equivalence in law between the degrees along the path to a random node in a random tree with given degree statistics, and a random truncation of a size-biased ordering of the degrees of such a tree. We also exploit a Poissonization trick introduced by Camarri and Pitman (this https URL) in the context of inhomogeneous continuum random trees, which we adapt to the setting of random trees with fixed degrees. Finally, we propose and justify a change to the conventions of branching process nomenclature: the name ""Galton-Watson trees"" should be permanently retired by the community, and replaced with the name ""Bienaym\'e trees"". ",Universal height and width bounds for random trees
206,1391741409212968961,4851173447,Henrique F. de Arruda,"['Ou new preprint is out <LINK>. In this study, we analyzed the robustness of citation networks regarding the keywords used for collecting the articles. @LdaFCosta @moondark', '@qeios @LdaFCosta @moondark Thanks!']",https://arxiv.org/abs/2105.01693,"Citation networks can reveal many important information regarding the development of science and the relationship between different areas of knowledge. Thus, many studies have analyzed the topological properties of such networks. Frequently, citation networks are created using articles acquired from a set of relevant keywords or queries. Here, we study the robustness of citation networks with regards to the keywords that were used for collecting the respective articles. A perturbation approach is proposed, in which the influence of missing keywords on the topology and community structure of citation networks is quantified. In addition, the relationship between keywords and the community structure of citation networks is studied using networks generated from a simple model. We find that, owing to its highly modular structure, the community structure of citation networks tends to be preserved even when many relevant keywords are left out. Furthermore, the proposed model can reflect the impact of missing keywords on different situations. ",On the Stability of Citation Networks
207,1390850364698554375,737140237948715008,Michael J. Biercuk,"['Have you had a chance to discover that now we can use #ReinforcementLearning to have #quantumcomputers tune themselves up to find high fidelity #quantum logic? The capability, developed by @qctrlHQ is described in our new @arxiv post!\n\n<LINK>']",https://arxiv.org/abs/2105.01079,"Quantum computers promise tremendous impact across applications -- and have shown great strides in hardware engineering -- but remain notoriously error prone. Careful design of low-level controls has been shown to compensate for the processes which induce hardware errors, leveraging techniques from optimal and robust control. However, these techniques rely heavily on the availability of highly accurate and detailed physical models which generally only achieve sufficient representative fidelity for the most simple operations and generic noise modes. In this work, we use deep reinforcement learning to design a universal set of error-robust quantum logic gates on a superconducting quantum computer, without requiring knowledge of a specific Hamiltonian model of the system, its controls, or its underlying error processes. We experimentally demonstrate that a fully autonomous deep reinforcement learning agent can design single qubit gates up to $3\times$ faster than default DRAG operations without additional leakage error, and exhibiting robustness against calibration drifts over weeks. We then show that $ZX(-\pi/2)$ operations implemented using the cross-resonance interaction can outperform hardware default gates by over $2\times$ and equivalently exhibit superior calibration-free performance up to 25 days post optimization using various metrics. We benchmark the performance of deep reinforcement learning derived gates against other black box optimization techniques, showing that deep reinforcement learning can achieve comparable or marginally superior performance, even with limited hardware access. ","Experimental Deep Reinforcement Learning for Error-Robust Gateset Design
  on a Superconducting Quantum Computer"
208,1390718417196523527,3374566037,Kate Storey-Fisher,"[""it's an exciting #paperday - check out <LINK> for all of your weird galaxy needs!\n\nwe train a generative adversarial network to detect anomalous galaxies in hyper suprime-cam images. head to <LINK> to find more yourself‚ú®\n\nthread w/ fun pictures:"", 'we train a GAN on ~1 million images from HSC in a magnitude slice. one of these panels is from the real sample, the other is randomly sampled images generated by our GAN trained to follow the distribution of the real data\n(like for GAN is (a), retweet for GAN is (b)) https://t.co/eoj4XEYfwg', 'we set the GAN to generate its best reconstruction of each image, and assign an ""anomaly score"" based on the residual between the real &amp; reconstruction, in both pixel-space and the discriminator\'s feature-space. (the discriminator is better at finding *interesting* anomalies!) https://t.co/RXmVAf63jl', 'the tricky part is distinguishing scientifically interesting anomalies from optical artifacts. to do this we train an autoencoder to reduce the dimensionality of the image residuals, which contain info about why the image is anomalous. a UMAP on these shows useful clustering! https://t.co/jbussKXgCV', ""with this approach we find many interesting anomalies! here's a categorized selection - from galaxy mergers to extreme blue star-forming regions, and some unknown unknowns https://t.co/oEE1jz4yT1"", 'one category was bright blue sources off-center in diffuse regions, and we collaborated with observers to follow some up. one turns out to be a super weird system with an extremely blue, possibly enriched HII region spatially offset from a metal-poor star-forming dwarf galaxy. https://t.co/cTQOY05yUC', 'this project has been loads of fun &amp; learning, and a cool collaboration between ML folks and observational galaxy folks - huge thanks to my coauthors, Marc, Nesar, Francois, Alexie, Yifei, @dr_guangtou, and X, and my KSPA cohort!', '@lachlancaster thanks lachlan!!', '@mung__bean :) thanks Mitchell!!', '@BenneHolwerda thank you!!']",https://arxiv.org/abs/2105.02434,"The problem of anomaly detection in astronomical surveys is becoming increasingly important as data sets grow in size. We present the results of an unsupervised anomaly detection method using a Wasserstein generative adversarial network (WGAN) on nearly one million optical galaxy images in the Hyper Suprime-Cam (HSC) survey. The WGAN learns to generate realistic HSC-like galaxies that follow the distribution of the data set; anomalous images are defined based on a poor reconstruction by the generator and outlying features learned by the discriminator. We find that the discriminator is more attuned to potentially interesting anomalies compared to the generator, and compared to a simpler autoencoder-based anomaly detection approach, so we use the discriminator-selected images to construct a high-anomaly sample of $\sim$13,000 objects. We propose a new approach to further characterize these anomalous images: we use a convolutional autoencoder to reduce the dimensionality of the residual differences between the real and WGAN-reconstructed images and perform UMAP clustering on these. We report detected anomalies of interest including galaxy mergers, tidal features, and extreme star-forming galaxies. A follow-up spectroscopic analysis of one of these anomalies is detailed in the Appendix; we find that it is an unusual system most likely to be a metal-poor dwarf galaxy with an extremely blue, higher-metallicity HII region. We have released a catalog with the WGAN anomaly scores; the code and catalog are available at this https URL, and our interactive visualization tool for exploring the clustered data is at this https URL ","Anomaly detection in Hyper Suprime-Cam galaxy images with generative
  adversarial networks"
209,1390716629156200455,904198497783738368,Sridhar B,"['Our study on energetics of Bay of Bengal show interesting results. We see different regions (south, central, and north) in Bay of Bengal displaying some unique dynamics &amp; mechanisms by which energy is produced and transferred.\n\nMore details here:\n\n<LINK>']",http://arxiv.org/abs/2105.02430,"Regional dynamics of Bay of Bengal is studied using Open Boundary Condition (OBC) in Modular Ocean Model (MOM) to understand the effect of primary and secondary mesoscale features on various bulk ocean products and turbulent fluxes. A horizontal resolution of 0.1o is adopted to resolve a range of mesoscale features. In order to parametrize the vertical mixing, K-Profile Parametrization (KPP) scheme is implemented. The results show successful implementation of OBC in a regional domain with exchange of mass and energy and conservation of mass at the boundaries. The Sea Surface Temperature (SST) and Sea Surface Salinity (SSS) are validated with SODA reanalysis, which are found to be in good agreement. The Mixed Layer Depth (MLD) pattern is very well represented, albeit the magnitude is slightly under-predicted in comparison with SODA reanalysis. Additionally, analysis of flow energetics reveal regional differences in turbulent kinetic energy (K), production flux (P), buoyancy flux (B), and dissipation ({\epsilon}). Our results clearly reveal the presence of inverse energy-cascade in the southern Bay of Bengal, wherein energy flows back into mean flow structures from the turbulent eddies. The re-energization of mean flow structures is likely to allow the large-scale circulation to persist for longer periods, thereby modifying the local dynamics. Further analysis shows that the B term is the major source of turbulence production in the north and central Bay of Bengal regions. These results convey the various mechanisms by which energy is produced and transferred in different regions of Bay of Bengal. ","Regional modeling of surface and sub-surface dynamics in the Bay of
  Bengal using Modular Ocean Model"
210,1390703468248281090,1195375581111689216,Joseph Viviano,"[""Always brilliant @SashaMTL and I looked into the darkest recesses of the @CommonCrawl to find what unsavory content large language models are trained on, and didn't like what we found! Our work was accepted into @aclmeeting and you can read about it here: <LINK>"", 'Key takeaway: ""language quality"" measures like perplexity are ~not~ strongly correlated with tools specifically identifying hate speech, violence, or sexual content, but these are the measures commonly used to filter inputs when training large language models!', ""This raises a question: what kinds of guarantees can we make about the behavior, especially generations, of a large language model if we don't have a clear understanding of the data it was trained with? https://t.co/FOhHQO0Ls4"", ""We don't have the answer to this question, but think it's important for the community to come up with scalable solutions to either dataset curation or controllable generation with guarantees, as these models are deployed and interact with society at large."", '... and somehow the @emojisigilbot generates a near-perfect sigil for our paper on the same day https://t.co/3V6i6euf3P', '@lorenlugosch @SashaMTL @CommonCrawl @aclmeeting We don\'t address this directly. I see 2 paths forward: removing ""bad"" from the training sets (I am not convinced by Buckman that such a model would generate OOD ""bad"" behavior), or regularizing the model to avoid such behaviour. Both require a better understanding of the dataset.', '@zngu @SashaMTL @CommonCrawl @aclmeeting @ParaCrawl This is great! I will be important to benchmark these and other detection tools, hopefully in collaboration with a group that has a lot of compute to spare! The common crawl is ~huge~.', ""@zngu @SashaMTL @CommonCrawl @aclmeeting @ParaCrawl This sounds like a great opportunity. We would be happy to help! It should be relatively easy to extend what we've done to include other methods / quality scores and run them on our combined compute... this paper is only the first step."", '@jacobmbuckman @lorenlugosch @SashaMTL @CommonCrawl @aclmeeting Yes that is pretty alarming. Always willing to change my mind with evidence! Suggests the better strategy would be to control generation. I still think that will come with understanding the datasets better. Active learning will be hard to scale reliably on its own.', '@jacobmbuckman @lorenlugosch @SashaMTL @CommonCrawl @aclmeeting That said, a language model not trained on sex would also not have this problem. Depending on the application, it might be reasonable to require this.']",https://arxiv.org/abs/2105.02732,"Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis. ","What's in the Box? A Preliminary Analysis of Undesirable Content in the
  Common Crawl Corpus"
211,1390599285209718784,882303076505456642,Timon Emken,"[""On today's @arxiv, we present a new study on general #DarkMatter-electron interactions and how crystal targets, e.g. semiconductors, could respond to them.\n\n<LINK>\n\n@ChalmersPhysics @ETHen @TheOKC @Stockholm_Uni @Fysikum\n\n1/6"", 'This paper is a crucial extension of our previous groundwork on effective interactions between DM particles and electrons bound in isolated atoms to more complex condensed matter systems.\n\nhttps://t.co/tbqOcbQ1go\n\n2/6', 'Experiments like SENSEI probe sub-GeV DM-electron interactions in semiconductor crystals. \n\nDue to specific assumptions on the fundamental DM interaction, the condensed matter physics can be absorbed into one form factor first derived by Essig et al.\n\nhttps://t.co/BM8qsvnof5\n\n3/6', 'Our more general, effective approach reveals 4 novel form factors or ""crystal response functions"", necessary to make predictions for many DM models.\n\nAs we know nothing about the potential DM interactions apart from gravity, this is crucial to interpret experimental data.\n\n4/6 https://t.co/fCMGPZ6tM7', 'This paper is the result of an awesome Swedish-Swish collaboration between the two DM groups of Riccardo Catena (@ChalmersPhysics) and @janconrad351 (@TheOKC @Stockholm_Uni @Fysikum), and the Materials Theory group of @NicolaSpaldin (@ETHen).\n\n5/6', ""The code QEdark-EFT to compute the crystal response functions was developed by Einar Urdshals, PhD student @ChalmersPhysics and this project's driving force, and Marek Matas, postdoc @ETHen.\n\nIt is available under\n\nhttps://t.co/8BBFKqVIpO\n\n6/6""]",https://arxiv.org/abs/2105.02233,"We develop a formalism to describe the scattering of dark matter (DM) particles by electrons bound in crystals for a general form of the underlying DM-electron interaction. Such a description is relevant for direct-detection experiments of DM particles lighter than a nucleon, which might be observed in operating DM experiments via electron excitations in semiconductor crystal detectors. Our formalism is based on an effective theory approach to general non-relativistic DM-electron interactions, including the anapole, and magnetic and electric dipole couplings, combined with crystal response functions defined in terms of electron wave function overlap integrals. Our main finding is that, for the usual simplification of the velocity integral, the rate of DM-induced electronic transitions in a semiconductor material depends on at most five independent crystal response functions, four of which were not known previously. We identify these crystal responses, and evaluate them using density functional theory for crystalline silicon and germanium, which are used in operating DM direct detection experiments. Our calculations allow us to set 90% confidence level limits on the strength of DM-electron interactions from data reported by the SENSEI and EDELWEISS experiments. The novel crystal response functions discovered in this work encode properties of crystalline solids that do not interact with conventional experimental probes, suggesting the use of the DM wind as a probe to reveal new kinds of hidden order in materials. ",Crystal responses to general dark matter-electron interactions
212,1390568528252751874,1279047283452063744,Francesca Lizzi,"['New pre-print on arXiv! We developed a system, named LungQuant, to quantify the percentage of infected lung on CT scans of COVID-19 patients and to assess CT-SS. We used only public available data and studied whether is possible to aggregate CT data.  <LINK>', '@DataScience_PhD @DinoPedreschi']",https://arxiv.org/abs/2105.02566,"The automatic assignment of a severity score to the CT scans of patients affected by COVID-19 pneumonia could reduce the workload in radiology departments. This study aims at exploiting Artificial intelligence (AI) for the identification, segmentation and quantification of COVID-19 pulmonary lesions. We investigated the effects of using multiple datasets, heterogeneously populated and annotated according to different criteria. We developed an automated analysis pipeline, the LungQuant system, based on a cascade of two U-nets. The first one (U-net_1) is devoted to the identification of the lung parenchyma, the second one (U-net_2) acts on a bounding box enclosing the segmented lungs to identify the areas affected by COVID-19 lesions. Different public datasets were used to train the U-nets and to evaluate their segmentation performances, which have been quantified in terms of the Dice index. The accuracy in predicting the CT-Severity Score (CT-SS) of the LungQuant system has been also evaluated. Both Dice and accuracy showed a dependency on the quality of annotations of the available data samples. On an independent and publicly available benchmark dataset, the Dice values measured between the masks predicted by LungQuant system and the reference ones were 0.95$\pm$0.01 and 0.66$\pm$0.13 for the segmentation of lungs and COVID-19 lesions, respectively. The accuracy of 90% in the identification of the CT-SS on this benchmark dataset was achieved. We analysed the impact of using data samples with different annotation criteria in training an AI-based quantification system for pulmonary involvement in COVID-19 pneumonia. In terms of the Dice index, the U-net segmentation quality strongly depends on the quality of the lesion annotations. Nevertheless, the CT-SS can be accurately predicted on independent validation sets, demonstrating the satisfactory generalization ability of the LungQuant. ","Quantification of pulmonary involvement in COVID-19 pneumonia by means
  of a cascade oftwo U-nets: training and assessment on multipledatasets using
  different annotation criteria"
213,1390563362027802626,2739515118,christian majenz,"[""Together with colleagues at QuSoft, I realized that there are guides for first-time reviewers and even first-time PC-chairs, but we couldn't find any for first-time PC members. So we wrote one: <LINK>"", 'As far as I can see, only one of my coauthors, @Huebli, is on twitter. Is that correct?']",https://arxiv.org/abs/2105.02773,"In theoretical computer science, conferences play an important role in the scientific process. The decisions whether to accept or reject articles is taken by the program committee (PC) members. Serving on a PC for the first time can be a daunting experience. This guide will help new program-committee members to understand how the system works, and provide useful tips and guidelines. It discusses every phase of the paper-selection process, and the tasks associated to it. ","A Guide for New Program Committee Members at Theoretical Computer
  Science Conferences"
214,1390478689066962950,2911287964,Thomas Kupfer,"['How do you find rapid brightness changes? Well, just take continuous observations for a few hours and search for brightness changes. That is what we did and we did it where most stars are: In the Galactic Plane. <LINK> \nStay tuned for a summary tomorrow.']",https://arxiv.org/abs/2105.02758,"We present the goals, strategy and first results of the high-cadence Galactic plane survey using the Zwicky Transient Facility (ZTF). The goal of the survey is to unveil the Galactic population of short-period variable stars, including short period binaries and stellar pulsators with periods less than a few hours. Between June 2018 and January 2019, we observed 64 ZTF fields resulting in 2990 deg$^2$ of high stellar density in ZTF-$r$ band along the Galactic Plane. Each field was observed continuously for 1.5 to 6 hrs with a cadence of 40 sec. Most fields have between 200 and 400 observations obtained over 2-3 continuous nights. As part of this survey we extract a total of $\approx$230 million individual objects with at least 80 epochs obtained during the high-cadence Galactic Plane survey reaching an average depth of ZTF-$r$ $\approx$20.5 mag. For four selected fields with 2 million to 10 million individual objects per field we calculate different variability statistics and find that $\approx$1-2% of the objects are astrophysically variable over the observed period. We present a progress report on recent discoveries, including a new class of compact pulsators, the first members of a new class of Roche Lobe filling hot subdwarf binaries as well as new ultracompact double white dwarfs and flaring stars. Finally we present a sample of 12 new single-mode hot subdwarf B-star pulsators with pulsation amplitudes between ZTF-$r$ = 20-76 mmag and pulsation periods between $P$ = 5.8-16 min with a strong cluster of systems with periods $\approx$ 6 min. All of the data have now been released in either ZTF Data Release 3 or data release 4. ","Year 1 of the ZTF high-cadence Galactic Plane Survey: Strategy, goals,
  and early results on new single-mode hot subdwarf B-star pulsators"
215,1389869832338817024,2209253130,Vito Dichio,"['On ArXiv now the output of a long work and a true friendship more than a collaboration - which started about 1y ago, in Stockholm. \nWe play with Statistical Genetics, do calculations, find results, write appendices, discuss things.\n#biophysics #genetics \n<LINK>']",https://arxiv.org/abs/2105.01428,"This review is about statistical genetics, an interdisciplinary topic between Statistical Physics and Population Biology. The focus is on the phase of Quasi-Linkage Equilibrium (QLE). Our first objective is to clarify under which conditions the QLE phase can be expected to hold in population biology, and how parameters describing a QLE phase relate to underlying population dynamics. Our second objective is to clarify how the stability of the QLE phase is lost. The QLE state was studied at the global genome scale by Neher \& Shraiman (2011): what we will refer to as the Kimura-Neher-Shraiman (KNS) theory describes a population evolving due to the mutations, recombination, genetic drift, natural selection (pairwise epistatic fitness). The main conclusion of KNS is that QLE phase exists at sufficiently high recombination rate (r) with respect to the variability in selection strength (fitness). Combining the results of the KNS theory with the techniques of the Direct Coupling Analysis (DCA), we show that in QLE epistatic fitness can be inferred from the knowledge of the (dynamical) distribution of genotypes in a population. We further consider evolution of a population at higher selection strength with respect to recombination and mutation parameters. We identify a new bi-stable phase which we call the Non-Random Coexistence (NRC) phase where variability persist in the population without either fixating or disappearing. We also identify an intermediate region in the parameter space where a finite population jumps stochastically between a QLE-like state and NRC-like behaviour. ","Statistical Genetics and Direct Coupling Analysis in and out of
  Quasi-Linkage Equilibrium"
216,1389636390808428551,4765523742,Paul Chen,"['Our (non-covid) preprint is online! <LINK>\n\nWe developed a framework to characterize supersaturation throughout colloidal nanocrystal growth. Also use to predict NC growth profiles, find/modulate supersaturation-associated shape evolutions &amp; alter NC features', 'Our amazing co-authors include @AaronJC1 and @FrankGuLab Longer tweetorial to come after peer-review process']",https://arxiv.org/abs/2105.00050,"Supersaturation is the fundamental parameter driving crystal formation, yet its dynamics during the growth of colloidal nanocrystals (NCs) are poorly understood. Experimental characterization of supersaturation in colloidal syntheses has been difficult, limiting insight into the phenomena underlying NC growth. Hence, despite significant interest in the topic, how many types of NCs grow remain unclear. Here, we develop a framework to quantitatively characterize supersaturation in situ throughout NC growth. Using this approach, we investigate the seed-mediated synthesis of colloidal Au nanocubes, revealing a triphasic sequence for the supersaturation dynamics: rapid monomer consumption, sustained supersaturation, and then gradual monomer depletion. These NCs undergo different shape evolutions in different phases of the supersaturation dynamics. As shown with the Au nanocubes, we can use the supersaturation profile to theoretically predict the growth profile of NCs. We then apply these insights to rationally modulate shape evolutions, decreasing the yield of impurity NCs. Our findings demonstrate that the supersaturation dynamics of NC growth can be more complex than previously understood. While this study focuses experimentally on Au NCs, our framework is facile and applicable to a broad range of NCs undergoing classical growth. Thus, our methodology facilitates deeper understanding of the phenomena governing nanoscale crystal growth and provides insight towards the rational design of NCs. ",Framework elucidating the supersaturation dynamics of nanocrystal growth
217,1389591056140886025,3881712928,Valentina,"['New paper today <LINK> with Ernest Ma (University of California, Riverside). We study light freeze-in dark matter in an extension of the singlet majoron model with a Majorana fermion singlet with L=2 and one dark complex scalar singlet with L=1. <LINK>']",https://arxiv.org/abs/2105.00552,"The singlet majoron model of seesaw neutrino mass is appended by one dark Majorana fermion singlet $\chi$ with $L=2$ and one dark complex scalar singlet $\zeta$ with $L=1$. This simple setup allows $\chi$ to obtain a small radiative mass anchored by the same heavy right-handed neutrinos, whereas the one-loop decay of the standard-model Higgs boson to $\chi \chi + \bar{\chi} \bar{\chi}$ provides the freeze-in mechanism for $\chi$ to be the light dark matter of the Universe. ",Radiative Seesaw Dark Matter
218,1400106602737381382,1158385581476515840,Allyson Ettinger,"[""New paper with @langyu94: Previously we found transformer LMs fail to show intelligent phrase meaning composition <LINK> -- now we find that models fine-tuned on tasks seemingly promising for composition (PAWS, SST) don't cut it either <LINK> 1/"", 'In particular, we find often detrimental influence from PAWS fine-tuning -- and follow-up analyses turn up a spurious cue of word swapping distance that may be undermining focus on actual sentence meaning when training on PAWS 2/', 'SST fares a bit better, with (very) localized improvements on phrase composition tests. We speculate that training on such fine-grained phrase labeling may be promising for learning composition -- but ideally labels should involve something more meaning-rich than sentiment 3/', 'This paper is now up on arXiv, and coming out in ACL Findings 4/']",https://arxiv.org/abs/2010.03763,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models. ",Assessing Phrasal Representation and Composition in Transformers
219,1400043486020243457,1302632072201334784,Alexander Dante Camuto,"['Super excited about my latest work with @MatthewWilletts where we study Variational Autoencoders using harmonic analysis. \n\narxiv: <LINK>\n\n1/n', 'We study VAEs using Gaussian spaces, a type of L2 space equipped with the Gaussian measure. We show that VAEs which learn large encoder variances learn decoder functions with a low frequency Fourier representation and can be represented as a low degree polynomial. \n\n2/n https://t.co/C0PeexCuiz', ""We can't directly apply the previous analysis to the VAE encoder but we study the effect of adding Gaussian noise to VAE inputs during training. This noising operation effectively turns the input space into a Gaussian space on a per-datapoint basis.\n\n3/n"", 'Similarly as the variance of the injected noise increases in variance, the encoder learns a lower frequency function which can be represented as a low-degree polynomial. \n\n4/n', ""Finally we use Nash‚Äôs Poincar√© inequality for general Gaussian spaces to show that as the encoder variance and the variance of injected noise increase, the Lipschitz constants of a VAE's encoder and decoder decrease. \n\n5/n"", 'These Lipschitz constants have been shown to be critical to VAE robustness to adversarial attack, but imposing constraints on them is difficult and seriously degrades model quality. It turns out tuning the encoder variance  can impose soft constraints on these constants.\n\n6/n', 'We confirm empirically that increasing the encoder variance and the variance of noise on data improves VAE robustness to adversarial attack by way of smaller network Lipschitz constants. \n\n7/n https://t.co/zZrc0KMixU']",https://arxiv.org/abs/2105.14866,"In this work we study Variational Autoencoders (VAEs) from the perspective of harmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a variety of measure space, we derive a series of results that show that the encoder variance of a VAE controls the frequency content of the functions parameterised by the VAE encoder and decoder neural networks. In particular we demonstrate that larger encoder variances reduce the high frequency content of these functions. Our analysis allows us to show that increasing this variance effectively induces a soft Lipschitz constraint on the decoder network of a VAE, which is a core contributor to the adversarial robustness of VAEs. We further demonstrate that adding Gaussian noise to the input of a VAE allows us to more finely control the frequency content and the Lipschitz constant of the VAE encoder networks. To support our theoretical analysis we run experiments with VAEs with small fully-connected neural networks and with larger convolutional networks, demonstrating empirically that our theory holds for a variety of neural network architectures. ",Variational Autoencoders: A Harmonic Perspective
220,1399728707816857604,948010010,Antonis Anastasopoulos,"['How much data do you really need to train a question answering system? In our #ACL2021NLP paper, we find that you can do just as well with only a few training examples, and we make suggestions for future dataset creators.\nCamera-Ready: <LINK>\nüëáShort threadüëá <LINK>', 'We work with the TyDi-QA dataset, and find that with just 10 training examples (for 9 languages) you can get 80% of skyline accuracy (skyline=train with 1000s of examples). With 100/lang, you get 88% and with 500/lang you get almost 95% of skyline accuracy!', 'Throw in some data augmentation through translation (we translate the English SQuAD dataset to all TyDiQA languages) and it will take you to more than 97% of the skyline accuracy!', ""This also has big implications for the systems' equitability: the difference between having seen a language during fine-tuning and attempting zero-shot cross-lingual transfer is almost 10 percentage points. So, for a fixed annotation budget, we advocate that it's better to"", 'collect e.g. 500 training examples in 19 languages instead of thousands of training examples in 9 languages (also keeping 4k examples for dev+test per lang -- numbers based on actual TyDiQA total instances), and e.g. 200 training examples for 30 languages would be even better!', 'This more egalitarian budget allocation leads to way more equitable performance across the covered languages!', 'Shoutout to Arnab Debnath, Fardina Alam, and @NavidR20 who turned their NLP class project into a nice paper!\nData+Code here: https://t.co/WUhewtR1f2']",https://arxiv.org/abs/2105.14115,"Question answering (QA) in English has been widely explored, but multilingual datasets are relatively new, with several methods attempting to bridge the gap between high- and low-resourced languages using data augmentation through translation and cross-lingual transfer. In this project, we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce QA systems in many languages. Specifically, we perform extensive analysis to measure the efficacy of few-shot approaches augmented with automatic translations and permutations of context-question-answer pairs. In addition, we make suggestions for future dataset development efforts that make better use of a fixed annotation budget, with a goal of increasing the language coverage of QA datasets and systems. Code and data for reproducing our experiments are available here: this https URL ","Towards More Equitable Question Answering Systems: How Much More Data Do
  You Need?"
221,1395536919774121984,68538286,Dan Hendrycks,"[""Can Transformers crack the coding interview? We collected 10,000 programming problems to find out. GPT-3 isn't very good, but new models like GPT-Neo are starting to be able to solve introductory coding challenges.\n\npaper: <LINK>\ndataset: <LINK> <LINK>""]",https://arxiv.org/abs/2105.09938,"While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements. ",Measuring Coding Challenge Competence With APPS
222,1392356192261795844,442597508,Genta Indra Winata,"['Our NMT work was accepted at Findings of ACL @aclmeeting #acl2021nlp #nlproc.  We propose mixed-language training to address unseen language pairs on NMT.\n\n""Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation""\n\n<LINK> <LINK>', '@nedjmaou @aclmeeting @johanliu96 @pascalefung thanks! @nedjmaou also big congrats on your paper acceptance!', 'Congrats to all authors @johanliu96 @pascalefung !!!', 'We show the effectiveness of using mixed-language input generated from monolingual data without using any real translation data. https://t.co/gneOfG1jxE']",https://arxiv.org/abs/2105.03953,"The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Fine-tuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pre-training mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the fine-tuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at this https URL ","Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural
  Machine Translation"
223,1390626303863410690,4272310881,Samin Aref,"['üö®Preprint Alert!üö®\n@zpneal and I introduce new models to optimally partition‚ûï#SignedNetworks‚ûñbased on generalized balance. Applying them on US House networks, we find a previously obscured third coalition composed of highly effective legislators. <LINK>\nRT pls <LINK>']",https://arxiv.org/abs/2105.01913,"In network science, identifying optimal partitions of a signed network into internally cohesive and mutually divisive clusters based on generalized balance theory is computationally challenging. We reformulate and generalize two binary linear programming models that tackle this challenge, demonstrating their practicality by applying them them to partition networks of collaboration in the US House of Representatives. These models guarantee a globally optimal network partition and can be practically applied to signed networks containing up to 30,000 edges. In the US House context, we find that a three-cluster partition is better than a conventional two-cluster partition, where the otherwise hidden third coalition is composed of highly effective legislators who are ideologically aligned with the majority party. ","Identifying hidden coalitions in the US House of Representatives by
  optimally partitioning signed networks based on generalized balance"
224,1390127257478434822,104529881,Diogo Souto,"['New paper on @arXiver!\nIn this work we used the @APOGEEsurvey spectra to determine metallicities for a sample of FGKM dwarfs stars from the Coma Berenices open cluster. This is the first APOGEE study determining detailed metallicities of Mdwarfs from an OC <LINK> <LINK>', 'We also observe the signature of atomic diffusion operating in the warmer stars from the cluster, as can be seen in the Figure above. Enjoy the reading!']",https://arxiv.org/abs/2105.01667,"We present a study of metallicities in a sample of main sequence stars with spectral types M, K, G and F ($T_{\rm eff}$ $\sim$ 3200 -- 6500K and log $g$ $\sim$ 4.3 -- 5.0 dex) belonging to the solar neighborhood young open cluster Coma Berenices. Metallicities were determined using the high-resolution (R=$\lambda$/$\Delta$ $\lambda$ $\sim$ 22,500) NIR spectra ($\lambda$1.51 -- $\lambda$1.69 $\mu$m) of the SDSS-IV APOGEE survey. Membership to the cluster was confirmed using previous studies in the literature along with APOGEE radial velocities and Gaia DR2. An LTE analysis using plane-parallel MARCS model atmospheres and the APOGEE DR16 line list was adopted to compute synthetic spectra and derive atmospheric parameters ($T_{\rm eff}$ and log $g$) for the M dwarfs and metallicities for the sample. The derived metallicities are near solar and are homogeneous at the level of the expected uncertainties, in particular when considering stars from a given stellar class. The mean metallicity computed for the sample of G, K, and M dwarfs is $\langle$[Fe/H]$\rangle$ = +0.04 $\pm$ 0.02 dex; however, the metallicities of the F-type stars are slightly lower, by about 0.04 dex, when compared to cooler and less massive members. Models of atomic diffusion can explain this modest abundance dip for the F dwarfs, indicating that atomic diffusion operates in Coma Berenices stars. The [Fe/H] dip occurs in nearly the same effective temperature range as that found in previous analyses of the lithium and beryllium abundances in Coma Berenices. ","A metallicity study of F, G, K and M dwarfs in the Coma Berenices open
  cluster from the APOGEE survey"
225,1389961081225875458,987603820325822464,Miguel R. Alarcon,['My first first-author paper is out! We have used 11M data from 44 different TESS photometers (@stars4all_eu) to study the natural night sky brightness during the last solar minimum. \nüëâ<LINK> <LINK>'],https://arxiv.org/abs/2105.01066,"In 2018, Solar Cycle 24 entered into a solar minimum phase. During this period, 11 million zenithal night sky brightness (NSB) data were collected at different dark sites around the planet, including astronomical observatories and natural protected areas, with identical broadband Telescope Encoder and Sky Sensor photometers (based on the Unihedron Sky Quality Meter TSL237 sensor). A detailed observational review of the multiple effects that contribute to the NSB measurement has been conducted with optimal filters designed to avoid brightening effects by the Sun, the Moon, clouds, and other astronomical sources (the Galaxy and zodiacal light). The natural NSB has been calculated from the percentiles for 44 different photometers by applying these new filters. The pristine night sky was measured to change with an amplitude of 0.1 mag/arcsec$^2$ in all the photometers, which is suggested to be due to NSB variations on scales of up to months and to be compatible with semiannual oscillations. We report the systematic observation of short-time variations in NSB on the vast majority of the nights and find these to be related to airglow events forming above the mesosphere. ",Natural Night Sky Brightness during Solar Minimum
